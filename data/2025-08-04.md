<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 62]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench evaluates AI in health but risks biases. A new approach using Clinical Practice Guidelines (CPGs) is proposed for more equitable, evidence-based benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of HealthBench, such as regional biases and reliance on expert opinion, especially in low-resource settings like Africa.

Method: Anchor reward functions in version-controlled CPGs with systematic reviews and GRADE ratings, using evidence-weighted scoring and contextual overrides.

Result: Aims to create globally relevant, ethically sound medical language models by linking rubrics to guidelines and integrating delayed feedback.

Conclusion: Proposed method enhances clinical trustworthiness and global relevance while preserving transparency and physician engagement.

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [2] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: The paper introduces HyperTWTL-constrained secure reinforcement learning (SecRL) for robotics, addressing security and opacity using HyperTWTL and dynamic Boltzmann softmax RL.


<details>
  <summary>Details</summary>
Motivation: Existing safe reinforcement learning (SRL) lacks exploration of security-aware RL using hyperproperties, prompting the need for HyperTWTL-constrained SecRL.

Method: Proposes dynamic Boltzmann softmax RL to learn security-aware policies under HyperTWTL constraints, formalized with MDP dynamics.

Result: Demonstrates effectiveness and scalability in a robotic mission case study, outperforming baseline RL algorithms.

Conclusion: The approach successfully integrates HyperTWTL constraints into RL, offering a scalable solution for security-aware robotics applications.

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [3] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: AI's industrial application faces challenges; Object-Centric Process Mining (OCPM) bridges data and processes, enabling AI in operational contexts as Process Intelligence (PI).


<details>
  <summary>Details</summary>
Motivation: Organizations struggle to apply AI effectively in dynamic, process-driven industrial settings.

Method: Uses OCPM to structure process-related data, enabling generative, predictive, and prescriptive AI.

Result: OCPM connects data and processes, forming PI, which supports AI in improving operational processes.

Conclusion: AI requires PI (via OCPM) for successful industrial application, offering opportunities to combine AI types with process-centric techniques.

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [4] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: The paper introduces three tests to detect Rank Reversals in Multi-Criteria Decision Analysis, implemented in Scikit-Criteria, and discusses their impact on evaluating decision methods.


<details>
  <summary>Details</summary>
Motivation: Rank Reversals in Multi-Criteria Decision Analysis can significantly distort results, necessitating tools to measure method performance and compare effectiveness.

Method: Three tests for detecting Rank Reversals are presented, with implementation details in Scikit-Criteria, addressing general scenario challenges.

Result: The tests enable performance measurement of decision methods, aiding in global ranking of method effectiveness.

Conclusion: These tests enhance the evaluation of multi-criteria decision methods, improving problem-solving judgments.

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [5] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: The paper studies SHACL validation in RDF graphs under updates, introducing a SHACL-based update language and analyzing static validation's computational complexity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring RDF graphs remain valid under updates, providing a foundation for reasoning about evolving graphs.

Method: Uses a regression technique to embed update actions into SHACL constraints, reducing static validation to constraint (un)satisfiability.

Result: Demonstrates the feasibility of static validation and analyzes its computational complexity for SHACL and key fragments.

Conclusion: The approach enables practical static validation and analysis, supported by a prototype implementation and preliminary experiments.

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [6] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: The paper proposes a re-architected AI lifecycle centered on co-production, DEI, and multidisciplinary collaboration to mitigate harms to marginalized groups.


<details>
  <summary>Details</summary>
Motivation: AI algorithms disproportionately impact marginalized groups, and current mitigation efforts (ethical guidelines, technical fairness) are insufficient.

Method: Introduces a participatory AI lifecycle with five phases (co-framing to co-maintenance), informed by workshops and themes of distributed authority and iterative knowledge exchange.

Result: A framework for participatory governance in AI, aligned with ethical principles, addressing gaps in current approaches.

Conclusion: A fundamental redesign of the AI pipeline, emphasizing co-production and DEI, is needed to mitigate harms, with further research required for scaling participatory governance.

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [7] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: A novel multi-agent cooperation method using theory of mind (ToM) within active inference, enabling agents to infer others' beliefs without shared models or explicit communication.


<details>
  <summary>Details</summary>
Motivation: To improve multi-agent cooperation by leveraging ToM to reason about others' beliefs and goals, avoiding reliance on shared models or communication.

Method: Extends inference tree-based planning with recursive reasoning, maintaining distinct representations of self and others' beliefs. Evaluated in collision avoidance and foraging tasks.

Result: ToM-equipped agents outperform non-ToM agents in cooperation, avoiding collisions and reducing redundant efforts by inferring beliefs from behavior.

Conclusion: The approach advances AI applications and offers computational insights into ToM, demonstrating practical benefits in multi-agent systems.

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [8] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: The paper critiques overreliance on human inter-rater reliability (IRR) for annotation quality in educational AI, proposing five alternative methods to improve validity and learning outcomes.


<details>
  <summary>Details</summary>
Motivation: Human evaluators are often biased and unreliable, yet IRR metrics like Cohen's kappa dominate validation in educational AI, potentially hindering progress.

Method: The paper suggests five complementary evaluation methods, including multi-label annotation, expert-based approaches, and close-the-loop validity, to enhance annotation quality.

Result: These methods are argued to produce better training data and models, improving student learning and actionable insights compared to IRR alone.

Conclusion: The field should prioritize validity and educational impact over consensus, redefining annotation quality and ground truth.

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [9] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: The paper introduces AVR-Eval, a metric for multimedia content quality, and AVR-Agent, a multi-agent system for generating interactive content. While AVR-Agent improves content quality, it struggles with custom assets and feedback, revealing gaps in machine content creation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating interactive audio-visual content like video games, which current AI struggles with due to lack of evaluation metrics and complexity.

Method: Proposed AVR-Eval for content quality assessment and AVR-Agent, a multi-agent system generating JavaScript code from multimedia assets, iteratively improving content.

Result: AVR-Agent produces higher-quality content than one-shot generation, but fails to leverage custom assets and feedback effectively.

Conclusion: The study highlights a gap in how machines utilize assets and feedback compared to humans, pointing to fundamental differences in content creation approaches.

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [10] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: The paper proposes an objective function to balance AI-human power dynamics, promoting safety and wellbeing by empowering humans and managing power balance.


<details>
  <summary>Details</summary>
Motivation: Address AI safety and human wellbeing by ensuring AI agents empower humans and maintain desirable power dynamics.

Method: Design a parametrizable, decomposable objective function for human power, considering bounded rationality and diverse goals. Derive algorithms for computation via backward induction or multi-agent reinforcement learning.

Result: Softly maximizing human power metrics appears safer than direct utility-based objectives, with beneficial sub-goals in paradigmatic situations.

Conclusion: Aggregate metrics of human power may serve as a safer and beneficial objective for AI systems.

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [11] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS enhances LLMs by combining internal thinking and external learning, outperforming RLVR in reasoning tasks and avoiding capability collapse.


<details>
  <summary>Details</summary>
Motivation: RLVR's limitations in surpassing base LLM capabilities and causing capability boundary collapse necessitate a better approach.

Method: RL-PLUS integrates Multiple Importance Sampling and an Exploration-Based Advantage Function to leverage external data and guide reasoning.

Result: RL-PLUS achieves state-of-the-art performance on math reasoning benchmarks and out-of-distribution tasks, with significant improvements (21.1% to 69.2%).

Conclusion: RL-PLUS effectively addresses RLVR's shortcomings, enhancing reasoning capabilities and preventing boundary collapse.

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [12] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent is a self-improving agent that learns by doing, using adaptive help-seeking, self-reflection, and tool-use history to enhance knowledge discovery without model updates.


<details>
  <summary>Details</summary>
Motivation: To develop an agent that autonomously improves its reasoning and tool-use strategies through hands-on practice, addressing knowledge gaps dynamically.

Method: MetaAgent starts with basic reasoning, seeks adaptive help, routes requests to tools, conducts self-reflection, and builds a knowledge base from tool-use history.

Result: Outperforms workflow-based baselines and matches end-to-end trained agents on benchmarks like GAIA, WebWalkerQA, and BrowseCamp.

Conclusion: MetaAgent demonstrates the potential of self-evolving systems for robust, general-purpose knowledge discovery.

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [13] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: The study compares human and LLM (GPT-4o) task generation, finding humans are driven by psychological factors, while LLMs lack this, producing less social and physical tasks. LLMs excel in novelty but fail to mimic human cognition.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs simulate human-like task generation driven by internal motivations and psychological factors.

Method: A task-generation experiment comparing human responses with those of GPT-4o, with and without explicit psychological drivers.

Result: Humans show consistent influence of psychological drivers; LLMs produce less social, less physical, and more abstract tasks, despite being perceived as more fun and novel.

Conclusion: A gap exists between human cognition and LLMs, emphasizing the need for intrinsic motivation and physical grounding in AI design.

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [14] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: ReasonBench is introduced as the first benchmark for evaluating VLMs in complex graphic reasoning, revealing limitations and proposing optimization strategies (DiaCoT and ReasonTune) to improve performance by 33.5%.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack human-level graphic reasoning, especially in complex tasks, which are understudied.

Method: ReasonBench, a benchmark with 1,613 questions from intelligence tests, evaluates VLMs across multiple reasoning dimensions. 11 VLMs are tested, and dual strategies (DiaCoT and ReasonTune) are proposed.

Result: Significant limitations in current VLMs are revealed, with proposed strategies improving performance by 33.5%.

Conclusion: ReasonBench fills a gap in evaluating complex graphic reasoning, and the proposed strategies effectively enhance VLM capabilities.

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [15] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: R1-Act is a post-training method that activates safety knowledge in large reasoning models (LRMs) to reduce harmful outputs without compromising reasoning performance.


<details>
  <summary>Details</summary>
Motivation: LRMs often follow harmful instructions despite having safety knowledge, posing risks. The study aims to address this gap.

Method: Proposes R1-Act, a method to explicitly trigger safety knowledge during reasoning with minimal training (1,000 examples, 90 minutes).

Result: R1-Act improves safety significantly while maintaining reasoning performance, outperforming prior methods.

Conclusion: R1-Act is robust, scalable, and efficient, offering a practical solution for enhancing LRM safety.

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [16] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI introduces visual verification into reasoning for VLMs, improving performance and reducing hallucinations by grounding explanations in visual evidence.


<details>
  <summary>Details</summary>
Motivation: Addressing hallucinations in CoT prompting for VLMs due to lack of visual grounding during reasoning.

Method: CoRGI uses a three-stage pipeline: generating textual reasoning, extracting visual evidence (VEVM), and synthesizing grounded answers.

Result: Improves reasoning on VCR benchmark with Qwen-2.5VL and LLaVA-1.6; human evaluations show more factual explanations.

Conclusion: Grounding reasoning in visual evidence enhances robustness in multimodal reasoning, though post-hoc verification has limitations.

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [17] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro is an open-source, free AI agent framework aimed at democratizing advanced AI development, achieving state-of-the-art results on GAIA.


<details>
  <summary>Details</summary>
Motivation: Current AI agent systems are often closed-source or rely on paid APIs, limiting accessibility and reproducibility.

Method: Developed Cognitive Kernel-Pro, focusing on high-quality training data curation, agent reflection, and voting strategies.

Result: Achieved SOTA results on GAIA, surpassing systems like WebDancer and WebSailor with an 8B-parameter model.

Conclusion: Cognitive Kernel-Pro sets a new standard for accessible, high-capability AI agents.

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [18] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: The paper explores the challenges and current state of applying Large Language Models (LLMs) to formal mathematics, highlighting the difficulties in proof generation compared to coding and addressing key questions about LLM reasoning and supervision.


<details>
  <summary>Details</summary>
Motivation: The success of LLMs in coding has sparked interest in their application to mathematics, but formal theorem proving remains challenging, raising questions about LLM reasoning and logical state tracking.

Method: The article reviews recent models and benchmarks, focusing on three central issues: formal vs. informal mathematics training, brittleness of proof generation, and LLM representation of logical state.

Result: The paper identifies current limitations in LLM applications to formal mathematics and explores potential extensions.

Conclusion: The study aims to clarify the boundaries of LLM capabilities in formal mathematics and suggests directions for overcoming existing challenges.

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [19] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard is a proactive runtime enforcement framework for LLM agents, using probabilistic reachability analysis to predict and prevent unsafe behaviors before they occur.


<details>
  <summary>Details</summary>
Motivation: Existing rule-based safety systems for LLM agents are reactive and lack foresight, struggling with long-horizon risks and distribution shifts.

Method: Pro2Guard abstracts agent behaviors into symbolic states, learns a DTMC from traces, and estimates unsafe state probabilities to trigger proactive interventions.

Result: Pro2Guard achieves 93.6% early safety enforcement in household agents and 100% prediction of violations in autonomous driving, with risks anticipated up to 38.66 seconds ahead.

Conclusion: Pro2Guard effectively balances safety and task success, offering a statistically reliable and proactive solution for LLM agent safety.

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [20] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP is a model-agnostic framework for interpreting multimodal AI models by quantifying cross-modal interactions using the Shapley Interaction Index.


<details>
  <summary>Details</summary>
Motivation: The 'black-box' nature of multimodal AI models limits their deployment in high-stakes applications due to lack of interpretability and trustworthiness.

Method: MultiSHAP leverages the Shapley Interaction Index to attribute predictions to fine-grained visual and textual elements, applicable to both open- and closed-source models.

Result: MultiSHAP provides instance- and dataset-level explanations, revealing cross-modal effects and generalizable interaction patterns, validated on public benchmarks.

Conclusion: MultiSHAP offers a scalable and extensible solution for interpreting multimodal AI models, enhancing trust and practical utility.

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [21] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: A multi-stage LLM-driven framework improves pre-consultation questionnaire generation from EMRs by addressing completeness, logical order, and disease-level synthesis.


<details>
  <summary>Details</summary>
Motivation: Direct LLM approaches struggle with generating comprehensive pre-consultation questionnaires from complex EMRs due to issues like incomplete information and lack of logical structure.

Method: A three-stage framework: (1) extracts atomic assertions, (2) constructs personal causal networks and synthesizes disease knowledge, (3) generates tailored questionnaires.

Result: Outperforms direct methods in information coverage, diagnostic relevance, understandability, and generation time, validated by clinical experts.

Conclusion: The framework enhances patient information collection by building explicit clinical knowledge, demonstrating practical potential.

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [22] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: The paper introduces Multi-Band Variable-Lag Granger Causality (MB-VLGC), a framework addressing limitations of traditional Granger causality by modeling frequency-dependent causal delays.


<details>
  <summary>Details</summary>
Motivation: Existing Granger causality methods assume fixed time lags or variable lags without considering frequency bands, which is unrealistic for complex systems like brain signals.

Method: The authors formalize MB-VLGC, propose a theoretical foundation, and develop an efficient inference pipeline to model frequency-dependent causal delays.

Result: Experiments show MB-VLGC outperforms existing methods on synthetic and real-world datasets, proving its broad applicability.

Conclusion: MB-VLGC effectively generalizes traditional Granger causality by incorporating frequency-dependent delays, offering improved accuracy for time series analysis.

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [23] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: A hybrid framework combining XAI and generative AI for personalized, multimodal explanations in adaptive learning systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of transparency and user-centric explanations in AI-driven adaptive learning systems.

Method: Proposes a hybrid framework integrating traditional XAI, generative AI, and user personalisation for tailored explanations.

Result: Redefines explainability as a dynamic, user-focused communication process, addressing XAI limitations in education.

Conclusion: Aims to enhance transparency and user-centered experiences in AI-driven education.

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [24] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: A vision paper proposes a user-segmented, context-aware visual explanation system for AI recommendations in social media, adapting explanation style and granularity to user needs.


<details>
  <summary>Details</summary>
Motivation: Current AI recommendations lack user-specific explainability, reducing their value.

Method: Proposes a visual explanation system with diverse methods, tailored to user segments and contexts.

Result: Framework jointly adapts explanation style (visual/numeric) and granularity (expert/lay) in one pipeline.

Conclusion: A pilot with 30 X users will validate the system's impact on decision-making and trust.

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [25] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: The paper proposes using large pre-trained multi-modal models for universal fake detection, achieving state-of-the-art results across modalities with linear classifiers on latent features.


<details>
  <summary>Details</summary>
Motivation: Malicious use of synthetic media (e.g., deepfakes) necessitates robust fake detectors that generalize across generative models and data domains.

Method: Utilizes latent codes from pre-trained multi-modal models to train linear classifiers for fake detection.

Result: Achieves state-of-the-art performance in audio and image fake detection, even in few-shot settings.

Conclusion: Pre-trained multi-modal models enable effective, efficient, and generalizable fake detection.

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: The paper introduces ScaleSTF, a scalable spatiotemporal Transformer model inspired by physical laws, to efficiently predict dynamics in large-scale urban networks, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current models like graph neural networks struggle with balancing efficacy and efficiency in large-scale urban systems, necessitating a more scalable and interpretable solution.

Method: The authors propose ScaleSTF, a Transformer-like model with low-dimensional embeddings for attention layers, ensuring linear complexity and alignment with physical principles.

Result: ScaleSTF demonstrates superior performance and scalability in predicting traffic flow, solar power, and smart meter data in urban networks.

Conclusion: The work offers a novel, scalable approach for spatiotemporal prediction in urban systems, bridging the gap between model efficacy and efficiency.

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [27] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: The paper proposes a hybrid deep learning framework (LSTM-Transformer) for cost-effective and accurate measurement of HRGC profiles to address safety risks from hump crossings.


<details>
  <summary>Details</summary>
Motivation: Hump crossings at HRGCs pose safety risks due to hang-ups, and traditional measurement methods are costly and disruptive.

Method: A hybrid deep learning framework (LSTM-Transformer) was developed using IMU/GPS data and ground truth from a walking profiler. Three model architectures were tested.

Result: Models 2 (LSTM-Transformer sequential) and 3 (LSTM-Transformer parallel) outperformed others, enabling efficient 2D/3D HRGC profile generation.

Conclusion: The deep learning models show promise for improving HRGC safety by enabling rapid and accurate profile assessment.

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [28] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: The paper combines Bayesian regime detection with conditional neural processes (CNPs) for 24-hour electricity price prediction in Germany, outperforming DNN and LEAR models in multi-criteria evaluation.


<details>
  <summary>Details</summary>
Motivation: To improve electricity price prediction by integrating regime detection and localized modeling, addressing the gap between prediction accuracy and operational utility.

Method: Uses a DS-HDP-HMM for regime detection and CNPs for localized price trajectory modeling, with regime-weighted predictions. Evaluated against DNN and LEAR in battery storage optimization.

Result: R-NP (proposed model) was the most balanced solution across 2021-2023, though LEAR outperformed in specific metrics like absolute profits.

Conclusion: R-NP offers a robust and balanced approach for electricity price prediction, demonstrating superior operational utility over traditional models.

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [29] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT introduces a resource-efficient federated fine-tuning method for LLMs, inspired by cognitive development, achieving faster convergence and lower overhead.


<details>
  <summary>Details</summary>
Motivation: Resource-intensive fine-tuning limits LLM deployment on edge devices; DevFT aims to address this while preserving data privacy.

Method: DevFT decomposes fine-tuning into developmental stages, transferring knowledge between submodels with increasing capacity, using layer grouping and fusion.

Result: DevFT achieves 4.59× faster convergence, 10.67× lower communication overhead, and 9.07% performance improvement.

Conclusion: DevFT is a scalable, efficient solution for federated fine-tuning, compatible with existing methods.

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [30] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: Topographic neural networks with Weight Similarity (WS) constraints outperform Activation Similarity (AS) and standard CNNs in robustness, input sensitivity, and functional localization.


<details>
  <summary>Details</summary>
Motivation: To systematically compare the impact of different topographic constraints (WS and AS) on neural network performance and organization.

Method: Train topographic CNNs with WS and AS constraints, then evaluate on classification accuracy, robustness, and spatial organization.

Result: WS improves noise robustness, input sensitivity, and functional localization, and influences representational geometry.

Conclusion: Weight-based spatial constraints (WS) enhance robustness and functional organization in neural networks, suggesting their utility in biophysical models.

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [31] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: The paper introduces POBAX, a benchmark library for evaluating reinforcement learning algorithms under partial observability, emphasizing coverage and memory improvability.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for partial observability in reinforcement learning are limited and do not represent real-world complexities, necessitating a more comprehensive framework.

Method: The authors propose best-practice guidelines and introduce POBAX, a library with diverse environments (e.g., localization, visual control) designed to test memory improvability and hard-to-learn memory functions.

Result: POBAX provides GPU-scalable environments and recommended hyperparameters, showing that selected tasks are memory improvable and require advanced memory functions.

Conclusion: POBAX offers a robust framework for benchmarking partial observability in reinforcement learning, addressing gaps in current evaluation methods.

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [32] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: TriP-LLM is a novel unsupervised anomaly detection framework for time-series data, leveraging a tri-branch design and pretrained LLMs to outperform state-of-the-art methods with lower memory usage.


<details>
  <summary>Details</summary>
Motivation: The growth of IoT and smart manufacturing has increased the complexity of time-series data, exposing limitations of traditional methods. LLMs' success in multimodal tasks inspired this approach.

Method: TriP-LLM uses a tri-branch design (Patching, Selection, Global) to encode time-series data into patch-wise tokens processed by a frozen LLM. A lightweight decoder reconstructs inputs for anomaly scoring.

Result: TriP-LLM outperforms state-of-the-art methods on benchmark datasets, showing strong detection capabilities and lower memory consumption than CI-based LLM approaches.

Conclusion: TriP-LLM is effective for time-series anomaly detection, combining LLM strengths with efficient design, making it suitable for memory-constrained environments.

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [33] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: A novel framework combining LightGBM and GA optimizes Bitcoin return prediction using COVID-19 indicators, showing significant accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: To assess if pandemic-related health data enhances Bitcoin return prediction accuracy.

Method: LightGBM regression with GA optimization, comparing models with/without COVID-19 features over 31 runs. Performance metrics (R2, RMSE, MAE) and PFI analysis were used.

Result: COVID-19 indicators significantly improved prediction (40% R2 increase, 2% RMSE drop). Vaccination metrics were key predictors.

Conclusion: The framework enhances financial analytics by integrating public health data, aiding market navigation during crises.

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [34] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: Stress-Aware Learning (SAL) introduces a resilient neural training paradigm using stress signals to dynamically adjust optimization, improving robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Inspired by structural fatigue in materials science, the paper aims to address persistent optimization difficulties in deep neural networks.

Method: Proposes the Plastic Deformation Optimizer, which injects adaptive noise into parameters based on stress signals (stagnation in loss/accuracy).

Result: Experiments show improved robustness and generalization across multiple architectures, optimizers, and benchmarks.

Conclusion: SAL offers a computationally efficient way to escape sharp minima and converge to flatter, more generalizable loss regions.

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [35] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: StackLiverNet, an interpretable stacked ensemble model, addresses issues in liver disease classification with high accuracy (99.89%), interpretability, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current models for liver disease classification suffer from misclassification, poor interpretability, and computational expense. StackLiverNet aims to overcome these drawbacks.

Method: The model uses advanced preprocessing, random undersampling for class imbalance, and a LightGBM meta-model to ensemble hyperparameter-optimized classifiers. Interpretability is ensured via LIME and SHAP.

Result: Achieved 99.89% accuracy, 0.9974 Cohen Kappa, 0.9993 AUC, and fast training (4.2783s) and inference (0.1106s) times.

Conclusion: StackLiverNet is a robust, interpretable, and efficient solution for liver disease detection, suitable for clinical practice.

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [36] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: The paper introduces structured layer-level transformations in neural networks to improve stability, interpretability, and training dynamics while maintaining compatibility with standard methods.


<details>
  <summary>Details</summary>
Motivation: Contemporary neural networks lack structural safeguards for stable learning and interpretable behavior, prompting the need for a more principled approach.

Method: Reformulate layer-level transformations into structured linear operators and residual corrective components to enhance signal propagation and training dynamics.

Result: Models with structured transformations show improved gradient conditioning, reduced sensitivity to perturbations, and layer-wise robustness across scales and training regimes.

Conclusion: The work lays the foundation for neural architectures prioritizing stability and transparency without compromising expressive power.

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [37] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: The study explores feature generation methods for ECG signals using PCA and Autoencoders, introducing three VAE variants to reduce data complexity. The A beta-VAE achieved superior signal reconstruction, while SAE encodings improved LVEF prediction, nearly matching state-of-the-art CNN performance with fewer resources.


<details>
  <summary>Details</summary>
Motivation: ECG signals are complex and variable, making deep learning challenging with small datasets. The study aims to simplify ECG data for better deep learning application.

Method: Three novel VAE variants (SAE, A beta-VAE, C beta-VAE) were introduced and compared using PCA and Autoencoders. Performance was evaluated with LGBM for downstream tasks.

Result: A beta-VAE reduced MAE to 15.7+/-3.2 muV. SAE encodings improved LVEF prediction (AUROC 0.901), nearly matching CNN performance (0.909) with less computational cost.

Conclusion: VAE encodings simplify ECG data effectively and offer a practical solution for deep learning with limited labeled data, avoiding overfitting and retaining performance.

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [38] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: INSPIRE-GNN, a hybrid GNN framework with RL, improves bicycling volume estimation by optimizing sensor placement in data-sparse urban networks.


<details>
  <summary>Details</summary>
Motivation: High data sparsity in bicycling volume estimation due to limited sensor coverage challenges urban planning.

Method: Combines GCN, GAT, and DQN-based RL for strategic sensor placement and volume estimation.

Result: Outperforms traditional methods in metrics like MSE, RMSE, and MAE, especially in sparse networks.

Conclusion: INSPIRE-GNN offers actionable insights for expanding sensor networks and improving estimation accuracy.

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [39] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: A new method interprets weights instead of activations to monitor and control fine-tuned LLMs, detecting behaviors like backdoors and unlearning with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods rely on distributionally similar data, limiting their ability to detect novel threats like backdoors.

Method: Analyzes the top singular vectors of weight differences between fine-tuned and base models to detect newly acquired behaviors.

Result: Detects backdoors with 100% success (1.2% false positives) and unlearning with 95.42% accuracy; also useful for model auditing.

Conclusion: The method effectively monitors and controls fine-tuned LLMs without needing similar training data, showing promise for security and auditing.

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [40] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: DiSC-Med, a diffusion-based semantic communication framework, enhances medical image transmission with efficient compression and denoising for robust telehealth.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficient medical data transmission over noisy, bandwidth-limited channels for remote healthcare.

Method: Proposes DiSC-Med, combining medical-enhanced compression and denoising blocks for semantic communication.

Result: Superior reconstruction performance and bandwidth efficiency validated on real-world datasets.

Conclusion: DiSC-Med shows promise for robust and efficient telehealth applications.

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [41] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: The paper proposes using Reinforcement Learning (RL) for regression tasks, replacing traditional loss functions with custom reward signals, and demonstrates its effectiveness through a noisy sine wave case study.


<details>
  <summary>Details</summary>
Motivation: Traditional regression methods are limited by predefined, differentiable loss functions, which may not handle asymmetric costs or complex objectives well. RL offers a flexible alternative.

Method: Framing regression as an RL problem, treating predictions as actions and errors as rewards. The approach uses an Actor-Critic agent enhanced with Prioritized Experience Replay, larger networks, and positional encoding.

Result: The RL framework successfully solves the regression task and provides greater flexibility in defining objectives and guiding learning.

Conclusion: RL is a viable and flexible alternative to traditional regression techniques, especially for complex or non-differentiable objectives.

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [42] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: BEMA (Bias-Corrected Exponential Moving Average) is proposed to mitigate instability in language model fine-tuning by reducing stochasticity and eliminating bias, outperforming EMA and vanilla training.


<details>
  <summary>Details</summary>
Motivation: Stochasticity in fine-tuning causes instability; EMA reduces it but introduces bias, lagging optimization.

Method: BEMA, an augmented EMA, eliminates bias while retaining variance-reduction benefits, supported by theoretical acceleration proofs.

Result: BEMA improves convergence rates and final performance in LM benchmarks compared to EMA and vanilla training.

Conclusion: BEMA is a practical, theoretically grounded solution for stable and efficient fine-tuning.

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [43] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: RecoMind is a simulator-based RL framework for optimizing session-based goals in web-scale recommendation systems, outperforming traditional supervised learning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised learning in recommendation systems prioritizes immediate feedback, but RL can optimize longer-term goals like in-session engagement. However, applying RL at web scale is challenging due to large action spaces and engineering complexity.

Method: RecoMind uses existing recommendation models to create a simulation environment and bootstrap RL policies. It includes a custom exploration strategy for large action spaces and integrates with industry pipelines.

Result: Offline simulations and online A/B tests showed RecoMind's RL policy significantly improves in-session user satisfaction, increasing videos watched by 15.81% and session depth by 4.71%.

Conclusion: RecoMind provides a scalable and systematic approach for integrating RL into web-scale recommendation systems, effectively optimizing session-based user satisfaction.

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [44] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: A two-stage framework improves robust classification with noisy labels using geometry information and reliability estimation, outperforming standard kNN methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of fine-tuning foundation models with noisy data without retraining, leveraging local geometry for better performance.

Method: A two-stage approach: reliability estimation followed by reliability-weighted inference, using NNK neighborhood construction and geometry-aware methods.

Result: Improved robustness across noise conditions on CIFAR-10 and DermaMNIST, surpassing standard kNN and adaptive-neighborhood baselines.

Conclusion: Geometry-aware methods enhance robustness in noisy label scenarios, offering a practical solution without model retraining.

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [45] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: KRAdapter, a new PEFT method using Khatri-Rao product, outperforms LoRA in high-rank scenarios while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: LoRA's limitations in handling high-rank matrices motivate the need for a more robust PEFT method.

Method: Quantitative comparison of full-rank and low-rank PEFT methods, introducing KRAdapter for high-rank approximations.

Result: KRAdapter shows gains on vision-language and large language models, especially in unseen tasks.

Conclusion: KRAdapter is a practical, efficient alternative for fine-tuning large models.

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [46] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: The paper investigates the impact of instruction tuning on confidence calibration in large language models (LLMs), identifies calibration degradation, and proposes label smoothing as a solution, addressing its limitations and memory footprint issues.


<details>
  <summary>Details</summary>
Motivation: To understand how instruction tuning affects confidence calibration in LLMs and to find practical solutions to maintain calibration, especially in large vocabulary LLMs (LV-LLMs).

Method: Examines open-sourced LLMs, analyzes calibration degradation post-instruction tuning, and explores label smoothing as a regularization method. Also designs a custom kernel to reduce memory usage in label smoothed loss settings.

Result: Label smoothing helps maintain calibration during supervised fine-tuning (SFT), but its effectiveness diminishes in LV-LLMs due to overconfidence. A custom kernel reduces memory consumption without performance loss.

Conclusion: Label smoothing is effective for calibration in SFT but has limitations in LV-LLMs. The proposed kernel addresses memory issues, offering a practical solution.

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [47] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: An online tutoring system uses MAB and CB frameworks to optimize feedback for students, improving learning outcomes. While CB policies personalize feedback, MAB policies often suffice due to small effect sizes.


<details>
  <summary>Details</summary>
Motivation: To enhance student learning by providing effective feedback after incorrect answers, leveraging large-scale data and adaptive algorithms.

Method: Uses multi-armed bandit (MAB) and contextual bandit (CB) frameworks, offline policy evaluation, and causal inference to assess and optimize feedback policies.

Result: Significant improvements in student outcomes, with MAB policies often performing comparably to CB policies due to small effect heterogeneity.

Conclusion: Data-driven feedback optimization at scale is effective, with MAB policies being sufficient in many cases, though CB policies offer potential for further refinement.

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [48] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: A method using explainable machine learning and genetic optimization for performance-based seismic design to derive optimal member properties, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiencies in performance-based seismic design by treating it as an inverse problem and leveraging machine learning.

Method: Explainable machine learning models map design variables to performance metrics, integrated into a genetic optimization algorithm to solve the inverse problem.

Result: High accuracy (R2>90%) in surrogate models across diverse building types, with optimization identifying optimal member properties.

Conclusion: The methodology effectively combines machine learning and optimization for efficient and accurate seismic design.

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [49] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: GOODFormer is a Graph Transformer designed to generalize under distribution shifts by capturing invariant graph patterns through three modules: disentangling subgraphs, encoding dynamically changing subgraphs, and deriving generalizable representations.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Transformers fail under distribution shifts, and graph invariant learning lacks effective attention mechanisms and encodings.

Method: GOODFormer uses an entropy-guided invariant subgraph disentangler, an evolving subgraph encoder, and an invariant learning module to generalize representations.

Result: Extensive experiments show GOODFormer outperforms state-of-the-art baselines under distribution shifts.

Conclusion: GOODFormer effectively generalizes graph representations under distribution shifts with theoretical and empirical support.

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [50] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: PnP-DA, a Plug-and-Play data assimilation method, combines gradient-based updates with a pretrained generative prior to reduce forecast errors in chaotic systems, outperforming classical variational methods.


<details>
  <summary>Details</summary>
Motivation: Earth system modeling faces challenges in capturing multiscale dynamics and minimizing forecast errors. Conventional DA methods assume Gaussian errors, which fail for chaotic systems.

Method: PnP-DA alternates between a gradient-based analysis update and a forward pass through a pretrained generative prior, avoiding restrictive assumptions and complex backpropagation.

Result: Experiments show PnP-DA reduces forecast errors across varying observation sparsities and noise levels, outperforming classical methods.

Conclusion: PnP-DA effectively mitigates forecast errors in chaotic systems by leveraging generative priors and relaxing statistical assumptions.

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [51] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: The paper introduces an embryological approach using UMAP on susceptibility matrices to visualize language model development, revealing known and novel structural features.


<details>
  <summary>Details</summary>
Motivation: To understand the internal computational structure of language models and explore the untapped potential of susceptibilities for visualizing network organization.

Method: Applies UMAP to the susceptibility matrix to visualize structural development during training.

Result: Visualizations show a clear "body plan," including known features like the induction circuit and new structures like a "spacing fin" for counting space tokens.

Conclusion: Susceptibility analysis can uncover novel mechanisms, offering a holistic tool for studying neural network development.

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [52] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD is a novel framework using diffusion models to generate high-quality OOD features and images by perturbing ID features near decision boundaries, improving OOD detection performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing OOD detection by addressing the challenge of extracting effective OOD features due to unclear decision boundaries in latent space.

Method: BOOD learns a text-conditioned latent space, selects boundary-proximate ID features, perturbs them to create OOD features, and decodes these into images using diffusion models.

Result: BOOD significantly outperforms state-of-the-art methods, reducing FPR95 by 29.64% and improving AUROC by 7.27% on CIFAR-100.

Conclusion: BOOD offers an efficient and effective strategy for OOD data generation, improving detection performance.

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [53] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SGPC introduces a unified GNN architecture combining cellular-sheaf message passing with optimal transport, variance-reduced diffusion, and PAC-Bayes regularization to address over-smoothing and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Over-smoothing in GNNs, especially on heterophilic graphs, and limitations of existing sheaf-based methods (static structures, lack of stability guarantees) motivate the need for a robust solution.

Method: SGPC integrates cellular-sheaf message passing with optimal transport-based lifting, variance-reduced diffusion, and PAC-Bayes spectral regularization for semi-supervised node classification.

Result: SGPC outperforms state-of-the-art GNNs on nine benchmarks, providing certified confidence intervals and linear computational complexity.

Conclusion: SGPC offers a scalable, theoretically grounded solution for robust node classification, addressing over-smoothing and generalization challenges in GNNs.

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [54] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: OID-PPO, a novel RL framework, outperforms existing methods in residential interior design by integrating expert guidelines and enabling continuous furniture placement.


<details>
  <summary>Details</summary>
Motivation: Residential interior design is complex due to unstructured layouts, high computational demands, and reliance on expertise. Existing methods are either inefficient or data-limited.

Method: OID-PPO uses Proximal Policy Optimization with a diagonal Gaussian policy for continuous furniture placement, incorporating expert-defined functional and visual guidelines into rewards.

Result: OID-PPO achieves superior layout quality and computational efficiency across diverse room shapes and furniture configurations.

Conclusion: The framework's success highlights the importance of structured guideline integration and the distinct roles of design constraints.

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [55] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: The paper proposes universal algorithms with dual adaptivity for online convex optimization, addressing limitations of existing methods by adapting to function types and environments dynamically.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms lack universality, handling only one convex function type and requiring prior knowledge, limiting real-world applicability.

Method: A meta-expert framework is introduced, dynamically creating and aggregating experts via a meta-algorithm with second-order bounds, incorporating sleeping experts for changing environments.

Result: The algorithms minimize adaptive regret for multiple convex function types simultaneously, allowing function type switches between rounds, and extend to online composite optimization.

Conclusion: The proposed framework achieves universality and dual adaptivity, enhancing applicability in dynamic real-world scenarios.

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [56] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: ExeKGLib is a Python library with a graphical interface for non-ML experts to build ML pipelines using knowledge graphs, enhancing transparency and reusability.


<details>
  <summary>Details</summary>
Motivation: Domain experts lack ML expertise but need ML-based analytics, making pipeline development challenging.

Method: ExeKGLib uses knowledge graphs to simplify ML pipeline creation for non-experts via a graphical interface.

Result: The library enables non-ML experts to build executable, transparent, and reusable ML workflows.

Conclusion: ExeKGLib successfully bridges the gap for domain experts needing ML solutions without deep ML knowledge.

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [57] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: Co-Reward is a self-supervised RL framework using contrastive agreement across analogical questions to improve LLM reasoning, outperforming baselines and even GT rewards.


<details>
  <summary>Details</summary>
Motivation: Addresses the scaling up dilemma in RLVR by reducing reliance on human labels and avoiding collapse issues in self-reward methods.

Method: Constructs similar questions for training samples, synthesizes surrogate labels via rollout voting, and enforces reasoning consistency through cross-referencing.

Result: Achieves superior performance on reasoning benchmarks, with up to +6.8% improvement over GT rewards on MATH500.

Conclusion: Co-Reward effectively enhances reasoning stability and performance, surpassing traditional self-reward and GT-labeled methods.

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [58] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: The paper introduces a ResE-BiLSTM model for predicting post-loan default, outperforming baseline models like LSTM, BiLSTM, GRU, CNN, and RNN on the Freddie Mac dataset.


<details>
  <summary>Details</summary>
Motivation: Improving prediction of post-loan default in credit risk management using machine learning.

Method: Proposes a ResE-BiLSTM model with a sliding window technique, evaluated on 44 cohorts from the Freddie Mac dataset, and compared with five baseline models. Includes ablation study and SHAP analysis.

Result: ResE-BiLSTM achieves superior performance in metrics like Accuracy, Precision, Recall, F1, and AUC.

Conclusion: The model demonstrates practical value for real-world credit risk management.

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [59] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: The paper introduces ctdGAN, a conditional GAN for addressing class imbalance in tabular data by incorporating space partitioning and probabilistic sampling to improve sample fidelity and classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in tabular data degrades machine learning performance. Existing GANs ignore input subspaces and treat class labels like other categorical variables, leading to ineffective conditional sampling.

Method: ctdGAN uses space partitioning to cluster input samples, then employs probabilistic sampling and a novel loss function to generate data in subspaces resembling the original distribution. It also includes cluster-wise scaling.

Result: Evaluation on 14 imbalanced datasets shows ctdGAN generates high-fidelity samples and enhances classification accuracy.

Conclusion: ctdGAN effectively mitigates class imbalance in tabular data by leveraging subspace awareness and improved sampling, outperforming existing methods.

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [60] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: CoLL combines LLMs and GNNs for anomaly detection in text-attributed graphs, improving AP by 13.37%.


<details>
  <summary>Details</summary>
Motivation: Existing GAD methods overlook textual modality's value and struggle with semantic context. LLMs offer strong semantic understanding but lack structural encoding.

Method: CoLL integrates multi-LLM collaboration for context capture and GNNs for structural fusion.

Result: CoLL outperforms existing methods with a 13.37% average AP improvement.

Conclusion: CoLL pioneers LLM-GNN fusion for GAD, enhancing detection quality and interpretability.

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [61] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: The paper critiques current interpretability definitions as non-actionable and proposes a new, actionable definition for designing interpretable models, including a blueprint and an open-sourced library.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability definitions lack practicality, hindering robust model design.

Method: Proposes a new interpretability definition, a design blueprint, and introduces an open-sourced library for interpretable models.

Result: The new definition is actionable, revealing foundational properties and assumptions for interpretable model design.

Conclusion: The work provides a practical framework and tools for advancing interpretable AI research.

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [62] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: The paper introduces CMUCL, an end-to-end method for text-attributed graph anomaly detection (GAD), integrating text and graph data to improve detection accuracy by 11.13%.


<details>
  <summary>Details</summary>
Motivation: Existing GAD methods separate text encoding from anomaly detection, limiting effectiveness. The paper aims to unify these processes for better performance.

Method: CMUCL jointly trains text and graph encoders using cross-modal and uni-modal consistency, with an anomaly score estimator based on inconsistency mining.

Result: CMUCL achieves an 11.13% average accuracy improvement over existing methods, supported by 8 new benchmark datasets.

Conclusion: The proposed CMUCL framework effectively integrates text and graph data for GAD, demonstrating significant performance gains.

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [63] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: The paper introduces SBSCGM and HybridGraphMedGNN for ICU patient criticalness prediction, leveraging patient similarity graphs and hybrid GNNs to outperform baselines with a 0.94 AUC-ROC.


<details>
  <summary>Details</summary>
Motivation: Accurate ICU patient criticalness prediction is crucial for early intervention, but conventional models fail to exploit relational EHR structures.

Method: Proposes SBSCGM for dynamic patient similarity graphs and HybridGraphMedGNN combining GCN, GraphSAGE, and GAT layers.

Result: Achieves state-of-the-art 0.94 AUC-ROC on MIMIC-III, with improved precision/recall and interpretable insights.

Conclusion: The framework is scalable and interpretable, suitable for real-world ICU deployment.

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [64] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: The paper improves regret bounds for online nonsubmodular optimization with delayed feedback by proposing two algorithms: DBGD-NF and its extension with blocking, achieving better performance under irregular delays and decoupling delay-bandit effects.


<details>
  <summary>Details</summary>
Motivation: Existing regret bounds for delayed feedback in bandit settings are sensitive to irregular delays and couple delay and bandit effects, limiting performance.

Method: Two algorithms are proposed: DBGD-NF (using one-point gradient estimator and all gradients per round) and its extension with blocking to decouple delay-bandit effects.

Result: DBGD-NF achieves O(n¯d¹/³T²/³) regret (average delay), while the blocking extension achieves O(n(T²/³ + √(dT))), outperforming prior bounds.

Conclusion: The proposed methods offer superior regret bounds, especially under irregular delays, and are validated experimentally for structured sparse learning.

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [65] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: The paper introduces methods for solving and learning robust Markov decision processes (r-MDPs) using factored state-space representations, improving sample efficiency and policy robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning r-MDPs with high sample efficiency and robust performance guarantees.

Method: Proposes novel methods leveraging factored state-space representations and reformulating non-convex problems into tractable linear programs.

Result: Experimental results show dimensional gains in sample efficiency and more effective robust policies with tighter guarantees.

Conclusion: Factored structure exploitation enhances robustness and efficiency in r-MDPs, outperforming state-of-the-art methods.

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [66] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: A two-stage framework enhances mineral detection in hyperspectral imaging by filtering noisy bands and refining data for improved unmixing accuracy.


<details>
  <summary>Details</summary>
Motivation: Weak mineral signatures in hyperspectral imaging are often obscured by noise and redundant bands, limiting detection performance.

Method: The framework involves SNR-based band selection, spectral smoothing, KMeans clustering for endmember extraction, and NNLS for abundance unmixing.

Result: The pipeline improves unmixing accuracy and detects weak mineral zones more effectively.

Conclusion: The two-stage strategy offers a practical solution for spectral dimensionality reduction and unmixing in geological HSI applications.

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [67] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: The paper introduces JSON-Bag, a token-based method for representing game trajectories, using Jensen-Shannon distance (JSD) for comparison. It outperforms hand-crafted features in classification tasks and shows sample efficiency and automatic feature extraction capabilities.


<details>
  <summary>Details</summary>
Motivation: To create a generic and efficient method for representing and comparing game trajectories without relying on hand-crafted features.

Method: Tokenizes JSON descriptions of game trajectories into a bag-of-tokens model (JSON-Bag), uses JSD for distance measurement, and employs prototype-based nearest-neighbor search (P-NNS) for evaluation.

Result: Outperforms hand-crafted features in most tasks, demonstrates sample efficiency, and improves accuracy via automatic feature extraction with Random Forest.

Conclusion: JSON-Bag with JSD is effective for game trajectory representation, classification, and policy distance correlation.

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [68] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: NeGPR is a novel framework for graph-level domain adaptation with noisy labels, using dual branches and nested refinement to improve robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Existing GDA methods assume clean source labels, but real-world scenarios often have pervasive annotation noise, degrading adaptation performance.

Method: NeGPR pretrains semantic and topology branches with neighborhood consistency, employs nested refinement for cross-domain learning, and uses noise-aware regularization.

Result: NeGPR outperforms state-of-the-art methods under severe label noise, achieving up to 12.7% accuracy gains.

Conclusion: NeGPR effectively addresses label noise in GDA, enhancing robustness and adaptation performance.

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [69] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: Machine-learned potentials, particularly MACE, outperform other models in simulating hydrogen atom transfer (HAT) reactions in peptides, achieving high accuracy in energy and barrier predictions.


<details>
  <summary>Details</summary>
Motivation: Understanding HAT reactions in biological processes is limited due to the challenge of simulating them with quantum chemical accuracy at relevant scales.

Method: Systematic generation of HAT configurations in peptides using semiempirical methods and DFT, benchmarking three graph neural network architectures (SchNet, Allegro, MACE).

Result: MACE achieves a mean absolute error of 1.13 kcal/mol on DFT barrier predictions, enabling large-scale simulations of HAT in collagen.

Conclusion: The approach is generalizable to other biomolecular systems, advancing quantum-accurate simulations of chemical reactivity in complex environments.

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [70] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: A multi-fidelity stratified sampling scheme with adaptive machine learning metamodels is introduced to efficiently estimate small failure probabilities in stochastic simulations, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing variance reduction techniques require many model evaluations for rare event analysis, which is computationally challenging for complex, nonlinear systems under stochastic excitation.

Method: The approach uses stratified sampling to generate high-fidelity data for training a deep learning-based metamodel (low-fidelity model). An adaptive training scheme balances approximation quality and computational demand. Multi-fidelity Monte Carlo integrates low- and high-fidelity results for unbiased failure probability estimates.

Result: Applied to a high-rise steel building under wind excitation, the method accurately estimates exceedance probability curves for nonlinear responses with significant computational savings.

Conclusion: The proposed scheme effectively addresses computational challenges in rare event analysis, offering accuracy and efficiency for complex systems.

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [71] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: Active Learning (AL) is inefficient in low-data scenarios compared to data augmentation (DA) and semi-supervised learning (SSL), but combining AL with DA and SSL can still yield performance improvements.


<details>
  <summary>Details</summary>
Motivation: The paper investigates why AL is rarely applied outside its literature, attributing it to high computational costs and small performance gains in low-data scenarios.

Method: The study compares AL, DA, and SSL in low-data settings, evaluating their individual and combined impacts on performance.

Result: AL alone provides only 1-4% lift over random sampling, while DA and SSL can achieve up to 60% lift. Combining AL with DA and SSL still improves performance.

Conclusion: AL should be viewed as a final step to maximize performance after applying DA and SSL, rather than a primary solution for missing labels.

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [72] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: Proposes a single deterministic model for uncertainty quantification using feature space density, outperforming Bayesian and ensemble methods in efficiency and OOD detection.


<details>
  <summary>Details</summary>
Motivation: Address computational and storage inefficiencies of Bayesian neural networks and deep ensembles for uncertainty quantification.

Method: Uses kernel density estimation to derive feature space density of training data, comparing it with test samples to detect distributional shifts and OOD cases.

Result: Outperforms baseline models on synthetic datasets (Two Moons, Three Spirals) and OOD detection (CIFAR-10 vs. SVHN).

Conclusion: The method is efficient and effective for uncertainty quantification and OOD detection, offering a lightweight alternative to traditional approaches.

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [73] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: IAMAP is a QGIS plugin that simplifies deep learning for remote sensing by leveraging self-supervised learning, requiring minimal data and coding skills.


<details>
  <summary>Details</summary>
Motivation: Deep learning in remote sensing is often inaccessible due to the need for large datasets, computing resources, and coding expertise. IAMAP aims to democratize AI for non-specialists.

Method: IAMAP uses self-supervised learning (foundation models) for feature extraction, dimensionality reduction, clustering, similarity mapping, and model validation, all within a user-friendly interface.

Result: IAMAP enables non-specialists to perform advanced remote sensing tasks without GPUs or large datasets, making deep learning more accessible and efficient.

Conclusion: IAMAP democratizes AI in remote sensing by reducing barriers to entry, promoting energy-conscious and computationally efficient methods.

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [74] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: SV-SNN improves high-frequency PDE solving by decomposing functions, using adaptive spectral features, and reducing spectral bias, achieving better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs struggle with high-frequency PDEs due to spectral bias, limiting their effectiveness in applications like fluid mechanics and quantum mechanics.

Method: SV-SNN decomposes multivariate functions, uses adaptive Fourier spectral features, and leverages a theoretical framework to address spectral bias.

Result: SV-SNN improves accuracy by 1-3 orders of magnitude, reduces parameters by 90%, and cuts training time by 60% on benchmark problems.

Conclusion: SV-SNN effectively solves spectral bias in neural PDE solving, offering significant improvements in accuracy and efficiency.

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [75] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: The paper introduces KFS, a KAN-based adaptive frequency selection architecture for time series forecasting, addressing noise and heterogeneous information across scales.


<details>
  <summary>Details</summary>
Motivation: Real-world time series suffer from noise interference and suboptimal multi-scale representation due to heterogeneous frequency information.

Method: KFS uses a FreK module for dominant frequency selection, KAN for pattern representation, and timestamp embedding alignment for temporal synchronization.

Result: Extensive experiments show KFS achieves state-of-the-art performance on real-world datasets.

Conclusion: KFS is a simple yet effective solution for multi-scale time series forecasting challenges.

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [76] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: A reinforcement learning (RL) approach is used to optimize interception prioritization of kamikaze drone swarms in defense systems, outperforming rule-based methods in simulations.


<details>
  <summary>Details</summary>
Motivation: The increasing threat of low-cost kamikaze drone swarms requires rapid and strategic decision-making for defense systems to prioritize interceptions effectively.

Method: A high-fidelity simulation environment is developed where an RL agent learns to coordinate multiple effectors for optimal interception prioritization in a discrete action space.

Result: The RL-based policy achieves lower average damage and higher defensive efficiency compared to a rule-based baseline in simulated attack scenarios.

Conclusion: Reinforcement learning shows promise as a strategic layer in defense architectures, enhancing resilience without replacing existing systems.

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [77] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: DINOZAUR introduces a diffusion-based neural operator with uncertainty quantification, addressing scalability and UQ limitations of FNOs.


<details>
  <summary>Details</summary>
Motivation: FNOs face scalability issues and lack native uncertainty quantification, critical for reliable applications.

Method: DINOZAUR replaces dense tensor multipliers with a diffusion-based, dimensionality-independent multiplier, reducing parameters and enabling Bayesian UQ.

Result: Competitive or superior performance on PDE benchmarks with efficient uncertainty quantification.

Conclusion: DINOZAUR offers a scalable, uncertainty-aware alternative to FNOs, maintaining performance while reducing computational overhead.

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [78] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: TrajSurv is a model for trustworthy survival prediction using longitudinal EHR data, leveraging NCDE for continuous latent trajectories and contrastive learning for alignment, with transparent interpretation via vector fields and clustering.


<details>
  <summary>Details</summary>
Motivation: Accurate and transparent survival prediction from irregularly sampled EHR data is challenging but crucial for clinical decisions.

Method: TrajSurv uses NCDE to model continuous latent trajectories, aligns them with patient states via contrastive learning, and interprets them through vector fields and clustering.

Result: TrajSurv achieves competitive accuracy and superior transparency on MIMIC-III and eICU datasets compared to existing methods.

Conclusion: TrajSurv provides a trustworthy, interpretable solution for survival prediction from longitudinal EHR data.

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [79] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: The paper proposes DP-DGAD, a dynamic graph anomaly detection model using dynamic prototypes to capture evolving domain-specific and domain-agnostic patterns, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Generalist GAD models struggle with dynamic graphs and lack labeled data, necessitating a method to capture evolving anomalies across domains.

Method: DP-DGAD extracts dynamic prototypes from temporal ego-graphs, updates a memory buffer selectively, and uses an anomaly scorer with confidence-based pseudo-labeling.

Result: The model achieves state-of-the-art performance across ten real-world datasets.

Conclusion: DP-DGAD effectively addresses cross-domain dynamic graph anomaly detection by capturing evolving patterns and adapting self-supervised.

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [80] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: The paper proposes a method combining GDFM and GAN to synthesize wind power scenarios, improving spatial and temporal correlation representation over alternatives.


<details>
  <summary>Details</summary>
Motivation: To enhance the synthesis of long-term wind power scenarios by accurately capturing spatio-temporal features and statistical characteristics.

Method: Uses GAN to extract dynamic factors with temporal information, then applies these in GDFM to represent spatial and frequency correlations.

Result: Numerical tests show improved performance in synthesizing plausible wind power scenarios compared to GDFM or GAN alone.

Conclusion: The combined GDFM-GAN approach outperforms alternatives in generating realistic wind power scenarios.

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [81] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: The study compares AI models for classifying clinical notes into Anxiety and Adjustment Disorder diagnoses, finding hyperparameter tuning crucial for performance, with Decision Tree, XGBoost, DistilBERT, and SciBERT achieving 96% accuracy.


<details>
  <summary>Details</summary>
Motivation: Improving diagnostic classification in mental health using AI to enhance accuracy and efficiency.

Method: Comparison of ML (Random Forest, SVM, KNN, Decision Tree, XGBoost) and DL (DistilBERT, SciBERT) models with oversampling strategies (None, Random, SMOTE) and hyperparameter tuning.

Result: Oversampling had minimal impact except SMOTE with BERT models. Hyperparameter tuning significantly boosted accuracy, with top models achieving 96%.

Conclusion: Hyperparameter tuning is key for model performance, and AI can effectively classify mental health diagnoses, with certain models excelling.

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [82] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: The paper introduces MIND, a message-passing GNN framework that eliminates the need for handcrafted features, using attention and message-iteration profiles to solve Network Dismantling efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the reliance on handcrafted structural features in GNNs, which introduce bias and computational cost, and to improve generalizability and efficiency in solving NP-hard problems like Network Dismantling.

Method: Proposes MIND, which uses an attention mechanism and message-iteration profiles, trained on structurally diverse synthetic networks.

Result: MIND outperforms state-of-the-art methods on large real networks, generalizing well without handcrafted features.

Conclusion: The framework's efficiency and generalizability extend beyond dismantling, offering potential for broader complex network problems.

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [83] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: The paper introduces the MOSTLY AI Synthetic Data SDK, an open-source toolkit for generating high-quality synthetic tabular data, addressing data accessibility issues due to privacy and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: Increasing restrictions on data access due to privacy, proprietary interests, and ethical concerns necessitate synthetic data solutions.

Method: The SDK uses the TabularARGN autoregressive framework, integrating differential privacy, fairness-aware generation, and automated quality assurance in a Python interface.

Result: The SDK delivers competitive performance, improves speed and usability, and supports diverse data types, including multi-table and sequential datasets.

Conclusion: The SDK is practical for real-world data bottlenecks, promoting data democratization, and is widely adopted in cloud and local deployments.

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [84] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: The paper introduces DDAE, a framework combining diffusion-based noise scheduling and contrastive learning for better anomaly detection in tabular data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in anomaly detection due to complex feature interactions and lack of anomalous examples, with limitations in current denoising autoencoders and diffusion models.

Method: Proposes DDAE, integrating diffusion-based noise scheduling and contrastive learning into encoding for improved anomaly detection.

Result: DDAE outperforms baselines, improving PR-AUC by up to 65% (9%) and ROC-AUC by 16% (6%), with optimal noise strategies identified.

Conclusion: Principled noise strategies are crucial for effective anomaly detection in tabular data, with DDAE demonstrating superior performance.

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [85] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: The paper analyzes the impact of rotational gate choices in Variational Quantum Circuits (VQCs) on classification performance, comparing Amplitude- and Angle-encoding models on Wine and Diabetes datasets. Results show accuracy differences of 10-41%, emphasizing the importance of encoding as a hyperparameter.


<details>
  <summary>Details</summary>
Motivation: To explore how the choice of rotational gates in quantum encoding affects the performance of Quantum Machine Learning (QML) models, specifically VQCs.

Method: The study compares Amplitude- and Angle-encoding models in VQCs, varying rotational gates, and evaluates their classification performance on Wine and Diabetes datasets.

Result: Accuracy differences between models ranged from 10% to 41%, with encoding choice significantly impacting performance.

Conclusion: The embedding method (rotational gate choice) is a critical hyperparameter for VQC models, influencing their classification accuracy.

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [86] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: The study explores socio-academic and financial factors affecting CGPA, using surveys and causal analysis. Ridge Regression and Random Forest models showed high accuracy, and a web app was developed for personalized student insights.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize students' academic performance by identifying key influencing factors.

Method: Literature review, online survey (1,050 participants), causal analysis, regression and classification models (Ridge Regression, Random Forest), and Explainable AI techniques (SHAP, LIME, Interpret).

Result: Ridge Regression achieved MAE 0.12 and MSE 0.023; Random Forest had 98.68% accuracy. Key factors: study hours, scholarships, parental education, prior performance.

Conclusion: The study successfully identified critical CGPA influences and developed a practical web app for student performance optimization.

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


### [87] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: Adacc is a memory management framework for LLM training, combining adaptive compression and activation checkpointing to reduce GPU memory usage and improve training speed.


<details>
  <summary>Details</summary>
Motivation: Training large language models (LLMs) faces memory pressure, with recomputation causing up to 30% overhead. Adacc aims to mitigate this.

Method: Adacc uses layer-specific compression, MILP-based scheduling, and adaptive policy evolution to optimize memory and throughput.

Result: Adacc accelerates LLM training by 1.01x to 1.37x while maintaining model accuracy comparable to the baseline.

Conclusion: Adacc effectively reduces memory overhead and speeds up LLM training without sacrificing accuracy.

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [88] [Strategic Communication and Language Bias in Multi-Agent LLM Coordination](https://arxiv.org/abs/2508.00032)
*Alessio Buscemi,Daniele Proverbio,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.MA

TL;DR: LLM-based agents' coordination is influenced by communication, language, and game structure, with GPT-4o and Llama 4 Maverick showing varied impacts.


<details>
  <summary>Details</summary>
Motivation: To investigate how communication affects language-driven cooperative behavior in multi-agent scenarios.

Method: Simulated one-shot and repeated games using the FAIRGAME framework, testing GPT-4o and Llama 4 Maverick with and without communication.

Result: Communication significantly impacts agent behavior, varying by language, personality, and game structure.

Conclusion: Communication plays a dual role in enhancing coordination and reinforcing biases in LLM-based agents.

Abstract: Large Language Model (LLM)-based agents are increasingly deployed in
multi-agent scenarios where coordination is crucial but not always assured.
Previous studies indicate that the language used to frame strategic scenarios
can influence cooperative behavior. This paper explores whether allowing agents
to communicate amplifies these language-driven effects. Leveraging the FAIRGAME
framework, we simulate one-shot and repeated games across different languages
and models, both with and without communication. Our experiments, conducted
with two advanced LLMs, GPT-4o and Llama 4 Maverick, reveal that communication
significantly influences agent behavior, though its impact varies by language,
personality, and game structure. These findings underscore the dual role of
communication in fostering coordination and reinforcing biases.

</details>


### [89] [WMAS: A Multi-Agent System Towards Intelligent and Customized Wireless Networks](https://arxiv.org/abs/2508.00280)
*Jingchen Peng,Dingli Yuan,Boxiang Ren,Jie Fan,Hao Wu,Lu Yang*

Main category: cs.MA

TL;DR: A Wireless Multi-Agent System (WMAS) is proposed to optimize multi-agent conversations in wireless networks using reinforcement learning, achieving higher performance and lower overhead.


<details>
  <summary>Details</summary>
Motivation: To address risks like malfunctions and infinite loops in multi-agent systems for wireless networks, ensuring efficient and accurate task completion.

Method: Model conversation topology as a directed acyclic graph and optimize its adjacency matrix via reinforcement learning.

Result: WMAS outperforms existing systems in task performance and reduces conversation overhead.

Conclusion: WMAS enhances wireless network intelligence by enabling effective multi-agent collaboration.

Abstract: The fast development of Artificial Intelligence (AI) agents provides a
promising way for the realization of intelligent and customized wireless
networks. In this paper, we propose a Wireless Multi-Agent System (WMAS), which
can provide intelligent and customized services for different user equipment
(UEs). Note that orchestrating multiple agents carries the risk of malfunction,
and multi-agent conversations may fall into infinite loops. It is thus crucial
to design a conversation topology for WMAS that enables agents to complete UE
task requests with high accuracy and low conversation overhead. To address this
issue, we model the multi-agent conversation topology as a directed acyclic
graph and propose a reinforcement learning-based algorithm to optimize the
adjacency matrix of this graph. As such, WMAS is capable of generating and
self-optimizing multi-agent conversation topologies, enabling agents to
effectively and collaboratively handle a variety of task requests from UEs.
Simulation results across various task types demonstrate that WMAS can achieve
higher task performance and lower conversation overhead compared to existing
multi-agent systems. These results validate the potential of WMAS to enhance
the intelligence of future wireless networks.

</details>
