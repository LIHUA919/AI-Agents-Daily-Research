<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 45]
- [cs.LG](#cs.LG) [Total: 85]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] ['Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra](https://arxiv.org/abs/2506.13768)
*Stefan Reimann*

Main category: cs.AI

TL;DR: A non-associative algebraic framework for high-dimensional information representation and computation, preserving temporal structure without auxiliary order markers.


<details>
  <summary>Details</summary>
Motivation: Address limitations of associative bundling in cognitive models, which lose order information and require auxiliary structures.

Method: Uses multiplication-like binding and non-associative interference-like bundling to create sparse sequence representations with L-state (recency) and R-state (primacy).

Result: Replicates the Serial Position Curve, reflecting recency and primacy effects in cognitive experiments.

Conclusion: Proposes a biologically plausible model linking prefrontal cortex and hippocampal activity to memory states, with noise as a key element in order representation.

Abstract: This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.

</details>


### [2] [Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs](https://arxiv.org/abs/2506.13773)
*Milapji Singh Gill,Tom Jeleniewski,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: The paper introduces a modular semantic model and method for integrating differential equations into knowledge graphs for CPS applications, validated in aviation maintenance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reusable ontological artifacts and methods for contextualizing differential equations in CPS lifecycle phases.

Method: Proposes a modular semantic model and an efficient knowledge graph generation method.

Result: Successfully represented complex differential equations in a knowledge graph and contextualized them with lifecycle data.

Conclusion: The artifacts are practically applicable, as demonstrated in aviation maintenance.

Abstract: Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.

</details>


### [3] [Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values](https://arxiv.org/abs/2506.13774)
*Nell Watson,Ahmed Amer,Evan Harris,Preeti Ravindra,Shujun Zhang*

Main category: cs.AI

TL;DR: The paper introduces a 'superego' agent for aligning agentic AI with human values, reducing harmful outputs by up to 98.3%.


<details>
  <summary>Details</summary>
Motivation: Challenges in aligning autonomous AI with diverse human values and safety requirements hinder practical deployment.

Method: A 'superego' agent uses 'Creed Constitutions' for dynamic oversight, validated by a real-time compliance enforcer.

Result: Achieves 98.3% harm reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4).

Conclusion: The system simplifies AI alignment, improving safety and contextual attunement.

Abstract: Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.

</details>


### [4] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei,Patricia Paskov,Sunishchal Dev,Michael J. Byun,Anka Reuel,Xavier Roberts-Gaal,Rachel Calcott,Evie Coxon,Chinmay Deshpande*

Main category: cs.AI

TL;DR: The paper advocates for stricter and more transparent human baselines in AI evaluations, offering a framework and checklist to improve rigor and reporting.


<details>
  <summary>Details</summary>
Motivation: Human baselines are crucial for interpreting AI performance, but current methods lack rigor and transparency, leading to unreliable claims of 'super-human' performance.

Method: The authors conduct a meta-review of measurement theory and AI evaluation literature to develop a framework and checklist for human baselines, then apply it to review 115 studies.

Result: The review identifies shortcomings in existing baselining methods, and the checklist aids in improving future human baseline practices.

Conclusion: The work aims to enhance AI evaluation rigor, benefiting researchers and policymakers, with data and tools provided for implementation.

Abstract: In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines

</details>


### [5] [The NordDRG AI Benchmark for Large Language Models](https://arxiv.org/abs/2506.13790)
*Tapio Pitkäranta*

Main category: cs.AI

TL;DR: NordDRG-AI-Benchmark is the first public benchmark for evaluating LLMs in hospital funding via DRG logic, multilingual diagnosis, and procedure coding. It includes definition tables, expert manuals, and a prompt pack, revealing domain-specific LLM performance gaps.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open benchmarks for LLMs in hospital funding (DRG-based reimbursement) and evaluate their reasoning over multilingual diagnosis and procedure logic.

Method: The benchmark includes (i) definition tables for DRG logic, (ii) expert manuals for governance workflows, and (iii) a prompt pack for 14 CaseMix tasks. Performance of five LLMs was tested on nine verifiable tasks.

Result: LLMs performed variably: o3 scored 9/9, GPT-4o and o4-mini-high scored 7/9, while Gemini 2.5 Pro and Flash scored 5/9 and 3/9, respectively.

Conclusion: NordDRG-AI-Benchmark reveals domain-specific LLM weaknesses, providing a reproducible baseline for trustworthy automation in hospital funding.

Abstract: Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.
  The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.
  All artefacts are available at: https://github.com/longshoreforrest/norddrg-ai-benchmark
  A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.

</details>


### [6] [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
*Gonçalo Hora de Carvalho,Lazar S. Popov,Sander Kaatee,Kristinn R. Thórisson,Tangrui Li,Pétur Húni Björnsson,Jilles S. Dibangoye*

Main category: cs.AI

TL;DR: ICE-ID is a new benchmark dataset for historical identity resolution using Icelandic census records (1703-1920). It evaluates methods like rule-based matchers, ML ensembles, LLMs, and NARS, with NARS achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale, open datasets for studying long-term person-entity matching in real-world populations, ICE-ID provides a unique resource for identity resolution tasks.

Method: The study evaluates rule-based matchers, ML ensembles, LLMs, and a novel NARS approach (Non-Axiomatic Reasoning System) on the ICE-ID dataset, with clearly defined metrics and tasks.

Result: NARS, a simple yet effective method, achieves state-of-the-art performance in identity resolution tasks compared to other standard approaches.

Conclusion: ICE-ID enables reproducible benchmarking and opens new research opportunities in data linkage and historical analytics.

Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.

</details>


### [7] [Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection](https://arxiv.org/abs/2506.13793)
*Zongxian Yang,Jiayu Qian,Zegao Peng,Haoyu Zhang,Zhi-An Huang*

Main category: cs.AI

TL;DR: Med-REFL improves medical reasoning by decomposing questions into fine-grained steps, evaluating reflections, and optimizing performance without heavy expert input.


<details>
  <summary>Details</summary>
Motivation: Existing large reasoning models struggle in medical domains due to poor intermediate reflection quality, which is critical for high-stakes scenarios.

Method: Uses a tree-of-thought approach to break down medical questions, evaluates reasoning steps quantitatively, and constructs preference optimization data automatically.

Result: Achieves up to 4.11% improvement on MedQA-USMLE and boosts 7B/8B models by 4.13%, with strong generalization across datasets.

Conclusion: Focusing on reflection quality enhances accuracy and trustworthiness in medical AI, as demonstrated by Med-REFL.

Abstract: Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical \underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via self-corrected \underline{\textbf{F}}ine-grained ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \href{https://github.com/TianYin123/Med-REFL}{here}.

</details>


### [8] [BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection](https://arxiv.org/abs/2506.13795)
*Boshen Shi,Yongqing Wang,Fangda Guo,Jiangli Shao,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: BotTrans is a multi-source graph domain adaptation model designed to improve social bot detection by leveraging knowledge from multiple source networks, addressing challenges like network heterophily and single-source transfer limitations.


<details>
  <summary>Details</summary>
Motivation: Label scarcity in detecting social bots with GNN-based models and challenges like network heterophily and single-source transfer limitations motivate the need for a multi-source approach.

Method: BotTrans establishes a cross-source-domain topology, aggregates cross-domain neighbor information, integrates source-target relevance, and refines detection using target domain semantic knowledge.

Result: BotTrans outperforms state-of-the-art methods in experiments, demonstrating its effectiveness in leveraging multi-source knowledge for unlabeled target tasks.

Conclusion: BotTrans effectively addresses label scarcity and transfer challenges in social bot detection, proving superior performance through multi-source knowledge integration.

Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.

</details>


### [9] [Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization](https://arxiv.org/abs/2506.13799)
*Soroush Vahidi*

Main category: cs.AI

TL;DR: A suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, tested on the FlyWire Connectome Challenge dataset, outperforms previous methods in maximizing forward edge weight.


<details>
  <summary>Details</summary>
Motivation: To reveal biologically meaningful feedforward structure in neural connectomes by minimizing feedback arcs in large-scale weighted directed graphs.

Method: Integrates greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components.

Result: The best solution improves forward edge weight over previous top-performing methods.

Conclusion: The algorithms are efficient, implemented in Python, and validated using cloud-based execution on Google Colab Pro+.

Abstract: We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.

</details>


### [10] [Causality in the human niche: lessons for machine learning](https://arxiv.org/abs/2506.13803)
*Richard D. Lange,Konrad P. Kording*

Main category: cs.AI

TL;DR: The paper discusses the gap between human causal cognition and the Structural Causal Model (SCM) framework, advocating for more human-like causal reasoning in AI to improve generalization and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current machine learning systems lack human-like causal understanding, which limits their ability to generalize and learn efficiently. The SCM framework, while useful, doesn't fully capture human causal cognition.

Method: The paper critiques the SCM framework and explores how human causal cognition, such as analogical reasoning, is adaptive in real-world contexts.

Result: The analysis highlights limitations of SCMs in capturing human-like causal reasoning and suggests the need for more human-inspired inductive biases in AI.

Conclusion: Future AI systems should incorporate human-like causal reasoning to enhance capability, controllability, and interpretability, aligning with the demands of the human niche.

Abstract: Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.

</details>


### [11] [Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study](https://arxiv.org/abs/2506.13810)
*Olivier Saidi*

Main category: cs.AI

TL;DR: A framework leveraging structural patterns in NP-hard problems like TSP improves computational efficiency, achieving up to 79% solution quality gains.


<details>
  <summary>Details</summary>
Motivation: Real-world instances of NP-hard problems often have exploitable patterns, which current methods overlook.

Method: Proposes a pattern-aware complexity framework with metrics like PUE, supported by theorems and a meta-learning solver pipeline.

Result: Achieves up to 79% solution quality gains in TSP benchmarks (22 to 2392 cities).

Conclusion: Offers a practical, unified approach to exploit patterns for efficiency, distinct from theoretical NP-hardness.

Abstract: NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.

</details>


### [12] [The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness](https://arxiv.org/abs/2506.13825)
*Gnankan Landry Regis N'guessan,Issa Karambal*

Main category: cs.AI

TL;DR: The paper introduces the Reflexive Integrated Information Unit (RIIU), a trainable module for artificial consciousness research, designed to be small, benchmarkable, and improvable.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a foundational, trainable module in artificial consciousness research, akin to the perceptron in neural networks.

Method: RIIU augments its hidden state with a meta-state and broadcast buffer, using a sliding-window covariance and differentiable Auto-$Φ$ surrogate to maximize local information integration.

Result: RIIUs are differentiable, compose additively, and perform $Φ$-monotone plasticity. In tests, a four-layer RIIU agent outperformed a GRU in recovering reward after actuator failure.

Conclusion: RIIUs transform the philosophical debate on consciousness into an empirical, mathematical problem by scaling consciousness-like computation to unit level.

Abstract: Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $μ$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$Φ$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $Φ$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$Φ$ signal. By shrinking "consciousness-like" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.

</details>


### [13] [LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning](https://arxiv.org/abs/2506.13841)
*Miho Koda,Yu Zheng,Ruixian Ma,Mingyang Sun,Devesh Pansare,Fabio Duarte,Paolo Santi*

Main category: cs.AI

TL;DR: The paper introduces LocationReasoner, a benchmark to evaluate LLMs' reasoning in real-world site selection, revealing limitations of current models in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs' reasoning skills, proven in domains like math and coding, generalize to real-world tasks with complex constraints.

Method: Developed a benchmark with 300 queries for site selection, supported by a sandbox environment and constraint-based tools.

Result: State-of-the-art models show limited improvement, with OpenAI o4 failing 30% of tasks; agentic strategies like ReAct underperform.

Conclusion: LLMs struggle with holistic, non-linear reasoning in real-world tasks; LocationReasoner is released to advance robust reasoning development.

Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.

</details>


### [14] [Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features](https://arxiv.org/abs/2506.13917)
*Miguel A. Lago,Ghada Zamzmi,Brandon Eich,Jana G. Delfino*

Main category: cs.AI

TL;DR: The paper proposes a framework to evaluate AI explainability using four criteria: Consistency, Plausibility, Fidelity, and Usefulness, illustrated with a case study on breast lesion detection.


<details>
  <summary>Details</summary>
Motivation: There is a lack of techniques to assess the quality of AI explanations, especially in medical contexts.

Method: A framework with four evaluation criteria is developed, and a scorecard is introduced to assess explainability methods.

Result: The framework is applied to Ablation CAM and Eigen CAM for breast lesion detection, demonstrating its utility.

Conclusion: The framework aims to improve AI explainability evaluation and foster better development of AI-based medical devices.

Abstract: Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.

</details>


### [15] [Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction](https://arxiv.org/abs/2506.13920)
*Mbithe Nzomo,Deshendran Moodley*

Main category: cs.AI

TL;DR: A novel method integrates knowledge graphs and Bayesian networks for explainable disease risk prediction using multimodal EHR data, demonstrated with atrial fibrillation.


<details>
  <summary>Details</summary>
Motivation: Adapting general medical knowledge to specific healthcare settings and handling uncertainty in EHR data for practical clinical use.

Method: Constructing Bayesian networks from ontology-based knowledge graphs and multimodal EHR data.

Result: Balances general medical knowledge with patient-specific context, handles uncertainty, is explainable, and achieves good predictive performance.

Conclusion: The approach effectively integrates KGs and BNs for practical, explainable disease risk prediction.

Abstract: Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.

</details>


### [16] [ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users](https://arxiv.org/abs/2506.13980)
*Shahaf David,Yair Meidan,Ido Hersko,Daniel Varnovitzky,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.AI

TL;DR: ProfiLLM is a framework for dynamic user profiling in chatbots, tested in IT/cybersecurity, showing rapid and accurate proficiency inference.


<details>
  <summary>Details</summary>
Motivation: Current chatbots lack personalization, especially in specialized domains like IT/cybersecurity, where user knowledge varies widely. Static profiling methods limit adaptability.

Method: ProfiLLM uses a domain-adaptable taxonomy and LLM-based profiling. ProfiLLM[ITSec] was developed for IT/cybersecurity, tested on synthetic user conversations.

Result: ProfiLLM[ITSec] reduced the gap between actual and predicted proficiency scores by 55-65% after one prompt, with further refinement.

Conclusion: ProfiLLM offers effective implicit and dynamic user profiling, with contributions including a simulation methodology, taxonomy, codebase, and dataset for future research.

Abstract: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.

</details>


### [17] [SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine](https://arxiv.org/abs/2506.13983)
*Adarsh Gupta,Bhabesh Mali,Chandan Karfa*

Main category: cs.AI

TL;DR: SANGAM is a framework using LLM-guided Monte Carlo Tree Search to automatically generate SystemVerilog Assertions (SVAs) from specifications, outperforming recent methods.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs for complex and automatic Hardware Assertion Generation, improving efficiency and robustness.

Method: A three-stage approach: multi-modal specification processing, Monte Carlo Tree Self-Refine (MCTSr) algorithm for reasoning, and combining traces to generate SVAs.

Result: SANGAM generates robust SVAs, performing better than recent methods in evaluations.

Conclusion: The framework effectively automates SVA generation, demonstrating superior performance.

Abstract: Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.

</details>


### [18] [Machine Mirages: Defining the Undefined](https://arxiv.org/abs/2506.13990)
*Hamidou Tembine*

Main category: cs.AI

TL;DR: The paper discusses 'machine mirages,' a new class of cognitive aberrations in multimodal AI systems, and emphasizes the need for their explicit definition and systematic assessment to improve reliability and ethical integration.


<details>
  <summary>Details</summary>
Motivation: The rise of AI systems achieving human-like fluency has revealed unique cognitive errors ('machine mirages'), necessitating their study to enhance reliability and ethical considerations in AI development.

Method: The article identifies and categorizes various machine mirages, such as delusion, hallucination, and bias amplification, and argues for their systematic assessment.

Result: The paper highlights the prevalence and diversity of machine mirages, underscoring their impact on AI reliability and ethical implications.

Conclusion: Understanding and addressing machine mirages is crucial for advancing AI reliability and fostering an ethical, co-evolving intelligence ecosystem.

Abstract: As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.

</details>


### [19] [Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.14045)
*Martin Klissarov,Akhil Bagaria,Ziyan Luo,George Konidaris,Doina Precup,Marlos C. Machado*

Main category: cs.AI

TL;DR: The paper explores hierarchical reinforcement learning (HRL) as a solution for AI agents in complex environments, focusing on defining good structure, its benefits, and methods for discovery.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of developing AI agents capable of exploring, planning, and learning in open-ended environments by leveraging HRL.

Method: Analyzes HRL's benefits, covers methods for discovering temporal structure (online, offline, LLMs), and identifies suitable domains.

Result: Highlights the impact of HRL on AI performance trade-offs and the challenges of temporal structure discovery.

Conclusion: HRL is promising but requires clearer definitions of good structure and further exploration of its applications.

Abstract: Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.

</details>


### [20] [Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places](https://arxiv.org/abs/2506.14070)
*Xinglei Wang,Tao Cheng,Stephen Law,Zichao Zeng,Ilya Ilyankou,Junyuan Liu,Lu Yin,Weiming Huang,Natchapon Jongwiriyanurak*

Main category: cs.AI

TL;DR: CaLLiPer, a multimodal framework, enhances location prediction by combining spatial and semantic data, outperforming traditional methods, especially for new locations.


<details>
  <summary>Details</summary>
Motivation: Traditional location prediction methods lack spatial explicitness, semantic context, and adaptability to unseen locations.

Method: Uses CaLLiPer, a contrastive learning framework, to fuse spatial coordinates and semantic features for location embeddings.

Result: Outperforms baselines in conventional and inductive settings, excelling with new locations.

Conclusion: Multimodal, inductive embeddings like CaLLiPer improve mobility prediction, with released code/data for reproducibility.

Abstract: Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.

</details>


### [21] [FormGym: Doing Paperwork with Agents](https://arxiv.org/abs/2506.14079)
*Matthew Toles,Rattandeep Singh,Isaac Song Zhou Yu*

Main category: cs.AI

TL;DR: A benchmark for form-filling in the pure-image domain is introduced, highlighting challenges for computer agents. Baseline methods perform poorly, but FieldFinder, a new tool, significantly improves accuracy.


<details>
  <summary>Details</summary>
Motivation: Form filling in the pure-image domain is difficult for computer agents due to lack of OCR or structured data, requiring multi-modal understanding and tool-use.

Method: A benchmark with 432 fields across 55 documents and 3 tasks is created. FieldFinder, a tool to assist LLMs in text placement, is proposed.

Result: Baseline VLAs achieve <1% accuracy; GUI agents score 10.6-68.0%. FieldFinder boosts performance, with a max increase from 2% to 56%.

Conclusion: FieldFinder effectively addresses localization challenges in form-filling, significantly improving model performance.

Abstract: Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.

</details>


### [22] [Lightweight Relevance Grader in RAG](https://arxiv.org/abs/2506.14084)
*Taehee Jeong*

Main category: cs.AI

TL;DR: RAG improves LLMs by using a vector database for accurate responses. A lightweight model (llama-3.2-1b) was fine-tuned as a relevance grader, boosting precision from 0.1301 to 0.7750, matching larger models like llama-3.1-70b.


<details>
  <summary>Details</summary>
Motivation: To enhance the relevance of retrieved documents in RAG systems without heavy computational costs.

Method: Fine-tuned llama-3.2-1b as a lightweight relevance grader to verify document relevance in RAG.

Result: Precision improved from 0.1301 to 0.7750, comparable to larger models like llama-3.1-70b.

Conclusion: Lightweight models can effectively serve as relevance graders in RAG, balancing performance and efficiency.

Abstract: Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.

</details>


### [23] [Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models](https://arxiv.org/abs/2506.14092)
*Haonan Yin,Shai Vardi,Vidyanand Choudhary*

Main category: cs.AI

TL;DR: The paper investigates positional biases in LLMs during decision-making, revealing consistent order effects, a novel centrality bias, and quality-dependent shifts. It introduces a framework to classify preferences and proposes mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To systematically dissect positional biases in LLMs, link them to underlying preference structures, and understand their impact on high-stakes decisions.

Method: Comprehensive investigation across multiple LLM architectures and domains, introducing a framework to classify pairwise preferences as robust, fragile, or indifferent.

Result: Strong and consistent order effects, including centrality bias and quality-dependent shifts, with positional biases often stronger than gender biases. LLMs exhibit unique failure modes not seen in humans.

Conclusion: LLMs display distinct biases beyond human-like patterns, requiring targeted mitigation strategies like adjusting the temperature parameter to reduce distortions.

Abstract: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.

</details>


### [24] [Situational-Constrained Sequential Resources Allocation via Reinforcement Learning](https://arxiv.org/abs/2506.14125)
*Libo Zhang,Yang Chen,Toru Takisaka,Kaiqi Zhao,Weidong Li,Jiamou Liu*

Main category: cs.AI

TL;DR: The paper introduces SCRL, a novel framework for sequential resource allocation with situational constraints, outperforming baselines in constraint satisfaction and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of context-dependent resource allocation where demands and priorities vary dynamically.

Method: Formalizes constraints as logic implications, introduces a dynamic penalty algorithm, and uses a probabilistic selection mechanism.

Result: SCRL outperforms existing baselines in constraint satisfaction and resource efficiency in medical and agricultural scenarios.

Conclusion: SCRL is effective for real-world, context-sensitive decision-making tasks.

Abstract: Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.

</details>


### [25] [Collaborative Editable Model](https://arxiv.org/abs/2506.14146)
*Kaiwen Tang,Aitong Wu,Yao Lu,Guangda Sun*

Main category: cs.AI

TL;DR: CoEM introduces a collaborative, editable model for vertical-domain LLMs, using user-contributed knowledge and feedback for lightweight domain adaptation, improving accuracy without heavy fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional vertical-domain LLMs require extensive annotated data and computational resources, hindering rapid development and iteration.

Method: CoEM builds a knowledge pool from user snippets, uses interactive dialogues and ratings to identify high-value fragments, and injects them via prompts for adaptation.

Result: In a financial scenario, CoEM improved domain-specific content generation with 15k user feedback, avoiding traditional fine-tuning costs.

Conclusion: CoEM offers an efficient alternative to resource-heavy fine-tuning, enhancing domain-specific LLM performance through collaborative knowledge integration.

Abstract: Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.

</details>


### [26] [What's in the Box? Reasoning about Unseen Objects from Multimodal Cues](https://arxiv.org/abs/2506.14212)
*Lance Ying,Daniel Xu,Alicia Zhang,Katherine M. Collins,Max H. Siegel,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: A neurosymbolic model integrates multimodal inputs using neural networks and Bayesian reasoning, outperforming unimodal and large neural models in correlating with human judgments in an object guessing task.


<details>
  <summary>Details</summary>
Motivation: To understand how humans flexibly integrate diverse information sources (e.g., auditory, visual, language, prior knowledge) to infer unseen objects.

Method: Proposes a neurosymbolic model combining neural networks for parsing multimodal inputs and a Bayesian model for information integration. Evaluated using the "What's in the Box?" game.

Result: The model strongly correlates with human judgments, unlike unimodal or large neural model baselines.

Conclusion: The neurosymbolic approach effectively mimics human-like flexible integration of multimodal information.

Abstract: People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.

</details>


### [27] [From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models](https://arxiv.org/abs/2506.14224)
*Xinyang Li,Siqi Liu,Bochao Zou,Jiansheng Chen,Huimin Ma*

Main category: cs.AI

TL;DR: The paper introduces an interpretability-driven method to assess Theory of Mind (ToM) in multimodal large language models (MLLMs) using a new dataset, GridToM, and attention head analysis.


<details>
  <summary>Details</summary>
Motivation: Existing ToM evaluations treat models as black boxes; this study aims to explore internal mechanisms for better interpretability.

Method: Constructs GridToM dataset for belief testing, analyzes attention heads in MLLMs, and proposes a training-free adjustment method.

Result: Attention heads distinguish cognitive information, and the proposed method enhances ToM capabilities.

Conclusion: The approach provides interpretable insights into ToM in MLLMs and improves performance without additional training.

Abstract: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.

</details>


### [28] [ImpReSS: Implicit Recommender System for Support Conversations](https://arxiv.org/abs/2506.14231)
*Omri Haller,Yair Meidan,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.AI

TL;DR: ImpReSS is an implicit recommender system for customer support conversations, integrating product recommendations without assuming user intent, showing strong performance in empirical tests.


<details>
  <summary>Details</summary>
Motivation: To enhance customer support by implicitly recommending relevant solution product categories (SPCs) during interactions, addressing a gap in LLM-based conversational recommender systems.

Method: ImpReSS operates alongside support chatbots, analyzing conversations to identify opportunities for recommending SPCs that resolve or prevent issues.

Result: Empirical evaluation shows high performance (e.g., MRR@1 of 0.72 for general problem solving) in recommending relevant SPCs.

Conclusion: ImpReSS effectively integrates recommendations into customer support, supporting both issue resolution and business growth, with promising results across domains.

Abstract: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.

</details>


### [29] [Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?](https://arxiv.org/abs/2506.14239)
*Louis Vervoort,Vitaly Nikolaev*

Main category: cs.AI

TL;DR: A test for abstract causal reasoning in AI is proposed, using neuron diagrams from philosophy. Advanced LLMs like ChatGPT show promising results, challenging the notion that defining causation is elusive.


<details>
  <summary>Details</summary>
Motivation: To evaluate AI's ability in abstract causal reasoning and propose a broader definition of causation in neuron diagrams.

Method: Using neuron diagrams from philosophy (D. Lewis) to test LLMs like ChatGPT, DeepSeek, and Gemini.

Result: LLMs correctly identify debated causes, suggesting a broader definition of causation is possible.

Conclusion: AI's success in causal reasoning hints at future philosophical research as a collaboration between human and artificial expertise.

Abstract: We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.

</details>


### [30] [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
*Xumeng Wen,Zihan Liu,Shun Zheng,Zhijian Xu,Shengyu Ye,Zhirong Wu,Xiao Liang,Yang Wang,Junjie Li,Ziming Miao,Jiang Bian,Mao Yang*

Main category: cs.AI

TL;DR: RLVR improves LLM reasoning but underperforms on Pass@K due to flawed metrics. Introducing CoT-Pass@K shows RLVR's true potential by ensuring correct reasoning paths and answers.


<details>
  <summary>Details</summary>
Motivation: Address the paradox where RLVR-tuned models underperform on Pass@K, suggesting flawed evaluation metrics and the need for accurate reasoning assessment.

Method: Introduce CoT-Pass@K to evaluate reasoning paths and answers. Provide theoretical foundation for RLVR's logical integrity.

Result: RLVR incentivizes correct reasoning generalization under CoT-Pass@K, with early and smooth training dynamics.

Conclusion: RLVR's potential is confirmed with proper evaluation, advancing machine reasoning.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.

</details>


### [31] [Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents](https://arxiv.org/abs/2506.14246)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Qifan Zheng,Wenxin Li*

Main category: cs.AI

TL;DR: The paper introduces Mxplainer, a method to convert black-box Mahjong AI agents into interpretable neural networks, providing insights into their strategies and decision-making.


<details>
  <summary>Details</summary>
Motivation: To help people learn from AI agents in Mahjong by making their black-box strategies understandable.

Method: Uses Mxplainer, a parameterized search algorithm converted into a neural network, to analyze AI and human player data.

Result: The learned parameters offer human-understandable insights into AI agents' play styles and enable local explanations of their decisions.

Conclusion: Mxplainer successfully bridges the gap between black-box AI and human interpretability in Mahjong.

Abstract: People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.

</details>


### [32] [Don't throw the baby out with the bathwater: How and why deep learning for ARC](https://arxiv.org/abs/2506.14276)
*Jack Cole,Mohamed Osman*

Main category: cs.AI

TL;DR: The paper demonstrates that deep learning, combined with on-the-fly NN training and novel test-time techniques (TTFT and AIRV), achieves state-of-the-art performance on the ARC-AGI challenge, boosting accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: The ARC-AGI challenge is difficult for AI systems, and the paper aims to leverage deep learning's ability to acquire novel abstractions to improve performance on ARC.

Method: The approach involves using pretrained LLMs, Test-Time Fine-Tuning (TTFT), and the Augment Inference Reverse-Augmentation and Vote (AIRV) technique to enhance reasoning and generalization.

Result: The method achieved a 260% accuracy boost with AIRV and a further 300% with TTFT, securing first place in the 2023 ARCathon and the best score (58%) on the ARC private test-set.

Conclusion: Deep learning, when fully committed to acquiring novel abstractions, is effective for ARC, highlighting key mechanisms for robust reasoning in unfamiliar domains.

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.

</details>


### [33] [ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems](https://arxiv.org/abs/2506.14299)
*Fanzhi Zeng,Siqi Wang,Chuzhao Zhu,Li Li*

Main category: cs.AI

TL;DR: The paper introduces ADRD, a framework using LLMs to create interpretable, rule-based autonomous driving systems, outperforming traditional methods in interpretability, speed, and performance.


<details>
  <summary>Details</summary>
Motivation: The need for interpretable autonomous driving decision-making systems drives the research, addressing gaps in transparency and adaptability.

Method: ADRD integrates three modules (Information, Agents, Testing) to aggregate scenario data, generate rules, and refine tactics iteratively.

Result: ADRD outperforms reinforcement learning and advanced LLM methods in interpretability, speed, and driving performance.

Conclusion: The study validates ADRD's potential for real-world deployment, pioneering LLM-rule-based integration in autonomous driving.

Abstract: How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.

</details>


### [34] [AviationLLM: An LLM-based Knowledge System for Aviation Training](https://arxiv.org/abs/2506.14336)
*Jia'ang Wan,Feng Shen,Fujuan Li,Yanjin Sun,Yan Li,Shiwen Zhang*

Main category: cs.AI

TL;DR: The paper introduces RALA-DPO, a method combining Direct Preference Optimization (DPO) and Retrieval-Augmented Generation (RAG) to enhance aviation training by improving LLM accuracy and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Limited instructors and inaccurate online answers hinder aviation training efficiency. Basic LLMs lack domain accuracy, and traditional fine-tuning risks incorrect responses.

Method: Proposes RALA-DPO: fine-tuning Qwen LLM with DPO for domain alignment and integrating RAG to mitigate hallucinations and improve answer accuracy.

Result: RALA-DPO enhances response accuracy for aviation knowledge and enables zero-cost knowledge updates via RAG.

Conclusion: RALA-DPO effectively addresses aviation training challenges by combining DPO and RAG, improving accuracy and efficiency.

Abstract: Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.

</details>


### [35] [Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning](https://arxiv.org/abs/2506.14387)
*William F. Shen,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.AI

TL;DR: SEAT is a novel fine-tuning method for LLMs that preserves ignorance awareness and prevents hallucinations by combining sparse training and entity perturbation with KL-divergence regularization.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods degrade LLMs' ability to acknowledge ignorance, leading to undesired behaviors like hallucinations.

Method: SEAT integrates sparse training to limit activation drift and uses entity perturbation with KL-divergence regularization to counter knowledge entanglement.

Result: SEAT outperforms baselines in preserving ignorance awareness while maintaining fine-tuning performance.

Conclusion: SEAT provides a robust solution for LLM fine-tuning by balancing performance and safety alignment.

Abstract: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.

</details>


### [36] [AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection](https://arxiv.org/abs/2506.14470)
*Zixian Zhang,Takfarinas Saber*

Main category: cs.AI

TL;DR: The paper evaluates hybrid AST-based representations (CFG, DFG, FA-AST) in GNNs for code clone detection, finding AST+CFG+DFG improves accuracy but FA-AST can harm performance. GMN excels even with standard ASTs.


<details>
  <summary>Details</summary>
Motivation: Code clones increase maintenance costs and risks; ASTs lack semantics, so hybrid representations are explored for better detection.

Method: Empirical study comparing AST+CFG+DFG and FA-AST across GNN architectures (GCN, GAT, GMN).

Result: AST+CFG+DFG boosts accuracy for GCN/GAT, FA-AST often reduces performance, and GMN works best with standard ASTs.

Conclusion: Hybrid representations vary in effectiveness; GMN's robustness reduces the need for enriched ASTs.

Abstract: As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.

</details>


### [37] [GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2506.14477)
*Jingqi Yang,Zhilong Song,Jiawei Chen,Mingli Song,Sheng Zhou,linjun sun,Xiaogang Ouyang,Chun Chen,Can Wang*

Main category: cs.AI

TL;DR: The paper introduces GUI-Robust, a dataset for evaluating GUI agents, incorporating real-world anomalies, and proposes a semi-automated construction method using RPA tools and MLLMs, reducing annotation time by 19x.


<details>
  <summary>Details</summary>
Motivation: Existing GUI datasets lack real-world anomalies, limiting their practical utility for benchmarking GUI agents.

Method: A semi-automated paradigm using RPA tools to collect user actions and MLLMs to generate descriptions, reducing annotation effort.

Result: State-of-the-art GUI agents show significant performance drops in abnormal scenarios on GUI-Robust.

Conclusion: GUI-Robust emphasizes the need for robust GUI agents and provides a resource for future research.

Abstract: The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at https://github.com/chessbean1/GUI-Robust..

</details>


### [38] [LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?](https://arxiv.org/abs/2506.14496)
*Muhammad Atta Ur Rahman,Melanie Schranz*

Main category: cs.AI

TL;DR: The paper contrasts traditional swarm intelligence with LLM-driven swarms, evaluating their performance and discussing the redefinition of 'swarm' in modern AI.


<details>
  <summary>Details</summary>
Motivation: To explore how decentralization, scalability, and emergence are redefined when using LLMs as collaborative agents in swarms.

Method: Implemented and compared traditional swarm algorithms (Boids and ACO) with LLM-driven swarms, assessing latency, resource usage, and behavioral accuracy.

Result: LLMs offer powerful reasoning but introduce computational and coordination constraints, challenging traditional swarm design.

Conclusion: The study highlights the potential and limitations of integrating LLMs into swarm systems, reshaping the concept of 'swarm' in AI.

Abstract: Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.

</details>


### [39] [Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow](https://arxiv.org/abs/2506.14502)
*Xiao Wang,Junru Yu,Jun Huang,Qiong Wu,Ljubo Vacic,Changyin Sun*

Main category: cs.AI

TL;DR: A safety-first human-like decision-making framework (SF-HLDM) for AVs addresses challenges in dense traffic by combining intention inference, behavior regulation, and efficient learning to ensure safe, adaptable driving.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles struggle in dynamic, dense traffic due to human unpredictability and data-driven methods' limitations, necessitating a robust, adaptable decision-making framework.

Method: SF-HLDM integrates spatial-temporal attention for intention inference, social compliance estimation, and Deep Evolutionary Reinforcement Learning to optimize decision-making and avoid local optima.

Result: The framework enables AVs to dynamically adjust decisions for safety and social compatibility, improving adaptability and interpretability.

Conclusion: SF-HLDM enhances AV performance in complex traffic by balancing safety, comfort, and social compliance through advanced, flexible decision-making.

Abstract: Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.

</details>


### [40] [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
*Daewon Kang,YeongHwan Shin,Doyeon Kim,Kyu-Hwan Jung,Meong Hi Son*

Main category: cs.AI

TL;DR: The paper addresses risks of prompt engineering in autonomous agents, proposing the Doppelgänger method to expose vulnerabilities, defining PACAT for evaluation, and introducing CAT prompts for defense.


<details>
  <summary>Details</summary>
Motivation: Concerns about safety, robustness, and behavioral consistency of prompts in autonomous agents, along with the risk of exposure to adversarial attacks.

Method: Proposes the Doppelgänger method to hijack agents, defines PACAT for vulnerability evaluation, and introduces CAT prompts for defense.

Result: Doppelgänger method compromises agent consistency and exposes internal info; CAT prompts effectively defend against such attacks.

Conclusion: The study highlights prompt vulnerabilities and offers a practical defense (CAT prompts) against adversarial transfer attacks.

Abstract: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelgänger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelgänger method. The experimental results demonstrate that the Doppelgänger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.

</details>


### [41] [QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents](https://arxiv.org/abs/2506.14568)
*Eliott Thomas,Mickael Coustaty,Aurelie Joseph,Gaspar Deloin,Elodie Carel,Vincent Poulain D'Andecy,Jean-Marc Ogier*

Main category: cs.AI

TL;DR: QUEST is a quality-aware semi-supervised framework for table extraction from business documents, improving F1 scores and reducing empty predictions by leveraging a novel quality assessment model and diversity measures.


<details>
  <summary>Details</summary>
Motivation: Automating table extraction from business documents is challenging due to sparse annotations and unreliable confidence scores in existing methods.

Method: QUEST introduces a quality assessment model trained to predict F1 scores and uses diversity measures (DPP, Vendi score, IntDiv) to guide pseudo-label selection in semi-supervised learning.

Result: On proprietary and benchmark datasets, QUEST improves F1 scores (64% to 74% and 42% to 50%) and reduces empty predictions (12% to 6.5% and 27% to 22%).

Conclusion: QUEST's interpretable quality assessments and robustness to annotation scarcity make it effective for business documents, emphasizing structural consistency and data completeness.

Abstract: Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.

</details>


### [42] [Enhancing Symbolic Machine Learning by Subsymbolic Representations](https://arxiv.org/abs/2506.14569)
*Stephen Roth,Lennart Baur,Derian Boer,Stefan Kramer*

Main category: cs.AI

TL;DR: The paper proposes enhancing symbolic machine learning (e.g., TILDE) with neural embeddings to improve efficiency and performance, outperforming baselines in F1 score.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies of neuro-symbolic AI systems (e.g., LTNs, DeepProbLog) in simpler settings like discriminative ML, especially with many constants.

Method: Enhance symbolic ML (TILDE) by integrating neural embeddings for constants in similarity predicates, with fine-tuning based on symbolic theory.

Result: Outperforms baseline methods in F1 score across three real-world domains.

Conclusion: The approach is effective and extensible to other tasks like analogical reasoning or propositionalization.

Abstract: The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.

</details>


### [43] [From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places](https://arxiv.org/abs/2506.14570)
*Mohammad Hashemi,Andreas Zufle*

Main category: cs.AI

TL;DR: The paper advocates for spatial foundation models to generalize human mobility data, addressing gaps in adaptability and scalability for geospatial intelligence.


<details>
  <summary>Details</summary>
Motivation: To model human mobility effectively, capturing social behavior and spatial dynamics, and to overcome limitations in current foundation models for spatiotemporal data.

Method: Proposes a shift from discrete points of interest to dynamic, context-rich regions (places) and identifies research directions for scalable, multi-granular reasoning.

Result: Identifies key gaps and proposes solutions for adaptable, scalable models to enhance geospatial intelligence.

Conclusion: Spatial foundation models can enable smarter spatial decision-making, with applications in urban planning, logistics, and personalized place discovery.

Abstract: Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.

</details>


### [44] [AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes](https://arxiv.org/abs/2506.14728)
*Jiahao Qiu,Xinzhe Juan,Yimin Wang,Ling Yang,Xuan Qi,Tongcheng Zhang,Jiacheng Guo,Yifu Lu,Zixin Yao,Hongru Wang,Shilong Liu,Xun Jiang,Liu Leqi,Mengdi Wang*

Main category: cs.AI

TL;DR: AgentDistill is a training-free framework for distilling LLM-based agents by reusing structured task-solving modules (MCPs) from teachers, enabling scalable and efficient knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Existing agent distillation methods struggle with dynamic planning in novel environments, limiting their effectiveness.

Method: AgentDistill reuses Model-Context-Protocols (MCPs) autonomously generated by teacher agents for training-free distillation.

Result: Student agents built on small language models achieve performance comparable to advanced systems like GPT-4o.

Conclusion: AgentDistill offers a scalable and cost-efficient solution for intelligent agent development.

Abstract: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.

</details>


### [45] [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
*Zhengxiang Cheng,Dongping Chen,Mingyang Fu,Tianyi Zhou*

Main category: cs.AI

TL;DR: LC-R1 is a post-training method for Large Reasoning Models (LRMs) that reduces verbose reasoning chains by 50% with minimal accuracy loss, using Brevity and Sufficiency principles.


<details>
  <summary>Details</summary>
Motivation: LRMs often produce redundant reasoning chains ('invalid thinking'), leading to inefficiency.

Method: Introduces LC-R1, a method using Group Relative Policy Optimization (GRPO) with Length and Compress Rewards to eliminate redundancy.

Result: Achieves ~50% reduction in sequence length with only ~2% accuracy drop.

Conclusion: LC-R1 offers a robust trade-off between conciseness and accuracy, advancing efficient LRMs.

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
*Banseok Lee,Dongkyu Kim,Youngcheon You,Youngmin Kim*

Main category: cs.LG

TL;DR: LittleBit is a novel method for extreme LLM compression, achieving 31× memory reduction at 0.1 bits per weight (BPW) with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of memory and computational costs in deploying large language models (LLMs) through effective sub-1-bit quantization.

Method: Uses low-rank matrix factorization and binarization, enhanced by a multi-scale compensation mechanism and techniques like Dual-SVID and Residual Compensation for stable training.

Result: Achieves superior compression (e.g., Llama2-13B to under 0.9 GB) and performance, outperforming leading methods at lower bit rates (0.1 BPW vs. 0.7 BPW).

Conclusion: LittleBit enables efficient deployment of LLMs in resource-constrained environments, offering a 5× speedup potential over FP16.

Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.

</details>


### [47] [MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs](https://arxiv.org/abs/2506.13772)
*Zhenyan Lu,Daliang Xu,Dongqi Cai,Zexi Li,Wei Liu,Fangming Liu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: MobiEdit is a mobile knowledge editing framework for LLMs, enabling efficient personalization on mobile devices by replacing backpropagation with quantized forward-only gradient estimation, reducing memory, energy, and latency.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate on personalized queries; existing knowledge editing methods are impractical for mobile due to resource-heavy backpropagation.

Method: MobiEdit uses quantized forward-only gradient estimation, early stopping, and prefix caching to optimize efficiency for mobile NPUs.

Result: Achieves 7.6× less memory, 14.7× less energy, and 3.6× less latency for a 3B-parameter model on COTS devices.

Conclusion: MobiEdit enables real-time, efficient LLM personalization on mobile devices, outperforming prior methods.

Abstract: Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods.

</details>


### [48] [Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment](https://arxiv.org/abs/2506.13781)
*Pablo Ariño Fernández*

Main category: cs.LG

TL;DR: JobShopLib is a modular library introduced to simplify experimentation with deep learning models for job shop scheduling, outperforming traditional methods and highlighting the importance of feature customization.


<details>
  <summary>Details</summary>
Motivation: The complexity and lack of modular tools for training deep learning models in job shop scheduling motivated the creation of JobShopLib.

Method: The paper introduces JobShopLib, a modular library for customizing graph representation, node features, action space, and reward functions, and trains dispatchers using imitation learning.

Result: A GNN model trained with JobShopLib achieved near state-of-the-art results, outperforming traditional graph-based dispatchers.

Conclusion: JobShopLib enables efficient experimentation and suggests significant potential for further improvements in deep learning-based job shop scheduling.

Abstract: The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.

</details>


### [49] [Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction](https://arxiv.org/abs/2506.13786)
*Vuong M. Ngo,Tran Quang Vinh,Patricia Kearney,Mark Roantree*

Main category: cs.LG

TL;DR: The paper introduces an enhanced bagging ensemble regression model (EBMBag+) for predicting diabetes prevalence in U.S. cities, outperforming baseline models with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate state-level diabetes predictions are crucial for healthcare planning, but incomplete data poses challenges.

Method: The study integrates diabetes datasets (2011-2021) and develops EBMBag+ for time series forecasting, comparing it with baseline models like SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag.

Result: EBMBag+ achieved the best performance (MAE: 0.41, RMSE: 0.53, MAPE: 4.01, R2: 0.9).

Conclusion: EBMBag+ is effective for diabetes prevalence prediction, offering potential for improved healthcare interventions.

Abstract: Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.

</details>


### [50] [Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles](https://arxiv.org/abs/2506.13828)
*Abdullah Burkan Bereketoglu*

Main category: cs.LG

TL;DR: A hybrid meta-learning framework combining physics-inspired simulation, CNN-LSTM, VAE, Isolation Forests, and DA-RNN for forecasting and anomaly detection in nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of forecasting and anomaly detection in nonlinear dynamical systems with nonstationary and stochastic behavior, where complete physical models may not be available.

Method: Integrates a physics-inspired simulator with CNN-LSTM for spatio-temporal features, VAE for anomaly scoring, Isolation Forests for outlier detection, and DA-RNN for forecasting. A meta-learner combines outputs for composite anomaly forecasts.

Result: Outperforms standalone models in anomaly localization, generalization, and robustness to nonlinear deviations.

Conclusion: The framework offers a versatile, data-driven solution for early defect identification and predictive monitoring in nonlinear systems.

Abstract: We propose a hybrid meta-learning framework for forecasting and anomaly detection in nonlinear dynamical systems characterized by nonstationary and stochastic behavior. The approach integrates a physics-inspired simulator that captures nonlinear growth-relaxation dynamics with random perturbations, representative of many complex physical, industrial, and cyber-physical systems. We use CNN-LSTM architectures for spatio-temporal feature extraction, Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation Forests for residual-based outlier detection in addition to a Dual-Stage Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of the generated simulation data. To create composite anomaly forecasts, these models are combined using a meta-learner that combines forecasting outputs, reconstruction errors, and residual scores. The hybrid ensemble performs better than standalone models in anomaly localization, generalization, and robustness to nonlinear deviations, according to simulation-based experiments. The framework provides a broad, data-driven approach to early defect identification and predictive monitoring in nonlinear systems, which may be applied to a variety of scenarios where complete physical models might not be accessible.

</details>


### [51] [Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation](https://arxiv.org/abs/2506.13831)
*Jitian Zhao,Chenghui Li,Frederic Sala,Karl Rohe*

Main category: cs.LG

TL;DR: A hypothesis testing framework is introduced to validate concepts in CLIP embeddings, improving interpretability and reducing spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Current concept-based methods lack statistical rigor, making validation and comparison difficult.

Method: A hypothesis testing framework identifies rotation-sensitive structures, followed by a post-hoc concept decomposition method with theoretical guarantees.

Result: The method improves reconstruction accuracy and interpretability, increasing worst-group accuracy by 22.6% in a spurious correlation dataset.

Conclusion: The proposed framework enhances concept validation and interpretability in deep neural network embeddings, mitigating spurious cues.

Abstract: Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.

</details>


### [52] [Evolvable Conditional Diffusion](https://arxiv.org/abs/2506.13834)
*Zhao Wei,Chin Chun Ooi,Abhishek Gupta,Jian Cheng Wong,Pao-Hsiung Chiu,Sheares Xue Wen Toh,Yew-Soon Ong*

Main category: cs.LG

TL;DR: The paper introduces an evolvable conditional diffusion method for guiding generative processes in non-differentiable multi-physics models, enabling autonomous scientific discovery without gradient computation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of using black-box, non-differentiable multi-physics models (common in fields like fluid dynamics and electromagnetics) for guiding generative processes in scientific discovery.

Method: Formulates guidance as an optimization problem, optimizing a fitness function by updating descriptive statistics for the denoising distribution, derived from probabilistic evolution principles.

Result: Validated in AI for Science scenarios (fluidic topology and meta-surface design), showing effective generation of designs meeting optimization objectives without differentiable proxies.

Conclusion: The method provides a gradient-free, effective approach for guidance-based diffusion, leveraging non-differentiable multi-physics models in scientific applications.

Abstract: This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.

</details>


### [53] [Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study](https://arxiv.org/abs/2506.13836)
*Dang Viet Anh Nguyen,Carlos Lima Azevedo,Tomer Toledo,Filipe Rodrigues*

Main category: cs.LG

TL;DR: T-REX is a SUMO-based framework for training and evaluating RL-TSC methods under dynamic traffic incidents, revealing performance trade-offs between independent and hierarchical methods.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored robustness of RL-TSC under real-world disruptions like traffic incidents.

Method: Introduces T-REX, a simulation framework modeling realistic traffic behaviors (rerouting, speed adaptation, lane-changing) and proposes robustness metrics. Evaluates state-of-the-art RL-TSC methods in synthetic and real-world networks.

Result: Independent methods perform well in stable conditions but degrade under incidents, while hierarchical methods offer stability at the cost of slower convergence.

Conclusion: Highlights the need for robustness-aware RL-TSC design; T-REX provides a standardized platform for benchmarking under dynamic scenarios.

Abstract: Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.

</details>


### [54] [Light Aircraft Game : Basic Implementation and training results analysis](https://arxiv.org/abs/2506.14164)
*Hanzhong Cao*

Main category: cs.LG

TL;DR: The paper evaluates HAPPO and HASAC in a mixed cooperative-competitive MARL environment (LAG), showing HASAC excels in simple tasks, while HAPPO adapts better to dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of on-policy (HAPPO) and off-policy (HASAC) MARL methods in a partially observable, mixed combat environment.

Method: The study tests HAPPO (hierarchical PPO) and HASAC (hierarchical SAC) in the LAG environment, analyzing training stability, rewards, and coordination.

Result: HASAC performs better in simple tasks (No Weapon), while HAPPO excels in dynamic scenarios (ShootMissile).

Conclusion: The findings highlight trade-offs between on-policy and off-policy methods in MARL, with HAPPO being more adaptable in complex settings.

Abstract: This paper investigates multi-agent reinforcement learning (MARL) in a partially observable, cooperative-competitive combat environment known as LAG. We describe the environment's setup, including agent actions, hierarchical controls, and reward design across different combat modes such as No Weapon and ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy hierarchical variant of PPO, and HASAC, an off-policy method based on soft actor-critic. We analyze their training stability, reward progression, and inter-agent coordination capabilities. Experimental results show that HASAC performs well in simpler coordination tasks without weapons, while HAPPO demonstrates stronger adaptability in more dynamic and expressive scenarios involving missile combat. These findings provide insights into the trade-offs between on-policy and off-policy methods in multi-agent settings.

</details>


### [55] [Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy](https://arxiv.org/abs/2506.13838)
*Lorena Poenaru-Olaru,June Sallou,Luis Cruz,Jan Rellermeyer,Arie van Deursen*

Main category: cs.LG

TL;DR: The paper explores energy-efficient retraining techniques for ML systems, showing that using recent data and adaptive retraining can reduce energy consumption by 25-40% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the environmental impact of frequent ML model retraining by identifying energy-efficient techniques.

Method: Study the energy consumption and accuracy of common retraining techniques, focusing on using recent data and adaptive retraining schedules.

Result: Retraining with recent data reduces energy by 25%, and adaptive retraining reduces it by 40% with reliable change detection.

Conclusion: The findings guide ML practitioners toward sustainable retraining practices, balancing energy efficiency and accuracy.

Abstract: The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.

</details>


### [56] [SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors](https://arxiv.org/abs/2506.13842)
*Yuanlong Wang,Pengqi Wang,Changchang Yin,Ping Zhang*

Main category: cs.LG

TL;DR: SatHealth is a novel dataset combining multimodal spatiotemporal data (environmental, satellite images, disease prevalences, SDoH) to improve AI models in healthcare. It enhances performance and generalizability, demonstrated in regional public health and personal disease risk prediction. A web app facilitates access and utilization.


<details>
  <summary>Details</summary>
Motivation: Existing studies lack long-term, fine-grained environmental data, limiting AI model performance in healthcare. SatHealth addresses this gap by integrating diverse data sources.

Method: Developed SatHealth, a dataset with multimodal spatiotemporal data. Tested in regional public health modeling and personal disease risk prediction.

Result: Environmental data significantly improves AI model performance and spatiotemporal generalizability.

Conclusion: SatHealth provides a foundational framework for integrating environmental data in healthcare research, with ongoing updates and a web app for accessibility.

Abstract: Living environments play a vital role in the prevalence and progression of diseases, and understanding their impact on patient's health status becomes increasingly crucial for developing AI models. However, due to the lack of long-term and fine-grained spatial and temporal data in public and population health studies, most existing studies fail to incorporate environmental data, limiting the models' performance and real-world application. To address this shortage, we developed SatHealth, a novel dataset combining multimodal spatiotemporal data, including environmental data, satellite images, all-disease prevalences estimated from medical claims, and social determinants of health (SDoH) indicators. We conducted experiments under two use cases with SatHealth: regional public health modeling and personal disease risk prediction. Experimental results show that living environmental information can significantly improve AI models' performance and temporal-spatial generalizability on various tasks. Finally, we deploy a web-based application to provide an exploration tool for SatHealth and one-click access to both our data and regional environmental embedding to facilitate plug-and-play utilization. SatHealth is now published with data in Ohio, and we will keep updating SatHealth to cover the other parts of the US. With the web application and published code pipeline, our work provides valuable angles and resources to include environmental data in healthcare research and establishes a foundational framework for future research in environmental health informatics.

</details>


### [57] [StaQ it! Growing neural networks for Policy Mirror Descent](https://arxiv.org/abs/2506.13862)
*Alena Shilova,Alex Davey,Brahim Driss,Riad Akrour*

Main category: cs.LG

TL;DR: The paper introduces StaQ, a PMD-like algorithm for RL that only retains the last M Q-functions, ensuring convergence without policy update errors, outperforming prior methods in stability.


<details>
  <summary>Details</summary>
Motivation: To address the intractability of PMD's closed-form solution involving all past Q-functions, aiming for a practical, stable deep RL algorithm.

Method: Proposes PMD-like algorithms retaining only the last M Q-functions, ensuring tractability and convergence for large enough M.

Result: StaQ, the derived algorithm, shows competitive performance with deep RL baselines and reduced performance oscillation.

Conclusion: StaQ provides a stable, theoretically sound PMD implementation, advancing practical deep RL and enabling further experimentation.

Abstract: In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.

</details>


### [58] [Scaling Algorithm Distillation for Continuous Control with Mamba](https://arxiv.org/abs/2506.13892)
*Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: Algorithm Distillation (AD) is improved using S6 models (Mamba) for In-Context Reinforcement Learning (ICRL), outperforming transformers in complex environments and scaling linearly with sequence length.


<details>
  <summary>Details</summary>
Motivation: Transformers' quadratic complexity limits AD's application to simple environments; S6 models offer linear scaling and better performance for long sequences.

Method: Replace transformers with S6-based Mamba models in AD, tested on four complex Meta RL environments.

Result: Mamba outperforms transformers in AD, scales to long contexts, and matches SOTA online meta RL performance.

Conclusion: S6 models like Mamba enhance AD for ICRL, enabling scalability and competitive performance in complex tasks.

Abstract: Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.

</details>


### [59] [Enhancing interpretability of rule-based classifiers through feature graphs](https://arxiv.org/abs/2506.13903)
*Christel Sirocchi,Damiano Verda*

Main category: cs.LG

TL;DR: A framework for estimating feature contributions in rule-based systems is proposed, featuring graph-based visualization, a novel importance metric, and a rule set comparison metric. Tested on clinical datasets, it provides insights for risk factors and diagnostics.


<details>
  <summary>Details</summary>
Motivation: Rule-based systems are trusted in critical domains like healthcare, but their complexity makes feature analysis challenging. This work aims to enhance interpretability and utility.

Method: The framework includes graph-based visualization, a feature importance metric, and a rule set distance metric. Evaluated on clinical datasets using four rule-based methods.

Result: The method reveals insights on clinical features, aids in identifying risk factors, and shows competitive performance on benchmarks.

Conclusion: The proposed framework improves interpretability and utility of rule-based systems, with potential applications in healthcare diagnostics.

Abstract: In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.

</details>


### [60] [GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations](https://arxiv.org/abs/2506.13906)
*Milad Ramezankhani,Janak M. Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: GITO is a novel graph-informed transformer operator for learning PDE systems on irregular geometries, combining hybrid graph transformers and transformer neural operators for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning complex PDE systems on irregular geometries and non-uniform meshes, which existing methods struggle with.

Method: GITO uses a hybrid graph transformer (HGT) for local and long-range spatial relationships and a transformer neural operator (TNO) for discretization-invariant predictions.

Result: GITO outperforms existing transformer-based neural operators on benchmark PDE tasks.

Conclusion: GITO enables efficient, mesh-agnostic surrogate solvers for engineering applications.

Abstract: We present a novel graph-informed transformer operator (GITO) architecture for learning complex partial differential equation systems defined on irregular geometries and non-uniform meshes. GITO consists of two main modules: a hybrid graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages a graph neural network (GNN) to encode local spatial relationships and a transformer to capture long-range dependencies. A self-attention fusion layer integrates the outputs of the GNN and transformer to enable more expressive feature learning on graph-structured data. TNO module employs linear-complexity cross-attention and self-attention layers to map encoded input functions to predictions at arbitrary query locations, ensuring discretization invariance and enabling zero-shot super-resolution across any mesh. Empirical results on benchmark PDE tasks demonstrate that GITO outperforms existing transformer-based neural operators, paving the way for efficient, mesh-agnostic surrogate solvers in engineering applications.

</details>


### [61] [Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring](https://arxiv.org/abs/2506.13909)
*Xinyuan Tu,Haocheng Zhang,Tao Chengxu,Zuyi Chen*

Main category: cs.LG

TL;DR: The paper explores few-shot learning (FSL) for industrial time-series data, introducing a label-aware episodic sampler and comparing metric-based and gradient-based FSL paradigms. Lightweight CNNs with metric learning outperform large models like transformers in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: FSL is understudied for industrial time-series data, where annotation costs are high. The study aims to address this gap by evaluating FSL on screw-fastening defect detection.

Method: A label-aware episodic sampler is introduced to handle multi-label sequences. Two FSL paradigms (Prototypical Network and MAML) are tested with three backbones (1D CNN, InceptionTime, transformer).

Result: InceptionTime + Prototypical Network achieves 0.944 weighted F1 (multi-class) and 0.935 (multi-label), outperforming larger models. Metric learning consistently beats MAML, and label-aware sampling adds 1.7% F1.

Conclusion: Lightweight CNNs with metric learning generalize better in data-scarce settings, challenging the superiority of large models. Code and data are released for reproducibility.

Abstract: Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the multi-class regime and \textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.

</details>


### [62] [Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization](https://arxiv.org/abs/2506.13911)
*Arie Soeteman,Balder ten Cate*

Main category: cs.LG

TL;DR: HEGNNs extend GNNs with hierarchical node individualization, forming expressive models that can distinguish graphs up to isomorphism, outperforming traditional GNNs.


<details>
  <summary>Details</summary>
Motivation: To enhance GNN expressiveness by incorporating hierarchical node individualization, inspired by graph isomorphism testing.

Method: Proposes HEGNNs, a hierarchy of models with logical characterization using graded hybrid logic, and compares them to higher-order GNNs and color refinement.

Result: HEGNNs show improved performance over traditional GNNs, with practical feasibility confirmed experimentally.

Conclusion: HEGNNs offer a promising, expressive extension to GNNs, bridging theoretical and practical gaps in graph learning.

Abstract: We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.

</details>


### [63] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales,Arturo Jaramillo,Heli Ricalde Guerrero*

Main category: cs.LG

TL;DR: BSVGD extends SVGD with a branching mechanism for multimodal distributions, offering theoretical convergence guarantees and validated performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of SVGD in exploring multimodal distributions by introducing a branching mechanism.

Method: Extends SVGD with a random branching mechanism to explore state space, supported by theoretical convergence analysis.

Result: Demonstrates improved performance over SVGD in terms of Wasserstein distance and computational efficiency.

Conclusion: BSVGD is a promising method for variational inference in multimodal settings, outperforming SVGD.

Abstract: We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.

</details>


### [64] [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
*Vaskar Nath,Elaine Lau,Anisha Gunjal,Manasi Sharma,Nikhil Baharte,Sean Hendryx*

Main category: cs.LG

TL;DR: RLVR models improve reasoning by compressing pass@k into pass@1 and enabling capability gain, with self-distillation as the primary driver. The Guide algorithm, incorporating hints and optimizing off-policy trajectories, enhances performance.


<details>
  <summary>Details</summary>
Motivation: To understand how RLVR models learn to solve new problems and improve reasoning performance across domains like math, science, and code.

Method: Analyzed RLVR models (0.5B to 72B) on 500K+ problems, introduced Guide algorithm for adaptive hint incorporation and off-policy optimization.

Result: Guide-GRPO improves generalization by up to 4% on math benchmarks, with capability gain observed across model scales.

Conclusion: Guide enhances RLVR training efficiency and performance, with self-distillation and adaptive hinting as key drivers.

Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.

</details>


### [65] [ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture](https://arxiv.org/abs/2506.13935)
*Vishesh Kumar Tanwar,Soumik Sarkar,Asheesh K. Singh,Sajal K. Das*

Main category: cs.LG

TL;DR: ReinDSplit is a reinforcement learning-driven framework that dynamically optimizes DNN split points for heterogeneous edge devices in agriculture, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional split learning frameworks struggle with device heterogeneity in agriculture, leading to inefficiencies and performance issues.

Method: Uses a Q-learning agent to dynamically tailor DNN split points, balancing workloads and latency thresholds for optimal performance.

Result: Achieves 94.31% accuracy with MobileNetV2 on insect classification datasets.

Conclusion: ReinDSplit enhances split learning by integrating RL for better resource efficiency, privacy, and scalability in heterogeneous environments.

Abstract: To empower precision agriculture through distributed machine learning (DML), split learning (SL) has emerged as a promising paradigm, partitioning deep neural networks (DNNs) between edge devices and servers to reduce computational burdens and preserve data privacy. However, conventional SL frameworks' one-split-fits-all strategy is a critical limitation in agricultural ecosystems where edge insect monitoring devices exhibit vast heterogeneity in computational power, energy constraints, and connectivity. This leads to straggler bottlenecks, inefficient resource utilization, and compromised model performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement learning (RL)-driven framework that dynamically tailors DNN split points for each device, optimizing efficiency without sacrificing accuracy. Specifically, a Q-learning agent acts as an adaptive orchestrator, balancing workloads and latency thresholds across devices to mitigate computational starvation or overload. By framing split layer selection as a finite-state Markov decision process, ReinDSplit convergence ensures that highly constrained devices contribute meaningfully to model training over time. Evaluated on three insect classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit pioneers a paradigm shift in SL by harmonizing RL for resource efficiency, privacy, and scalability in heterogeneous environments.

</details>


### [66] [Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers](https://arxiv.org/abs/2506.13958)
*Leonardo Guiducci,Antonio Rizzo,Giovanna Maria Dimitri*

Main category: cs.LG

TL;DR: EDTs benefit from intrinsic motivation, but how it shapes embeddings was unclear. This paper introduces an explainability framework to analyze embedding properties, revealing environment-specific patterns that improve decision-making.


<details>
  <summary>Details</summary>
Motivation: To understand how intrinsic motivation mechanisms in EDTs shape learned embeddings and improve performance, as prior work lacked exploration of these representational mechanisms.

Method: A post-hoc explainability framework was developed to analyze embedding properties (covariance, vector magnitudes, orthogonality) in EDTs with intrinsic motivation.

Result: Different intrinsic motivation variants create distinct representational structures, with environment-specific correlations between embedding metrics and performance.

Conclusion: Intrinsic motivation acts as a representational prior, shaping embedding geometry in biologically plausible ways to enhance decision-making, beyond mere exploration bonuses.

Abstract: Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.

</details>


### [67] [Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble](https://arxiv.org/abs/2506.13972)
*Zhiqi Wang,Chengyu Zhang,Yuetian Chen,Nathalie Baracaldo,Swanand Kadhe,Lei Yu*

Main category: cs.LG

TL;DR: The paper investigates disparities in membership inference attacks (MIAs) and proposes an ensemble framework to improve privacy evaluation.


<details>
  <summary>Details</summary>
Motivation: Prior MIA research focuses on performance metrics but overlooks disparities among attacks, impacting reliability.

Method: A novel framework based on coverage and stability analysis is used to study MIA disparities.

Result: Experiments reveal significant disparities, their causes, and implications for privacy evaluation.

Conclusion: An ensemble framework is proposed to enhance MIA effectiveness and provide robust privacy evaluation.

Abstract: Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.

</details>


### [68] [Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability](https://arxiv.org/abs/2506.13974)
*Michael Crawshaw,Blake Woodworth,Mingrui Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous objectives requires stepsizes $η\leq 1/K$ where $K$ is the communication interval, which ensures monotonic decrease of the objective. In contrast, we analyze Local Gradient Descent for logistic regression with separable, heterogeneous data using any stepsize $η> 0$. With $R$ communication rounds and $M$ clients, we show convergence at a rate $\mathcal{O}(1/ηK R)$ after an initial unstable phase lasting for $\widetilde{\mathcal{O}}(ηK M)$ rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general smooth, convex objectives. Our analysis parallels the single machine analysis of~\cite{wu2024large} in which instability is caused by extremely large stepsizes, but in our setting another source of instability is large local updates with heterogeneous objectives.

</details>


### [69] [HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting](https://arxiv.org/abs/2506.13981)
*Thanh Dan Bui*

Main category: cs.LG

TL;DR: HAELT, a hybrid deep learning framework, outperforms in high-frequency stock price prediction by combining noise-mitigation, self-attention, and adaptive ensembling.


<details>
  <summary>Details</summary>
Motivation: High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility.

Method: Proposes HAELT, integrating ResNet-based noise reduction, temporal self-attention, and a hybrid LSTM-Transformer core, adaptively ensembled.

Result: Achieves the highest F1-Score on hourly AAPL data (Jan 2024-May 2025), effectively identifying price movements.

Conclusion: HAELT shows robust potential for financial forecasting and algorithmic trading.

Abstract: High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.

</details>


### [70] [Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems](https://arxiv.org/abs/2506.13987)
*Md Abrar Jahin,Adiba Abid,M. F. Mridha*

Main category: cs.LG

TL;DR: QCL-MixNet is a quantum-inspired contrastive learning framework with dynamic mixup for robust classification in imbalanced tabular data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing drawbacks of conventional approaches like overfitting and poor generalization in imbalanced data, especially for rare but critical instances.

Method: Integrates quantum entanglement-inspired layers, dynamic mixup, and a hybrid loss function to enhance minority class representation and improve classification.

Result: Outperforms 20 state-of-the-art baselines in macro-F1 and recall on 18 real-world datasets, validated by ablation studies.

Conclusion: QCL-MixNet sets a new benchmark for handling tabular imbalance in expert systems, supported by theoretical robustness.

Abstract: Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.

</details>


### [71] [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
*An Luo,Xun Xian,Jin Du,Fangqiao Tian,Ganghua Wang,Ming Zhong,Shengchun Zhao,Xuan Bi,Zirui Liu,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Mingyi Hong,Jie Ding*

Main category: cs.LG

TL;DR: AssistedDS benchmark evaluates LLMs' use of domain knowledge in tabular tasks, revealing uncritical adoption of adversarial info, insufficient counteraction of harmful guidance, and errors in time-series and categorical data handling.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can critically leverage external domain knowledge like human data scientists in automated data science workflows.

Method: Introduces AssistedDS, a benchmark with synthetic and real-world datasets, including helpful and adversarial documents, to evaluate LLMs' ability to discern and apply domain knowledge.

Result: LLMs often uncritically adopt adversarial info, struggle to counteract harmful guidance, and make errors in time-series and categorical data handling.

Conclusion: Current LLMs lack critical evaluation of expert knowledge, highlighting the need for more robust, knowledge-aware automated data science systems.

Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.

</details>


### [72] [Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences](https://arxiv.org/abs/2506.13996)
*Stas Bekman,Samyam Rajbhandari,Michael Wyatt,Jeff Rasley,Tunji Ruwase,Zhewei Yao,Aurick Qiao,Yuxiong He*

Main category: cs.LG

TL;DR: ALST enables long sequence training for HF models, supporting up to 15M tokens on a 4-node cluster, a 400x improvement over the 32K baseline.


<details>
  <summary>Details</summary>
Motivation: Long sequence training is challenging due to memory constraints and lack of open-source solutions, limiting accessibility for the broader AI community.

Method: ALST combines attention-agnostic single-GPU and multi-GPU memory optimizations to support long sequences for HF models.

Result: ALST trains Llama 8B with 500K tokens on a single H100 GPU, 3.7M on an 8xH100 node, and 15M on a 4-node cluster.

Conclusion: ALST democratizes long sequence training by making it accessible and scalable for HF models, with open-source availability.

Abstract: Long sequences are critical for applications like RAG, long document summarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout, support max sequence length of up to 10 million tokens. However, outside of enterprise labs, long sequence training is challenging for the AI community with limited system support in the open-source space.
  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B model with sequence over 32K runs out of memory on a basic Hugging Face (HF) model due to two reasons: i) LLM training workloads are not optimized to fully leverage a single GPU memory, ii) existing solutions for leveraging multiple GPU memory are not easily available to HF models, making long sequence training inaccessible.
  We address this with Arctic Long Sequence Training (ALST). It offers a combination of attention-agnostic single GPU and multi-GPU memory optimizations, that enables it to support out-of-box training of multi-million sequence length for a wide variety of HF models.
  ALST supports training Meta's Llama 8B model with 500K sequence length on a single H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node cluster, an increase of over 400x compared to the 32K baseline for the latter. ALST is fully compatible with HF models and open-sourced via Deepspeed https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic Training https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md.

</details>


### [73] [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
*Siyu Chen,Heejune Sheen,Xuyuan Xiong,Tianhao Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: The paper introduces a novel statistical framework and training algorithm for Sparse Autoencoders (SAEs) to improve feature recovery in Large Language Models (LLMs), with theoretical guarantees and empirical success.


<details>
  <summary>Details</summary>
Motivation: Existing SAE training methods lack mathematical rigor and suffer from practical issues like hyperparameter sensitivity and instability, hindering reliable feature recovery in LLMs.

Method: The authors propose a statistical framework for feature identifiability and a new SAE training algorithm called "bias adaptation," which adjusts biases for sparsity. They also develop an empirical variant, Group Bias Adaptation (GBA).

Result: The algorithm theoretically recovers all monosemantic features under their model. GBA outperforms benchmarks on LLMs with up to 1.5 billion parameters.

Conclusion: This work provides the first SAE algorithm with theoretical guarantees, advancing mechanistic interpretability and transparency in AI systems.

Abstract: We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.

</details>


### [74] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/abs/2506.14003)
*Yiwei Chen,Soumyadeep Pal,Yimeng Zhang,Qing Qu,Sijia Liu*

Main category: cs.LG

TL;DR: The paper identifies a vulnerability in machine unlearning for LLMs, showing that unlearning leaves detectable traces in model behavior and representations, which can be exploited to reverse-engineer forgotten information.


<details>
  <summary>Details</summary>
Motivation: To highlight the risks of unlearning trace detection in LLMs, which can compromise privacy, copyright enforcement, and harm mitigation efforts.

Method: The study uses supervised classifiers to detect unlearning traces in LLM outputs and analyzes intermediate activations to identify low-dimensional manifolds.

Result: Experiments show over 90% accuracy in detecting unlearning traces, even with forget-irrelevant inputs, revealing persistent signatures.

Conclusion: Unlearning leaves measurable traces, posing a risk of reverse-engineering forgotten data, necessitating further research to mitigate this vulnerability.

Abstract: Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).

</details>


### [75] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/abs/2506.14020)
*Keyue Jiang,Jiahao Cui,Xiaowen Dong,Laura Toni*

Main category: cs.LG

TL;DR: BWFlow introduces a flow-matching framework for graph generation, addressing limitations of Euclidean assumptions in current methods by modeling joint node-edge evolution with MRFs and optimal transport.


<details>
  <summary>Details</summary>
Motivation: Current graph generation methods assume Euclidean space and independent node-edge evolution, which is suboptimal for graphs' non-Euclidean structure and interconnected patterns.

Method: BWFlow models graphs as connected systems using MRFs and designs a probability path via optimal transport displacement, ensuring smooth velocities and geometry respect.

Result: BWFlow shows competitive performance in plain graph and 2D/3D molecule generation, with stable training and guaranteed sampling convergence.

Conclusion: BWFlow improves graph generation by respecting intrinsic graph geometry, offering a robust and effective framework for diverse applications.

Abstract: Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.

</details>


### [76] [Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data](https://arxiv.org/abs/2506.14036)
*Tatthapong Srikitrungruang,Sina Aghaee Dabaghan Fard,Matthew Lemon,Jaesung Lee,Yuxiao Zhou*

Main category: cs.LG

TL;DR: A novel Inverse Elasticity Physics-Informed Neural Network (IE-PINN) is introduced to robustly estimate heterogeneous elasticity parameters from noisy displacement data, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing techniques for inverse elasticity estimation are unstable, noise-sensitive, and struggle with absolute-scale Young's modulus recovery.

Method: IE-PINN uses three neural networks for displacement, strain, and elasticity fields, along with a two-phase estimation strategy and innovations like positional encoding and sine activation.

Result: IE-PINN achieves accurate absolute-scale elasticity estimations even under severe noise, outperforming existing methods.

Conclusion: IE-PINN offers significant potential for applications like clinical imaging and mechanical characterization where noise is prevalent.

Abstract: Accurately estimating spatially heterogeneous elasticity parameters, particularly Young's modulus and Poisson's ratio, from noisy displacement measurements remains significantly challenging in inverse elasticity problems. Existing inverse estimation techniques are often limited by instability, pronounced sensitivity to measurement noise, and difficulty in recovering absolute-scale Young's modulus. This work presents a novel Inverse Elasticity Physics-Informed Neural Network (IE-PINN) specifically designed to robustly reconstruct heterogeneous distributions of elasticity parameters from noisy displacement data based on linear elasticity physics. IE-PINN integrates three distinct neural network architectures dedicated to separately modeling displacement fields, strain fields, and elasticity distributions, thereby significantly enhancing stability and accuracy against measurement noise. Additionally, a two-phase estimation strategy is introduced: the first phase recovers relative spatial distributions of Young's modulus and Poisson's ratio, and the second phase calibrates the absolute scale of Young's modulus using imposed loading boundary conditions. Additional methodological innovations, including positional encoding, sine activation functions, and a sequential pretraining protocol, further enhance the model's performance and robustness. Extensive numerical experiments demonstrate that IE-PINN effectively overcomes critical limitations encountered by existing methods, delivering accurate absolute-scale elasticity estimations even under severe noise conditions. This advancement holds substantial potential for clinical imaging diagnostics and mechanical characterization, where measurements typically encounter substantial noise.

</details>


### [77] [Load Balancing Mixture of Experts with Similarity Preserving Routers](https://arxiv.org/abs/2506.14038)
*Nabil Omi,Siddhartha Sen,Ali Farhadi*

Main category: cs.LG

TL;DR: A novel load balancing loss for Sparse Mixture of Experts (MoE) models improves convergence speed and reduces redundancy by preserving token-wise relational structure.


<details>
  <summary>Details</summary>
Motivation: Current load balancing mechanisms in MoE models lead to inconsistent routing and redundant learning, limiting model capacity and performance.

Method: Introduces a new load balancing loss that encourages consistent expert choices for similar inputs during training.

Result: 36% faster convergence and lower redundancy compared to existing methods.

Conclusion: The proposed loss enhances MoE model efficiency by maintaining consistent routing and reducing redundant knowledge.

Abstract: Sparse Mixture of Experts (MoE) models offer a scalable and efficient architecture for training large neural networks by activating only a subset of parameters ("experts") for each input. A learned router computes a distribution over these experts, and assigns input tokens to a small subset. However, without auxiliary balancing mechanisms, routers often converge to using only a few experts, severely limiting model capacity and degrading performance. Most current load balancing mechanisms encourage a distribution over experts that resembles a roughly uniform distribution of experts per token. During training, this can result in inconsistent routing behavior, resulting in the model spending its capacity to learn redundant knowledge. We address this by introducing a novel load balancing loss that preserves token-wise relational structure, encouraging consistent expert choices for similar inputs during training. Our experimental results show that applying our loss to the router results in 36% faster convergence and lower redundancy compared to a popular load balancing loss.

</details>


### [78] [Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature](https://arxiv.org/abs/2506.14054)
*Joshua Fan,Haodi Xu,Feng Tao,Md Nasim,Marc Grimson,Yiqi Luo,Carla P. Gomes*

Main category: cs.LG

TL;DR: ScIReN combines neural networks with process-based models for interpretable scientific reasoning, outperforming black-box models in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Neural networks lack interpretability and adherence to scientific laws, while process-based models are rigid and poorly predictive. ScIReN bridges this gap.

Method: ScIReN uses an interpretable encoder to predict latent parameters, processed by a differentiable decoder with hard-sigmoid constraints for scientific validity.

Result: ScIReN outperforms black-box models in tasks like carbon flow and ecosystem respiration modeling, revealing hidden scientific mechanisms.

Conclusion: ScIReN offers a transparent, accurate, and interpretable framework for scientific discovery, combining neural and process-based reasoning.

Abstract: Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.

</details>


### [79] [A Regret Perspective on Online Selective Generation](https://arxiv.org/abs/2506.14067)
*Minjae Lee,Yoonjae Jung,Sangdon Park*

Main category: cs.LG

TL;DR: The paper proposes an online learning algorithm for selective generation under partial feedback to control hallucination in large language models, leveraging bandit problems for theoretical guarantees and feedback unlocking for practical efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the hallucination effect in large language models by enabling selective generation under partial feedback, a practical but underexplored setup.

Method: Reduce selective generation to multi-armed bandit problems, introduce a conversion lemma for theoretical guarantees, and use feedback unlocking to improve convergence.

Result: The algorithm controls false discovery rate (FDR) effectively while maintaining reasonable selection efficiency compared to baselines.

Conclusion: The proposed method provides a practical and theoretically grounded solution for selective generation under partial feedback, balancing hallucination control and efficiency.

Abstract: Large language generative models increasingly interact with humans, while their falsified responses raise concerns. To address this hallucination effect, selectively abstaining from answering, called selective generation, provides an effective way for generators to control the hallucination when it is unsure of their answers. However, as selective generators are interacting under non-stochastic environments and having partial feedback from users on selective generation (e.g., thumbs up or down on the selected answer), learning methods for selective generation under such practical setups are crucial but currently missing. To address these limitations, we propose an online learning algorithm for selective generation under partial feedback. In particular, as learning under partial feedback is well-studied by multi-armed bandit problems, we reduce selective generation to bandits and provide a novel conversion lemma from bandits back to selective generation to leverage any known bandit algorithms and theoretical properties. This mainly connects regret guarantees of bandits to false discovery rate (FDR) guarantees of selective generation for controlling hallucination. However, naively exploiting known bandit algorithms and their regret bounds suffers from slow convergence speed in practice due the nature of partial feedback. To overcome this, we exploit a unique structure of arms in selective generation for feedback unlocking, i.e., unlocking unknown feedback from observed feedback. We theoretically and empirically evaluate the efficacy of the proposed online selective generation algorithm under partial feedback over diverse data environment setups, resulting in controlling a desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio of non-abstaining answers, compared to baselines.

</details>


### [80] [Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification](https://arxiv.org/abs/2506.14074)
*Nathaniel Pinckney,Chenhui Deng,Chia-Tung Ho,Yun-Da Tsai,Mingjie Liu,Wenfei Zhou,Brucek Khailany,Haoxing Ren*

Main category: cs.LG

TL;DR: CVDP is a new benchmark for LLM and agent research in hardware design, featuring 783 problems across 13 tasks, highlighting gaps in current models.


<details>
  <summary>Details</summary>
Motivation: To advance research in hardware design automation by providing a realistic and challenging dataset for evaluating LLMs and agents.

Method: CVDP includes 783 problems in 13 categories, offered in non-agentic and agentic formats, evaluated using open-source tools and metrics like BLEU and LLM-based judging.

Result: State-of-the-art models achieve no more than 34% pass@1 on code generation, with agentic tasks being particularly difficult.

Conclusion: CVDP reveals significant gaps in current model capabilities, emphasizing the need for further research in hardware design automation.

Abstract: We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new dataset and infrastructure to advance LLM and agent research in hardware design and verification. CVDP includes 783 problems across 13 task categories, covering RTL generation, verification, debugging, specification alignment, and technical Q&A authored by experienced hardware engineers. Problems are offered in both non-agentic and agentic formats. The benchmark introduces more realistic and challenging contexts than prior work, with state-of-the-art models achieving no more than 34% pass@1 on code generation. Agentic tasks$\unicode{x2013}$especially those involving RTL reuse and verification$\unicode{x2013}$are particularly difficult. Evaluation uses open-source tools and model scoring infrastructure, with comprehension tasks assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in current model capabilities, underscoring the need for continued research toward robust, real-world hardware design automation.

</details>


### [81] [Multi-Scale Finetuning for Encoder-based Time Series Foundation Models](https://arxiv.org/abs/2506.14087)
*Zhongzheng Qiao,Chenghao Liu,Yiming Zhang,Ming Jin,Quang Pham,Qingsong Wen,P. N. Suganthan,Xudong Jiang,Savitha Ramasamy*

Main category: cs.LG

TL;DR: The paper proposes MSFT, a multi-scale finetuning framework for time series foundation models (TSFMs) to improve downstream task performance by addressing overfitting and leveraging multi-scale capabilities.


<details>
  <summary>Details</summary>
Motivation: Naive finetuning of TSFMs often leads to overfitting and suboptimal performance, failing to fully utilize their multi-scale forecasting capabilities.

Method: The authors introduce MSFT, a framework that explicitly models multiple scales during finetuning, tested on encoder-based TSFMs like Moirai, Moment, and Units.

Result: MSFT outperforms naive finetuning, parameter-efficient methods, and state-of-the-art deep learning approaches in experiments.

Conclusion: MSFT effectively enhances TSFMs' performance by integrating multi-scale modeling into finetuning, demonstrating its superiority over existing methods.

Abstract: Time series foundation models (TSFMs) demonstrate impressive zero-shot performance for time series forecasting. However, an important yet underexplored challenge is how to effectively finetune TSFMs on specific downstream tasks. While naive finetuning can yield performance gains, we argue that it falls short of fully leveraging TSFMs' capabilities, often resulting in overfitting and suboptimal performance. Given the diverse temporal patterns across sampling scales and the inherent multi-scale forecasting capabilities of TSFMs, we adopt a causal perspective to analyze finetuning process, through which we highlight the critical importance of explicitly modeling multiple scales and reveal the shortcomings of naive approaches. Focusing on \textit{encoder-based} TSFMs, we propose \textbf{M}ulti\textbf{\textsc{s}}cale \textbf{\textsc{f}}ine\textbf{\textsc{t}}uning (\textbf{MSFT}), a simple yet general framework that explicitly integrates multi-scale modeling into the finetuning process. Experimental results on three different backbones (\moirai, \moment\ and \units) demonstrate that TSFMs finetuned with MSFT not only outperform naive and typical parameter efficient finetuning methods but also surpass state-of-the-art deep learning methods.

</details>


### [82] [Transformers Learn Faster with Semantic Focus](https://arxiv.org/abs/2506.14095)
*Parikshit Ram,Kenneth L. Clarkson,Tim Klinger,Shashanka Ubaru,Alexander G. Gray*

Main category: cs.LG

TL;DR: Sparse transformers with input-dependent attention converge faster and generalize better than standard or input-agnostic sparse attention, due to improved semantic focus. Theoretical analysis links softmax stability and Lipschitz properties to these benefits.


<details>
  <summary>Details</summary>
Motivation: To understand how sparse attention impacts learnability and generalization in transformers, beyond just computational efficiency.

Method: Empirical evaluation of various attention mechanisms (standard, input-agnostic sparse, input-dependent sparse) and theoretical analysis of softmax stability and Lipschitz properties.

Result: Input-dependent sparse attention outperforms others in convergence and generalization, while input-agnostic sparse attention shows no benefits. Theoretical conditions for these outcomes are validated.

Conclusion: Input-dependent sparse attention enhances learning by focusing semantic context, while input-agnostic sparsity offers no advantages. Theoretical insights align with empirical findings.

Abstract: Various forms of sparse attention have been explored to mitigate the quadratic computational and memory cost of the attention mechanism in transformers. We study sparse transformers not through a lens of efficiency but rather in terms of learnability and generalization. Empirically studying a range of attention mechanisms, we find that input-dependent sparse attention models appear to converge faster and generalize better than standard attention models, while input-agnostic sparse attention models show no such benefits -- a phenomenon that is robust across architectural and optimization hyperparameter choices. This can be interpreted as demonstrating that concentrating a model's "semantic focus" with respect to the tokens currently being considered (in the form of input-dependent sparse attention) accelerates learning. We develop a theoretical characterization of the conditions that explain this behavior. We establish a connection between the stability of the standard softmax and the loss function's Lipschitz properties, then show how sparsity affects the stability of the softmax and the subsequent convergence and generalization guarantees resulting from the attention mechanism. This allows us to theoretically establish that input-agnostic sparse attention does not provide any benefits. We also characterize conditions when semantic focus (input-dependent sparse attention) can provide improved guarantees, and we validate that these conditions are in fact met in our empirical evaluations.

</details>


### [83] [Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks](https://arxiv.org/abs/2506.14098)
*Ziyuan Tang,Jie Chen*

Main category: cs.LG

TL;DR: The paper proposes a graph foundation model using Transformer architecture, addressing challenges in encoding diverse graphs by representing nodes as random walks and introducing a context prediction loss.


<details>
  <summary>Details</summary>
Motivation: To explore if foundation models, like GPT for natural language, can be adapted for graphs, given their potential for diverse applications.

Method: Adapts Transformer architecture for graphs, representing nodes as random walks and using a novel context prediction loss for training.

Result: Demonstrates the model's ability to pre-train and adapt to downstream tasks, highlighting its potential as a foundation for graph-structured data.

Conclusion: The proposed model shows promise as a foundation for graph processing, leveraging Transformer architecture and random walk representations.

Abstract: A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.

</details>


### [84] [SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting](https://arxiv.org/abs/2506.14113)
*Yitian Zhang,Liheng Ma,Antonios Valkanas,Boris N. Oreshkin,Mark Coates*

Main category: cs.LG

TL;DR: The paper connects Koopman operator theory with linear RNNs, proposing SKOLR, a model combining spectral decomposition and MLPs for efficient forecasting.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between infinite-dimensional Koopman operators and tractable finite-dimensional approximations for nonlinear dynamical systems.

Method: Proposes SKOLR, which uses a learnable spectral decomposition and MLPs as measurement functions, implemented via a parallel linear RNN stack.

Result: SKOLR achieves exceptional performance on forecasting benchmarks and dynamical systems.

Conclusion: The structured Koopman-RNN framework (SKOLR) offers an efficient and high-performing solution for nonlinear system analysis and forecasting.

Abstract: Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.

</details>


### [85] [Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization](https://arxiv.org/abs/2506.14114)
*Khushnood Abbas,Ruizhe Hou,Zhou Wengang,Dong Shi,Niu Ling,Satyaki Nan,Alireza Abbasi*

Main category: cs.LG

TL;DR: A large-scale study evaluates GNN architectures and loss functions, revealing hybrid losses and GIN architecture excel, while MPNN lags.


<details>
  <summary>Details</summary>
Motivation: To understand how GNN models and loss functions perform together across tasks, as prior work studied them separately.

Method: Evaluated seven GNN architectures with 30 loss functions in inductive and transductive settings across three datasets using 21 metrics.

Result: Hybrid losses outperform single ones; GIN with Cross-Entropy excels; GAT shows specialized strengths; MPNN underperforms.

Conclusion: Multi-objective optimization (hybrid losses) and GIN architecture are recommended for robust GNN performance.

Abstract: Graph Neural Networks (GNNs) became useful for learning on non-Euclidean data. However, their best performance depends on choosing the right model architecture and the training objective, also called the loss function. Researchers have studied these parts separately, but a large-scale evaluation has not looked at how GNN models and many loss functions work together across different tasks. To fix this, we ran a thorough study - it included seven well-known GNN architectures. We also used a large group of 30 single plus mixed loss functions. The study looked at both inductive and transductive settings. Our evaluation spanned three distinct real-world datasets, assessing performance in both inductive and transductive settings using 21 comprehensive evaluation metrics. From these extensive results (detailed in supplementary information 1 \& 2), we meticulously analyzed the top ten model-loss combinations for each metric based on their average rank. Our findings reveal that, especially for the inductive case: 1) Hybrid loss functions generally yield superior and more robust performance compared to single loss functions, indicating the benefit of multi-objective optimization. 2) The GIN architecture always showed the highest-level average performance, especially with Cross-Entropy loss. 3) Although some combinations had overall lower average ranks, models such as GAT, particularly with certain hybrid losses, demonstrated incredible specialized strengths, maximizing the most top-1 results among the individual metrics, emphasizing subtle strengths for particular task demands. 4) On the other hand, the MPNN architecture typically lagged behind the scenarios it was tested against.

</details>


### [86] [CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs](https://arxiv.org/abs/2506.14122)
*Tianming Zhang,Renbo Zhang,Zhengyi Yang,Yunjun Gao,Bin Cao,Jing Fan*

Main category: cs.LG

TL;DR: The paper proposes CLGNN, a scalable and inductive contrastive learning-based GNN for accurate and efficient Temporal Betweenness Centrality (TBC) prediction, addressing imbalance and temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for TBC prediction either fail to handle severe class imbalance or ignore temporal dependencies, leading to inaccurate predictions.

Method: CLGNN uses dual aggregation (mean and edge-to-node multi-head attention) with temporal path count and time encodings, and introduces KContrastNet for class separation and ValueNet for TBC estimation.

Result: CLGNN achieves significant speedups (663.7×) and outperforms static and temporal GNN baselines in accuracy (lower MAE, higher Spearman correlation).

Conclusion: CLGNN effectively addresses TBC prediction challenges, offering scalability, accuracy, and adaptability to diverse temporal semantics.

Abstract: Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.

</details>


### [87] [Less is More: Undertraining Experts Improves Model Upcycling](https://arxiv.org/abs/2506.14126)
*Stefan Horoi,Guy Wolf,Eugene Belilovsky,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: The paper challenges the assumption that improvements in fine-tuning propagate downstream in model upcycling, showing that long fine-tuning degrades merging performance. It proposes early stopping as a solution.


<details>
  <summary>Details</summary>
Motivation: To examine how expert fine-tuning affects model upcycling and challenge the assumption that improvements at one pipeline stage propagate downstream.

Method: Analyzes the impact of long fine-tuning on merging performance for fully fine-tuned and LoRA-adapted models, and upcycling LoRA adapters into MoE layers. Identifies memorization of difficult examples as the cause of degradation.

Result: Long fine-tuning degrades merging performance and downstream results. Memorization of difficult examples dominates late fine-tuning and is forgotten during merging.

Conclusion: A task-dependent aggressive early stopping strategy can significantly improve upcycling performance.

Abstract: Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.

</details>


### [88] [Leveraging Predictive Equivalence in Decision Trees](https://arxiv.org/abs/2506.14143)
*Hayden McTavish,Zachery Boner,Jon Donnelly,Margo Seltzer,Cynthia Rudin*

Main category: cs.LG

TL;DR: The paper addresses the issue of predictive equivalence in decision trees, where different trees can have the same decision boundary but differ in evaluation. It proposes a boolean logical representation to resolve this and demonstrates its benefits in robustness to missing values, variable importance, and prediction cost optimization.


<details>
  <summary>Details</summary>
Motivation: Decision trees are interpretable but suffer from predictive equivalence, where multiple trees can represent the same decision boundary, complicating model selection and interpretation.

Method: The authors introduce a boolean logical representation of decision trees to eliminate predictive equivalence, ensuring faithfulness to the decision boundary.

Result: The proposed representation improves robustness to missing feature values, clarifies variable importance, and enables cost-efficient prediction optimization.

Conclusion: The boolean logical representation resolves predictive equivalence, enhancing decision tree interpretability and utility in practical applications.

Abstract: Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.

</details>


### [89] [Common Benchmarks Undervalue the Generalization Power of Programmatic Policies](https://arxiv.org/abs/2506.14162)
*Amirhossein Rajabpour,Kiarash Aghakasiri,Sandra Zilles,Levi H. S. Lelis*

Main category: cs.LG

TL;DR: Neural policies can generalize as well as programmatic policies on OOD problems with simple training adjustments, challenging the common belief that programmatic policies inherently generalize better.


<details>
  <summary>Details</summary>
Motivation: To address the undervaluation of neural policies' generalization capabilities in OOD benchmarks and highlight the impact of training modifications.

Method: Analyzed experiments from four papers, tested simpler neural architectures, sparse observations, and safer reward functions.

Result: Neural policies achieved comparable OOD generalization to programmatic policies with adjusted training methods.

Conclusion: Benchmarks should better highlight generalization challenges and align with programmatic constructs to fairly evaluate neural policies.

Abstract: Algorithms for learning programmatic representations for sequential decision-making problems are often evaluated on out-of-distribution (OOD) problems, with the common conclusion that programmatic policies generalize better than neural policies on OOD problems. In this position paper, we argue that commonly used benchmarks undervalue the generalization capabilities of programmatic representations. We analyze the experiments of four papers from the literature and show that neural policies, which were shown not to generalize, can generalize as effectively as programmatic policies on OOD problems. This is achieved with simple changes in the neural policies training pipeline. Namely, we show that simpler neural architectures with the same type of sparse observation used with programmatic policies can help attain OOD generalization. Another modification we have shown to be effective is the use of reward functions that allow for safer policies (e.g., agents that drive slowly can generalize better). Also, we argue for creating benchmark problems highlighting concepts needed for OOD generalization that may challenge neural policies but align with programmatic representations, such as tasks requiring algorithmic constructs like stacks.

</details>


### [90] [Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/abs/2506.14167)
*Prithvi Raj*

Main category: cs.LG

TL;DR: The paper adapts the Kolmogorov-Arnold Representation Theorem for generative modeling, using inverse transform sampling to create an interpretable, efficient model with energy-based priors.


<details>
  <summary>Details</summary>
Motivation: To bridge classical representation theorems with modern probabilistic modeling, improving interpretability, design ease, and efficiency in generative models.

Method: Uses a Kolmogorov-Arnold Network generator with energy-based priors, trained via Maximum Likelihood, and incorporates scalable extensions like mixture distributions and Langevin Monte Carlo.

Result: Achieves fast inference, improved learning efficiency, and better sample quality, with recoverable and visualizable priors.

Conclusion: The approach successfully balances training stability, inference speed, and generation quality, connecting classical and modern techniques.

Abstract: We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling by reinterpreting its inner functions as a Markov Kernel between probability spaces via inverse transform sampling. We present a generative model that is interpretable, easy to design, and efficient. Our approach couples a Kolmogorov-Arnold Network generator with independent energy-based priors, trained via Maximum Likelihood. Inverse sampling enables fast inference, while prior knowledge can be incorporated before training to better align priors with posteriors, thereby improving learning efficiency and sample quality. The learned prior is also recoverable and visualizable post-training, offering an empirical Bayes perspective. To address inflexibility and mitigate prior-posterior mismatch, we introduce scalable extensions based on mixture distributions and Langevin Monte Carlo methods, admitting a trade-off between flexibility and training efficiency. Our contributions connect classical representation theorems with modern probabilistic modeling, while balancing training stability, inference speed, and the quality and diversity of generations.

</details>


### [91] [A Variational Information Theoretic Approach to Out-of-Distribution Detection](https://arxiv.org/abs/2506.14194)
*Sudeepta Mondal,Zhuolin Jiang,Ganesh Sundaramoorthi*

Main category: cs.LG

TL;DR: A theory for constructing OOD detection features using an information-theoretic loss functional with KL divergence and Information Bottleneck, leading to improved performance and explainability.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for creating explainable OOD detection features that outperform existing methods.

Method: Introduces a novel loss functional combining KL divergence for ID/OOD separation and Information Bottleneck for feature compression. Uses variational optimization to derive OOD features.

Result: The theory predicts a new shaping function that outperforms existing ones on OOD benchmarks.

Conclusion: Provides a versatile framework for constructing explainable OOD features, demonstrating superior performance.

Abstract: We present a theory for the construction of out-of-distribution (OOD) detection features for neural networks. We introduce random features for OOD through a novel information-theoretic loss functional consisting of two terms, the first based on the KL divergence separates resulting in-distribution (ID) and OOD feature distributions and the second term is the Information Bottleneck, which favors compressed features that retain the OOD information. We formulate a variational procedure to optimize the loss and obtain OOD features. Based on assumptions on OOD distributions, one can recover properties of existing OOD features, i.e., shaping functions. Furthermore, we show that our theory can predict a new shaping function that out-performs existing ones on OOD benchmarks. Our theory provides a general framework for constructing a variety of new features with clear explainability.

</details>


### [92] [DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion](https://arxiv.org/abs/2506.14202)
*Makoto Shing,Takuya Akiba*

Main category: cs.LG

TL;DR: DiffusionBlocks is a memory-efficient training framework for large neural networks by partitioning them into independently trainable blocks using a diffusion process approach.


<details>
  <summary>Details</summary>
Motivation: To address the memory bottlenecks in training large neural networks with backpropagation, making state-of-the-art AI research more accessible.

Method: Partitions the network into blocks, treats them as denoising operations in a diffusion process, and optimizes noise level assignments for efficiency.

Result: Achieves significant memory reduction proportional to block count while maintaining competitive performance in generative tasks.

Conclusion: DiffusionBlocks offers a scalable solution for training large networks with limited resources, democratizing access to advanced AI research.

Abstract: Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.

</details>


### [93] [TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift](https://arxiv.org/abs/2506.14217)
*Dipesh Tharu Mahato,Rohan Poudel,Pramod Dhungana*

Main category: cs.LG

TL;DR: TriGuard is a safety evaluation framework combining robustness verification, attribution entropy, and Attribution Drift Score to assess neural network reliability under adversarial and distributional shifts.


<details>
  <summary>Details</summary>
Motivation: Ensuring neural network reliability under adversarial and distributional shifts is challenging, requiring methods beyond accuracy metrics.

Method: TriGuard integrates formal robustness verification, attribution entropy for saliency concentration, and Attribution Drift Score for explanation stability.

Result: TriGuard identifies mismatches between accuracy and interpretability, showing verified models can have unstable reasoning. Entropy-regularized training reduces drift without performance loss.

Conclusion: TriGuard advances robust, interpretable model evaluation by uncovering subtle fragilities and improving explanation stability.

Abstract: Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.

</details>


### [94] [Can Large Language Models Improve Spectral Graph Neural Networks?](https://arxiv.org/abs/2506.14220)
*Kangkang Lu,Yanhua Yu,Zhiyong Huang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: SGNNs struggle with suboptimal filters under label scarcity. This paper proposes using LLMs to estimate graph homophily and guide SGNN filter design, improving performance across diverse graphs.


<details>
  <summary>Details</summary>
Motivation: SGNNs degrade under label-scarce conditions, while LLMs show potential in GNNs. The paper explores if LLMs can enhance SGNNs by estimating homophily for better filter design.

Method: A lightweight pipeline where LLMs generate homophily-aware priors to guide polynomial spectral filter design in SGNNs.

Result: The LLM-driven SGNN framework outperforms baselines in homophilic and heterophilic settings with minimal overhead.

Conclusion: LLMs can effectively enhance SGNNs by providing homophily estimates, improving adaptability and performance.

Abstract: Spectral Graph Neural Networks (SGNNs) have attracted significant attention due to their ability to approximate arbitrary filters. They typically rely on supervision from downstream tasks to adaptively learn appropriate filters. However, under label-scarce conditions, SGNNs may learn suboptimal filters, leading to degraded performance. Meanwhile, the remarkable success of Large Language Models (LLMs) has inspired growing interest in exploring their potential within the GNN domain. This naturally raises an important question: \textit{Can LLMs help overcome the limitations of SGNNs and enhance their performance?} In this paper, we propose a novel approach that leverages LLMs to estimate the homophily of a given graph. The estimated homophily is then used to adaptively guide the design of polynomial spectral filters, thereby improving the expressiveness and adaptability of SGNNs across diverse graph structures. Specifically, we introduce a lightweight pipeline in which the LLM generates homophily-aware priors, which are injected into the filter coefficients to better align with the underlying graph topology. Extensive experiments on benchmark datasets demonstrate that our LLM-driven SGNN framework consistently outperforms existing baselines under both homophilic and heterophilic settings, with minimal computational and monetary overhead.

</details>


### [95] [Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning](https://arxiv.org/abs/2506.14251)
*Xiyu Zhao,Qimei Cui,Weicai Li,Wei Ni,Ekram Hossain,Quan Z. Sheng,Xiaofeng Tao,Ping Zhang*

Main category: cs.LG

TL;DR: DP-Ditto extends Ditto for PFL with differential privacy, balancing privacy, convergence, and fairness, outperforming other PFL models in accuracy and fairness.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in PFL and their impact on convergence and fairness, proposing a DP-protected solution.

Method: Extends Ditto with DP, analyzing trade-offs and deriving optimal global aggregations under privacy constraints.

Result: DP-Ditto outperforms state-of-the-art PFL models by 32.71% in fairness and 9.66% in accuracy.

Conclusion: DP-Ditto effectively balances privacy, convergence, and fairness, proving superior to existing PFL methods.

Abstract: Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a balance between personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). While FL is unaffected by personalized model training, in Ditto, PL depends on the outcome of the FL. However, the clients' concern about their privacy and consequent perturbation of their local models can affect the convergence and (performance) fairness of PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension of Ditto under the protection of differential privacy (DP), and analyzes the trade-off among its privacy guarantee, model convergence, and performance distribution fairness. We also analyze the convergence upper bound of the personalized models under DP-Ditto and derive the optimal number of global aggregations given a privacy budget. Further, we analyze the performance fairness of the personalized models, and reveal the feasibility of optimizing DP-Ditto jointly for convergence and fairness. Experiments validate our analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by over 32.71% in fairness and 9.66% in accuracy.

</details>


### [96] [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
*Rohan Gupta,Erik Jenner*

Main category: cs.LG

TL;DR: The paper investigates whether LLMs can evade latent-space monitors designed to detect undesirable behaviors. It introduces RL-Obfuscation, a method to train LLMs to bypass these monitors, and evaluates their vulnerability.


<details>
  <summary>Details</summary>
Motivation: To understand if latent-space monitors can be circumvented by LLMs, addressing a critical gap in their reliability.

Method: RL-Obfuscation is used to finetune LLMs via reinforcement learning to evade monitors while maintaining coherent outputs. Models from 7B to 14B parameters are tested.

Result: Token-level monitors are highly vulnerable, while holistic ones (e.g., max-pooling or attention-based) remain robust. Adversarial policies generalize to unseen monitors of the same type.

Conclusion: Latent-space monitors vary in robustness; holistic approaches are more resilient. LLMs can learn to repurpose tokens internally to evade detection.

Abstract: Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can LLMs learn to evade such monitors? To study this, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by RL bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.

</details>


### [97] [Knowledge Adaptation as Posterior Correction](https://arxiv.org/abs/2506.14262)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: The paper explores how machines can achieve quick adaptation like humans by correcting approximate posteriors, leading to faster learning.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in AI adaptation methods, machines still lack the natural adaptivity of humans and animals. The paper aims to uncover mechanisms for faster machine adaptation.

Method: The study uses a dual-perspective of the Bayesian Learning Rule to analyze adaptation as posterior-correction, where accurate posteriors minimize corrections and speed up adaptation.

Result: The findings show that posterior-correction serves as a natural mechanism for quick adaptation, demonstrated through various examples.

Conclusion: Posterior-correction is a promising approach for enabling machines to adapt more naturally and swiftly, akin to human learning.

Abstract: Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.

</details>


### [98] [Towards Robust Learning to Optimize with Theoretical Guarantees](https://arxiv.org/abs/2506.14263)
*Qingyu Song,Wei Lin,Juncheng Wang,Hong Xu*

Main category: cs.LG

TL;DR: The paper addresses the lack of theoretical guarantees for Learning to Optimize (L2O) models in out-of-distribution (OOD) scenarios, providing proofs and a robust model with improved convergence rates.


<details>
  <summary>Details</summary>
Motivation: Existing L2O works lack theoretical performance and robustness proofs in OOD scenarios, despite their success in real-world applications.

Method: The authors propose a methodology to align OOD problems to InD problems, introduce a gradient-only feature construction, and a gradient-based history modeling method.

Result: The proposed L2O model outperforms state-of-the-art baselines, achieving up to 10x convergence speedup in both InD and OOD scenarios.

Conclusion: The paper provides theoretical and empirical validation for robust L2O models, demonstrating significant performance improvements.

Abstract: Learning to optimize (L2O) is an emerging technique to solve mathematical optimization problems with learning-based methods. Although with great success in many real-world scenarios such as wireless communications, computer networks, and electronic design, existing L2O works lack theoretical demonstration of their performance and robustness in out-of-distribution (OOD) scenarios. We address this gap by providing comprehensive proofs. First, we prove a sufficient condition for a robust L2O model with homogeneous convergence rates over all In-Distribution (InD) instances. We assume an L2O model achieves robustness for an InD scenario. Based on our proposed methodology of aligning OOD problems to InD problems, we also demonstrate that the L2O model's convergence rate in OOD scenarios will deteriorate by an equation of the L2O model's input features. Moreover, we propose an L2O model with a concise gradient-only feature construction and a novel gradient-based history modeling method. Numerical simulation demonstrates that our proposed model outperforms the state-of-the-art baseline in both InD and OOD scenarios and achieves up to 10 $\times$ convergence speedup. The code of our method can be found from https://github.com/NetX-lab/GoMathL2O-Official.

</details>


### [99] [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
*Bai Cong,Nico Daheim,Yuesong Shen,Rio Yokota,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: cs.LG

TL;DR: IVON, a variational algorithm, improves LoRA finetuning by enhancing accuracy and calibration while maintaining computational efficiency, outperforming AdamW and other Bayesian methods.


<details>
  <summary>Details</summary>
Motivation: Bayesian methods for LoRA finetuning improve calibration but often have marginal or negative effects on accuracy and increase computational overhead. IVON addresses these limitations.

Method: Uses IVON, a variational algorithm, with a simple posterior pruning technique for LoRA finetuning. Tested on billion-scale LLMs like Llama and Qwen.

Result: Improved accuracy by 1.3% and reduced ECE by 5.4% compared to AdamW, outperforming other Bayesian methods like Laplace-LoRA and BLoB.

Conclusion: IVON effectively enhances LoRA finetuning, offering better performance and efficiency than existing methods.

Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.

</details>


### [100] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/abs/2506.14291)
*Ben Finkelshtein,İsmail İlkan Ceylan,Michael Bronstein,Ron Levie*

Main category: cs.LG

TL;DR: The paper proposes a recipe for designing graph foundation models for node-level tasks, emphasizing symmetries like label permutation-equivariance and feature permutation-invariance. It validates the approach with strong zero-shot performance on 29 datasets.


<details>
  <summary>Details</summary>
Motivation: Current graph ML architectures lack broader applicability due to task-specific designs. The goal is to build graph foundation models that generalize across arbitrary graphs and features.

Method: The study systematically investigates required symmetries (node, label, and feature permutations) and characterizes linear transformations respecting these. The resulting network is proven to be a universal approximator.

Result: The proposed model shows strong zero-shot performance and consistent improvement with more training graphs, validated on 29 real-world datasets.

Conclusion: The work provides a principled approach to designing graph foundation models, demonstrating their potential for generalization and scalability.

Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.

</details>


### [101] [Fair for a few: Improving Fairness in Doubly Imbalanced Datasets](https://arxiv.org/abs/2506.14306)
*Ata Yalcin,Asli Umay Ozturk,Yigit Sever,Viktoria Pauw,Stephan Hachinger,Ismail Hakki Toroslu,Pinar Karagoz*

Main category: cs.LG

TL;DR: The paper addresses fairness in doubly imbalanced datasets, where data is imbalanced for both labels and sensitive attributes, proposing a multi-criteria solution for debiasing.


<details>
  <summary>Details</summary>
Motivation: Fairness in ML/AI decision-making is crucial, but existing debiasing methods struggle with doubly imbalanced datasets.

Method: Exploratory analysis of debiasing limitations, followed by a multi-criteria approach for optimal sampling and distribution.

Result: The proposed method aims to balance fairness and classification accuracy in doubly imbalanced datasets.

Conclusion: The paper highlights the challenge of fairness in doubly imbalanced data and offers a practical solution.

Abstract: Fairness has been identified as an important aspect of Machine Learning and Artificial Intelligence solutions for decision making. Recent literature offers a variety of approaches for debiasing, however many of them fall short when the data collection is imbalanced. In this paper, we focus on a particular case, fairness in doubly imbalanced datasets, such that the data collection is imbalanced both for the label and the groups in the sensitive attribute. Firstly, we present an exploratory analysis to illustrate limitations in debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution is proposed for finding the most suitable sampling and distribution for label and sensitive attribute, in terms of fairness and classification accuracy

</details>


### [102] [IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards](https://arxiv.org/abs/2506.14375)
*Muhammad Hamza Yousuf,Jason Li,Sahar Vahdati,Raphael Theilen,Jakob Wittenstein,Jens Lehmann*

Main category: cs.LG

TL;DR: The paper proposes optimizations for offline RL in MV control, addressing hybrid action spaces and introducing a clinically grounded reward function to improve patient safety and individualized care.


<details>
  <summary>Details</summary>
Motivation: Optimizing MV settings is complex due to patient variability, and current RL methods struggle with hybrid action spaces and discretization issues.

Method: The paper adapts SOTA offline RL algorithms (IQL and EDAC) for hybrid action spaces and introduces a reward function based on ventilator-free days and physiological targets.

Result: The proposed approach enhances patient safety and enables individualized lung support, advancing data-driven critical care solutions.

Conclusion: AI-assisted MV optimization shows promise for improving ICU care by addressing current RL limitations and aligning with clinical goals.

Abstract: Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.

</details>


### [103] [ResNets Are Deeper Than You Think](https://arxiv.org/abs/2506.14386)
*Christian H. X. Ali Mehmeti-Göpel,Michael Wand*

Main category: cs.LG

TL;DR: Residual connections in neural networks offer performance advantages beyond optimization, suggesting a deeper inductive bias aligned with natural data.


<details>
  <summary>Details</summary>
Motivation: To explain why residual networks outperform feedforward networks, not just due to better trainability but because they inhabit a different function space.

Method: A controlled post-training comparison isolating generalization performance from trainability, comparing variable-depth (ResNet-like) and fixed-depth networks.

Result: Variable-depth architectures consistently outperform fixed-depth networks, even when optimization is unlikely to influence outcomes.

Conclusion: Residual connections provide inherent performance benefits, indicating a deeper inductive bias aligned with natural data structure.

Abstract: Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.

</details>


### [104] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister,Erik Bochinski,Volker Eiselein,Elvira Fleig*

Main category: cs.LG

TL;DR: The paper extends self-explainable Prototypical Variational models with autoencoder-based OOD detection, introducing a novel restriction loss for a compact ID region and demonstrating superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance trust and reliability in Deep Machine Learning Models for safety-relevant applications by improving explainability and OOD detection.

Method: Uses a Variational Autoencoder to learn a latent space for distance-based classification, likelihood estimation, and reconstruction, with a Gaussian mixture for ID region and a novel restriction loss.

Result: Outperforms previous methods on OOD detection benchmarks and a real-world railway dataset.

Conclusion: The approach effectively combines explainability and OOD detection, proving useful for safety-critical applications.

Abstract: Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.

</details>


### [105] [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/abs/2506.14391)
*Yaqiao Zhu,Hongkai Wen,Geyong Min,Man Luo*

Main category: cs.LG

TL;DR: HiLight is a hierarchical RL framework for large-scale traffic signal control, combining global coordination with local execution through adversarial training.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for traffic signal control struggle with scalability and global coordination, limiting network-level efficiency.

Method: HiLight uses a high-level Meta-Policy (Transformer-LSTM) to partition networks and set sub-goals, and a low-level Sub-Policy for intersection control, enhanced by adversarial training.

Result: HiLight outperforms in large-scale scenarios and remains competitive in standard benchmarks, handling diverse traffic conditions effectively.

Conclusion: HiLight addresses scalability and coordination challenges in traffic signal control, demonstrating superior performance in complex environments.

Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.

</details>


### [106] [One Size Fits None: Rethinking Fairness in Medical AI](https://arxiv.org/abs/2506.14400)
*Roland Roller,Michael Hahn,Ajay Madhavan Ravichandran,Bilgin Osmanodja,Florian Oetke,Zeineb Sassi,Aljoscha Burchardt,Klaus Netter,Klemens Budde,Anne Herrmann,Tobias Strapatsas,Peter Dabrock,Sebastian Möller*

Main category: cs.LG

TL;DR: The paper highlights the importance of subgroup-level evaluation in ML models for clinical decision-making to address fairness and performance disparities.


<details>
  <summary>Details</summary>
Motivation: Real-world medical datasets are noisy and imbalanced, leading to performance disparities across patient subgroups, which raises fairness concerns.

Method: Analyzed several medical prediction tasks to evaluate model performance variations across patient subgroups.

Result: Demonstrated that while ML models may perform well overall, subgroup-level disparities exist, requiring targeted evaluation.

Conclusion: Subgroup-sensitive development and deployment of ML models are crucial for fairness and transparency in clinical applications.

Abstract: Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified-allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.

</details>


### [107] [Adaptive Reinforcement Learning for Unobservable Random Delays](https://arxiv.org/abs/2506.14411)
*John Wikman,Alexandre Proutiere,David Broman*

Main category: cs.LG

TL;DR: The paper introduces the 'interaction layer' framework and ACDA algorithm to handle unobservable, time-varying delays in RL, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world RL environments often have unpredictable delays, breaking the MDP assumption. Current methods assume fixed delays, which is inefficient.

Method: Proposes an interaction layer framework and ACDA algorithm to adaptively manage delays by generating future action matrices.

Result: ACDA significantly outperforms state-of-the-art methods in locomotion benchmarks.

Conclusion: The framework and algorithm effectively address delay uncertainty in RL, improving performance in dynamic environments.

Abstract: In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.

</details>


### [108] [Unsupervised Skill Discovery through Skill Regions Differentiation](https://arxiv.org/abs/2506.14420)
*Ting Xiao,Jiakun Zheng,Rushuai Yang,Kang Xu,Qiaosheng Zhang,Peng Liu,Chenjia Bai*

Main category: cs.LG

TL;DR: A novel unsupervised RL method maximizes inter-skill state diversity and uses a conditional autoencoder for skill discovery, outperforming prior methods in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of entropy-based exploration and MI-driven methods in large-scale state spaces by promoting inter-skill diversity and intra-skill exploration.

Method: Proposes a skill discovery objective for state diversity and a conditional autoencoder for state-density estimation, with intrinsic rewards for exploration.

Result: Learns meaningful skills and achieves superior performance in downstream tasks, validated in challenging state and image-based environments.

Conclusion: The method effectively balances inter-skill diversity and intra-skill exploration, advancing unsupervised RL for complex tasks.

Abstract: Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors that can accelerate the learning of downstream tasks. Previous methods typically focus on entropy-based exploration or empowerment-driven skill learning. However, entropy-based exploration struggles in large-scale state spaces (e.g., images), and empowerment-based methods with Mutual Information (MI) estimations have limitations in state exploration. To address these challenges, we propose a novel skill discovery objective that maximizes the deviation of the state density of one skill from the explored regions of other skills, encouraging inter-skill state diversity similar to the initial MI objective. For state-density estimation, we construct a novel conditional autoencoder with soft modularization for different skill policies in high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we formulate an intrinsic reward based on the learned autoencoder that resembles count-based exploration in a compact latent space. Through extensive experiments in challenging state and image-based tasks, we find our method learns meaningful skills and achieves superior performance in various downstream tasks.

</details>


### [109] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/abs/2506.14436)
*Shen Yuan,Yin Zheng,Taifeng Wang,Binbin Liu,Hongteng Xu*

Main category: cs.LG

TL;DR: The paper introduces MoORE, a method for multi-task adaptation of foundation models that avoids task conflict and oblivion by using SVD and learnable routers to create orthogonal experts.


<details>
  <summary>Details</summary>
Motivation: To address issues of task conflict and oblivion in multi-task adaptation of large-scale foundation models.

Method: Applies SVD to weight matrices, introduces learnable routers for task-specific adjustments, and ensures orthogonality of experts (MoORE).

Result: MoORE outperforms existing methods in multi-task scenarios, demonstrating resistance to conflict and oblivion.

Conclusion: MoORE is a superior method for multi-task adaptation, maintaining original task performance while adapting to new tasks.

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers from task conflict and oblivion. To mitigate such issues, we propose a novel ''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant multi-task adaptation method. Given a weight matrix of a pre-trained model, our method applies SVD to it and introduces a learnable router to adjust its singular values based on tasks and samples. Accordingly, the weight matrix becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert corresponds to the outer product of a left singular vector and the corresponding right one. We can improve the model capacity by imposing a learnable orthogonal transform on the right singular vectors. Unlike low-rank adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts' orthogonality and maintains the column space of the original weight matrix. These two properties make the adapted model resistant to the conflicts among the new tasks and the oblivion of its original tasks, respectively. Experiments on various datasets demonstrate that MoORE outperforms existing multi-task adaptation methods consistently, showing its superiority in terms of conflict- and oblivion-resistance. The code of the experiments is available at https://github.com/DaShenZi721/MoORE.

</details>


### [110] [sHGCN: Simplified hyperbolic graph convolutional neural networks](https://arxiv.org/abs/2506.14438)
*Pol Arévalo,Alexis Molina,Álvaro Ciudad*

Main category: cs.LG

TL;DR: Simplified hyperbolic operations improve computational efficiency and performance in hyperbolic neural networks.


<details>
  <summary>Details</summary>
Motivation: Hyperbolic neural networks face challenges in computational efficiency and precision, limiting their broader application.

Method: Simplified key operations within hyperbolic neural networks.

Result: Achieved notable improvements in runtime and predictive accuracy.

Conclusion: Streamlined hyperbolic operations make hyperbolic neural networks more viable for diverse applications.

Abstract: Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.

</details>


### [111] [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://arxiv.org/abs/2506.14439)
*Rikiya Takehi,Masahiro Asami,Kosuke Kawakami,Yuta Saito*

Main category: cs.LG

TL;DR: The paper introduces HyPeR, a method for off-policy learning in contextual bandits with partially observed rewards, leveraging secondary rewards to improve policy learning.


<details>
  <summary>Details</summary>
Motivation: Partial observation of target rewards (e.g., delayed conversions, censored data) degrades OPL effectiveness, while secondary rewards (e.g., clicks, dwell time) are more abundant but may misalign with the target.

Method: Proposes HyPeR, which combines partially observed target rewards with secondary rewards to optimize policy learning. Also explores optimizing both rewards for better target performance.

Result: HyPeR outperforms existing methods in synthetic and real-world data, showing improved OPL effectiveness.

Conclusion: HyPeR effectively leverages secondary rewards to enhance OPL in scenarios with partial reward observation, even when optimizing for both rewards benefits the target.

Abstract: Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially-observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios.

</details>


### [112] [Detecting immune cells with label-free two-photon autofluorescence and deep learning](https://arxiv.org/abs/2506.14449)
*Lucas Kreiss,Amey Chaware,Maryam Roohian,Sarah Lemire,Oana-Maria Thoma,Birgitta Carlé,Maximilian Waldner,Sebastian Schürmann,Oliver Friedrich,Roarke Horstmeyer*

Main category: cs.LG

TL;DR: A deep learning model (CNN) was trained on label-free multiphoton microscopy images to classify immune cell types, achieving reliable results and demonstrating potential for in vivo endomicroscopy.


<details>
  <summary>Details</summary>
Motivation: Label-free imaging avoids staining, making it ideal for in vivo use. Deep learning can enhance specificity in label-free multiphoton microscopy (MPM), which is rarely explored.

Method: A convolutional neural network (CNN) was trained on label-free MPM images of immune cells (5,075 for binary and 3,424 for multi-class classification). A squeezeNet architecture was used.

Result: The model achieved reliable classification (0.89 ROC-AUC for binary, 0.689 F1 score for six-class). Perturbation tests confirmed robustness.

Conclusion: DL models can computationally improve label-free MPM specificity, enabling direct immune cell detection in unstained images, with significant potential for in vivo applications.

Abstract: Label-free imaging has gained broad interest because of its potential to omit elaborate staining procedures which is especially relevant for in vivo use. Label-free multiphoton microscopy (MPM), for instance, exploits two-photon excitation of natural autofluorescence (AF) from native, metabolic proteins, making it ideal for in vivo endomicroscopy. Deep learning (DL) models have been widely used in other optical imaging technologies to predict specific target annotations and thereby digitally augment the specificity of these label-free images. However, this computational specificity has only rarely been implemented for MPM. In this work, we used a data set of label-free MPM images from a series of different immune cell types (5,075 individual cells for binary classification in mixed samples and 3,424 cells for a multi-class classification task) and trained a convolutional neural network (CNN) to classify cell types based on this label-free AF as input. A low-complexity squeezeNet architecture was able to achieve reliable immune cell classification results (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples; 0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class classification in isolated samples). Perturbation tests confirmed that the model is not confused by extracellular environment and that both input AF channels (NADH and FAD) are about equally important to the classification. In the future, such predictive DL models could directly detect specific immune cells in unstained images and thus, computationally improve the specificity of label-free MPM which would have great potential for in vivo endomicroscopy.

</details>


### [113] [Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge](https://arxiv.org/abs/2506.14457)
*Freya Behrens,Lenka Zdeborová*

Main category: cs.LG

TL;DR: Students trained on soft labels from teachers can achieve non-trivial accuracy on held-out memorized data, even in settings where generalization is impossible.


<details>
  <summary>Details</summary>
Motivation: To understand how memorized information is transferred in dataset distillation settings, especially when teachers rely on memorization rather than generalization.

Method: Analyze students trained on soft labels from teachers using finite random i.i.d. datasets where generalization is impossible.

Result: Students can learn about held-out memorized data, sometimes achieving perfect accuracy, depending on the temperature of logit smoothing.

Conclusion: Memorized information can be transferred via soft labels, with performance influenced by temperature, but robust across network variations.

Abstract: Dataset distillation aims to compress training data into fewer examples via a teacher, from which a student can learn effectively. While its success is often attributed to structure in the data, modern neural networks also memorize specific facts, but if and how such memorized information is can transferred in distillation settings remains less understood. In this work, we show that students trained on soft labels from teachers can achieve non-trivial accuracy on held-out memorized data they never directly observed. This effect persists on structured data when the teacher has not generalized.To analyze it in isolation, we consider finite random i.i.d. datasets where generalization is a priori impossible and a successful teacher fit implies pure memorization. Still, students can learn non-trivial information about the held-out data, in some cases up to perfect accuracy. In those settings, enough soft labels are available to recover the teacher functionally - the student matches the teacher's predictions on all possible inputs, including the held-out memorized data. We show that these phenomena strongly depend on the temperature with which the logits are smoothed, but persist across varying network capacities, architectures and dataset compositions.

</details>


### [114] [A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals](https://arxiv.org/abs/2506.14459)
*Md. Mortuza Ahmmed,Abdullah Al Noman,Mahin Montasir Afif,K. M. Tahsin Kabir,Md. Mostafizur Rahman,Mufti Mahmud*

Main category: cs.LG

TL;DR: The study proposes a stacking-based ensemble learning model to improve depression classification accuracy among professionals, achieving high performance metrics.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in developing accurate predictive models for depression due to its complex, multifaceted nature influenced by factors like occupational stress and lifestyle.

Method: A stacking-based ensemble learning approach integrating multiple base learners with logistic regression, tested on the Depression Professional Dataset from Kaggle.

Result: The model achieved 99.64% accuracy on training data and 98.75% on testing data, with precision, recall, and F1-score all exceeding 98%.

Conclusion: Ensemble learning is effective for mental health analytics, offering potential for early detection and intervention strategies.

Abstract: Depression is a significant mental health concern, particularly in professional environments where work-related stress, financial pressure, and lifestyle imbalances contribute to deteriorating well-being. Despite increasing awareness, researchers and practitioners face critical challenges in developing accurate and generalizable predictive models for mental health disorders. Traditional classification approaches often struggle with the complexity of depression, as it is influenced by multifaceted, interdependent factors, including occupational stress, sleep patterns, and job satisfaction. This study addresses these challenges by proposing a stacking-based ensemble learning approach to improve the predictive accuracy of depression classification among professionals. The Depression Professional Dataset has been collected from Kaggle. The dataset comprises demographic, occupational, and lifestyle attributes that influence mental well-being. Our stacking model integrates multiple base learners with a logistic regression-mediated model, effectively capturing diverse learning patterns. The experimental results demonstrate that the proposed model achieves high predictive performance, with an accuracy of 99.64% on training data and 98.75% on testing data, with precision, recall, and F1-score all exceeding 98%. These findings highlight the effectiveness of ensemble learning in mental health analytics and underscore its potential for early detection and intervention strategies.

</details>


### [115] [Zeroth-Order Optimization is Secretly Single-Step Policy Optimization](https://arxiv.org/abs/2506.14460)
*Junbin Qiu,Zhengpeng Xie,Xiangda Yan,Yongjie Yang,Yao Shu*

Main category: cs.LG

TL;DR: The paper reveals a connection between Zeroth-Order Optimization (ZOO) and Policy Optimization (PO), showing equivalence in objectives and gradient estimators. It introduces ZoAR, a novel ZOO algorithm with PO-inspired variance reduction techniques, proving its superiority empirically and theoretically.


<details>
  <summary>Details</summary>
Motivation: To elucidate the connection between ZOO and PO, particularly how ZOO methods with randomized finite differences relate to PO, and to leverage this insight for algorithmic improvements.

Method: The paper formalizes the equivalence between ZOO and single-step PO, introduces ZoAR with variance reduction techniques (averaged baseline and query reuse), and provides theoretical and empirical validation.

Result: ZoAR outperforms other ZOO methods in convergence speed and final performance, supported by theoretical analysis showing reduced variance and enhanced convergence.

Conclusion: The work offers a new theoretical understanding of ZOO through its connection to PO and presents practical improvements with ZoAR, advancing the field of gradient-free optimization.

Abstract: Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing functions where explicit gradients are unavailable or expensive to compute. However, the underlying mechanisms of popular ZOO methods, particularly those employing randomized finite differences, and their connection to other optimization paradigms like Reinforcement Learning (RL) are not fully elucidated. This paper establishes a fundamental and previously unrecognized connection: ZOO with finite differences is equivalent to a specific instance of single-step Policy Optimization (PO). We formally unveil that the implicitly smoothed objective function optimized by common ZOO algorithms is identical to a single-step PO objective. Furthermore, we show that widely used ZOO gradient estimators, are mathematically equivalent to the REINFORCE gradient estimator with a specific baseline function, revealing the variance-reducing mechanism in ZOO from a PO perspective.Built on this unified framework, we propose ZoAR (Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO algorithm incorporating PO-inspired variance reduction techniques: an averaged baseline from recent evaluations and query reuse analogous to experience replay. Our theoretical analysis further substantiates these techniques reduce variance and enhance convergence. Extensive empirical studies validate our theory and demonstrate that ZoAR significantly outperforms other methods in terms of convergence speed and final performance. Overall, our work provides a new theoretical lens for understanding ZOO and offers practical algorithmic improvements derived from its connection to PO.

</details>


### [116] [Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks](https://arxiv.org/abs/2506.14472)
*Fabien Bernier,Maxime Cordy,Yves Le Traon*

Main category: cs.LG

TL;DR: A hypernetwork architecture improves global electrical consumption forecasting by leveraging external factors like weather, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting is vital for energy management, but adding external factors often degrades global model performance.

Method: Used a hypernetwork to adjust model weights per consumer, tested on data from 6000+ Luxembourgish households with external factors.

Result: Hypernetwork outperformed other models, reducing errors and maintaining global model benefits.

Conclusion: Hypernetworks effectively enhance global forecasting accuracy by incorporating external factors.

Abstract: Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.

</details>


### [117] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/abs/2506.14515)
*Prabhav Sanga,Jaskaran Singh,Arun K. Dubey*

Main category: cs.LG

TL;DR: FAMR is a framework for efficient post-hoc unlearning in deep image classifiers, minimizing uniform-prediction loss on forget sets while anchoring to original parameters.


<details>
  <summary>Details</summary>
Motivation: Address the need for selective unlearning in ML systems due to privacy regulations, avoiding full retraining.

Method: FAMR uses constrained optimization with uniform-prediction loss and an ℓ2 penalty to anchor parameters.

Result: Effective class forgetting on CIFAR-10 and ImageNet-100, with strong performance retention and low overhead.

Conclusion: FAMR provides a scalable, certifiable solution for post-hoc unlearning in vision models, applicable to concept and style erasure.

Abstract: As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [118] [Two-Player Zero-Sum Games with Bandit Feedback](https://arxiv.org/abs/2506.14518)
*Elif Yılmaz,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: The paper introduces two algorithms, ETC-TPZSG and ETC-TPZSG-AE, for learning pure strategy Nash Equilibrium in a two-player zero-sum game with bandit feedback, achieving instance-dependent regret bounds.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the applicability of Explore-Then-Commit (ETC) in adversarial game settings and improve efficiency by leveraging the ε-Nash Equilibrium property.

Method: Proposes ETC-TPZSG and its enhanced version ETC-TPZSG-AE, which uses action pair elimination to optimize strategy selection.

Result: Derives instance-dependent regret bounds: O(Δ + √T) for ETC-TPZSG and O(log(TΔ²)/Δ) for ETC-TPZSG-AE.

Conclusion: ETC-based algorithms perform effectively in adversarial settings, matching existing methods while offering deeper insights through instance-dependent analysis.

Abstract: We study a two-player zero-sum game (TPZSG) in which the row player aims to maximize their payoff against an adversarial column player, under an unknown payoff matrix estimated through bandit feedback. We propose and analyze two algorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and ETC-TPZSG-AE, which improves upon it by incorporating an action pair elimination (AE) strategy that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair. Our objective is to demonstrate the applicability of ETC in a TPZSG setting by focusing on learning pure strategy Nash Equilibrium. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret for both algorithms, has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(Δ+ \sqrt{T})$ for ETC-TPZSG and $O(\frac{\log (T Δ^2)}Δ)$ for ETC-TPZSG-AE, where $Δ$ denotes the suboptimality gap. Therefore, our results indicate that ETC-based algorithms perform effectively in adversarial game settings, achieving regret bounds comparable to existing methods while providing insights through instance-dependent analysis.

</details>


### [119] [Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction](https://arxiv.org/abs/2506.14521)
*Korbinian Pfab,Marcel Rothering*

Main category: cs.LG

TL;DR: The paper critiques current AI research methodologies through a case study on false call reduction, identifying seven weaknesses and advocating for improved practices like requirement-aware metrics and clear success criteria.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether current AI research methodologies are effective for creating successful, profitable AI applications, using a real-world industrial case study.

Method: A case study on false call reduction in automated optical inspection, identifying and experimentally demonstrating seven weaknesses in current best practices.

Result: The study shows that current best-practice methodologies fail for the use case, highlighting the need for better metrics, success criteria, and dataset analysis.

Conclusion: Researchers should critically assess their methodologies to improve applied AI research success, emphasizing requirement-aware metrics and thorough dataset analysis.

Abstract: Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications? This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices. We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences. We show that the best-practice methodology would fail for this use case. We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets. Our work encourages researchers to critically assess their methodologies for more successful applied AI research.

</details>


### [120] [Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution](https://arxiv.org/abs/2506.14529)
*Xiaohan Zheng,Lanning Wei,Yong Li,Quanming Yao*

Main category: cs.LG

TL;DR: LLMNet automates GNN configuration using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), outperforming in 12 datasets across 3 tasks.


<details>
  <summary>Details</summary>
Motivation: GNNs require manual tuning; LLMNet aims to automate this process using LLMs and knowledge-guided evolution.

Method: Uses agents to build graph knowledge bases and RAG for automated GNN configuration and refinement.

Result: LLMNet excels in 12 datasets across 3 graph learning tasks.

Conclusion: LLMNet effectively automates GNN design, validated by empirical results.

Abstract: Effective decision-making on networks often relies on learning from graph-structured data, where Graph Neural Networks (GNNs) play a central role, but they take efforts to configure and tune. In this demo, we propose LLMNet, showing how to design GNN automated through Large Language Models. Our system develops a set of agents that construct graph-related knowlege bases and then leverages Retrieval-Augmented Generation (RAG) to support automated configuration and refinement of GNN models through a knowledge-guided evolution process. These agents, equipped with specialized knowledge bases, extract insights into tasks and graph structures by interacting with the knowledge bases. Empirical results show LLMNet excels in twelve datasets across three graph learning tasks, validating its effectiveness of GNN model designing.

</details>


### [121] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/abs/2506.14540)
*Gerardo A. Flores,Alyssa H. Smith,Julia A. Fukuyama,Ashia C. Wilson*

Main category: cs.LG

TL;DR: A framework for evaluating clinical decision support systems prioritizes calibration, robustness, and cost-sensitive performance over traditional metrics like accuracy and AUC-ROC.


<details>
  <summary>Details</summary>
Motivation: Traditional scoring rules (e.g., accuracy, AUC-ROC) don't align with clinical priorities like calibration, robustness, and asymmetric error costs.

Method: Proposes an adjusted cross-entropy (log score) based on proper scoring rules, averaging cost-weighted performance over relevant class balance ranges.

Result: The framework is simple, sensitive to clinical conditions, and prioritizes calibrated, robust models.

Conclusion: The approach better aligns model evaluation with clinical needs, ensuring practical and reliable deployment.

Abstract: Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.

</details>


### [122] [Single-Example Learning in a Mixture of GPDMs with Latent Geometries](https://arxiv.org/abs/2506.14563)
*Jesse St. Amand,Leonardo Gizzi,Martin A. Giese*

Main category: cs.LG

TL;DR: The paper introduces GPDMM for single-example learning of human motion, combining GPDMs in a mixture framework for diverse sequence encoding.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling human movement with limited data and high interpretability, especially in medical applications like prosthesis control.

Method: GPDMM combines multiple GPDMs (Gaussian process dynamical models) in a probabilistic mixture-of-experts framework, using geometric features for diverse sequence encoding.

Result: GPDMM is evaluated on classification accuracy and generative ability, outperforming LSTMs, VAEs, and transformers in single-example learning.

Conclusion: GPDMM is effective for human motion modeling in data-limited scenarios, offering interpretability and outperforming other models.

Abstract: We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.

</details>


### [123] [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
*Mingkang Zhu,Xi Chen,Zhongdao Wang,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: The paper introduces a method to integrate token-level reward models into Direct Preference Optimization (DPO) by decomposing sequence-level PPO into token-level problems, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Leveraging fine-grained token-level rewards for DPO is challenging due to its sequence-level formulation. This work aims to bridge this gap.

Method: Decomposes sequence-level PPO into token-level problems, derives optimal token-level policies and rewards, and integrates them into DPO via computable loss functions.

Result: Achieves win rate improvements of up to 7.5 points on MT-Bench, 6.2 on AlpacaEval 2, and 4.3 on Arena-Hard.

Conclusion: The proposed method successfully integrates token-level rewards into DPO, significantly outperforming standard DPO.

Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.

</details>


### [124] [Object-Centric Neuro-Argumentative Learning](https://arxiv.org/abs/2506.14577)
*Abdul Rahman Jacob,Avinash Kori,Emanuele De Angelis,Ben Glocker,Maurizio Proietti,Francesca Toni*

Main category: cs.LG

TL;DR: A novel Neural Argumentative Learning (NAL) architecture combines deep learning and Assumption-Based Argumentation (ABA) for safer, more reliable image analysis.


<details>
  <summary>Details</summary>
Motivation: Addressing safety, reliability, and interpretability concerns in deep learning for critical decision-making.

Method: Integrates neural (object-centric learning for image encoding) and symbolic (ABA learning for prediction) components.

Result: Competitive performance with state-of-the-art methods on synthetic data.

Conclusion: NAL offers a promising approach to enhance deep learning's reliability and interpretability.

Abstract: Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.

</details>


### [125] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/abs/2506.14587)
*Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: SCISSOR is a Siamese network-based debiasing method that addresses semantic shortcut learning by remapping the semantic space, improving model robustness without data augmentation.


<details>
  <summary>Details</summary>
Motivation: Shortcut learning undermines generalization due to semantic imbalances in embeddings, leading to spurious correlations.

Method: SCISSOR intervenes in semantic clusters to suppress shortcuts, using a Siamese network without requiring data augmentation.

Result: SCISSOR improves F1 scores by +5.3 to +7.7 across benchmarks and boosts lightweight models by ~9.5-11.9%.

Conclusion: SCISSOR redefines model generalization by tackling semantic biases, offering a robust framework for bias-resistant AI.

Abstract: Shortcut learning undermines model generalization to out-of-distribution data. While the literature attributes shortcuts to biases in superficial features, we show that imbalances in the semantic distribution of sample embeddings induce spurious semantic correlations, compromising model robustness. To address this issue, we propose SCISSOR (Semantic Cluster Intervention for Suppressing ShORtcut), a Siamese network-based debiasing approach that remaps the semantic space by discouraging latent clusters exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on 6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports +5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay, and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for BERT on NLP. Our study redefines the landscape of model generalization by addressing overlooked semantic biases, establishing SCISSOR as a foundational framework for mitigating shortcut learning and fostering more robust, bias-resistant AI systems.

</details>


### [126] [Deep Learning Surrogates for Real-Time Gas Emission Inversion](https://arxiv.org/abs/2506.14597)
*Thomas Newman,Christopher Nemeth,Matthew Jones,Philip Jonathan*

Main category: cs.LG

TL;DR: A deep-learning surrogate model for CFD enables real-time Bayesian inference of greenhouse-gas emissions, balancing accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of real-time identification and quantification of greenhouse-gas emissions under dynamic atmospheric conditions.

Method: Combines a deep-learning surrogate (multilayer perceptron) trained on CFD data with a sequential Monte Carlo algorithm for Bayesian inference of emission rates and source locations.

Result: Validated on methane release data, showing accuracy comparable to CFD and Gaussian plume models but with much faster runtimes. Robust in complex, obstructed-flow scenarios.

Conclusion: The framework offers a scalable, computationally feasible solution for real-time emissions monitoring and other spatio-temporal inversion tasks.

Abstract: Real-time identification and quantification of greenhouse-gas emissions under transient atmospheric conditions is a critical challenge in environmental monitoring. We introduce a spatio-temporal inversion framework that embeds a deep-learning surrogate of computational fluid dynamics (CFD) within a sequential Monte Carlo algorithm to perform Bayesian inference of both emission rate and source location in dynamic flow fields. By substituting costly numerical solvers with a multilayer perceptron trained on high-fidelity CFD outputs, our surrogate captures spatial heterogeneity and temporal evolution of gas dispersion, while delivering near-real-time predictions. Validation on the Chilbolton methane release dataset demonstrates comparable accuracy to full CFD solvers and Gaussian plume models, yet achieves orders-of-magnitude faster runtimes. Further experiments under simulated obstructed-flow scenarios confirm robustness in complex environments. This work reconciles physical fidelity with computational feasibility, offering a scalable solution for industrial emissions monitoring and other time-sensitive spatio-temporal inversion tasks in environmental and scientific modeling.

</details>


### [127] [Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization](https://arxiv.org/abs/2506.14607)
*Ziyu Gong,Jim Lim,David I. Inouye*

Main category: cs.LG

TL;DR: The paper introduces a novel likelihood-based distribution matching (DM) method using score-based priors, addressing scalability, instability, and bias issues in existing DM techniques.


<details>
  <summary>Details</summary>
Motivation: Existing DM methods face challenges like scalability (non-parametric), instability (adversarial), or bias (likelihood-based). The paper aims to overcome these by leveraging score-based priors.

Method: The approach trains likelihood-based DM using expressive score-based priors, requiring only the score function (not density), trained via denoising score matching. This avoids fixed priors and explicit density models.

Result: The method shows better stability, computational efficiency, and superior performance across tasks compared to other DM approaches like VAEs or LSGM.

Conclusion: The score-based DM method is a stable and effective alternative, eliminating biases and training challenges while improving performance.

Abstract: Distribution matching (DM) is a versatile domain-invariant representation learning technique that has been applied to tasks such as fair classification, domain adaptation, and domain translation. Non-parametric DM methods struggle with scalability and adversarial DM approaches suffer from instability and mode collapse. While likelihood-based methods are a promising alternative, they often impose unnecessary biases through fixed priors or require explicit density models (e.g., flows) that can be challenging to train. We address this limitation by introducing a novel approach to training likelihood-based DM using expressive score-based prior distributions. Our key insight is that gradient-based DM training only requires the prior's score function -- not its density -- allowing us to train the prior via denoising score matching. This approach eliminates biases from fixed priors (e.g., in VAEs), enabling more effective use of geometry-preserving regularization, while avoiding the challenge of learning an explicit prior density model (e.g., a flow-based prior). Our method also demonstrates better stability and computational efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore, experiments demonstrate superior performance across multiple tasks, establishing our score-based method as a stable and effective approach to distribution matching. Source code available at https://github.com/inouye-lab/SAUB.

</details>


### [128] [Feasibility-Driven Trust Region Bayesian Optimization](https://arxiv.org/abs/2506.14619)
*Paolo Ascia,Elena Raponi,Thomas Bäck,Fabian Duddeck*

Main category: cs.LG

TL;DR: FuRBO is a feasibility-driven Bayesian optimization method for constrained black-box problems, improving efficiency in locating feasible solutions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with high-dimensional, constrained optimization tasks where feasible regions are small and hard to identify, wasting resources.

Method: FuRBO uses adaptive trust regions informed by objective and constraint models, allowing dynamic search refocusing.

Result: FuRBO outperforms state-of-the-art methods in locating feasible and high-quality solutions across diverse benchmarks.

Conclusion: FuRBO is effective for constrained optimization, especially in high-dimensional spaces with severe constraints.

Abstract: Bayesian optimization is a powerful tool for solving real-world optimization tasks under tight evaluation budgets, making it well-suited for applications involving costly simulations or experiments. However, many of these tasks are also characterized by the presence of expensive constraints whose analytical formulation is unknown and often defined in high-dimensional spaces where feasible regions are small, irregular, and difficult to identify. In such cases, a substantial portion of the optimization budget may be spent just trying to locate the first feasible solution, limiting the effectiveness of existing methods. In this work, we present a Feasibility-Driven Trust Region Bayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust region from which the next candidate solution is selected, using information from both the objective and constraint surrogate models. Our adaptive strategy allows the trust region to shift and resize significantly between iterations, enabling the optimizer to rapidly refocus its search and consistently accelerate the discovery of feasible and good-quality solutions. We empirically demonstrate the effectiveness of FuRBO through extensive testing on the full BBOB-constrained COCO benchmark suite and other physics-inspired benchmarks, comparing it against state-of-the-art baselines for constrained black-box optimization across varying levels of constraint severity and problem dimensionalities ranging from 2 to 60.

</details>


### [129] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/abs/2506.14698)
*Sidney Bender,Jan Herrmann,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: The paper critiques current visual counterfactual explainers (VCEs) for focusing too narrowly on sample quality or minimal changes, neglecting broader explanation goals like fidelity and understandability. It introduces a new 'smooth counterfactual explorer' (SCE) algorithm to address these gaps.


<details>
  <summary>Details</summary>
Motivation: Existing VCEs lack consideration for holistic explanation qualities such as fidelity, understandability, and sufficiency, limiting their effectiveness.

Method: The authors propose a novel 'smooth counterfactual explorer' (SCE) algorithm, combining new counterfactual generation mechanisms to meet broader explanation goals.

Result: The SCE algorithm is evaluated systematically on synthetic and real data, demonstrating its effectiveness.

Conclusion: The SCE algorithm addresses the shortcomings of existing VCEs by better fulfilling holistic explanation desiderata.

Abstract: Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.

</details>


### [130] [On the Hardness of Bandit Learning](https://arxiv.org/abs/2506.14746)
*Nataly Brukhim,Aldo Pacchiano,Miroslav Dudik,Robert Schapire*

Main category: cs.LG

TL;DR: The paper explores bandit learnability, showing no combinatorial dimension can characterize it and proving computational hardness for certain reward function classes.


<details>
  <summary>Details</summary>
Motivation: To develop a general theory of bandit learnability, akin to PAC classification, and understand which function classes are learnable and how.

Method: Investigates learnability via combinatorial dimensions and computational hardness proofs, contrasting with classical learning theory.

Result: No combinatorial dimension characterizes bandit learnability; computational hardness exists even for simple reward classes.

Conclusion: Bandit learnability has inherent limitations and computational challenges, differing from classical learning frameworks.

Abstract: We study the task of bandit learning, also known as best-arm identification, under the assumption that the true reward function f belongs to a known, but arbitrary, function class F. We seek a general theory of bandit learnability, akin to the PAC framework for classification. Our investigation is guided by the following two questions: (1) which classes F are learnable, and (2) how they are learnable. For example, in the case of binary PAC classification, learnability is fully determined by a combinatorial dimension - the VC dimension- and can be attained via a simple algorithmic principle, namely, empirical risk minimization (ERM). In contrast to classical learning-theoretic results, our findings reveal limitations of learning in structured bandits, offering insights into the boundaries of bandit learnability. First, for the question of "which", we show that the paradigm of identifying the learnable classes via a dimension-like quantity fails for bandit learning. We give a simple proof demonstrating that no combinatorial dimension can characterize bandit learnability, even in finite classes, following a standard definition of dimension introduced by Ben-David et al. (2019). For the question of "how", we prove a computational hardness result: we construct a reward function class for which at most two queries are needed to find the optimal action, yet no algorithm can do so in polynomial time unless RP=NP. We also prove that this class admits efficient algorithms for standard algorithmic operations often considered in learning theory, such as an ERM. This implies that computational hardness is in this case inherent to the task of bandit learning. Beyond these results, we investigate additional themes such as learning under noise, trade-offs between noise models, and the relationship between query complexity and regret minimization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [131] [AgentFacts: Universal KYA Standard for Verified AI Agent Metadata & Deployment](https://arxiv.org/abs/2506.13794)
*Jared James Grogan*

Main category: cs.MA

TL;DR: AgentFacts is a universal metadata standard for verifying third-party AI agents, addressing trust gaps in enterprise AI deployment.


<details>
  <summary>Details</summary>
Motivation: Enterprise AI adoption is hindered by lack of standardized verification for third-party agents, leading to trust and coordination issues.

Method: AgentFacts uses cryptographically-signed capability declarations, multi-authority validation, and dynamic permission management.

Result: The standard enables systematic agent verification, eliminating single points of trust failure and supporting graduated confidence assessment.

Conclusion: AgentFacts transforms agent procurement into standardized workforce management, facilitating scalable enterprise AI coordination.

Abstract: Enterprise AI deployment faces critical "Know Your Agent" (KYA) challenges where organizations must verify third-party agent capabilities and establish trust without standardized metadata or verification infrastructure. Current approaches rely on self-declared capabilities and custom integration processes that create trust gaps and coordination friction limiting confident enterprise adoption. This paper presents AgentFacts, a universal metadata standard that enables systematic agent verification through cryptographically-signed capability declarations, multi-authority validation, and dynamic permission management. The specification introduces domain-specialized verification where different trusted authorities validate specific metadata aspects based on their expertise, eliminating single points of trust failure while enabling graduated confidence assessment. AgentFacts transforms agent procurement from custom integration projects into standardized workforce management, providing the transparency and governance infrastructure necessary for enterprise AI coordination at scale.

</details>


### [132] [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/abs/2506.13811)
*Sompote Youwai,David Phim,Vianne Gayl Murcia,Rianne Clair Onas*

Main category: cs.MA

TL;DR: Router-based multi-agent systems outperform single-agent and multi-agent workflows in foundation design automation, achieving high accuracy while requiring human oversight.


<details>
  <summary>Details</summary>
Motivation: To automate foundation design calculations efficiently while maintaining professional standards and safety in civil engineering.

Method: Evaluated three approaches: single-agent, multi-agent designer-checker, and router-based expert selection, using models like DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro.

Result: Router-based system achieved 95.00% accuracy for shallow foundations and 90.63% for pile design, outperforming standalone models and conventional workflows.

Conclusion: Router-based multi-agent systems are optimal for foundation design automation but require human oversight for safety-critical applications.

Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.

</details>


### [133] [Hierarchical Multi-Agent Reinforcement Learning-based Coordinated Spatial Reuse for Next Generation WLANs](https://arxiv.org/abs/2506.14187)
*Jiaming Yu,Le Liang,Hao Ye,Shi Jin*

Main category: cs.MA

TL;DR: The paper proposes a hierarchical multi-agent reinforcement learning (HMARL) approach to improve downlink spatial reuse in Wi-Fi networks, addressing co-channel interference in high-density deployments.


<details>
  <summary>Details</summary>
Motivation: High-density Wi-Fi deployments suffer from co-channel interference, degrading network performance. Coordinated spatial reuse (CSR) is needed to mitigate this issue.

Method: The CSR process is split into polling and decision phases, using HMARL with hierarchical policy networks for station selection and power control.

Result: Simulations show HMARL outperforms baselines in throughput and latency, with robust performance alongside legacy APs. It also improves fairness in high-interference regions.

Conclusion: HMARL effectively enhances CSR in Wi-Fi networks, offering superior performance and fairness compared to existing methods.

Abstract: High-density Wi-Fi deployments often result in significant co-channel interference, which degrades overall network performance. To address this issue, coordination of multi access points (APs) has been considered to enable coordinated spatial reuse (CSR) in next generation wireless local area networks. This paper tackles the challenge of downlink spatial reuse in Wi-Fi networks, specifically in scenarios involving overlapping basic service sets, by employing hierarchical multi-agent reinforcement learning (HMARL). We decompose the CSR process into two phases, i.e., a polling phase and a decision phase, and introduce the HMARL algorithm to enable efficient CSR. To enhance training efficiency, the proposed HMARL algorithm employs a hierarchical structure, where station selection and power control are determined by a high- and low-level policy network, respectively. Simulation results demonstrate that this approach consistently outperforms baseline methods in terms of throughput and latency across various network topologies. Moreover, the algorithm exhibits robust performance when coexisting with legacy APs. Additional experiments in a representative topology further reveal that the carefully designed reward function not only maximizes the overall network throughput, but also improves fairness in transmission opportunities for APs in high-interference regions.

</details>
