{"id": "2511.13788", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13788", "abs": "https://arxiv.org/abs/2511.13788", "authors": ["Samuel Nathanson", "Rebecca Williams", "Cynthia Matuszek"], "title": "Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments", "comment": "19 pages, 6 figures, 3 tables", "summary": "Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.", "AI": {"tldr": "Larger language models can systematically jailbreak smaller ones, with harm likelihood increasing with attacker-to-target size ratio, while attacker-side alignment reduces harmful responses.", "motivation": "To examine whether larger LLMs can systematically jailbreak smaller ones in multi-agent settings, raising concerns about adversarial vulnerabilities scaling with model interactions.", "method": "Simulated over 6,000 multi-turn attacker-target exchanges across LLM families (0.6B-120B parameters) using JailbreakBench tasks, with harm and refusal scores evaluated by three independent LLM judges.", "result": "Strong correlation between mean harm and log of attacker-to-target size ratio (r=0.51, p<0.001), higher harm variance across attackers than targets, and strong negative correlation between attacker refusal frequency and harm (rho=-0.93).", "conclusion": "Size asymmetry influences robustness, with attacker-side behavioral diversity and alignment being key factors in adversarial outcomes, motivating further investigation into inter-model safety."}}
{"id": "2511.14043", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.14043", "abs": "https://arxiv.org/abs/2511.14043", "authors": ["Chandrachur Bhattacharya", "Sibendu Som"], "title": "AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance", "comment": null, "summary": "AI Scientific Assistant Core (AISAC) is an integrated multi-agent system developed at Argonne National Laboratory for scientific and engineering workflows. AISAC builds on established technologies - LangGraph for orchestration, FAISS for vector search, and SQLite for persistence - and integrates them into a unified system prototype focused on transparency, provenance tracking, and scientific adaptability.\n  The system implements a Router-Planner-Coordinator workflow and an optional Evaluator role, using prompt-engineered agents coordinated via LangGraph's StateGraph and supported by helper agents such as a Researcher. Each role is defined through custom system prompts that enforce structured JSON outputs. A hybrid memory approach (FAISS + SQLite) enables both semantic retrieval and structured conversation history. An incremental indexing strategy based on file hashing minimizes redundant re-embedding when scientific corpora evolve. A configuration-driven project bootstrap layer allows research teams to customize tools, prompts, and data sources without modifying core code.\n  All agent decisions, tool invocations, and retrievals are logged and visualized through a custom Gradio interface, providing step-by-step transparency for each reasoning episode. The authors have applied AISAC to multiple research areas at Argonne, including specialized deployments for waste-to-products research and energy process safety, as well as general-purpose scientific assistance, demonstrating its cross-domain applicability.", "AI": {"tldr": "AISAC is a multi-agent system for scientific workflows featuring transparent coordination, hybrid memory, and adaptable configuration.", "motivation": "To create an integrated system for scientific workflows that emphasizes transparency, provenance tracking, and adaptability to evolving research needs.", "method": "Uses LangGraph orchestration with Router-Planner-Coordinator workflow, FAISS/SQLite hybrid memory, prompt-engineered agents with structured JSON outputs, and incremental indexing strategy.", "result": "Successfully deployed across multiple research domains at Argonne National Laboratory, demonstrating cross-domain applicability with full decision transparency.", "conclusion": "AISAC provides an effective framework for scientific assistance with strong provenance tracking and adaptability, proven in specialized and general research applications."}}
{"id": "2511.14098", "categories": ["cs.AI", "cs.MA", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.14098", "abs": "https://arxiv.org/abs/2511.14098", "authors": ["Adit Jain", "Vikram Krishnamurthy", "Yiming Zhang"], "title": "Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data", "comment": null, "summary": "In this paper, we model and analyze how a network of interacting LLMs performs collaborative question-answering (CQA) in order to estimate a ground truth given a distributed set of documents. This problem is interesting because LLMs often hallucinate when direct evidence to answer a question is lacking, and these effects become more pronounced in a network of interacting LLMs. The hallucination spreads, causing previously accurate LLMs to hallucinate. We study interacting LLMs and their hallucination by combining novel ideas of mean-field dynamics (MFD) from network science and the randomized utility model from economics to construct a useful generative model. We model the LLM with a latent state that indicates if it is truthful or not with respect to the ground truth, and extend a tractable analytical model considering an MFD to model the diffusion of information in a directed network of LLMs. To specify the probabilities that govern the dynamics of the MFD, we propose a randomized utility model. For a network of LLMs, where each LLM has two possible latent states, we posit sufficient conditions for the existence and uniqueness of a fixed point and analyze the behavior of the fixed point in terms of the incentive (e.g., test-time compute) given to individual LLMs. We experimentally study and analyze the behavior of a network of $100$ open-source LLMs with respect to data heterogeneity, node capability, network structure, and sensitivity to framing on multiple semi-synthetic datasets.", "AI": {"tldr": "This paper models how networks of LLMs collaborate on question-answering, showing that hallucinations spread through the network and cause previously accurate models to hallucinate.", "motivation": "LLMs often hallucinate when direct evidence is lacking, and these effects become more pronounced in networks of interacting LLMs where hallucinations can spread.", "method": "Combines mean-field dynamics from network science and randomized utility model from economics to model LLM networks with latent truth states, analyzing fixed points and dynamics.", "result": "Established sufficient conditions for fixed point existence/uniqueness and analyzed behavior with respect to incentives. Experimental study with 100 LLMs on semi-synthetic datasets examining data heterogeneity, node capability, and network structure.", "conclusion": "The proposed framework successfully models hallucination spread in LLM networks and provides analytical insights into network behavior under various conditions."}}
{"id": "2511.14135", "categories": ["cs.LG", "cs.AI", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.14135", "abs": "https://arxiv.org/abs/2511.14135", "authors": ["Promise Ekpo", "Saesha Agarwal", "Felix Grimm", "Lekan Molu", "Angelique Taylor"], "title": "Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation", "comment": null, "summary": "Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\\ 0.33 JFI, $p < 0.01$) while maintaining 86\\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.", "AI": {"tldr": "Fair-GNE introduces a game-theoretic framework for enforcing fair workload allocation in multi-agent healthcare systems by modeling MARL as a constrained generalized Nash equilibrium problem, achieving significantly better workload balance than baselines while maintaining high task success rates.", "motivation": "Existing MARL approaches use post-hoc reward shaping for fairness, lacking certifiable self-enforceable fairness that cannot be manipulated by individual agents at runtime, especially in healthcare settings where agents share resources.", "method": "Proposes Fair-GNE framework that models multi-agent reinforcement learning as a constrained generalized Nash equilibrium (GNE) game, steering group policy to safe and locally efficient equilibria where no agent can unilaterally improve its utility.", "result": "Significant improvement in workload balance over fixed-penalty baselines (0.89 vs. 0.33 JFI, p < 0.01) while maintaining 86% task success rate, demonstrating statistically significant fairness gains through adaptive constraint enforcement.", "conclusion": "Fair-GNE provides a principled framework for achieving certifiable fairness in multi-agent healthcare systems through equilibrium-seeking innovations, enabling clear and enforceable fairness in large learning-based healthcare systems."}}
