<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents](https://arxiv.org/abs/2512.24189)
*Yankai Jiang,Wenjie Lou,Lilong Wang,Zhenyu Tang,Shiyang Feng,Jiaxuan Lu,Haoran Sun,Yaning Pan,Shuang Gu,Haoyang Su,Feng Liu,Wangxu Wei,Pan Tan,Dongzhan Zhou,Fenghua Ling,Cheng Tan,Bo Zhang,Xiaosong Wang,Lei Bai,Bowen Zhou*

Main category: cs.AI

TL;DR: SCP is an open-source standard for creating a global network of autonomous scientific agents, enabling seamless integration and orchestration of tools, data, and instruments.


<details>
  <summary>Details</summary>
Motivation: To accelerate scientific discovery by overcoming barriers like platform fragmentation and lack of interoperability, which hinder collaboration and reproducibility in agent-driven science.

Method: SCP relies on two pillars: a protocol for unified resource integration and a service architecture for end-to-end experiment lifecycle management, implemented via SCP Hub and federated servers.

Result: Based on SCP, a platform with over 1,600 tool resources was built. It enhances secure collaboration, reduces integration overhead, and improves reproducibility across diverse use cases.

Conclusion: SCP provides essential infrastructure for scalable, multi-institution, agent-driven science by standardizing scientific context and orchestration at the protocol level.

Abstract: We introduce SCP: the Science Context Protocol, an open-source standard designed to accelerate discovery by enabling a global network of autonomous scientific agents. SCP is built on two foundational pillars: (1) Unified Resource Integration: At its core, SCP provides a universal specification for describing and invoking scientific resources, spanning software tools, models, datasets, and physical instruments. This protocol-level standardization enables AI agents and applications to discover, call, and compose capabilities seamlessly across disparate platforms and institutional boundaries. (2) Orchestrated Experiment Lifecycle Management: SCP complements the protocol with a secure service architecture, which comprises a centralized SCP Hub and federated SCP Servers. This architecture manages the complete experiment lifecycle (registration, planning, execution, monitoring, and archival), enforces fine-grained authentication and authorization, and orchestrates traceable, end-to-end workflows that bridge computational and physical laboratories. Based on SCP, we have constructed a scientific discovery platform that offers researchers and agents a large-scale ecosystem of more than 1,600 tool resources. Across diverse use cases, SCP facilitates secure, large-scale collaboration between heterogeneous AI systems and human researchers while significantly reducing integration overhead and enhancing reproducibility. By standardizing scientific context and tool orchestration at the protocol level, SCP establishes essential infrastructure for scalable, multi-institution, agent-driven science.

</details>


### [2] [The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models](https://arxiv.org/abs/2512.23850)
*Rahul Baxi*

Main category: cs.AI

TL;DR: The paper introduces the Drill-Down and Fabricate Test (DDFT) protocol to measure epistemic robustness in language models, showing that robustness is independent of model size and architecture, and identifies error detection as a key bottleneck.


<details>
  <summary>Details</summary>
Motivation: Current evaluations like MMLU and TruthfulQA fail to assess how robustly models maintain factual accuracy under challenging conditions such as semantic compression or adversarial attacks, leading to unreliable knowledge assessment.

Method: We propose a two-system cognitive model (Semantic System and Epistemic Verifier) and evaluate 9 frontier models across 8 domains at 5 compression levels, conducting 1,800 turn-level evaluations using the DDFT protocol.

Result: Epistemic robustness is orthogonal to conventional paradigms: neither parameter count (r=0.083, p=0.832) nor architecture (r=0.153, p=0.695) predicts it; error detection strongly correlates with robustness (rho=-0.817, p=0.007); flagship models are brittle while smaller ones can be robust.

Conclusion: The DDFT provides a framework for epistemic robustness assessment, revealing that training methodology and verification mechanisms, not model size, drive robustness, which is crucial for deployment in critical applications.

Abstract: Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems](https://arxiv.org/abs/2512.23809)
*Samaresh Kumar Singh,Joyjit Roy,Martin So*

Main category: cs.LG

TL;DR: A framework called Zero-Trust Agentic Federated Learning (ZTA-FL) is proposed to enhance the security of Industrial IoT by combining TPM-based attestation, SHAP-weighted aggregation, and adversarial training, achieving high detection accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Recent attacks on critical infrastructure, such as the 2021 Oldsmar water treatment breach and 2023 Danish energy sector compromises, expose security gaps in Industrial IoT (IIoT). Existing Federated Learning (FL) frameworks for privacy-preserving intrusion detection are vulnerable to Byzantine poisoning attacks and lack robust agent authentication.

Method: ZTA-FL integrates three components: (1) TPM-based cryptographic attestation with a false acceptance rate below 0.0000001, (2) a novel SHAP-weighted aggregation algorithm for explainable Byzantine detection under non-IID conditions with theoretical guarantees, and (3) privacy-preserving on-device adversarial training.

Result: Comprehensive experiments on three IDS benchmarks (Edge-IIoTset, CIC-IDS2017, UNSW-NB15) show ZTA-FL achieves 97.8% detection accuracy, 93.2% accuracy under 30% Byzantine attacks (outperforming FLAME by 3.1%, p < 0.01), 89.3% adversarial robustness, and reduces communication overhead by 34%.

Conclusion: ZTA-FL significantly improves security for IIoT deployments, providing defense in depth against attacks, supported by theoretical analysis, failure mode characterization, and released code for reproducibility.

Abstract: Recent attacks on critical infrastructure, including the 2021 Oldsmar water treatment breach and 2023 Danish energy sector compromises, highlight urgent security gaps in Industrial IoT (IIoT) deployments. While Federated Learning (FL) enables privacy-preserving collaborative intrusion detection, existing frameworks remain vulnerable to Byzantine poisoning attacks and lack robust agent authentication. We propose Zero-Trust Agentic Federated Learning (ZTA-FL), a defense in depth framework combining: (1) TPM-based cryptographic attestation achieving less than 0.0000001 false acceptance rate, (2) a novel SHAP-weighted aggregation algorithm providing explainable Byzantine detection under non-IID conditions with theoretical guarantees, and (3) privacy-preserving on-device adversarial training. Comprehensive experiments across three IDS benchmarks (Edge-IIoTset, CIC-IDS2017, UNSW-NB15) demonstrate that ZTA-FL achieves 97.8 percent detection accuracy, 93.2 percent accuracy under 30 percent Byzantine attacks (outperforming FLAME by 3.1 percent, p less than 0.01), and 89.3 percent adversarial robustness while reducing communication overhead by 34 percent. We provide theoretical analysis, failure mode characterization, and release code for reproducibility.

</details>


### [4] [Network Traffic Analysis with Process Mining: The UPSIDE Case Study](https://arxiv.org/abs/2512.23718)
*Francesco Vitale,Paolo Palmiero,Massimiliano Rak,Nicola Mazzocca*

Main category: cs.LG

TL;DR: A process mining method is applied to gaming network traffic for unsupervised state characterization, Petri net modeling, and video game classification, achieving good modeling coherence and separation but moderate classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Online gaming's market relevance and complex network infrastructure drive research in modeling device behavior for bandwidth management, load prediction, and security, with process mining offering data-driven insights.

Method: A process mining-based method analyzes gaming network traffic for unsupervised characterization of states, encoding states into interpretable Petri nets, and classifying traffic to identify video games.

Result: Applied to UPSIDE case study with Clash Royale and Rocket League data, the method shows effective modeling: 94.02% inter-device similarity, 174.99% inter-state separation, and 73.84% AUC for game classification.

Conclusion: Process mining enables interpretable modeling and classification of gaming network behavior, with potential for further improvement in accuracy and broader application to diverse games and scenarios.

Abstract: Online gaming is a popular activity involving the adoption of complex systems and network infrastructures. The relevance of gaming, which generates large amounts of market revenue, drove research in modeling network devices' behavior to evaluate bandwidth consumption, predict and sustain high loads, and detect malicious activity. In this context, process mining appears promising due to its ability to combine data-driven analyses with model-based insights. In this paper, we propose a process mining-based method that analyzes gaming network traffic, allowing: unsupervised characterization of different states from gaming network data; encoding such states through process mining into interpretable Petri nets; and classification of gaming network traffic data to identify different video games being played. We apply the method to the UPSIDE case study, involving gaming network data of several devices interacting with two video games: Clash Royale and Rocket League. Results demonstrate that the gaming network behavior can be effectively and interpretably modeled through states represented as Petri nets with sufficient coherence (94.02% inter-device similarity) and specificity (174.99% inter-state separation) while maintaining a good classification accuracy of the two different video games (73.84% AUC).

</details>
