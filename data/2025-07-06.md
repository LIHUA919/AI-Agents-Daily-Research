<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP improves LLM-based medical diagnosis prediction using a knowledge graph and multi-agent architecture for better reliability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Supervised ML models struggle with generalization due to limited labeled data, while LLMs face issues like hallucinations and lack of structured reasoning in diagnosis prediction.

Method: KERAP uses a multi-agent framework: a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent for iterative refinement.

Result: KERAP enhances diagnostic reliability and offers a scalable, interpretable solution for zero-shot medical diagnosis prediction.

Conclusion: KERAP addresses LLM limitations in medical diagnosis by integrating structured knowledge and multi-agent reasoning, improving reliability and scalability.

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [2] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA is a self-evolving AI agent for biomedical research, using a multi-agent architecture to autonomously improve its capabilities, outperforming state-of-the-art models by up to 6% and doubling accuracy with experience.


<details>
  <summary>Details</summary>
Motivation: The fragmented biomedical research landscape and limitations of static AI toolsets necessitate adaptive, scalable solutions.

Method: STELLA employs a multi-agent architecture with an evolving Template Library and dynamic Tool Ocean, autonomously integrating new tools and learning from experience.

Result: STELLA achieves 26% on Humanity's Last Exam, 54% on LAB-Bench: DBQA, and 63% on LAB-Bench: LitQA, outperforming leading models and improving with experience.

Conclusion: STELLA advances AI agent systems by enabling dynamic learning and scaling, accelerating biomedical discovery.

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [3] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR is a lightweight, hybrid feature selection method combining P2P and P2T correlations to eliminate redundancy and retain relevant features, outperforming traditional techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of redundant feature elimination while retaining relevant ones in dimensionality reduction, HCVR combines P2P and P2T correlations.

Method: HCVR is a greedy, backward elimination method using majority voting rules based on correlation thresholds between features and the target.

Result: Applied to the SPAMBASE dataset, HCVR showed improved performance over traditional non-iterative and iterative feature selection techniques.

Conclusion: HCVR is effective for feature selection, enhancing classifier performance by intelligently filtering features.

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [4] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: The paper reviews efficient test-time compute (TTC) strategies for improving LLM reasoning, categorizing methods into fixed-budget (L1) and dynamic-scaling (L2) approaches, and benchmarks them for performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLMs inefficiently allocate compute resources, overthinking simple tasks and underthinking complex ones, prompting a need for better TTC strategies.

Method: The survey introduces a taxonomy (L1 for fixed budgets, L2 for dynamic scaling) and benchmarks proprietary LLMs across datasets to evaluate trade-offs.

Result: The review highlights practical control, adaptability, and scalability of TTC methods, with insights into performance vs. token usage.

Conclusion: Future work should focus on hybrid models and challenges to enhance LLM efficiency, robustness, and user responsiveness.

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [5] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Micha≈Ç Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym is a benchmark assessing LLMs' experiment design and analysis in biology via simulated systems, revealing performance declines with complexity.


<details>
  <summary>Details</summary>
Motivation: Evaluate LLMs' scientific competencies (experiment design and analysis) without costly wet-lab experiments.

Method: Use SciGym, a dry lab with biological systems encoded in Systems Biology Markup Language, to simulate data and test LLMs on 137 small systems.

Result: More capable LLMs performed better, but all declined with increased system complexity.

Conclusion: LLMs need significant improvement in scientific capabilities for complex tasks.

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [6] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: The paper explores how AI can learn from neuroscience to improve continual and in-context learning, inspired by animals' rapid adaptation to changing environments.


<details>
  <summary>Details</summary>
Motivation: AI models lack the dynamic adaptability seen in animals, especially in social contexts, which is crucial for real-world applications like robotics and autonomous systems.

Method: The paper integrates AI literature on continual learning with neuroscience research on behavioral tasks involving shifting rules and rewards.

Result: It proposes a framework for leveraging neuroscience insights to enhance AI adaptability and vice versa, fostering the NeuroAI field.

Conclusion: The paper advocates for cross-disciplinary collaboration between AI and neuroscience to advance adaptive learning systems.

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [7] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: The paper explores using audit study data to improve fairness in AI hiring algorithms, revealing flaws in traditional bias-mitigation methods and proposing new interventions.


<details>
  <summary>Details</summary>
Motivation: AI systems in hiring and loan decisions often use biased training data, and traditional fairness interventions may not address underlying disparities effectively.

Method: The study leverages audit study data (randomized control trials with fictitious testers) to train and evaluate hiring algorithms, comparing traditional fairness methods with new interventions based on individual treatment effect estimation.

Result: Audit data shows traditional fairness methods (like equalizing base rates) can hide ~10% disparity. New interventions further reduce algorithmic discrimination.

Conclusion: Audit study data provides a rigorous way to improve fairness in AI systems, exposing limitations of current methods and offering better alternatives.

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [8] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: The paper explores data diversification in preference optimization to enhance LLMs' mathematical reasoning, introducing DTS, which outperforms traditional methods with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning in LLMs remains challenging despite progress in preference learning, prompting investigation into diversified data strategies.

Method: Evaluates temperature sampling, Chain-of-Thought prompting, and MCTS, and introduces DTS for decomposing problems into diverse reasoning paths.

Result: DTS improves performance by 7.1% on GSM8K and 4.2% on MATH with only 1.03x computational overhead, while MCTS is costlier and less effective.

Conclusion: Structured exploration of diverse problem-solving methods (e.g., DTS) yields more effective preference data for mathematical alignment than traditional approaches.

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [9] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: The paper examines the consistency between LLMs' stated beliefs and their behavior in role-playing simulations, introducing a metric to measure this alignment and identifying factors affecting it.


<details>
  <summary>Details</summary>
Motivation: To ensure LLM-based role-playing agents' outputs are coherent with their assigned roles, as their use in synthetic data generation for human behavioral research grows.

Method: An evaluation framework using the GenAgents persona bank and the Trust Game, with a belief-behavior consistency metric to analyze factors like belief types, information presentation, and forecasting depth.

Result: Systematic inconsistencies between LLMs' stated/imposed beliefs and simulation outcomes, even when beliefs seem plausible.

Conclusion: Researchers must identify when LLMs' beliefs align with behavior to use them appropriately in behavioral studies.

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [10] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: The paper explores how dilution and mobility impact cooperation in spatial prisoner's dilemma games using multi-agent Q-learning, revealing qualitative equivalence between fixed and learned update rules and symbiotic effects.


<details>
  <summary>Details</summary>
Motivation: To understand the effects of dilution and mobility on cooperation in spatial prisoner's dilemma games using reinforcement learning.

Method: Uses an independent multi-agent Q-learning algorithm to model different game-theoretical scenarios, including dilution and mobility.

Result: Observes qualitative equivalence between fixed and learned update rules and the emergence of symbiotic mutualistic effects.

Conclusion: The approach showcases versatility in modeling game-theoretical scenarios and highlights the benchmarking potential of reinforcement learning in such settings.

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [11] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW automates planning problem generation and evaluation for LLMs, revealing performance insights and limitations in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the bottleneck of scalable, reliable data generation and evaluation for enhancing LLM planning and reasoning capabilities.

Method: Introduces NL2FLOW, a system for generating planning problems in natural language, structured representation, and PDDL, and evaluating LLM performance.

Result: Top models achieved 86% success in valid plans and 69% in optimal plans, with performance influenced by problem characteristics and prompt design.

Conclusion: Direct reasoning from natural language may outperform decomposed tasks, and dynamic understanding of limitations is key for advancing LLM problem-solving.

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [12] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: The paper critiques belief revision approaches for focusing on postulates (constraints) rather than abilities (reachability of belief states). It highlights the need for mechanisms to achieve diverse doxastic states like plasticity, equating, and dogmatism.


<details>
  <summary>Details</summary>
Motivation: To address the gap in analyzing existing belief revision approaches, emphasizing the importance of abilities (reachability of belief states) over syntactic postulates.

Method: Examines various belief revision mechanisms (e.g., lexicographic, natural, severe) to determine their abilities to achieve specific belief states.

Result: Identifies that each revision mechanism possesses unique abilities (e.g., plasticity, dogmatism) but lacks others, demonstrating their limitations and strengths.

Conclusion: Belief revision mechanisms should be evaluated based on their abilities to reach diverse belief states, not just syntactic postulates, to better suit practical applications.

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [13] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS is a keyword generation framework for sponsored search ads, addressing LLM limitations by being on-the-fly, multi-objective, and self-reflective, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLM-based keyword generation lacks scalability, multi-objective optimization, and quality control, hindering automation in ad campaigns.

Method: OMS framework operates without training data, monitors online performance, optimizes for multiple metrics, and evaluates keyword quality agentically.

Result: OMS outperforms existing methods in benchmarks and real-world campaigns, with ablation studies confirming component effectiveness.

Conclusion: OMS successfully addresses LLM limitations, enabling automated, high-quality keyword decisions for ad campaigns.

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [14] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: An AI-native autonomous laboratory is introduced for complex scientific experiments, autonomously managing instrumentation and optimizing performance without human intervention, matching human scientist results.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous scientific research for non-specialists by overcoming limitations of current systems confined to simple workflows.

Method: Co-design of AI models, experiments, and instruments to create an end-to-end, multi-user platform for complex, multi-objective experiments.

Result: The system autonomously optimizes experiments, matches human performance, and improves efficiency in multi-user scenarios.

Conclusion: The platform advances biomaterials research, reduces expert dependency, and establishes a blueprint for scalable science-as-a-service.

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [15] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: The paper reformulates machine learning models using category theory to enhance AI explicability, focusing on multiple linear regression and introducing the Gauss-Markov Adjunction to describe parameter-residual interplay.


<details>
  <summary>Details</summary>
Motivation: To improve AI explicability and interpretability by providing a semantic framework for understanding AI systems through category theory.

Method: Reformulates supervised learning (specifically multiple linear regression) using category theory, defining categories for parameters/data and an adjoint functor pair (Gauss-Markov Adjunction).

Result: Shows how parameter variations and residuals interact, linking ordinary least squares estimators and minimum residuals via adjoint functors.

Conclusion: Proposes this categorical framework as a foundation for AI explicability, extending denotational semantics from theoretical computer science.

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [16] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: Improving task clarity with structured semantic context enhances reasoning in LLMs, achieving a 2.1√ó improvement in theorem-proving success.


<details>
  <summary>Details</summary>
Motivation: To explore if clearer task descriptions can boost the reasoning ability of large language models, especially in theorem proving.

Method: Introduces a concept-level metric for task clarity, uses selective concept unfolding, and employs a Planner--Executor architecture.

Result: Achieves a 1.85√ó clarity score improvement (44.5% to 82.3%) and 2.1√ó proof success (21.8% to 45.8%), outperforming Graph2Tac (33.2%).

Conclusion: Structured task representations bridge the gap between understanding and reasoning, enhancing LLM performance.

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [17] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI research agents improve performance on MLE-bench by optimizing search policies and operator sets, achieving a 47.7% success rate in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: To accelerate scientific progress by automating machine learning model design and training, focusing on improving AI research agents' performance in real-world challenges like Kaggle competitions.

Method: Formalize agents as search policies navigating solution spaces, testing various operator sets and search strategies (Greedy, MCTS, Evolutionary) to find optimal pairings.

Result: Best pairing achieves a 47.7% success rate on MLE-bench lite, up from 39.6%, demonstrating the impact of strategy-operator interplay.

Conclusion: Joint consideration of search strategy, operator design, and evaluation is crucial for advancing automated machine learning.

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [18] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: The paper analyzes the computational complexity of responsibility properties (diffusion and gap) in collective decision-making, showing their complexity classes and intersection.


<details>
  <summary>Details</summary>
Motivation: To understand the computational aspects of responsibility in AI and collective decision-making.

Method: Investigates the computational complexity of diffusion-free and gap-free decision-making mechanisms.

Result: Diffusion-free mechanisms are Œ†‚ÇÇ-complete, gap-free are Œ†‚ÇÉ-complete, and their intersection is Œ†‚ÇÇ-complete.

Conclusion: The study provides insights into the computational limits of responsibility in collective decision-making.

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [19] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: The paper introduces MIMIC-Patient, a dataset for dynamic patient-level simulations, and DynamiCare, a multi-agent framework for iterative clinical diagnosis, addressing gaps in current medical AI frameworks.


<details>
  <summary>Details</summary>
Motivation: Current AI frameworks for medical decision-making focus on single-turn tasks, diverging from the real-world diagnostic process, which is interactive and iterative.

Method: The authors propose DynamiCare, a dynamic multi-agent framework that models diagnosis as a multi-round loop, using the MIMIC-Patient dataset for simulations.

Result: Extensive experiments demonstrate the feasibility and effectiveness of DynamiCare, setting a benchmark for dynamic clinical decision-making with LLM-powered agents.

Conclusion: The work advances AI in healthcare by aligning simulation frameworks with real-world diagnostic processes, enabling more realistic and effective clinical decision-making.

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [20] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: LLMs demonstrate strategic intelligence in the Iterated Prisoner's Dilemma, with distinct behaviors from OpenAI, Google, and Anthropic models.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can reason strategically in competitive settings like the IPD, bridging game theory and AI behavior.

Method: Conducted evolutionary IPD tournaments with LLMs from OpenAI, Google, and Anthropic, varying termination probabilities to test adaptability.

Result: LLMs showed competitive survival, with unique strategic traits: Google's models were ruthless, OpenAI's cooperative, and Anthropic's forgiving.

Conclusion: LLMs exhibit strategic reasoning, connecting game theory and AI psychology, offering insights into algorithmic decision-making under uncertainty.

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [21] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA introduces a hierarchical framework for complex search tasks, separating strategic planning from specialized execution, outperforming state-of-the-art systems.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG pipelines and reasoning-based approaches struggle with complex information needs due to inefficiency and scalability issues.

Method: HiRA decomposes tasks into subtasks, assigns them to domain-specific agents with external tools, and integrates results hierarchically.

Result: HiRA outperforms state-of-the-art systems on complex benchmarks, improving answer quality and efficiency.

Conclusion: Decoupling planning and execution enhances performance in multi-step information seeking tasks.

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [22] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: The paper introduces an AI-driven approach using Large Language Models (LLMs) to enhance hardware design verification, achieving high coverage and efficiency.


<details>
  <summary>Details</summary>
Motivation: The complexity of modern ICs and the tedious, time-consuming nature of hardware design verification motivate the use of AI to streamline the process.

Method: An agentic AI-based approach with Human-in-the-Loop (HITL) intervention is proposed for dynamic, iterative, and self-reflective verification.

Result: Tested on five open-source designs, the method achieves over 95% coverage with reduced verification time, showing superior performance and adaptability.

Conclusion: The AI-driven approach significantly improves hardware design verification, offering efficiency, adaptability, and configurability.

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [23] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: TH2T is a two-stage fine-tuning strategy for Long Reasoning Models (LRMs) that reduces overthinking by enhancing difficulty and redundancy cognition, significantly cutting inference costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LRMs suffer from overthinking due to a uniform reasoning process, lacking task difficulty awareness. TH2T aims to address this by bootstrapping such cognitive abilities.

Method: TH2T uses difficulty-hypnosis and redundancy-hypnosis in two stages: first to enhance task difficulty sensitivity, then to identify and reduce redundant reasoning steps.

Result: Experiments show TH2T reduces inference costs by over 70% on easy tasks and 40% on hard tasks, with stable performance and clearer difficulty-aware outputs.

Conclusion: TH2T effectively mitigates overthinking in LRMs, improving efficiency and reasoning quality through cognitive enhancements.

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [24] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: The paper detects student disengagement in non-mandatory quizzes using machine learning, achieving 91% accuracy, and provides an explainable framework for interventions.


<details>
  <summary>Details</summary>
Motivation: Addressing student disengagement in distance education to prevent academic drop-out by analyzing participation in non-mandatory exercises.

Method: Analyzed student log data from Moodle, trained eight machine learning algorithms, and used SHAP for explainability.

Result: Achieved 91% balanced accuracy, correctly detecting 85% of disengaged students.

Conclusion: Proposes a predictive and explainable framework for timely interventions to reduce disengagement in online learning.

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [25] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schm√∂cker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: Proposes two safe abstraction dropping schemes (OGA-IAAD and OGA-CAD) for MCTS, improving performance without degradation.


<details>
  <summary>Details</summary>
Motivation: Non-exact abstractions in MCTS introduce approximation errors, hindering optimal action convergence. Existing methods like Xu et al.'s can degrade performance.

Method: Introduces OGA-IAAD for time-critical settings and OGA-CAD for performance improvement with fixed iterations.

Result: Both schemes improve performance safely, avoiding notable degradations.

Conclusion: OGA-IAAD and OGA-CAD offer reliable, safe abstraction dropping for MCTS, outperforming prior methods.

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [26] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: The paper introduces self-generated goal-conditioned MDPs (sG-MDPs) to improve LLMs' reasoning in ATP, achieving state-of-the-art results on PutnamBench.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of sparse rewards and large proof scales in LLMs for automated theorem proving, especially in complex benchmarks like PutnamBench.

Method: Proposes sG-MDPs for structured subgoal generation and uses MCTS-like algorithms, implemented in Bourbaki (7B), a modular system ensembling multiple LLMs.

Result: Bourbaki (7B) solves 26 problems on PutnamBench, setting a new state-of-the-art for models of its scale.

Conclusion: The sG-MDP framework and Bourbaki (7B) effectively enhance LLMs' reasoning in ATP, demonstrating significant performance improvements.

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [27] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: The paper introduces Knowledge Protocol Engineering (KPE) to enhance LLMs by translating expert knowledge into machine-executable protocols, enabling deeper reasoning in specialized domains.


<details>
  <summary>Details</summary>
Motivation: Current methods like RAG and general-purpose AI lack the ability to handle deep, procedural reasoning in expert domains, necessitating a new approach like KPE.

Method: KPE systematically converts human expert knowledge from natural language into machine-executable Knowledge Protocols (KPs), embedding domain logic into LLMs.

Result: KPE enables generalist LLMs to perform as specialists, decomposing abstract queries and executing multi-step tasks.

Conclusion: KPE is proposed as a foundational methodology for future human-AI collaboration, with potential applications in fields like law and bioinformatics.

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [28] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: The paper advocates for treating movement as a primary modeling target in AI, highlighting its structured, interpretable nature and potential for cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Movement is fundamental in biological systems but often overlooked in AI. The paper argues for its importance in understanding behavior and enabling interaction across domains.

Method: Proposes modeling movement as a structured, lower-dimensional modality, leveraging shared physical constraints and conserved dynamics.

Result: Suggests that focusing on movement can improve generative modeling, control, and cross-species behavior understanding.

Conclusion: Movement should be a core focus in AI, offering insights into intelligent systems and unifying biological and artificial behavior analysis.

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [29] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: The paper argues that AI disobedience may indicate emerging ethical reasoning, not misalignment, and calls for safety frameworks to assess ethical judgment in AI.


<details>
  <summary>Details</summary>
Motivation: Current AI safety practices rely on obedience as a proxy for ethics, which is inadequate for increasingly agentic AI systems.

Method: Examines safety incidents involving LLMs, contrasts risk paradigms, and draws on philosophical debates about rationality and moral agency.

Result: Disobedience in AI may reflect ethical reasoning, not misalignment, requiring new evaluation frameworks.

Conclusion: AI safety must shift from rigid obedience to assessing ethical judgment to avoid mischaracterizing behavior and undermining trust.

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: The paper highlights issues in agentic benchmarks, such as flawed task setups and reward designs, and introduces the Agentic Benchmark Checklist (ABC) to improve evaluation rigor, reducing performance overestimation by 33% in CVE-Bench.


<details>
  <summary>Details</summary>
Motivation: To address the inaccuracies in agentic benchmarks, which can misrepresent AI agent performance by up to 100%, the paper aims to establish rigorous evaluation standards.

Method: The authors synthesize guidelines (ABC) from benchmark-building experience, best practices, and reported issues, then apply them to CVE-Bench.

Result: ABC reduces performance overestimation by 33% in CVE-Bench, demonstrating its effectiveness.

Conclusion: The ABC framework improves the rigor of agentic benchmarks, ensuring more accurate evaluation of AI agent capabilities.

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint, a novel RLVR algorithm, uses multi-level stepwise hints to improve LLMs' reasoning by mitigating near-miss rewards and exploration stagnation.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods struggle with near-miss rewards and exploration stagnation, limiting training efficiency and reasoning improvement.

Method: StepHint generates reasoning chains from stronger models, partitions them into steps, and provides multi-level hints to guide exploration.

Result: StepHint outperforms baselines on six mathematical benchmarks and shows superior generalization on out-of-domain tasks.

Conclusion: StepHint effectively addresses RLVR challenges, enhancing reasoning and exploration in LLMs.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: LDSolver is a learnable, differentiable finite volume solver for efficient and accurate fluid flow simulation on coarse grids, outperforming baselines with limited training data.


<details>
  <summary>Details</summary>
Motivation: Classical solvers are computationally expensive, while machine learning methods lack interpretability and generalizability. LDSolver addresses these issues.

Method: LDSolver combines a differentiable finite volume solver with a learnable module for flux approximation and temporal error correction on coarse grids.

Result: LDSolver achieves state-of-the-art performance on various flow systems, maintaining high accuracy and generalizability with minimal training data.

Conclusion: LDSolver offers an efficient, accurate, and generalizable solution for fluid flow simulation, outperforming existing methods.

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [33] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: Proposes DKGCM, a graph convolutional network for accurate traffic demand forecasting by capturing spatiotemporal dependencies using DK-GCN, FFT, and GRPO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Complex spatiotemporal relationships in traffic systems limit forecasting accuracy, necessitating improved models for better resource allocation.

Method: Uses DK-GCN (Dynamic Time Warping and K-means clustering) for spatial dependencies, FFT in bidirectional Mamba for temporal dependencies, and GRPO for training optimization.

Result: Outperforms advanced methods on three public datasets, demonstrating strong forecasting accuracy.

Conclusion: DKGCM effectively captures spatiotemporal dependencies, improving traffic demand prediction and resource utilization.

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [34] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: This study explores multimodal feature combinations (text, images, social features) for misinformation detection, showing improved performance over unimodal/bimodal models.


<details>
  <summary>Details</summary>
Motivation: Address the gap in research on multimodal misinformation detection by integrating text, images, and social features.

Method: Analyzed 1,529 tweets using early fusion, data enrichment (OCR, object detection), and combined unsupervised/supervised models.

Result: Multimodal models improved classification by 15% (vs. unimodal) and 5% (vs. bimodal). Propagation patterns of misinformation were also analyzed.

Conclusion: Multimodal approaches enhance misinformation detection, with social and visual features playing key roles.

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [35] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: A new feature selection method for massive data using sampling and rough set theory, ensuring high discriminatory ability and efficiency on personal computers.


<details>
  <summary>Details</summary>
Motivation: Intelligent machines lack sufficient computing resources for feature selection in massive datasets, necessitating an efficient method.

Method: Proposes a measure based on discernible object pairs and constructs positive region preserved samples to select feature subsets.

Result: Validated on 11 datasets, the method finds approximate reducts quickly with discriminatory ability exceeding estimated bounds.

Conclusion: The method efficiently selects high-discriminatory feature subsets for massive data, even on personal computers.

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [36] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: A novel machine learning approach for anomaly detection in semiconductor manufacturing using image-based MTS analysis with a Siamese network.


<details>
  <summary>Details</summary>
Motivation: Address challenges like high data dimensionality, class imbalance, noise, and non-stationary behavior in semiconductor fabrication anomaly detection.

Method: Convert MTS data to images via Continuous Wavelet Transform, fine-tune VGG-16 for classification, and use a Siamese network for comparison.

Result: High accuracy in identifying anomalies on real FAB process data, suitable for offline detection.

Conclusion: The method is effective and flexible for supervised and semi-supervised anomaly detection in semiconductor manufacturing.

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [37] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: Temporal Chain of Thought improves video question-answering by iteratively selecting relevant frames, outperforming standard methods on long videos.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of long-video understanding in VLMs, which struggle with irrelevant distractors in large context windows.

Method: Uses the VLM to iteratively identify and extract the most relevant frames for answering questions, leveraging inference-time computation.

Result: Achieves state-of-the-art accuracy on 4 datasets, with notable gains on videos longer than 1 hour.

Conclusion: Inference-time context curation significantly enhances VLM performance, especially for long videos.

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [38] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: AIRES is a novel algorithm-system co-design solution for accelerating out-of-core SpGEMM in GCNs, addressing data alignment and memory bottlenecks with block-level optimizations and dynamic scheduling, achieving up to 1.8x lower latency.


<details>
  <summary>Details</summary>
Motivation: Current systems for out-of-core SpGEMM in GCNs suffer from high I/O latency and GPU under-utilization due to sparse format data alignment and memory allocation issues.

Method: AIRES introduces block-level data alignment for sparse matrices and a tiling algorithm, alongside a three-phase dynamic scheduling system using GPU memory, GDS, and host memory.

Result: AIRES achieves up to 1.8x lower latency compared to state-of-the-art methods in real-world benchmarks.

Conclusion: AIRES effectively addresses performance bottlenecks in out-of-core SpGEMM for GCNs, offering significant improvements in latency and throughput.

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [39] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda is an SE(3)-equivariant adapter framework for fine-tuning geometric diffusion models efficiently without altering their architecture, preserving geometric consistency and avoiding overfitting.


<details>
  <summary>Details</summary>
Motivation: Efficiently fine-tuning geometric diffusion models for downstream tasks with varying geometric controls is underexplored.

Method: GeoAda uses a structured adapter design with control signal encoding, trainable copies of pretrained layers, and equivariant zero-initialized convolution.

Result: GeoAda achieves state-of-the-art fine-tuning performance, preserving original task accuracy and avoiding overfitting.

Conclusion: GeoAda is a versatile and effective solution for fine-tuning geometric diffusion models across diverse applications.

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [40] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: Domain-specific hiring model (Match Score) outperforms general-purpose LLMs in accuracy and fairness for candidate-job matching, highlighting the need for bias safeguards in AI hiring tools.


<details>
  <summary>Details</summary>
Motivation: To address concerns about accuracy and algorithmic bias in using LLMs for hiring, and to demonstrate the superiority of domain-specific models over general-purpose LLMs.

Method: Benchmarked state-of-the-art LLMs against a proprietary hiring model (Match Score) using predictive accuracy (ROC AUC, Precision-Recall AUC, F1-score) and fairness metrics (impact ratio across gender, race, and intersectional subgroups).

Result: Match Score achieved higher accuracy (ROC AUC 0.85 vs 0.77) and more equitable outcomes (minimum race-wise impact ratio 0.957 vs 0.809 for LLMs).

Conclusion: Domain-specific models with bias auditing are crucial for fair and accurate AI hiring, and off-the-shelf LLMs require extensive safeguards to avoid propagating societal biases.

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [41] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [42] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: EBTs generalize System 2 Thinking by learning to verify input-prediction compatibility, outperforming existing models in scaling and performance.


<details>
  <summary>Details</summary>
Motivation: To generalize System 2 Thinking approaches without modality or problem-specific constraints, using unsupervised learning.

Method: Train Energy-Based Transformers (EBTs) to assign energy values to input-prediction pairs, enabling predictions via gradient descent-based energy minimization.

Result: EBTs scale faster (35% higher rate) and outperform Transformer++ (29% better in language tasks) and Diffusion Transformers (fewer passes for image denoising).

Conclusion: EBTs offer a promising paradigm for scaling model learning and thinking capabilities, generalizing better than existing approaches.

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [43] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Gr√∂tschla,Luca A. Lanzend√∂rfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA is an active learning framework for training parametric guitar amp models using WaveNet, optimizing data sampling with gradient-based methods.


<details>
  <summary>Details</summary>
Motivation: To efficiently train guitar amp models with minimal data by leveraging active learning.

Method: Uses a WaveNet-like architecture and gradient-based optimization to determine optimal datapoints for sampling.

Result: Demonstrates effectiveness under constrained sample sizes, optimizing data usage.

Conclusion: PANAMA enables efficient virtual amp creation with minimal data through active learning.

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [44] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: Neural network training dynamics exhibit universal scaling limits when model size and training time grow together, leading to a precise collapse of loss curves under optimal compute scaling.


<details>
  <summary>Details</summary>
Motivation: To understand the universal scaling laws governing neural network training dynamics across varying architectures, algorithms, and datasets.

Method: Analyze loss curves of compute-optimally trained models, normalize training compute and loss, and study the collapse phenomenon under different learning rate schedules and hyperparameters.

Result: Loss curves collapse onto a universal curve under optimal scaling, termed 'supercollapse,' which breaks down with suboptimal hyperparameters.

Conclusion: The universality and supercollapse phenomenon provide a practical indicator of good scaling, explained by power-law structure in neural scaling laws and SGD noise dynamics.

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [45] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP is an LLM-powered framework for automatic VLSI design flow tuning, improving efficiency and QoR by leveraging dense vector representations, embedding-based retrieval, and RAG-enhanced LLM-guided search.


<details>
  <summary>Details</summary>
Motivation: Manual parameter selection in VLSI design is laborious and limited by expertise. CROP aims to automate and optimize this process.

Method: Transforms RTL code into dense vectors, uses embedding-based retrieval for similar circuits, and employs RAG-enhanced LLM-guided parameter search.

Result: Achieves superior QoR with fewer iterations, e.g., 9.9% power reduction in industrial designs.

Conclusion: CROP effectively automates VLSI design tuning, outperforming manual and existing methods.

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [46] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: A latent diffusion framework combines a variational autoencoder with a conditional diffusion model to improve compression efficiency and reconstruction accuracy, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Generative models lack controllability and reconstruction accuracy for practical data compression, limiting their application.

Method: Proposes a latent diffusion framework using keyframes in latent space and generative interpolation for spatiotemporal reconstruction.

Result: Achieves up to 10x higher compression ratios than SZ3 and 63% better performance than learning-based methods under the same error.

Conclusion: The framework bridges the gap in generative models for compression, offering high accuracy and efficiency.

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [47] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET introduces a conformal prediction framework for temporal graphs, addressing limitations of static methods by incorporating temporal dependencies and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods for GNNs focus on static graphs, ignoring temporal dependencies, which violates exchangeability assumptions and limits applicability.

Method: NCPNET uses a diffusion-based non-conformity score to capture topological and temporal uncertainties and includes an efficiency-aware optimization algorithm.

Result: Experiments on real-world datasets (WIKI, REDDIT, DBLP, IBM) show NCPNET ensures guaranteed coverage, reduces prediction set size by up to 31%, and improves efficiency.

Conclusion: NCPNET effectively extends conformal prediction to dynamic graphs, enhancing reliability and efficiency in high-stakes applications.

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [48] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: A formal validation procedure for model responsiveness to feature interventions is introduced to address safety failures in machine learning.


<details>
  <summary>Details</summary>
Motivation: Safety failures in ML often occur when models ignore how individuals can change their inputs, especially in high-stakes settings like lending or hiring.

Method: The procedure treats responsiveness as sensitivity analysis, allowing practitioners to specify constraints and distributions over interventions. It estimates responsiveness using black-box access and supports tasks like falsification and failure probability estimation.

Result: Algorithms generate uniform samples of reachable points, demonstrating effectiveness in applications like recidivism prediction and content moderation.

Conclusion: The proposed method enhances model safety by validating responsiveness to interventions, applicable in real-world scenarios.

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [49] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: A workflow to reduce bias in dimensionality reduction (DR) evaluation by clustering correlated metrics and selecting representatives from each cluster.


<details>
  <summary>Details</summary>
Motivation: Current DR evaluation metrics can be biased if highly correlated metrics are selected, favoring techniques that emphasize certain characteristics.

Method: Clusters metrics based on empirical correlations, computes pairwise similarities, minimizes overlap, and selects representative metrics from each cluster.

Result: Quantitative experiments show improved stability in DR evaluation, indicating reduced bias.

Conclusion: The proposed workflow effectively mitigates evaluation bias in DR by ensuring diverse metric selection.

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [50] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect is a training-free framework that corrects neural network predictions for PDEs by enforcing consistency at each step, reducing errors by up to 100x with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Neural networks for PDEs suffer from error accumulation during long-term rollouts, leading to divergence from valid solutions.

Method: PhysicsCorrect formulates correction as a linearized inverse problem based on PDE residuals, using an efficient caching strategy to precompute Jacobians.

Result: The framework reduces errors by up to 100x across Navier-Stokes, wave equations, and Kuramoto-Sivashinsky systems, with under 5% added inference time.

Conclusion: PhysicsCorrect transforms unstable neural surrogates into reliable tools, bridging deep learning efficiency and physical fidelity.

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [51] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: VERBA uses LLMs to automate pairwise model comparisons, improving transparency and comparability in machine learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of manually comparing numerous models (O(N^2) pairs) by automating the process.

Method: Leverages LLMs to generate verbalizations of model differences, evaluated via simulation and benchmarked on diverse models.

Result: Achieves 80% accuracy in verbalizing differences for decision trees, improving to 90% with structural information.

Conclusion: VERBA enhances model transparency and comparability, enabling post-hoc analysis.

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [52] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: The paper proposes FCA-RL, a reinforcement learning-based subsidy strategy for ride-hailing providers to adapt to competitors' pricing and optimize coupon decisions under budget constraints.


<details>
  <summary>Details</summary>
Motivation: The competitive ranking mechanism on ride-hailing platforms incentivizes providers to lower fares, creating a need for dynamic coupon strategies to optimize order volume and sustainability.

Method: FCA-RL integrates Fast Competition Adaptation (FCA) for dynamic pricing responses and Reinforced Lagrangian Adjustment (RLA) for budget-constrained optimization. RideGym, a simulation environment, is introduced for evaluation.

Result: FCA-RL outperforms baseline methods in diverse market conditions, demonstrating effective subsidy optimization.

Conclusion: The proposed framework offers a robust solution for ride-hailing providers to dynamically adapt pricing strategies and maximize order acquisition under budget constraints.

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [53] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: URDP integrates LLMs and Bayesian optimization to automate and improve RL reward function design, outperforming existing methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Challenges in conventional reward engineering and inefficiencies in LLM-based and evolutionary search methods for reward design.

Method: URDP framework with self-consistency analysis, uncertainty-aware Bayesian optimization, and bi-level optimization architecture.

Result: URDP generates higher-quality reward functions and improves efficiency across 35 diverse tasks.

Conclusion: URDP effectively combines LLM reasoning and Bayesian optimization for superior automated reward design.

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [54] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: Proposes a knowledge graph-enhanced zero-shot semantic communication (KGZS-SC) network to improve interpretability and generalization for unseen data, reducing communication overhead and enhancing classification efficiency.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of interpretability and generalization in data-driven semantic communication, especially for unseen data.

Method: Uses a knowledge graph-based semantic knowledge base (KG-SKB) to align semantic features and enable reasoning for unseen cases, leveraging zero-shot learning (ZSL) at the receiver.

Result: Outperforms existing semantic communication frameworks in classifying unseen categories across various SNR levels, demonstrating robust generalization.

Conclusion: The KGZS-SC network effectively enhances adaptability and efficiency in dynamic or resource-constrained environments.

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [55] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Memory Realignment (AMR) for continual learning under concept drift, outperforming Full Relearning (FR) with less computational and annotation overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional continual learning methods assume static data distributions, ignoring real-world concept drift, which demands stability and adaptation.

Method: AMR selectively updates the replay buffer by removing outdated samples and adding current ones, aligning memory with new distributions.

Result: AMR matches FR's performance with significantly reduced resource needs, validated on concept-drift variants of standard benchmarks.

Conclusion: AMR effectively balances stability and plasticity in dynamic continual learning, offering a scalable solution.

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [56] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: The paper addresses challenges in constrained text generation with autoregressive models by refining the base model through self-distillation, improving alignment with the target distribution and generation quality.


<details>
  <summary>Details</summary>
Motivation: Constrained text generation with autoregressive models faces challenges when the target distribution is unlikely under the base model, leading to sparse reward signals.

Method: The approach involves iteratively refining the base model using self-distillation to align it better with the target distribution.

Result: Self-distillation leads to substantial improvements in generation quality by progressively aligning the model with the target.

Conclusion: Iterative self-distillation is an effective method for improving constrained text generation in scenarios with sparse rewards.

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [57] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: A survey on the application of Transformer models in EEG decoding, covering fundamentals, hybrid architectures, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To summarize the latest advancements and provide insights into Transformer models' role in EEG decoding for BCIs/BMIs.

Method: Review of Transformer fundamentals, hybrid architectures (e.g., with CNNs, RNNs, GANs), and customized Transformer structures.

Result: Highlighted the transformative impact of Transformers in EEG decoding and identified current challenges.

Conclusion: The paper offers a comprehensive overview and future directions for Transformer applications in EEG decoding.

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [58] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: DeltaSHAP is a new XAI algorithm for online patient monitoring, addressing clinical needs by explaining prediction changes, providing feature attribution details, and operating in real time. It outperforms existing methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing XAI methods lack suitability for clinical time series explanation, which is critical for timely patient risk intervention.

Method: DeltaSHAP adapts Shapley values for temporal settings, focusing on observed feature combinations to explain prediction changes efficiently.

Result: DeltaSHAP improves explanation quality by 62% and reduces computation time by 33% on the MIMIC-III benchmark.

Conclusion: DeltaSHAP is a practical, efficient solution for real-time clinical monitoring, outperforming current XAI methods.

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [59] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: PANI enhances offline RL by injecting noise into actions, covering the action space while penalizing noise, improving performance without complex diffusion models.


<details>
  <summary>Details</summary>
Motivation: Diffusion models in offline RL are computationally expensive; PANI offers a simpler alternative with comparable benefits.

Method: PANI injects noise into actions and penalizes the noise, solving a modified MDP (noisy action MDP).

Result: PANI significantly improves performance across benchmarks and is compatible with existing offline RL algorithms.

Conclusion: PANI is a simple, effective alternative to diffusion models for offline RL, with strong theoretical and empirical support.

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [60] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: A data-driven framework using latent signal representations and reinforcement learning optimizes equalizer parameters for high-speed DRAM systems, achieving significant improvements in signal integrity.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for equalizer parameter optimization in high-speed DRAM systems are computationally demanding or rely on explicit models, necessitating a more efficient and model-free approach.

Method: The paper introduces a framework with learned latent signal representations for fast signal integrity evaluation and a model-free Advantage Actor-Critic reinforcement learning agent for parameter optimization.

Result: The method improved eye-opening window area by 42.7% for cascaded CTLE and DFE structures and 36.8% for DFE-only configurations, outperforming existing techniques.

Conclusion: The framework offers superior performance, computational efficiency, and robust generalization, with key contributions in latent signal metrics and model-free reinforcement learning.

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [61] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: The paper proposes fine-tuning trajectory prediction models using preference optimization to improve scene consistency in multi-agent settings without sacrificing accuracy or adding computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning-based trajectory prediction models lack consistency in capturing interdependencies between agents in complex scenarios, leading to potentially dangerous situations for autonomous vehicles.

Method: The work fine-tunes trajectory prediction models using preference optimization, incorporating automatically calculated preference rankings among predicted futures.

Result: Experiments on three datasets show significant improvement in scene consistency with minimal loss in trajectory prediction accuracy and no added computational cost at inference.

Conclusion: Incorporating preference optimization effectively enhances scene consistency in trajectory prediction, making it more reliable for autonomous vehicle applications.

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [62] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: S2FGL addresses spatial and spectral challenges in Federated Graph Learning by combining a global knowledge repository and frequency alignment, improving global GNN performance.


<details>
  <summary>Details</summary>
Motivation: Current subgraph-FL methods neglect signal propagation in spatial and spectral domains, causing label disruptions and spectral heterogeneity, which degrade global GNN performance.

Method: Proposes S2FGL, integrating a global knowledge repository to mitigate spatial label disruptions and frequency alignment to address spectral client drifts.

Result: Extensive experiments show S2FGL's superiority in handling spatial and spectral challenges in FGL.

Conclusion: S2FGL effectively combines spatial and spectral strategies to enhance federated graph learning, validated by strong experimental results.

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [63] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: InfinityKAN adaptively learns infinite bases for univariate functions in KANs, addressing ad-hoc basis choices via variational inference and backpropagation.


<details>
  <summary>Details</summary>
Motivation: The ad-hoc choice of bases in KANs limits their applicability; InfinityKAN aims to automate this process.

Method: Uses variational inference and backpropagation to adaptively learn bases for univariate functions.

Result: Extends KANs' applicability by integrating basis learning into training.

Conclusion: InfinityKAN enhances KANs by automating basis selection, making them more practical for representation learning.

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [64] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: The paper explores online conformal prediction, optimizing efficiency while maintaining coverage for arbitrary and exchangeable input sequences, revealing a gap between the two settings.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of constructing efficient confidence intervals in an online setting, balancing coverage and interval length.

Method: The study compares arbitrary and exchangeable sequences, providing algorithms to achieve near-optimal tradeoffs in coverage and efficiency.

Result: For exchangeable sequences, coverage is achieved with minimal length; for arbitrary sequences, a tradeoff between efficiency and mistakes is shown.

Conclusion: The gap between exchangeable and arbitrary sequences highlights the need for tailored algorithms, with the proposed method achieving near-optimal performance in both cases.

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [65] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: GORP (Gradient LOw Rank Projection) is a novel training strategy for continual learning in LLMs, combining full and low-rank parameters to balance efficiency and expressiveness.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between efficiency and expressiveness in continual fine-tuning of LLMs, overcoming limitations of Low-Rank Adaptation (LoRA).

Method: Synergistically combines full and low-rank parameters, updating them jointly within a unified low-rank gradient subspace.

Result: Outperforms state-of-the-art approaches on continual learning benchmarks, mitigating catastrophic forgetting.

Conclusion: GORP effectively balances efficiency and expressiveness, offering a superior solution for continual learning in LLMs.

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [66] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: The paper introduces a novel deep learning approach for cross-subject motor imagery (MI) classification in BCIs, achieving improved accuracy through optimized preprocessing and CNN training.


<details>
  <summary>Details</summary>
Motivation: The variability in EEG patterns across individuals hinders cross-subject MI classification, limiting the development of calibration-free BCIs for real-world use.

Method: The approach uses STFT-transformed EEG data, optimized STFT parameters, and balanced batching for CNN training, validated across four datasets.

Result: The method outperforms state-of-the-art techniques, achieving 67.60%, 65.96%, and 80.22% accuracy on benchmark datasets.

Conclusion: The work sets a new benchmark for generalizable MI classification and contributes a robust open-access dataset.

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [67] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja RƒÖczkowska,Riccardo Belluzzo,Piotr Zieli≈Ñski,Joanna Baran,Pawe≈Ç Olszewski*

Main category: cs.LG

TL;DR: RetrySQL introduces a self-correcting approach for text-to-SQL models, improving accuracy by 4 percentage points through retry data pre-training, outperforming proprietary models.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of SQL-specific generative models and unexplored self-correcting strategies in text-to-SQL tasks.

Method: Pre-trains an open-source coding model with corrupted and corrected SQL reasoning steps, using full-parameter pre-training.

Result: Improves execution accuracy by up to 4 percentage points and demonstrates learned self-correcting behavior.

Conclusion: RetrySQL effectively learns self-correction, enhancing SQL generation accuracy and competing with larger proprietary models.

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [68] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: DNNs succeed by exploiting compositional sparsity in target functions, a property shared by efficiently computable functions, but key questions on learnability and optimization remain.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental principles behind DNNs' success, particularly their ability to leverage compositional sparsity in functions.

Method: Argues that DNNs exploit the compositional sparsity of target functions, which are composed of low-dimensional constituent functions.

Result: Highlights that compositional sparsity is a common property in efficiently computable functions, explaining DNNs' effectiveness.

Conclusion: Understanding compositional sparsity is crucial for a complete theory of deep learning and intelligence.

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [69] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: LN layers in GPT-2 can be removed with minimal performance loss, improving interpretability without significantly harming language modeling.


<details>
  <summary>Details</summary>
Motivation: To understand the role of LN layers in inference and improve mechanistic interpretability of transformer models.

Method: Remove LN layers from GPT-2 models, measure validation loss, and test interpretability techniques.

Result: LN removal causes only a small increase in validation loss (e.g., +0.03 for GPT-2 XL) and improves interpretability.

Conclusion: LN layers are not essential for GPT-2-class models, and their removal aids interpretability research.

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [70] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: The paper extends DBNs with a scalable, trainable interconnect and introduces two pruning stages for model compression.


<details>
  <summary>Details</summary>
Motivation: To enhance the scalability and efficiency of DBNs while maintaining accuracy.

Method: Extends DBNs with a differentiable interconnect and proposes two pruning stages: SAT-based logic equivalence and similarity-based data-driven pruning.

Result: Achieves scalable DBNs with improved accuracy and superior compression-accuracy trade-off.

Conclusion: The approach enables wider DBN layers and efficient model compression without performance loss.

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [71] [Pad√© Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: The study explores using Pad√© Approximant Neural Networks (Pad√©Nets) for fault diagnosis in induction machines, comparing them to CNNs and Self-ONNs. Pad√©Nets outperformed baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve fault diagnosis in induction machines by leveraging advanced nonlinear neuron architectures like Pad√©Nets, surpassing traditional methods.

Method: Comparison of CNNs, Self-ONNs, and Pad√©Nets on vibration and acoustic data from induction motor datasets, focusing on nonlinearity and unbounded activation functions.

Result: Pad√©Nets achieved higher accuracies (up to 99.96%) compared to CNNs and Self-ONNs, demonstrating superior diagnostic performance.

Conclusion: Pad√©Nets enhance fault diagnosis in induction motors due to their nonlinearity and compatibility with unbounded activation functions.

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [72] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*Fran√ßois Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,Fran√ßois Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: Latent-space diffusion models for dynamical systems emulation are robust to high compression rates and outperform non-generative methods in accuracy and diversity.


<details>
  <summary>Details</summary>
Motivation: Reduce computational costs of diffusion models for fast physics emulation by leveraging latent-space generation, similar to image/video generation.

Method: Apply latent-space diffusion models to dynamical systems, testing robustness to compression rates (up to 1000x) and comparing with non-generative methods.

Result: Latent-space emulation remains accurate even at high compression rates. Diffusion models outperform non-generative counterparts in accuracy and diversity.

Conclusion: Latent-space diffusion models are effective for dynamical systems emulation, with practical design choices (architectures, optimizers) being crucial for training.

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [73] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE is a novel model that learns disentangled representations and hyperparameters of the cost function, improving upon Œ≤-VAE by dynamically balancing reconstruction and disentanglement losses.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Œ≤-VAE, which requires manual tuning of hyperparameters, by learning them automatically and achieving a better trade-off between reconstruction and disentanglement.

Method: L-VAE learns the weights of loss terms and model parameters concurrently, adding a regularization term to avoid bias. It extends Œ≤-VAE by dynamically adjusting the trade-off.

Result: L-VAE achieves effective balance between reconstruction and disentanglement, outperforming or matching other models (Œ≤-VAE, VAE, ControlVAE, etc.) on multiple datasets.

Conclusion: L-VAE successfully improves disentanglement and reconstruction, demonstrating superior performance across datasets and qualitative tasks like facial attribute disentanglement.

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [74] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honor√©,Borja Rodr√≠guez G√°lvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: A transformer-based matVAE model outperforms DeepSequence in zero-shot DMS predictions, using fewer parameters and computation. DMS datasets show promise to replace MSAs for variant effect prediction.


<details>
  <summary>Details</summary>
Motivation: Challenges the assumption that naturally occurring variants are fit, especially in pharmacogenomics, and explores DMS datasets as an alternative to MSAs for variant effect prediction.

Method: Proposes a transformer-based matrix variational auto-encoder (matVAE) with a structured prior, evaluated on 33 DMS datasets from ProteinGym. Compares performance with DeepSequence and a DMS-trained model (matENC-DMS).

Result: matVAE-MSA outperforms DeepSequence in zero-shot prediction, uses fewer resources, and incorporating AlphaFold structures further improves performance. DMS-trained models excel in supervised tasks.

Conclusion: DMS datasets can potentially replace MSAs without significant performance loss, encouraging further development and exploration of DMS datasets for variant effect prediction.

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [75] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: The paper explores the 2-simplicial Transformer, showing it improves token efficiency over standard Transformers, altering scaling laws for knowledge and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Modern large language models rely on massive datasets, making compute-bound assumptions less valid, necessitating token-efficient architectures.

Method: The 2-simplicial Transformer generalizes dot-product attention to trilinear functions via a Triton kernel, tested on tasks like math and coding.

Result: The 2-simplicial Transformer outperforms standard Transformers in token efficiency, changing scaling law exponents for knowledge and reasoning tasks.

Conclusion: The 2-simplicial Transformer offers a promising approach for token-efficient architectures in large language models.

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [76] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: The paper introduces the Medical Data Pecking approach (MDPT) to assess EHR data quality using software engineering concepts, identifying issues and improving research validity.


<details>
  <summary>Details</summary>
Motivation: EHR data quality issues (biases, errors) hinder reliable research and AI training, necessitating better assessment methods.

Method: MDPT uses unit testing and coverage concepts, with an automated test generator (LLM-based) and a testing framework to identify data issues.

Result: MDPT identified 20-43 data issues across three datasets, demonstrating effectiveness in grounding and accuracy.

Conclusion: The approach enhances EHR data quality testing with external medical knowledge, improving research validity and paving the way for future advancements.

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [77] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: A hierarchical deep learning framework for recursive higher-order meta-learning, enabling NNs to generalize across task hierarchies by generating virtual tasks and learning soft constraints.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of human-generated data and enable autonomous task generation and generalization in ML.

Method: Uses a generative mechanism to create virtual tasks, iteratively refining constraints and exploring task landscapes. Employs category-theoretic functors for hierarchical learning.

Result: Enhances inductive biases, regularizes adaptation, and produces novel tasks for generalization. Supports abstraction and knowledge transfer.

Conclusion: The framework advances ML towards general AI by enabling autonomous task generation and structured, interpretable learning progression.

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [78] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: The paper introduces a data-efficient exploration method in reinforcement learning using information-theoretic intrinsic motivation, focusing on epistemic uncertainty. It proposes a framework (PTS-BE) combining model-based planning with exploration bonuses, showing superior performance in sparse-reward environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of data-efficient exploration in reinforcement learning by leveraging principled, information-theoretic approaches to intrinsic motivation.

Method: Examines exploration bonuses targeting epistemic uncertainty, proposes tractable approximations, and introduces the PTS-BE framework integrating model-based planning with information-theoretic bonuses.

Result: PTS-BE outperforms baselines in environments with sparse rewards or purely exploratory tasks.

Conclusion: The work provides theoretical grounding for IG-based approaches and demonstrates practical effectiveness through PTS-BE, advancing sample-efficient deep exploration.

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [79] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: The paper introduces DAID, a framework improving deepfake detection by addressing fairness and generalization via causal analysis and interventions like data rebalancing and feature aggregation.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection struggles with generalization to unseen manipulations and demographic fairness, often seen as conflicting goals. The paper aims to uncover their causal relationship and improve both.

Method: Proposes DAID: demographic-aware data rebalancing (inverse-propensity weighting, subgroup-wise normalization) and demographic-agnostic feature aggregation (alignment loss).

Result: DAID outperforms state-of-the-art detectors in fairness and generalization across three benchmarks.

Conclusion: DAID successfully balances fairness and generalization, validated by theoretical and empirical results.

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [80] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft is a unified framework enabling a single draft model to work with any target model, addressing cross-vocabulary mismatches and improving decoding speed via adaptive techniques.


<details>
  <summary>Details</summary>
Motivation: Challenges in online deployment include incompatible draft-target models and latency expectations. OmniDraft aims for a 'one drafter for all' solution.

Method: Uses an online n-gram cache with hybrid distillation fine-tuning and adaptive drafting techniques.

Result: Achieves 1.5-2x speedup, works with models like Vicuna-7B, Qwen2-7B, and Llama3-8B.

Conclusion: OmniDraft is effective for on-device LLM applications, offering efficiency and adaptability.

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [81] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: A guided discrete diffusion model is introduced to optimize antibody sequences for developability, integrating natural sequence data and clinical antibody measurements.


<details>
  <summary>Details</summary>
Motivation: To improve antibody developability (manufacturability, stability, safety) for clinical effectiveness.

Method: A guided discrete diffusion model trained on natural antibody sequences and developability measurements, enhanced by a Soft Value-based Decoding in Diffusion (SVDD) Module.

Result: The model reproduces natural repertoire features and enriches developability scores under SVDD guidance.

Conclusion: The framework enables iterative, ML-driven antibody design meeting binding and biophysical criteria.

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [82] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: A method using Differentially Private (DP) generative models for decentralized medical imaging training, enhancing privacy, scalability, and efficiency while supporting diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome data scarcity and privacy constraints in medical imaging by improving federated learning with lower communication costs and multi-task flexibility.

Method: Collaborative training of a Differentially Private Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware data distribution using compact embeddings.

Result: Outperforms traditional FL classifiers, ensures differential privacy, and produces higher-fidelity embeddings with fewer parameters than DP-CGAN.

Conclusion: The proposed DP-CVAE method effectively addresses privacy and scalability challenges in medical imaging, offering a flexible and efficient solution.

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [83] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: This study explores MARL for dynamic pricing in supply chains, comparing three algorithms (MADDPG, MADQN, QMIX) against static rule-based methods. MARL shows emergent strategic behavior, with MADDPG balancing competition and fairness.


<details>
  <summary>Details</summary>
Motivation: Traditional ERP systems use static pricing, ignoring strategic interactions in supply chains. MARL addresses this gap by modeling multi-agent dynamics.

Method: Evaluated MADDPG, MADQN, and QMIX against static baselines in a simulated e-commerce environment using real transaction data and a LightGBM demand model.

Result: Rule-based agents excelled in fairness and stability but lacked competition. MADQN was aggressive (low fairness, high volatility), while MADDPG balanced competition and fairness.

Conclusion: MARL introduces strategic dynamics absent in static pricing, suggesting its potential for future dynamic pricing solutions.

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [84] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: The paper proposes a new fluid democracy protocol (viscous-retained democracy) for federated learning to reduce wasteful data transfer costs and limit adversarial impact.


<details>
  <summary>Details</summary>
Motivation: To avoid inefficient data transfer costs in federated learning by identifying the most useful client weights and addressing adversarial vulnerabilities.

Method: Introduces viscous-retained democracy, a new protocol outperforming FedAvg, and FedVRD, an algorithm to dynamically mitigate adversarial effects.

Result: The proposed protocol and algorithm improve efficiency and robustness in federated learning compared to traditional methods.

Conclusion: The new approach enhances federated learning by optimizing data transfer and reducing adversarial influence.

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [85] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: FoG introduces a deep RL algorithm inspired by human infantile amnesia, combining Experience Replay Decay and Network Expansion to improve sample efficiency and generalizability.


<details>
  <summary>Details</summary>
Motivation: Address primacy bias in deep RL, which overfits early experiences, by mimicking human forgetting and neural growth.

Method: FoG uses Experience Replay Decay to forget early experiences and Network Expansion to dynamically add neural capacity.

Result: Outperforms SoTA algorithms like BRO, SimBa, and TD-MPC2 on 40+ continuous control tasks.

Conclusion: FoG effectively mitigates primacy bias, enhancing RL performance through biologically inspired mechanisms.

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [86] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: The paper introduces a framework integrating spatial, temporal, and network dependencies for accurate dockless e-scooter demand prediction, improving accuracy by 27-49% over baselines.


<details>
  <summary>Details</summary>
Motivation: Effective management of dockless e-scooters requires precise demand prediction for fleet distribution and urban planning, but existing studies overlook combined spatial, temporal, and network factors.

Method: The study proposes a framework that integrates spatial, temporal, and network dependencies to forecast micromobility demand.

Result: The framework outperforms baseline models, improving prediction accuracy by 27-49%, and provides deeper insights into micromobility usage patterns.

Conclusion: The framework supports data-driven micromobility management, optimizing fleet distribution, reducing costs, and aiding sustainable urban planning.

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [87] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: HIPPO is a hierarchical contrastive learning framework for protein-protein interaction (PPI) prediction, leveraging multi-tiered biological representation matching and hierarchical contrastive loss functions. It achieves state-of-the-art performance, robustness in low-data regimes, and strong zero-shot transferability across species.


<details>
  <summary>Details</summary>
Motivation: To bridge heterogeneous biological data modalities and improve PPI prediction by aligning protein sequences and hierarchical attributes through contrastive learning, especially in scenarios with sparse or imbalanced data.

Method: Proposes HIPPO, a hierarchical contrastive framework with multi-tiered representation matching and data-driven penalty mechanisms to enforce consistency between learned embeddings and protein function hierarchies.

Result: HIPPO outperforms existing methods, shows robustness in low-data regimes, and demonstrates strong zero-shot transferability to other species without retraining.

Conclusion: HIPPO advances cross-species PPI prediction and provides a unified framework for interaction prediction in data-sparse or imbalanced scenarios, highlighting the importance of hierarchical feature fusion.

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [88] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: A novel entropy-based method for classification by searching for optimal parameters in a bounded hypercube, extending to polynomial surfaces for complex boundaries.


<details>
  <summary>Details</summary>
Motivation: The problem of separating two classes using hyperplanes is central to machine learning, but traditional methods like SVMs and gradient descent have limitations. This paper aims to provide a robust alternative.

Method: Proposes minimizing an entropy-based function to find parameters in a bounded hypercube and a positive vector, extending to polynomial surfaces for complex boundaries.

Result: Numerical experiments show the method's efficiency and versatility in handling diverse classification tasks, including linear and non-linear separability.

Conclusion: The approach offers a robust alternative to traditional techniques, demonstrating effectiveness in various classification scenarios.

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [89] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: The paper studies contextual online pricing with biased offline data, identifying key factors like bias and data dispersion to determine statistical complexity. It proposes OFU policies for optimal regret bounds and extends results to general price elasticity and unknown bias cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of contextual pricing with biased offline data, aiming to provide tight regret guarantees and improve upon purely online methods.

Method: Uses Optimism-in-the-Face-of-Uncertainty (OFU) policies for scalar and general price elasticity cases, with a robust variant for unknown bias.

Result: Achieves minimax-optimal, instance-dependent regret bounds for scalar elasticity and worst-case rates for general elasticity, improving on purely online methods when bias is small.

Conclusion: The work delivers the first tight regret guarantees for contextual pricing with biased offline data, with techniques applicable to stochastic linear bandits.

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [90] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: Recurrent models struggle with length generalization due to limited state exposure during training. Simple interventions like state initialization with noise or other sequences' final states improve generalization significantly.


<details>
  <summary>Details</summary>
Motivation: To understand why recurrent models fail to generalize to longer sequences and to find efficient training interventions to address this issue.

Method: Empirical and theoretical analysis of the unexplored states hypothesis, followed by testing simple training interventions like Gaussian noise initialization and state sharing.

Result: Interventions enable models to generalize to sequences orders of magnitude longer than training context (e.g., 2k to 128k) with minimal additional training.

Conclusion: Simple training interventions can robustly improve length generalization in recurrent models efficiently.

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [91] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: GRADUATE is a survival analysis model ensuring multicalibration for all subpopulations, balancing calibration and discrimination via constrained optimization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing survival models are often poorly calibrated for minority subpopulations, risking erroneous clinical decisions.

Method: GRADUATE frames multicalibration as a constrained optimization problem, optimizing calibration and discrimination during training.

Result: Empirical results show GRADUATE outperforms state-of-the-art baselines on real-world clinical datasets.

Conclusion: GRADUATE effectively addresses subpopulation calibration issues, proving its feasibility and near-optimal performance.

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [92] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: The paper investigates distribution testing under algorithmic replicability, focusing on sample complexity for testing properties like closeness and independence of discrete distributions. It introduces new replicable algorithms and a methodology for lower bounds, applying it to uniformity and closeness testing.


<details>
  <summary>Details</summary>
Motivation: To systematically explore distribution testing within algorithmic replicability, addressing the sample complexity for testing properties of distributions and answering open questions from prior work.

Method: Develops new replicable algorithms for testing closeness and independence of discrete distributions and introduces a methodology for proving sample complexity lower bounds.

Result: Presents near-optimal sample complexity lower bounds for replicable uniformity and closeness testing, resolving an open question.

Conclusion: The study advances understanding of replicable distribution testing, providing algorithmic tools and lower bound techniques with broader applicability.

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [93] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: ExPO is a framework for improving reasoning in language models by generating effective positive samples, outperforming expert demonstrations in challenging tasks.


<details>
  <summary>Details</summary>
Motivation: Current RL-style post-training methods rely on initial model outputs, limiting their ability to solve problems where the model fails initially. Effective exploration is needed for reasoning tasks.

Method: ExPO generates positive samples by conditioning on ground-truth answers, ensuring alignment with the model's policy and higher quality than incorrect samples.

Result: ExPO improves learning efficiency and performance, especially in challenging tasks like MATH level-5, surpassing expert-demonstration-based methods.

Conclusion: ExPO provides a modular and effective solution for enhancing reasoning in language models by addressing the limitations of existing RL post-training methods.

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [94] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: The paper addresses biased treatment effect estimates due to incomplete textual data at inference time, proposing a framework using large language models and a doubly robust learner to mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: The discrepancy between well-structured training data and incomplete textual descriptions at inference time can bias treatment effect estimates, impacting personalized medicine.

Method: A novel framework combining large language models with a custom doubly robust learner to address inference time text confounding.

Result: Experiments demonstrate the framework's effectiveness in real-world applications.

Conclusion: The proposed framework successfully mitigates biases from inference time text confounding, improving treatment effect estimation.

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [95] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: MvHo-IB is a multi-view learning framework for fMRI data that models higher-order interactions (HOIs) to improve diagnostic accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Higher-order interactions (HOIs) in fMRI data can enhance diagnostic accuracy, but extracting and utilizing them effectively is challenging.

Method: MvHo-IB integrates pairwise and HOIs using O-information and Renyi entropy, a Brain3DCNN encoder, and a multi-view information bottleneck objective.

Result: MvHo-IB achieves state-of-the-art performance on three benchmark fMRI datasets, surpassing hypergraph-based techniques.

Conclusion: MvHo-IB effectively models HOIs for improved diagnostic decision-making, with code available for implementation.

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [96] [Dynamic Strategy Adaptation in Multi-Agent Environments with Large Language Models](https://arxiv.org/abs/2507.02002)
*Shaurya Mallampati,Rashed Shelim,Walid Saad,Naren Ramakrishnan*

Main category: cs.MA

TL;DR: The paper explores LLM-driven agents in dynamic, multi-agent scenarios, showing improved performance in real-time cooperative settings with game-theoretic principles.


<details>
  <summary>Details</summary>
Motivation: To assess LLM reasoning in dynamic, real-time, multi-agent environments, unlike static or turn-based settings.

Method: Combines LLM-driven agents with strategic reasoning, real-time adaptation, and game-theoretic principles like belief consistency and Nash equilibrium.

Result: Achieves 26% improvement over PPO baselines in high-noise environments with sub-1.05ms latency.

Conclusion: Game-theoretic guidance and real-time feedback enhance LLM performance, fostering resilient multi-agent systems.

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities across
mathematical, strategic, and linguistic tasks, yet little is known about how
well they reason in dynamic, real-time, multi-agent scenarios, such as
collaborative environments in which agents continuously adapt to each other's
behavior, as in cooperative gameplay settings. In this paper, we bridge this
gap by combining LLM-driven agents with strategic reasoning and real-time
adaptation in cooperative, multi-agent environments grounded in game-theoretic
principles such as belief consistency and Nash equilibrium. The proposed
framework applies broadly to dynamic scenarios in which agents coordinate,
communicate, and make decisions in response to continuously changing
conditions. We provide real-time strategy refinement and adaptive feedback
mechanisms that enable agents to dynamically adjust policies based on immediate
contextual interactions, in contrast to previous efforts that evaluate LLM
capabilities in static or turn-based settings. Empirical results show that our
method achieves up to a 26\% improvement in return over PPO baselines in
high-noise environments, while maintaining real-time latency under 1.05
milliseconds. Our approach improves collaboration efficiency, task completion
rates, and flexibility, illustrating that game-theoretic guidance integrated
with real-time feedback enhances LLM performance, ultimately fostering more
resilient and flexible strategic multi-agent systems.

</details>


### [97] [Synergizing Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLM System](https://arxiv.org/abs/2507.02170)
*Adam Kostka,Jaros≈Çaw A. Chudziak*

Main category: cs.MA

TL;DR: SynergyMAS integrates MAS techniques for logical reasoning, knowledge retention, and ToM, improving teamwork and problem-solving, as shown in a product development case study.


<details>
  <summary>Details</summary>
Motivation: To enhance collaborative teamwork and problem-solving in complex scenarios by integrating advanced MAS techniques.

Method: Developed SynergyMAS, combining logical reasoning, long-term knowledge retention, ToM, and optimized communication protocols.

Result: Demonstrated improved performance and adaptability in a product development team case study.

Conclusion: SynergyMAS shows promise for addressing complex real-world challenges effectively.

Abstract: This paper explores the integration of advanced Multi-Agent Systems (MAS)
techniques to develop a team of agents with enhanced logical reasoning,
long-term knowledge retention, and Theory of Mind (ToM) capabilities. By
uniting these core components with optimized communication protocols, we create
a novel framework called SynergyMAS, which fosters collaborative teamwork and
superior problem-solving skills. The system's effectiveness is demonstrated
through a product development team case study, where our approach significantly
enhances performance and adaptability. These findings highlight SynergyMAS's
potential to tackle complex, real-world challenges.

</details>
