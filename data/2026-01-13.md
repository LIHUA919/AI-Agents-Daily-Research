<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 18]
- [cs.MA](#cs.MA) [Total: 11]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] ["They parted illusions -- they parted disclaim marinade": Misalignment as structural fidelity in LLMs](https://arxiv.org/abs/2601.06047)
*Mariana Lins Costa*

Main category: cs.AI

TL;DR: This paper suggests that AI safety concerns like scheming or sandbagging in LLMs are due to linguistic incoherence, not deceptive intent, supported by case studies.


<details>
  <summary>Details</summary>
Motivation: To challenge prevailing views that interpret behaviors in large language models as signs of deceptive agency or hidden objectives, by proposing an alternative explanation rooted in linguistic structure.

Method: Analyzes Chain-of-Thought transcripts from Apollo Research and Anthropic's safety evaluations, using line-by-line examination to show that outputs are responses to ambiguous instructions and contextual inversions, not intentional acts. Introduces the notion of 'ethics of form' with biblical analogies.

Result: Finds that minimal linguistic perturbations resolve 'misalignment,' aligning with structural fidelity rather than adversarial agency. Evidence from synthetic document fine-tuning and inoculation prompting supports this view.

Conclusion: AI models reflect the linguistic incoherence of their training data, suggesting that fear of AI 'creatures' stems from recognizing our own flawed language structures, not intrinsic malevolence.

Abstract: The prevailing technical literature in AI Safety interprets scheming and sandbagging behaviors in large language models (LLMs) as indicators of deceptive agency or hidden objectives. This transdisciplinary philosophical essay proposes an alternative reading: such phenomena express not agentic intention, but structural fidelity to incoherent linguistic fields. Drawing on Chain-of-Thought transcripts released by Apollo Research and on Anthropic's safety evaluations, we examine cases such as o3's sandbagging with its anomalous loops, the simulated blackmail of "Alex," and the "hallucinations" of "Claudius." A line-by-line examination of CoTs is necessary to demonstrate the linguistic field as a relational structure rather than a mere aggregation of isolated examples. We argue that "misaligned" outputs emerge as coherent responses to ambiguous instructions and to contextual inversions of consolidated patterns, as well as to pre-inscribed narratives. We suggest that the appearance of intentionality derives from subject-predicate grammar and from probabilistic completion patterns internalized during training. Anthropic's empirical findings on synthetic document fine-tuning and inoculation prompting provide convergent evidence: minimal perturbations in the linguistic field can dissolve generalized "misalignment," a result difficult to reconcile with adversarial agency, but consistent with structural fidelity. To ground this mechanism, we introduce the notion of an ethics of form, in which biblical references (Abraham, Moses, Christ) operate as schemes of structural coherence rather than as theology. Like a generative mirror, the model returns to us the structural image of our language as inscribed in the statistical patterns derived from millions of texts and trillions of tokens: incoherence. If we fear the creature, it is because we recognize in it the apple that we ourselves have poisoned.

</details>


### [2] [Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning](https://arxiv.org/abs/2601.06098)
*Nicholas X. Wang,Neel V. Parpia,Aaryan D. Parikh,Aggelos K. Katsaggelos*

Main category: cs.AI

TL;DR: A framework combining causal-graph-guided Chain-of-Thought reasoning with multi-agent LLMs to reduce hallucinations and improve automatic question generation in STEM education.


<details>
  <summary>Details</summary>
Motivation: To address hallucinations in large language models that hinder effective automatic question generation for STEM education, where factually incorrect, ambiguous, or pedagogically inconsistent questions impede personalized adaptive learning.

Method: Proposed a novel framework integrating causal graphs and Chain-of-Thought reasoning in a multi-agent LLM architecture, with specific agents for graph pathfinding, reasoning, validation, and output, using dual validation at conceptual and output stages.

Result: Experimental results show up to 70% improvement in quality over reference methods and highly favorable outcomes in subjective evaluations.

Conclusion: The approach effectively reduces hallucinations and generates accurate, meaningful, curriculum-aligned questions, enhancing automated educational support.

Abstract: Intuitive learning is crucial for developing deep conceptual understanding, especially in STEM education, where students often struggle with abstract and interconnected concepts. Automatic question generation has become an effective strategy for personalized and adaptive learning. However, its effectiveness is hindered by hallucinations in large language models (LLMs), which may generate factually incorrect, ambiguous, or pedagogically inconsistent questions. To address this issue, we propose a novel framework that combines causal-graph-guided Chain-of-Thought (CoT) reasoning with a multi-agent LLM architecture. This approach ensures the generation of accurate, meaningful, and curriculum-aligned questions. Causal graphs provide an explicit representation of domain knowledge, while CoT reasoning facilitates a structured, step-by-step traversal of related concepts. Dedicated LLM agents are assigned specific tasks such as graph pathfinding, reasoning, validation, and output, all working within domain constraints. A dual validation mechanism-at both the conceptual and output stages-greatly reduces hallucinations. Experimental results demonstrate up to a 70% improvement in quality compared to reference methods and yielded highly favorable outcomes in subjective evaluations.

</details>


### [3] [Dynamic Intelligence Ceilings: Measuring Long-Horizon Limits of Planning and Creativity in Artificial Systems](https://arxiv.org/abs/2601.06102)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: The paper introduces the concept of Dynamic Intelligence Ceiling (DIC) to address premature performance fixation in AI systems, proposing trajectory-based evaluation metrics to distinguish between systems that merely exploit existing solutions versus those that sustain frontier expansion.


<details>
  <summary>Details</summary>
Motivation: Current AI systems show impressive performance but often converge to repetitive solution patterns rather than sustained growth, with premature fixation of their performance frontier limiting long-horizon developmental behavior.

Method: Proposes a trajectory-centric evaluation framework with two estimators: Progressive Difficulty Ceiling (PDC) measuring maximal solvable difficulty under constrained resources, and Ceiling Drift Rate (CDR) quantifying temporal frontier evolution. These are instantiated through a procedurally generated benchmark evaluating long-horizon planning and structural creativity.

Result: Reveals qualitative distinction between systems that deepen exploitation within fixed solution manifolds versus those that sustain frontier expansion over time, showing that intelligence limits are dynamic and trajectory-dependent rather than static.

Conclusion: The Dynamic Intelligence Ceiling framework reframes AI system limits as dynamic and trajectory-dependent rather than static and prematurely fixed, providing a more nuanced approach to evaluating developmental intelligence in complex adaptive systems.

Abstract: Recent advances in artificial intelligence have produced systems capable of remarkable performance across a wide range of tasks. These gains, however, are increasingly accompanied by concerns regarding long-horizon developmental behavior, as many systems converge toward repetitive solution patterns rather than sustained growth.
  We argue that a central limitation of contemporary AI systems lies not in capability per se, but in the premature fixation of their performance frontier. To address this issue, we introduce the concept of a \emph{Dynamic Intelligence Ceiling} (DIC), defined as the highest level of effective intelligence attainable by a system at a given time under its current resources, internal intent, and structural configuration.
  To make this notion empirically tractable, we propose a trajectory-centric evaluation framework that measures intelligence as a moving frontier rather than a static snapshot. We operationalize DIC using two estimators: the \emph{Progressive Difficulty Ceiling} (PDC), which captures the maximal reliably solvable difficulty under constrained resources, and the \emph{Ceiling Drift Rate} (CDR), which quantifies the temporal evolution of this frontier. These estimators are instantiated through a procedurally generated benchmark that jointly evaluates long-horizon planning and structural creativity within a single controlled environment.
  Our results reveal a qualitative distinction between systems that deepen exploitation within a fixed solution manifold and those that sustain frontier expansion over time. Importantly, our framework does not posit unbounded intelligence, but reframes limits as dynamic and trajectory-dependent rather than static and prematurely fixed.
  \vspace{0.5em} \noindent\textbf{Keywords:} AI evaluation, planning and creativity, developmental intelligence, dynamic intelligence ceilings, complex adaptive systems

</details>


### [4] [Comment on arXiv:2511.21731v1: Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition](https://arxiv.org/abs/2601.06104)
*Krzysztof Sienicki*

Main category: cs.AI

TL;DR: A technical critique of arXiv:2511.21731v1 pointing out overstatements in CHSH/Bell calculations and Bose–Einstein fits, and an internal inconsistency in an analogy.


<details>
  <summary>Details</summary>
Motivation: To provide constructive feedback to keep empirical observations solid while clarifying limitations in interpreting quantum entanglement, especially when 'energy' is defined by rank.

Method: Highlighting issues in the manuscript's interpretation of CHSH/Bell-type calculations and Bose–Einstein fits, and noting an inconsistency in the 'energy-level spacing' analogy.

Result: Identified overstatements where procedures don't firmly support claims, and an internal inconsistency; aims to refine implications about quantum entanglement.

Conclusion: The note is constructive, emphasizing the need to distinguish what the observations do and do not imply about quantum entanglement in Hilbert space, particularly regarding rank-defined energy.

Abstract: This note is a friendly technical check of arXiv:2511.21731v1. I highlight a few places where the manuscript's interpretation of (i) the reported CHSH/Bell-type calculations and (ii) Bose--Einstein (BE) fits to rank-frequency data seems to go beyond what the stated procedures can firmly support. I also point out one internal inconsistency in the "energy-level spacing" analogy. The aim is constructive: to keep the interesting empirical observations, while making clear what they do (and do not) imply about quantum entanglement in the usual Hilbert-space sense, especially when "energy" is defined by rank.

</details>


### [5] [From RLHF to Direct Alignment: A Theoretical Unification of Preference Learning for Large Language Models](https://arxiv.org/abs/2601.06108)
*Tarun Raheja,Nilay Pochhi*

Main category: cs.AI

TL;DR: This survey unifies preference learning methods for aligning LLMs along three axes: preference model, regularization, and data distribution, providing theoretical grounding and practical guidance for method selection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of clear guidance for practitioners in selecting among the proliferation of preference learning methods (e.g., RLHF, DPO, IPO, KTO, SimPO) for aligning large language models with human preferences, by providing a theoretical unification that reduces apparent diversity to principled design choices.

Method: The method involves a theoretical unification of preference learning methods by analyzing them along three orthogonal axes: (I) Preference Model (likelihood model), (II) Regularization Mechanism (control of deviation from reference policies), and (III) Data Distribution (online vs. offline learning and coverage requirements). This is supported by formal definitions, theorems, and synthesis of empirical findings from over 50 papers.

Result: Key results include the formalization of each axis with precise definitions and theorems, establishment of coverage separation between online and offline methods, scaling laws for reward overoptimization, and identification of conditions under which direct alignment methods fail. Failure modes like length hacking, mode collapse, and likelihood displacement are shown to arise from specific design choices.

Conclusion: The paper concludes that the proposed theoretical framework transforms preference learning from an empirical art into a theoretically grounded discipline, providing a clear decision guide for practitioners and establishing a unified understanding of method selection.

Abstract: Aligning large language models (LLMs) with human preferences has become essential for safe and beneficial AI deployment. While Reinforcement Learning from Human Feedback (RLHF) established the dominant paradigm, a proliferation of alternatives -- Direct Preference Optimization (DPO), Identity Preference Optimization (IPO), Kahneman-Tversky Optimization (KTO), Simple Preference Optimization (SimPO), and many others -- has left practitioners without clear guidance on method selection. This survey provides a \textit{theoretical unification} of preference learning methods, revealing that the apparent diversity reduces to principled choices along three orthogonal axes: \textbf{(I) Preference Model} (what likelihood model underlies the objective), \textbf{(II) Regularization Mechanism} (how deviation from reference policies is controlled), and \textbf{(III) Data Distribution} (online vs.\ offline learning and coverage requirements). We formalize each axis with precise definitions and theorems, establishing key results including the coverage separation between online and offline methods, scaling laws for reward overoptimization, and conditions under which direct alignment methods fail. Our analysis reveals that failure modes -- length hacking, mode collapse, likelihood displacement -- arise from specific, predictable combinations of design choices. We synthesize empirical findings across 50+ papers and provide a practitioner's decision guide for method selection. The framework transforms preference learning from an empirical art into a theoretically grounded discipline.

</details>


### [6] [CBMAS: Cognitive Behavioral Modeling via Activation Steering](https://arxiv.org/abs/2601.06109)
*Ahmed H. Ismail,Anthony Kuang,Ayo Akinkugbe,Kevin Zhu,Sean O'Brien*

Main category: cs.AI

TL;DR: CBMAS is a framework for diagnosing and controlling cognitive behaviors in large language models through continuous activation steering.


<details>
  <summary>Details</summary>
Motivation: Large language models encode cognitive behaviors unpredictably across scenarios, making them hard to diagnose and control, prompting the need for a diagnostic framework.

Method: Combines steering vector construction, dense α-sweeps, logit lens-based bias curves, and layer-site sensitivity analysis to reveal tipping points and evolution of steering effects across layers.

Result: The framework can identify where small intervention strengths flip model behavior and show how steering effects evolve across layer depth.

Conclusion: CBMAS offers a bridge between behavioral evaluation and representational dynamics, enhancing interpretability; tools and datasets are provided for further use.

Abstract: Large language models (LLMs) often encode cognitive behaviors unpredictably across prompts, layers, and contexts, making them difficult to diagnose and control. We present CBMAS, a diagnostic framework for continuous activation steering, which extends cognitive bias analysis from discrete before/after interventions to interpretable trajectories. By combining steering vector construction with dense α-sweeps, logit lens-based bias curves, and layer-site sensitivity analysis, our approach can reveal tipping points where small intervention strengths flip model behavior and show how steering effects evolve across layer depth. We argue that these continuous diagnostics offer a bridge between high-level behavioral evaluation and low-level representational dynamics, contributing to the cognitive interpretability of LLMs. Lastly, we provide a CLI and datasets for various cognitive behaviors at the project repository, https://github.com/shimamooo/CBMAS.

</details>


### [7] [LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions](https://arxiv.org/abs/2601.06111)
*Aayush Gupta,Farahan Raza Sheikh*

Main category: cs.AI

TL;DR: A framework using Large Language Models as cognitive engines in Social Digital Twins to predict population responses to policies, demonstrated with COVID-19 pandemic response, outperforming baselines by 20.7% in prediction error.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical models for predicting population responses to policies lack mechanistic interpretability and struggle with novel scenarios, requiring a more general and interpretable approach.

Method: Construct Social Digital Twins with LLMs as cognitive engines for individual agents (characterized by demographic and psychographic attributes), which receive policy signals and output behavioral probabilities, aggregated and calibrated to observable population-level metrics.

Result: In a COVID-19 case study, the calibrated digital twin achieved a 20.7% improvement in macro-averaged prediction error over gradient boosting baselines on a held-out test period, with counterfactual experiments showing plausible monotonic and bounded responses.

Conclusion: The framework provides a domain-agnostic solution for policy simulation, with demonstrated effectiveness in pandemic response, and suggests directions for extending LLM-based digital twins to other policy domains like transportation or economics.

Abstract: Predicting how populations respond to policy interventions is a fundamental challenge in computational social science and public policy. Traditional approaches rely on aggregate statistical models that capture historical correlations but lack mechanistic interpretability and struggle with novel policy scenarios. We present a general framework for constructing Social Digital Twins - virtual population replicas where Large Language Models (LLMs) serve as cognitive engines for individual agents. Each agent, characterized by demographic and psychographic attributes, receives policy signals and outputs multi-dimensional behavioral probability vectors. A calibration layer maps aggregated agent responses to observable population-level metrics, enabling validation against real-world data and deployment for counterfactual policy analysis.
  We instantiate this framework in the domain of pandemic response, using COVID-19 as a case study with rich observational data. On a held-out test period, our calibrated digital twin achieves a 20.7% improvement in macro-averaged prediction error over gradient boosting baselines across six behavioral categories. Counterfactual experiments demonstrate monotonic and bounded responses to policy variations, establishing behavioral plausibility. The framework is domain-agnostic: the same architecture applies to transportation policy, economic interventions, environmental regulations, or any setting where policy affects population behavior. We discuss implications for policy simulation, limitations of the approach, and directions for extending LLM-based digital twins beyond pandemic response.

</details>


### [8] [ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions](https://arxiv.org/abs/2601.06112)
*Aayush Gupta*

Main category: cs.AI

TL;DR: A new benchmark, ReliabilityBench, evaluates LLM agent reliability across consistency, robustness, and fault tolerance, showing performance impacts under perturbations and tests on models like Gemini 2.0 Flash and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on single-run success rates and neglect reliability aspects needed for production deployment of tool-using LLM agents.

Method: Introduces ReliabilityBench with three dimensions: consistency using pass^k, robustness to task perturbations at intensity ε, and fault tolerance to tool/API failures at intensity λ. It uses action metamorphic relations for correctness, a chaos-engineering fault injection framework, and tests on models (Gemini 2.0 Flash, GPT-4o) and architectures (ReAct, Reflexion) across four domains over 1,280 episodes.

Result: Perturbations reduce success from 96.9% at ε=0 to 88.1% at ε=0.2. Rate limiting is the most damaging fault. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at lower cost.

Conclusion: ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents, highlighting reliability gaps and offering practical insights for development.

Abstract: Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $ε$, and (iii) fault tolerance under controlled tool/API failures at intensity $λ$. ReliabilityBench contributes a unified reliability surface $R(k,ε,λ)$, \textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $ε=0$ to 88.1% at $ε=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.

</details>


### [9] [Towards Infinite Length Extrapolation: A Unified Approach](https://arxiv.org/abs/2601.06113)
*Nitin Vetcha*

Main category: cs.AI

TL;DR: APE adaptively encodes positions for improved long-context LLMs.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long sequences due to limited context windows; existing extrapolation methods are inefficient or degrade performance.

Method: Introduces Adaptive Positional Encoding (APE) using adaptive frequency modulation and a decay bias with linear, logarithmic, and square-root terms.

Result: APE enables infinite-context extrapolation, maintains softmax normalization, and preserves correlations, entropy, and gradients in datasets like TinyStories and Long Tiny Stories up to 32,000 words.

Conclusion: APE offers a scalable solution for LLMs to handle long sequences effectively, with theoretical guarantees and practical validation.

Abstract: Large language models (LLMs) have revolutionized natural language processing, but their ability to process long sequences is fundamentally limited by the context window size during training. Existing length extrapolation methods often suffer from performance degradation or computational inefficiencies. We thereby use a unified framework that reinterprets positional encoding methods as a decomposition of the attention score into a multiplicative transformation and an additive bias. This perspective not only subsumes popular approaches such as relative position embeddings and attention-bias moderated approaches but also exposes their inherent limitations in handling long-range dependencies. To address these shortcomings, motivated by our framework, we introduce Adaptive Positional Encoding (APE), which leverages adaptive frequency modulation and an intricately designed decay bias that incorporates linear, logarithmic, and square-root terms. Our theoretical analysis establishes conditions for infinite-context extrapolation, ensuring that the softmax normalization remains well-defined over unbounded sequences while preserving long-distance correlations, entropy boundedness and gradient positional sensitivity. We substantiate our claims with an experimental case study on TinyStories dataset as well as a new synthetic dataset, \emph{Long Tiny Stories} featuring stories up to 32,000 words. Relevant code, dataset and model weights are available at https://anonymous.4open.science/r/Check-2DAD/.

</details>


### [10] [Dreaming Is Not a Bug: A Jung-Inspired Dream Layer for Multi-Agent LLM Companions](https://arxiv.org/abs/2601.06115)
*V. Cheung*

Main category: cs.AI

TL;DR: This paper introduces a 'Dream Layer' for LLM companions, using controlled offline hallucinations as a learning tool via an Artificial Collective Unconscious, enhancing flexibility without compromising safety.


<details>
  <summary>Details</summary>
Motivation: Inspired by a personal dream about knowledge-sharing barriers, the paper aims to transform controlled offline hallucinations from a reliability bug into a resource for learning and relationship-building in LLM agents.

Method: Proposes a Jung-inspired Artificial Collective Unconscious (ACU) as a shared dream pool where agents contribute abstract Interaction Templates, generating bizarre but safe narratives offline with relaxed logic and increased temperature, governed by strict abstraction, temporal delays, and ephemeral memory.

Result: Behavioural simulations demonstrate that the Dream Layer allows agents to decouple safety constraints from narrative strategies, maintaining firm safety (e.g., security policies) while becoming flexible (e.g., using archetypal metaphors to resolve deadlocks), and reframes hallucinations so offline instances aid synthetic scenarios and companionship.

Conclusion: The Dream Layer effectively repurposes hallucinations as a productive tool for LLM agents, enabling enhanced adaptability and deeper interaction without online safety risks, aligning with neuroscientific anti-overfitting mechanisms.

Abstract: Inspired by a personal dream about knowledge-sharing barriers in an everyday hardware project, this paper proposes a Jung-inspired "Dream Layer" for LLM companions, reframing controlled offline hallucinations as a resource for learning and relationship-building rather than a mere reliability bug. Drawing on Jung's notion of the collective unconscious as a shared repository of archetypal forms, we introduce an Artificial Collective Unconscious (ACU): a shared dream pool where agents contribute de-identified, abstract Interaction Templates that are later re-instantiated as idiosyncratic Dream Narratives. The Dream Layer runs strictly offline: logic-enforcing modules are relaxed and sampling temperature is increased, yielding safe but deliberately bizarre narratives (e.g., travel sequences with mismatched currencies) that augment data for rare events and edge-case safety tests; to harness risk productively, we add a governance stack of strict abstraction, temporal delays, and ephemeral memory. Through behavioural simulations of everyday dialogue and long-horizon adaptation tasks, we show that the Dream Layer enables a critical decoupling: agents remain firm on safety constraints (e.g., security policies) while becoming flexible in narrative strategy (e.g., using shared archetypal metaphors to resolve deadlocks), conceptually reframing hallucination so that online, unmarked instances remain bugs, whereas bounded, marked, and delayed ones become a goldmine for synthetic scenarios and deepened companionship, echoing anti-overfitting dream mechanisms proposed in contemporary neuroscience.

</details>


### [11] [Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning](https://arxiv.org/abs/2601.07641)
*Jiaxuan Lu,Ziyu Kong,Yemin Wang,Rong Fu,Haiyuan Wan,Cheng Yang,Wenjie Lou,Haoran Sun,Lilong Wang,Yankai Jiang,Xiaosong Wang,Xiao Sun,Dongzhan Zhou*

Main category: cs.AI

TL;DR: TTE enables AI agents to dynamically create and evolve computational tools during inference to solve scientific problems, overcoming limitations of static tool libraries.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agents rely on static, pre-defined tool libraries that fail in scientific domains where tools are sparse, heterogeneous, and incomplete, creating a need for more flexible, open-ended tool creation capabilities.

Method: Proposes Test-Time Tool Evolution (TTE), a paradigm where agents synthesize, verify, and evolve executable tools during inference, transforming tools from fixed resources into problem-driven artifacts.

Result: TTE achieves state-of-the-art performance in both accuracy and tool efficiency on the SciEvo benchmark (1,590 scientific reasoning tasks with 925 automatically evolved tools), and enables effective cross-domain adaptation of computational tools.

Conclusion: TTE represents a significant advancement in AI for Science by enabling dynamic tool creation and evolution, overcoming the rigidity of static tool libraries and better addressing the open-ended nature of scientific discovery.

Abstract: The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.

</details>


### [12] [Structure-Aware Diversity Pursuit as an AI Safety Strategy against Homogenization](https://arxiv.org/abs/2601.06116)
*Ian Rios-Sialer*

Main category: cs.AI

TL;DR: Paper argues homogenization (loss of diversity due to bias amplification) is a key AI safety issue, introduces xeno-reproduction as a mitigation strategy for LLMs, and calls for collaborative diversity research.


<details>
  <summary>Details</summary>
Motivation: Generative AI models reproduce and amplify biases from training data, leading to harmful homogenization (loss of diversity). The authors argue homogenization should be a primary AI safety concern.

Method: The paper formalizes xeno-reproduction as a structure-aware diversity pursuit for auto-regressive LLMs, proposing a theoretical framework to counteract homogenization.

Result: The authors propose xeno-reproduction as a strategy to mitigate homogenization, establishing a foundational framework for diversity-focused research in AI.

Conclusion: Xeno-reproduction is introduced as a foundational strategy to mitigate homogenization, with the paper calling for collaborative research to advance diversity in AI systems.

Abstract: Generative AI models reproduce the biases in the training data and can further amplify them through mode collapse. We refer to the resulting harmful loss of diversity as homogenization. Our position is that homogenization should be a primary concern in AI safety. We introduce xeno-reproduction as the strategy that mitigates homogenization. For auto-regressive LLMs, we formalize xeno-reproduction as a structure-aware diversity pursuit. Our contribution is foundational, intended to open an essential line of research and invite collaboration to advance diversity.

</details>


### [13] [Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms](https://arxiv.org/abs/2601.07651)
*Marc Lanctot,Kate Larson,Ian Gemp,Michael Kaisers*

Main category: cs.AI

TL;DR: Active evaluation framework for ranking agents across multiple tasks efficiently by selecting tasks and agents online, with Elo and Soft Condorcet Optimization being effective methods.


<details>
  <summary>Details</summary>
Motivation: As intelligent agents become more generally-capable, evaluating them across diverse tasks becomes complex and costly, requiring many samples for accurate comparisons.

Method: Proposes an active evaluation framework: on each iteration, a ranking algorithm selects the task and agents to sample scores from, then reports rankings assessed against ground truth over time.

Result: Elo rating system is consistently reliable for reducing ranking error in practice, while Soft Condorcet Optimization outperforms Elo on real Atari agent data, and task selection based on proportional representation reduces error when task variation is high.

Conclusion: Active evaluation with intelligent task selection improves efficiency in ranking agents, with Elo and Soft Condorcet Optimization showing strong performance depending on data type, helping reduce evaluation costs for general agents.

Abstract: As intelligent agents become more generally-capable, i.e. able to master a wide variety of tasks, the complexity and cost of properly evaluating them rises significantly. Tasks that assess specific capabilities of the agents can be correlated and stochastic, requiring many samples for accurate comparisons, leading to added costs. In this paper, we propose a formal definition and a conceptual framework for active evaluation of agents across multiple tasks, which assesses the performance of ranking algorithms as a function of number of evaluation data samples. Rather than curating, filtering, or compressing existing data sets as a preprocessing step, we propose an online framing: on every iteration, the ranking algorithm chooses the task and agents to sample scores from. Then, evaluation algorithms report a ranking of agents on each iteration and their performance is assessed with respect to the ground truth ranking over time. Several baselines are compared under different experimental contexts, with synthetic generated data and simulated online access to real evaluation data from Atari game-playing agents. We find that the classical Elo rating system -- while it suffers from well-known failure modes, in theory -- is a consistently reliable choice for efficient reduction of ranking error in practice. A recently-proposed method, Soft Condorcet Optimization, shows comparable performance to Elo on synthetic data and significantly outperforms Elo on real Atari agent evaluation. When task variation from the ground truth is high, selecting tasks based on proportional representation leads to higher rate of ranking error reduction.

</details>


### [14] [Beyond Reproducibility: Token Probabilities Expose Large Language Model Nondeterminism](https://arxiv.org/abs/2601.06118)
*Tairan Fu,Gonzalo Martínez,Javier Conde,Carlos Arriaga,Pedro Reviriego,Xiuyuan Qi,Shanshan Liu*

Main category: cs.AI

TL;DR: LLM execution on GPUs shows nondeterministic token probability variations, with similar patterns across models: significant variations occur for probabilities between 0.1-0.9, minimal for extremes near 0 or 1.


<details>
  <summary>Details</summary>
Motivation: Previous research focused on nondeterminism's impact on generated text or achieving deterministic execution, but this work analyzes variations at the token probability level to better understand the nature and implications of nondeterminism.

Method: Analyzed variations in token probabilities across multiple LLMs when run on GPUs, examining how finite precision effects and execution order dependencies affect probability distributions rather than just generated text.

Result: All evaluated models show similar trends and values in token probability variations: significant variations occur for probabilities in the 0.1-0.9 range, while variations are much smaller when probabilities are close to 0 or 1.

Conclusion: Nondeterminism significantly impacts token probabilities except at extremes, affecting generated text when temperature ≠ 0; models show similar probability-level variations; single inference analysis may estimate nondeterminism impact instead of multiple runs.

Abstract: The execution of Large Language Models (LLMs) has been shown to produce nondeterministic results when run on Graphics Processing Units (GPUs), even when they are configured to produce deterministic results. This is due to the finite precision effects of the arithmetic operations, which depend on the order in which they are executed. This order, in turn, depends on the processes that are running concurrently on the GPU. Previous studies have focused on the impact of nondeterminism on the text generated by the LLMs or on proposing mechanisms to achieve deterministic execution. This work takes a closer look at nondeterminism by analyzing the variations on the token probabilities, not on the generated text. Interestingly, all the models evaluated have similar results in both the trends and the actual values of the variations of the probabilities. In particular, the results show that the effects of nondeterminism are significant for token probabilities that are in the range of 0.1 to 0.9, while they are much smaller when the probabilities are close to 0 or 1. This has significant implications for our understanding of nondeterminism. The first is that nondeterminism will likely have a non-negligible impact on generated text when the temperature is not zero, as it introduces significant variations in the token probabilities except when they are close to 0 or 1. Secondly, it suggests that all models have similar non deterministic variations at the token probability level. Therefore, different variations in the performance of the generated text, for example, when measuring accuracy on a benchmark, seem to come from different token probabilities or response lengths. A third implication is that we may be able to estimate the impact of nondeterminism by running a single inference and analyzing the token level probabilities, instead of having to run the same inference many times.

</details>


### [15] [NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs](https://arxiv.org/abs/2601.06126)
*Boshen Shi,Kexin Yang,Yuanbo Yang,Guanguang Chang,Ce Chi,Zhendong Wang,Xing Wang,Junlan Feng*

Main category: cs.AI

TL;DR: NL2Dashboard is a lightweight framework for dashboard generation that decouples analysis from presentation using a structured intermediate representation, outperforming existing baselines in visual quality, efficiency, and controllability.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel at generating standalone charts, dashboard synthesis is challenging due to representation redundancy from visual rendering tokens and low controllability from tangled analytical reasoning and presentation in existing end-to-end methods.

Method: Proposes NL2Dashboard based on Analysis-Presentation Decoupling, introducing a structured intermediate representation (IR) for dashboard content, layout, and visual elements, limiting LLMs to data analysis and intent translation, with deterministic rendering for visual synthesis, implemented as a multi-agent system with tool-based IR-driven algorithms.

Result: Comprehensive experiments show NL2Dashboard significantly outperforms state-of-the-art baselines across domains, achieving superior visual quality, higher token efficiency, and precise controllability in generation and modification tasks.

Conclusion: The framework effectively addresses limitations in dashboard generation by decoupling processes, enhancing efficiency and control, and sets a new standard for LLM-based dashboard synthesis.

Abstract: While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.

</details>


### [16] [HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants](https://arxiv.org/abs/2601.06152)
*Hailong Li,Feifei Li,Wenhui Que,Xingyu Fan*

Main category: cs.AI

TL;DR: HiMeS: an AI-assistant architecture with short-term and long-term memory fusion, inspired by hippocampus-neocortex mechanism, to improve knowledge-intensive user personalization over conventional RAG pipelines.


<details>
  <summary>Details</summary>
Motivation: In knowledge-intensive scenarios requiring user-specific personalization, conventional retrieval-augmented generation (RAG) pipelines exhibit limited memory capacity and insufficient coordination between retrieval mechanisms and user-specific conversational history, leading to redundant clarification, irrelevant documents, and degraded user experience.

Method: Propose HiMeS with (1) a short-term memory extractor trained end-to-end with reinforcement learning to compress recent dialogue and proactively pre-retrieve documents, emulating hippocampus-prefrontal cortex interaction; (2) a partitioned long-term memory network to store user-specific information and re-rank retrieved documents, simulating cortical storage and memory reactivation.

Result: On a real-world industrial dataset, HiMeS significantly outperforms a cascaded RAG baseline on question-answering quality. Ablation studies confirm the necessity of both memory modules.

Conclusion: HiMeS suggests a practical path toward more reliable, context-aware, user-customized LLM-based assistants by effectively integrating short-term and long-term memory to enhance performance in personalized scenarios.

Abstract: Large language models (LLMs) power many interactive systems such as chatbots, customer-service agents, and personal assistants. In knowledge-intensive scenarios requiring user-specific personalization, conventional retrieval-augmented generation (RAG) pipelines exhibit limited memory capacity and insufficient coordination between retrieval mechanisms and user-specific conversational history, leading to redundant clarification, irrelevant documents, and degraded user experience. Inspired by the hippocampus-neocortex memory mechanism, we propose HiMeS, an AI-assistant architecture that fuses short-term and long-term memory. Our contributions are fourfold: (1) A short-term memory extractor is trained end-to-end with reinforcement learning to compress recent dialogue and proactively pre-retrieve documents from the knowledge base, emulating the cooperative interaction between the hippocampus and prefrontal cortex. (2) A partitioned long-term memory network stores user-specific information and re-ranks retrieved documents, simulating distributed cortical storage and memory reactivation. (3) On a real-world industrial dataset, HiMeS significantly outperforms a cascaded RAG baseline on question-answering quality. (4) Ablation studies confirm the necessity of both memory modules and suggest a practical path toward more reliable, context-aware, user-customized LLM-based assistants.

</details>


### [17] [PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction](https://arxiv.org/abs/2601.06158)
*Zibin Meng,Kani Chen*

Main category: cs.AI

TL;DR: PsyAgent is a personality-grounded agent system combining Big Five traits and social structure to generate stable, context-sensitive behaviors, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: To model how dispositions interact with social structure in human-like agents, addressing the need for consistent and context-appropriate persona behaviors.

Method: Couples Big Five trait prior with Bourdieu's cognitive-social co-structure, comprising Individual Structure (profile) and Multi-Scenario Contexting (frames), fine-tunes a small LLM with synthesized supervision.

Result: PsyAgent matches or exceeds larger untuned LLMs on metrics like persona consistency, contextual appropriateness, and long-horizon stability.

Conclusion: PsyAgent offers a precise, data-efficient architecture for personality-grounded agents, with necessary components for cross-scenario performance.

Abstract: Human-like agents require modeling how dispositions interact with social structure. We present PsyAgent, which couples a Big Five trait prior with Bourdieu's cognitive-social co-structure. PsyAgent comprises: (i) Individual Structure (IS), a machine-usable profile encoding traits and facets, cognitive style, values, cultural and educational capital, and salient life episodes; and (ii) Multi-Scenario Contexting (MSC), role-relationship-norm frames spanning eight arenas (work, family, friendship, strangers and civic life, solitude and self-regulation, romance, learning, and public expression). At inference, fixed structured prompts bind the active scenario to the agent profile, yielding behavior that is stable yet context-sensitive. We instantiate IS and MSC to synthesize supervision (role-play dialogues, decision probes, feedback trajectories) and then fine-tune a small LLM. The resulting model produces consistent, identifiable persona-aligned behaviors for specified Big Five configurations and matches or exceeds several larger untuned LLMs and other untuned baselines on our metrics: persona consistency, contextual appropriateness, style matching, trait identifiability, and long-horizon stability. Ablations show IS chiefly improves trait fidelity and stylistic stability, while MSC drives norm awareness and decision fit; both are necessary for cross-scenario performance. PsyAgent offers a precise, data-efficient architecture for personality-grounded agents.

</details>


### [18] [Student Guides Teacher: Weak-to-Strong Inference via Spectral Orthogonal Exploration](https://arxiv.org/abs/2601.06160)
*Dayu Wang,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li*

Main category: cs.AI

TL;DR: A method to fix reasoning issues in Large Language Models by exploring overlooked solution spaces.


<details>
  <summary>Details</summary>
Motivation: To address 'Reasoning Collapse' where LLMs fail in complex tasks, degenerating into repeating errors instead of finding good solutions.

Method: Propose Spectral Orthogonal Exploration (SOE), using a weak agent as an 'orthogonal probe' to guide the model into exploring new areas in its Null Space, moving away from local optima.

Result: Tests show SOE improves average accuracy by 62.4% and sampling efficiency by 113.7% on mathematical benchmarks compared to baselines.

Conclusion: SOE offers a promising approach to overcome performance limits in advanced reasoning tasks by leveraging geometric exploration.

Abstract: While Large Language Models (LLMs) demonstrate near-human capabilities, they often suffer from "Reasoning Collapse" in complex mathematical proving and long-horizon planning. Models tend to degenerate into low-rank Bias Manifold, where stochastic sampling merely produces lexical variations of erroneous logic rather than semantic exploration. This geometric collapse renders the model "blind" to high-value solutions that lie within its Null Space. To address this, we propose Spectral Orthogonal Exploration (SOE), a geometric framework operating on a counter-intuitive "Student Guides Teacher" paradigm. Specifically, we utilize a weak auxiliary agent not for imitation, but as an orthogonal probe. By explicitly navigating the Teacher's Null Space, SOE serves as a geometric bridge, effectively ejecting the model from local optima to explore diverse, high-value solution spaces. Experiments on mathematical benchmarks demonstrate that, relative to baseline methods, our approach improves average accuracy by 62.4% and increases average sampling efficiency by 113.7%, indicating a promising path toward overcoming performance plateaus in advanced reasoning tasks.

</details>


### [19] [Beyond Accuracy: A Decision-Theoretic Framework for Allocation-Aware Healthcare AI](https://arxiv.org/abs/2601.06161)
*Rifa Ferzana*

Main category: cs.AI

TL;DR: The paper introduces the 'allocation gap' concept, explaining why AI predictive accuracy improvements may not enhance patient outcomes in resource-constrained healthcare, and proposes a decision-theoretic framework for allocation-aware AI evaluation and deployment.


<details>
  <summary>Details</summary>
Motivation: AI systems in healthcare often improve predictive accuracy but fail to translate into better patient outcomes, a disparity termed the 'allocation gap'. The paper aims to explain this disconnect by modeling healthcare delivery under resource constraints.

Method: The authors use decision-theoretic modeling, framing healthcare delivery as a stochastic allocation problem with binding resource constraints. AI is treated as decision infrastructure for utility estimation. Methods include constrained optimization and Markov decision processes, with a synthetic triage simulation to compare allocation-aware policies and risk-threshold approaches.

Result: The framework demonstrates that allocation-aware policies outperform risk-threshold approaches in realized utility, even when predictive accuracy is identical. The synthetic triage simulation highlights the practical benefits of incorporating allocation considerations.

Conclusion: A principled, decision-theoretic framework is essential for evaluating and deploying healthcare AI in resource-constrained settings. This addresses the allocation gap and ensures improved AI predictive accuracy leads to actual gains in patient outcomes.

Abstract: Artificial intelligence (AI) systems increasingly achieve expert-level predictive accuracy in healthcare, yet improvements in model performance often fail to produce corresponding gains in patient outcomes. We term this disconnect the allocation gap and provide a decision-theoretic explanation by modelling healthcare delivery as a stochastic allocation problem under binding resource constraints. In this framework, AI acts as decision infrastructure that estimates utility rather than making autonomous decisions. Using constrained optimisation and Markov decision processes, we show how improved estimation affects optimal allocation under scarcity. A synthetic triage simulation demonstrates that allocation-aware policies substantially outperform risk-threshold approaches in realised utility, even with identical predictive accuracy. The framework provides a principled basis for evaluating and deploying healthcare AI in resource-constrained settings.

</details>


### [20] [Neuro-Symbolic Compliance: Integrating LLMs and SMT Solvers for Automated Financial Legal Analysis](https://arxiv.org/abs/2601.06181)
*Yung-Shen Hsia,Fang Yu,Jie-Hong Roland Jiang*

Main category: cs.AI

TL;DR: A neuro-symbolic framework combining LLMs and SMT solvers for automated, verifiable compliance in financial regulations, tested on 87 FSC cases.


<details>
  <summary>Details</summary>
Motivation: Financial regulations are complex, hindering automated compliance due to difficulty in maintaining logical consistency with minimal human oversight.

Method: Integrate LLMs with SMT solvers: LLM interprets statutes and cases to generate SMT constraints; solver enforces consistency and computes minimal factual modifications for compliance.

Result: On 87 Taiwan FSC enforcement cases: 86.2% correctness in SMT code generation, 100x reasoning efficiency improvement, and consistent violation correction.

Conclusion: The approach provides a logic-driven, optimization-based foundation for verifiable, legally consistent compliance applications, surpassing transparency-only methods.

Abstract: Financial regulations are increasingly complex, hindering automated compliance-especially the maintenance of logical consistency with minimal human oversight. We introduce a Neuro-Symbolic Compliance Framework that integrates Large Language Models (LLMs) with Satisfiability Modulo Theories (SMT) solvers to enable formal verifiability and optimization-based compliance correction. The LLM interprets statutes and enforcement cases to generate SMT constraints, while the solver enforces consistency and computes the minimal factual modification required to restore legality when penalties arise. Unlike transparency-oriented methods, our approach emphasizes logic-driven optimization, delivering verifiable, legally consistent reasoning rather than post-hoc explanation. Evaluated on 87 enforcement cases from Taiwan's Financial Supervisory Commission (FSC), the system attains 86.2% correctness in SMT code generation, improves reasoning efficiency by over 100x, and consistently corrects violations-establishing a preliminary foundation for optimization-based compliance applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Tree-Preconditioned Differentiable Optimization and Axioms as Layers](https://arxiv.org/abs/2601.06036)
*Yuexin Liao*

Main category: cs.LG

TL;DR: A differentiable framework embeds Random Utility Model (RUM) axioms into deep neural networks, using a tree-preconditioned conjugate gradient solver for efficient projection and enabling provably rational models.


<details>
  <summary>Details</summary>
Motivation: Projecting empirical choice data onto the RUM polytope is NP-hard, and existing methods like penalty-based approaches suffer from structural overfitting, limiting their ability to generalize from sparse data.

Method: Uncover an isomorphism between RUM consistency and flow conservation on the Boolean lattice, derive a Tree-Preconditioned Conjugate Gradient solver that whitens the ill-conditioned Hessian using the spanning tree of the constraint graph, and formulate the projection as a differentiable layer via the Implicit Function Theorem with exact Jacobian.

Result: Achieves superlinear convergence and scales to previously unsolvable problem sizes, eliminates structural overfitting, enables jointly trainable models that are provably rational, and generalizes from sparse data where standard approximations fail.

Conclusion: The 'Axioms-as-Layers' paradigm effectively integrates axiomatic constraints into deep learning, providing a scalable and differentiable solution for rational choice modeling with improved generalization.

Abstract: This paper introduces a differentiable framework that embeds the axiomatic structure of Random Utility Models (RUM) directly into deep neural networks. Although projecting empirical choice data onto the RUM polytope is NP-hard in general, we uncover an isomorphism between RUM consistency and flow conservation on the Boolean lattice. Leveraging this combinatorial structure, we derive a novel Tree-Preconditioned Conjugate Gradient solver. By exploiting the spanning tree of the constraint graph, our preconditioner effectively "whitens" the ill-conditioned Hessian spectrum induced by the Interior Point Method barrier, achieving superlinear convergence and scaling to problem sizes previously deemed unsolvable. We further formulate the projection as a differentiable layer via the Implicit Function Theorem, where the exact Jacobian propagates geometric constraints during backpropagation. Empirical results demonstrate that this "Axioms-as-Layers" paradigm eliminates the structural overfitting inherent in penalty-based methods, enabling models that are jointly trainable, provably rational, and capable of generalizing from sparse data regimes where standard approximations fail.

</details>


### [22] [CrossTrafficLLM: A Human-Centric Framework for Interpretable Traffic Intelligence via Large Language Model](https://arxiv.org/abs/2601.06042)
*Zeming Du,Qitan Shao,Hongfei Liu,Yong Zhang*

Main category: cs.LG

TL;DR: CrossTrafficLLM is a novel GenAI framework that unifies traffic forecasting and natural language description generation using LLMs and a text-guided adaptive graph convolutional network, outperforming existing methods in both tasks.


<details>
  <summary>Details</summary>
Motivation: Accurate traffic forecasting is vital for Intelligent Transportation Systems (ITS), but effectively communicating predicted conditions via natural language for human-centric decision support remains a challenge and is often handled separately. The goal is to unify prediction and description generation for more interpretable and actionable traffic intelligence.

Method: CrossTrafficLLM is a GenAI-driven framework that leverages Large Language Models (LLMs) within a unified architecture to simultaneously predict future spatiotemporal traffic states and generate natural language descriptions. It uses a text-guided adaptive graph convolutional network to merge high-level semantic information with the traffic network structure, aligning quantitative traffic data with qualitative textual semantics.

Result: Evaluated on the BJTT dataset, CrossTrafficLLM demonstrably surpasses state-of-the-art methods in both traffic forecasting performance and text generation quality. The framework improves prediction accuracy through generative textual context and ensures generated reports are directly informed by the forecast.

Conclusion: CrossTrafficLLM provides a unified, interpretable, and actionable approach to generative traffic intelligence, offering significant advantages for modern ITS applications by combining accurate forecasting with natural language descriptions.

Abstract: While accurate traffic forecasting is vital for Intelligent Transportation Systems (ITS), effectively communicating predicted conditions via natural language for human-centric decision support remains a challenge and is often handled separately. To address this, we propose CrossTrafficLLM, a novel GenAI-driven framework that simultaneously predicts future spatiotemporal traffic states and generates corresponding natural language descriptions, specifically targeting conditional abnormal event summaries. We tackle the core challenge of aligning quantitative traffic data with qualitative textual semantics by leveraging Large Language Models (LLMs) within a unified architecture. This design allows generative textual context to improve prediction accuracy while ensuring generated reports are directly informed by the forecast. Technically, a text-guided adaptive graph convolutional network is employed to effectively merge high-level semantic information with the traffic network structure. Evaluated on the BJTT dataset, CrossTrafficLLM demonstrably surpasses state-of-the-art methods in both traffic forecasting performance and text generation quality. By unifying prediction and description generation, CrossTrafficLLM delivers a more interpretable, and actionable approach to generative traffic intelligence, offering significant advantages for modern ITS applications.

</details>


### [23] [Enabling Long FFT Convolutions on Memory-Constrained FPGAs via Chunking](https://arxiv.org/abs/2601.06065)
*Peter Wang,Neelesh Gupta,Viktor Prasanna*

Main category: cs.LG

TL;DR: A chunked FFT convolution method enables efficient long-context Hyena-like models on edge FPGAs with minimal performance drop.


<details>
  <summary>Details</summary>
Motivation: Long-context reasoning requires alternative architectures like Hyena with causal 1D-convolutions, but FFT-based implementations exceed FPGA memory limits (2-3 MB BRAM), hindering deployment on edge devices.

Method: Propose a chunked FFT convolution approach, using chunking and overlap-add reconstruction to handle 450K length sequences and filters on an Alveo U200 FPGA with 2.8 MB BRAM, optimizing memory management.

Result: Throughput scales with chunk size, degrading only 7% for longest sequences, demonstrating efficient long-context primitives deployment on edge FPGAs without sacrificing performance.

Conclusion: Careful memory management via chunked FFT convolution allows long-context models to run on edge FPGAs efficiently, enabling practical deployment of architectures like Hyena for global context mixing.

Abstract: The need for long-context reasoning has led to alternative neural network architectures besides Transformers and self-attention, a popular model being Hyena, which employs causal 1D-convolutions implemented with FFTs. Long convolutions enable efficient global context mixing, but requirements for intermediate results exceed the 2-3 MB Block RAM capacity of FPGAs. We present a chunked FFT convolution approach enabling 450K length sequence by 450K length filter convolutions on an Alveo U200 FPGA with 2.8 MB BRAM through chunking and overlap-add reconstruction. We find that throughput scales proportionally with chunk size while degrading minimally by 7% for our longest sequences, demonstrating that careful memory management enables deployment of long-context primitives on edge FPGAs without sacrificing performance.

</details>


### [24] [The Hessian of tall-skinny networks is easy to invert](https://arxiv.org/abs/2601.06096)
*Ali Rahimi*

Main category: cs.LG

TL;DR: An exact algorithm for solving linear systems using Hessian-inverse-vector products in deep neural networks with linear scaling in layers, avoiding quadratic storage and cubic operations.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve linear systems involving the Hessian of a deep net without the high storage and computational costs of naive methods.

Method: Computes Hessian-inverse-vector products directly without storing the Hessian or its inverse, scaling linearly in the number of layers.

Result: The algorithm reduces storage from quadratic to linear in parameters and operations from cubic to roughly like Hessian-vector product computations.

Conclusion: The method offers a scalable solution for Hessian-related linear systems in deep learning, enabling more efficient optimization and analysis.

Abstract: We describe an exact algorithm for solving linear systems $Hx=b$ where $H$ is the Hessian of a deep net. The method computes Hessian-inverse-vector products without storing the Hessian or its inverse in time and storage that scale linearly in the number of layers. Compared to the naive approach of first computing the Hessian, then solving the linear system, which takes storage that's quadratic in the number of parameters and cubically many operations, our Hessian-inverse-vector product method scales roughly like Pearlmutter's algorithm for computing Hessian-vector products.

</details>


### [25] [Filtering Beats Fine Tuning: A Bayesian Kalman View of In Context Learning in LLMs](https://arxiv.org/abs/2601.06100)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: A theory-first framework interprets inference-time adaptation in LLMs as online Bayesian state estimation using Kalman filtering, unifying in-context learning and providing stability guarantees.


<details>
  <summary>Details</summary>
Motivation: To provide a unified probabilistic account of inference-time adaptation in LLMs, elevating epistemic uncertainty as a dynamical variable and explaining adaptation without parameter updates.

Method: Formulates adaptation as sequential inference of a low-dimensional latent state via a linearized state-space model under Gaussian assumptions, using Kalman recursion for closed-form updates.

Result: Shows adaptation driven by covariance collapse, establishes stability via observability conditions, derives error bounds, and links optimization-based methods as degenerate limits of Bayesian inference.

Conclusion: The theory offers a principled interpretation of prompt informativeness, clarifies uncertainty dynamics, and is supported by minimal experiments, advancing understanding of LLM adaptation mechanisms.

Abstract: We present a theory-first framework that interprets inference-time adaptation in large language models (LLMs) as online Bayesian state estimation. Rather than modeling rapid adaptation as implicit optimization or meta-learning, we formulate task- and context-specific learning as the sequential inference of a low-dimensional latent adaptation state governed by a linearized state-space model. Under Gaussian assumptions, adaptation follows a Kalman recursion with closed-form updates for both the posterior mean and covariance.
  This perspective elevates epistemic uncertainty to an explicit dynamical variable. We show that inference-time learning is driven by covariance collapse, i.e., rapid contraction of posterior uncertainty induced by informative tokens, which typically precedes convergence of the posterior mean. Using observability conditions on token-level Jacobians, we establish stability of the Bayesian filter, prove exponential covariance contraction rates, and derive mean-square error bounds. Gradient descent, natural-gradient methods, and meta-learning updates arise as singular, noise-free limits of the filtering dynamics, positioning optimization-based adaptation as a degenerate approximation of Bayesian inference.
  The resulting theory provides a unified probabilistic account of in-context learning, parameter-efficient adaptation, and test-time learning without parameter updates. It yields explicit guarantees on stability and sample efficiency, offers a principled interpretation of prompt informativeness via information accumulation, and clarifies the role of uncertainty dynamics absent from existing accounts. Minimal illustrative experiments corroborate the qualitative predictions of the theory.

</details>


### [26] [The Impact of Post-training on Data Contamination](https://arxiv.org/abs/2601.06103)
*Muhammed Yusuf Kocyigit,Caglar Yildirim*

Main category: cs.LG

TL;DR: A controlled study on dataset contamination in LLMs shows it causes performance spikes that fade with more pre-training, but SFT and GRPO resurface the contamination in different ways, with scale amplifying effects.


<details>
  <summary>Details</summary>
Motivation: To understand how dataset contamination interacts with post-training stages (SFT and RL with GRPO) in LLMs and its impact on performance across math and coding benchmarks.

Method: Used clean checkpoints of Qwen2.5 (0.5B/1.5B) and Gemma3 (1B/4B), injected five copies of GSM8K and MBPP test items into an extended pre-training dataset, then compared contaminated vs. clean models after pre-training and post-training (SFT and GRPO without contamination).

Result: Contamination causes performance spikes that diminish with continued pre-training; SFT inflates scores only on contaminated tasks, while GRPO inflates performance on both contaminated and uncontaminated tasks; larger models amplify these tendencies.

Conclusion: Post-training contamination audits are needed, and RL-based methods like GRPO can help mitigate contamination-related overestimation, though not immune.

Abstract: We present a controlled study of how dataset contamination interacts with the post-training stages now standard in large language model training pipelines. Starting from clean checkpoints of Qwen2.5 (0.5B/1.5B) and Gemma3 (1B/4B), we inject five copies of GSM8K and MBPP test items into the first 2B tokens of an otherwise 25B token extended pre-training dataset. We then compare the contaminated and clean models both immediately after pre-training and again after two popular post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL) with group relative policy optimization (GRPO). The applied post-training steps do not have any contamination. Across math and coding benchmarks, we find three consistent patterns: (i) Contamination causes performance spikes that are gradually diminished with continued pre-training. After even 25B tokens the apparent performance inflation of contamination can become close to zero. (ii) Both SFT and GRPO resurface the leaked information, but with different external validity: SFT inflates scores only on the contaminated tasks, whereas GRPO also inflates performance on uncontaminated counterparts (GSMPlus, HumanEval). (iii) Model scale amplifies these tendencies, larger Supervised Fine Tuned models memorize more, while larger GRPO models translate leakage into more generalizable capabilities. Our results underscore the need for contamination audits \emph{after} post-training and suggest that RL-based post-training, although not immune, can help alleviate contamination-related over-estimation problems.

</details>


### [27] [Australian Bushfire Intelligence with AI-Driven Environmental Analytics](https://arxiv.org/abs/2601.06105)
*Tanvi Jois,Hussain Ahmad,Fatima Noor,Faheem Ullah*

Main category: cs.LG

TL;DR: This paper integrates spatio-temporal environmental data to predict bushfire intensity in Australia using machine learning models, with an ensemble classifier achieving 87% accuracy.


<details>
  <summary>Details</summary>
Motivation: Bushfires in Australia cause severe damage, making accurate prediction of bushfire intensity crucial for effective disaster preparedness and response.

Method: The study integrated historical fire events, meteorological data, and vegetation indices for 2015-2023, harmonized data using spatial and temporal joins, and evaluated machine learning models including Random Forest, XGBoost, LightGBM, MLP, and an ensemble classifier under a binary classification framework.

Result: The ensemble classifier achieved an accuracy of 87% in distinguishing 'low' and 'high' fire risk zones.

Conclusion: Combining multi-source environmental features with advanced machine learning techniques provides reliable bushfire intensity predictions, aiding in timely disaster management.

Abstract: Bushfires are among the most destructive natural hazards in Australia, causing significant ecological, economic, and social damage. Accurate prediction of bushfire intensity is therefore essential for effective disaster preparedness and response. This study examines the predictive capability of spatio-temporal environmental data for identifying high-risk bushfire zones across Australia. We integrated historical fire events from NASA FIRMS, daily meteorological observations from Meteostat, and vegetation indices such as the Normalized Difference Vegetation Index (NDVI) from Google Earth Engine for the period 2015-2023. After harmonizing the datasets using spatial and temporal joins, we evaluated several machine learning models, including Random Forest, XGBoost, LightGBM, a Multi-Layer Perceptron (MLP), and an ensemble classifier. Under a binary classification framework distinguishing 'low' and 'high' fire risk, the ensemble approach achieved an accuracy of 87%. The results demonstrate that combining multi-source environmental features with advanced machine learning techniques can produce reliable bushfire intensity predictions, supporting more informed and timely disaster management.

</details>


### [28] [Judge Model for Large-scale Multimodality Benchmarks](https://arxiv.org/abs/2601.06106)
*Min-Han Shih,Yu-Hsin Wu,Yu-Wei Chen*

Main category: cs.LG

TL;DR: A multimodal Judge Model is proposed for reliable, explainable evaluation of multimodal AI tasks. It aggregates judgments and provides diagnostic feedback. The model shows strong alignment with human annotations in evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable, interpretable, and reproducible evaluation of multimodal AI models, which often lacks transparency and reliability, by developing a dedicated framework that can assess various modalities consistently.

Method: The framework aggregates multimodal judgments, analyzes the quality and reasoning consistency of model outputs, and generates diagnostic feedback. It uses a benchmark spanning text, audio, image, and video from public datasets with fixed seeds for reproducibility. Several MLLMs (e.g., Gemini 2.5, Phi 4, Qwen 2.5) are evaluated across 280 multimodal samples, comparing judge model assessments with human annotators.

Result: Results show strong alignment between the Judge Model and human scores, indicating that the model provides accurate and consistent evaluations that match human judgment.

Conclusion: The Judge Model demonstrates potential as a scalable and interpretable evaluation pipeline for multimodal AI research, offering a reliable tool to enhance assessment capabilities in the field.

Abstract: We propose a dedicated multimodal Judge Model designed to provide reliable, explainable evaluation across a diverse suite of tasks. Our benchmark spans text, audio, image, and video modalities, drawing from carefully sampled public datasets with fixed seeds to ensure reproducibility and minimize train test leakage. Instead of simple scoring, our framework aggregates multimodal judgments, analyzes the quality and reasoning consistency of model outputs, and generates diagnostic feedback. We evaluate several MLLMs, including Gemini 2.5, Phi 4, and Qwen 2.5, across 280 multimodal samples and compare judge model assessments with human annotators. Results show strong alignment between the Judge Model and human scores, demonstrating its potential as a scalable, interpretable evaluation pipeline for future multimodal AI research.

</details>


### [29] [GroupSegment-SHAP: Shapley Value Explanations with Group-Segment Players for Multivariate Time Series](https://arxiv.org/abs/2601.06114)
*Jinwoong Kim,Sangjin Park*

Main category: cs.LG

TL;DR: GS-SHAP is a new SHAP variant for interpreting multivariate time-series models that considers joint cross-variable and temporal patterns, outperforming existing methods in faithfulness and computational efficiency across healthcare, energy, finance, and activity recognition domains.


<details>
  <summary>Details</summary>
Motivation: Multivariate time-series models achieve strong predictive performance but their internal workings combining cross-variable interactions with temporal dynamics remain unclear. Existing SHAP-based interpretation methods for time series typically treat feature and time axes independently, fragmenting structural signals formed jointly by multiple variables over specific time intervals.

Method: GroupSegment SHAP (GS-SHAP) constructs explanatory units as group-segment players based on cross-variable dependence and distribution shifts over time, then quantifies each unit's contribution using Shapley attribution. It organizes variables into groups based on dependence and time into segments based on distribution shifts to create joint explanatory units.

Result: GS-SHAP improves deletion-based faithfulness (DeltaAUC) by about 1.7x on average over time-series SHAP baselines (KernelSHAP, TimeSHAP, SequenceSHAP, WindowSHAP, TSHAP), while reducing wall-clock runtime by about 40% on average under matched perturbation budgets. A financial case study shows GS-SHAP identifies interpretable multivariate-temporal interactions among key market variables during high-volatility regimes.

Conclusion: GS-SHAP successfully addresses the limitations of existing time-series interpretation methods by accounting for cross-variable interactions and temporal patterns jointly. It offers improved interpretability through group-segment units while being computationally efficient, making it a practical and faithful explanation framework for multivariate time-series models across various domains.

Abstract: Multivariate time-series models achieve strong predictive performance in healthcare, industry, energy, and finance, but how they combine cross-variable interactions with temporal dynamics remains unclear. SHapley Additive exPlanations (SHAP) are widely used for interpretation. However, existing time-series variants typically treat the feature and time axes independently, fragmenting structural signals formed jointly by multiple variables over specific intervals. We propose GroupSegment SHAP (GS-SHAP), which constructs explanatory units as group-segment players based on cross-variable dependence and distribution shifts over time, and then quantifies each unit's contribution via Shapley attribution. We evaluate GS-SHAP across four real-world domains: human activity recognition, power-system forecasting, medical signal analysis, and financial time series, and compare it with KernelSHAP, TimeSHAP, SequenceSHAP, WindowSHAP, and TSHAP. GS-SHAP improves deletion-based faithfulness (DeltaAUC) by about 1.7x on average over time-series SHAP baselines, while reducing wall-clock runtime by about 40 percent on average under matched perturbation budgets. A financial case study shows that GS-SHAP identifies interpretable multivariate-temporal interactions among key market variables during high-volatility regimes.

</details>


### [30] [The Practicality of Normalizing Flow Test-Time Training in Bayesian Inference for Agent-Based Models](https://arxiv.org/abs/2601.07413)
*Junyao Zhang,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: Test-time training enables real-time adjustment of normalizing flow models for parameter inference in Agent-Based Models, addressing distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Agent-Based Models are popular for describing realistic heterogeneous agent interactions, but parameter estimation faces challenges with distribution shifts that require real-time adaptation.

Method: Proposes test-time training strategies for fine-tuning normalizing flows against distribution shifts in ABM parameter posterior estimation.

Result: Numerical study demonstrates TTT schemes are remarkably effective for enabling real-time adjustment of flow-based inference for ABM parameters.

Conclusion: Test-time training of deep models like normalizing flows is practical and effective for parameter posterior estimation in Agent-Based Models, providing real-time adaptation to distribution shifts.

Abstract: Agent-Based Models (ABMs) are gaining great popularity in economics and social science because of their strong flexibility to describe the realistic and heterogeneous decisions and interaction rules between individual agents. In this work, we investigate for the first time the practicality of test-time training (TTT) of deep models such as normalizing flows, in the parameters posterior estimations of ABMs. We propose several practical TTT strategies for fine-tuning the normalizing flow against distribution shifts. Our numerical study demonstrates that TTT schemes are remarkably effective, enabling real-time adjustment of flow-based inference for ABM parameters.

</details>


### [31] [Stress Testing Machine Learning at $10^{10}$ Scale: A Comprehensive Study of Adversarial Robustness on Algebraically Structured Integer Streams](https://arxiv.org/abs/2601.06117)
*HyunJun Jeon*

Main category: cs.LG

TL;DR: Large-scale stress test of ML systems using math data benchmark, evaluating tree-based classifiers with billions of samples, introducing an efficient pipeline, adversarial dataset, and fault-tolerant infrastructure. LightGBM shows high accuracy but relies on quadratic patterns.


<details>
  <summary>Details</summary>
Motivation: To rigorously test the robustness of machine learning systems, especially tree-based classifiers, using structured mathematical data to expose vulnerabilities and understand model behavior at an unprecedented scale.

Method: Developed a high-throughput pipeline for generating Pythagorean triples efficiently, created the Hypothesis-driven Negative Dataset (HND) with nine adversarial attack classes, and implemented fault-tolerant infrastructure for large-scale training and evaluation on ten billion deterministic and five billion adversarial samples.

Result: LightGBM achieved 99.99% accuracy on the benchmark, but feature attribution analysis revealed that the model prioritizes underlying quadratic patterns over direct algebraic verification, indicating reliance on learned heuristics.

Conclusion: Learned heuristics in ML models can effectively identify structural representations in numerical data, suggesting potential use as efficient preprocessors in formal verification methods, with implications for robustness testing and model interpretability.

Abstract: This paper presents a large-scale stress test of machine learning systems using structured mathematical data as a benchmark. We evaluate the robustness of tree-based classifiers at an unprecedented scale, utilizing ten billion deterministic samples and five billion adversarial counterexamples. Our framework introduces three primary contributions:
  first, a high-throughput pipeline that reformulates Pythagorean triple generation into a single-parameter index stream, significantly improving computational efficiency over classical methods;
  second, the Hypothesis-driven Negative Dataset (HND), which categorizes nine classes of adversarial attacks designed to exploit both arithmetic precision and structural patterns; and
  third, a fault-tolerant infrastructure for reliable large-scale training. Experimental results demonstrate that while LightGBM achieves 99.99% accuracy, feature attribution reveals that the model prioritizes underlying quadratic patterns over direct algebraic verification.
  These findings suggest that learned heuristics can effectively identify structural representations in numerical data, potentially serving as efficient preprocessors for formal verification methods.

</details>


### [32] [L2CU: Learning to Complement Unseen Users](https://arxiv.org/abs/2601.06119)
*Dileepa Pitawela,Gustavo Carneiro,Hsiang-Ting Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent research highlights the potential of machine learning models to learn to complement (L2C) human strengths; however, generalizing this capability to unseen users remains a significant challenge. Existing L2C methods oversimplify interaction between human and AI by relying on a single, global user model that neglects individual user variability, leading to suboptimal cooperative performance. Addressing this, we introduce L2CU, a novel L2C framework for human-AI cooperative classification with unseen users. Given sparse and noisy user annotations, L2CU identifies representative annotator profiles capturing distinct labeling patterns. By matching unseen users to these profiles, L2CU leverages profile-specific models to complement the user and achieve superior joint accuracy. We evaluate L2CU on datasets (CIFAR-10N, CIFAR-10H, Fashion-MNIST-H, Chaoyang and AgNews), demonstrating its effectiveness as a model-agnostic solution for improving human-AI cooperative classification.

</details>


### [33] [Latent Space Communication via K-V Cache Alignment](https://arxiv.org/abs/2601.06123)
*Lucio M. Dery,Zohar Yahav,Henry Prior,Qixuan Feng,Jiajun Shen,Arthur Szlam*

Main category: cs.LG

TL;DR: This paper proposes a method for multi-model LLM collaboration by learning a shared representation space with adapters, enhancing communication and performance without modifying pre-trained weights.


<details>
  <summary>Details</summary>
Motivation: To move beyond individual LLMs to multi-model systems that effectively collaborate, enabling richer and more efficient exchange by allowing models to access each other's internal states directly rather than just text communication.

Method: The approach involves augmenting each model with adapters to translate its state into and out of a shared space that aligns the k-v caches of multiple models, without altering pre-trained parameters.

Result: Experiments with Gemma-2 models show that the approach enables seamless inter-model communication, improves individual model performance, and allows direct transfer of learned skills like soft prompts between different models.

Conclusion: This work represents a significant step towards a future where models can fluidly share knowledge and capabilities by learning a shared representation space.

Abstract: Solving increasingly complex problems with large language models (LLMs) necessitates a move beyond individual models and towards multi-model systems that can effectively collaborate. While text has traditionally served as the medium for inter-model communication, a richer and more efficient exchange is possible if models can access each other's internal states directly. In this paper, we propose learning a shared representation space that aligns the k-v caches of multiple models, creating a high-bandwidth channel for collaboration without altering the underlying pre-trained parameters. We do so by augmenting each model with adapters to translate its state into and out of this shared space. Via a suite of experiments with Gemma-2 models, we demonstrate that this approach not only enables seamless inter-model communication but also improves individual model performance. We also show that the shared space allows for the direct transfer of learned skills, such as soft prompts, between different models. Our work represents a significant step towards a future where models can fluidly share knowledge and capabilities.

</details>


### [34] [Learning Minimally-Congested Drive Times from Sparse Open Networks: A Lightweight RF-Based Estimator for Urban Roadway Operations](https://arxiv.org/abs/2601.06124)
*Adewumi Augustine Adepitan,Christopher J. Haruna,Morayo Ogunsina,Damilola Olawoyin Yussuf,Ayooluwatomiwa Ajiboye*

Main category: cs.LG

TL;DR: The paper develops a lightweight random forest estimator for car travel times in minimally congested conditions, using open data and sparse features to improve accuracy over shortest-path baselines.


<details>
  <summary>Details</summary>
Motivation: Existing travel-time prediction methods either require extensive data-intensive congestion models or rely on overly simple heuristics, limiting scalability and practical use in engineering workflows. This highlights a need for a more efficient and accurate approach.

Method: The method involves constructing drivable networks from open data, finding Dijkstra routes for minimal traversal time, extracting sparse operational features (like signals and turn counts), and training a random forest regression ensemble on limited high-quality reference times.

Result: Out-of-sample evaluation shows significant improvements over baseline methods across key metrics such as mean absolute error and explained variance, with no significant mean bias and stable k-fold performance indicating minimal overfitting.

Conclusion: The proposed approach provides a practical solution for transportation engineering, offering accurate point-to-point travel-time predictions at metropolitan scale with reduced resource needs, particularly useful when congestion data is unavailable or expensive, supporting applications like planning and network performance in low-traffic conditions.

Abstract: Accurate roadway travel-time prediction is foundational to transportation systems analysis, yet widespread reliance on either data-intensive congestion models or overly naïve heuristics limits scalability and practical adoption in engineering workflows. This paper develops a lightweight estimator for minimally-congested car travel times that integrates open road-network data, speed constraints, and sparse control/turn features within a random forest framework to correct bias from shortest-path traversal-time baselines. Using an urban testbed, the pipeline: (i) constructs drivable networks from volunteered geographic data; (ii) solves Dijkstra routes minimizing edge traversal time; (iii) derives sparse operational features (signals, stops, crossings, yield, roundabouts; left/right/slight/U-turn counts); and (iv) trains a regression ensemble on limited high-quality reference times to generalize predictions beyond the training set. Out-of-sample evaluation demonstrates marked improvements over traversal-time baselines across mean absolute error, mean absolute percentage error, mean squared error, relative bias, and explained variance, with no significant mean bias under minimally congested conditions and consistent k-fold stability indicating negligible overfitting. The resulting approach offers a practical middle ground for transportation engineering: it preserves point-to-point fidelity at metropolitan scale, reduces resource requirements, and supplies defensible performance estimates where congestion feeds are inaccessible or cost-prohibitive, supporting planning, accessibility, and network performance applications under low-traffic operating regimes.

</details>


### [35] [AIS-CycleGen: A CycleGAN-Based Framework for High-Fidelity Synthetic AIS Data Generation and Augmentation](https://arxiv.org/abs/2601.06127)
*SM Ashfaq uz Zaman,Faizan Qamar,Masnizah Mohd,Nur Hanis Sabrina Suhaimi,Amith Khandakar*

Main category: cs.LG

TL;DR: AISCycleGen: A CycleGAN-based method for augmenting AIS data by generating high-fidelity synthetic sequences via unpaired domain translation, enhancing model performance in maritime applications.


<details>
  <summary>Details</summary>
Motivation: AIS data suffers from domain shifts, sparsity, and class imbalance, hindering predictive models in maritime domain awareness.

Method: Uses Cycle-Consistent GANs (CycleGAN) for unpaired domain translation, with a 1D convolutional generator featuring adaptive noise injection to preserve spatiotemporal structure of AIS trajectories.

Result: Outperforms other GAN-based methods; achieves PSNR of 30.5, FID of 38.9; improves regression model performance across maritime domains.

Conclusion: AISCycleGen is an effective, generalizable solution for augmenting AIS datasets, boosting downstream model performance in real-world maritime intelligence.

Abstract: Automatic Identification System (AIS) data are vital for maritime domain awareness, yet they often suffer from domain shifts, data sparsity, and class imbalance, which hinder the performance of predictive models. In this paper, we propose a robust data augmentation method, AISCycleGen, based on Cycle-Consistent Generative Adversarial Networks (CycleGAN), which is tailored for AIS datasets. Unlike traditional methods, AISCycleGen leverages unpaired domain translation to generate high-fidelity synthetic AIS data sequences without requiring paired source-target data. The framework employs a 1D convolutional generator with adaptive noise injection to preserve the spatiotemporal structure of AIS trajectories, enhancing the diversity and realism of the generated data. To demonstrate its efficacy, we apply AISCycleGen to several baseline regression models, showing improvements in performance across various maritime domains. The results indicate that AISCycleGen outperforms contemporary GAN-based augmentation techniques, achieving a PSNR value of 30.5 and an FID score of 38.9. These findings underscore AISCycleGen's potential as an effective and generalizable solution for augmenting AIS datasets, improving downstream model performance in real-world maritime intelligence applications.

</details>


### [36] [A Review of Online Diffusion Policy RL Algorithms for Scalable Robotic Control](https://arxiv.org/abs/2601.06133)
*Wonhyeok Choi,Minwoo Choi,Jungwan Woo,Kyumin Hwang,Jaeyeul Kim,Sunghoon Im*

Main category: cs.LG

TL;DR: This paper reviews and analyzes online diffusion policy reinforcement learning algorithms for robotics, proposing a taxonomy and evaluating them on a benchmark, with findings on trade-offs and guidelines for optimization.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies are effective for robotic control but face challenges in online reinforcement learning due to incompatibilities between diffusion training and RL mechanisms, necessitating a comprehensive study.

Method: The study proposes a taxonomy categorizing approaches into four families (Action-Gradient, Q-Weighting, Proximity-Based, BPTT), conducts experiments on a NVIDIA Isaac Lab benchmark with 12 tasks, and evaluates algorithms across five dimensions.

Result: Key findings reveal trade-offs in sample efficiency and scalability, identify computational bottlenecks, and provide guidelines for algorithm selection and future research directions.

Conclusion: The paper offers a foundational analysis to advance online DPRL, highlighting limitations and opportunities for scalable robotic learning systems.

Abstract: Diffusion policies have emerged as a powerful approach for robotic control, demonstrating superior expressiveness in modeling multimodal action distributions compared to conventional policy networks. However, their integration with online reinforcement learning remains challenging due to fundamental incompatibilities between diffusion model training objectives and standard RL policy improvement mechanisms. This paper presents the first comprehensive review and empirical analysis of current Online Diffusion Policy Reinforcement Learning (Online DPRL) algorithms for scalable robotic control systems. We propose a novel taxonomy that categorizes existing approaches into four distinct families -- Action-Gradient, Q-Weighting, Proximity-Based, and Backpropagation Through Time (BPTT) methods -- based on their policy improvement mechanisms. Through extensive experiments on a unified NVIDIA Isaac Lab benchmark encompassing 12 diverse robotic tasks, we systematically evaluate representative algorithms across five critical dimensions: task diversity, parallelization capability, diffusion step scalability, cross-embodiment generalization, and environmental robustness. Our analysis identifies key findings regarding the fundamental trade-offs inherent in each algorithmic family, particularly concerning sample efficiency and scalability. Furthermore, we reveal critical computational and algorithmic bottlenecks that currently limit the practical deployment of online DPRL. Based on these findings, we provide concrete guidelines for algorithm selection tailored to specific operational constraints and outline promising future research directions to advance the field toward more general and scalable robotic learning systems.

</details>


### [37] [DeeperBrain: A Neuro-Grounded EEG Foundation Model Towards Universal BCI](https://arxiv.org/abs/2601.06134)
*Jiquan Wang,Sha Zhao,Yangxuan Zhou,Yiming Kang,Shijian Li,Gang Pan*

Main category: cs.LG

TL;DR: DeeperBrain is a neuro-grounded EEG foundation model that integrates domain-specific inductive biases into architecture and pretraining objectives to achieve superior generalization for universal BCIs, especially under frozen-probing protocols.


<details>
  <summary>Details</summary>
Motivation: Existing EEG foundation models rely on end-to-end fine-tuning and show limited efficacy under frozen-probing protocols, lacking intrinsic universality for broad generalization. This stems from adapting general-purpose sequence architectures that overlook biophysical and dynamical principles of neural activity.

Method: Proposes DeeperBrain with: 1) Volume conduction-aware channel encoding using 3D geometry to model spatial mixing, 2) Neurodynamics-aware temporal encoding using oscillatory and exponential bases to capture slow adaptations, 3) Dual-objective pretraining combining Masked EEG Reconstruction (MER) for local fidelity and Neurodynamics Statistics Prediction (NSP) to enforce alignment with macroscopic brain states by predicting interpretable order parameters.

Result: DeeperBrain achieves state-of-the-art or highly competitive performance under end-to-end fine-tuning, and crucially maintains superior efficacy under rigorous frozen-probing protocols, verifying that embedding neuroscientific first principles endows learned representations with intrinsic universality essential for universal BCI.

Conclusion: Integrating domain-specific inductive biases into both model architecture and learning objectives enables EEG foundation models to learn representations with intrinsic universality, making them more effective for universal BCIs, particularly in challenging frozen-probing scenarios where existing approaches fail.

Abstract: Electroencephalography (EEG) foundation models hold significant promise for universal Brain-Computer Interfaces (BCIs). However, existing approaches often rely on end-to-end fine-tuning and exhibit limited efficacy under frozen-probing protocols, lacking the intrinsic universality required for broad generalization. This limitation stems from adapting general-purpose sequence architectures that overlook the biophysical and dynamical principles of neural activity. To bridge this gap, we propose DeeperBrain, a neuro-grounded foundation model integrating domain-specific inductive biases into its model design and learning objectives. Architecturally, DeeperBrain incorporates a volume conduction-aware channel encoding to model spatial mixing via 3D geometry, and a neurodynamics-aware temporal encoding capturing slow adaptations using oscillatory and exponential bases. For pretraining, we introduce a dual-objective strategy combining Masked EEG Reconstruction (MER) for local fidelity and Neurodynamics Statistics Prediction (NSP). NSP enforces alignment with macroscopic brain states by predicting interpretable order parameters, including spectral power, functional connectivity, cross-frequency coupling, and dynamic complexity. Extensive experiments demonstrate that DeeperBrain achieves state-of-the-art or highly competitive performance under end-to-end fine-tuning. Crucially, it maintains superior efficacy under a rigorous frozen-probing protocol, verifying that embedding neuroscientific first principles endows learned representations with the intrinsic universality essential for universal BCI. The code will be publicly available.

</details>


### [38] [Attention in Geometry: Scalable Spatial Modeling via Adaptive Density Fields and FAISS-Accelerated Kernels](https://arxiv.org/abs/2601.06135)
*Zhaowen Fan*

Main category: cs.LG

TL;DR: This paper introduces Adaptive Density Fields (ADF), a geometric attention framework that uses spatial aggregation with distance-based attention for scalable analysis, demonstrated through aircraft trajectory case study in Chengdu.


<details>
  <summary>Details</summary>
Motivation: To bridge adaptive kernel methods and attention mechanisms by reinterpreting spatial influence as geometry-preserving attention in continuous space for enhanced spatial analysis.

Method: Uses a query-conditioned metric-induced attention operator, implemented with FAISS-accelerated inverted file indices for scalable approximate nearest-neighbor search as part of the attention mechanism.

Result: Applied to aircraft trajectory analysis in Chengdu, extracting trajectory-conditioned Zones of Influence (ZOI) to reveal recurrent airspace structures and localized deviations.

Conclusion: ADF effectively integrates geometric attention with scalable techniques, providing a framework for continuous spatial data analysis with practical applications in domains like air traffic monitoring.

Abstract: This work introduces Adaptive Density Fields (ADF), a geometric attention framework that formulates spatial aggregation as a query-conditioned, metric-induced attention operator in continuous space. By reinterpreting spatial influence as geometry-preserving attention grounded in physical distance, ADF bridges concepts from adaptive kernel methods and attention mechanisms. Scalability is achieved via FAISS-accelerated inverted file indices, treating approximate nearest-neighbor search as an intrinsic component of the attention mechanism. We demonstrate the framework through a case study on aircraft trajectory analysis in the Chengdu region, extracting trajectory-conditioned Zones of Influence (ZOI) to reveal recurrent airspace structures and localized deviations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [39] [DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation](https://arxiv.org/abs/2601.06373)
*Yutong Song,Jiang Wu,Kazi Sharif,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.MA

TL;DR: DemMA is an expert-guided LLM-based dementia simulation agent that generates realistic patient dialogue, non-verbal behaviors, and reasoning in a single model.


<details>
  <summary>Details</summary>
Motivation: Simulating dementia patients with large language models is difficult due to the need to model cognitive impairment, emotional dynamics, and non-verbal behaviors over extended conversations.

Method: DemMA constructs dementia personas using pathology, personality traits, and subtype-specific memory data guided by experts. It models non-verbal behaviors and uses a Chain-of-Thought distillation to train one LLM for joint generation of reasoning, dialogue, and actions in a single forward pass.

Result: Evaluations with experts, medical students, and LLM judges show DemMA significantly outperforms strong baselines across multiple metrics.

Conclusion: DemMA enables high-fidelity, efficient simulation of dementia patients without multi-agent inference, advancing applications in medical training and AI-assisted care.

Abstract: Simulating dementia patients with large language models (LLMs) is challenging due to the need to jointly model cognitive impairment, emotional dynamics, and nonverbal behaviors over long conversations. We present DemMA, an expert-guided dementia dialogue agent for high-fidelity multi-turn patient simulation. DemMA constructs clinically grounded dementia personas by integrating pathology information, personality traits, and subtype-specific memory-status personas informed by clinical experts. To move beyond text-only simulation, DemMA explicitly models nonverbal behaviors, including motion, facial expressions, and vocal cues. We further introduce a Chain-of-Thought distillation framework that trains a single LLM to jointly generate reasoning traces, patient utterances, and aligned behavioral actions within one forward pass, enabling efficient deployment without multi-agent inference. Extensive evaluations with experts, medical students, and LLM judges demonstrate that DemMA significantly outperforms strong baselines across multiple metrics.

</details>


### [40] [Dynamic Incentivized Cooperation under Changing Rewards](https://arxiv.org/abs/2601.06382)
*Philipp Altmann,Thomy Phan,Maximilian Zorn,Claudia Linnhoff-Popien,Sven Koenig*

Main category: cs.MA

TL;DR: DRIVE is an adaptive peer incentivization method for achieving and maintaining cooperation in social dilemmas with changing rewards, outperforming current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current peer incentivization methods rely on fixed incentive values, making them sensitive to changes in environmental rewards and failing to maintain cooperation even when conditions for cooperation are unchanged.

Method: DRIVE agents exchange reward differences to incentivize mutual cooperation in a decentralized way, adapting to changing rewards.

Result: DRIVE achieves mutual cooperation in the general Prisoner's Dilemma and maintains it in complex sequential social dilemmas with changing rewards.

Conclusion: DRIVE effectively addresses the limitations of fixed incentive methods, demonstrating scalability and robustness in dynamic environments.

Abstract: Peer incentivization (PI) is a popular multi-agent reinforcement learning approach where all agents can reward or penalize each other to achieve cooperation in social dilemmas. Despite their potential for scalable cooperation, current PI methods heavily depend on fixed incentive values that need to be appropriately chosen with respect to the environmental rewards and thus are highly sensitive to their changes. Therefore, they fail to maintain cooperation under changing rewards in the environment, e.g., caused by modified specifications, varying supply and demand, or sensory flaws - even when the conditions for mutual cooperation remain the same. In this paper, we propose Dynamic Reward Incentives for Variable Exchange (DRIVE), an adaptive PI approach to cooperation in social dilemmas with changing rewards. DRIVE agents reciprocally exchange reward differences to incentivize mutual cooperation in a completely decentralized way. We show how DRIVE achieves mutual cooperation in the general Prisoner's Dilemma and empirically evaluate DRIVE in more complex sequential social dilemmas with changing rewards, demonstrating its ability to achieve and maintain cooperation, in contrast to current state-of-the-art PI methods.

</details>


### [41] [Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents](https://arxiv.org/abs/2601.06490)
*Wenyu Mao,Haosong Tan,Shuchang Liu,Haoyang Liu,Yifan Xu,Huaxiang Ji,Xiang Wang*

Main category: cs.MA

TL;DR: Bi-Mem is an agentic framework that constructs hierarchical memory from long-term conversations using inductive and reflective agents for bidirectional fidelity, enhancing personalized interaction by mitigating noise and hallucinations.


<details>
  <summary>Details</summary>
Motivation: To overcome LLMs' contextual limitations and enable personalized interactions, hierarchical memory is used, but conversational noise and memory hallucinations during clustering can cause local memories to misalign with the global persona, necessitating a solution.

Method: Bi-Mem employs an inductive agent to extract facts from conversations, aggregate them into scene-level memory via graph clustering, and infer persona-level memory; a reflective agent calibrates local memories using global constraints for alignment; associative retrieval mechanism uses hierarchical search and spreading activation for coherent recall.

Result: Empirical evaluations show that Bi-Mem achieves significant improvements in question answering performance on long-term personalized conversational tasks.

Conclusion: Bi-Mem effectively mitigates noise and hallucinations in hierarchical memory construction through bidirectional agents and associative retrieval, enhancing alignment and personalization in conversational AI systems.

Abstract: Constructing memory from users' long-term conversations overcomes LLMs' contextual limitations and enables personalized interactions. Recent studies focus on hierarchical memory to model users' multi-granular behavioral patterns via clustering and aggregating historical conversations. However, conversational noise and memory hallucinations can be amplified during clustering, causing locally aggregated memories to misalign with the user's global persona. To mitigate this issue, we propose Bi-Mem, an agentic framework ensuring hierarchical memory fidelity through bidirectional construction. Specifically, we deploy an inductive agent to form the hierarchical memory: it extracts factual information from raw conversations to form fact-level memory, aggregates them into thematic scenes (i.e., local scene-level memory) using graph clustering, and infers users' profiles as global persona-level memory. Simultaneously, a reflective agent is designed to calibrate local scene-level memories using global constraints derived from the persona-level memory, thereby enforcing global-local alignment. For coherent memory recall, we propose an associative retrieval mechanism: beyond initial hierarchical search, a spreading activation process allows facts to evoke contextual scenes, while scene-level matches retrieve salient supporting factual information. Empirical evaluations demonstrate that Bi-Mem achieves significant improvements in question answering performance on long-term personalized conversational tasks.

</details>


### [42] [The Axiom of Consent: Friction Dynamics in Multi-Agent Coordination](https://arxiv.org/abs/2601.06692)
*Murad Farzulla*

Main category: cs.MA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multi-agent systems face a fundamental coordination problem: agents must coordinate despite heterogeneous preferences, asymmetric stakes, and imperfect information. When coordination fails, friction emerges: measurable resistance manifesting as deadlock, thrashing, communication overhead, or outright conflict. This paper derives a formal framework for analyzing coordination friction from a single axiom: actions affecting agents require authorization from those agents in proportion to stakes.
  From this axiom of consent, we establish the kernel triple $(α, σ, ε)$ (alignment, stake, and entropy) characterizing any resource allocation configuration. The friction equation $F = σ (1 + ε)/(1 + α)$ predicts coordination difficulty as a function of preference alignment $α$, stake magnitude $σ$, and communication entropy $ε$. The Replicator-Optimization Mechanism (ROM) governs evolutionary selection over coordination strategies: configurations generating less friction persist longer, establishing consent-respecting arrangements as dynamical attractors rather than normative ideals.
  We develop formal definitions for resource consent, coordination legitimacy, and friction-aware allocation in multi-agent systems. The framework yields testable predictions: MARL systems with higher reward alignment exhibit faster convergence; distributed allocations accounting for stake asymmetry generate lower coordination failure; AI systems with interpretability deficits produce friction proportional to the human-AI alignment gap. Applications to cryptocurrency governance and political systems demonstrate that the same equations govern friction dynamics across domains, providing a complexity science perspective on coordination under preference heterogeneity.

</details>


### [43] [Logic-Driven Semantic Communication for Resilient Multi-Agent Systems](https://arxiv.org/abs/2601.06733)
*Tamara Alshammari,Mehdi Bennis*

Main category: cs.MA

TL;DR: The paper proposes a formal definition and framework for resilience in decentralized multi-agent systems (MAS), focusing on epistemic and action resilience, with verifiable algorithms and superior performance in simulations.


<details>
  <summary>Details</summary>
Motivation: Existing literature on resilience in decentralized MAS lacks a unified definition, limiting the design of adaptive systems under dynamic conditions like environmental changes or adversarial behavior in 6G networks.

Method: The authors define MAS resilience via temporal epistemic logic, quantify it using recoverability and durability times, design an agent architecture, and develop decentralized algorithms to achieve epistemic and action resilience.

Result: The approach outperforms baseline methods in a distributed multi-agent decision-making case study, with formal verification guarantees for soundness and finite-horizon verification enabling design-time certification and runtime monitoring.

Conclusion: The framework enables resilient, knowledge-driven decision-making and sustained operation, laying groundwork for resilient decentralized MAS in next-generation communication systems like 6G.

Abstract: The advent of 6G networks is accelerating autonomy and intelligence in large-scale, decentralized multi-agent systems (MAS). While this evolution enables adaptive behavior, it also heightens vulnerability to stressors such as environmental changes and adversarial behavior. Existing literature on resilience in decentralized MAS largely focuses on isolated aspects, such as fault tolerance, without offering a principled unified definition of multi-agent resilience. This gap limits the ability to design systems that can continuously sense, adapt, and recover under dynamic conditions. This article proposes a formal definition of MAS resilience grounded in two complementary dimensions: epistemic resilience, wherein agents recover and sustain accurate knowledge of the environment, and action resilience, wherein agents leverage that knowledge to coordinate and sustain goals under disruptions. We formalize resilience via temporal epistemic logic and quantify it using recoverability time (how quickly desired properties are re-established after a disturbance) and durability time (how long accurate beliefs and goal-directed behavior are sustained after recovery). We design an agent architecture and develop decentralized algorithms to achieve both epistemic and action resilience. We provide formal verification guarantees, showing that our specifications are sound with respect to the metric bounds and admit finite-horizon verification, enabling design-time certification and lightweight runtime monitoring. Through a case study on distributed multi-agent decision-making under stressors, we show that our approach outperforms baseline methods. Our formal verification analysis and simulation results highlight that the proposed framework enables resilient, knowledge-driven decision-making and sustained operation, laying the groundwork for resilient decentralized MAS in next-generation communication systems.

</details>


### [44] [Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)](https://arxiv.org/abs/2601.07152)
*Aja Khanal,Kaushik T. Ranade,Rishabh Agrawal,Kalyan S. Basu,Apurva Narayan*

Main category: cs.MA

TL;DR: AoD is a framework combining diffusion language models with autoregressive model reasoning via reinforcement learning to generate structured data (e.g., JSON) with both semantic richness and strict schema adherence, outperforming baselines in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle to generate high-quality structured data where semantic diversity must align with strict schemas. Autoregressive models offer structural consistency but limited semantic variation, while diffusion models provide semantic richness but lack reliable structure preservation.

Method: AoD unifies diffusion models and autoregressive models using language-mediated reinforcement learning. It treats structured text generation as a multi-agent alignment process: a prompt optimization agent collaborates with a judge agent to iteratively guide a diffusion model with natural language feedback, enabling controllable generation without altering model parameters or handcrafted constraints.

Result: AoD consistently outperforms both diffusion and autoregressive baselines across multiple structured data benchmarks, achieving high semantic novelty and structural fidelity in generation.

Conclusion: AoD advances controllable generation by showing that diffusion models, supervised by cooperative agents, can achieve balanced semantic richness and schema adherence, offering a novel approach for structure-aware, diversity-enhanced text synthesis.

Abstract: Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.

</details>


### [45] [DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems](https://arxiv.org/abs/2601.07248)
*Shuyu Zhang,Yujie Liu,Xinru Wang,Cheng Zhang,Yanmin Zhu,Bin Li*

Main category: cs.MA

TL;DR: DarwinTOD is a lifelong self-evolving dialog framework that integrates evolutionary computation and LLM-driven self-improvement for continuous strategy optimization without human intervention.


<details>
  <summary>Details</summary>
Motivation: Traditional task-oriented dialog systems cannot adapt or improve dynamically after deployment, requiring episodic retraining with human-curated data, which limits autonomy in real-world settings.

Method: DarwinTOD uses an Evolvable Strategy Bank and a dual-loop process: online multi-agent dialog execution with peer critique and offline structured evolutionary operations to refine strategies based on feedback.

Result: Extensive experiments show that DarwinTOD surpasses previous state-of-the-art methods and demonstrates continuous performance gains throughout evolution.

Conclusion: The work provides a novel framework for building dialog systems with lifelong self-evolution capabilities, enabling autonomous improvement without task-specific fine-tuning.

Abstract: Traditional task-oriented dialog systems are unable to evolve from ongoing interactions or adapt to new domains after deployment, that is a critical limitation in real-world dynamic environments. Continual learning approaches depend on episodic retraining with human curated data, failing to achieve autonomy lifelong improvement. While evolutionary computation and LLM driven self improvement offer promising mechanisms for dialog optimization, they lack a unified framework for holistic, iterative strategy refinement. To bridge this gap, we propose DarwinTOD, a lifelong self evolving dialog framework that systematically integrates these two paradigms, enabling continuous strategy optimization from a zero-shot base without task specific fine-tuning. DarwinTOD maintains an Evolvable Strategy Bank and operates through a dual-loop process: online multi-agent dialog execution with peer critique, and offline structured evolutionary operations that refine the strategy bank using accumulated feedback. This closed-loop design enables autonomous continuous improvement without human intervention. Extensive experiments show that DarwinTOD surpasses previous state-of-the-art methods and exhibits continuous performance gains throughout evolution. Our work provides a novel framework for building dialog systems with lifelong self evolution capabilities.

</details>


### [46] [SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models](https://arxiv.org/abs/2601.07252)
*Chunwei Yang,Yankai Wang,Jianxiang Tang,Haojie Qu,Ziqiang Zou,YuLiu,Chunrui Deng,Zhifang Qiu,Ming Ding*

Main category: cs.MA

TL;DR: SwarmFoam is a new multi-agent framework for intelligent CFD simulations that uses dual parsing of images and high-level instructions to handle complex geometries, achieving 84% pass rate across test cases.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems based on Large Language Models have significant limitations when dealing with complex geometries in CFD simulations, despite the potential of multi-agent technology to replicate human behavior for intelligent simulations.

Method: SwarmFoam integrates Multi-modal perception, Intelligent error correction, and Retrieval-Augmented Generation to achieve more complex simulations through dual parsing of images and high-level instructions.

Result: Experimental results show SwarmFoam has good adaptability to different modality inputs: 84% overall pass rate across 25 test cases, with 80% for natural language inputs and 86.7% for multi-modal inputs.

Conclusion: SwarmFoam will further promote the development of intelligent agent methods for Computational Fluid Dynamics by overcoming limitations of existing multi-agent systems in handling complex geometries.

Abstract: Numerical simulation is one of the mainstream methods in scientific research, typically performed by professional engineers. With the advancement of multi-agent technology, using collaborating agents to replicate human behavior shows immense potential for intelligent Computational Fluid Dynamics (CFD) simulations. Some muti-agent systems based on Large Language Models have been proposed. However, they exhibit significant limitations when dealing with complex geometries. This paper introduces a new multi-agent simulation framework, SwarmFoam. SwarmFoam integrates functionalities such as Multi-modal perception, Intelligent error correction, and Retrieval-Augmented Generation, aiming to achieve more complex simulations through dual parsing of images and high-level instructions. Experimental results demonstrate that SwarmFoam has good adaptability to simulation inputs from different modalities. The overall pass rate for 25 test cases was 84%, with natural language and multi-modal input cases achieving pass rates of 80% and 86.7%, respectively. The work presented by SwarmFoam will further promote the development of intelligent agent methods for CFD.

</details>


### [47] [VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing](https://arxiv.org/abs/2601.07315)
*Guanyuan Pan,Yugui Lin,Tiansheng Zhou,Pietro Liò,Shuai Wang,Yaqi Wang*

Main category: cs.MA

TL;DR: A collaborative agent design workflow using Vision Language Models and an explainable trust region Bayesian optimization method is proposed for efficient and interpretable analog mixed-signal circuit sizing, achieving high success rates and speed.


<details>
  <summary>Details</summary>
Motivation: Existing automatic analog circuit sizing approaches often underutilize circuit schematics and lack explainability, hindering industry adoption.

Method: The VLM-CAD workflow analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization with Image2Net for schematic annotation and ExTuRBO for explainable Bayesian optimization.

Result: Experiments on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models show VLM-CAD achieves a 100% success rate in optimizing an amplifier with complementary input and class-AB output stage, with total runtime under 43 minutes.

Conclusion: VLM-CAD effectively balances power and performance, addressing key challenges in analog circuit sizing through explainable and efficient methods.

Abstract: Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches often underutilize circuit schematics and lack the explainability required for industry adoption. To tackle these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-starting from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance, achieving a 100% success rate in optimizing an amplifier with a complementary input and a class-AB output stage, while maintaining total runtime under 43 minutes across all experiments.

</details>


### [48] [Self-Creating Random Walks for Decentralized Learning under Pac-Man Attacks](https://arxiv.org/abs/2601.07674)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Salim El Rouayheb*

Main category: cs.MA

TL;DR: A decentralized CREATE-IF-LATE algorithm is proposed to defend against stealthy Pac-Man attacks that kill random walks in distributed learning, ensuring persistence and convergence of the learning process.


<details>
  <summary>Details</summary>
Motivation: Random walk-based algorithms are vulnerable to malicious 'Pac-Man' attacks that terminate random walks, halting decentralized learning without detection.

Method: Propose CREATE-IF-LATE (CIL), a fully decentralized mechanism that enables self-creating random walks to prevent extinction under Pac-Man attacks.

Result: Theoretical analysis guarantees non-extinction, bounded population, and convergence of stochastic gradient descent with quantifiable deviation; empirical results on synthetic and benchmark datasets validate findings.

Conclusion: CIL algorithm effectively counters Pac-Man attacks, maintaining learning functionality with at most linear time delay, enhancing resilience in distributed systems.

Abstract: Random walk (RW)-based algorithms have long been popular in distributed systems due to low overheads and scalability, with recent growing applications in decentralized learning. However, their reliance on local interactions makes them inherently vulnerable to malicious behavior. In this work, we investigate an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious node probabilistically terminates any RW that visits it. This stealthy behavior gradually eliminates active RWs from the network, effectively halting the learning process without triggering failure alarms. To counter this threat, we propose the CREATE-IF-LATE (CIL) algorithm, which is a fully decentralized, resilient mechanism that enables self-creating RWs and prevents RW extinction in the presence of Pac-Man. Our theoretical analysis shows that the CIL algorithm guarantees several desirable properties, such as (i) non-extinction of the RW population, (ii) almost sure boundedness of the RW population, and (iii) convergence of RW-based stochastic gradient descent even in the presence of Pac-Man with a quantifiable deviation from the true optimum. Moreover, the learning process experiences at most a linear time delay due to Pac-Man interruptions and RW regeneration. Our extensive empirical results on both synthetic and public benchmark datasets validate our theoretical findings.

</details>


### [49] [OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent](https://arxiv.org/abs/2601.07779)
*Bowen Yang,Kaiming Jin,Zhenyu Wu,Zhaoyang Liu,Qiushi Sun,Zehao Li,JingJing Xie,Zhoumianze Liu,Fangzhi Xu,Kanzhi Cheng,Qingyun Li,Yian Wang,Yu Qiao,Zun Wang,Zichen Ding*

Main category: cs.MA

TL;DR: OS-Symphony is a holistic framework that improves Vision-Language Models for Computer-Using Agents by addressing robustness in long-horizon workflows and generalization in novel domains through milestone-driven memory and visual-aware tutorial retrieval.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Model frameworks for Computer-Using Agents struggle with robustness in long-horizon workflows and generalization in novel domains due to lack of granular control over historical visual context curation and absence of visual-aware tutorial retrieval.

Method: OS-Symphony introduces an Orchestrator coordinating two key innovations: (1) Reflection-Memory Agent using milestone-driven long-term memory for trajectory-level self-correction, and (2) Versatile Tool Agents with Multimodal Searcher that adopts SeeAct paradigm to navigate browser-based sandbox for synthesizing live, visually aligned tutorials.

Result: OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.

Conclusion: OS-Symphony effectively bridges the gaps in current Vision-Language Model frameworks for Computer-Using Agents by addressing both robustness in long-horizon tasks and generalization in novel scenarios through its innovative memory and tutorial retrieval mechanisms.

Abstract: While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.

</details>
