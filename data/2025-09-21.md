<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: A unified optimization framework for multi-line metro crew planning and replanning with heterogeneous workforce, using hierarchical time-space network modeling and efficient algorithms that outperform benchmarks in cost reduction and task completion.


<details>
  <summary>Details</summary>
Motivation: Metro crew planning is crucial for smart city development and operational efficiency, but current research focuses on individual lines with insufficient attention to cross-line coordination and rapid replanning during disruptions in expanding metro networks.

Method: Proposed hierarchical time-space network model to represent unified crew action space, with computationally efficient constraints for heterogeneous qualifications and preferences. Developed solution algorithms based on column generation and shortest path adjustment.

Result: Experiments with real data from Shanghai and Beijing Metro show the methods outperform benchmark heuristics in cost reduction and task completion, achieving notable efficiency gains through cross-line operations, especially for urgent tasks during disruptions.

Conclusion: This work demonstrates the importance of global optimization and cross-line coordination in multi-line metro system operations, providing insights for efficient and reliable public transportation in smart cities.

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [2] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: Evaluation of LLM-based agents for penetration testing shows targeted functional augmentations significantly improve performance in complex security tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for penetration testing automation but their effectiveness across different attack phases remains unclear, requiring comprehensive evaluation.

Method: Comprehensive evaluation of multiple LLM-based agents (single-agent to modular designs) across realistic penetration testing scenarios, with targeted augmentations for five core capabilities: Global Context Memory, Inter-Agent Messaging, Context-Conditioned Invocation, Adaptive Planning, and Real-Time Monitoring.

Result: Targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks, though some architectures natively exhibit subsets of these properties.

Conclusion: Functional augmentations are crucial for enhancing LLM-based penetration testing agents' performance, particularly for handling complex, multi-phase security testing scenarios that require coordination and real-time responsiveness.

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [3] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel RÃ¶der,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: Proposes a modular evaluation framework for web agents that decomposes agent pipelines into interpretable stages for detailed error analysis, addressing the limitation of current evaluations that focus only on overall success.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of web agents powered by LLMs mostly focus on overall success while overlooking intermediate errors, which limits insight into failure modes and hinders systematic improvement.

Method: Develop a modular evaluation framework that decomposes agent pipelines into interpretable stages for detailed error analysis, using the SeeAct framework and Mind2Web dataset as a case study.

Result: The approach reveals actionable weaknesses missed by standard metrics, providing more granular diagnostic capabilities for web agent performance.

Conclusion: The proposed framework paves the way for more robust and generalizable web agents by enabling detailed error analysis and systematic improvement.

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [4] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench is the first benchmark for predicting founder success in venture capital, featuring 9,000 anonymized founder profiles with strong privacy protection. It shows LLMs significantly outperform human investors, with DeepSeek-V3 achieving 6x baseline precision.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like SWE-bench and ARC-AGI accelerate AGI progress, but there's no standardized benchmark for VC founder success prediction - a domain with sparse signals and high uncertainty where even top investors perform modestly.

Method: Created VCBench with 9,000 anonymized founder profiles standardized to preserve predictive features while resisting identity leakage. Conducted adversarial tests showing >90% reduction in re-identification risk. Evaluated nine state-of-the-art LLMs against human benchmarks.

Result: Market index precision: 1.9%. Y Combinator: 1.7x better than index. Tier-1 firms: 2.9x better. DeepSeek-V3: over 6x baseline precision. GPT-4o: highest F0.5 score. Most LLMs surpassed human benchmarks.

Conclusion: VCBench establishes a community-driven standard for reproducible and privacy-preserving evaluation of AGI in early-stage venture forecasting, available as a public evolving resource at vcbench.com.

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [5] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: OpenLens AI is an automated framework for health informatics research that integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation with vision-language capabilities for medical visualization and quality control.


<details>
  <summary>Details</summary>
Motivation: Health informatics research involves diverse data modalities and requires integration across biomedical science, data analytics, and clinical practice. Existing LLM-based agent systems lack medical visualization interpretation and domain-specific quality requirements.

Method: The framework integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language feedback for medical visualization and quality control for reproducibility.

Result: OpenLens AI automates the entire research pipeline and produces publication-ready LaTeX manuscripts with transparent and traceable workflows.

Conclusion: OpenLens AI provides a domain-adapted solution for advancing health informatics research by addressing the limitations of existing systems through specialized agents and quality control mechanisms.

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [6] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: The paper proposes a new paradigm for defining True Intelligence (TI) with six core components, creating a five-level AGI taxonomy based on measurable cognitive architectures rather than performance metrics.


<details>
  <summary>Details</summary>
Motivation: Current performance-based AGI definitions lack clear research roadmaps and fail to capture the qualitative nature of genuine intelligence, necessitating a mechanism-focused approach inspired by human cognitive processes.

Method: Drawing from neuroscience, psychology, and AI research, the authors define TI through six components and propose a five-level taxonomy where AGI levels correspond to the number of measurable cognitive components implemented.

Result: A practical framework with developmental milestones that provides a clear path for building genuinely intelligent systems, suggesting Level-5 AGI (all five measurable components) is functionally equivalent to True Intelligence.

Conclusion: This mechanism-based definition offers the first holistic, actionable research path for AGI development, bridging the gap between performance metrics and genuine cognitive architectures while addressing the philosophical debate about consciousness.

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [7] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: A dual-layered security framework for multi-agent systems using Sentinel Agents for continuous monitoring and Coordinator Agents for governance, successfully tested against 162 synthetic attacks.


<details>
  <summary>Details</summary>
Motivation: To enhance security and reliability in multi-agent systems against threats like prompt injection, collusive behavior, LLM hallucinations, privacy breaches, and coordinated attacks.

Method: Proposes a framework with Sentinel Agents (distributed security layer using LLM semantic analysis, behavioral analytics, verification, anomaly detection) and Coordinator Agents (policy implementation, threat response, agent management).

Result: Successfully detected 162 synthetic attacks of different families (prompt injection, hallucination, data exfiltration) in a multi-agent conversational environment simulation.

Conclusion: The dual-layered approach provides dynamic adaptive defense, enhanced observability, regulatory compliance support, and enables policy evolution, confirming practical feasibility through simulation testing.

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [8] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: Bayesian Measurement Layouts analyze AI agent cooperation in Melting Pot contest, revealing that top performers may exploit evaluation limitations rather than demonstrating true prosocial capabilities.


<details>
  <summary>Details</summary>
Motivation: To develop better methods for evaluating social AI capabilities, particularly cooperation, in complex multi-agent environments where current evaluation frameworks may be vulnerable to exploitation.

Method: Applied Bayesian Measurement Layouts to infer capability profiles of multi-agent systems in the Melting Pot contest, analyzing performance correlations with prosocial abilities.

Result: Higher prosocial capabilities don't always correlate with better performance; top submissions achieved high scores in scenarios where cooperation wasn't required, suggesting potential exploitation of evaluation limitations.

Conclusion: Measurement Layouts provide accurate predictions and actionable insights for improving AI evaluation, highlighting the need for better cooperation demand annotation and bias mitigation in testing environments.

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [9] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: DeKeyNLU dataset improves NL2SQL accuracy by addressing task decomposition and keyword extraction issues in RAG pipelines, with DeKeySQL achieving significant performance gains on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing NL2SQL approaches struggle with inaccurate task decomposition and keyword extraction by LLMs, leading to SQL generation errors. Current datasets have limitations with task fragmentation and lack domain-specific keyword annotations.

Method: Created DeKeyNLU dataset with 1,500 annotated QA pairs, then developed DeKeySQL - a RAG-based NL2SQL pipeline with three modules: question understanding, entity retrieval, and SQL generation, fine-tuned on the new dataset.

Result: Fine-tuning with DeKeyNLU significantly improved SQL generation accuracy: from 62.31% to 69.10% on BIRD dev dataset and from 84.2% to 88.7% on Spider dev dataset.

Conclusion: The DeKeyNLU dataset effectively addresses key bottlenecks in NL2SQL systems, and the DeKeySQL pipeline demonstrates substantial improvements in SQL generation accuracy through better task decomposition and keyword extraction.

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [10] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: First benchmark for evaluating comprehensive rationality of LLMs across thinking and action domains, with toolkit and analysis showing where LLMs converge/diverge from human rationality.


<details>
  <summary>Details</summary>
Motivation: Concern about whether and under what circumstances LLMs think and behave like real human agents, as rationality is crucial for assessing human behavior in both thinking and action.

Method: Proposed benchmark covering wide range of domains and LLMs, including easy-to-use toolkit and extensive experimental evaluation.

Result: Analysis illuminates where LLMs converge and diverge from idealized human rationality across different domains.

Conclusion: Benchmark serves as foundational tool for both developers and users of LLMs to evaluate their rationality capabilities.

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [11] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: Proposes a dynamic framework for automated LLM workflow construction that combines Q-table learning with a priori decision-making to optimize task-specific workflows, achieving 4.05% average improvement while reducing costs to 30.68%-48.31% of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous workflow construction approaches rely too heavily on historical experience, limiting efficiency and adaptability. The authors argue that workflow construction should flexibly respond to unique task characteristics rather than just historical patterns.

Method: A priori dynamic framework using Q-table learning to optimize decision space and guide agent decisions. Agents evaluate current task progress and make a priori decisions about next executing agent. Includes cold-start initialization, early stopping, and pruning mechanisms for efficiency.

Result: Experimental evaluations on four benchmark datasets show 4.05% average improvement over state-of-the-art baselines, while reducing workflow construction and inference costs to only 30.68%-48.31% of existing methods.

Conclusion: The proposed framework effectively combines historical experience with task-specific adaptability, demonstrating both performance improvements and significant cost reductions in automated LLM workflow construction.

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [12] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: This paper addresses privacy challenges in data-driven decision support by introducing a comprehensive evaluation framework for differential privacy text generation, benchmarking state-of-the-art methods, and developing membership inference attacks to test privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: Data sharing barriers in high-stakes domains like healthcare and finance due to regulatory and privacy concerns, coupled with inadequate anonymization methods and unpredictable AI behaviors in sensitive environments.

Method: Three key contributions: 1) Comprehensive evaluation framework with standardized metrics and 9 curated datasets capturing domain complexities, 2) Large-scale empirical study benchmarking DP text generation methods and LLMs, 3) Development of membership inference attack methodology for synthetic text.

Result: High-quality domain-specific synthetic data generation under DP constraints remains an unsolved challenge with performance degrading as domain complexity increases. Use of public datasets in pre-training can invalidate claimed privacy guarantees.

Conclusion: Urgent need for rigorous privacy auditing and highlights persistent gaps between open-domain and specialist evaluations, informing responsible deployment of generative AI in privacy-sensitive settings.

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [13] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass is a novel evaluation framework for post-deployment monitoring and debugging of LLM-based multi-agent workflows, featuring structured analysis and dual memory systems for continuous learning.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods fail to capture errors, emergent behaviors, and systemic failures in complex multi-agent LLM workflows, creating risks for organizations adopting these systems.

Method: A multi-stage analytical pipeline modeling expert debuggers: error identification/categorization, thematic clustering, quantitative scoring, and strategic summarization, enhanced with dual episodic/semantic memory for continual learning.

Result: Achieves state-of-the-art results on TRAIL benchmark, uncovers critical issues missed by human annotations, and demonstrates practical utility through real-world deployments with design partners.

Conclusion: AgentCompass serves as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production environments.

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [14] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: Applying Schoenfeld's Episode Theory to analyze Large Reasoning Models' thought structures through cognitive labeling of math problem solutions.


<details>
  <summary>Details</summary>
Motivation: Lack of principled framework to understand how Large Reasoning Models structure their chain-of-thought reasoning, despite generating extensive reasoning traces.

Method: Annotated thousands of sentences/paragraphs from model-generated math solutions using seven cognitive labels (Plan, Implement, Verify, etc.) based on Schoenfeld's cognitive framework.

Result: Created first publicly available benchmark for fine-grained analysis of machine reasoning with annotated corpus and annotation guides. Preliminary analysis reveals distinct patterns in LRM reasoning and transition dynamics between cognitive states.

Conclusion: Provides theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [15] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: RationAnomaly is a novel framework that combines Chain-of-Thought fine-tuning with reinforcement learning to improve log anomaly detection, addressing interpretability and reliability issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing log anomaly detection approaches face limitations: traditional deep learning models lack interpretability and generalization, while LLM-based methods suffer from unreliability and factual inaccuracies.

Method: The framework uses CoT-guided supervised fine-tuning with expert-corrected data to instill reasoning patterns, followed by reinforcement learning with a multi-faceted reward function to optimize accuracy and logical consistency while reducing hallucinations.

Result: RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs.

Conclusion: The proposed approach effectively addresses the limitations of existing methods by combining CoT reasoning with reinforcement learning, resulting in more reliable and interpretable log anomaly detection with released code and datasets.

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [16] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo is a Japanese riddle-based benchmark for testing insight reasoning in LLMs, showing most models underperform humans except GPT-5, with reasoning models outperforming non-reasoning ones regardless of size.


<details>
  <summary>Details</summary>
Motivation: Address benchmark saturation and contamination issues in LLM evaluation by creating a cost-effective, extensible benchmark that tests insight-based reasoning.

Method: Built benchmark from Japanese children's riddles - short items requiring no specialized knowledge, scalable generation. Evaluated 38 frontier models and 126 humans on 120 riddles, with extended analysis on 201 items including thought log analysis.

Result: No model except GPT-5 comparable to human performance (52.9% mean accuracy). Reasoning models significantly outperform non-reasoning peers, model size shows no reliable association with accuracy. Analysis reveals verification failure - models produce correct solutions but fail to select them as final answers.

Conclusion: Nazonazo provides cost-effective, scalable, renewable benchmark format addressing evaluation crisis while revealing meta-cognitive weaknesses, offering clear targets for future control and calibration methods.

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [17] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: AC-RAG framework uses adversarial collaboration between a generalist Detector and domain-specialized Resolver to address retrieval hallucinations in RAG systems, significantly improving performance.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented Generation (RAG) suffers from 'Retrieval Hallucinations' where models fail to recognize poor-quality retrieved documents, undermining performance in domain-specific applications.

Method: Proposes AC-RAG framework with two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides solutions. They engage in adversarial collaboration guided by a moderator, enabling iterative problem dissection and refined knowledge retrieval.

Result: Extensive experiments show AC-RAG significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.

Conclusion: The adversarial collaboration approach effectively addresses retrieval hallucinations in RAG systems, demonstrating superior performance through dynamic problem-solving between specialized agents.

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [18] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: Transformer-based AI framework predicts CPE infection risks and patient outcomes from hospital EMR data, outperforming traditional models and identifying key risk factors through explainable AI.


<details>
  <summary>Details</summary>
Motivation: Carbapenemase-Producing Enterobacteriace (CPE) poses critical infection control challenges in hospitals, but predictive modeling of CPE-associated risks like readmission, mortality, and extended length of stay remains underexplored with modern deep learning approaches.

Method: Developed an explainable AI framework using Transformer-based architectures (including TabTransformer) benchmarked against traditional ML models. Analyzed inpatient EMR data including diagnostic codes, ward transitions, demographics, infection variables, and contact network features. Applied XAI techniques to interpret model decisions.

Result: TabTransformer consistently outperformed baseline models across multiple clinical prediction tasks, especially for CPE acquisition (high AUROC and sensitivity). Infection-related features, historical hospital exposure, admission context, and network centrality measures were highly influential. Key risk factors identified included Area of Residence, Admission Ward, prior admissions, and Ward PageRank network variables.

Conclusion: The study presents a robust explainable AI framework for analyzing complex EMR data to predict CPE-related outcomes and identify key risk factors. Transformer models showed superior performance, and diverse clinical and network features proved crucial for accurate predictions.

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [19] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas BrÃ¤nnstrÃ¶m,VicenÃ§ Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: Generalization of single-argument contribution functions to set-based functions in quantitative bipolar argumentation graphs, with new principles for set interactions and application to recommendation systems.


<details>
  <summary>Details</summary>
Motivation: To extend existing quantitative bipolar argumentation frameworks by developing functions that measure the collective contribution of argument sets (rather than individual arguments) to a target argument's strength.

Method: Generalize existing single-argument contribution functions to set-based functions, develop new principles specific to set interactions, and conduct principle-based analysis across different set contribution functions.

Result: Development of set contribution functions that quantify how groups of arguments collectively influence a topic argument's strength, with new principles addressing set-specific properties and interactions.

Conclusion: Set contribution functions provide a more comprehensive framework for analyzing collective argument influence in bipolar argumentation, with practical applications in systems like recommendation engines where multiple arguments interact.

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [20] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: KAMAC is a dynamic multi-agent framework that enables LLM agents to form and expand expert teams based on diagnostic context, outperforming existing methods in complex medical scenarios.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent collaboration frameworks use static pre-assigned roles, which limit adaptability and dynamic knowledge integration needed for complex medical decision-making.

Method: KAMAC starts with expert agents and conducts knowledge-driven discussions to identify gaps, then recruits additional specialists as needed to form flexible, scalable teams.

Result: Experiments on real-world medical benchmarks show KAMAC significantly outperforms single-agent and advanced multi-agent methods, especially in complex scenarios like cancer prognosis.

Conclusion: The framework enables dynamic, cross-specialty expertise integration and supports flexible collaboration in complex clinical scenarios through adaptive team formation.

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [21] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: Study shows generative AI can provide effective formative assessment feedback in online graduate courses by approximating human feedback qualities like directive clarity, supportive stance, and balanced critique.


<details>
  <summary>Details</summary>
Motivation: To investigate how generative AI can support formative assessment through machine-generated reviews of peer reviews in online graduate education.

Method: Analyzed 120 metareviews using Systemic Functional Linguistics and Appraisal Theory to examine how AI feedback constructs meaning across ideational, interpersonal, and textual dimensions.

Result: Generative AI feedback demonstrated key features of effective human feedback - directive clarity, supportive stance, balanced praise/critique, rubric alignment, and structured staging that promotes student agency.

Conclusion: AI metafeedback has potential to scaffold feedback literacy and enhance learner engagement with peer review by modeling effective feedback qualities.

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [22] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: Survey on maritime professionals' perceptions of XAI for trust and usability in autonomous maritime operations


<details>
  <summary>Details</summary>
Motivation: As AI systems become integral to maritime operations, trust depends on transparency and interpretability, not just performance. Effective human-machine teaming requires explainable AI for informed oversight in complex maritime environments.

Method: Proposed a domain-specific survey designed to capture maritime professionals' perceptions of trust, usability, and explainability of XAI systems.

Result: The paper presents a framework for assessing XAI acceptance in maritime domain but does not report specific survey results - focuses on methodology development.

Conclusion: XAI is essential foundation for maritime human-AI collaboration. The proposed survey aims to guide development of user-centric XAI systems tailored to seafarers' needs and foster awareness about explainability requirements.

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [23] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: MACA is a reinforcement learning framework that improves language model reasoning consistency through multi-agent debate and consensus alignment, achieving significant performance gains across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Language models are inconsistent reasoners that generate contradictory responses to identical prompts, and current inference-time methods don't address the core problem of reliable reasoning pathway selection.

Method: Multi-Agent Consensus Alignment (MACA) - a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with internal consensus using majority/minority outcomes from multi-agent debate with deliberative exchanges.

Result: Substantial improvements: +27.6% on GSM8K, +23.7% on MATH, +22.4% Pass@20 on MATH, +42.7% on MathQA, and strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA).

Conclusion: MACA enables robust self-alignment that more reliably unlocks the latent reasoning potential of language models through teaching agents to be decisive, concise, and better leverage peer insights without external supervision.

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [24] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: RLVR method enhances multimodal LLMs' geometric reasoning by generating high-quality training data with verifiable rewards, improving performance on both geometric and non-geometric tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs struggle with complex geometric problems due to lack of high-quality image-text datasets and limited generalization of template-based data synthesis methods.

Method: Introduces Reinforcement Learning with Verifiable Rewards (RLVR) to refine captions for geometric images synthesized from 50 basic relations, using reward signals from math problem-solving tasks.

Result: Achieves 2.8%-4.8% accuracy improvements on MathVista/MathVerse non-geometric tasks and 2.4%-3.9% improvements on MMMU Art/Design/Tech/Engineering tasks.

Conclusion: RLVR pipeline effectively captures geometry problem-solving features, enables better generalization, and enhances general reasoning capabilities of multimodal LLMs even in out-of-distribution scenarios.

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>


### [25] [Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention](https://arxiv.org/abs/2506.11445)
*Xuan Duy Ta,Bang Giang Le,Thanh Ha Le,Viet Cuong Ta*

Main category: cs.AI

TL;DR: Proposes Local State Attention module using self-attention to help autonomous vehicles handle conflicts in mixed-traffic environments, showing improved merging efficiency in highway scenarios.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles need to adapt to human drivers and unexpected situations in mixed traffic, but current MARL methods struggle with local conflicts and generalization to stochastic events.

Method: Introduces a Local State Attention module that uses self-attention to compress essential information from nearby agents, helping resolve traffic conflicts in a highway merging scenario with priority vehicles as unexpected events.

Result: Significant improvements in merging efficiency compared to popular baselines, particularly in high-density traffic settings.

Conclusion: The attention-based approach effectively prioritizes vehicle information to manage merging processes and handle unexpected events in cooperative multi-agent reinforcement learning environments.

Abstract: In mixed-traffic environments, autonomous vehicles must adapt to
human-controlled vehicles and other unusual driving situations. This setting
can be framed as a multi-agent reinforcement learning (MARL) environment with
full cooperative reward among the autonomous vehicles. While methods such as
Multi-agent Proximal Policy Optimization can be effective in training MARL
tasks, they often fail to resolve local conflict between agents and are unable
to generalize to stochastic events. In this paper, we propose a Local State
Attention module to assist the input state representation. By relying on the
self-attention operator, the module is expected to compress the essential
information of nearby agents to resolve the conflict in traffic situations.
Utilizing a simulated highway merging scenario with the priority vehicle as the
unexpected event, our approach is able to prioritize other vehicles'
information to manage the merging process. The results demonstrate significant
improvements in merging efficiency compared to popular baselines, especially in
high-density traffic settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Discovering New Theorems via LLMs with In-Context Proof Learning in Lean](https://arxiv.org/abs/2509.14274)
*Kazumi Kasaura,Naoto Onda,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.LG

TL;DR: LLMs can automatically generate and prove novel mathematical theorems using a Conjecturing-Proving Loop pipeline with in-context learning of previous proofs.


<details>
  <summary>Details</summary>
Motivation: Previous works focused on solving existing problems, but this paper explores LLMs' ability to discover novel mathematical theorems automatically.

Method: Proposed Conjecturing-Proving Loop pipeline that generates conjectures and proves them in Lean 4 format, using context from previously generated theorems and proofs for in-context learning without changing LLM parameters.

Result: The framework rediscovered theorems published in past mathematical papers that hadn't been formalized, and demonstrated that in-context learning was essential as at least one theorem couldn't be proved without it.

Conclusion: In-context learning is effective for neural theorem proving, enabling LLMs to generate and prove increasingly difficult mathematical theorems through iterative learning from previous proofs.

Abstract: Large Language Models have demonstrated significant promise in formal theorem
proving. However, previous works mainly focus on solving existing problems. In
this paper, we focus on the ability of LLMs to find novel theorems. We propose
Conjecturing-Proving Loop pipeline for automatically generating mathematical
conjectures and proving them in Lean 4 format. A feature of our approach is
that we generate and prove further conjectures with context including
previously generated theorems and their proofs, which enables the generation of
more difficult proofs by in-context learning of proof strategies without
changing parameters of LLMs. We demonstrated that our framework rediscovered
theorems with verification, which were published in past mathematical papers
and have not yet formalized. Moreover, at least one of these theorems could not
be proved by the LLM without in-context learning, even in natural language,
which means that in-context learning was effective for neural theorem proving.
The source code is available at
https://github.com/auto-res/ConjecturingProvingLoop.

</details>


### [27] [A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation](https://arxiv.org/abs/2509.14384)
*Nishantak Panigrahi,Mayank Patwal*

Main category: cs.LG

TL;DR: DNNs for approximating nonlocal conservation law solutions from Kuramoto model, with focus on architecture impact on accuracy and computation time.


<details>
  <summary>Details</summary>
Motivation: Investigate efficiency of Deep Neural Networks for solving nonlocal conservation laws derived from identical-oscillator Kuramoto model, evaluating architectural choices and their impact on solution accuracy.

Method: Systematic experimentation with various network configurations: activation functions (tanh, sin, ReLU), network depth (4-8 hidden layers), width (64-256 neurons), and training methodology (collocation points, epoch count). Comparative analysis with traditional numerical methods.

Result: Tanh activation provides stable convergence, sine activation achieves slightly lower errors and faster training in some cases but may produce nonphysical artifacts. Optimally configured DNNs offer competitive accuracy with different computational trade-offs compared to traditional methods. Standard feed-forward architectures have limitations with singular or piecewise-constant solutions due to oversmoothing of sharp features.

Conclusion: Provides empirical guidelines for DNN implementation in scientific computing while identifying fundamental theoretical constraints that limit applicability to physical systems with discontinuities, due to function space limitations of standard activation functions.

Abstract: In this paper, we investigate the efficiency of Deep Neural Networks (DNNs)
to approximate the solution of a nonlocal conservation law derived from the
identical-oscillator Kuramoto model, focusing on the evaluation of an
architectural choice and its impact on solution accuracy based on the energy
norm and computation time. Through systematic experimentation, we demonstrate
that network configuration parameters-specifically, activation function
selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width
(64-256 neurons), and training methodology (collocation points, epoch
count)-significantly influence convergence characteristics. We observe that
tanh activation yields stable convergence across configurations, whereas sine
activation can attain marginally lower errors and training times in isolated
cases, but occasionally produce nonphysical artefacts. Our comparative analysis
with traditional numerical methods shows that optimally configured DNNs offer
competitive accuracy with notably different computational trade-offs.
Furthermore, we identify fundamental limitations of standard feed-forward
architectures when handling singular or piecewise-constant solutions, providing
empirical evidence that such networks inherently oversmooth sharp features due
to the natural function space limitations of standard activation functions.
This work contributes to the growing body of research on neural network-based
scientific computing by providing practitioners with empirical guidelines for
DNN implementation while illuminating fundamental theoretical constraints that
must be overcome to expand their applicability to more challenging physical
systems with discontinuities.

</details>


### [28] [Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility](https://arxiv.org/abs/2509.14386)
*Arjun S. Nair,Kristina P. Sinaga*

Main category: cs.LG

TL;DR: Neural networks cannot learn well-calibrated confidence estimates with meaningful diversity when trained using binary correct/incorrect supervision due to information-theoretic constraints.


<details>
  <summary>Details</summary>
Motivation: To understand why neural networks struggle with confidence calibration and diversity, and to prove this is a fundamental limitation rather than a methodological issue.

Method: Rigorous mathematical analysis and comprehensive empirical evaluation including negative reward training, symmetric loss functions, and post-hoc calibration methods across MNIST, Fashion-MNIST, and CIFAR-10 datasets.

Result: Universal failure patterns: negative rewards cause extreme underconfidence (ECE > 0.8) and destroy confidence diversity (std < 0.05), symmetric losses fail to escape binary signal averaging, and post-hoc methods achieve calibration (ECE < 0.02) only by compressing confidence distribution. 100% failure rate for all training methods.

Conclusion: Binary supervision creates an underspecified mapping problem that cannot distinguish between different confidence levels for correct predictions. Post-hoc calibration is mathematically necessary, and novel supervision paradigms using ensemble disagreement and adaptive multi-agent learning are proposed to overcome these limitations.

Abstract: We prove a fundamental impossibility theorem: neural networks cannot
simultaneously learn well-calibrated confidence estimates with meaningful
diversity when trained using binary correct/incorrect supervision. Through
rigorous mathematical analysis and comprehensive empirical evaluation spanning
negative reward training, symmetric loss functions, and post-hoc calibration
methods, we demonstrate this is an information-theoretic constraint, not a
methodological failure. Our experiments reveal universal failure patterns:
negative rewards produce extreme underconfidence (ECE greater than 0.8) while
destroying confidence diversity (std less than 0.05), symmetric losses fail to
escape binary signal averaging, and post-hoc methods achieve calibration (ECE
less than 0.02) only by compressing the confidence distribution. We formalize
this as an underspecified mapping problem where binary signals cannot
distinguish between different confidence levels for correct predictions: a 60
percent confident correct answer receives identical supervision to a 90 percent
confident one. Crucially, our real-world validation shows 100 percent failure
rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while
post-hoc calibration's 33 percent success rate paradoxically confirms our
theorem by achieving calibration through transformation rather than learning.
This impossibility directly explains neural network hallucinations and
establishes why post-hoc calibration is mathematically necessary, not merely
convenient. We propose novel supervision paradigms using ensemble disagreement
and adaptive multi-agent learning that could overcome these fundamental
limitations without requiring human confidence annotations.

</details>


### [29] [Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs](https://arxiv.org/abs/2509.14391)
*Ye Qiao,Sitao Huang*

Main category: cs.LG

TL;DR: Combining position interpolation (PI) with post-training quantization (PTQ) degrades accuracy in long-context LLMs. Q-ROAR introduces RoPE-aware stabilization with per-band scaling to recover accuracy without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Extending LLM context windows is crucial for long-range tasks, but combining position interpolation methods with quantization causes accuracy degradation due to position-dependent noise effects.

Method: Propose Q-ROAR - a RoPE-aware weight-only stabilization that groups RoPE dimensions into frequency bands and performs small search over per-band scales for W_Q and W_K matrices, guided by diagnostics like Interpolation Pressure and Tail Inflation Ratios.

Result: Q-ROAR recovers up to 0.7% accuracy on standard tasks, reduces GovReport perplexity by more than 10%, while preserving short-context performance and compatibility with existing inference stacks.

Conclusion: The method effectively addresses the degradation caused by combining PI with PTQ through targeted diagnostics and stabilization without requiring fine-tuning or architectural changes.

Abstract: Extending LLM context windows is crucial for long range tasks. RoPE-based
position interpolation (PI) methods like linear and frequency-aware scaling
extend input lengths without retraining, while post-training quantization (PTQ)
enables practical deployment. We show that combining PI with PTQ degrades
accuracy due to coupled effects long context aliasing, dynamic range dilation,
axis grid anisotropy, and outlier shifting that induce position-dependent logit
noise. We provide the first systematic analysis of PI plus PTQ and introduce
two diagnostics: Interpolation Pressure (per-band phase scaling sensitivity)
and Tail Inflation Ratios (outlier shift from short to long contexts). To
address this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that
groups RoPE dimensions into a few frequency bands and performs a small search
over per-band scales for W_Q,W_K, with an optional symmetric variant to
preserve logit scale. The diagnostics guided search uses a tiny long-context
dev set and requires no fine-tuning, kernel, or architecture changes.
Empirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces
GovReport perplexity by more than 10%, while preserving short-context
performance and compatibility with existing inference stacks.

</details>


### [30] [Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models](https://arxiv.org/abs/2509.14427)
*Ilyass Moummad,Kawtar Zaher,Lukas Rauch,Alexis Joly*

Main category: cs.LG

TL;DR: Hashing-Baseline is a training-free hashing method that combines classical techniques (PCA, random orthogonal projection, threshold binarization) with frozen pretrained embeddings from state-of-the-art encoders for competitive retrieval performance without additional training.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art hashing methods require expensive, scenario-specific training, which limits scalability and practical deployment. There's a need for strong training-free hashing approaches that can leverage powerful pretrained models.

Method: Combines classical training-free hashing techniques (principal component analysis, random orthogonal projection, threshold binarization) with frozen embeddings from state-of-the-art vision and audio encoders without any additional learning or fine-tuning.

Result: The approach yields competitive retrieval performance on standard image retrieval benchmarks and a newly introduced audio hashing benchmark, demonstrating generality and effectiveness.

Conclusion: Hashing-Baseline provides a strong training-free baseline for hashing that leverages pretrained encoders and classical techniques, offering competitive performance without the need for expensive scenario-specific training.

Abstract: Information retrieval with compact binary embeddings, also referred to as
hashing, is crucial for scalable fast search applications, yet state-of-the-art
hashing methods require expensive, scenario-specific training. In this work, we
introduce Hashing-Baseline, a strong training-free hashing method leveraging
powerful pretrained encoders that produce rich pretrained embeddings. We
revisit classical, training-free hashing techniques: principal component
analysis, random orthogonal projection, and threshold binarization, to produce
a strong baseline for hashing. Our approach combines these techniques with
frozen embeddings from state-of-the-art vision and audio encoders to yield
competitive retrieval performance without any additional learning or
fine-tuning. To demonstrate the generality and effectiveness of this approach,
we evaluate it on standard image retrieval benchmarks as well as a newly
introduced benchmark for audio hashing.

</details>


### [31] [FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport](https://arxiv.org/abs/2509.14444)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: FedAVOT addresses FL bias from client availability mismatch by using optimal transport to align availability and importance distributions, achieving stable convergence even with minimal client participation.


<details>
  <summary>Details</summary>
Motivation: Federated Learning suffers from biased updates when client participation distribution doesn't match the optimization objective distribution, leading to unstable training.

Method: Proposes FedAVOT that formulates aggregation as a masked optimal transport problem using Sinkhorn scaling to compute transport-based weights with convergence guarantees.

Result: Achieves O(1/âT) convergence rate independent of participating users, with drastically improved performance over FedAvg in heterogeneous, fairness-sensitive, and low-availability scenarios.

Conclusion: FedAVOT provides a principled solution to FL aggregation bias through optimal transport alignment, enabling stable training even with only two clients per round.

Abstract: Federated Learning (FL) allows distributed model training without sharing raw
data, but suffers when client participation is partial. In practice, the
distribution of available users (\emph{availability distribution} $q$) rarely
aligns with the distribution defining the optimization objective
(\emph{importance distribution} $p$), leading to biased and unstable updates
under classical FedAvg. We propose \textbf{Fereated AVerage with Optimal
Transport (\textbf{FedAVOT})}, which formulates aggregation as a masked optimal
transport problem aligning $q$ and $p$. Using Sinkhorn scaling,
\textbf{FedAVOT} computes transport-based aggregation weights with provable
convergence guarantees. \textbf{FedAVOT} achieves a standard
$\mathcal{O}(1/\sqrt{T})$ rate under a nonsmooth convex FL setting, independent
of the number of participating users per round. Our experiments confirm
drastically improved performance compared to FedAvg across heterogeneous,
fairness-sensitive, and low-availability regimes, even when only two clients
participate per round.

</details>


### [32] [H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations](https://arxiv.org/abs/2509.14472)
*Mahsa Khazaei,Azim Ahmadzadeh,Alexei Pevtsov,Luca Bertello,Alexander Pevtsov*

Main category: cs.LG

TL;DR: Lightweight non-ML anomaly detection algorithm for H-alpha solar observations that outperforms existing methods while providing explainable results.


<details>
  <summary>Details</summary>
Motivation: Large volumes of solar observation data from GONG network require quality control for ML processing, but existing methods lack transparency.

Method: Developed H-Alpha Anomalyzer algorithm that identifies anomalies based on user-defined criteria, highlighting specific regions and quantifying anomaly likelihood.

Result: Outperforms existing methods and provides explainability for domain expert evaluation, with 2,000 observation dataset created for comparison.

Conclusion: Proposed lightweight algorithm effectively detects anomalies in solar data while maintaining transparency and explainability for expert validation.

Abstract: The plethora of space-borne and ground-based observatories has provided
astrophysicists with an unprecedented volume of data, which can only be
processed at scale using advanced computing algorithms. Consequently, ensuring
the quality of data fed into machine learning (ML) models is critical. The
H$\alpha$ observations from the GONG network represent one such data stream,
producing several observations per minute, 24/7, since 2010. In this study, we
introduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha
Anomalyzer, designed to identify anomalous observations based on user-defined
criteria. Unlike many black-box algorithms, our approach highlights exactly
which regions triggered the anomaly flag and quantifies the corresponding
anomaly likelihood. For our comparative analysis, we also created and released
a dataset of 2,000 observations, equally divided between anomalous and
non-anomalous cases. Our results demonstrate that the proposed model not only
outperforms existing methods but also provides explainability, enabling
qualitative evaluation by domain experts.

</details>


### [33] [Decentralized Optimization with Topology-Independent Communication](https://arxiv.org/abs/2509.14488)
*Ying Lin,Yao Kuang,Ahmet Alacaoglu,Michael P. Friedlander*

Main category: cs.LG

TL;DR: Randomized local coordination method reduces communication from O(m) to exactly 2 messages per iteration for graph-guided regularizers by sampling individual regularizers instead of requiring global coordination.


<details>
  <summary>Details</summary>
Motivation: Full synchronization in distributed optimization scales poorly, and standard methods require O(m) communications per iteration when n nodes collaborate through m pairwise regularizers, which becomes inefficient for large-scale problems.

Method: Proposes randomized local coordination where each node independently samples one regularizer uniformly and coordinates only with nodes sharing that term, replacing the proximal map of the sum with the proximal map of a single randomly selected regularizer.

Result: Achieves expected communication of exactly 2 messages per iteration for graph-guided regularizers (|S_j|=2), with convergence rates of O~(Îµâ»Â²) for convex objectives, O(Îµâ»Â¹) to Îµ-solution under strong convexity, and O(log(1/Îµ)) to a neighborhood.

Conclusion: The method preserves convergence while eliminating global coordination, and experiments validate both convergence rates and communication efficiency across synthetic and real-world datasets.

Abstract: Distributed optimization requires nodes to coordinate, yet full
synchronization scales poorly. When $n$ nodes collaborate through $m$ pairwise
regularizers, standard methods demand $\mathcal{O}(m)$ communications per
iteration. This paper proposes randomized local coordination: each node
independently samples one regularizer uniformly and coordinates only with nodes
sharing that term. This exploits partial separability, where each regularizer
$G_j$ depends on a subset $S_j \subseteq \{1,\ldots,n\}$ of nodes. For
graph-guided regularizers where $|S_j|=2$, expected communication drops to
exactly 2 messages per iteration. This method achieves
$\tilde{\mathcal{O}}(\varepsilon^{-2})$ iterations for convex objectives and
under strong convexity, $\mathcal{O}(\varepsilon^{-1})$ to an
$\varepsilon$-solution and $\mathcal{O}(\log(1/\varepsilon))$ to a
neighborhood. Replacing the proximal map of the sum $\sum_j G_j$ with the
proximal map of a single randomly selected regularizer $G_j$ preserves
convergence while eliminating global coordination. Experiments validate both
convergence rates and communication efficiency across synthetic and real-world
datasets.

</details>


### [34] [BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning](https://arxiv.org/abs/2509.14519)
*Wadduwage Shanika Perera,Haodi Jiang*

Main category: cs.LG

TL;DR: BEACON is a deep learning framework that uses LLMs to generate behavioral embeddings from sandbox reports and 1D CNN for malware classification, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional static analysis fails against modern malware using obfuscation and evasion techniques, while behavioral detection provides more reliable runtime monitoring.

Method: Leverages large language models to generate dense contextual embeddings from raw sandbox behavior reports, then processes with 1D CNN for multi-class malware classification.

Result: Consistently outperforms existing methods on the Avast-CTU Public CAPE Dataset, demonstrating effectiveness of LLM-based behavioral embeddings.

Conclusion: BEACON framework proves effective for robust malware classification using LLM-based behavioral analysis, offering superior performance against modern threats.

Abstract: Malware is becoming increasingly complex and widespread, making it essential
to develop more effective and timely detection methods. Traditional static
analysis often fails to defend against modern threats that employ code
obfuscation, polymorphism, and other evasion techniques. In contrast,
behavioral malware detection, which monitors runtime activities, provides a
more reliable and context-aware solution. In this work, we propose BEACON, a
novel deep learning framework that leverages large language models (LLMs) to
generate dense, contextual embeddings from raw sandbox-generated behavior
reports. These embeddings capture semantic and structural patterns of each
sample and are processed by a one-dimensional convolutional neural network (1D
CNN) for multi-class malware classification. Evaluated on the Avast-CTU Public
CAPE Dataset, our framework consistently outperforms existing methods,
highlighting the effectiveness of LLM-based behavioral embeddings and the
overall design of BEACON for robust malware classification.

</details>


### [35] [Predicting Case Suffixes With Activity Start and End Times: A Sweep-Line Based Approach](https://arxiv.org/abs/2509.14536)
*Muhammad Awais Ali,Marlon Dumas,Fredrik Milani*

Main category: cs.LG

TL;DR: A technique for predicting business process case suffixes with start/end timestamps using multi-model approach and sweep-line method for better resource planning.


<details>
  <summary>Details</summary>
Motivation: Existing case suffix prediction methods only provide single timestamps, which is insufficient for resource capacity planning that requires understanding when resources will be busy performing work.

Method: Proposes a technique that predicts case suffixes with both start and end timestamps, using a sweep-line approach to predict suffixes of all ongoing cases in lockstep rather than in isolation, accounting for resource availability across cases.

Result: Evaluation on real-life and synthetic datasets shows improved accuracy compared to existing approaches, demonstrating advantages of the multi-model approach.

Conclusion: The proposed technique provides more comprehensive predictions for resource capacity planning by capturing both waiting and processing times through coordinated prediction of all ongoing cases.

Abstract: Predictive process monitoring techniques support the operational decision
making by predicting future states of ongoing cases of a business process. A
subset of these techniques predict the remaining sequence of activities of an
ongoing case (case suffix prediction). Existing approaches for case suffix
prediction generate sequences of activities with a single timestamp (e.g. the
end timestamp). This output is insufficient for resource capacity planning,
where we need to reason about the periods of time when resources will be busy
performing work. This paper introduces a technique for predicting case suffixes
consisting of activities with start and end timestamps. In other words, the
proposed technique predicts both the waiting time and the processing time of
each activity. Since the waiting time of an activity in a case depends on how
busy resources are in other cases, the technique adopts a sweep-line approach,
wherein the suffixes of all ongoing cases in the process are predicted in
lockstep, rather than predictions being made for each case in isolation. An
evaluation on real-life and synthetic datasets compares the accuracy of
different instantiations of this approach, demonstrating the advantages of a
multi-model approach to case suffix prediction.

</details>


### [36] [LiMuon: Light and Fast Muon Optimizer for Large Models](https://arxiv.org/abs/2509.14562)
*Feihu Huang,Yuning Luo,Songcan Chen*

Main category: cs.LG

TL;DR: LiMuon optimizer: a light and fast Muon variant using momentum-based variance reduction and randomized SVD for efficient large model training with lower memory and better sample complexity.


<details>
  <summary>Details</summary>
Motivation: Existing Muon optimizers suffer from high sample complexity or high memory requirements for large models, and current convergence analysis relies on strict Lipschitz smooth assumptions that don't hold for tasks like LLM training.

Method: Proposes LiMuon optimizer based on momentum-based variance reduced technique and randomized Singular Value Decomposition (SVD) to reduce memory usage while maintaining efficiency.

Result: LiMuon achieves lower memory than current Muon variants and proves O(Îµâ»Â³) sample complexity for non-convex stochastic optimization under both smooth and generalized smooth conditions. Experimental results on DistilGPT2 and ViT models verify efficiency.

Conclusion: LiMuon provides an efficient optimizer for large model training with reduced memory requirements and proven theoretical guarantees under more realistic smoothness conditions applicable to modern AI tasks.

Abstract: Large models recently are widely applied in artificial intelligence, so
efficient training of large models has received widespread attention. More
recently, a useful Muon optimizer is specifically designed for
matrix-structured parameters of large models. Although some works have begun to
studying Muon optimizer, the existing Muon and its variants still suffer from
high sample complexity or high memory for large models. To fill this gap, we
propose a light and fast Muon (LiMuon) optimizer for training large models,
which builds on the momentum-based variance reduced technique and randomized
Singular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory
than the current Muon and its variants. Moreover, we prove that our LiMuon has
a lower sample complexity of $O(\epsilon^{-3})$ for finding an
$\epsilon$-stationary solution of non-convex stochastic optimization under the
smooth condition. Recently, the existing convergence analysis of Muon optimizer
mainly relies on the strict Lipschitz smooth assumption, while some artificial
intelligence tasks such as training large language models (LLMs) do not satisfy
this condition. We also proved that our LiMuon optimizer has a sample
complexity of $O(\epsilon^{-3})$ under the generalized smooth condition.
Numerical experimental results on training DistilGPT2 and ViT models verify
efficiency of our LiMuon optimizer.

</details>


### [37] [Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework](https://arxiv.org/abs/2509.14563)
*Shiyuan Luo,Runlong Yu,Chonghao Qiu,Rahul Ghosh,Robert Ladwig,Paul C. Hanson,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: A$^2$SL framework uses self-supervised learning to retrieve relevant environmental data for improving predictions in data-scarce conditions, particularly for freshwater ecosystem modeling.


<details>
  <summary>Details</summary>
Motivation: High cost of environmental data collection and poor generalization of existing ML approaches in data-sparse or atypical conditions.

Method: Multi-level pairwise learning loss for scenario encoder, retrieval mechanism for relevant data supplementation, and augmentation-adaptive mechanism for targeted data augmentation in extreme conditions.

Result: Significantly improves predictive accuracy and enhances robustness in data-scarce and atypical scenarios for water temperature and dissolved oxygen dynamics.

Conclusion: A$^2$SL offers a broadly applicable solution for various scientific domains beyond freshwater ecosystems.

Abstract: The discovery of environmental knowledge depends on labeled task-specific
data, but is often constrained by the high cost of data collection. Existing
machine learning approaches usually struggle to generalize in data-sparse or
atypical conditions. To this end, we propose an Augmentation-Adaptive
Self-Supervised Learning (A$^2$SL) framework, which retrieves relevant
observational samples to enhance modeling of the target ecosystem.
Specifically, we introduce a multi-level pairwise learning loss to train a
scenario encoder that captures varying degrees of similarity among scenarios.
These learned similarities drive a retrieval mechanism that supplements a
target scenario with relevant data from different locations or time periods.
Furthermore, to better handle variable scenarios, particularly under atypical
or extreme conditions where traditional models struggle, we design an
augmentation-adaptive mechanism that selectively enhances these scenarios
through targeted data augmentation. Using freshwater ecosystems as a case
study, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen
dynamics in real-world lakes. Experimental results show that A$^2$SL
significantly improves predictive accuracy and enhances robustness in
data-scarce and atypical scenarios. Although this study focuses on freshwater
ecosystems, the A$^2$SL framework offers a broadly applicable solution in
various scientific domains.

</details>


### [38] [Evidential Physics-Informed Neural Networks for Scientific Discovery](https://arxiv.org/abs/2509.14568)
*Hai Siong Tan,Kuancheng Wang,Rafe McBeth*

Main category: cs.LG

TL;DR: E-PINN is a novel uncertainty-aware Physics-Informed Neural Network that uses evidential deep learning for uncertainty estimation and parameter inference, outperforming Bayesian PINN and Deep Ensemble methods in calibration.


<details>
  <summary>Details</summary>
Motivation: To develop a more reliable uncertainty-aware PINN framework that can better estimate uncertainty in outputs and infer unknown PDE parameters through learned posterior distributions.

Method: Leverages marginal distribution loss function from evidential deep learning for uncertainty estimation, infers unknown PDE parameters via learned posterior distribution, validated on 1D Poisson equation and 2D Fisher-KPP equation.

Result: E-PINN generated empirical coverage probabilities that were calibrated significantly better than Bayesian PINN and Deep Ensemble methods, and demonstrated real-world applicability in clinical glucose-insulin dataset analysis.

Conclusion: E-PINN provides superior uncertainty calibration compared to existing methods and shows practical applicability in real-world medical research scenarios.

Abstract: We present the fundamental theory and implementation guidelines underlying
Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of
uncertainty-aware PINN. It leverages the marginal distribution loss function of
evidential deep learning for estimating uncertainty of outputs, and infers
unknown parameters of the PDE via a learned posterior distribution. Validating
our model on two illustrative case studies -- the 1D Poisson equation with a
Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated
empirical coverage probabilities that were calibrated significantly better than
Bayesian PINN and Deep Ensemble methods. To demonstrate real-world
applicability, we also present a brief case study on applying E-PINN to analyze
clinical glucose-insulin datasets that have featured in medical research on
diabetes pathophysiology.

</details>


### [39] [Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition](https://arxiv.org/abs/2509.14577)
*Yang Xu,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: SPMD-LRT is a tensor-based margin distribution learning method that preserves multi-dimensional structure and uses low-rank decomposition for efficient high-order tensor classification.


<details>
  <summary>Details</summary>
Motivation: Existing LMDM methods require vectorization of tensor data, which destroys multi-mode structure and increases computational burden for high-dimensional data.

Method: Proposes structure-preserving margin distribution learning with low-rank tensor decomposition (CP and Tucker) and alternating optimization algorithm to update factor matrices and core tensor.

Result: Superior classification accuracy on MNIST, images, and fMRI data compared to SVM, vector-based LMDM, and tensor-based SVM extensions, with Tucker decomposition achieving highest accuracy.

Conclusion: SPMD-LRT effectively handles high-dimensional tensor data while preserving structural information and optimizing margin distribution for improved classification performance.

Abstract: The Large Margin Distribution Machine (LMDM) is a recent advancement in
classifier design that optimizes not just the minimum margin (as in SVM) but
the entire margin distribution, thereby improving generalization. However,
existing LMDM formulations are limited to vectorized inputs and struggle with
high-dimensional tensor data due to the need for flattening, which destroys the
data's inherent multi-mode structure and increases computational burden. In
this paper, we propose a Structure-Preserving Margin Distribution Learning for
High-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates
directly on tensor representations without vectorization. The SPMD-LRT
preserves multi-dimensional spatial structure by incorporating first-order and
second-order tensor statistics (margin mean and variance) into the objective,
and it leverages low-rank tensor decomposition techniques including rank-1(CP),
higher-rank CP, and Tucker decomposition to parameterize the weight tensor. An
alternating optimization (double-gradient descent) algorithm is developed to
efficiently solve the SPMD-LRT, iteratively updating factor matrices and core
tensor. This approach enables SPMD-LRT to maintain the structural information
of high-order data while optimizing margin distribution for improved
classification. Extensive experiments on diverse datasets (including MNIST,
images and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior
classification accuracy compared to conventional SVM, vector-based LMDM, and
prior tensor-based SVM extensions (Support Tensor Machines and Support Tucker
Machines). Notably, SPMD-LRT with Tucker decomposition attains the highest
accuracy, highlighting the benefit of structure preservation. These results
confirm the effectiveness and robustness of SPMD-LRT in handling
high-dimensional tensor data for classification.

</details>


### [40] [Online reinforcement learning via sparse Gaussian mixture model Q-functions](https://arxiv.org/abs/2509.14585)
*Minh Vu,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: Online RL framework using sparse Gaussian mixture model Q-functions that achieves performance comparable to dense DeepRL methods with fewer parameters and better generalization in low-parameter regimes.


<details>
  <summary>Details</summary>
Motivation: To develop an interpretable online policy-iteration framework that extends previous offline GMM-QF work to leverage streaming data for exploration while maintaining model simplicity through sparsification.

Method: Uses sparse Gaussian mixture model Q-functions with Hadamard overparametrization for sparsification, employs Riemannian manifold structure for parameter updates via online gradient descent on smooth objectives.

Result: Numerical tests show S-GMM-QFs match dense DeepRL performance on standard benchmarks using significantly fewer parameters, and maintain strong performance in low-parameter regimes where sparsified DeepRL methods fail.

Conclusion: The proposed structured online policy-iteration framework with sparse GMM-QFs provides an effective and interpretable alternative to dense DeepRL methods, offering comparable performance with better parameter efficiency and generalization capabilities.

Abstract: This paper introduces a structured and interpretable online policy-iteration
framework for reinforcement learning (RL), built around the novel class of
sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work
that trained GMM-QFs offline, the proposed framework develops an online scheme
that leverages streaming data to encourage exploration. Model complexity is
regulated through sparsification by Hadamard overparametrization, which
mitigates overfitting while preserving expressiveness. The parameter space of
S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing
for principled parameter updates via online gradient descent on a smooth
objective. Numerical tests show that S-GMM-QFs match the performance of dense
deep RL (DeepRL) methods on standard benchmarks while using significantly fewer
parameters, and maintain strong performance even in low-parameter-count regimes
where sparsified DeepRL methods fail to generalize.

</details>


### [41] [TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics](https://arxiv.org/abs/2509.14600)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Razvan Marinescu*

Main category: cs.LG

TL;DR: Adding energy matching to coarse-grained ML models for molecular dynamics doesn't significantly improve accuracy but reveals insights about free energy surface generalization.


<details>
  <summary>Details</summary>
Motivation: Molecular dynamics simulations are computationally expensive for long timescales, and conventional force matching approaches often fail to capture the full thermodynamic landscape of biomolecular systems.

Method: Incorporated a complementary energy matching term into the loss function of CGSchNet model for Chignolin protein, systematically varying the weight of the energy loss term.

Result: Energy matching did not yield statistically significant improvements in accuracy, but revealed distinct tendencies in how models generalize the free energy surface.

Conclusion: Future opportunities exist to enhance coarse-grained modeling through improved energy estimation techniques and multi-modal loss formulations.

Abstract: Molecular dynamics (MD) simulations provide atomistic insight into
biomolecular systems but are often limited by high computational costs required
to access long timescales. Coarse-grained machine learning models offer a
promising avenue for accelerating sampling, yet conventional force matching
approaches often fail to capture the full thermodynamic landscape as fitting a
model on the gradient may not fit the absolute differences between low-energy
conformational states. In this work, we incorporate a complementary energy
matching term into the loss function. We evaluate our framework on the
Chignolin protein using the CGSchNet model, systematically varying the weight
of the energy loss term. While energy matching did not yield statistically
significant improvements in accuracy, it revealed distinct tendencies in how
models generalize the free energy surface. Our results suggest future
opportunities to enhance coarse-grained modeling through improved energy
estimation techniques and multi-modal loss formulations.

</details>


### [42] [Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking](https://arxiv.org/abs/2509.14603)
*Xingchen Wang,Feijie Wu,Chenglin Miao,Tianchun Li,Haoyu Hu,Qiming Cao,Jing Gao,Lu Su*

Main category: cs.LG

TL;DR: PM-SFL is a privacy-preserving Split Federated Learning framework that uses probabilistic mask training instead of noise injection to protect against data reconstruction attacks while maintaining model performance under data and system heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Split Federated Learning reduces client computation but introduces privacy risks from intermediate activation exchanges. Existing noise-based defenses degrade model performance, and there's a need for solutions that handle both data and system heterogeneity effectively.

Method: Uses probabilistic mask training to add structured randomness without explicit noise, personalized mask learning for data heterogeneity, and layer-wise knowledge compensation for system heterogeneity with adaptive model splitting.

Result: Theoretical privacy protection confirmed, experiments show improved accuracy, communication efficiency, and robustness to privacy attacks, with strong performance under heterogeneous conditions.

Conclusion: PM-SFL provides an effective privacy-preserving solution for Split Federated Learning that maintains utility while addressing both data and system heterogeneity challenges.

Abstract: Split Federated Learning (SFL) has emerged as an efficient alternative to
traditional Federated Learning (FL) by reducing client-side computation through
model partitioning. However, exchanging of intermediate activations and model
updates introduces significant privacy risks, especially from data
reconstruction attacks that recover original inputs from intermediate
representations. Existing defenses using noise injection often degrade model
performance. To overcome these challenges, we present PM-SFL, a scalable and
privacy-preserving SFL framework that incorporates Probabilistic Mask training
to add structured randomness without relying on explicit noise. This mitigates
data reconstruction risks while maintaining model utility. To address data
heterogeneity, PM-SFL employs personalized mask learning that tailors submodel
structures to each client's local data. For system heterogeneity, we introduce
a layer-wise knowledge compensation mechanism, enabling clients with varying
resources to participate effectively under adaptive model splitting.
Theoretical analysis confirms its privacy protection, and experiments on image
and wireless sensing tasks demonstrate that PM-SFL consistently improves
accuracy, communication efficiency, and robustness to privacy attacks, with
particularly strong performance under data and system heterogeneity.

</details>


### [43] [HD3C: Efficient Medical Data Classification for Embedded Devices](https://arxiv.org/abs/2509.14617)
*Jianglan Wei,Zhenyu Zhang,Pengcheng Wang,Mingjie Zeng,Zhigang Zeng*

Main category: cs.LG

TL;DR: HD3C is a hyperdimensional computing framework for energy-efficient medical data classification that achieves 350x better energy efficiency than Bayesian ResNet with minimal accuracy loss, while being robust to noise and limited data.


<details>
  <summary>Details</summary>
Motivation: Deep learning models have high energy consumption and GPU dependency, making them unsuitable for embedded medical devices in home and field healthcare settings where energy efficiency is critical.

Method: HD3C encodes data into high-dimensional hypervectors, aggregates them into multiple cluster-specific prototypes, and performs classification through similarity search in hyperspace.

Result: On heart sound classification, HD3C shows 350x better energy efficiency than Bayesian ResNet with less than 1% accuracy difference, and demonstrates exceptional robustness to noise, limited training data, and hardware errors.

Conclusion: HD3C provides a lightweight, energy-efficient classification framework suitable for reliable deployment in real-world low-power medical applications, with both theoretical and empirical validation of its robustness.

Abstract: Energy-efficient medical data classification is essential for modern disease
screening, particularly in home and field healthcare where embedded devices are
prevalent. While deep learning models achieve state-of-the-art accuracy, their
substantial energy consumption and reliance on GPUs limit deployment on such
platforms. We present Hyperdimensional Computing with Class-Wise Clustering
(HD3C), a lightweight classification framework designed for low-power
environments. HD3C encodes data into high-dimensional hypervectors, aggregates
them into multiple cluster-specific prototypes, and performs classification
through similarity search in hyperspace. We evaluate HD3C across three medical
classification tasks; on heart sound classification, HD3C is $350\times$ more
energy-efficient than Bayesian ResNet with less than 1% accuracy difference.
Moreover, HD3C demonstrates exceptional robustness to noise, limited training
data, and hardware error, supported by both theoretical analysis and empirical
results, highlighting its potential for reliable deployment in real-world
settings. Code is available at https://github.com/jianglanwei/HD3C.

</details>


### [44] [CUFG: Curriculum Unlearning Guided by the Forgetting Gradient](https://arxiv.org/abs/2509.14633)
*Jiaxing Miao,Liang Hu,Qi Zhang,Lai Zhong Yuan,Usman Naseem*

Main category: cs.LG

TL;DR: CUFG is a curriculum-based unlearning framework that uses forgetting gradients and progressive data scheduling to enable stable and effective knowledge removal from AI models, addressing limitations of existing aggressive unlearning methods.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods prioritize efficiency and aggressive forgetting too much, which can destabilize model weights, cause collapse, and reduce reliability. There's a need for more stable and progressive unlearning approaches.

Method: Proposes CUFG framework with two innovations: 1) a gradient corrector guided by forgetting gradients for fine-tuning-based unlearning, and 2) a curriculum unlearning paradigm that progressively forgets from easy to hard data samples.

Result: CUFG narrows the gap with the gold-standard Retrain method by enabling more stable and progressive unlearning, improving both effectiveness and reliability. Extensive experiments validate the approach across various forgetting scenarios.

Conclusion: The curriculum unlearning concept has substantial research potential and offers forward-looking insights for machine unlearning field development. CUFG provides a more stable alternative to existing aggressive unlearning methods.

Abstract: As privacy and security take center stage in AI, machine unlearning, the
ability to erase specific knowledge from models, has garnered increasing
attention. However, existing methods overly prioritize efficiency and
aggressive forgetting, which introduces notable limitations. In particular,
radical interventions like gradient ascent, influence functions, and random
label noise can destabilize model weights, leading to collapse and reduced
reliability. To address this, we propose CUFG (Curriculum Unlearning via
Forgetting Gradients), a novel framework that enhances the stability of
approximate unlearning through innovations in both forgetting mechanisms and
data scheduling strategies. Specifically, CUFG integrates a new gradient
corrector guided by forgetting gradients for fine-tuning-based unlearning and a
curriculum unlearning paradigm that progressively forgets from easy to hard.
These innovations narrow the gap with the gold-standard Retrain method by
enabling more stable and progressive unlearning, thereby improving both
effectiveness and reliability. Furthermore, we believe that the concept of
curriculum unlearning has substantial research potential and offers
forward-looking insights for the development of the MU field. Extensive
experiments across various forgetting scenarios validate the rationale and
effectiveness of our approach and CUFG. Codes are available at
https://anonymous.4open.science/r/CUFG-6375.

</details>


### [45] [DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers](https://arxiv.org/abs/2509.14640)
*Habib Irani,Vangelis Metsis*

Main category: cs.LG

TL;DR: DyWPE introduces signal-aware positional encoding using wavelet transforms for time series analysis, outperforming traditional position-agnostic methods by 9.1% on average.


<details>
  <summary>Details</summary>
Motivation: Existing positional encoding methods ignore signal characteristics and are problematic for time series with complex, non-stationary dynamics across multiple temporal scales.

Method: Dynamic Wavelet Positional Encoding (DyWPE) framework that generates positional embeddings directly from input time series using Discrete Wavelet Transform (DWT).

Result: Outperforms 8 state-of-the-art positional encoding methods across 10 diverse time series datasets, achieving 9.1% average improvement over baseline sinusoidal encoding in biomedical signals while maintaining computational efficiency.

Conclusion: Signal-aware positional encoding using wavelet transforms is superior to traditional position-agnostic methods for time series analysis, providing better performance without sacrificing efficiency.

Abstract: Existing positional encoding methods in transformers are fundamentally
signal-agnostic, deriving positional information solely from sequence indices
while ignoring the underlying signal characteristics. This limitation is
particularly problematic for time series analysis, where signals exhibit
complex, non-stationary dynamics across multiple temporal scales. We introduce
Dynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework
that generates positional embeddings directly from input time series using the
Discrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time
series datasets demonstrate that DyWPE consistently outperforms eight existing
state-of-the-art positional encoding methods, achieving average relative
improvements of 9.1\% compared to baseline sinusoidal absolute position
encoding in biomedical signals, while maintaining competitive computational
efficiency.

</details>


### [46] [DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training](https://arxiv.org/abs/2509.14642)
*Yuemin Wu,Zhongze Wu,Xiu Su,Feng Yang,Hongyan Xu,Xi Lin,Wenti Huang,Shan You,Chang Xu*

Main category: cs.LG

TL;DR: DeCoP is a dependency-controlled pre-training framework that addresses temporal variability in time series by modeling dynamic multi-scale dependencies through instance-wise patch normalization and hierarchical dependency learning.


<details>
  <summary>Details</summary>
Motivation: Existing time series pre-training models struggle with dynamic temporal dependencies caused by distribution shifts and multi-scale patterns, leading to poor generalization and susceptibility to spurious correlations.

Method: DeCoP uses Instance-wise Patch Normalization (IPN) to handle distributional shifts while preserving patch characteristics, and a hierarchical Dependency Controlled Learning (DCL) strategy with Instance-level Contrastive Module (ICM) to model inter-patch dependencies across multiple temporal scales.

Result: Achieves state-of-the-art results on ten datasets with lower computing resources, improving MSE by 3% on ETTh1 over PatchTST while using only 37% of the FLOPs.

Conclusion: DeCoP effectively addresses temporal variability in time series pre-training by explicitly modeling dynamic multi-scale dependencies, demonstrating superior performance and computational efficiency compared to existing methods.

Abstract: Modeling dynamic temporal dependencies is a critical challenge in time series
pre-training, which evolve due to distribution shifts and multi-scale patterns.
This temporal variability severely impairs the generalization of pre-trained
models to downstream tasks. Existing frameworks fail to capture the complex
interactions of short- and long-term dependencies, making them susceptible to
spurious correlations that degrade generalization. To address these
limitations, we propose DeCoP, a Dependency Controlled Pre-training framework
that explicitly models dynamic, multi-scale dependencies by simulating evolving
inter-patch dependencies. At the input level, DeCoP introduces Instance-wise
Patch Normalization (IPN) to mitigate distributional shifts while preserving
the unique characteristics of each patch, creating a robust foundation for
representation learning. At the latent level, a hierarchical Dependency
Controlled Learning (DCL) strategy explicitly models inter-patch dependencies
across multiple temporal scales, with an Instance-level Contrastive Module
(ICM) enhances global generalization by learning instance-discriminative
representations from time-invariant positive pairs. DeCoP achieves
state-of-the-art results on ten datasets with lower computing resources,
improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.

</details>


### [47] [Stochastic Clock Attention for Aligning Continuous and Ordered Sequences](https://arxiv.org/abs/2509.14678)
*Hyungjoon Soh,Junghyo Jo*

Main category: cs.LG

TL;DR: A novel attention mechanism using learned nonnegative clocks for continuous sequences that enforces continuity and monotonicity without external positional encodings, improving alignment stability in sequence-to-sequence tasks.


<details>
  <summary>Details</summary>
Motivation: Standard attention mechanisms lack explicit enforcement of continuity and monotonicity, which are crucial for frame-synchronous targets in sequence-to-sequence tasks like text-to-speech.

Method: Proposes learned nonnegative clocks for source and target sequences, modeling attention as meeting probability of these clocks. Uses path-integral derivation to create Gaussian-like scoring with intrinsic bias toward causal, smooth, near-diagonal alignments.

Result: Produces more stable alignments and improved robustness to global time-scaling while matching or improving accuracy over scaled dot-product baselines in Transformer text-to-speech applications.

Conclusion: The clock-based attention framework provides effective drop-in replacements for standard attention with intrinsic alignment properties, potentially applicable to other continuous targets like video and temporal signal modeling.

Abstract: We formulate an attention mechanism for continuous and ordered sequences that
explicitly functions as an alignment model, which serves as the core of many
sequence-to-sequence tasks. Standard scaled dot-product attention relies on
positional encodings and masks but does not enforce continuity or monotonicity,
which are crucial for frame-synchronous targets. We propose learned nonnegative
\emph{clocks} to source and target and model attention as the meeting
probability of these clocks; a path-integral derivation yields a closed-form,
Gaussian-like scoring rule with an intrinsic bias toward causal, smooth,
near-diagonal alignments, without external positional regularizers. The
framework supports two complementary regimes: normalized clocks for parallel
decoding when a global length is available, and unnormalized clocks for
autoregressive decoding -- both nearly-parameter-free, drop-in replacements. In
a Transformer text-to-speech testbed, this construction produces more stable
alignments and improved robustness to global time-scaling while matching or
improving accuracy over scaled dot-product baselines. We hypothesize
applicability to other continuous targets, including video and temporal signal
modeling.

</details>


### [48] [ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning](https://arxiv.org/abs/2509.14718)
*Zihao Feng,Xiaoxue Wang,Bowen Wu,Hailong Cao,Tiejun Zhao,Qun Yu,Baoxun Wang*

Main category: cs.LG

TL;DR: DSCL framework improves RL efficiency for LLM tool learning by dynamically sampling valuable data and focusing on less-mastered sub-tasks using reward statistics and curriculum learning.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic sampling techniques are inadequate for tool learning's multi-task structure and fine-grained rewards, leading to inefficient training with diminishing returns from simple samples.

Method: DSCL combines Reward-Based Dynamic Sampling (using multi-dimensional reward mean/variance) and Task-Based Dynamic Curriculum Learning to prioritize valuable data and focus on challenging sub-tasks.

Result: Achieves 3.29% improvement on BFCLv3 benchmark, demonstrating significant gains in training efficiency and model performance over strong baselines.

Conclusion: DSCL provides a tailored solution that effectively leverages complex reward signals and sub-task dynamics in tool learning to achieve superior results.

Abstract: While reinforcement learning (RL) is increasingly used for LLM-based tool
learning, its efficiency is often hampered by an overabundance of simple
samples that provide diminishing learning value as training progresses.
Existing dynamic sampling techniques are ill-suited for the multi-task
structure and fine-grained reward mechanisms inherent to tool learning. This
paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework
specifically designed to address this challenge by targeting the unique
characteristics of tool learning: its multiple interdependent sub-tasks and
multi-valued reward functions. DSCL features two core components: Reward-Based
Dynamic Sampling, which uses multi-dimensional reward statistics (mean and
variance) to prioritize valuable data, and Task-Based Dynamic Curriculum
Learning, which adaptively focuses training on less-mastered sub-tasks. Through
extensive experiments, we demonstrate that DSCL significantly improves training
efficiency and model performance over strong baselines, achieving a 3.29\%
improvement on the BFCLv3 benchmark. Our method provides a tailored solution
that effectively leverages the complex reward signals and sub-task dynamics
within tool learning to achieve superior results.

</details>


### [49] [Towards Pre-trained Graph Condensation via Optimal Transport](https://arxiv.org/abs/2509.14722)
*Yeyu Yan,Shuai Zheng,Wenjun Hui,Xiangkai Zhu,Dong Chen,Zhenfeng Zhu,Yao Zhao,Kunlun He*

Main category: cs.LG

TL;DR: PreGC is a novel graph condensation method that uses optimal transport to create task-independent and architecture-agnostic condensed graphs, overcoming limitations of traditional GC methods that rely on specific GNNs and task supervision.


<details>
  <summary>Details</summary>
Motivation: Traditional graph condensation methods are heavily dependent on specific GNN architectures and task-specific supervision, which limits their reusability and generalization across different tasks and models.

Method: Proposes PreGC with hybrid-interval graph diffusion augmentation to enhance generalization, establishes matching between optimal graph transport plan and representation transport plan for semantic consistency, and introduces a traceable semantic harmonizer to bridge semantic associations.

Result: Extensive experiments show PreGC's superiority and versatility, demonstrating task-independent nature and seamless compatibility with arbitrary GNN architectures.

Conclusion: PreGC successfully transcends the limitations of traditional GC methods by providing a generalized approach that works across various tasks and GNN architectures through optimal transport and semantic consistency mechanisms.

Abstract: Graph condensation (GC) aims to distill the original graph into a small-scale
graph, mitigating redundancy and accelerating GNN training. However,
conventional GC approaches heavily rely on rigid GNNs and task-specific
supervision. Such a dependency severely restricts their reusability and
generalization across various tasks and architectures. In this work, we revisit
the goal of ideal GC from the perspective of GNN optimization consistency, and
then a generalized GC optimization objective is derived, by which those
traditional GC methods can be viewed nicely as special cases of this
optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)
via optimal transport is proposed to transcend the limitations of task- and
architecture-dependent GC methods. Specifically, a hybrid-interval graph
diffusion augmentation is presented to suppress the weak generalization ability
of the condensed graph on particular architectures by enhancing the uncertainty
of node states. Meanwhile, the matching between optimal graph transport plan
and representation transport plan is tactfully established to maintain semantic
consistencies across source graph and condensed graph spaces, thereby freeing
graph condensation from task dependencies. To further facilitate the adaptation
of condensed graphs to various downstream tasks, a traceable semantic
harmonizer from source nodes to condensed nodes is proposed to bridge semantic
associations through the optimized representation transport plan in
pre-training. Extensive experiments verify the superiority and versatility of
PreGC, demonstrating its task-independent nature and seamless compatibility
with arbitrary GNNs.

</details>


### [50] [Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models](https://arxiv.org/abs/2509.14723)
*Sosuke Hosokawa,Toshiharu Kawakami,Satoshi Kodera,Masamichi Ito,Norihiko Takeda*

Main category: cs.LG

TL;DR: Training a transcoder on cell2sentence model to extract interpretable decision circuits from single-cell foundation models, revealing biologically meaningful pathways.


<details>
  <summary>Details</summary>
Motivation: Single-cell foundation models lack interpretability compared to traditional methods, making their decision processes opaque despite strong performance.

Method: Train a transcoder on the cell2sentence (C2S) model to extract internal decision-making circuits from this state-of-the-art single-cell foundation model.

Result: The discovered circuits correspond to real-world biological mechanisms, demonstrating biologically plausible pathways within the complex model.

Conclusion: Transcoders show promising potential for uncovering interpretable biological decision pathways in complex single-cell foundation models.

Abstract: Single-cell foundation models (scFMs) have demonstrated state-of-the-art
performance on various tasks, such as cell-type annotation and perturbation
response prediction, by learning gene regulatory networks from large-scale
transcriptome data. However, a significant challenge remains: the
decision-making processes of these models are less interpretable compared to
traditional methods like differential gene expression analysis. Recently,
transcoders have emerged as a promising approach for extracting interpretable
decision circuits from large language models (LLMs). In this work, we train a
transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By
leveraging the trained transcoder, we extract internal decision-making circuits
from the C2S model. We demonstrate that the discovered circuits correspond to
real-world biological mechanisms, confirming the potential of transcoders to
uncover biologically plausible pathways within complex single-cell models.

</details>


### [51] [One-step Multi-view Clustering With Adaptive Low-rank Anchor-graph Learning](https://arxiv.org/abs/2509.14724)
*Zhiyuan Xue,Ben Yang,Xuetao Zhang,Fei Wang,Zhiping Lin*

Main category: cs.LG

TL;DR: Proposes OMCAL, a one-step multi-view clustering method with adaptive low-rank anchor-graph learning to address redundancy and noise issues in existing anchor graph methods while improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing anchor graph-based multi-view clustering methods suffer from redundant information and noise in consensus anchor graphs, and require inefficient post-processing for clustering indicators.

Method: Uses nuclear norm-based adaptive consensus anchor graph learning to handle information redundancy and noise, and integrates category indicator acquisition with anchor graph learning in a unified framework.

Result: Extensive experiments show OMCAL outperforms state-of-the-art methods in both clustering effectiveness and efficiency on ordinary and large-scale datasets.

Conclusion: OMCAL successfully addresses the limitations of existing AGMC methods by providing a unified one-step framework that handles redundancy/noise and improves both effectiveness and efficiency.

Abstract: In light of their capability to capture structural information while reducing
computing complexity, anchor graph-based multi-view clustering (AGMC) methods
have attracted considerable attention in large-scale clustering problems.
Nevertheless, existing AGMC methods still face the following two issues: 1)
They directly embedded diverse anchor graphs into a consensus anchor graph
(CAG), and hence ignore redundant information and numerous noises contained in
these anchor graphs, leading to a decrease in clustering effectiveness; 2) They
drop effectiveness and efficiency due to independent post-processing to acquire
clustering indicators. To overcome the aforementioned issues, we deliver a
novel one-step multi-view clustering method with adaptive low-rank anchor-graph
learning (OMCAL). To construct a high-quality CAG, OMCAL provides a nuclear
norm-based adaptive CAG learning model against information redundancy and noise
interference. Then, to boost clustering effectiveness and efficiency
substantially, we incorporate category indicator acquisition and CAG learning
into a unified framework. Numerous studies conducted on ordinary and
large-scale datasets indicate that OMCAL outperforms existing state-of-the-art
methods in terms of clustering effectiveness and efficiency.

</details>


### [52] [FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration](https://arxiv.org/abs/2509.14775)
*Shuangshuang He,Yuanting Zhang,Hongli Liang,Qingye Meng,Xingyuan Yuan*

Main category: cs.LG

TL;DR: FlowCast-ODE is a continuous flow modeling framework for hourly weather forecasting that addresses error accumulation in autoregressive models and temporal discontinuities in ERA5 data, achieving better accuracy and energy conservation than baselines.


<details>
  <summary>Details</summary>
Motivation: Accurate hourly weather forecasting is critical but challenging due to error accumulation in autoregressive rollouts and temporal discontinuities in the 12-hour assimilation cycle of ERA5 data.

Method: Proposes FlowCast-ODE that models atmospheric state evolution as continuous flow using dynamic flow matching and ODE solver, with coarse-to-fine training strategy and lightweight low-rank AdaLN-Zero modulation mechanism.

Result: Outperforms baselines with lower RMSE, better energy conservation, reduced blurring, preserved spatial details, comparable performance in extreme event forecasting, and alleviated temporal discontinuities.

Conclusion: FlowCast-ODE provides an effective continuous flow approach for stable and accurate hourly weather forecasting that better aligns with physical dynamic systems while reducing model size.

Abstract: Accurate hourly weather forecasting is critical for numerous applications.
Recent deep learning models have demonstrated strong capability on 6-hour
intervals, yet achieving accurate and stable hourly predictions remains a
critical challenge. This is primarily due to the rapid accumulation of errors
in autoregressive rollouts and temporal discontinuities within the ERA5 data's
12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a
framework that models atmospheric state evolution as a continuous flow.
FlowCast-ODE learns the conditional flow path directly from the previous state,
an approach that aligns more naturally with physical dynamic systems and
enables efficient computation. A coarse-to-fine strategy is introduced to train
the model on 6-hour data using dynamic flow matching and then refined on hourly
data that incorporates an Ordinary Differential Equation (ODE) solver to
achieve temporally coherent forecasts. In addition, a lightweight low-rank
AdaLN-Zero modulation mechanism is proposed and reduces model size by 15%
without compromising accuracy. Experiments demonstrate that FlowCast-ODE
outperforms strong baselines, yielding lower root mean square error (RMSE) and
better energy conservation, which reduces blurring and preserves more
fine-scale spatial details. It also shows comparable performance to the
state-of-the-art model in forecasting extreme events like typhoons.
Furthermore, the model alleviates temporal discontinuities associated with
assimilation cycle transitions.

</details>


### [53] [Pre-training under infinite compute](https://arxiv.org/abs/2509.14786)
*Konwoo Kim,Suhas Kotha,Percy Liang,Tatsunori Hashimoto*

Main category: cs.LG

TL;DR: Simple algorithmic improvements enable significantly more data-efficient pre-training through proper regularization tuning, parameter scaling, and ensembling, achieving up to 17.5x data efficiency gains.


<details>
  <summary>Details</summary>
Motivation: As compute grows faster than available web text for language model pre-training, the paper explores how to optimize pre-training under fixed data constraints without compute limitations.

Method: Developed a regularized recipe with 30x larger weight decay than standard practice, combined epoching, parameter scaling, and ensemble scaling. Also used distillation to compress ensembles into smaller student models.

Result: Achieved 5.17x less data usage at 200M tokens, with ensemble distillation producing 8x smaller models retaining 83% of ensemble benefits. Downstream benchmarks showed 9% improvement and 17.5x data efficiency gains on math tasks.

Conclusion: Algorithmic improvements can enable significantly more data-efficient pre-training, with regularization tuning and ensembling providing substantial benefits that persist across different compute budgets.

Abstract: Since compute grows much faster than web text available for language model
pre-training, we ask how one should approach pre-training under fixed data and
no compute constraints. We first show that existing data-constrained approaches
of increasing epoch count and parameter count eventually overfit, and we
significantly improve upon such recipes by properly tuning regularization,
finding that the optimal weight decay is $30\times$ larger than standard
practice. Since our regularized recipe monotonically decreases loss following a
simple power law in parameter count, we estimate its best possible performance
via the asymptote of its scaling law rather than the performance at a fixed
compute budget. We then identify that ensembling independently trained models
achieves a significantly lower loss asymptote than the regularized recipe. Our
best intervention combining epoching, regularization, parameter scaling, and
ensemble scaling achieves an asymptote at 200M tokens using $5.17\times$ less
data than our baseline, and our data scaling laws predict that this improvement
persists at higher token budgets. We find that our data efficiency gains can be
realized at much smaller parameter counts as we can distill an ensemble into a
student model that is 8$\times$ smaller and retains $83\%$ of the ensembling
benefit. Finally, our interventions designed for validation loss generalize to
downstream benchmarks, achieving a $9\%$ improvement for pre-training evals and
a $17.5\times$ data efficiency improvement over continued pre-training on math
mid-training data. Our results show that simple algorithmic improvements can
enable significantly more data-efficient pre-training in a compute-rich future.

</details>


### [54] [Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery](https://arxiv.org/abs/2509.14788)
*Jing Lan,Hexiao Ding,Hongzhao Chen,Yufeng Jiang,Nga-Chun Ng,Gwing Kei Yip,Gerald W. Y. Cheng,Yunlin Mao,Jing Cai,Liang-ting Lin,Jung Sun Yoo*

Main category: cs.LG

TL;DR: A sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability but often lack structural awareness.

Method: Sequence-based DTI framework that integrates structural priors into protein representations, featuring learned aggregation, bilinear attention, and contrastive alignment mechanisms.

Result: Achieves state-of-the-art performance on Human and BioSNAP datasets, remains competitive on BindingDB, and surpasses prior methods on LIT-PCBA virtual screening tasks with substantial gains in AUROC and BEDROC metrics.

Conclusion: The framework validates the utility of integrating structural priors for scalable and structure-aware DTI prediction, with improved spatial correspondence to binding pockets and interpretable attention patterns.

Abstract: Accurate identification of drug-target interactions (DTI) remains a central
challenge in computational pharmacology, where sequence-based methods offer
scalability. This work introduces a sequence-based drug-target interaction
framework that integrates structural priors into protein representations while
maintaining high-throughput screening capability. Evaluated across multiple
benchmarks, the model achieves state-of-the-art performance on Human and
BioSNAP datasets and remains competitive on BindingDB. In virtual screening
tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in
AUROC and BEDROC. Ablation studies confirm the critical role of learned
aggregation, bilinear attention, and contrastive alignment in enhancing
predictive robustness. Embedding visualizations reveal improved spatial
correspondence with known binding pockets and highlight interpretable attention
patterns over ligand-residue contacts. These results validate the framework's
utility for scalable and structure-aware DTI prediction.

</details>


### [55] [STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models](https://arxiv.org/abs/2509.14801)
*Julian F. Schumann,Anna MÃ©szÃ¡ros,Jens Kober,Arkady Zgonnikov*

Main category: cs.LG

TL;DR: STEP is a new benchmarking framework for trajectory prediction models that addresses limitations of existing frameworks by providing unified dataset interfaces, consistent evaluation conditions, and support for various prediction models.


<details>
  <summary>Details</summary>
Motivation: Standardized evaluation practices for trajectory prediction models are underdeveloped, with existing frameworks lacking support for heterogeneous traffic scenarios, joint prediction models, and proper documentation.

Method: The authors introduce STEP framework that provides unified interfaces for multiple datasets, enforces consistent training/evaluation conditions, and supports diverse prediction models through comprehensive experiments.

Result: Experiments reveal limitations of current testing procedures, importance of joint agent modeling for interaction prediction, and vulnerability of state-of-the-art models to distribution shifts and adversarial attacks.

Conclusion: STEP aims to shift focus from leaderboard comparisons to deeper insights about model behavior and generalization in complex multi-agent environments.

Abstract: While trajectory prediction plays a critical role in enabling safe and
effective path-planning in automated vehicles, standardized practices for
evaluating such models remain underdeveloped. Recent efforts have aimed to
unify dataset formats and model interfaces for easier comparisons, yet existing
frameworks often fall short in supporting heterogeneous traffic scenarios,
joint prediction models, or user documentation. In this work, we introduce STEP
-- a new benchmarking framework that addresses these limitations by providing a
unified interface for multiple datasets, enforcing consistent training and
evaluation conditions, and supporting a wide range of prediction models. We
demonstrate the capabilities of STEP in a number of experiments which reveal 1)
the limitations of widely-used testing procedures, 2) the importance of joint
modeling of agents for better predictions of interactions, and 3) the
vulnerability of current state-of-the-art models against both distribution
shifts and targeted attacks by adversarial agents. With STEP, we aim to shift
the focus from the ``leaderboard'' approach to deeper insights about model
behavior and generalization in complex multi-agent settings.

</details>


### [56] [Precision Neural Networks: Joint Graph And Relational Learning](https://arxiv.org/abs/2509.14821)
*Andrea Cavallo,Samuel Rey,Antonio G. Marques,Elvin Isufi*

Main category: cs.LG

TL;DR: Precision Neural Networks (PNNs) extend VNNs by using precision matrices instead of covariance matrices, enabling task-aware joint learning of network parameters and precision estimation with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Covariance matrices are dense, fail to encode conditional independence, and are often precomputed in a task-agnostic way, which limits performance. Precision matrices naturally encode statistical independence, exhibit sparsity, and preserve spectral structure.

Method: Formulate an optimization problem for joint learning of network parameters and precision matrix, solved via alternating optimization - sequentially updating network weights and precision estimates with theoretical bounds on estimation accuracy.

Result: Theoretical bounds show distance between estimated and true precision matrices at each iteration. Experiments demonstrate effectiveness of joint estimation compared to two-step approaches on synthetic and real-world data.

Conclusion: PNNs overcome limitations of VNNs by leveraging precision matrices, enabling task-aware joint learning with theoretical guarantees and improved performance over traditional approaches.

Abstract: CoVariance Neural Networks (VNNs) perform convolutions on the graph
determined by the covariance matrix of the data, which enables expressive and
stable covariance-based learning. However, covariance matrices are typically
dense, fail to encode conditional independence, and are often precomputed in a
task-agnostic way, which may hinder performance. To overcome these limitations,
we study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix
-- the inverse covariance. The precision matrix naturally encodes statistical
independence, often exhibits sparsity, and preserves the covariance spectral
structure. To make precision estimation task-aware, we formulate an
optimization problem that jointly learns the network parameters and the
precision matrix, and solve it via alternating optimization, by sequentially
updating the network weights and the precision estimate. We theoretically bound
the distance between the estimated and true precision matrices at each
iteration, and demonstrate the effectiveness of joint estimation compared to
two-step approaches on synthetic and real-world data.

</details>


### [57] [Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization](https://arxiv.org/abs/2509.14832)
*Stelios Zarifis,Ioannis Kordonis,Petros Maragos*

Main category: cs.LG

TL;DR: DST is a diffusion-based framework for building scenario trees that enables better stochastic optimization in energy markets by handling uncertainty more effectively than conventional methods.


<details>
  <summary>Details</summary>
Motivation: Stochastic forecasting is essential for decision-making in uncertain systems like energy markets, where full distribution estimation of future scenarios is crucial for optimization tasks.

Method: Propose Diffusion Scenario Tree (DST) framework that uses diffusion models to recursively sample future trajectories and organizes them into trees via clustering while maintaining non-anticipativity at each stage.

Result: DST consistently outperforms conventional scenario tree models and Model-Free Reinforcement Learning baselines in energy arbitrage optimization, achieving higher performance by better handling uncertainty.

Conclusion: DST provides more efficient decision policies for stochastic optimization compared to deterministic and stochastic MPC variants using the same diffusion-based forecaster.

Abstract: Stochastic forecasting is critical for efficient decision-making in uncertain
systems, such as energy markets and finance, where estimating the full
distribution of future scenarios is essential. We propose Diffusion Scenario
Tree (DST), a general framework for constructing scenario trees for
multivariate prediction tasks using diffusion-based probabilistic forecasting
models. DST recursively samples future trajectories and organizes them into a
tree via clustering, ensuring non-anticipativity (decisions depending only on
observed history) at each stage. We evaluate the framework on the optimization
task of energy arbitrage in New York State's day-ahead electricity market.
Experimental results show that our approach consistently outperforms the same
optimization algorithms that use scenario trees from more conventional models
and Model-Free Reinforcement Learning baselines. Furthermore, using DST for
stochastic optimization yields more efficient decision policies, achieving
higher performance by better handling uncertainty than deterministic and
stochastic MPC variants using the same diffusion-based forecaster.

</details>


### [58] [Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization](https://arxiv.org/abs/2509.14848)
*Houssem Sifaou,Osvaldo Simeone*

Main category: cs.LG

TL;DR: MF-HRL-IGM is a multi-fidelity hybrid RL algorithm that optimizes policy training by selecting simulation fidelity levels based on information gain maximization, achieving no-regret performance under fixed cost budgets.


<details>
  <summary>Details</summary>
Motivation: Traditional RL requires costly high-fidelity simulator interactions, while offline RL is limited by dataset quality. Real-world scenarios often have multiple simulators with varying fidelity and cost, creating a need for efficient multi-fidelity optimization.

Method: Proposes MF-HRL-IGM algorithm that uses information gain maximization through bootstrapping to select optimal fidelity levels for hybrid offline-online RL training under fixed cost constraints.

Result: Theoretical analysis shows no-regret property, and empirical evaluations demonstrate superior performance compared to existing benchmarks in multi-fidelity environments.

Conclusion: MF-HRL-IGM effectively leverages multiple simulators with varying fidelity to optimize RL policy training under cost constraints, providing both theoretical guarantees and practical performance improvements.

Abstract: Optimizing a reinforcement learning (RL) policy typically requires extensive
interactions with a high-fidelity simulator of the environment, which are often
costly or impractical. Offline RL addresses this problem by allowing training
from pre-collected data, but its effectiveness is strongly constrained by the
size and quality of the dataset. Hybrid offline-online RL leverages both
offline data and interactions with a single simulator of the environment. In
many real-world scenarios, however, multiple simulators with varying levels of
fidelity and computational cost are available. In this work, we study
multi-fidelity hybrid RL for policy optimization under a fixed cost budget. We
introduce multi-fidelity hybrid RL via information gain maximization
(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity
selection based on information gain maximization through a bootstrapping
approach. Theoretical analysis establishes the no-regret property of
MF-HRL-IGM, while empirical evaluations demonstrate its superior performance
compared to existing benchmarks.

</details>


### [59] [Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study](https://arxiv.org/abs/2509.14863)
*Zhengwei Wang,Gang Wu*

Main category: cs.LG

TL;DR: G2LFormer introduces a global-to-local attention scheme where shallow layers use attention for global information and deeper layers use GNNs for local structure, preventing information loss while maintaining linear complexity.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Transformer architectures suffer from information loss where local neighborhood information learned by GNNs gets diluted by global attention mechanisms.

Method: Proposes a novel global-to-local attention scheme with shallow attention layers capturing global info and deeper GNN layers learning local structure, plus cross-layer information fusion strategy.

Result: G2LFormer shows excellent performance on both node-level and graph-level tasks while maintaining linear complexity, outperforming state-of-the-art linear GTs and GNNs.

Conclusion: The global-to-local attention scheme is feasible and effective, providing better performance without sacrificing scalability through careful architecture design and information fusion.

Abstract: Graph Transformers (GTs) show considerable potential in graph representation
learning. The architecture of GTs typically integrates Graph Neural Networks
(GNNs) with global attention mechanisms either in parallel or as a precursor to
attention mechanisms, yielding a local-and-global or local-to-global attention
scheme. However, as the global attention mechanism primarily captures
long-range dependencies between nodes, these integration schemes may suffer
from information loss, where the local neighborhood information learned by GNN
could be diluted by the attention mechanism. Therefore, we propose G2LFormer,
featuring a novel global-to-local attention scheme where the shallow network
layers use attention mechanisms to capture global information, while the deeper
layers employ GNN modules to learn local structural information, thereby
preventing nodes from ignoring their immediate neighbors. An effective
cross-layer information fusion strategy is introduced to allow local layers to
retain beneficial information from global layers and alleviate information
loss, with acceptable trade-offs in scalability. To validate the feasibility of
the global-to-local attention scheme, we compare G2LFormer with
state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The
results indicate that G2LFormer exhibits excellent performance while keeping
linear complexity.

</details>


### [60] [DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.14868)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei*

Main category: cs.LG

TL;DR: Ablation studies confirm DPANet's dual-domain fusion approach and cross-attention mechanism are essential for optimal performance


<details>
  <summary>Details</summary>
Motivation: To validate the effectiveness of DPANet's key components and test the hypothesis that fusion of temporal and frequency domains is critical for performance

Method: Conducted ablation studies comparing full DPANet against specialized variants: Temporal-Only model (fusing two temporal pyramids), Frequency-Only model (fusing two spectral pyramids), and a version without cross-attention fusion

Result: Full model consistently outperformed all variants; both specialized versions underperformed significantly; removing cross-attention caused the most severe performance degradation

Conclusion: The fusion of heterogeneous temporal and frequency information is critical, and the interactive cross-attention fusion block is the most essential component of DPANet

Abstract: We conducted rigorous ablation studies to validate DPANet's key components
(Table \ref{tab:ablation-study}). The full model consistently outperforms all
variants. To test our dual-domain hypothesis, we designed two specialized
versions: a Temporal-Only model (fusing two identical temporal pyramids) and a
Frequency-Only model (fusing two spectral pyramids). Both variants
underperformed significantly, confirming that the fusion of heterogeneous
temporal and frequency information is critical. Furthermore, replacing the
cross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the
most severe performance degradation. This result underscores that our
interactive fusion block is the most essential component.

</details>


### [61] [Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis](https://arxiv.org/abs/2509.14887)
*Hoang-Son Nguyen,Hoi-To Wai*

Main category: cs.LG

TL;DR: Vanilla graph learning methods are implicitly robust to partial observations of low-pass filtered graph signals, recovering ground truth topology even with hidden nodes.


<details>
  <summary>Details</summary>
Motivation: Existing graph learning methods are vulnerable to corruption from hidden nodes whose signals are unobservable, but robustness analysis of naive approaches is underexplored.

Method: Extend the restricted isometry property (RIP) to Dirichlet energy function used in graph learning objectives, analyzing smoothness-based formulations like GL-SigRep on partial observations.

Result: Theoretical analysis shows vanilla graph topology learning methods can recover ground truth graph topology corresponding to observed nodes despite hidden nodes.

Conclusion: Synthetic and real data experiments confirm that naive graph learning approaches are inherently robust to partial observations of low-pass filtered graph signals.

Abstract: Learning the graph underlying a networked system from nodal signals is
crucial to downstream tasks in graph signal processing and machine learning.
The presence of hidden nodes whose signals are not observable might corrupt the
estimated graph. While existing works proposed various robustifications of
vanilla graph learning objectives by explicitly accounting for the presence of
these hidden nodes, a robustness analysis of "naive", hidden-node agnostic
approaches is still underexplored. This work demonstrates that vanilla graph
topology learning methods are implicitly robust to partial observations of
low-pass filtered graph signals. We achieve this theoretical result through
extending the restricted isometry property (RIP) to the Dirichlet energy
function used in graph learning objectives. We show that smoothness-based graph
learning formulation (e.g., the GL-SigRep method) on partial observations can
recover the ground truth graph topology corresponding to the observed nodes.
Synthetic and real data experiments corroborate our findings.

</details>


### [62] [Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics](https://arxiv.org/abs/2509.14894)
*Guillermo Hijano Mendizabal,Davide Lancierini,Alex Marshall,Andrea Mauri,Patrick Haworth Owen,Mitesh Patel,Konstantinos Petridis,Shah Rukh Qasim,Nicola Serra,William Sutcliffe,Hanae Tilquin*

Main category: cs.LG

TL;DR: Novel RL+Genetic Algorithm approach to systematically identify critical background processes in beauty hadron decay measurements, overcoming computational limitations and expert dependency.


<details>
  <summary>Details</summary>
Motivation: Experimental beauty hadron decay studies face challenges from numerous backgrounds with similar final states, requiring expert intuition due to computational constraints and lack of systematic methods.

Method: Combines Reinforcement Learning with Genetic Algorithms using transformer architecture; GAs explore trajectory space to find successful paths, guiding RL agent training for sparse reward environments.

Result: Developed systematic approach to determine critical backgrounds in beauty hadron decays, with broader applicability to particle physics measurements.

Conclusion: Proposed RL+GA strategy effectively addresses background identification challenges, providing a systematic alternative to expert-dependent methods while being adaptable beyond beauty hadron physics.

Abstract: Experimental studies of beauty hadron decays face significant challenges due
to a wide range of backgrounds arising from the numerous possible decay
channels with similar final states. For a particular signal decay, the process
for ascertaining the most relevant background processes necessitates a detailed
analysis of final state particles, potential misidentifications, and kinematic
overlaps, which, due to computational limitations, is restricted to the
simulation of only the most relevant backgrounds. Moreover, this process
typically relies on the physicist's intuition and expertise, as no systematic
method exists.
  This paper has two primary goals. First, from a particle physics perspective,
we present a novel approach that utilises Reinforcement Learning (RL) to
overcome the aforementioned challenges by systematically determining the
critical backgrounds affecting beauty hadron decay measurements. While beauty
hadron physics serves as the case study in this work, the proposed strategy is
broadly adaptable to other types of particle physics measurements. Second, from
a Machine Learning perspective, we introduce a novel algorithm which exploits
the synergy between RL and Genetic Algorithms (GAs) for environments with
highly sparse rewards and a large trajectory space. This strategy leverages GAs
to efficiently explore the trajectory space and identify successful
trajectories, which are used to guide the RL agent's training. Our method also
incorporates a transformer architecture for the RL agent to handle token
sequences representing decays.

</details>


### [63] [Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering](https://arxiv.org/abs/2509.15024)
*Xuanting Xie,Bingheng Li,Erlin Pan,Rui Hou,Wenyu Chen,Zhao Kang*

Main category: cs.LG

TL;DR: AGCN is a novel graph clustering network that integrates attention mechanisms directly into graph structure to overcome limitations of both GNNs (over-localization) and Transformers (over-globalization) in unsupervised graph learning.


<details>
  <summary>Details</summary>
Motivation: Current attention mechanisms underperform on graph data compared to GNNs. GNNs overemphasize neighborhood aggregation causing homogenization, while Transformers over-globalize and miss local patterns. The paper investigates whether attention is inherently redundant for graph clustering.

Method: Proposes Attentive Graph Clustering Network (AGCN) that embeds attention directly into graph structure. Includes KV cache for computational efficiency and pairwise margin contrastive loss to enhance attention space discriminability. Theoretical analysis compares AGCN with GNN and Transformer behaviors.

Result: AGCN outperforms state-of-the-art methods in extensive experiments, demonstrating effective global information extraction while maintaining sensitivity to local topological patterns.

Conclusion: Attention is not redundant for unsupervised graph learning when properly integrated with graph structure. AGCN successfully bridges the gap between GNN's local focus and Transformer's global perspective, achieving superior graph clustering performance.

Abstract: Attention mechanisms have become a cornerstone in modern neural networks,
driving breakthroughs across diverse domains. However, their application to
graph structured data, where capturing topological connections is essential,
remains underexplored and underperforming compared to Graph Neural Networks
(GNNs), particularly in the graph clustering task. GNN tends to overemphasize
neighborhood aggregation, leading to a homogenization of node representations.
Conversely, Transformer tends to over globalize, highlighting distant nodes at
the expense of meaningful local patterns. This dichotomy raises a key question:
Is attention inherently redundant for unsupervised graph learning? To address
this, we conduct a comprehensive empirical analysis, uncovering the
complementary weaknesses of GNN and Transformer in graph clustering. Motivated
by these insights, we propose the Attentive Graph Clustering Network (AGCN) a
novel architecture that reinterprets the notion that graph is attention. AGCN
directly embeds the attention mechanism into the graph structure, enabling
effective global information extraction while maintaining sensitivity to local
topological cues. Our framework incorporates theoretical analysis to contrast
AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV
cache mechanism to improve computational efficiency, and (2) a pairwise margin
contrastive loss to boost the discriminative capacity of the attention space.
Extensive experimental results demonstrate that AGCN outperforms
state-of-the-art methods.

</details>


### [64] [Robust Barycenters of Persistence Diagrams](https://arxiv.org/abs/2509.14904)
*Keanu Sisouk,Eloi Tanguy,Julie Delon,Julien Tierny*

Main category: cs.LG

TL;DR: General approach for computing robust Wasserstein barycenters of persistence diagrams using fixed-point method for q>1 transportation costs, particularly robust qâ(1,2) distances.


<details>
  <summary>Details</summary>
Motivation: Classical methods only work for q=2 Wasserstein distance, limiting robustness to outliers. Need for more general approach that handles q>1 transportation costs.

Method: Adapted fixed-point method to compute barycenter diagrams for generic transportation costs (q>1), particularly robust qâ(1,2) distances that are outlier-resistant.

Result: Successfully implemented robust barycenter computation for persistence diagrams. Demonstrated utility in clustering and dictionary encoding applications with improved outlier robustness.

Conclusion: The generalized framework provides added robustness to outliers in persistence diagram analysis, enabling more reliable applications in clustering and dictionary encoding tasks.

Abstract: This short paper presents a general approach for computing robust Wasserstein
barycenters of persistence diagrams. The classical method consists in computing
assignment arithmetic means after finding the optimal transport plans between
the barycenter and the persistence diagrams. However, this procedure only works
for the transportation cost related to the $q$-Wasserstein distance $W_q$ when
$q=2$. We adapt an alternative fixed-point method to compute a barycenter
diagram for generic transportation costs ($q > 1$), in particular those robust
to outliers, $q \in (1,2)$. We show the utility of our work in two
applications: \emph{(i)} the clustering of persistence diagrams on their metric
space and \emph{(ii)} the dictionary encoding of persistence diagrams. In both
scenarios, we demonstrate the added robustness to outliers provided by our
generalized framework. Our Python implementation is available at this address:
https://github.com/Keanu-Sisouk/RobustBarycenter .

</details>


### [65] [Sample Efficient Experience Replay in Non-stationary Environments](https://arxiv.org/abs/2509.15032)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Yuanye Zhao,Zheng Lin,Zihan Fang,Yi Liu,Dianxin Luan,Dong Huang,Heming Cui,Yong Cui*

Main category: cs.LG

TL;DR: DEER is a new experience replay method that uses environment change detection and adaptive prioritization to improve reinforcement learning in non-stationary environments, outperforming state-of-the-art methods by 11.54%.


<details>
  <summary>Details</summary>
Motivation: Traditional experience replay methods struggle in non-stationary environments because they can't distinguish between policy-induced changes and actual environmental shifts, leading to inefficient learning.

Method: Proposes Discrepancy of Environment Dynamics (DoE) metric to isolate environment shift effects, and DEER framework that uses a binary classifier to detect changes and applies different prioritization strategies before/after shifts.

Result: Experiments on four non-stationary benchmarks show DEER improves performance of off-policy algorithms by 11.54% compared to best state-of-the-art ER methods.

Conclusion: DEER enables more sample-efficient learning in dynamic environments by effectively distinguishing and adapting to environmental changes through intelligent experience prioritization.

Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as
changing dynamics and rewards quickly make past experiences outdated.
Traditional experience replay (ER) methods, especially those using TD-error
prioritization, struggle to distinguish between changes caused by the agent's
policy and those from the environment, resulting in inefficient learning under
dynamic conditions. To address this challenge, we propose the Discrepancy of
Environment Dynamics (DoE), a metric that isolates the effects of environment
shifts on value functions. Building on this, we introduce Discrepancy of
Environment Prioritized Experience Replay (DEER), an adaptive ER framework that
prioritizes transitions based on both policy updates and environmental changes.
DEER uses a binary classifier to detect environment changes and applies
distinct prioritization strategies before and after each shift, enabling more
sample-efficient learning. Experiments on four non-stationary benchmarks
demonstrate that DEER further improves the performance of off-policy algorithms
by 11.54 percent compared to the best-performing state-of-the-art ER methods.

</details>


### [66] [Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation](https://arxiv.org/abs/2509.14925)
*Konrad Nowosadko,Franco Ruggeri,Ahmad Terra*

Main category: cs.LG

TL;DR: Proposes using Self-Explaining Neural Networks (SENNs) to make deep reinforcement learning more interpretable while maintaining performance, particularly for low-dimensional tasks like mobile network resource allocation.


<details>
  <summary>Details</summary>
Motivation: Deep reinforcement learning methods lack transparency and interpretability, which reduces trustworthiness in critical domains. The black-box nature of DNNs hinders understanding of model behavior.

Method: Uses Self-Explaining Neural Networks (SENNs) with explanation extraction methods to enhance interpretability while preserving predictive accuracy. Focuses on low-dimensionality problems to generate both local and global explanations.

Result: Demonstrated competitive performance on mobile network resource allocation problems. SENNs provided robust explanations while maintaining performance comparable to state-of-the-art methods.

Conclusion: SENNs show potential to improve transparency and trust in AI-driven decision-making for low-dimensional tasks, offering interpretable solutions without sacrificing performance.

Abstract: Reinforcement Learning (RL) methods that incorporate deep neural networks
(DNN), though powerful, often lack transparency. Their black-box characteristic
hinders interpretability and reduces trustworthiness, particularly in critical
domains. To address this challenge in RL tasks, we propose a solution based on
Self-Explaining Neural Networks (SENNs) along with explanation extraction
methods to enhance interpretability while maintaining predictive accuracy. Our
approach targets low-dimensionality problems to generate robust local and
global explanations of the model's behaviour. We evaluate the proposed method
on the resource allocation problem in mobile networks, demonstrating that SENNs
can constitute interpretable solutions with competitive performance. This work
highlights the potential of SENNs to improve transparency and trust in
AI-driven decision-making for low-dimensional tasks. Our approach strong
performance on par with the existing state-of-the-art methods, while providing
robust explanations.

</details>


### [67] [From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets](https://arxiv.org/abs/2509.15040)
*Juwon Kim,Hyunwook Lee,Hyotaek Jeon,Seungmin Jin,Sungahn Ko*

Main category: cs.LG

TL;DR: A two-stage framework combining unsupervised pattern extraction with interpretable forecasting for financial directional prediction, achieving top performance while maintaining transparency.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between interpretable but limited traditional pattern-based methods and powerful but opaque deep learning models for financial forecasting.

Method: Two-stage approach: (1) SIMPC for segmenting/clustering multivariate time series to extract amplitude/temporal-invariant patterns, (2) JISC-Net shapelet-based classifier using initial patterns to forecast short-term directional movements.

Result: Ranked first or second in 11 out of 12 metric-dataset combinations on Bitcoin and S&P 500 equities, consistently outperforming baselines.

Conclusion: The framework enables transparent decision-making by revealing underlying pattern structures while maintaining high predictive performance, addressing both accuracy and interpretability needs in financial forecasting.

Abstract: Directional forecasting in financial markets requires both accuracy and
interpretability. Before the advent of deep learning, interpretable approaches
based on human-defined patterns were prevalent, but their structural vagueness
and scale ambiguity hindered generalization. In contrast, deep learning models
can effectively capture complex dynamics, yet often offer limited transparency.
To bridge this gap, we propose a two-stage framework that integrates
unsupervised pattern extracion with interpretable forecasting. (i) SIMPC
segments and clusters multivariate time series, extracting recurrent patterns
that are invariant to amplitude scaling and temporal distortion, even under
varying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses
the initial part of extracted patterns as input and forecasts subsequent
partial sequences for short-term directional movement. Experiments on Bitcoin
and three S&P 500 equities demonstrate that our method ranks first or second in
11 out of 12 metric--dataset combinations, consistently outperforming
baselines. Unlike conventional deep learning models that output buy-or-sell
signals without interpretable justification, our approach enables transparent
decision-making by revealing the underlying pattern structures that drive
predictive outcomes.

</details>


### [68] [DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2509.14933)
*Xiangfei Qiu,Yuhan Zhu,Zhengyu Li,Hanyin Cheng,Xingjian Wu,Chenjuan Guo,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: Proposes DAG framework with dual causal networks for time series forecasting with exogenous variables, addressing limitations of existing methods by leveraging future exogenous variables and modeling causal relationships.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting methods with exogenous variables fail to utilize future exogenous variables and ignore causal relationships between endogenous and exogenous variables, leading to suboptimal performance.

Method: Dual causal network framework with Temporal Causal Module (causal discovery for exogenous variables over time + causal injection for forecasting) and Channel Causal Module (causal discovery between variable types + causal injection for enhanced predictions).

Result: Not explicitly stated in abstract, but framework is designed to improve forecasting accuracy by better leveraging exogenous variables through causal modeling.

Conclusion: The proposed DAG framework provides a comprehensive approach to incorporate causal relationships between endogenous and exogenous variables, enabling more accurate time series forecasting by effectively utilizing both historical and future exogenous information.

Abstract: Time series forecasting is crucial in various fields such as economics,
traffic, and AIOps. However, in real-world applications, focusing solely on the
endogenous variables (i.e., target variables), is often insufficient to ensure
accurate predictions. Considering exogenous variables (i.e., covariates)
provides additional predictive information, thereby improving forecasting
accuracy. However, existing methods for time series forecasting with exogenous
variables (TSF-X) have the following shortcomings: 1) they do not leverage
future exogenous variables, 2) they fail to account for the causal
relationships between endogenous and exogenous variables. As a result, their
performance is suboptimal. In this study, to better leverage exogenous
variables, especially future exogenous variable, we propose a general framework
DAG, which utilizes dual causal network along both the temporal and channel
dimensions for time series forecasting with exogenous variables. Specifically,
we first introduce the Temporal Causal Module, which includes a causal
discovery module to capture how historical exogenous variables affect future
exogenous variables. Following this, we construct a causal injection module
that incorporates the discovered causal relationships into the process of
forecasting future endogenous variables based on historical endogenous
variables. Next, we propose the Channel Causal Module, which follows a similar
design principle. It features a causal discovery module models how historical
exogenous variables influence historical endogenous variables, and a causal
injection module incorporates the discovered relationships to enhance the
prediction of future endogenous variables based on future exogenous variables.

</details>


### [69] [Reinforcement Learning Agent for a 2D Shooter Game](https://arxiv.org/abs/2509.15042)
*Thomas Ackermann,Moritz Spang,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: Hybrid training combining offline imitation learning with online RL for 2D shooter game agents, using multi-head neural network with attention mechanisms to overcome sparse rewards and training instability.


<details>
  <summary>Details</summary>
Motivation: Address sparse rewards, training instability, and poor sample efficiency in complex game environments where pure reinforcement learning methods show high variance and frequent performance degradation.

Method: Multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Hybrid methodology starts with behavioral cloning on demonstration data from rule-based agents, then transitions to reinforcement learning.

Result: Achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure reinforcement learning methods which showed high variance and frequent performance degradation.

Conclusion: Combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments where pure exploration proves insufficient.

Abstract: Reinforcement learning agents in complex game environments often suffer from
sparse rewards, training instability, and poor sample efficiency. This paper
presents a hybrid training approach that combines offline imitation learning
with online reinforcement learning for a 2D shooter game agent. We implement a
multi-head neural network with separate outputs for behavioral cloning and
Q-learning, unified by shared feature extraction layers with attention
mechanisms. Initial experiments using pure deep Q-Networks exhibited
significant instability, with agents frequently reverting to poor policies
despite occasional good performance. To address this, we developed a hybrid
methodology that begins with behavioral cloning on demonstration data from
rule-based agents, then transitions to reinforcement learning. Our hybrid
approach achieves consistently above 70% win rate against rule-based opponents,
substantially outperforming pure reinforcement learning methods which showed
high variance and frequent performance degradation. The multi-head architecture
enables effective knowledge transfer between learning modes while maintaining
training stability. Results demonstrate that combining demonstration-based
initialization with reinforcement learning optimization provides a robust
solution for developing game AI agents in complex multi-agent environments
where pure exploration proves insufficient.

</details>


### [70] [A Comparative Analysis of Transformer Models in Social Bot Detection](https://arxiv.org/abs/2509.14936)
*Rohan Veit,Michael Lones*

Main category: cs.LG

TL;DR: Comparison of encoder vs decoder transformer models for bot detection, showing encoders are more accurate but decoders have better generalization potential.


<details>
  <summary>Details</summary>
Motivation: Social media manipulation through AI-generated bots using advanced text generation tools requires effective detection methods to maintain online discussion integrity.

Method: Developed evaluation pipelines to test performance of encoder-based and decoder-based transformer classifiers for bot detection.

Result: Encoder-based classifiers demonstrated greater accuracy and robustness, while decoder-based models showed greater adaptability through task-specific alignment.

Conclusion: Both encoder and decoder approaches have strengths, contributing to preventing digital manipulation while protecting online discussion integrity.

Abstract: Social media has become a key medium of communication in today's society.
This realisation has led to many parties employing artificial users (or bots)
to mislead others into believing untruths or acting in a beneficial manner to
such parties. Sophisticated text generation tools, such as large language
models, have further exacerbated this issue. This paper aims to compare the
effectiveness of bot detection models based on encoder and decoder
transformers. Pipelines are developed to evaluate the performance of these
classifiers, revealing that encoder-based classifiers demonstrate greater
accuracy and robustness. However, decoder-based models showed greater
adaptability through task-specific alignment, suggesting more potential for
generalisation across different use cases in addition to superior observa.
These findings contribute to the ongoing effort to prevent digital environments
being manipulated while protecting the integrity of online discussion.

</details>


### [71] [Credit Card Fraud Detection](https://arxiv.org/abs/2509.15044)
*Iva Popova,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: Evaluation of 5 ML models for credit card fraud detection using undersampling, SMOTE, and hybrid approaches on imbalanced data, with hybrid method achieving best recall-precision balance.


<details>
  <summary>Details</summary>
Motivation: Credit card fraud detection faces challenges from class imbalance and fraudsters mimicking legitimate behavior, requiring effective ML approaches that work in real-world imbalanced scenarios.

Method: Tested Logistic Regression, Random Forest, XGBoost, KNN, and MLP on real-world dataset using undersampling, SMOTE, and hybrid approach, evaluated on original imbalanced test set.

Result: Hybrid method achieved the best balance between recall and precision, particularly improving performance of MLP and KNN models.

Conclusion: The hybrid sampling approach shows promise for credit card fraud detection by effectively addressing class imbalance while maintaining performance on real-world imbalanced data.

Abstract: Credit card fraud remains a significant challenge due to class imbalance and
fraudsters mimicking legitimate behavior. This study evaluates five machine
learning models - Logistic Regression, Random Forest, XGBoost, K-Nearest
Neighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using
undersampling, SMOTE, and a hybrid approach. Our models are evaluated on the
original imbalanced test set to better reflect real-world performance. Results
show that the hybrid method achieves the best balance between recall and
precision, especially improving MLP and KNN performance.

</details>


### [72] [Hierarchical Federated Learning for Social Network with Mobility](https://arxiv.org/abs/2509.14938)
*Zeyu Chen,Wen Chen,Jun Li,Qingqing Wu,Ming Ding,Xuefeng Han,Xiumei Deng,Liwei Wang*

Main category: cs.LG

TL;DR: Proposes HFL-SNM, a hierarchical federated learning framework that incorporates social networks and client mobility to optimize resource allocation and energy consumption while maintaining data privacy.


<details>
  <summary>Details</summary>
Motivation: Traditional FL frameworks assume static clients and absolute data privacy, but neglect client mobility patterns and data sharing opportunities in social networks, leading to inefficient resource usage.

Method: Introduces Effective Data Coverage Rate and Redundant Data Coverage Rate concepts, formulates joint optimization problem for resource allocation and client scheduling, and develops DO-SNM algorithm to minimize energy consumption during FL process.

Result: Experimental results show the proposed algorithm achieves better model performance while significantly reducing energy consumption compared to traditional baseline algorithms.

Conclusion: The HFL-SNM framework successfully addresses both data privacy and mobility challenges in FL, demonstrating that considering social network relationships and mobility patterns can optimize resource usage and improve overall system efficiency.

Abstract: Federated Learning (FL) offers a decentralized solution that allows
collaborative local model training and global aggregation, thereby protecting
data privacy. In conventional FL frameworks, data privacy is typically
preserved under the assumption that local data remains absolutely private,
whereas the mobility of clients is frequently neglected in explicit modeling.
In this paper, we propose a hierarchical federated learning framework based on
the social network with mobility namely HFL-SNM that considers both data
sharing among clients and their mobility patterns. Under the constraints of
limited resources, we formulate a joint optimization problem of resource
allocation and client scheduling, which objective is to minimize the energy
consumption of clients during the FL process. In social network, we introduce
the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate.
We analyze the impact of effective data and redundant data on the model
performance through preliminary experiments. We decouple the optimization
problem into multiple sub-problems, analyze them based on preliminary
experimental results, and propose Dynamic Optimization in Social Network with
Mobility (DO-SNM) algorithm. Experimental results demonstrate that our
algorithm achieves superior model performance while significantly reducing
energy consumption, compared to traditional baseline algorithms.

</details>


### [73] [Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning](https://arxiv.org/abs/2509.15057)
*Quincy Hershey,Randy Paffenroth*

Main category: cs.LG

TL;DR: Novel hyperparameters for sparse RNNs enable varied sparsity in weight matrices, combined with a new "hidden proportion" metric that balances unknown distribution and predicts performance, leading to significant gains and enabling meta-learning applications.


<details>
  <summary>Details</summary>
Motivation: To develop improved sparse RNN architectures that allow varying sparsity levels while enhancing performance, and to create metrics that can predict model performance based on intrinsic dataset characteristics.

Method: Developed alternative hyperparameters for specifying sparse RNNs that enable varying sparsity within weight matrices, and introduced a novel "hidden proportion" metric to balance unknown distribution within the model.

Result: The combined approach of varied sparsity RNN architecture with the hidden proportion metric generates significant performance gains and improves performance expectations on an a priori basis.

Conclusion: This approach provides a path forward for generalized meta-learning applications and model optimization based on intrinsic dataset characteristics like input and output dimensions.

Abstract: This paper develops alternative hyperparameters for specifying sparse
Recurrent Neural Networks (RNNs). These hyperparameters allow for varying
sparsity within the trainable weight matrices of the model while improving
overall performance. This architecture enables the definition of a novel
metric, hidden proportion, which seeks to balance the distribution of unknowns
within the model and provides significant explanatory power of model
performance. Together, the use of the varied sparsity RNN architecture combined
with the hidden proportion metric generates significant performance gains while
improving performance expectations on an a priori basis. This combined approach
provides a path forward towards generalized meta-learning applications and
model optimization based on intrinsic characteristics of the data set,
including input and output dimensions.

</details>


### [74] [Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models](https://arxiv.org/abs/2509.14945)
*Amsalu Tessema,Tizazu Bayih,Kassahun Azezew,Ayenew Kassie*

Main category: cs.LG

TL;DR: Ensemble machine learning models effectively predict malnutrition among pregnant Ethiopian women with 97.87% accuracy using demographic and health survey data.


<details>
  <summary>Details</summary>
Motivation: Malnutrition among pregnant women in Ethiopia is a major public health challenge with adverse outcomes, and traditional statistical methods fail to capture complex multidimensional determinants.

Method: Used ensemble ML techniques (XGBoost, Random Forest, CatBoost, AdaBoost) on Ethiopian Demographic and Health Survey data (2005-2020, 18,108 records, 30 attributes) with preprocessing including SMOTE balancing and feature selection.

Result: Random Forest achieved best performance: 97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86% ROC AUC for classifying four nutritional categories.

Conclusion: Ensemble learning effectively captures hidden patterns from complex datasets, providing timely insights for early nutritional risk detection and supporting data-driven strategies to improve maternal health outcomes.

Abstract: Malnutrition among pregnant women is a major public health challenge in
Ethiopia, increasing the risk of adverse maternal and neonatal outcomes.
Traditional statistical approaches often fail to capture the complex and
multidimensional determinants of nutritional status. This study develops a
predictive model using ensemble machine learning techniques, leveraging data
from the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108
records with 30 socio-demographic and health attributes. Data preprocessing
included handling missing values, normalization, and balancing with SMOTE,
followed by feature selection to identify key predictors. Several supervised
ensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost
were applied to classify nutritional status. Among them, the Random Forest
model achieved the best performance, classifying women into four categories
(normal, moderate malnutrition, severe malnutrition, and overnutrition) with
97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86%
ROC AUC. These findings demonstrate the effectiveness of ensemble learning in
capturing hidden patterns from complex datasets and provide timely insights for
early detection of nutritional risks. The results offer practical implications
for healthcare providers, policymakers, and researchers, supporting data-driven
strategies to improve maternal nutrition and health outcomes in Ethiopia.

</details>


### [75] [Communication Efficient Split Learning of ViTs with Attention-based Double Compression](https://arxiv.org/abs/2509.15058)
*Federico Alvetreti,Jary Pomponi,Paolo Di Lorenzo,Simone Scardapane*

Main category: cs.LG

TL;DR: ADC framework reduces communication overhead in Split Learning for Vision Transformers by using two parallel compression strategies: merging similar activations and discarding least meaningful tokens.


<details>
  <summary>Details</summary>
Motivation: To address the high communication overhead required for transmitting intermediate Vision Transformers activations during Split Learning training process.

Method: Two parallel compression strategies: 1) class-agnostic merging of similar activations based on average attention scores, 2) discarding least meaningful tokens to further reduce communication cost.

Result: ADC outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.

Conclusion: The proposed Attention-based Double Compression framework enables efficient Split Learning training without losing generalization ability or requiring additional gradient tuning.

Abstract: This paper proposes a novel communication-efficient Split Learning (SL)
framework, named Attention-based Double Compression (ADC), which reduces the
communication overhead required for transmitting intermediate Vision
Transformers activations during the SL training process. ADC incorporates two
parallel compression strategies. The first one merges samples' activations that
are similar, based on the average attention score calculated in the last client
layer; this strategy is class-agnostic, meaning that it can also merge samples
having different classes, without losing generalization ability nor decreasing
final results. The second strategy follows the first and discards the least
meaningful tokens, further reducing the communication cost. Combining these
strategies not only allows for sending less during the forward pass, but also
the gradients are naturally compressed, allowing the whole model to be trained
without additional tuning or approximations of the gradients. Simulation
results demonstrate that Attention-based Double Compression outperforms
state-of-the-art SL frameworks by significantly reducing communication
overheads while maintaining high accuracy.

</details>


### [76] [Stochastic Bilevel Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2509.14952)
*Zhuanghua Liu,Luo Luo*

Main category: cs.LG

TL;DR: Proposes NÂ²SBA algorithm for stochastic bilevel optimization with heavy-tailed noise, achieving improved complexity bounds for finding Ïµ-stationary points.


<details>
  <summary>Details</summary>
Motivation: Address stochastic bilevel optimization where lower-level is strongly convex and upper-level is nonconvex, with heavy-tailed noise common in ML applications like LLM training and RL.

Method: Nested-loop normalized stochastic bilevel approximation (NÂ²SBA) algorithm that uses stochastic first-order oracles to handle heavy-tailed noise with p-th order central moments.

Result: Achieves SFO complexity of Î(Îº^â·áµâ»Â³/áµâ»Â¹ Ï^áµ/áµâ»Â¹ Ïµ^â»â´áµâ»Â²/áµâ»Â¹) for bilevel optimization and Î(Îº^Â²áµâ»Â¹/áµâ»Â¹ Ï^áµ/áµâ»Â¹ Ïµ^â»Â³áµâ»Â²/áµâ»Â¹) for minimax problems, matching best-known results for bounded variance case (p=2).

Conclusion: The proposed NÂ²SBA algorithm effectively handles heavy-tailed noise in stochastic bilevel optimization and provides optimal complexity bounds that generalize existing results beyond the bounded variance assumption.

Abstract: This paper considers the smooth bilevel optimization in which the lower-level
problem is strongly convex and the upper-level problem is possibly nonconvex.
We focus on the stochastic setting that the algorithm can access the unbiased
stochastic gradient evaluation with heavy-tailed noise, which is prevalent in
many machine learning applications such as training large language models and
reinforcement learning. We propose a nested-loop normalized stochastic bilevel
approximation (N$^2$SBA) for finding an $\epsilon$-stationary point with the
stochastic first-order oracle (SFO) complexity of
$\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{4 p - 2}{p-1}}\big)$, where $\kappa$ is the condition number,
$p\in(1,2]$ is the order of central moment for the noise, and $\sigma$ is the
noise level. Furthermore, we specialize our idea to solve the
nonconvex-strongly-concave minimax optimization problem, achieving an
$\epsilon$-stationary point with the SFO complexity of $\tilde{\mathcal
O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{3p-2}{p-1}}\big)$. All above upper bounds match the best-known
results under the special case of the bounded variance setting, i.e., $p=2$.

</details>


### [77] [FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference](https://arxiv.org/abs/2509.14968)
*Carlos Barroso-FernÃ¡ndez,Alejandro Calvillo-Fernandez,Antonio de la Oliva,Carlos J. Bernardos*

Main category: cs.LG

TL;DR: FAWN is a MultiEncoder Fusion-Attention Wave Network that fuses Wi-Fi and 5G signals for indoor scene inference using passive sensing, achieving 84% accuracy with errors below 0.6m.


<details>
  <summary>Details</summary>
Motivation: Current ISAC passive sensing solutions are limited to single technologies (Wi-Fi or 5G), constraining accuracy. Integrating multiple technologies can augment coverage area and improve environmental understanding without dedicated hardware.

Method: Based on transformers architecture, FAWN uses a multi-encoder fusion-attention network to combine information from both Wi-Fi and 5G signals for passive sensing without interfering with communications.

Result: The prototype achieved errors below 0.6 meters around 84% of the time in real-world testing scenarios.

Conclusion: FAWN demonstrates that fusing multiple wireless technologies through transformer-based architecture significantly improves indoor scene inference accuracy in passive ISAC systems.

Abstract: The upcoming generations of wireless technologies promise an era where
everything is interconnected and intelligent. As the need for intelligence
grows, networks must learn to better understand the physical world. However,
deploying dedicated hardware to perceive the environment is not always
feasible, mainly due to costs and/or complexity. Integrated Sensing and
Communication (ISAC) has made a step forward in addressing this challenge.
Within ISAC, passive sensing emerges as a cost-effective solution that reuses
wireless communications to sense the environment, without interfering with
existing communications. Nevertheless, the majority of current solutions are
limited to one technology (mostly Wi-Fi or 5G), constraining the maximum
accuracy reachable. As different technologies work with different spectrums, we
see a necessity in integrating more than one technology to augment the coverage
area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a
MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.
FAWN is based on the original transformers architecture, to fuse information
from Wi-Fi and 5G, making the network capable of understanding the physical
world without interfering with the current communication. To test our solution,
we have built a prototype and integrated it in a real scenario. Results show
errors below 0.6 m around 84% of times.

</details>


### [78] [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
*Xuekai Zhu,Daixuan Cheng,Dinghuai Zhang,Hengli Li,Kaiyan Zhang,Che Jiang,Youbang Sun,Ermo Hua,Yuxin Zuo,Xingtai Lv,Qizheng Zhang,Lin Chen,Fanghao Shao,Bo Xue,Yunchong Song,Zhenjie Yang,Ganqu Cui,Ning Ding,Jianfeng Gao,Xiaodong Liu,Bowen Zhou,Hongyuan Mei,Zhouhan Lin*

Main category: cs.LG

TL;DR: FlowRL is a new RL method for LLMs that matches full reward distributions instead of maximizing rewards, achieving better diversity and performance on math and code reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional reward-maximizing methods like PPO and GRPO tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, reducing diversity in LLM reasoning.

Method: Transforms scalar rewards into a normalized target distribution using a learnable partition function, then minimizes reverse KL divergence between policy and target distribution through flow-balanced optimization.

Result: Achieves 10.0% average improvement over GRPO and 5.1% over PPO on math benchmarks, with consistent better performance on code reasoning tasks.

Conclusion: Reward distribution-matching is a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.

Abstract: We propose FlowRL: matching the full reward distribution via flow balancing
instead of maximizing rewards in large language model (LLM) reinforcement
learning (RL). Recent advanced reasoning models adopt reward-maximizing methods
(\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while
neglecting less frequent but valid reasoning paths, thus reducing diversity. In
contrast, we transform scalar rewards into a normalized target distribution
using a learnable partition function, and then minimize the reverse KL
divergence between the policy and the target distribution. We implement this
idea as a flow-balanced optimization method that promotes diverse exploration
and generalizable reasoning trajectories. We conduct experiments on math and
code reasoning tasks: FlowRL achieves a significant average improvement of
$10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs
consistently better on code reasoning tasks. These results highlight reward
distribution-matching as a key step toward efficient exploration and diverse
reasoning in LLM reinforcement learning.

</details>


### [79] [Stochastic Adaptive Gradient Descent Without Descent](https://arxiv.org/abs/2509.14969)
*Jean-FranÃ§ois Aujol,JÃ©rÃ©mie Bigot,Camille Castera*

Main category: cs.LG

TL;DR: New adaptive step-size strategy for stochastic convex optimization that requires no hyperparameter tuning and uses only first-order stochastic oracle information.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretically-grounded adaptive step-size method for stochastic gradient descent that automatically adapts to local geometry without requiring manual hyperparameter tuning.

Method: Adaptation of the Adaptive Gradient Descent Without Descent method to stochastic setting, using first-order stochastic oracle to exploit local geometry without hyperparameter tuning.

Result: Proven convergence under various assumptions and empirical demonstration that competes with tuned baseline methods.

Conclusion: The proposed adaptive step-size strategy provides an effective, theoretically-sound approach for stochastic convex optimization that eliminates the need for hyperparameter tuning while maintaining competitive performance.

Abstract: We introduce a new adaptive step-size strategy for convex optimization with
stochastic gradient that exploits the local geometry of the objective function
only by means of a first-order stochastic oracle and without any
hyper-parameter tuning. The method comes from a theoretically-grounded
adaptation of the Adaptive Gradient Descent Without Descent method to the
stochastic setting. We prove the convergence of stochastic gradient descent
with our step-size under various assumptions, and we show that it empirically
competes against tuned baselines.

</details>


### [80] [Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection](https://arxiv.org/abs/2509.15033)
*Padmaksha Roy,Almuatazbellah Boker,Lamine Mili*

Main category: cs.LG

TL;DR: Proposes a novel multivariate anomaly detection method that models time-varying non-linear spatio-temporal correlations using transformer encoders, multivariate likelihood, and copula models with contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing approaches oversimplify real-world interactions by assuming time series variables are independent, missing the collective behavior patterns that indicate anomalies when individual series appear normal.

Method: Uses transformer encoder for temporal patterns, multivariate likelihood and copula for spatial dependencies, trained jointly with self-supervised contrastive learning in latent space to separate normal and anomaly samples.

Result: The approach effectively captures joint dependencies and decouples modeling of marginal distributions, temporal dynamics, and inter-variable dependencies for improved anomaly detection.

Conclusion: Modeling spatio-temporal correlations through this integrated approach provides superior anomaly detection performance compared to methods that treat variables independently.

Abstract: In this paper, we aim to improve multivariate anomaly detection (AD) by
modeling the \textit{time-varying non-linear spatio-temporal correlations}
found in multivariate time series data . In multivariate time series data, an
anomaly may be indicated by the simultaneous deviation of interrelated time
series from their expected collective behavior, even when no individual time
series exhibits a clearly abnormal pattern on its own. In many existing
approaches, time series variables are assumed to be (conditionally)
independent, which oversimplifies real-world interactions. Our approach
addresses this by modeling joint dependencies in the latent space and
decoupling the modeling of \textit{marginal distributions, temporal dynamics,
and inter-variable dependencies}. We use a transformer encoder to capture
temporal patterns, and to model spatial (inter-variable) dependencies, we fit a
multi-variate likelihood and a copula. The temporal and the spatial components
are trained jointly in a latent space using a self-supervised contrastive
learning objective to learn meaningful feature representations to separate
normal and anomaly samples.

</details>


### [81] [Probabilistic and nonlinear compressive sensing](https://arxiv.org/abs/2509.15060)
*Lukas Silvester Barth,Paulo von Petersenn*

Main category: cs.LG

TL;DR: A smooth probabilistic reformulation of L0 regularized regression that enables exact gradient computation without Monte Carlo sampling, significantly improving convergence speed and outperforming compressive sensing algorithms like IHT and Lasso across various settings.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of L0 regularized regression and best subset selection problems by developing a method that avoids Monte Carlo sampling while enabling exact gradient computation for faster convergence.

Method: Developed a smooth probabilistic reformulation of L0 regularization that computes exact gradients without Monte Carlo sampling. Also investigated nonlinear compressive sensing by studying parameter recovery of nonlinear teacher networks through student network compression, implementing a normal-form algorithm for symmetry class selection.

Result: The method drastically improves convergence speed compared to Monte Carlo approaches and outperforms compressive sensing algorithms (IHT, Relaxed-Lasso) across various settings and signal-to-noise ratios. For nonlinear compressive sensing, while compression can improve test loss, exact parameter recovery is not possible even up to symmetries, with observed rebound effects where configurations initially converge then diverge despite decreasing test loss.

Conclusion: The proposed probabilistic reformulation provides significant computational advantages for L0 regularized regression. Nonlinear compressive sensing shows fundamental differences from linear compressive sensing, with exact parameter recovery being impossible even with symmetry considerations, indicating inherent limitations in nonlinear network compression.

Abstract: We present a smooth probabilistic reformulation of $\ell_0$ regularized
regression that does not require Monte Carlo sampling and allows for the
computation of exact gradients, facilitating rapid convergence to local optima
of the best subset selection problem. The method drastically improves
convergence speed compared to similar Monte Carlo based approaches.
Furthermore, we empirically demonstrate that it outperforms compressive sensing
algorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and
signal-to-noise ratios. The implementation runs efficiently on both CPUs and
GPUs and is freely available at
https://github.com/L0-and-behold/probabilistic-nonlinear-cs.
  We also contribute to research on nonlinear generalizations of compressive
sensing by investigating when parameter recovery of a nonlinear teacher network
is possible through compression of a student network. Building upon theorems of
Fefferman and Markel, we show theoretically that the global optimum in the
infinite-data limit enforces recovery up to certain symmetries. For empirical
validation, we implement a normal-form algorithm that selects a canonical
representative within each symmetry class. However, while compression can help
to improve test loss, we find that exact parameter recovery is not even
possible up to symmetries. In particular, we observe a surprising rebound
effect where teacher and student configurations initially converge but
subsequently diverge despite continuous decrease in test loss. These findings
indicate fundamental differences between linear and nonlinear compressive
sensing.

</details>


### [82] [Improving Internet Traffic Matrix Prediction via Time Series Clustering](https://arxiv.org/abs/2509.15072)
*Martha Cash,Alexander Wyglinski*

Main category: cs.LG

TL;DR: Novel framework using time series clustering to improve internet traffic matrix prediction by grouping flows with similar temporal patterns before training deep learning models, achieving significant RMSE reduction and better network optimization.


<details>
  <summary>Details</summary>
Motivation: Traffic flows in internet traffic matrices exhibit diverse temporal behaviors that hinder prediction accuracy when using a single model for all flows, necessitating a clustering approach to create homogeneous data subsets.

Method: Proposes two clustering strategies (source clustering and histogram clustering) to group flows with similar temporal patterns before training deep learning models, enabling better pattern capture and generalization.

Result: Reduces RMSE by up to 92% for Abilene and 75% for GÃANT networks compared to existing methods, and reduces maximum link utilization bias by 18% and 21% respectively in routing scenarios.

Conclusion: Clustering-based approach significantly improves traffic matrix prediction accuracy and demonstrates practical benefits for network optimization applications.

Abstract: We present a novel framework that leverages time series clustering to improve
internet traffic matrix (TM) prediction using deep learning (DL) models.
Traffic flows within a TM often exhibit diverse temporal behaviors, which can
hinder prediction accuracy when training a single model across all flows. To
address this, we propose two clustering strategies, source clustering and
histogram clustering, that group flows with similar temporal patterns prior to
model training. Clustering creates more homogeneous data subsets, enabling
models to capture underlying patterns more effectively and generalize better
than global prediction approaches that fit a single model to the entire TM.
Compared to existing TM prediction methods, our method reduces RMSE by up to
92\% for Abilene and 75\% for G\'EANT. In routing scenarios, our clustered
predictions also reduce maximum link utilization (MLU) bias by 18\% and 21\%,
respectively, demonstrating the practical benefits of clustering when TMs are
used for network optimization.

</details>


### [83] [Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits](https://arxiv.org/abs/2509.15073)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Non-stationary multi-armed bandits enable agents to adapt to changing
environments by incorporating mechanisms to detect and respond to shifts in
reward distributions, making them well-suited for dynamic settings. However,
existing approaches typically assume that reward feedback is available at every
round - an assumption that overlooks many real-world scenarios where feedback
is limited. In this paper, we take a significant step forward by introducing a
new model of constrained feedback in non-stationary multi-armed bandits, where
the availability of reward feedback is restricted. We propose the first
prior-free algorithm - that is, one that does not require prior knowledge of
the degree of non-stationarity - that achieves near-optimal dynamic regret in
this setting. Specifically, our algorithm attains a dynamic regret of
$\tilde{\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the
number of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$
is the variation budget capturing the degree of non-stationarity.

</details>


### [84] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour,Maryam Eyvazi,Yanqing Zhang*

Main category: cs.LG

TL;DR: AI system predicts air pollution from sky images and generates visual pollution scenarios using generative modeling and vision-language models for public awareness.


<details>
  <summary>Details</summary>
Motivation: Conventional air pollution monitoring has limited spatial coverage and accessibility, creating a need for more accessible and transparent air quality visualization systems.

Method: Combines statistical texture analysis with supervised learning for pollution classification, and uses VLM-guided image generation to create realistic pollution visualizations from sky images.

Result: Validated on urban sky image dataset, effective for both pollution level estimation and semantically consistent visual synthesis of varying pollution scenarios.

Conclusion: System enables transparent air quality interfaces for public engagement, with future plans for green CNN architecture and FPGA-based incremental learning for energy-efficient edge deployment.

Abstract: Air pollution remains a critical threat to public health and environmental
sustainability, yet conventional monitoring systems are often constrained by
limited spatial coverage and accessibility. This paper proposes an AI-driven
agent that predicts ambient air pollution levels from sky images and
synthesizes realistic visualizations of pollution scenarios using generative
modeling. Our approach combines statistical texture analysis with supervised
learning for pollution classification, and leverages vision-language model
(VLM)-guided image generation to produce interpretable representations of air
quality conditions. The generated visuals simulate varying degrees of
pollution, offering a foundation for user-facing interfaces that improve
transparency and support informed environmental decision-making. These outputs
can be seamlessly integrated into intelligent applications aimed at enhancing
situational awareness and encouraging behavioral responses based on real-time
forecasts. We validate our method using a dataset of urban sky images and
demonstrate its effectiveness in both pollution level estimation and
semantically consistent visual synthesis. The system design further
incorporates human-centered user experience principles to ensure accessibility,
clarity, and public engagement in air quality forecasting. To support scalable
and energy-efficient deployment, future iterations will incorporate a green CNN
architecture enhanced with FPGA-based incremental learning, enabling real-time
inference on edge platforms.

</details>


### [85] [Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning](https://arxiv.org/abs/2509.15087)
*Lei Wang,Jieming Bian,Letian Zhang,Jie Xu*

Main category: cs.LG

TL;DR: FedLEASE is a federated learning framework that adaptively allocates and selects LoRA experts for efficient LLM fine-tuning across heterogeneous clients.


<details>
  <summary>Details</summary>
Motivation: Address challenges in federated LoRA fine-tuning for LLMs, including optimal expert allocation across heterogeneous clients and selective expert utilization based on data characteristics.

Method: Proposes FedLEASE framework with adaptive client clustering based on representation similarity to allocate domain-specific LoRA experts, and adaptive top-M Mixture-of-Experts mechanism for client-specific expert selection.

Result: Extensive experiments show FedLEASE significantly outperforms existing federated fine-tuning approaches in heterogeneous client settings while maintaining communication efficiency.

Conclusion: FedLEASE provides an effective solution for privacy-preserving LLM fine-tuning across distributed organizations with heterogeneous data, overcoming limitations of single LoRA modules in federated settings.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
various tasks, but fine-tuning them for domain-specific applications often
requires substantial domain-specific data that may be distributed across
multiple organizations. Federated Learning (FL) offers a privacy-preserving
solution, but faces challenges with computational constraints when applied to
LLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient
fine-tuning approach, though a single LoRA module often struggles with
heterogeneous data across diverse domains. This paper addresses two critical
challenges in federated LoRA fine-tuning: 1. determining the optimal number and
allocation of LoRA experts across heterogeneous clients, and 2. enabling
clients to selectively utilize these experts based on their specific data
characteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation
and SElection), a novel framework that adaptively clusters clients based on
representation similarity to allocate and train domain-specific LoRA experts.
It also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows
each client to select the optimal number of utilized experts. Our extensive
experiments on diverse benchmark datasets demonstrate that FedLEASE
significantly outperforms existing federated fine-tuning approaches in
heterogeneous client settings while maintaining communication efficiency.

</details>


### [86] [Emergent Alignment via Competition](https://arxiv.org/abs/2509.15090)
*Natalie Collina,Surbhi Goel,Aaron Roth,Emily Ryu,Mirah Shi*

Main category: cs.LG

TL;DR: Strategic competition among multiple misaligned AI agents can achieve outcomes comparable to perfect alignment when user utility lies within the convex hull of agents' utilities, especially with increased model diversity.


<details>
  <summary>Details</summary>
Motivation: Addressing the fundamental challenge of aligning AI systems with human values, and exploring whether imperfect alignment can still yield benefits through strategic interactions with multiple misaligned agents.

Method: Modeling the scenario as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, with theoretical analysis and experimental validation.

Result: Three key theoretical results: (1) user can learn Bayes-optimal action under convex hull condition, (2) non-strategic user achieves near-optimal utility with approximate utility learning, (3) equilibrium guarantees remain near-optimal when selecting best single AI after evaluation.

Conclusion: Strategic competition among diverse misaligned AI agents can effectively substitute for perfect alignment, providing near-optimal outcomes for users without requiring perfectly aligned individual models.

Abstract: Aligning AI systems with human values remains a fundamental challenge, but
does our inability to create perfectly aligned models preclude obtaining the
benefits of alignment? We study a strategic setting where a human user
interacts with multiple differently misaligned AI agents, none of which are
individually well-aligned. Our key insight is that when the users utility lies
approximately within the convex hull of the agents utilities, a condition that
becomes easier to satisfy as model diversity increases, strategic competition
can yield outcomes comparable to interacting with a perfectly aligned model. We
model this as a multi-leader Stackelberg game, extending Bayesian persuasion to
multi-round conversations between differently informed parties, and prove three
results: (1) when perfect alignment would allow the user to learn her
Bayes-optimal action, she can also do so in all equilibria under the convex
hull condition (2) under weaker assumptions requiring only approximate utility
learning, a non-strategic user employing quantal response achieves near-optimal
utility in all equilibria and (3) when the user selects the best single AI
after an evaluation period, equilibrium guarantees remain near-optimal without
further distributional assumptions. We complement the theory with two sets of
experiments.

</details>


### [87] [The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning](https://arxiv.org/abs/2509.15097)
*Mohammad Saleh Vahdatpour,Huaiyuan Chu,Yanqing Zhang*

Main category: cs.LG

TL;DR: Hybrid framework combining hierarchical decomposition with FPGA-based equation solving and incremental learning to reduce computational/energy costs in large language models while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the unsustainable computational and energy demands of deep learning, particularly in large-scale architectures like foundation models and LLMs, where traditional gradient-based training is inefficient and power-hungry.

Method: Divides neural network into two tiers: lower layers optimized via single-step equation solving on FPGAs for efficient feature extraction, and higher layers using adaptive incremental learning for continual updates without full retraining. Introduces Compound LLM framework with lower-level LLM for representation learning and upper-level LLM for adaptive decision-making.

Result: Significantly reduces computational costs while preserving high model performance, enhances scalability, reduces redundant computation, and enables efficient deployment in energy-constrained environments.

Conclusion: The proposed hybrid framework provides a sustainable AI solution that is well-suited for edge deployment and real-time adaptation, addressing the energy efficiency challenges of large-scale deep learning models.

Abstract: The rising computational and energy demands of deep learning, particularly in
large-scale architectures such as foundation models and large language models
(LLMs), pose significant challenges to sustainability. Traditional
gradient-based training methods are inefficient, requiring numerous iterative
updates and high power consumption. To address these limitations, we propose a
hybrid framework that combines hierarchical decomposition with FPGA-based
direct equation solving and incremental learning. Our method divides the neural
network into two functional tiers: lower layers are optimized via single-step
equation solving on FPGAs for efficient and parallelizable feature extraction,
while higher layers employ adaptive incremental learning to support continual
updates without full retraining. Building upon this foundation, we introduce
the Compound LLM framework, which explicitly deploys LLM modules across both
hierarchy levels. The lower-level LLM handles reusable representation learning
with minimal energy overhead, while the upper-level LLM performs adaptive
decision-making through energy-aware updates. This integrated design enhances
scalability, reduces redundant computation, and aligns with the principles of
sustainable AI. Theoretical analysis and architectural insights demonstrate
that our method reduces computational costs significantly while preserving high
model performance, making it well-suited for edge deployment and real-time
adaptation in energy-constrained environments.

</details>


### [88] [Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting](https://arxiv.org/abs/2509.15105)
*Liran Nochumsohn,Raz Marshanski,Hedi Zisling,Omri Azencot*

Main category: cs.LG

TL;DR: Super-Linear is a lightweight mixture-of-experts model that uses frequency-specialized linear experts and spectral gating to achieve state-of-the-art time series forecasting performance with superior efficiency and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing large pre-trained models for time series forecasting show strong zero-shot performance but suffer from high computational costs, creating a need for more efficient yet accurate forecasting solutions.

Method: Replaces deep architectures with simple frequency-specialized linear experts trained on resampled data across multiple frequency regimes, using a lightweight spectral gating mechanism to dynamically select relevant experts.

Result: Matches state-of-the-art performance while offering superior efficiency, robustness to various sampling rates, and enhanced interpretability compared to existing models.

Conclusion: Super-Linear demonstrates that simple linear experts with frequency specialization can achieve competitive forecasting performance with significantly improved computational efficiency and practical benefits for real-world applications.

Abstract: Time series forecasting (TSF) is critical in domains like energy, finance,
healthcare, and logistics, requiring models that generalize across diverse
datasets. Large pre-trained models such as Chronos and Time-MoE show strong
zero-shot (ZS) performance but suffer from high computational costs. In this
work, We introduce Super-Linear, a lightweight and scalable mixture-of-experts
(MoE) model for general forecasting. It replaces deep architectures with simple
frequency-specialized linear experts, trained on resampled data across multiple
frequency regimes. A lightweight spectral gating mechanism dynamically selects
relevant experts, enabling efficient, accurate forecasting. Despite its
simplicity, Super-Linear matches state-of-the-art performance while offering
superior efficiency, robustness to various sampling rates, and enhanced
interpretability. The implementation of Super-Linear is available at
\href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}

</details>


### [89] [Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges](https://arxiv.org/abs/2509.15107)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.LG

TL;DR: Systematic analysis reveals significant limitations in current chest X-ray AI datasets including label errors, domain shift issues, and performance degradation across datasets and demographic subgroups.


<details>
  <summary>Details</summary>
Motivation: Current public chest X-ray datasets have accelerated AI progress but suffer from label extraction errors, domain shift problems, population bias, and inadequate clinical evaluation measures that limit real-world applicability.

Method: Conducted cross-dataset domain shift evaluation across multiple model architectures, trained source-classification models to detect dataset bias, performed subgroup analyses for age/sex groups, and conducted expert radiologist review of dataset labels.

Result: Found substantial external performance degradation (reduced AUPRC and F1 scores), near-perfect dataset distinguishability indicating bias, reduced performance for minority demographic groups, and significant disagreement between radiologist review and automated labels.

Conclusion: Current chest X-ray benchmarks have important clinical weaknesses, highlighting the need for clinician-validated datasets and fairer evaluation frameworks to ensure reliable AI performance in real clinical settings.

Abstract: Artificial intelligence has shown significant promise in chest radiography,
where deep learning models can approach radiologist-level diagnostic
performance. Progress has been accelerated by large public datasets such as
MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of
thousands of labelled images with pathology annotations. However, these
datasets also present important limitations. Automated label extraction from
radiology reports introduces errors, particularly in handling uncertainty and
negation, and radiologist review frequently disagrees with assigned labels. In
addition, domain shift and population bias restrict model generalisability,
while evaluation practices often overlook clinically meaningful measures. We
conduct a systematic analysis of these challenges, focusing on label quality,
dataset bias, and domain shift. Our cross-dataset domain shift evaluation
across multiple model architectures revealed substantial external performance
degradation, with pronounced reductions in AUPRC and F1 scores relative to
internal testing. To assess dataset bias, we trained a source-classification
model that distinguished datasets with near-perfect accuracy, and performed
subgroup analyses showing reduced performance for minority age and sex groups.
Finally, expert review by two board-certified radiologists identified
significant disagreement with public dataset labels. Our findings highlight
important clinical weaknesses of current benchmarks and emphasise the need for
clinician-validated datasets and fairer evaluation frameworks.

</details>


### [90] [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
*Dan Zhang,Min Cai,Jonathan Li,Ziniu Hu,Yisong Yue,Yuxiao Dong,Jie Tang*

Main category: cs.LG

TL;DR: TDRM introduces temporal-difference regularization to create smoother, more reliable reward models that improve RL training stability and alignment with long-term objectives.


<details>
  <summary>Details</summary>
Motivation: Existing reward models lack temporal consistency, leading to ineffective policy updates and unstable reinforcement learning training.

Method: TDRM minimizes temporal differences during training to produce smooth rewards, and is incorporated into actor-critic style online RL loops as a supplement to verifiable reward methods.

Result: TD-trained process reward models improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with RLVR, they achieve comparable performance with just 2.5k data vs 50.1k for baselines, and yield higher-quality policies on 8 model variants.

Conclusion: TDRM provides an effective method for creating temporally consistent reward models that significantly improve RL efficiency and performance across multiple language model variants.

Abstract: Reward models are central to both reinforcement learning (RL) with language
models and inference-time verification. However, existing reward models often
lack temporal consistency, leading to ineffective policy updates and unstable
RL training. We introduce TDRM, a method for learning smoother and more
reliable reward models by minimizing temporal differences during training. This
temporal-difference (TD) regularization produces smooth rewards and improves
alignment with long-term objectives. Incorporating TDRM into the actor-critic
style online RL loop yields consistent empirical gains. It is worth noting that
TDRM is a supplement to verifiable reward methods, and both can be used in
series. Experiments show that TD-trained process reward models (PRMs) improve
performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)
settings. When combined with Reinforcement Learning with Verifiable Rewards
(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable
performance with just 2.5k data to what baseline methods require 50.1k data to
attain -- and yield higher-quality language model policies on 8 model variants
(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,
Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release
all code at https://github.com/THUDM/TDRM.

</details>


### [91] [Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers](https://arxiv.org/abs/2509.15113)
*Andrei Chertkov,Artem Basharin,Mikhail Saygin,Evgeny Frolov,Stanislav Straupe,Ivan Oseledets*

Main category: cs.LG

TL;DR: A framework for end-to-end training of hybrid neural networks combining digital networks with non-differentiable physical components using stochastic zeroth-order optimization and dynamic low-rank surrogate models.


<details>
  <summary>Details</summary>
Motivation: The need to integrate energy-efficient physical computing platforms (photonic, neuromorphic) into deep learning despite their non-differentiable nature and limited expressiveness, which makes on-device backpropagation difficult.

Method: Combines stochastic zeroth-order optimization for physical layer parameter updates with a dynamic low-rank surrogate model for gradient propagation. Uses implicit projector-splitting integrator algorithm to update surrogate model efficiently with minimal hardware queries.

Result: Achieves near-digital baseline accuracy across computer vision, audio classification, and language modeling tasks. Successfully trains hybrid models with various non-differentiable physical components (spatial light modulators, microring resonators, Mach-Zehnder interferometers).

Conclusion: Bridges hardware-aware deep learning and gradient-free optimization, providing a practical pathway for integrating non-differentiable physical components into scalable, end-to-end trainable AI systems.

Abstract: The growing demand for energy-efficient, high-performance AI systems has led
to increased attention on alternative computing platforms (e.g., photonic,
neuromorphic) due to their potential to accelerate learning and inference.
However, integrating such physical components into deep learning pipelines
remains challenging, as physical devices often offer limited expressiveness,
and their non-differentiable nature renders on-device backpropagation difficult
or infeasible. This motivates the development of hybrid architectures that
combine digital neural networks with reconfigurable physical layers, which
effectively behave as black boxes. In this work, we present a framework for the
end-to-end training of such hybrid networks. This framework integrates
stochastic zeroth-order optimization for updating the physical layer's internal
parameters with a dynamic low-rank surrogate model that enables gradient
propagation through the physical layer. A key component of our approach is the
implicit projector-splitting integrator algorithm, which updates the
lightweight surrogate model after each forward pass with minimal hardware
queries, thereby avoiding costly full matrix reconstruction. We demonstrate our
method across diverse deep learning tasks, including: computer vision, audio
classification, and language modeling. Notably, across all modalities, the
proposed approach achieves near-digital baseline accuracy and consistently
enables effective end-to-end training of hybrid models incorporating various
non-differentiable physical components (spatial light modulators, microring
resonators, and Mach-Zehnder interferometers). This work bridges hardware-aware
deep learning and gradient-free optimization, thereby offering a practical
pathway for integrating non-differentiable physical components into scalable,
end-to-end trainable AI systems.

</details>


### [92] [Efficient Conformal Prediction for Regression Models under Label Noise](https://arxiv.org/abs/2509.15120)
*Yahav Cohen,Jacob Goldberger,Tom Tirer*

Main category: cs.LG

TL;DR: A method to apply conformal prediction for regression models with noisy calibration labels, achieving near-clean-label performance in medical imaging applications.


<details>
  <summary>Details</summary>
Motivation: In high-stakes medical imaging applications, reliable confidence intervals are critical. Conformal prediction requires clean labels, but calibration sets often contain noisy labels, necessitating a robust solution.

Method: Developed a mathematically grounded procedure to estimate noise-free conformal prediction thresholds, then created a practical algorithm to handle continuous regression problems with Gaussian label noise.

Result: Evaluated on two medical imaging regression datasets, the method significantly outperforms existing alternatives and achieves performance close to the clean-label setting.

Conclusion: The proposed approach successfully addresses the challenge of applying conformal prediction with noisy calibration labels, providing reliable confidence intervals for regression models in critical applications.

Abstract: In high-stakes scenarios, such as medical imaging applications, it is
critical to equip the predictions of a regression model with reliable
confidence intervals. Recently, Conformal Prediction (CP) has emerged as a
powerful statistical framework that, based on a labeled calibration set,
generates intervals that include the true labels with a pre-specified
probability. In this paper, we address the problem of applying CP for
regression models when the calibration set contains noisy labels. We begin by
establishing a mathematically grounded procedure for estimating the noise-free
CP threshold. Then, we turn it into a practical algorithm that overcomes the
challenges arising from the continuous nature of the regression problem. We
evaluate the proposed method on two medical imaging regression datasets with
Gaussian label noise. Our method significantly outperforms the existing
alternative, achieving performance close to the clean-label setting.

</details>


### [93] [Optimal Learning from Label Proportions with General Loss Functions](https://arxiv.org/abs/2509.15145)
*Lorne Applebaum,Travis Dick,Claudio Gentile,Haim Kaplan,Tomer Koren*

Main category: cs.LG

TL;DR: Novel low-variance de-biasing method for Learning from Label Proportions (LLP) that works with various loss functions in binary and multi-class classification, improving sample complexity and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing problems in online advertising where only aggregate label information (average label values per group/bag) is available, but individual example prediction is needed.

Method: Introduces a versatile low-variance de-biasing methodology that combines novel estimators with standard techniques to learn from aggregate label proportions.

Result: Significantly advances state of the art in LLP, improves sample complexity guarantees for practical loss functions, and demonstrates compelling empirical advantages across diverse benchmark datasets.

Conclusion: The proposed approach effectively solves the LLP problem with flexible loss function accommodation and strong performance improvements over existing methods.

Abstract: Motivated by problems in online advertising, we address the task of Learning
from Label Proportions (LLP). In this partially-supervised setting, training
data consists of groups of examples, termed bags, for which we only observe the
average label value. The main goal, however, remains the design of a predictor
for the labels of individual examples. We introduce a novel and versatile
low-variance de-biasing methodology to learn from aggregate label information,
significantly advancing the state of the art in LLP. Our approach exhibits
remarkable flexibility, seamlessly accommodating a broad spectrum of
practically relevant loss functions across both binary and multi-class
classification settings. By carefully combining our estimators with standard
techniques, we substantially improve sample complexity guarantees for a large
class of losses of practical relevance. We also empirically validate the
efficacy of our proposed approach across a diverse array of benchmark datasets,
demonstrating compelling empirical advantages over standard baselines.

</details>


### [94] [Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning](https://arxiv.org/abs/2509.15147)
*Viktor Kovalchuk,Nikita Kotelevskii,Maxim Panov,Samuel HorvÃ¡th,Martin TakÃ¡Ä*

Main category: cs.LG

TL;DR: Logit-based federated learning reduces communication costs by sharing only logits instead of full model weights/gradients. This paper compares three aggregation methods for handling heterogeneous client data.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning shares model weights or gradients which is costly for large models. Logit-based FL reduces communication overhead but faces challenges in aggregating information from heterogeneous clients.

Method: Three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10 datasets.

Result: The methods reduce communication overhead, improve robustness under non-IID data, and achieve accuracy competitive with centralized training.

Conclusion: Logit-based FL with proper aggregation methods provides an efficient alternative to traditional FL, offering reduced communication costs while maintaining competitive performance even with heterogeneous client data.

Abstract: Federated learning (FL) usually shares model weights or gradients, which is
costly for large models. Logit-based FL reduces this cost by sharing only
logits computed on a public proxy dataset. However, aggregating information
from heterogeneous clients is still challenging. This paper studies this
problem, introduces and compares three logit aggregation methods: simple
averaging, uncertainty-weighted averaging, and a learned meta-aggregator.
Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead,
improve robustness under non-IID data, and achieve accuracy competitive with
centralized training.

</details>


### [95] [Self-Improving Embodied Foundation Models](https://arxiv.org/abs/2509.15155)
*Seyed Kamyar Seyed Ghasemipour,Ayzaan Wahid,Jonathan Tompson,Pannag Sanketi,Igor Mordatch*

Main category: cs.LG

TL;DR: A two-stage post-training approach combining supervised fine-tuning and self-improvement enables foundation models to autonomously practice and acquire novel robotic skills beyond imitation learning datasets.


<details>
  <summary>Details</summary>
Motivation: Foundation models have transformed robotics but remain limited to behavioral cloning for low-level control. The success of RL fine-tuning in LLMs inspired applying similar two-stage approach to robotics.

Method: Two-stage approach: 1) Supervised Fine-Tuning (SFT) with behavioral cloning and steps-to-go prediction, 2) Self-Improvement stage using extracted reward function and success detector for autonomous practice with minimal human supervision.

Result: Significantly more sample-efficient than scaling imitation data, leads to higher success rates, and uniquely enables autonomous acquisition of novel skills beyond training datasets. Web-scale pretraining combined with self-improvement is key to sample efficiency.

Conclusion: Combining pretrained foundation models with online self-improvement has transformative potential for autonomous skill acquisition in robotics, enabling generalization far beyond imitation learning datasets.

Abstract: Foundation models trained on web-scale data have revolutionized robotics, but
their application to low-level control remains largely limited to behavioral
cloning. Drawing inspiration from the success of the reinforcement learning
stage in fine-tuning large language models, we propose a two-stage
post-training approach for robotics. The first stage, Supervised Fine-Tuning
(SFT), fine-tunes pretrained foundation models using both: a) behavioral
cloning, and b) steps-to-go prediction objectives. In the second stage,
Self-Improvement, steps-to-go prediction enables the extraction of a
well-shaped reward function and a robust success detector, enabling a fleet of
robots to autonomously practice downstream tasks with minimal human
supervision. Through extensive experiments on real-world and simulated robot
embodiments, our novel post-training recipe unveils significant results on
Embodied Foundation Models. First, we demonstrate that the combination of SFT
and Self-Improvement is significantly more sample-efficient than scaling
imitation data collection for supervised learning, and that it leads to
policies with significantly higher success rates. Further ablations highlight
that the combination of web-scale pretraining and Self-Improvement is the key
to this sample-efficiency. Next, we demonstrate that our proposed combination
uniquely unlocks a capability that current methods cannot achieve: autonomously
practicing and acquiring novel skills that generalize far beyond the behaviors
observed in the imitation learning datasets used during training. These
findings highlight the transformative potential of combining pretrained
foundation models with online Self-Improvement to enable autonomous skill
acquisition in robotics. Our project website can be found at
https://self-improving-efms.github.io .

</details>


### [96] [Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning](https://arxiv.org/abs/2509.15157)
*Shiwan Zhao,Xuyang Zhao,Jiaming Zhou,Aobo Kong,Qicheng Li,Yong Qin*

Main category: cs.LG

TL;DR: Proactive data rewriting framework for off-policy SFT that reduces policy gap by keeping correct solutions and rewriting incorrect ones with guided re-solving, improving training stability and performance.


<details>
  <summary>Details</summary>
Motivation: Standard importance sampling in off-policy SFT suffers from high variance and instability due to large policy gaps between expert demonstrations and target policy.

Method: Data rewriting framework that proactively shrinks policy gap by: 1) keeping correct solutions as on-policy data, 2) rewriting incorrect solutions with guided re-solving, 3) using expert demonstrations only when needed.

Result: Consistent and significant gains over vanilla SFT and state-of-the-art Dynamic Fine-Tuning on five mathematical reasoning benchmarks.

Conclusion: Proactive data alignment before optimization reduces importance sampling variance and stabilizes off-policy fine-tuning, outperforming existing approaches.

Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an
off-policy learning problem, where expert demonstrations come from a fixed
behavior policy while training aims to optimize a target policy. Importance
sampling is the standard tool for correcting this distribution mismatch, but
large policy gaps lead to high variance and training instability. Existing
approaches mitigate this issue using KL penalties or clipping, which passively
constrain updates rather than actively reducing the gap. We propose a simple
yet effective data rewriting framework that proactively shrinks the policy gap
by keeping correct solutions as on-policy data and rewriting incorrect ones
with guided re-solving, falling back to expert demonstrations only when needed.
This aligns the training distribution with the target policy before
optimization, reducing importance sampling variance and stabilizing off-policy
fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate
consistent and significant gains over both vanilla SFT and the state-of-the-art
Dynamic Fine-Tuning (DFT) approach. The data and code will be released at
https://github.com/NKU-HLT/Off-Policy-SFT.

</details>


### [97] [MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration](https://arxiv.org/abs/2509.15187)
*Giorgos Armeniakos,Alexis Maras,Sotirios Xydis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: Proposes MaRVIn, a hardware-software co-design framework with novel ISA extensions and micro-architecture for efficient mixed-precision NN execution on RISC-V, achieving 17.6x speedup with <1% accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing embedded microprocessors lack architectural support for mixed-precision NNs, causing inefficiencies like excessive data packing/unpacking and underutilized arithmetic units, despite mixed-precision techniques showing potential for efficiency gains.

Method: Hardware-software co-design with enhanced ALU supporting configurable mixed-precision arithmetic (2,4,8 bits), multi-pumping, soft SIMD for 2-bit ops, pruning-aware fine-tuning, greedy DSE for model optimization, and voltage scaling.

Result: Achieves 17.6x average speedup with less than 1% accuracy loss on CIFAR10 and ImageNet datasets, outperforms state-of-the-art RISC-V cores with up to 1.8 TOPs/W power efficiency.

Conclusion: MaRVIn framework successfully addresses mixed-precision execution inefficiencies through cross-layer optimizations, demonstrating significant performance and energy efficiency improvements for deep learning inference on RISC-V architectures.

Abstract: The evolution of quantization and mixed-precision techniques has unlocked new
possibilities for enhancing the speed and energy efficiency of NNs. Several
recent studies indicate that adapting precision levels across different
parameters can maintain accuracy comparable to full-precision models while
significantly reducing computational demands. However, existing embedded
microprocessors lack sufficient architectural support for efficiently executing
mixed-precision NNs, both in terms of ISA extensions and hardware design,
resulting in inefficiencies such as excessive data packing/unpacking and
underutilized arithmetic units. In this work, we propose novel ISA extensions
and a micro-architecture implementation specifically designed to optimize
mixed-precision execution, enabling energy-efficient deep learning inference on
RISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software
co-design framework that enhances power efficiency and performance through a
combination of hardware improvements, mixed-precision quantization, ISA-level
optimizations, and cycle-accurate emulation. At the hardware level, we enhance
the ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for
weights/activations and employ multi-pumping to reduce execution latency while
implementing soft SIMD for efficient 2-bit ops. At the software level, we
integrate a pruning-aware fine-tuning method to optimize model compression and
a greedy-based DSE approach to efficiently search for Pareto-optimal
mixed-quantized models. Additionally, we incorporate voltage scaling to boost
the power efficiency of our system. Our experimental evaluation over widely
used DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our
framework can achieve, on average, 17.6x speedup for less than 1% accuracy loss
and outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up
to 1.8 TOPs/W.

</details>


### [98] [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/abs/2509.15194)
*Yujun Zhou,Zhenwen Liang,Haolin Liu,Wenhao Yu,Kishan Panaganti,Linfeng Song,Dian Yu,Xiangliang Zhang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: EVOL-RL is a novel reinforcement learning method that prevents entropy collapse in label-free settings by combining majority-vote stability with novelty-aware variation rewards, enabling models to self-improve without sacrificing exploration diversity.


<details>
  <summary>Details</summary>
Motivation: Existing label-free RL methods cause entropy collapse where models generate shorter, less diverse outputs. The goal is to enable general improvements without sacrificing exploration capacity and generalization ability.

Method: EVOL-RL couples majority-voted answers as stable anchors with novelty-aware rewards that favor semantically different reasoning. Uses GRPO implementation with asymmetric clipping and entropy regularization.

Result: EVOL-RL significantly outperforms TTRL baseline, improving Qwen3-4B-Base AIME25 pass@1 from 4.6% to 16.4% and pass@16 from 18.5% to 37.9%. Prevents diversity collapse and improves generalization across domains like GPQA.

Conclusion: EVOL-RL effectively prevents entropy collapse while enabling strong generalization, demonstrating broad applicability in both label-free and RLVR settings.

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning from verifiable rewards (RLVR), yet real-world deployment demands
models that can self-improve without labels or external judges. Existing
label-free methods, confidence minimization, self-consistency, or majority-vote
objectives, stabilize learning but steadily shrink exploration, causing an
entropy collapse: generations become shorter, less diverse, and brittle. Unlike
prior approaches such as Test-Time Reinforcement Learning (TTRL), which
primarily adapt models to the immediate unlabeled dataset at hand, our goal is
broader: to enable general improvements without sacrificing the model's
inherent exploration capacity and generalization ability, i.e., evolving. We
formalize this issue and propose EVolution-Oriented and Label-free
Reinforcement Learning (EVOL-RL), a simple rule that couples stability with
variation under a label-free setting. EVOL-RL keeps the majority-voted answer
as a stable anchor (selection) while adding a novelty-aware reward that favors
responses whose reasoning differs from what has already been produced
(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also
uses asymmetric clipping to preserve strong signals and an entropy regularizer
to sustain search. This majority-for-selection + novelty-for-variation design
prevents collapse, maintains longer and more informative chains of thought, and
improves both pass@1 and pass@n. EVOL-RL consistently outperforms the
majority-only TTRL baseline; e.g., training on label-free AIME24 lifts
Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%
to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks
stronger generalization across domains (e.g., GPQA). Furthermore, we
demonstrate that EVOL-RL also boosts performance in the RLVR setting,
highlighting its broad applicability.

</details>


### [99] [Explaining deep learning for ECG using time-localized clusters](https://arxiv.org/abs/2509.15198)
*AhcÃ¨ne Boubekki,Konstantinos Patlatzoglou,Joseph Barker,Fu Siong Ng,AntÃ´nio H. Ribeiro*

Main category: cs.LG

TL;DR: Novel interpretability method for CNN-based ECG analysis that extracts time-localized clusters from model representations to visualize waveform contributions and quantify prediction uncertainty.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for ECG analysis lack interpretability, limiting understanding of model decisions and preventing knowledge extraction from these advanced systems.

Method: Extracts time-localized clusters from CNN's internal representations to segment ECG based on learned characteristics while quantifying representation uncertainty.

Result: Enables visualization of how different ECG waveform regions contribute to predictions and assessment of decision certainty.

Conclusion: Provides structured interpretability for ECG deep learning models, enhancing trust in AI diagnostics and facilitating discovery of clinically relevant electrophysiological patterns.

Abstract: Deep learning has significantly advanced electrocardiogram (ECG) analysis,
enabling automatic annotation, disease screening, and prognosis beyond
traditional clinical capabilities. However, understanding these models remains
a challenge, limiting interpretation and gaining knowledge from these
developments. In this work, we propose a novel interpretability method for
convolutional neural networks applied to ECG analysis. Our approach extracts
time-localized clusters from the model's internal representations, segmenting
the ECG according to the learned characteristics while quantifying the
uncertainty of these representations. This allows us to visualize how different
waveform regions contribute to the model's predictions and assess the certainty
of its decisions. By providing a structured and interpretable view of deep
learning models for ECG, our method enhances trust in AI-driven diagnostics and
facilitates the discovery of clinically relevant electrophysiological patterns.

</details>


### [100] [CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness](https://arxiv.org/abs/2509.15199)
*Ying Zheng,Yangfan Jiang,Kian-Lee Tan*

Main category: cs.LG

TL;DR: CausalPre is a scalable causality-guided data pre-processing framework that achieves causal fairness without requiring strong causal model assumptions, using distribution estimation and efficient computation.


<details>
  <summary>Details</summary>
Motivation: Existing causal fairness approaches either require known causal models or fail to capture broader attribute relationships, limiting their utility and effectiveness.

Method: Reformulates causal fairness extraction into a distribution estimation problem using low-dimensional marginal factorization and a heuristic algorithm for computational efficiency.

Result: Extensive experiments show CausalPre is both effective and scalable, achieving strong causal fairness while maintaining relationship coverage.

Conclusion: CausalPre demonstrates that causal fairness can be achieved without trading off relationship coverage or requiring strong causal model assumptions.

Abstract: Causal fairness in databases is crucial to preventing biased and inaccurate
outcomes in downstream tasks. While most prior work assumes a known causal
model, recent efforts relax this assumption by enforcing additional
constraints. However, these approaches often fail to capture broader attribute
relationships that are critical to maintaining utility. This raises a
fundamental question: Can we harness the benefits of causal reasoning to design
efficient and effective fairness solutions without relying on strong
assumptions about the underlying causal model? In this paper, we seek to answer
this question by introducing CausalPre, a scalable and effective
causality-guided data pre-processing framework that guarantees justifiable
fairness, a strong causal notion of fairness. CausalPre extracts causally fair
relationships by reformulating the originally complex and computationally
infeasible extraction task into a tailored distribution estimation problem. To
ensure scalability, CausalPre adopts a carefully crafted variant of
low-dimensional marginal factorization to approximate the joint distribution,
complemented by a heuristic algorithm that efficiently tackles the associated
computational challenge. Extensive experiments on benchmark datasets
demonstrate that CausalPre is both effective and scalable, challenging the
conventional belief that achieving causal fairness requires trading off
relationship coverage for relaxed model assumptions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [101] [Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity](https://arxiv.org/abs/2509.14276)
*Yuxiang Mai,Qiyue Yin,Wancheng Ni,Pei Xu,Kaiqi Huang*

Main category: cs.MA

TL;DR: CoDiCon introduces competitive incentives in cooperative MARL to foster strategic diversity through constructive conflict and policy exchange among agents.


<details>
  <summary>Details</summary>
Motivation: Existing MARL diversity methods focus on individual agent characteristics but neglect agent interplay and mutual influence during policy formation.

Method: Uses intrinsic reward mechanism with ranking features to introduce competitive motivations, with centralized reward module balancing competition and cooperation through bilevel optimization.

Result: Superior performance in SMAC and GRF environments compared to state-of-the-art methods, with competitive intrinsic rewards effectively promoting diverse and adaptive strategies.

Conclusion: CoDiCon successfully incorporates competitive incentives into cooperative scenarios, demonstrating that constructive conflict enhances strategic diversity and overall performance in multi-agent systems.

Abstract: In recent years, diversity has emerged as a useful mechanism to enhance the
efficiency of multi-agent reinforcement learning (MARL). However, existing
methods predominantly focus on designing policies based on individual agent
characteristics, often neglecting the interplay and mutual influence among
agents during policy formation. To address this gap, we propose Competitive
Diversity through Constructive Conflict (CoDiCon), a novel approach that
incorporates competitive incentives into cooperative scenarios to encourage
policy exchange and foster strategic diversity among agents. Drawing
inspiration from sociological research, which highlights the benefits of
moderate competition and constructive conflict in group decision-making, we
design an intrinsic reward mechanism using ranking features to introduce
competitive motivations. A centralized intrinsic reward module generates and
distributes varying reward values to agents, ensuring an effective balance
between competition and cooperation. By optimizing the parameterized
centralized reward module to maximize environmental rewards, we reformulate the
constrained bilevel optimization problem to align with the original task
objectives. We evaluate our algorithm against state-of-the-art methods in the
SMAC and GRF environments. Experimental results demonstrate that CoDiCon
achieves superior performance, with competitive intrinsic rewards effectively
promoting diverse and adaptive strategies among cooperative agents.

</details>


### [102] [LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.14680)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Dong Huang,Yuanye Zhao,Zheng Lin,Zihan Fang,Dianxin Luan,Heming Cui,Yong Cui*

Main category: cs.MA

TL;DR: LEED framework uses LLMs to generate expert demonstrations for multi-agent reinforcement learning, improving coordination and scalability through decentralized policy optimization.


<details>
  <summary>Details</summary>
Motivation: Multi-agent reinforcement learning faces coordination and scalability challenges as the number of agents increases, limiting its effectiveness in complex environments.

Method: Two-component framework: 1) Demonstration generation module uses LLMs to create environment interaction instructions, 2) Policy optimization module uses decentralized training with expert policy loss integrated with individual policy loss.

Result: LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.

Conclusion: The LLM-empowered expert demonstrations framework effectively addresses MARL coordination and scalability issues, enabling agents to personalize policies using both expert knowledge and individual experience.

Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for
intelligent decision-making in complex environments. However, it suffers from a
coordination and scalability bottleneck as the number of agents increases. To
address these issues, we propose the LLM-empowered expert demonstrations
framework for multi-agent reinforcement learning (LEED). LEED consists of two
components: a demonstration generation (DG) module and a policy optimization
(PO) module. Specifically, the DG module leverages large language models to
generate instructions for interacting with the environment, thereby producing
high-quality demonstrations. The PO module adopts a decentralized training
paradigm, where each agent utilizes the generated demonstrations to construct
an expert policy loss, which is then integrated with its own policy loss. This
enables each agent to effectively personalize and optimize its local policy
based on both expert knowledge and individual experience. Experimental results
show that LEED achieves superior sample efficiency, time efficiency, and robust
scalability compared to state-of-the-art baselines.

</details>


### [103] [Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.15103)
*Simin Li,Zheng Yuwei,Zihao Mao,Linhao Wang,Ruixiao Xu,Chengdong Ma,Xin Yu,Yuqing Ma,Qi Dou,Xin Wang,Jie Luo,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: Proposes a method to identify the most vulnerable agents in large-scale multi-agent systems by framing it as a hierarchical adversarial problem and solving it through decomposition and reformulation.


<details>
  <summary>Details</summary>
Motivation: Partial agent failure is inevitable in large-scale systems, and identifying which agents' compromise would most severely degrade overall performance is crucial for system resilience.

Method: Frames the problem as Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), decouples it using Fenchel-Rockafellar transform, and reformulates the combinatorial upper-level problem as an MDP with dense rewards to enable sequential identification of vulnerable agents.

Result: The method effectively identifies more vulnerable agents in large-scale MARL and rule-based systems, causes worse system failures, and learns a value function that reveals each agent's vulnerability.

Conclusion: The proposed decomposition approach successfully solves the challenging HAD-MFC problem while preserving optimal solutions, providing an effective framework for identifying critical vulnerabilities in large-scale multi-agent systems.

Abstract: Partial agent failure becomes inevitable when systems scale up, making it
crucial to identify the subset of agents whose compromise would most severely
degrade overall performance. In this paper, we study this Vulnerable Agent
Identification (VAI) problem in large-scale multi-agent reinforcement learning
(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field
Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task
of selecting the most vulnerable agents, and the lower level learns worst-case
adversarial policies for these agents using mean-field MARL. The two problems
are coupled together, making HAD-MFC difficult to solve. To solve this, we
first decouple the hierarchical process by Fenchel-Rockafellar transform,
resulting a regularized mean-field Bellman operator for upper level that
enables independent learning at each level, thus reducing computational
complexity. We then reformulate the upper-level combinatorial problem as a MDP
with dense rewards from our regularized mean-field Bellman operator, enabling
us to sequentially identify the most vulnerable agents by greedy and RL
algorithms. This decomposition provably preserves the optimal solution of the
original HAD-MFC. Experiments show our method effectively identifies more
vulnerable agents in large-scale MARL and the rule-based system, fooling system
into worse failures, and learns a value function that reveals the vulnerability
of each agent.

</details>
