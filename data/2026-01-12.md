<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [MoEBlaze: Breaking the Memory Wall for Efficient MoE Training on Modern GPUs](https://arxiv.org/abs/2601.05296)
*Jiyuan Zhang,Yining Liu,Siqi Yan,Lisen Deng,Jennifer Cao,Shuqi Yang,Min Ni,Bi Xue,Shen Li*

Main category: cs.LG

TL;DR: MoEBlaze, a memory-efficient MoE training framework, addresses memory bottlenecks in large-scale Mixture-of-Experts architectures, achieving over 4x speedups and over 50% memory savings.


<details>
  <summary>Details</summary>
Motivation: Memory walls in modern large-scale MoE architectures, due to sparse arithmetic compute and activation memory overheads (e.g., large token routing buffers and intermediate tensors), limit batch sizes, sequence lengths, GPU utilization, and cause performance hindrances from excessive data movements.

Method: Introduces a co-designed system approach: (i) an end-to-end token dispatch and MoE training method with optimized data structures to eliminate intermediate buffers and activation materialization, and (ii) co-designed kernels with smart activation checkpointing to reduce memory footprint and improve performance.

Result: MoEBlaze demonstrates significant improvements, achieving over 4x speedups and over 50% memory savings compared to existing MoE frameworks.

Conclusion: MoEBlaze effectively mitigates the memory bottleneck in MoE architectures, enhancing training efficiency and scalability for large-scale models.

Abstract: The pervasive "memory wall" bottleneck is significantly amplified in modern large-scale Mixture-of-Experts (MoE) architectures. MoE's inherent architectural sparsity leads to sparse arithmetic compute and also introduces substantial activation memory overheads -- driven by large token routing buffers and the need to materialize and buffer intermediate tensors. This memory pressure limits the maximum batch size and sequence length that can fit on GPUs, and also results in excessive data movements that hinders performance and efficient model scaling. We present MoEBlaze, a memory-efficient MoE training framework that addresses these issues through a co-designed system approach: (i) an end-to-end token dispatch and MoE training method with optimized data structures to eliminate intermediate buffers and activation materializing, and (ii) co-designed kernels with smart activation checkpoint to mitigate memory footprint while simultaneously achieving better performance. We demonstrate that MoEBlaze can achieve over 4x speedups and over 50% memory savings compared to existing MoE frameworks.

</details>


### [2] [TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning](https://arxiv.org/abs/2601.05300)
*Susmit Das*

Main category: cs.LG

TL;DR: TIME introduces a behavioral alignment framework for LLMs that integrates explicit reasoning as context-sensitive, driven by discourse and temporal cues, reducing reasoning tokens significantly while improving performance on temporally grounded dialogue tasks.


<details>
  <summary>Details</summary>
Motivation: Current reasoning-oriented LLMs use long, explicit 'thinking' traces at the start of responses, which is costly, reduces auditability, and cannot be re-triggered. Dialogue models also lack sensitivity to temporal structure unless explicitly stated in text.

Method: TIME augments dialogue with optional ISO 8601 <time> tags, tick turns for silent gaps, and short <think> blocks for brief in-place reasoning. It uses a four-phase curriculum with a small, maximally diverse full-batch alignment step to train Qwen3 models.

Result: Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude.

Conclusion: TIME effectively integrates temporal intelligence and concise reasoning into LLMs, enhancing performance on temporally grounded dialogue benchmarks with fewer tokens, offering a more efficient and context-aware approach.

Abstract: Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench

</details>


### [3] [Ontology Neural Networks for Topologically Conditioned Constraint Satisfaction](https://arxiv.org/abs/2601.05304)
*Jaehong Oh*

Main category: cs.LG

TL;DR: An enhanced neuro-symbolic reasoning framework using topological conditioning, gradient stabilization, and evolutionary strategy achieves high energy reduction and high constraint satisfaction success across varying problem sizes with consistent convergence.


<details>
  <summary>Details</summary>
Motivation: Neuro-symbolic reasoning systems struggle to maintain semantic coherence while fulfilling physical and logical constraints, addressing challenges in optimization and stability.

Method: Integrates topological conditioning and gradient stabilization via Forman-Ricci curvature for graph topology, Deep Delta Learning for stable constraint projections using rank-one perturbations, and Covariance Matrix Adaptation Evolution Strategy for parameter optimization.

Result: Mean energy reduction to 1.15 compared to baseline 11.68, 95% success rate in constraint satisfaction tasks, seed-independent convergence, and graceful scaling up to twenty-node problems.

Conclusion: Topological structure can effectively guide gradient-based optimization without compromising interpretability or computational efficiency, enabling improved neuro-symbolic reasoning performance.

Abstract: Neuro-symbolic reasoning systems face fundamental challenges in maintaining semantic coherence while satisfying physical and logical constraints. Building upon our previous work on Ontology Neural Networks, we present an enhanced framework that integrates topological conditioning with gradient stabilization mechanisms. The approach employs Forman-Ricci curvature to capture graph topology, Deep Delta Learning for stable rank-one perturbations during constraint projection, and Covariance Matrix Adaptation Evolution Strategy for parameter optimization. Experimental evaluation across multiple problem sizes demonstrates that the method achieves mean energy reduction to 1.15 compared to baseline values of 11.68, with 95 percent success rate in constraint satisfaction tasks. The framework exhibits seed-independent convergence and graceful scaling behavior up to twenty-node problems, suggesting that topological structure can inform gradient-based optimization without sacrificing interpretability or computational efficiency.

</details>
