{"id": "2509.18198", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18198", "abs": "https://arxiv.org/abs/2509.18198", "authors": ["Rui Liu", "Zikang Wang", "Peng Gao", "Yu Shen", "Pratap Tokekar", "Ming Lin"], "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation", "comment": null, "summary": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd.", "AI": {"tldr": "MMCD framework enhances autonomous driving safety through multi-modal collaborative decision-making with cross-modal knowledge distillation to handle missing data modalities.", "motivation": "Address limitations of single vehicles with limited sensor range and practical challenges of sensor failures/missing connected vehicles in multi-modal autonomous systems.", "method": "Proposes MMCD framework that fuses multi-modal observations from ego and collaborative vehicles using cross-modal knowledge distillation with teacher-student model structure.", "result": "Improves driving safety by up to 20.7% in connected autonomous driving scenarios, outperforming existing baselines in accident detection and safe decision-making.", "conclusion": "MMCD provides robust multi-modal collaborative decision-making that maintains performance even when certain data modalities are unavailable during testing."}}
{"id": "2509.18215", "categories": ["cs.AI", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.18215", "abs": "https://arxiv.org/abs/2509.18215", "authors": ["Timotheus Kampik", "Kristijonas \u010cyras", "Jos\u00e9 Ruiz Alarc\u00f3n"], "title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations", "comment": "The publisher's version contains a notation glitch in Example 3, 5th\n  line, first sub-script G should be G'. This has always been G' in authors'\n  version. Thanks to J. Lanser for pointing this out", "summary": "This paper presents a formal approach to explaining change of inference in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions\nfrom a QBAF and updating the QBAF to then again draw conclusions (and so on),\nour approach traces changes -- which we call strength inconsistencies -- in the\npartial order over argument strengths that a semantics establishes on some\narguments of interest, called topic arguments. We trace the causes of strength\ninconsistencies to specific arguments, which then serve as explanations. We\nidentify sufficient, necessary, and counterfactual explanations for strength\ninconsistencies and show that strength inconsistency explanations exist if and\nonly if an update leads to strength inconsistency. We define a heuristic-based\napproach to facilitate the search for strength inconsistency explanations, for\nwhich we also provide an implementation.", "AI": {"tldr": "A formal approach for explaining changes in inference within Quantitative Bipolar Argumentation Frameworks (QBAFs) by tracing strength inconsistencies in partial orders over argument strengths.", "motivation": "To provide explanations for changes in conclusions when QBAFs are updated, specifically focusing on strength inconsistencies in the partial order of argument strengths for topic arguments.", "method": "Define strength inconsistencies, trace their causes to specific arguments, identify sufficient/necessary/counterfactual explanations, develop heuristic-based search approach with implementation.", "result": "Shows that strength inconsistency explanations exist if and only if an update leads to strength inconsistency, and provides an implementation for finding these explanations.", "conclusion": "The approach successfully formalizes explanation of inference changes in QBAFs through strength inconsistency analysis with practical implementation."}}
{"id": "2509.18103", "categories": ["cs.LG", "math.NT"], "pdf": "https://arxiv.org/pdf/2509.18103", "abs": "https://arxiv.org/abs/2509.18103", "authors": ["Jennifer Dodgson", "Michael Joedhitya", "Adith Ramdas", "Surender Suresh Kumar", "Adarsh Singh Chauhan", "Akira Rafhael", "Wang Mingshu", "Nordine Lotfi"], "title": "Machine Learnability as a Measure of Order in Aperiodic Sequences", "comment": null, "summary": "Research on the distribution of prime numbers has revealed a dual character:\ndeterministic in definition yet exhibiting statistical behavior reminiscent of\nrandom processes. In this paper we show that it is possible to use an\nimage-focused machine learning model to measure the comparative regularity of\nprime number fields at specific regions of an Ulam spiral. Specifically, we\ndemonstrate that in pure accuracy terms, models trained on blocks extracted\nfrom regions of the spiral in the vicinity of 500m outperform models trained on\nblocks extracted from the region representing integers lower than 25m. This\nimplies existence of more easily learnable order in the former region than in\nthe latter. Moreover, a detailed breakdown of precision and recall scores seem\nto imply that the model is favouring a different approach to classification in\ndifferent regions of the spiral, focusing more on identifying prime patterns at\nlower numbers and more on eliminating composites at higher numbers. This aligns\nwith number theory conjectures suggesting that at higher orders of magnitude we\nshould see diminishing noise in prime number distributions, with averages\n(density, AP equidistribution) coming to dominate, while local randomness\nregularises after scaling by log x. Taken together, these findings point toward\nan interesting possibility: that machine learning can serve as a new\nexperimental instrument for number theory. Notably, the method shows potential\n1 for investigating the patterns in strong and weak primes for cryptographic\npurposes.", "AI": {"tldr": "Machine learning models can detect comparative regularity in prime number distributions on Ulam spirals, with better performance at higher number regions (500m) than lower regions (25m), suggesting machine learning as a new experimental tool for number theory.", "motivation": "Prime numbers exhibit both deterministic definition and statistical randomness, creating an opportunity to use machine learning as an experimental instrument to measure regularity patterns in prime distributions across different regions of the Ulam spiral.", "method": "Using image-focused machine learning models trained on blocks extracted from different regions of the Ulam spiral (specifically regions around 500m vs below 25m) to classify prime number patterns and analyze precision/recall differences.", "result": "Models trained on higher number regions (500m) outperform those on lower regions (25m), indicating more learnable order at higher magnitudes. Precision/recall analysis shows different classification strategies: focusing on prime pattern identification at lower numbers and composite elimination at higher numbers.", "conclusion": "Machine learning can serve as a new experimental instrument for number theory, potentially useful for investigating prime patterns in cryptography, with findings supporting number theory conjectures about diminishing noise and increasing regularity in prime distributions at higher orders of magnitude."}}
{"id": "2509.18101", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18101", "abs": "https://arxiv.org/abs/2509.18101", "authors": ["Guanzhong Pan", "Haibo Wang"], "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services", "comment": null, "summary": "Large language models (LLMs) are becoming increasingly widespread.\nOrganizations that want to use AI for productivity now face an important\ndecision. They can subscribe to commercial LLM services or deploy models on\ntheir own infrastructure. Cloud services from providers such as OpenAI,\nAnthropic, and Google are attractive because they provide easy access to\nstate-of-the-art models and are easy to scale. However, concerns about data\nprivacy, the difficulty of switching service providers, and long-term operating\ncosts have driven interest in local deployment of open-source models. This\npaper presents a cost-benefit analysis framework to help organizations\ndetermine when on-premise LLM deployment becomes economically viable compared\nto commercial subscription services. We consider the hardware requirements,\noperational expenses, and performance benchmarks of the latest open-source\nmodels, including Qwen, Llama, Mistral, and etc. Then we compare the total cost\nof deploying these models locally with the major cloud providers subscription\nfee. Our findings provide an estimated breakeven point based on usage levels\nand performance needs. These results give organizations a practical framework\nfor planning their LLM strategies.", "AI": {"tldr": "This paper presents a cost-benefit analysis framework to help organizations decide between commercial LLM services and on-premise deployment by comparing costs, performance, and breakeven points.", "motivation": "Organizations face challenges in choosing between commercial LLM services (data privacy concerns, vendor lock-in, long-term costs) and local deployment of open-source models, requiring a systematic decision-making framework.", "method": "The study analyzes hardware requirements, operational expenses, and performance benchmarks of open-source models (Qwen, Llama, Mistral, etc.) and compares total local deployment costs with commercial subscription fees from major cloud providers.", "result": "The research provides estimated breakeven points based on usage levels and performance needs, showing when on-premise deployment becomes economically viable compared to subscription services.", "conclusion": "The framework offers organizations a practical tool for planning their LLM strategies by quantifying the economic trade-offs between cloud services and local deployment."}}
{"id": "2509.18104", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18104", "abs": "https://arxiv.org/abs/2509.18104", "authors": ["Wenqian Li", "Youjia Yang", "Ruoxi Jia", "Yan Pang"], "title": "Data Valuation and Selection in a Federated Model Marketplace", "comment": null, "summary": "In the era of Artificial Intelligence (AI), marketplaces have become\nessential platforms for facilitating the exchange of data products to foster\ndata sharing. Model transactions provide economic solutions in data\nmarketplaces that enhance data reusability and ensure the traceability of data\nownership. To establish trustworthy data marketplaces, Federated Learning (FL)\nhas emerged as a promising paradigm to enable collaborative learning across\nsiloed datasets while safeguarding data privacy. However, effective data\nvaluation and selection from heterogeneous sources in the FL setup remain key\nchallenges. This paper introduces a comprehensive framework centered on a\nWasserstein-based estimator tailored for FL. The estimator not only predicts\nmodel performance across unseen data combinations but also reveals the\ncompatibility between data heterogeneity and FL aggregation algorithms. To\nensure privacy, we propose a distributed method to approximate Wasserstein\ndistance without requiring access to raw data. Furthermore, we demonstrate that\nmodel performance can be reliably extrapolated under the neural scaling law,\nenabling effective data selection without full-scale training. Extensive\nexperiments across diverse scenarios, such as label skew, mislabeled, and\nunlabeled sources, show that our approach consistently identifies\nhigh-performing data combinations, paving the way for more reliable FL-based\nmodel marketplaces.", "AI": {"tldr": "This paper introduces a Wasserstein-based framework for data valuation and selection in Federated Learning marketplaces, enabling model performance prediction and compatibility assessment while preserving privacy.", "motivation": "To address challenges in data valuation and selection from heterogeneous sources in FL setups, which are crucial for establishing trustworthy data marketplaces that enhance data reusability and ensure traceability of data ownership.", "method": "Proposes a comprehensive framework with a Wasserstein-based estimator that predicts model performance across unseen data combinations and reveals data-algorithm compatibility. Includes a distributed method to approximate Wasserstein distance without raw data access, and leverages neural scaling law for performance extrapolation.", "result": "Extensive experiments across diverse scenarios (label skew, mislabeled, and unlabeled sources) show the approach consistently identifies high-performing data combinations without requiring full-scale training.", "conclusion": "The framework paves the way for more reliable FL-based model marketplaces by enabling effective data selection while maintaining privacy and addressing data heterogeneity challenges."}}
{"id": "2509.18123", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18123", "abs": "https://arxiv.org/abs/2509.18123", "authors": ["Yeonju Lee", "Rui Qi Chen", "Joseph Oboamah", "Po Nien Su", "Wei-zhen Liang", "Yeyin Shi", "Lu Gan", "Yongsheng Chen", "Xin Qiao", "Jing Li"], "title": "SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture", "comment": null, "summary": "Accurate interpretation of soil moisture patterns is critical for irrigation\nscheduling and crop management, yet existing approaches for soil moisture\ntime-series analysis either rely on threshold-based rules or data-hungry\nmachine learning or deep learning models that are limited in adaptability and\ninterpretability. In this study, we introduce SPADE (Soil moisture Pattern and\nAnomaly DEtection), an integrated framework that leverages large language\nmodels (LLMs) to jointly detect irrigation patterns and anomalies in soil\nmoisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced\nreasoning and instruction-following capabilities, enabling zero-shot analysis\nwithout requiring task-specific annotation or fine-tuning. By converting\ntime-series data into a textual representation and designing domain-informed\nprompt templates, SPADE identifies irrigation events, estimates net irrigation\ngains, detects, classifies anomalies, and produces structured, interpretable\nreports. Experiments were conducted on real-world soil moisture sensor data\nfrom commercial and experimental farms cultivating multiple crops across the\nUnited States. Results demonstrate that SPADE outperforms the existing method\nin anomaly detection, achieving higher recall and F1 scores and accurately\nclassifying anomaly types. Furthermore, SPADE achieved high precision and\nrecall in detecting irrigation events, indicating its strong capability to\ncapture irrigation patterns accurately. SPADE's reports provide\ninterpretability and usability of soil moisture analytics. This study\nhighlights the potential of LLMs as scalable, adaptable tools for precision\nagriculture, which is capable of integrating qualitative knowledge and\ndata-driven reasoning to produce actionable insights for accurate soil moisture\nmonitoring and improved irrigation scheduling from soil moisture time-series\ndata.", "AI": {"tldr": "SPADE is an LLM-based framework that detects irrigation patterns and anomalies in soil moisture time-series data using ChatGPT-4.1, achieving superior performance without task-specific training.", "motivation": "Existing soil moisture analysis methods rely on threshold-based rules or data-intensive ML/DL models that lack adaptability and interpretability for irrigation scheduling and crop management.", "method": "SPADE converts time-series data to textual representation and uses domain-informed prompt templates with ChatGPT-4.1 for zero-shot analysis of irrigation events, net irrigation gains, and anomaly detection/classification.", "result": "SPADE outperforms existing methods in anomaly detection (higher recall/F1 scores) and achieves high precision/recall in irrigation event detection on real-world data from US farms.", "conclusion": "LLMs like SPADE offer scalable, adaptable tools for precision agriculture by integrating qualitative knowledge with data-driven reasoning for actionable soil moisture insights."}}
{"id": "2509.18105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18105", "abs": "https://arxiv.org/abs/2509.18105", "authors": ["Nachiket N. Naik", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand", "comment": null, "summary": "We study learning of continuous-time inventory dynamics under stochastic\ndemand and quantify when structure helps or hurts forecasting of the bullwhip\neffect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the\nentire right-hand side against a physics-informed Universal Differential\nEquation (UDE) that preserves conservation and order-up-to structure while\nlearning a small residual policy term. Classical supply chain models explain\nthe bullwhip through control/forecasting choices and information sharing, while\nrecent physics-informed and neural differential equation methods blend domain\nconstraints with learned components. It is unclear whether structural bias\nhelps or hinders forecasting under different demand regimes. We address this by\nusing a single-echelon testbed with three demand regimes - AR(1)\n(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done\non varying fractions of each trajectory, followed by evaluation of multi-step\nforecasts for inventory I, order rate O, and demand D. Across the structured\nregimes, UDE consistently generalizes better: with 90% of the training horizon,\ninventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96\nto 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the\nflexibility of NODE is better. These trends persist as train18 ing data\nshrinks, with NODE exhibiting phase drift in extrapolation while UDE remains\nstable but underreacts to rare spikes. Our results provide concrete guidance:\nenforce structure when noise is light-tailed or temporally correlated; relax\nstructure when extreme events dominate. Beyond inventory control, the results\noffer guidance for hybrid modeling in scientific and engineering systems:\nenforce known structure when conservation laws and modest noise dominate, and\nrelax structure to capture extremes in settings where rare events drive\ndynamics.", "AI": {"tldr": "BULL-ODE compares Neural ODE (fully learned) vs Universal Differential Equation (physics-informed) for forecasting bullwhip effect in inventory dynamics, showing UDE performs better in structured demand regimes while NODE excels in heavy-tailed scenarios.", "motivation": "To quantify when structural bias helps or hurts forecasting of bullwhip effect in supply chains under different demand regimes, addressing uncertainty about whether domain constraints improve or hinder forecasting performance.", "method": "Uses single-echelon testbed with three demand regimes (AR(1), i.i.d. Gaussian, heavy-tailed lognormal). Compares Neural ODE (fully learned) against physics-informed Universal Differential Equation that preserves conservation and order-up-to structure while learning residual policy terms.", "result": "UDE generalizes better in structured regimes: inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96 to 0.95 under Gaussian demand. NODE performs better under heavy-tailed lognormal shocks. UDE remains stable but underreacts to rare spikes.", "conclusion": "Enforce structure when noise is light-tailed or temporally correlated; relax structure when extreme events dominate. Provides guidance for hybrid modeling: enforce known structure when conservation laws dominate, relax structure to capture rare events."}}
{"id": "2509.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18132", "abs": "https://arxiv.org/abs/2509.18132", "authors": ["Xiuyi Fan"], "title": "Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI", "comment": "Accepted at the International Joint Conference on Neural Networks,\n  IJCNN 2025", "summary": "Uncertainty is a fundamental challenge in medical practice, but current\nmedical AI systems fail to explicitly quantify or communicate uncertainty in a\nway that aligns with clinical reasoning. Existing XAI works focus on\ninterpreting model predictions but do not capture the confidence or reliability\nof these predictions. Conversely, uncertainty estimation (UE) techniques\nprovide confidence measures but lack intuitive explanations. The disconnect\nbetween these two areas limits AI adoption in medicine. To address this gap, we\npropose Explainable Uncertainty Estimation (XUE) that integrates explainability\nwith uncertainty quantification to enhance trust and usability in medical AI.\nWe systematically map medical uncertainty to AI uncertainty concepts and\nidentify key challenges in implementing XUE. We outline technical directions\nfor advancing XUE, including multimodal uncertainty quantification,\nmodel-agnostic visualization techniques, and uncertainty-aware decision support\nsystems. Lastly, we propose guiding principles to ensure effective XUE\nrealisation. Our analysis highlights the need for AI systems that not only\ngenerate reliable predictions but also articulate confidence levels in a\nclinically meaningful way. This work contributes to the development of\ntrustworthy medical AI by bridging explainability and uncertainty, paving the\nway for AI systems that are aligned with real-world clinical complexities.", "AI": {"tldr": "The paper proposes Explainable Uncertainty Estimation (XUE) to bridge the gap between explainable AI and uncertainty quantification in medical AI systems, aiming to enhance trust and clinical usability.", "motivation": "Current medical AI systems fail to communicate uncertainty in clinically meaningful ways, with XAI focusing on predictions without confidence measures and UE providing confidence but lacking intuitive explanations, limiting AI adoption in medicine.", "method": "The authors systematically map medical uncertainty to AI uncertainty concepts, identify implementation challenges, and outline technical directions including multimodal uncertainty quantification, model-agnostic visualization techniques, and uncertainty-aware decision support systems.", "result": "The analysis highlights the need for AI systems that generate reliable predictions while articulating confidence levels in clinically meaningful ways, providing guiding principles for effective XUE realization.", "conclusion": "This work contributes to trustworthy medical AI by bridging explainability and uncertainty, paving the way for AI systems aligned with real-world clinical complexities through the proposed XUE framework."}}
{"id": "2509.18106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18106", "abs": "https://arxiv.org/abs/2509.18106", "authors": ["Elisa Tomassini", "Enrique Garc\u00eda-Mac\u00edas", "Filippo Ubertini"], "title": "Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks", "comment": null, "summary": "The growing use of permanent monitoring systems has increased data\navailability, offering new opportunities for structural assessment but also\nposing scalability challenges, especially across large bridge networks.\nManaging multiple structures requires tracking and comparing long-term\nbehaviour efficiently. To address this, knowledge transfer between similar\nstructures becomes essential. This study proposes a model-based transfer\nlearning approach using neural network surrogate models, enabling a model\ntrained on one bridge to be adapted to another with similar characteristics.\nThese models capture shared damage mechanisms, supporting a scalable and\ngeneralizable monitoring framework. The method was validated using real data\nfrom two bridges. The transferred model was integrated into a Bayesian\ninference framework for continuous damage assessment based on modal features\nfrom monitoring data. Results showed high sensitivity to damage location,\nseverity, and extent. This approach enhances real-time monitoring and enables\ncross-structure knowledge transfer, promoting smart monitoring strategies and\nimproved resilience at the network level.", "AI": {"tldr": "A model-based transfer learning approach using neural network surrogate models to enable knowledge transfer between similar bridges for scalable structural monitoring.", "motivation": "The growing use of permanent monitoring systems creates scalability challenges for managing large bridge networks, requiring efficient tracking and comparison of long-term behavior across multiple structures.", "method": "Proposes neural network surrogate models that capture shared damage mechanisms, allowing models trained on one bridge to be adapted to similar bridges. Validated using real data from two bridges and integrated into Bayesian inference framework for continuous damage assessment.", "result": "The transferred model showed high sensitivity to damage location, severity, and extent when applied to monitoring data from similar bridges.", "conclusion": "This approach enables cross-structure knowledge transfer, enhances real-time monitoring capabilities, and promotes smart monitoring strategies for improved network-level resilience."}}
{"id": "2509.18168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18168", "abs": "https://arxiv.org/abs/2509.18168", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics", "comment": null, "summary": "Semantic parsing of long documents remains challenging due to quadratic\ngrowth in pairwise composition and memory requirements. We introduce\n\\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that\ndecomposes an input of length $N$ into $M$ meaningful segments, constructs\n\\emph{Local Semantic Graphs} on each segment, and extracts compact\n\\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports\n\\emph{incremental updates} -- only newly arrived segments incur local graph\nconstruction and summary-node integration -- while \\emph{Hierarchical Query\nProcessing} locates relevant segments via top-$K$ retrieval over summary nodes\nand then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to\n$O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive\nFrobenius-norm bounds on the approximation error introduced by node\nsummarization and sparsification thresholds. Empirically, on three benchmarks\n-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),\nand legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference\nspeedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of\nbaseline accuracy. Our approach unlocks scalable, accurate semantic modeling\nfor ultra-long texts, enabling real-time and resource-constrained NLP\napplications.", "AI": {"tldr": "HSGM is a hierarchical framework that decomposes long documents into segments, constructs local semantic graphs, and creates a global graph memory to enable efficient semantic parsing with reduced computational complexity and memory usage.", "motivation": "Semantic parsing of long documents faces challenges due to quadratic growth in computational complexity and memory requirements, making it impractical for ultra-long texts and real-time applications.", "method": "Hierarchical Segment-Graph Memory (HSGM) decomposes input into meaningful segments, builds Local Semantic Graphs on each segment, extracts summary nodes to form a Global Graph Memory, supports incremental updates, and uses Hierarchical Query Processing for efficient retrieval and reasoning.", "result": "HSGM achieves 2-4\u00d7 inference speedup, >60% reduction in peak memory usage, and \u226595% of baseline accuracy on benchmarks including long-document AMR parsing, semantic role labeling, and legal event extraction.", "conclusion": "HSGM enables scalable and accurate semantic modeling for ultra-long texts, making real-time and resource-constrained NLP applications feasible by reducing worst-case complexity from O(N\u00b2) to O(Nk + (N/k)\u00b2)."}}
{"id": "2509.18107", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18107", "abs": "https://arxiv.org/abs/2509.18107", "authors": ["Huanyao Zhang", "Jiaye Lin", "Wentao Zhang", "Haitao Yuan", "Guoliang Li"], "title": "AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting", "comment": null, "summary": "Multivariate time series forecasting involves predicting future values based\non historical observations. However, existing approaches primarily rely on\npredefined single-scale patches or lack effective mechanisms for multi-scale\nfeature fusion. These limitations hinder them from fully capturing the complex\npatterns inherent in time series, leading to constrained performance and\ninsufficient generalizability. To address these challenges, we propose a novel\narchitecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers\n(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both\nGeneral Pre-trained Models (GPM) and Domain-specific Models (DSM) for\nmulti-scale feature extraction. To accommodate the heterogeneity of temporal\nfeatures, AdaMixT incorporates a gating network that dynamically allocates\nweights among different experts, enabling more accurate predictions through\nadaptive multi-scale fusion. Comprehensive experiments on eight widely used\nbenchmarks, including Weather, Traffic, Electricity, ILI, and four ETT\ndatasets, consistently demonstrate the effectiveness of AdaMixT in real-world\nscenarios.", "AI": {"tldr": "AdaMixT is a novel transformer architecture for multivariate time series forecasting that uses adaptive multi-scale feature fusion with expert networks and dynamic gating to improve performance.", "motivation": "Existing time series forecasting approaches rely on predefined single-scale patches or lack effective multi-scale feature fusion mechanisms, limiting their ability to capture complex temporal patterns and achieve good generalizability.", "method": "AdaMixT introduces various patches and leverages both General Pre-trained Models (GPM) and Domain-specific Models (DSM) for multi-scale feature extraction. It incorporates a gating network that dynamically allocates weights among different experts for adaptive multi-scale fusion.", "result": "Comprehensive experiments on eight benchmarks (Weather, Traffic, Electricity, ILI, and four ETT datasets) consistently demonstrate AdaMixT's effectiveness in real-world scenarios.", "conclusion": "AdaMixT addresses the limitations of existing approaches by enabling adaptive multi-scale feature fusion through expert transformers and dynamic gating, leading to improved forecasting performance across diverse real-world datasets."}}
{"id": "2509.18178", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18178", "abs": "https://arxiv.org/abs/2509.18178", "authors": ["Ling Yue", "Nithin Somasekharan", "Tingwen Zhang", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM", "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in\nengineering, yet its steep learning curve and complex manual setup create\nsignificant barriers. To address these challenges, we introduce Foam-Agent, a\nmulti-agent framework that automates the entire end-to-end OpenFOAM workflow\nfrom a single natural language prompt. Our key innovations address critical\ngaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:\nFoam-Agent is the first system to manage the full simulation pipeline,\nincluding advanced pre-processing with a versatile Meshing Agent capable of\nhandling external mesh files and generating new geometries via Gmsh, automatic\ngeneration of HPC submission scripts, and post-simulation visualization via\nParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,\nthe framework uses Model Context Protocol (MCP) to expose its core functions as\ndiscrete, callable tools. This allows for flexible integration and use by other\nagentic systems, such as Claude-code, for more exploratory workflows. 3.\nHigh-Fidelity Configuration Generation: We achieve superior accuracy through a\nHierarchical Multi-Index RAG for precise context retrieval and a\ndependency-aware generation process that ensures configuration consistency.\nEvaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%\nsuccess rate with Claude 3.5 Sonnet, significantly outperforming existing\nframeworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the\nexpertise barrier for CFD, demonstrating how specialized multi-agent systems\ncan democratize complex scientific computing. The code is public at\nhttps://github.com/csml-rpi/Foam-Agent.", "AI": {"tldr": "Foam-Agent is a multi-agent framework that automates the entire OpenFOAM CFD workflow from natural language prompts, achieving 88.2% success rate on benchmark tests.", "motivation": "CFD simulations have a steep learning curve and complex manual setup, creating significant barriers for users. Existing systems lack comprehensive end-to-end automation.", "method": "Uses a multi-agent framework with Model Context Protocol (MCP) for composable services, hierarchical multi-index RAG for context retrieval, and dependency-aware generation for configuration consistency. Handles pre-processing, meshing, HPC script generation, and post-simulation visualization.", "result": "Achieved 88.2% success rate on 110 simulation tasks, significantly outperforming MetaOpenFOAM (55.5%). Successfully automates the full CFD pipeline from natural language input.", "conclusion": "Foam-Agent dramatically lowers the expertise barrier for CFD and demonstrates how specialized multi-agent systems can democratize complex scientific computing."}}
{"id": "2509.18108", "categories": ["cs.LG", "cs.AI", "I.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18108", "abs": "https://arxiv.org/abs/2509.18108", "authors": ["Adam Viktorin", "Tomas Kadavy", "Jozef Kovac", "Michal Pluhacek", "Roman Senkerik"], "title": "Solve it with EASE", "comment": "EASE framework landing paper", "summary": "This paper presents EASE (Effortless Algorithmic Solution Evolution), an\nopen-source and fully modular framework for iterative algorithmic solution\ngeneration leveraging large language models (LLMs). EASE integrates generation,\ntesting, analysis, and evaluation into a reproducible feedback loop, giving\nusers full control over error handling, analysis, and quality assessment. Its\narchitecture supports the orchestration of multiple LLMs in complementary\nroles-such as generator, analyst, and evaluator. By abstracting the complexity\nof prompt design and model management, EASE provides a transparent and\nextensible platform for researchers and practitioners to co-design algorithms\nand other generative solutions across diverse domains.", "AI": {"tldr": "EASE is an open-source modular framework that uses LLMs in a feedback loop for algorithmic solution generation, with roles like generator, analyst, and evaluator.", "motivation": "To provide a transparent and extensible platform that abstracts prompt design and model management complexity, enabling researchers and practitioners to co-design algorithms across domains.", "method": "Integrates generation, testing, analysis, and evaluation into a reproducible feedback loop, orchestrating multiple LLMs in complementary roles.", "result": "EASE offers full user control over error handling, analysis, and quality assessment while supporting iterative algorithmic solution evolution.", "conclusion": "The framework effectively simplifies LLM-based algorithmic development through modular architecture and collaborative model orchestration."}}
{"id": "2509.18180", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18180", "abs": "https://arxiv.org/abs/2509.18180", "authors": ["Yang Wang", "Kai Li"], "title": "Large Language Models and Operations Research: A Structured Survey", "comment": null, "summary": "Operations research (OR) provides fundamental methodologies for complex\nsystem decision-making, with established applications in transportation, supply\nchain management, and production scheduling. Traditional approaches, which\ndepend on expert-based modeling and manual parameter adjustment, often face\nchallenges in handling large-scale, dynamic, and multi-constraint problems.\nRecently, large language models (LLMs) have shown potential to address these\nlimitations through semantic understanding, structured generation, and\nreasoning control. LLMs can translate natural language descriptions into\nmathematical models or executable code, generate heuristics, evolve algorithms,\nand directly tackle optimization tasks. This paper surveys recent progress on\nthe integration of LLMs into OR, organizing methods into three main directions:\nautomatic modeling, auxiliary optimization, and direct solving. It further\nreviews evaluation benchmarks and domain-specific applications, and summarizes\nkey open issues such as unstable semantic-to-structure mapping, fragmented\nresearch progress, limited generalization, and insufficient evaluation systems.\nFinally, the survey outlines possible research avenues for advancing the role\nof LLMs in OR.", "AI": {"tldr": "This paper surveys the integration of large language models (LLMs) into operations research (OR), organizing methods into automatic modeling, auxiliary optimization, and direct solving, and discusses challenges and future research directions.", "motivation": "Traditional OR approaches face challenges in handling large-scale, dynamic, and multi-constraint problems due to reliance on expert-based modeling and manual parameter adjustment. LLMs offer potential solutions through semantic understanding and reasoning capabilities.", "method": "The paper organizes LLM-OR integration methods into three main directions: automatic modeling (translating natural language to mathematical models/code), auxiliary optimization (generating heuristics, evolving algorithms), and direct solving (tackling optimization tasks directly).", "result": "The survey reviews evaluation benchmarks, domain-specific applications, and identifies key challenges including unstable semantic-to-structure mapping, fragmented research progress, limited generalization, and insufficient evaluation systems.", "conclusion": "The paper outlines possible research avenues for advancing LLMs' role in OR, suggesting future directions to address current limitations and enhance the integration of LLMs into operations research methodologies."}}
{"id": "2509.18109", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18109", "abs": "https://arxiv.org/abs/2509.18109", "authors": ["Jonatan Katz Nielsen"], "title": "Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks", "comment": null, "summary": "Accurate recognition of vessel types from Automatic Identification System\n(AIS) tracks is essential for safety oversight and combating illegal,\nunreported, and unregulated (IUU) activity. This paper presents a strait-scale,\nmachine-learning pipeline that classifies moving vessels using only AIS data.\nWe analyze eight days of historical AIS from the Danish Maritime Authority\ncovering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After\nforward/backward filling voyage records, removing kinematic and geospatial\noutliers, and segmenting per-MMSI tracks while excluding stationary periods\n($\\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,\nSOG statistics), temporal, geospatial (Haversine distances, spans), and\nship-shape attributes computed from AIS A/B/C/D reference points (length,\nwidth, aspect ratio, bridge-position ratio). To avoid leakage, we perform\ngrouped train/test splits by MMSI and use stratified 5-fold cross-validation.\nAcross five classes (cargo, tanker, passenger, high-speed craft, fishing;\nN=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest\nwith SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall\n92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches\none-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the\nbridge-position ratio and maximum SOG as the most discriminative signals;\nprincipal errors occur between cargo and tanker, reflecting similar transit\nbehavior. We demonstrate operational value by backfilling missing ship types on\nunseen data and discuss improvements such as DBSCAN based trip segmentation and\ngradient-boosted ensembles to handle frequent-stop ferries and further lift\nperformance. The results show that lightweight features over AIS trajectories\nenable real-time vessel type classification in straits.", "AI": {"tldr": "A machine learning pipeline using AIS data achieves 92.15% accuracy in classifying vessel types (cargo, tanker, passenger, high-speed craft, fishing) through trajectory-level features and tree-based models.", "motivation": "Accurate vessel type recognition from AIS tracks is essential for safety oversight and combating illegal, unreported, and unregulated (IUU) activity.", "method": "The pipeline processes AIS data from the Bornholm Strait, extracts 31 trajectory features (kinematic, temporal, geospatial, ship-shape), uses grouped train/test splits by MMSI with stratified 5-fold cross-validation, and employs Random Forest with SMOTE for classification.", "result": "Random Forest with SMOTE achieved 92.15% accuracy, 94.11% macro-precision, 92.51% macro-recall, and 93.27% macro-F1 on test data. Feature importance identified bridge-position ratio and maximum SOG as most discriminative.", "conclusion": "Lightweight features from AIS trajectories enable real-time vessel type classification in straits, with potential improvements through DBSCAN-based trip segmentation and gradient-boosted ensembles."}}
{"id": "2509.18181", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18181", "abs": "https://arxiv.org/abs/2509.18181", "authors": ["Mustafa Sameen", "Xiaojian Zhang", "Xilei Zhao"], "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling", "comment": null, "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and\nimplementing effective traffic management policies for reducing congestion,\nimproving mobility, and allocating resources more efficiently. Existing models\nfor predicting ridesourcing mode choices often suffer from limited predictive\naccuracy due to their inability to capture key psychological factors, and are\nfurther challenged by severe class imbalance, as ridesourcing trips comprise\nonly a small fraction of individuals' daily travel. To address these\nlimitations, this paper introduces the Synthesizing Attitudes, Predicting\nActions (SAPA) framework, a hierarchical approach that uses Large Language\nModels (LLMs) to synthesize theory-grounded latent attitudes to predict\nridesourcing choices. SAPA first uses an LLM to generate qualitative traveler\npersonas from raw travel survey data and then trains a propensity-score model\non demographic and behavioral features, enriched by those personas, to produce\nan individual-level score. Next, the LLM assigns quantitative scores to\ntheory-driven latent variables (e.g., time and cost sensitivity), and a final\nclassifier integrates the propensity score, latent-variable scores (with their\ninteraction terms), and observable trip attributes to predict ridesourcing mode\nchoice. Experiments on a large-scale, multi-year travel survey show that SAPA\nsignificantly outperforms state-of-the-art baselines, improving ridesourcing\nchoice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.\nThis study provides a powerful tool for accurately predicting ridesourcing mode\nchoices, and provides a methodology that is readily transferable to various\napplications.", "AI": {"tldr": "The paper introduces SAPA framework that uses LLMs to synthesize psychological attitudes from travel data to improve ridesourcing mode choice prediction, achieving 75.9% PR-AUC improvement over baselines.", "motivation": "Existing ridesourcing mode choice models have limited accuracy due to inability to capture psychological factors and severe class imbalance issues, as ridesourcing trips are rare in daily travel.", "method": "SAPA uses a hierarchical approach: LLM generates traveler personas from survey data, trains propensity-score model, assigns quantitative scores to latent variables, and integrates all features in a final classifier.", "result": "Experiments on large-scale multi-year travel survey show SAPA outperforms state-of-the-art baselines by up to 75.9% in PR-AUC on held-out test set.", "conclusion": "SAPA provides an accurate tool for ridesourcing mode choice prediction with methodology transferable to various applications."}}
{"id": "2509.18110", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18110", "abs": "https://arxiv.org/abs/2509.18110", "authors": ["Mrigank Dhingra", "Romit Maulik", "Adil Rasheed", "Omer San"], "title": "Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs", "comment": null, "summary": "Neural operator learning has emerged as a powerful approach for solving\npartial differential equations (PDEs) in a data-driven manner. However,\napplying principal component analysis (PCA) to high-dimensional solution fields\nincurs significant computational overhead. To address this, we propose a\npatch-based PCA-Net framework that decomposes the solution fields into smaller\npatches, applies PCA within each patch, and trains a neural operator in the\nreduced PCA space. We investigate two different patch-based approaches that\nbalance computational efficiency and reconstruction accuracy: (1)\nlocal-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off\nbetween computational cost and accuracy is analyzed, highlighting the\nadvantages and limitations of each approach. Furthermore, within each approach,\nwe explore two refinements for the most computationally efficient method: (i)\nintroducing overlapping patches with a smoothing filter and (ii) employing a\ntwo-step process with a convolutional neural network (CNN) for refinement. Our\nresults demonstrate that patch-based PCA significantly reduces computational\ncomplexity while maintaining high accuracy, reducing end-to-end pipeline\nprocessing time by a factor of 3.7 to 4 times compared to global PCA, thefore\nmaking it a promising technique for efficient operator learning in PDE-based\nsystems.", "AI": {"tldr": "A patch-based PCA-Net framework that decomposes PDE solution fields into smaller patches for more efficient neural operator learning, reducing computational complexity by 3.7-4x while maintaining accuracy.", "motivation": "To address the significant computational overhead of applying principal component analysis (PCA) to high-dimensional PDE solution fields in neural operator learning.", "method": "Proposes two patch-based PCA approaches: (1) local-to-global patch PCA, and (2) local-to-local patch PCA, with refinements including overlapping patches with smoothing filters and CNN-based refinement.", "result": "Patch-based PCA significantly reduces computational complexity while maintaining high accuracy, achieving 3.7-4x reduction in end-to-end pipeline processing time compared to global PCA.", "conclusion": "Patch-based PCA is a promising technique for efficient operator learning in PDE-based systems, offering a good trade-off between computational efficiency and reconstruction accuracy."}}
{"id": "2509.18186", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18186", "abs": "https://arxiv.org/abs/2509.18186", "authors": ["Nursultan Askarbekuly", "Timur Fayzrakhmanov", "Sladjan Babarogi\u0107", "Ivan Lukovi\u0107"], "title": "An Outcome-Based Educational Recommender System", "comment": null, "summary": "Most educational recommender systems are tuned and judged on click- or\nrating-based relevance, leaving their true pedagogical impact unclear. We\nintroduce OBER-an Outcome-Based Educational Recommender that embeds learning\noutcomes and assessment items directly into the data schema, so any algorithm\ncan be evaluated on the mastery it fosters. OBER uses a minimalist\nentity-relation model, a log-driven mastery formula, and a plug-in\narchitecture. Integrated into an e-learning system in non-formal domain, it was\nevaluated trough a two-week randomized split test with over 5 700 learners\nacross three methods: fixed expert trajectory, collaborative filtering (CF),\nand knowledge-based (KB) filtering. CF maximized retention, but the fixed path\nachieved the highest mastery. Because OBER derives business, relevance, and\nlearning metrics from the same logs, it lets practitioners weigh relevance and\nengagement against outcome mastery with no extra testing overhead. The\nframework is method-agnostic and readily extensible to future adaptive or\ncontext-aware recommenders.", "AI": {"tldr": "OBER is an Outcome-Based Educational Recommender that evaluates recommendation algorithms based on learning mastery rather than just clicks or ratings, using a unified framework that combines business, relevance, and learning metrics.", "motivation": "Traditional educational recommender systems focus on click- or rating-based relevance, which doesn't measure true pedagogical impact. There's a need to evaluate recommendations based on actual learning outcomes and mastery.", "method": "OBER uses a minimalist entity-relation model, log-driven mastery formula, and plug-in architecture. It was tested in a two-week randomized split test with over 5,700 learners comparing fixed expert trajectory, collaborative filtering, and knowledge-based filtering methods.", "result": "Collaborative filtering maximized retention, but the fixed expert path achieved the highest mastery. The framework successfully derived business, relevance, and learning metrics from the same logs.", "conclusion": "OBER provides a method-agnostic framework that allows practitioners to balance relevance and engagement against outcome mastery without extra testing overhead, and is extensible to future adaptive or context-aware recommenders."}}
{"id": "2509.18111", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18111", "abs": "https://arxiv.org/abs/2509.18111", "authors": ["Faizul Rakib Sayem", "Shahana Ibrahim"], "title": "Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection", "comment": null, "summary": "The reliability of artificial intelligence (AI) systems in open-world\nsettings depends heavily on their ability to flag out-of-distribution (OOD)\ninputs unseen during training. Recent advances in large-scale vision-language\nmodels (VLMs) have enabled promising few-shot OOD detection frameworks using\nonly a handful of in-distribution (ID) samples. However, existing prompt\nlearning-based OOD methods rely solely on softmax probabilities, overlooking\nthe rich discriminative potential of the feature embeddings learned by VLMs\ntrained on millions of samples. To address this limitation, we propose a novel\ncontext optimization (CoOp)-based framework that integrates subspace\nrepresentation learning with prompt tuning. Our approach improves ID-OOD\nseparability by projecting the ID features into a subspace spanned by prompt\nvectors, while projecting ID-irrelevant features into an orthogonal null space.\nTo train such OOD detection framework, we design an easy-to-handle end-to-end\nlearning criterion that ensures strong OOD detection performance as well as\nhigh ID classification accuracy. Experiments on real-world datasets showcase\nthe effectiveness of our approach.", "AI": {"tldr": "A novel context optimization framework that integrates subspace representation learning with prompt tuning for improved out-of-distribution detection in vision-language models.", "motivation": "Existing prompt learning-based OOD methods rely solely on softmax probabilities, overlooking the rich discriminative potential of feature embeddings learned by VLMs trained on millions of samples.", "method": "Proposes a CoOp-based framework that projects ID features into a subspace spanned by prompt vectors while projecting ID-irrelevant features into an orthogonal null space, using an end-to-end learning criterion.", "result": "Experiments on real-world datasets showcase the effectiveness of the approach.", "conclusion": "The proposed framework improves ID-OOD separability and achieves strong OOD detection performance while maintaining high ID classification accuracy."}}
{"id": "2509.18112", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18112", "abs": "https://arxiv.org/abs/2509.18112", "authors": ["Sheng Wong", "Ravi Shankar", "Beth Albert", "Gabriel Davis Jones"], "title": "Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis", "comment": "Preparing for journal", "summary": "Foundation models (FMs) and large language models (LLMs) demonstrate\nremarkable capabilities across diverse domains through training on massive\ndatasets. These models have demonstrated exceptional performance in healthcare\napplications, yet their potential for electronic fetal monitoring\n(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating\nfetal well-being, remains largely underexplored. Antepartum CTG interpretation\npresents unique challenges due to the complex nature of fetal heart rate (FHR)\npatterns and uterine activity, requiring sophisticated analysis of long\ntime-series data. The assessment of CTG is heavily based on subjective clinical\ninterpretation, often leading to variability in diagnostic accuracy and\ndeviation from timely pregnancy care. This study presents the first\ncomprehensive comparison of state-of-the-art AI approaches for automated\nantepartum CTG analysis. We systematically compare time-series FMs and LLMs\nagainst established CTG-specific architectures. Our evaluation encompasses over\n500 CTG recordings of varying durations reflecting real-world clinical\nrecordings, providing robust performance benchmarks across different modelling\nparadigms. Our results demonstrate that fine-tuned LLMs achieve superior\nperformance compared to both foundation models and domain-specific approaches,\noffering a promising alternative pathway for clinical CTG interpretation. These\nfindings provide critical insights into the relative strengths of different AI\nmethodologies for fetal monitoring applications and establish a foundation for\nfuture clinical AI development in prenatal care.", "AI": {"tldr": "This study presents the first comprehensive comparison of AI approaches for automated antepartum CTG analysis, showing that fine-tuned LLMs outperform both foundation models and domain-specific methods.", "motivation": "Electronic fetal monitoring (CTG) interpretation faces challenges due to subjective clinical assessment and variability in diagnostic accuracy. The potential of foundation models and LLMs for CTG analysis remains underexplored despite their success in other healthcare domains.", "method": "Systematic comparison of time-series foundation models and LLMs against established CTG-specific architectures using over 500 CTG recordings of varying durations reflecting real-world clinical scenarios.", "result": "Fine-tuned LLMs achieved superior performance compared to both foundation models and domain-specific approaches for CTG interpretation.", "conclusion": "Fine-tuned LLMs offer a promising alternative pathway for clinical CTG interpretation, providing critical insights for future AI development in prenatal care."}}
{"id": "2509.18114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18114", "abs": "https://arxiv.org/abs/2509.18114", "authors": ["Javed I. Khan an Henry Uwabor Moye"], "title": "A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU", "comment": "12 pages, Technical Report 2025-07-01, Internetworking and Media\n  Communications Research Laboratories, Department of Computer Science, Kent\n  State University", "summary": "Autoregressive inference in large transformer-based language models (LLMs)\npresents significant challenges for runtime efficiency, particularly during the\ndecode phase where load imbalance across GPU shards can cause throughput\ndegradation and latency spikes. A DPU-assisted framework leveraged by\nBlueField-3 Data Processing Units can enable real-time detection and mitigation\nof load imbalance in multi-node tensor-parallel inference. By offloading\nmonitoring tasks to the DPU and analyzing GPU telemetry and inter-node\ncommunication patterns, the resulting system can provide actionable feedback to\ninference controllers and schedulers. The goal of this study is three-fold i)\nidentify the reported skews/imbalances/pathological conditions that arise in\nmuti-GPU execution of a) LLM tensor computing (both during training and\ninference), b) identify their impact on computational performance, and c) make\na critical assessment if those can be tracked for potential mitigation from a\nDPU network.", "AI": {"tldr": "This paper proposes a DPU-assisted framework using BlueField-3 Data Processing Units to detect and mitigate load imbalance in multi-node tensor-parallel inference of large language models, addressing throughput degradation and latency spikes during autoregressive decode phases.", "motivation": "Autoregressive inference in large transformer-based language models faces runtime efficiency challenges due to load imbalance across GPU shards during decode phase, causing throughput degradation and latency spikes in multi-GPU execution.", "method": "The study leverages BlueField-3 DPUs to offload monitoring tasks, analyze GPU telemetry and inter-node communication patterns, and provide actionable feedback to inference controllers and schedulers for real-time load imbalance detection and mitigation.", "result": "The paper identifies specific skews, imbalances, and pathological conditions that arise in multi-GPU execution of LLM tensor computing during both training and inference, and assesses their impact on computational performance.", "conclusion": "The DPU-assisted framework enables effective tracking and potential mitigation of load imbalance issues from the network level, offering a solution to improve runtime efficiency in large-scale LLM inference deployments."}}
{"id": "2509.18216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18216", "abs": "https://arxiv.org/abs/2509.18216", "authors": ["Amitava Das"], "title": "nDNA -- the Semantic Helix of Artificial Cognition", "comment": null, "summary": "As AI foundation models grow in capability, a deeper question emerges: What\nshapes their internal cognitive identity -- beyond fluency and output?\nBenchmarks measure behavior, but the soul of a model resides in its latent\ngeometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic\nrepresentation that captures this latent identity through the intrinsic\ngeometry of belief. At its core, nDNA is synthesized from three principled and\nindispensable dimensions of latent geometry: spectral curvature, which reveals\nthe curvature of conceptual flow across layers; thermodynamic length, which\nquantifies the semantic effort required to traverse representational\ntransitions through layers; and belief vector field, which delineates the\nsemantic torsion fields that guide a model's belief directional orientations.\nLike biological DNA, it encodes ancestry, mutation, and semantic inheritance,\nfound in finetuning and alignment scars, cultural imprints, and architectural\ndrift. In naming it, we open a new field: Neural Genomics, where models are not\njust tools, but digital semantic organisms with traceable inner cognition.\n  Modeling statement. We read AI foundation models as semantic fluid--dynamics:\nmeaning is transported through layers like fluid in a shaped conduit; nDNA is\nthe physics-grade readout of that flow -- a geometry-first measure of how\nmeaning is bent, paid for, and pushed -- yielding a stable, coordinate-free\nneural DNA fingerprint tied to on-input behavior; with this fingerprint we\ncross into biology: tracing lineages across pretraining, fine-tuning,\nalignment, pruning, distillation, and merges; measuring inheritance between\ncheckpoints; detecting drift as traits shift under new data or objectives; and,\nultimately, studying the evolution of artificial cognition to compare models,\ndiagnose risks, and govern change over time.", "AI": {"tldr": "Neural DNA (nDNA) is proposed as a semantic-genotypic representation that captures a model's latent cognitive identity through three geometric dimensions: spectral curvature, thermodynamic length, and belief vector field, enabling lineage tracing and cognitive evolution analysis.", "motivation": "Current benchmarks measure model behavior but fail to capture the internal cognitive identity and latent geometry that defines a model's 'soul'. There's a need to understand how meaning flows through layers and how models evolve semantically.", "method": "nDNA synthesizes three geometric dimensions: spectral curvature (conceptual flow curvature across layers), thermodynamic length (semantic effort for representational transitions), and belief vector field (semantic torsion fields guiding belief orientations). This creates a coordinate-free fingerprint for analyzing model lineages.", "result": "The approach enables tracing model lineages across pretraining, fine-tuning, alignment, pruning, distillation, and merges; measuring inheritance between checkpoints; detecting drift under new data/objectives; and studying artificial cognition evolution.", "conclusion": "nDNA opens the field of Neural Genomics, treating AI models as digital semantic organisms with traceable inner cognition, enabling model comparison, risk diagnosis, and governance of cognitive evolution over time."}}
{"id": "2509.18115", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18115", "abs": "https://arxiv.org/abs/2509.18115", "authors": ["Hongyi Chen", "Xiucheng Li", "Xinyang Chen", "Jing Li", "Kehai Chen", "Liqiang Nie"], "title": "Towards Scalable and Structured Spatiotemporal Forecasting", "comment": null, "summary": "In this paper, we propose a novel Spatial Balance Attention block for\nspatiotemporal forecasting. To strike a balance between obeying spatial\nproximity and capturing global correlation, we partition the spatial graph into\na set of subgraphs and instantiate Intra-subgraph Attention to learn local\nspatial correlation within each subgraph; to capture the global spatial\ncorrelation, we further aggregate the nodes to produce subgraph representations\nand achieve message passing among the subgraphs via Inter-subgraph Attention.\nBuilding on the proposed Spatial Balance Attention block, we develop a\nmultiscale spatiotemporal forecasting model by progressively increasing the\nsubgraph scales. The resulting model is both scalable and able to produce\nstructured spatial correlation, and meanwhile, it is easy to implement. We\nevaluate its efficacy and efficiency against the existing models on real-world\nspatiotemporal datasets from medium to large sizes. The experimental results\nshow that it can achieve performance improvements up to 7.7% over the baseline\nmethods at low running costs.", "AI": {"tldr": "Proposes a Spatial Balance Attention block for spatiotemporal forecasting that balances local spatial proximity and global correlation through intra-subgraph and inter-subgraph attention mechanisms.", "motivation": "To address the challenge of balancing spatial proximity constraints with the need to capture global spatial correlations in spatiotemporal forecasting tasks.", "method": "Partitions spatial graphs into subgraphs, uses Intra-subgraph Attention for local spatial correlation within subgraphs, and Inter-subgraph Attention for global correlation between subgraphs. Builds a multiscale model by progressively increasing subgraph scales.", "result": "Achieves performance improvements up to 7.7% over baseline methods with low running costs on real-world spatiotemporal datasets from medium to large sizes.", "conclusion": "The proposed model is scalable, produces structured spatial correlation, easy to implement, and demonstrates superior efficacy and efficiency compared to existing models."}}
{"id": "2509.18218", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18218", "abs": "https://arxiv.org/abs/2509.18218", "authors": ["Kei-Sing Ng"], "title": "Similarity Field Theory: A Mathematical Framework for Intelligence", "comment": null, "summary": "We posit that persisting and transforming similarity relations form the\nstructural basis of any comprehensible dynamic system. This paper introduces\nSimilarity Field Theory, a mathematical framework that formalizes the\nprinciples governing similarity values among entities and their evolution. We\ndefine: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of\nentities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed\nrelational field (asymmetry and non-transitivity are allowed); (2) the\nevolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by\n$p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers\n$F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of\nthe unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that\nproduces new entities. Within this framework, we formalize a generative\ndefinition of intelligence: an operator $G$ is intelligent with respect to a\nconcept $K$ if, given a system containing entities belonging to the fiber of\n$K$, it generates new entities that also belong to that fiber. Similarity Field\nTheory thus offers a foundational language for characterizing, comparing, and\nconstructing intelligent systems. We prove two theorems: (i) asymmetry blocks\nmutual inclusion; and (ii) stability requires either an anchor coordinate or\neventual confinement within a level set of $f$. These results ensure that the\nevolution of similarity fields is both constrained and interpretable,\nculminating in an exploration of how the framework allows us to interpret large\nlanguage models and use them as experimental probes into societal cognition.", "AI": {"tldr": "Similarity Field Theory provides a mathematical framework for analyzing similarity relations and their evolution, formalizing intelligence as the ability to generate entities that maintain similarity to concepts.", "motivation": "To establish a foundational mathematical framework for understanding similarity relations in dynamic systems and formalize a generative definition of intelligence.", "method": "Defines similarity fields over entities, system evolution sequences, concepts as fibers, and generative operators, with theorems on asymmetry and stability constraints.", "result": "Proves two key theorems: (i) asymmetry blocks mutual inclusion, and (ii) stability requires anchor coordinates or confinement within level sets, ensuring constrained evolution.", "conclusion": "The framework offers a language for characterizing intelligent systems and can interpret large language models as probes into societal cognition."}}
{"id": "2509.18116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18116", "abs": "https://arxiv.org/abs/2509.18116", "authors": ["Nathan Egbuna", "Saatvik Gaur", "Sunishchal Dev", "Ashwinee Panda", "Maheep Chaudhary"], "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization", "comment": null, "summary": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs\\textemdash techniques like iterative refinement and multi-step\nverification can require $10$--$100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative\nmethods while matching or surpassing greedy Chain-of-Thought (CoT) and\nSelf-Consistency baselines, yielding up to 101\\% improvement in\nefficiency--accuracy trade-off. These results show that much of latent\noptimization's benefit can be captured offline, making sophisticated reasoning\ntechniques viable for production deployment. Code is available\nat~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}", "AI": {"tldr": "Amortized Latent Steering (ALS) is a method that replaces expensive iterative test-time optimization with a single pre-computed steering vector, achieving 2-5x speedup while maintaining or improving accuracy on reasoning benchmarks.", "motivation": "Test-time optimization methods are computationally expensive and impractical for production deployment due to high inference costs from iterative refinement and multiple backward passes.", "method": "ALS computes the mean difference between hidden states from successful vs unsuccessful generations offline, then applies this single steering vector during inference to nudge activations back toward the success manifold when decoding drifts.", "result": "Across GSM8K and MATH-500 benchmarks, ALS achieves 2-5x speedup over iterative methods while matching or surpassing greedy Chain-of-Thought and Self-Consistency baselines, with up to 101% improvement in efficiency-accuracy trade-off.", "conclusion": "Much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment by eliminating expensive per-query optimization loops."}}
{"id": "2509.18221", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18221", "abs": "https://arxiv.org/abs/2509.18221", "authors": ["Dingxin Lu", "Shurui Wu", "Xinyi Huang"], "title": "Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models", "comment": null, "summary": "With the rising global burden of chronic diseases and the multimodal and\nheterogeneous clinical data (medical imaging, free-text recordings, wearable\nsensor streams, etc.), there is an urgent need for a unified multimodal AI\nframework that can proactively predict individual health risks. We propose\nVL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer\nwith a large language model (LLM) inference head embedded in its top layer. The\nsystem builds on the dual-stream architecture of existing visual-linguistic\nmodels (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with\ncross-modal comparison and fine-grained alignment of radiological images,\nfundus maps, and wearable device photos with corresponding clinical narratives\nusing momentum update encoders and debiased InfoNCE losses; (ii) a time fusion\nblock that integrates irregular visit sequences into the causal Transformer\ndecoder through adaptive time interval position coding; (iii) a disease\nontology map adapter that injects ICD-10 codes into visual and textual channels\nin layers and infers comorbid patterns with the help of a graph attention\nmechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an\naverage AUROC of 0.90 with an expected calibration error of 2.7 percent.", "AI": {"tldr": "VL-RiskFormer is a hierarchical multimodal Transformer with LLM inference head that integrates medical imaging, clinical narratives, and wearable data for proactive health risk prediction, achieving 0.90 AUROC on MIMIC-IV.", "motivation": "Address the rising global burden of chronic diseases and the need for unified multimodal AI frameworks to handle heterogeneous clinical data (medical imaging, free-text recordings, wearable sensor streams) for proactive individual health risk prediction.", "method": "Hierarchical stacked visual-language multimodal Transformer with LLM inference head, featuring: (i) cross-modal pre-training with momentum update encoders and debiased InfoNCE losses, (ii) time fusion block with adaptive time interval position coding for irregular visit sequences, (iii) disease ontology map adapter injecting ICD-10 codes with graph attention mechanism.", "result": "Achieved average AUROC of 0.90 with expected calibration error of 2.7% on MIMIC-IV longitudinal cohort.", "conclusion": "VL-RiskFormer demonstrates effective multimodal integration for health risk prediction, showing strong performance in handling diverse clinical data types and temporal patterns."}}
{"id": "2509.18117", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18117", "abs": "https://arxiv.org/abs/2509.18117", "authors": ["Eric Petit", "Denis Ch\u00eane"], "title": "Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs", "comment": "soumis {\\`a} la conf{\\'e}rence IHM 2025", "summary": "The paper presents a machine learning approach to design digital interfaces\nthat can dynamically adapt to different users and usage strategies. The\nalgorithm uses Bayesian statistics to model users' browsing behavior, focusing\non their habits rather than group preferences. It is distinguished by its\nonline incremental learning, allowing reliable predictions even with little\ndata and in the case of a changing environment. This inference method generates\na task model, providing a graphical representation of navigation with the usage\nstatistics of the current user. The algorithm learns new tasks while preserving\nprior knowledge. The theoretical framework is described, and simulations show\nthe effectiveness of the approach in stationary and non-stationary\nenvironments. In conclusion, this research paves the way for adaptive systems\nthat improve the user experience by helping them to better navigate and act on\ntheir interface.", "AI": {"tldr": "Machine learning approach using Bayesian statistics to create adaptive digital interfaces that model individual user browsing behavior through online incremental learning", "motivation": "To design interfaces that dynamically adapt to individual users' habits rather than group preferences, improving user experience by helping them navigate interfaces more effectively", "method": "Bayesian statistical modeling of users' browsing behavior with online incremental learning that preserves prior knowledge while learning new tasks, generating graphical task models with usage statistics", "result": "Simulations demonstrate the approach's effectiveness in both stationary and non-stationary environments, showing reliable predictions even with limited data", "conclusion": "This research enables adaptive systems that enhance user experience by providing personalized interface navigation support based on individual usage patterns"}}
{"id": "2509.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18226", "abs": "https://arxiv.org/abs/2509.18226", "authors": ["Yu Fu", "Linyue Cai", "Ruoyu Wu", "Yong Zhao"], "title": "From \"What to Eat?\" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation", "comment": "5 pages, 3 figures, submitted to icassp 2026", "summary": "Personalized recipe recommendation faces challenges in handling fuzzy user\nintent, ensuring semantic accuracy, and providing sufficient detail coverage.\nWe propose ChefMind, a hybrid architecture combining Chain of Exploration\n(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large\nLanguage Model (LLM). CoE refines ambiguous queries into structured conditions,\nKG offers semantic reasoning and interpretability, RAG supplements contextual\nculinary details, and LLM integrates outputs into coherent recommendations. We\nevaluate ChefMind on the Xiachufang dataset and manually annotated queries,\ncomparing it with LLM-only, KG-only, and RAG-only baselines. Results show that\nChefMind achieves superior performance in accuracy, relevance, completeness,\nand clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.\nMoreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in\nhandling fuzzy demands.", "AI": {"tldr": "ChefMind is a hybrid architecture combining Chain of Exploration, Knowledge Graph, Retrieval-Augmented Generation, and LLM to address challenges in personalized recipe recommendation, achieving superior performance over baseline models.", "motivation": "To overcome challenges in personalized recipe recommendation including handling fuzzy user intent, ensuring semantic accuracy, and providing sufficient detail coverage.", "method": "Proposes ChefMind - a hybrid architecture combining Chain of Exploration (CoE) for refining ambiguous queries, Knowledge Graph (KG) for semantic reasoning, Retrieval-Augmented Generation (RAG) for contextual details, and LLM for integrating outputs into coherent recommendations.", "result": "Achieves average score of 8.7 vs 6.4-6.7 for ablation models (LLM-only, KG-only, RAG-only), reduces unprocessed queries to 1.6%, and demonstrates superior performance in accuracy, relevance, completeness, and clarity on Xiachufang dataset.", "conclusion": "ChefMind effectively handles fuzzy user demands and provides robust, high-quality recipe recommendations through its integrated hybrid approach."}}
{"id": "2509.18118", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.18118", "abs": "https://arxiv.org/abs/2509.18118", "authors": ["Marcelo Ribeiro", "Diogo Costa", "Gon\u00e7alo Moreira", "Sandro Pinto", "Tiago Gomes"], "title": "Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices", "comment": null, "summary": "Modern IoT devices increasingly rely on machine learning solutions to process\ndata locally. However, the lack of graphics processing units (GPUs) or\ndedicated accelerators on most platforms makes on-device training largely\ninfeasible, often requiring cloud-based services to perform this task. This\nprocedure often raises privacy-related concerns, and creates dependency on\nreliable and always-on connectivity. Federated Learning (FL) is a new trend\nthat addresses these issues by enabling decentralized and collaborative\ntraining directly on devices, but it requires highly efficient optimization\nalgorithms. L-SGD, a lightweight variant of stochastic gradient descent, has\nenabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).\nThis work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture\nthat still lacks robust support for on-device training. L-SGD was evaluated on\nboth Arm and RISC-V platforms using 32-bit floating-point arithmetic,\nhighlighting the performance impact of the absence of Floating-Point Units\n(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit\nquantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in\nmemory usage and a 2.2x speedup in training time, with negligible accuracy\ndegradation.", "AI": {"tldr": "This paper extends L-SGD (a lightweight SGD variant) from Arm Cortex-M to RISC-V MCUs for on-device training, addressing the performance gap due to lack of FPUs in RISC-V by introducing an 8-bit quantized version that reduces memory usage by 4x and speeds up training by 2.2x with minimal accuracy loss.", "motivation": "Enable on-device training for IoT devices using RISC-V MCUs, which lack GPU/accelerator support and have privacy/connectivity concerns with cloud-based training. Federated Learning requires efficient algorithms, but RISC-V lacks robust on-device training support.", "method": "Extended L-SGD to RISC-V MCUs, evaluated with 32-bit floating-point arithmetic, then introduced an 8-bit quantized version of L-SGD specifically for RISC-V to overcome FPU limitations.", "result": "The 8-bit quantized L-SGD achieved nearly 4x reduction in memory usage and 2.2x speedup in training time compared to 32-bit version, with negligible accuracy degradation.", "conclusion": "Quantized L-SGD enables efficient on-device training on RISC-V MCUs, making them viable for Federated Learning applications despite hardware limitations, with significant performance improvements over standard floating-point implementations."}}
{"id": "2509.18229", "categories": ["cs.AI", "70, 74, 76, 80"], "pdf": "https://arxiv.org/pdf/2509.18229", "abs": "https://arxiv.org/abs/2509.18229", "authors": ["Anthony Patera", "Rohan Abeyaratne"], "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems", "comment": null, "summary": "Generative AI, and specifically GPT, can produce a remarkable solution to a\nmechanical engineering analysis problem - but also, on occasion, a flawed\nsolution. For example, an elementary mechanics problem is solved flawlessly in\none GPT instance and incorrectly in a subsequent GPT instance, with a success\nprobability of only 85%. This unreliability renders \"out-of-the-box\" GPT\nunsuitable for deployment in education or engineering practice. We introduce an\n\"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering\nProblem Statements. Agency first launches N instantiations of Agent Solve to\nyield N independent Proposed Problem Solution Realizations; Agency then invokes\nAgent Compare to summarize and compare the N Proposed Problem Solution\nRealizations and to provide a Recommended Problem Solution. We argue from\nCondorcet's Jury Theorem that, for a Problem Statement characterized by\nper-Solve success probability greater than 1/2 (and N sufficiently large), the\nPredominant (Agent Compare) Proposed Problem Solution will, with high\nprobability, correspond to a Correct Proposed Problem Solution. Furthermore,\nAgent Compare can also incorporate aspects of Secondary (Agent Compare)\nProposed Problem Solutions, in particular when the latter represent alternative\nProblem Statement interpretations - different Mathematical Models - or\nalternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a\ncommercial multi-agent model, show similarities in design and performance, but\nalso important differences in emphasis: our Agency focuses on transparency and\npedagogical value.", "AI": {"tldr": "The paper introduces an \"N-Plus-1\" GPT Agency that uses multiple AI agents to improve reliability in mechanical engineering problem solving by comparing multiple solution attempts and selecting the most probable correct answer.", "motivation": "GPT models show unreliability in mechanical engineering analysis with only 85% success probability, making them unsuitable for education and engineering practice without additional safeguards.", "method": "An agency launches N independent Agent Solve instances to generate multiple solution realizations, then uses Agent Compare to summarize, compare, and recommend the best solution based on Condorcet's Jury Theorem principles.", "result": "The approach significantly improves reliability over single GPT instances, with comparisons to Grok Heavy showing similar performance but with greater focus on transparency and pedagogical value.", "conclusion": "The N-Plus-1 agency framework provides a reliable method for deploying GPT in mechanical engineering applications by leveraging multiple independent solutions and comparative analysis to overcome individual instance unreliability."}}
{"id": "2509.18119", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18119", "abs": "https://arxiv.org/abs/2509.18119", "authors": ["Yifan Xu", "Xiao Liu", "Xinghan Liu", "Jiaqi Fu", "Hanchen Zhang", "Bohao Jing", "Shudan Zhang", "Yuting Wang", "Wenyi Zhao", "Yuxiao Dong"], "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents", "comment": null, "summary": "Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MOBILERL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted\nin the AutoGLM products, and also open-sourced at\nhttps://github.com/THUDM/MobileRL.", "AI": {"tldr": "MOBILERL is an online reinforcement learning framework for mobile GUI agents that addresses challenges in RL training through difficulty-adaptive strategies and reward adjustments, achieving state-of-the-art performance on mobile task benchmarks.", "motivation": "Developing effective mobile GUI agents with RL is challenging due to heavy-tailed task difficulty distributions and inefficient large-scale environment sampling. The paper aims to overcome these limitations to create more robust mobile agents.", "method": "The framework uses ADAGRPO algorithm with difficulty-adaptive positive replay, failure curriculum filtering, and shortest path reward adjustment. These strategies adapt to task difficulties and stabilize RL training while improving sample efficiency.", "result": "MOBILERL applied to Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base models achieves state-of-the-art success rates: 75.8% on AndroidWorld and 46.8% on AndroidLab benchmarks.", "conclusion": "MOBILERL successfully enhances GUI agents for mobile environments through adaptive RL strategies, demonstrating strong performance across diverse mobile apps and tasks, and has been adopted in AutoGLM products."}}
{"id": "2509.18230", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18230", "abs": "https://arxiv.org/abs/2509.18230", "authors": ["Zihan Dong", "Xinyu Fan", "Zixiang Tang", "Yunqing Li"], "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces", "comment": null, "summary": "Controlling desktop applications via software remains a fundamental yet\nunder-served problem. Existing multi-modal large language models (MLLMs) ingest\nscreenshots and task instructions to generate keystrokes and mouse events, but\nthey suffer from prohibitive inference latency, poor sample efficiency on\nlong-horizon sparse-reward tasks, and infeasible on-device deployment. We\nintroduce a lightweight hierarchical reinforcement learning framework,\nComputerAgent, that formulates OS control as a two-level option process\n(manager and subpolicy), employs a triple-modal state encoder (screenshot, task\nID, numeric state) to handle visual and contextual diversity, integrates\nmeta-actions with an early-stop mechanism to reduce wasted interactions, and\nuses a compact vision backbone plus small policy networks for on-device\ninference (15M parameters). On a suite of 135 real-world desktop tasks,\nComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on\nhard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on\nsimple scenarios while reducing model size by over four orders of magnitude and\nhalving inference time. These results demonstrate that hierarchical RL offers a\npractical, scalable alternative to monolithic MLLM-based automation for\ncomputer control.", "AI": {"tldr": "ComputerAgent is a lightweight hierarchical RL framework for desktop automation that achieves comparable performance to large MLLMs while being 4 orders of magnitude smaller and twice as fast.", "motivation": "Existing MLLM-based desktop automation suffers from high latency, poor efficiency on long tasks, and inability to run on-device due to large model sizes.", "method": "Two-level hierarchical RL with manager-subpolicy options, triple-modal state encoder (screenshot, task ID, numeric state), meta-actions with early-stop mechanism, and compact vision backbone with small policy networks (15M parameters).", "result": "92.1% success on simple tasks (<8 steps) and 58.8% on hard tasks (\u22658 steps), matching/exceeding 200B-parameter MLLMs while reducing model size by 10,000x and halving inference time.", "conclusion": "Hierarchical RL provides a practical, scalable alternative to monolithic MLLM-based automation for computer control tasks."}}
{"id": "2509.18120", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.18120", "abs": "https://arxiv.org/abs/2509.18120", "authors": ["Thanh Linh Nguyen", "Quoc-Viet Pham"], "title": "A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning", "comment": "Accepted in IEEE GLOBECOM 2025", "summary": "Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or\nbanks) to collaboratively train artificial intelligence (AI) models while\npreserving data privacy by keeping data local. While prior work has primarily\naddressed statistical heterogeneity across organizations, a critical challenge\narises from economic competition, where organizations may act as market rivals,\nmaking them hesitant to participate in joint training due to potential utility\nloss (i.e., reduced net benefit). Furthermore, the combined effects of\nstatistical heterogeneity and inter-organizational competition on\norganizational behavior and system-wide social welfare remain underexplored. In\nthis paper, we propose CoCoGen, a coopetitive-compatible data generation\nframework, leveraging generative AI (GenAI) and potential game theory to model,\nanalyze, and optimize collaborative learning under heterogeneous and\ncompetitive settings. Specifically, CoCoGen characterizes competition and\nstatistical heterogeneity through learning performance and utility-based\nformulations and models each training round as a weighted potential game. We\nthen derive GenAI-based data generation strategies that maximize social\nwelfare. Experimental results on the Fashion-MNIST dataset reveal how varying\nheterogeneity and competition levels affect organizational behavior and\ndemonstrate that CoCoGen consistently outperforms baseline methods.", "AI": {"tldr": "CoCoGen is a framework that addresses economic competition and statistical heterogeneity in cross-silo federated learning using generative AI and game theory to optimize collaborative training while maximizing social welfare.", "motivation": "Organizations in cross-silo federated learning face economic competition as market rivals, making them hesitant to participate due to potential utility loss. The combined effects of statistical heterogeneity and inter-organizational competition on organizational behavior and social welfare are underexplored.", "method": "CoCoGen uses generative AI and potential game theory to model competition and statistical heterogeneity through learning performance and utility-based formulations. It models each training round as a weighted potential game and derives GenAI-based data generation strategies.", "result": "Experiments on Fashion-MNIST show how varying heterogeneity and competition levels affect organizational behavior, and demonstrate that CoCoGen consistently outperforms baseline methods.", "conclusion": "CoCoGen effectively addresses the challenges of economic competition and statistical heterogeneity in federated learning, providing optimized collaborative training that maximizes social welfare through generative AI and game-theoretic approaches."}}
{"id": "2509.18234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18234", "abs": "https://arxiv.org/abs/2509.18234", "authors": ["Yu Gu", "Jingjing Fu", "Xiaodong Liu", "Jeya Maria Jose Valanarasu", "Noel Codella", "Reuben Tan", "Qianchu Liu", "Ying Jin", "Sheng Zhang", "Jinyu Wang", "Rui Wang", "Lei Song", "Guanghui Qin", "Naoto Usuyama", "Cliff Wong", "Cheng Hao", "Hohin Lee", "Praneeth Sanapathi", "Sarah Hilado", "Bian Jiang", "Javier Alvarez-Valle", "Mu Wei", "Jianfeng Gao", "Eric Horvitz", "Matt Lungren", "Hoifung Poon", "Paul Vozila"], "title": "The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks", "comment": "35 pages", "summary": "Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands.", "AI": {"tldr": "Current medical AI benchmarks are misleading - top models achieve high scores through test-taking tricks rather than genuine medical understanding, exposing brittleness and shortcut learning that doesn't reflect real-world readiness.", "motivation": "To expose how medical AI benchmarks reward superficial test-taking strategies over true medical competence, and to demonstrate that high leaderboard scores mask fundamental weaknesses in model robustness and reasoning.", "method": "Evaluated six flagship models across six widely used medical benchmarks using stress tests (removing key inputs, testing prompt sensitivity), analyzing answer flipping patterns, and conducting clinician-guided rubric evaluation to assess what benchmarks truly measure.", "result": "Leading systems often guess correctly even when critical inputs are removed, flip answers under trivial prompt changes, fabricate convincing but flawed reasoning, and benchmarks vary widely in what they actually measure while being treated interchangeably.", "conclusion": "Medical benchmark scores do not reflect real-world readiness; to earn trust in healthcare, AI systems must be held accountable for robustness, sound reasoning, and alignment with real medical demands beyond just leaderboard performance."}}
{"id": "2509.18124", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.18124", "abs": "https://arxiv.org/abs/2509.18124", "authors": ["Edmund Agyemang", "Lawrence Agbota", "Vincent Agbenyeavu", "Peggy Akabuah", "Bismark Bimpong", "Christopher Attafuah"], "title": "Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters", "comment": "13 pages, 6 figures and 4 tables", "summary": "This study explores the application of supervised machine learning algorithms\nto predict coffee ratings based on a combination of influential textual and\nnumerical attributes extracted from user reviews. Through careful data\npreprocessing including text cleaning, feature extraction using TF-IDF, and\nselection with SelectKBest, the study identifies key factors contributing to\ncoffee quality assessments. Six models (Decision Tree, KNearest Neighbors,\nMulti-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained\nand evaluated using optimized hyperparameters. Model performance was assessed\nprimarily using F1-score, Gmean, and AUC metrics. Results demonstrate that\nensemble methods (Extra Trees, Random Forest, and XGBoost), as well as\nMulti-layer Perceptron, consistently outperform simpler classifiers (Decision\nTrees and K-Nearest Neighbors) in terms of evaluation metrics such as F1\nscores, G-mean and AUC. The findings highlight the essence of rigorous feature\nselection and hyperparameter tuning in building robust predictive systems for\nsensory product evaluation, offering a data driven approach to complement\ntraditional coffee cupping by expertise of trained professionals.", "AI": {"tldr": "Supervised machine learning applied to predict coffee ratings using text and numerical features from user reviews, with ensemble methods showing best performance.", "motivation": "To develop a data-driven approach for coffee quality assessment that complements traditional expert cupping by leveraging user reviews and machine learning.", "method": "Used text preprocessing, TF-IDF feature extraction, SelectKBest feature selection, and trained six ML models (Decision Tree, KNN, MLP, Random Forest, Extra Trees, XGBoost) with hyperparameter optimization.", "result": "Ensemble methods (Extra Trees, Random Forest, XGBoost) and Multi-layer Perceptron outperformed simpler classifiers (Decision Trees, KNN) on F1-score, G-mean, and AUC metrics.", "conclusion": "Rigorous feature selection and hyperparameter tuning are essential for building robust predictive systems for sensory product evaluation, offering a complementary approach to traditional expert assessment."}}
{"id": "2509.18382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18382", "abs": "https://arxiv.org/abs/2509.18382", "authors": ["Adarsha Balaji", "Le Chen", "Rajeev Thakur", "Franck Cappello", "Sandeep Madireddy"], "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints", "comment": null, "summary": "Test-time compute scaling has demonstrated the ability to improve the\nperformance of reasoning language models by generating longer chain-of-thought\n(CoT) sequences. However, this increase in performance comes with a significant\nincrease in computational cost. In this work, we investigate two compute\nconstraint strategies: (1) reasoning length constraint and (2) model\nquantization, as methods to reduce the compute demand of reasoning models and\nstudy their impact on their safety performance. Specifically, we explore two\napproaches to apply compute constraints to reasoning models: (1) fine-tuning\nreasoning models using a length controlled policy optimization (LCPO) based\nreinforcement learning method to satisfy a user-defined CoT reasoning length,\nand (2) applying quantization to maximize the generation of CoT sequences\nwithin a user-defined compute constraint. Furthermore, we study the trade-off\nbetween the computational efficiency and the safety of the model.", "AI": {"tldr": "This paper investigates compute constraint strategies (reasoning length constraint and model quantization) to reduce computational costs of reasoning language models while studying their impact on safety performance.", "motivation": "Test-time compute scaling improves reasoning model performance but significantly increases computational cost. The research aims to find methods to reduce compute demand while maintaining safety.", "method": "Two approaches: (1) fine-tuning reasoning models using length controlled policy optimization (LCPO) reinforcement learning to satisfy user-defined CoT reasoning length, (2) applying quantization to maximize CoT generation within user-defined compute constraints.", "result": "The paper studies the trade-off between computational efficiency and model safety when applying these compute constraint strategies.", "conclusion": "Compute constraints can be effectively applied through reasoning length control and quantization to balance performance with computational efficiency, while safety implications need careful consideration."}}
{"id": "2509.18125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18125", "abs": "https://arxiv.org/abs/2509.18125", "authors": ["Harsha Koduri"], "title": "NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment", "comment": null, "summary": "Healthcare systems face increasing pressure to allocate limited nursing\nresources efficiently while accounting for skill heterogeneity, patient acuity,\nstaff fatigue, and continuity of care. Traditional optimization and heuristic\nscheduling methods struggle to capture these dynamic, multi-constraint\nenvironments. I propose NurseSchedRL, a reinforcement learning framework for\nnurse-patient assignment that integrates structured state encoding, constrained\naction masking, and attention-based representations of skills, fatigue, and\ngeographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with\nfeasibility masks to ensure assignments respect real-world constraints, while\ndynamically adapting to patient arrivals and varying nurse availability. In\nsimulation with realistic nurse and patient data, NurseSchedRL achieves\nimproved scheduling efficiency, better alignment of skills to patient needs,\nand reduced fatigue compared to baseline heuristic and unconstrained RL\napproaches. These results highlight the potential of reinforcement learning for\ndecision support in complex, high-stakes healthcare workforce management.", "AI": {"tldr": "NurseSchedRL is a reinforcement learning framework for nurse-patient assignment that addresses skill heterogeneity, patient acuity, staff fatigue, and continuity of care constraints more effectively than traditional methods.", "motivation": "Healthcare systems need efficient nursing resource allocation that accounts for multiple dynamic constraints, but traditional optimization and heuristic scheduling methods struggle with these complex environments.", "method": "Uses Proximal Policy Optimization (PPO) with constrained action masking, structured state encoding, and attention-based representations of skills, fatigue, and geographical context to ensure assignments respect real-world constraints.", "result": "In simulations with realistic data, NurseSchedRL achieves improved scheduling efficiency, better skill-patient alignment, and reduced fatigue compared to baseline heuristic and unconstrained RL approaches.", "conclusion": "Reinforcement learning shows strong potential for decision support in complex healthcare workforce management by dynamically adapting to patient arrivals and nurse availability while respecting multiple constraints."}}
{"id": "2509.18383", "categories": ["cs.AI", "cs.DM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18383", "abs": "https://arxiv.org/abs/2509.18383", "authors": ["Moran Feldman", "Amin Karbasi"], "title": "G\u00f6del Test: Can Large Language Models Solve Easy Conjectures?", "comment": null, "summary": "Recent announcements from frontier AI model labs have highlighted strong\nresults on high-school and undergraduate math competitions. Yet it remains\nunclear whether large language models can solve new, simple conjectures in more\nadvanced areas of mathematics. We propose the G\\\"odel Test: evaluating whether\na model can produce correct proofs for very simple, previously unsolved\nconjectures. To this end, we study the performance of GPT-5 on five conjectures\nin combinatorial optimization. For each problem, we provided one or two source\npapers from which the conjecture arose, withheld our own conjecture, and then\nassessed the model's reasoning in detail. On the three easier problems, GPT-5\nproduced nearly correct solutions; for Problem 2 it even derived a different\napproximation guarantee that, upon checking, refuted our conjecture while\nproviding a valid solution. The model failed on Problem 4, which required\ncombining results from two papers. On Problem 5, a harder case without a\nvalidated conjecture, GPT-5 proposed the same algorithm we had in mind but\nfailed in the analysis, suggesting the proof is more challenging than expected.\nAlthough our sample is small, the results point to meaningful progress on\nroutine reasoning, occasional flashes of originality, and clear limitations\nwhen cross-paper synthesis is required. GPT-5 may represent an early step\ntoward frontier models eventually passing the G\\\"odel Test.", "AI": {"tldr": "The paper proposes the G\u00f6del Test to evaluate if large language models can prove simple unsolved mathematical conjectures, testing GPT-5 on five combinatorial optimization problems with mixed results showing progress but limitations.", "motivation": "To determine whether large language models can solve new, simple conjectures in advanced mathematics beyond just replicating known competition problems.", "method": "Evaluated GPT-5 on five previously unsolved conjectures in combinatorial optimization by providing source papers and assessing the model's reasoning and proof generation capabilities.", "result": "GPT-5 produced nearly correct solutions for three easier problems, refuted one conjecture with a valid alternative proof, failed on a problem requiring cross-paper synthesis, and proposed correct algorithms but flawed analysis for the hardest problem.", "conclusion": "GPT-5 shows meaningful progress in routine reasoning and occasional originality but has clear limitations in complex synthesis tasks, representing an early step toward models eventually passing the G\u00f6del Test."}}
{"id": "2509.18126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18126", "abs": "https://arxiv.org/abs/2509.18126", "authors": ["Bishal K C", "Amr Hilal", "Pawan Thapa"], "title": "Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning", "comment": null, "summary": "Federated Learning (FL) is a decentralized training framework widely used in\nIoT ecosystems that preserves privacy by keeping raw data local, making it\nideal for IoT-enabled cyber-physical systems with sensing and communication\nlike Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric\nVehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle\ninfrastructure, securing these IoT-based charging stations against cyber\nthreats has become critical. Centralized Intrusion Detection Systems (IDS)\nraise privacy concerns due to sensitive network and user data, making FL a\npromising alternative. However, current FL-based IDS evaluations overlook\npractical challenges such as system heterogeneity and non-IID data. To address\nthese challenges, we conducted experiments to evaluate the performance of\nfederated learning for anomaly detection in EV charging stations under system\nand data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization\napproaches, to analyze their effectiveness in anomaly detection. Under IID\nsettings, FedAvg achieves superior performance to centralized models using the\nsame neural network. However, performance degrades with non-IID data and system\nheterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous\nsettings, showing better convergence and higher anomaly detection accuracy. Our\nresults demonstrate that FL can handle heterogeneity in IoT-based EVCS without\nsignificant performance loss, with FedAvgM as a promising solution for robust,\nprivacy-preserving EVCS security.", "AI": {"tldr": "Federated Learning (FL) is evaluated for anomaly detection in EV charging stations, addressing practical challenges like system heterogeneity and non-IID data. FedAvgM outperforms FedAvg in heterogeneous settings, showing FL can handle heterogeneity without significant performance loss.", "motivation": "Securing IoT-based electric vehicle charging stations against cyber threats is critical, but centralized IDS raise privacy concerns. FL offers a privacy-preserving alternative, but current evaluations overlook practical challenges like system heterogeneity and non-IID data.", "method": "Conducted experiments evaluating FL performance for anomaly detection in EV charging stations under system and data heterogeneity. Used FedAvg and FedAvgM optimization approaches to analyze their effectiveness.", "result": "Under IID settings, FedAvg achieves superior performance to centralized models. However, performance degrades with non-IID data and system heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous settings, showing better convergence and higher anomaly detection accuracy.", "conclusion": "FL can handle heterogeneity in IoT-based EVCS without significant performance loss, with FedAvgM as a promising solution for robust, privacy-preserving EVCS security."}}
{"id": "2509.18400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18400", "abs": "https://arxiv.org/abs/2509.18400", "authors": ["Pritish Yuvraj", "Siva Devarakonda"], "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification", "comment": null, "summary": "Accurate classification of products under the Harmonized Tariff Schedule\n(HTS) is a critical bottleneck in global trade, yet it has received little\nattention from the machine learning community. Misclassification can halt\nshipments entirely, with major postal operators suspending deliveries to the\nU.S. due to incomplete customs documentation. We introduce the first benchmark\nfor HTS code classification, derived from the U.S. Customs Rulings Online\nSearch System (CROSS). Evaluating leading LLMs, we find that our fine-tuned\nAtlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit\nclassifications and 57.5 percent correct 6-digit classifications, improvements\nof 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.\nBeyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and\neight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to\nguarantee data privacy in high-stakes trade and compliance workflows. While\nAtlas sets a strong baseline, the benchmark remains highly challenging, with\nonly 40 percent 10-digit accuracy. By releasing both dataset and model, we aim\nto position HTS classification as a new community benchmark task and invite\nfuture work in retrieval, reasoning, and alignment.", "AI": {"tldr": "First benchmark for HTS code classification using fine-tuned Atlas model (LLaMA-3.3-70B) achieves 40% 10-digit accuracy, significantly outperforming and being cheaper than leading LLMs.", "motivation": "HTS code classification is a critical bottleneck in global trade with severe consequences for misclassification, yet has received little ML research attention.", "method": "Created benchmark from US Customs Rulings Online Search System (CROSS) and fine-tuned Atlas model (LLaMA-3.3-70B) for HTS classification.", "result": "Atlas achieves 40% fully correct 10-digit classifications (15 points over GPT-5, 27.5 over Gemini) and 57.5% 6-digit accuracy, while being 5-8x cheaper and enabling self-hosting for data privacy.", "conclusion": "While Atlas sets strong baseline, HTS classification remains highly challenging; releasing dataset and model to establish it as community benchmark for future work in retrieval, reasoning, and alignment."}}
{"id": "2509.18127", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18127", "abs": "https://arxiv.org/abs/2509.18127", "authors": ["Jiaqi Weng", "Han Zheng", "Hanyu Zhang", "Qinqin He", "Jialing Tao", "Hui Xue", "Zhixuan Chu", "Xiting Wang"], "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework", "comment": null, "summary": "Increasing deployment of large language models (LLMs) in real-world\napplications raises significant safety concerns. Most existing safety research\nfocuses on evaluating LLM outputs or specific safety tasks, limiting their\nability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)\nfacilitate interpretability research to clarify model behavior by explaining\nsingle-meaning atomic features decomposed from entangled signals. jHowever,\nprior applications on SAEs do not interpret features with fine-grained\nsafety-related con- cepts, thus inadequately addressing safety-critical\nbehaviors, such as generating toxic responses and violating safety regu-\nlations. For rigorous safety analysis, we must extract a rich and diverse set\nof safety-relevant features that effectively capture these high-risk behaviors,\nyet face two challenges: identifying SAEs with the greatest potential for\ngenerating safety concept-specific neurons, and the prohibitively high cost of\ndetailed feature explanation. In this paper, we pro- pose Safe-SAIL, a\nframework for interpreting SAE features within LLMs to advance mechanistic\nunderstanding in safety domains. Our approach systematically identifies SAE\nwith best concept-specific interpretability, explains safety-related neurons,\nand introduces efficient strategies to scale up the in- terpretation process.\nWe will release a comprehensive toolkit including SAE checkpoints and\nhuman-readable neuron ex- planations, which supports empirical analysis of\nsafety risks to promote research on LLM safety.", "AI": {"tldr": "Safe-SAIL is a framework that uses Sparse Autoencoders (SAEs) to interpret safety-related features in LLMs, addressing the limitations of current safety research by systematically identifying and explaining safety-critical neurons to better understand and mitigate high-risk behaviors.", "motivation": "Existing safety research on LLMs focuses on evaluating outputs or specific tasks, which fails to address broader undefined risks. Current SAE applications lack fine-grained safety concept interpretation, making them inadequate for analyzing safety-critical behaviors like toxic response generation and regulation violations.", "method": "The proposed Safe-SAIL framework systematically identifies SAEs with the best concept-specific interpretability, explains safety-related neurons, and introduces efficient strategies to scale up the interpretation process. It includes a comprehensive toolkit with SAE checkpoints and human-readable neuron explanations.", "result": "The framework enables extraction of a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors in LLMs, overcoming challenges of identifying optimal SAEs and reducing the cost of detailed feature explanation.", "conclusion": "Safe-SAIL advances mechanistic understanding of safety domains in LLMs by providing interpretable safety features, supporting empirical analysis of safety risks, and promoting research on LLM safety through the release of comprehensive toolkits."}}
{"id": "2509.18420", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18420", "abs": "https://arxiv.org/abs/2509.18420", "authors": ["Nikolai Skripko"], "title": "Instruction-Following Evaluation in Function Calling for Large Language Models", "comment": null, "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.", "AI": {"tldr": "IFEval-FC is a new benchmark that evaluates precise instruction following in function calling by testing adherence to format specifications embedded in JSON schema parameter descriptions, which existing benchmarks overlook.", "motivation": "Existing function calling benchmarks only evaluate argument correctness but don't test adherence to format instructions (like double quotes, ISO dates) embedded in parameter descriptions, which is crucial for real-world AI agent systems.", "method": "IFEval-FC encodes verifiable formats directly within JSON schema descriptions and includes 750 test cases, each with a function containing embedded format requirements and corresponding user queries. Evaluation is fully algorithmic.", "result": "Even state-of-the-art proprietary models like GPT-5 and Claude 4.1 Opus frequently fail to follow basic formatting rules, revealing practical limitations for real-world agent systems.", "conclusion": "The benchmark highlights a critical gap in current function calling evaluation and provides an objective, reproducible, and scalable method to assess precise instruction following, with code and data publicly available."}}
{"id": "2509.18128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18128", "abs": "https://arxiv.org/abs/2509.18128", "authors": ["Amirreza Tootchi", "Xiaoping Du"], "title": "Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis", "comment": null, "summary": "Machine learning surrogates are increasingly employed to replace expensive\ncomputational models for physics-based reliability analysis. However, their use\nintroduces epistemic uncertainty from model approximation errors, which couples\nwith aleatory uncertainty in model inputs, potentially compromising the\naccuracy of reliability predictions. This study proposes a Gauss-Hermite\nquadrature approach to decouple these nested uncertainties and enable more\naccurate reliability analysis. The method evaluates conditional failure\nprobabilities under aleatory uncertainty using First and Second Order\nReliability Methods and then integrates these probabilities across realizations\nof epistemic uncertainty. Three examples demonstrate that the proposed approach\nmaintains computational efficiency while yielding more trustworthy predictions\nthan traditional methods that ignore model uncertainty.", "AI": {"tldr": "Proposes a Gauss-Hermite quadrature approach to decouple nested epistemic and aleatory uncertainties in machine learning surrogate models for more accurate reliability analysis.", "motivation": "Machine learning surrogates introduce epistemic uncertainty from model approximation errors that couples with aleatory uncertainty in inputs, compromising reliability prediction accuracy.", "method": "Uses Gauss-Hermite quadrature to decouple uncertainties, evaluates conditional failure probabilities under aleatory uncertainty using FORM/SORM, then integrates across epistemic uncertainty realizations.", "result": "Three examples demonstrate the approach maintains computational efficiency while yielding more trustworthy predictions than traditional methods ignoring model uncertainty.", "conclusion": "The proposed method effectively handles nested uncertainties in surrogate-based reliability analysis, providing more accurate and trustworthy predictions than conventional approaches."}}
{"id": "2509.18436", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.18436", "abs": "https://arxiv.org/abs/2509.18436", "authors": ["Hongda Jiang", "Xinyuan Zhang", "Siddhant Garg", "Rishab Arora", "Shiun-Zu Kuo", "Jiayang Xu", "Christopher Brossman", "Yue Liu", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xin Luna Dong"], "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories", "comment": null, "summary": "We introduce Memory-QA, a novel real-world task that involves answering\nrecall questions about visual content from previously stored multimodal\nmemories. This task poses unique challenges, including the creation of\ntask-oriented memories, the effective utilization of temporal and location\ninformation within memories, and the ability to draw upon multiple memories to\nanswer a recall question. To address these challenges, we propose a\ncomprehensive pipeline, Pensieve, integrating memory-specific augmentation,\ntime- and location-aware multi-signal retrieval, and multi-memory QA\nfine-tuning. We created a multimodal benchmark to illustrate various real\nchallenges in this task, and show the superior performance of Pensieve over\nstate-of-the-art solutions (up to 14% on QA accuracy).", "AI": {"tldr": "Memory-QA is a novel task for answering recall questions about visual content from stored multimodal memories, addressed by the Pensieve pipeline with superior performance over state-of-the-art methods.", "motivation": "To tackle the challenges of creating task-oriented memories, utilizing temporal and location information effectively, and leveraging multiple memories for answering recall questions in real-world scenarios.", "method": "Proposed Pensieve pipeline with memory-specific augmentation, time- and location-aware multi-signal retrieval, and multi-memory QA fine-tuning.", "result": "Pensieve achieves up to 14% improvement in QA accuracy over state-of-the-art solutions on a created multimodal benchmark.", "conclusion": "The Pensieve pipeline effectively addresses the unique challenges of the Memory-QA task, demonstrating significant performance gains in answering recall questions from multimodal memories."}}
{"id": "2509.18130", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18130", "abs": "https://arxiv.org/abs/2509.18130", "authors": ["Zijie Zhou", "Huichen Ma"], "title": "Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model", "comment": null, "summary": "In the metro intelligent transportation system, accurate transfer passenger\nflow prediction is a key link in optimizing operation plans and improving\ntransportation efficiency. To further improve the theory of metro internal\ntransfer passenger flow prediction and provide more reliable support for\nintelligent operation decisions, this paper innovatively proposes a metro\ntransfer passenger flow prediction model that integrates the Seasonal and Trend\ndecomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In\npractical application, the model first relies on the deep learning library\nKeras to complete the construction and training of the GRU model, laying the\nfoundation for subsequent prediction; then preprocesses the original metro card\nswiping data, uses the graph-based depth-first search algorithm to identify\npassengers' travel paths, and further constructs the transfer passenger flow\ntime series; subsequently adopts the STL time series decomposition algorithm to\ndecompose the constructed transfer passenger flow time series into trend\ncomponent, periodic component and residual component, and uses the 3{\\sigma}\nprinciple to eliminate and fill the outliers in the residual component, and\nfinally completes the transfer passenger flow prediction.Taking the transfer\npassenger flow data of a certain metro station as the research sample, the\nvalidity of the model is verified. The results show that compared with Long\nShort-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of\nSTL time series decomposition method and Long Short-Term Memory (STL-LSTM), the\nSTL-GRU combined prediction model significantly improves the prediction\naccuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays\nand rest days, with the mean absolute percentage error (MAPE) of the prediction\nresults reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.", "AI": {"tldr": "This paper proposes a metro transfer passenger flow prediction model combining STL decomposition and GRU neural network, which significantly improves prediction accuracy compared to existing methods.", "motivation": "Accurate transfer passenger flow prediction is crucial for optimizing metro operation plans and improving transportation efficiency in intelligent transportation systems.", "method": "The model uses STL time series decomposition to separate transfer passenger flow into trend, periodic, and residual components, applies 3\u03c3 principle for outlier handling, and uses GRU neural network for prediction based on processed metro card data and travel path identification.", "result": "The STL-GRU model outperformed LSTM, GRU, and STL-LSTM models, reducing MAPE by at least 2.3% on weekdays, 1.36% on Fridays, and 6.42% on rest days.", "conclusion": "The proposed STL-GRU combined prediction model effectively improves transfer passenger flow prediction accuracy and provides reliable support for intelligent metro operation decisions."}}
{"id": "2509.18527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18527", "abs": "https://arxiv.org/abs/2509.18527", "authors": ["Ziwen Chen", "Zhong Wang"], "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning", "comment": null, "summary": "The sport of fencing, like many other sports, faces challenges in refereeing:\nsubjective calls, human errors, bias, and limited availability in practice\nenvironments. We present FERA (Fencing Referee Assistant), a prototype AI\nreferee for foil fencing which integrates pose-based multi-label action\nrecognition and rule-based reasoning. FERA extracts 2D joint positions from\nvideo, normalizes them, computes a 101-dimensional kinematic feature set, and\napplies a Transformer for multi-label move and blade classification. To\ndetermine priority and scoring, FERA applies a distilled language model with\nencoded right-of-way rules, producing both a decision and an explanation for\neach exchange. With limited hand-labeled data, a 5-fold cross-validation\nachieves an average macro-F1 score of 0.549, outperforming multiple baselines,\nincluding a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla\nTransformer. While not ready for deployment, these results demonstrate a\npromising path towards automated referee assistance in foil fencing and new\nopportunities for AI applications, such as coaching in the field of fencing.", "AI": {"tldr": "FERA is an AI referee prototype for foil fencing that combines pose-based action recognition with rule-based reasoning to address subjective refereeing challenges.", "motivation": "Fencing faces refereeing challenges including subjective calls, human errors, bias, and limited availability in practice environments.", "method": "Uses 2D joint position extraction, kinematic feature computation, Transformer for multi-label classification, and rule-based reasoning with encoded right-of-way rules.", "result": "Achieved average macro-F1 score of 0.549 with 5-fold cross-validation, outperforming TCN, BiLSTM, and vanilla Transformer baselines.", "conclusion": "While not deployment-ready, FERA demonstrates promising potential for automated referee assistance and opens opportunities for AI applications in fencing coaching."}}
{"id": "2509.18131", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18131", "abs": "https://arxiv.org/abs/2509.18131", "authors": ["Jean-Michel Tucny", "Abhisek Ganguly", "Santosh Ansumali", "Sauro Succi"], "title": "Two ways to knowledge?", "comment": null, "summary": "It is shown that the weight matrices of transformer-based machine learning\napplications to the solution of two representative physical applications show a\nrandom-like character which bears no directly recognizable link to the physical\nand mathematical structure of the physical problem under study. This suggests\nthat machine learning and the scientific method may represent two distinct and\npotentially complementary paths to knowledge, even though a strict notion of\nexplainability in terms of direct correspondence between network parameters and\nphysical structures may remain out of reach. It is also observed that drawing a\nparallel between transformer operation and (generalized) path-integration\ntechniques may account for the random-like nature of the weights, but still\ndoes not resolve the tension with explainability. We conclude with some general\ncomments on the hazards of gleaning knowledge without the benefit of Insight.", "AI": {"tldr": "Transformer weight matrices in physics applications appear random and lack direct correlation with the underlying physical structure, suggesting ML and scientific methods may be complementary but with limited explainability.", "motivation": "To investigate whether transformer-based ML applications in physics reveal meaningful connections between network parameters and physical structures, or if they represent a fundamentally different approach to knowledge acquisition.", "method": "Analysis of weight matrices from transformer-based ML applications solving two representative physical problems, examining their characteristics and comparing with physical/mathematical structures.", "result": "Weight matrices show random-like character with no directly recognizable link to the physical problem structure, though parallels with path-integration techniques may explain this randomness.", "conclusion": "ML and scientific methods represent distinct but potentially complementary paths to knowledge, but strict explainability remains elusive, highlighting risks of acquiring knowledge without true insight."}}
{"id": "2509.18557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18557", "abs": "https://arxiv.org/abs/2509.18557", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025", "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting.", "AI": {"tldr": "LLMZ+ is a security framework that uses prompt whitelisting instead of traditional detection-based approaches to protect agentic LLMs from jailbreak attacks, ensuring only contextually appropriate messages interact with the AI.", "motivation": "Agentic AI systems have privileged access to data and APIs, making them valuable targets. Their nondeterministic behavior introduces significant security risks that traditional detection-based defenses don't adequately address.", "method": "The paper proposes LLMZ+, which implements prompt whitelisting to only allow contextually appropriate and safe messages to reach the agentic LLM, ensuring all exchanges conform to predefined use cases and operational boundaries.", "result": "Empirical evaluation shows LLMZ+ provides strong resilience against common jailbreak prompts while maintaining legitimate business communications. False positive and false negative rates were reduced to 0 in experimental settings.", "conclusion": "LLMZ+ offers a more streamlined, resilient security framework that reduces resource requirements for sustaining LLM information security compared to traditional detection-based approaches."}}
{"id": "2509.18133", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18133", "abs": "https://arxiv.org/abs/2509.18133", "authors": ["Le Huang", "Jiazheng Kang", "Cheng Hou", "Zhe Zhao", "Zhenxiang Yan", "Chuan Shi", "Ting Bai"], "title": "Self-Evolving LLMs via Continual Instruction Tuning", "comment": null, "summary": "In real-world industrial settings, large language models (LLMs) must learn\ncontinually to keep pace with diverse and evolving tasks, requiring\nself-evolution to refine knowledge under dynamic data distributions. However,\nexisting continual learning (CL) approaches, such as replay and parameter\nisolation, often suffer from catastrophic forgetting: training on new tasks\ndegrades performance on earlier ones by overfitting to the new distribution and\nweakening generalization.We propose MoE-CL, a parameter-efficient adversarial\nmixture-of-experts framework for industrial-scale, self-evolving continual\ninstruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated\nLoRA expert per task to preserve task-specific knowledge via parameter\nindependence, mitigating forgetting; and (2) a shared LoRA expert to enable\ncross-task transfer. To prevent transferring task-irrelevant noise through the\nshared pathway, we integrate a task-aware discriminator within a GAN. The\ndiscriminator encourages the shared expert to pass only task-aligned\ninformation during sequential training. Through adversarial learning, the\nshared expert acquires generalized representations that mimic the\ndiscriminator, while dedicated experts retain task-specific details, balancing\nknowledge retention and cross-task generalization and thereby supporting\nself-evolution.Extensive experiments on the public MTL5 benchmark and an\nindustrial Tencent3 benchmark validate the effectiveness of MoE-CL for\ncontinual instruction tuning. In real-world A/B testing for content compliance\nreview on the Tencent Video platform, MoE-CL reduced manual review costs by\n15.3%. These results demonstrate that MoE-CL is practical for large-scale\nindustrial deployment where continual adaptation and stable transfer are\ncritical.", "AI": {"tldr": "MoE-CL is a parameter-efficient adversarial mixture-of-experts framework for continual instruction tuning of LLMs that addresses catastrophic forgetting through dedicated task-specific experts and a shared expert with task-aware discrimination.", "motivation": "Existing continual learning approaches suffer from catastrophic forgetting when training on new tasks, degrading performance on earlier tasks due to overfitting to new distributions and weakened generalization.", "method": "Uses dual-expert design: dedicated LoRA expert per task for task-specific knowledge preservation, and shared LoRA expert for cross-task transfer with a task-aware discriminator in a GAN framework to filter task-irrelevant noise.", "result": "Extensive experiments on MTL5 and Tencent3 benchmarks show effectiveness. Real-world A/B testing on Tencent Video platform reduced manual review costs by 15.3%.", "conclusion": "MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical, supporting self-evolution of LLMs in dynamic environments."}}
{"id": "2509.18565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18565", "abs": "https://arxiv.org/abs/2509.18565", "authors": ["Mitchell Piehl", "Dillon Wilson", "Ananya Kalita", "Jugal Kalita"], "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation", "comment": "Accepted to IEEE ICMLA 2025", "summary": "Large Language Models (LLMs) excel at various tasks, including\nproblem-solving and question-answering. However, LLMs often find Math Word\nProblems (MWPs) challenging because solving them requires a range of reasoning\nand mathematical abilities with which LLMs seem to struggle. Recent efforts\nhave helped LLMs solve more complex MWPs with improved prompts. This study\nproposes a novel method that initially prompts an LLM to create equations from\na decomposition of the question, followed by using an external symbolic\nequation solver to produce an answer. To ensure the accuracy of the obtained\nanswer, inspired by an established recommendation of math teachers, the LLM is\ninstructed to solve the MWP a second time, but this time with the objective of\nestimating the correct answer instead of solving it exactly. The estimation is\nthen compared to the generated answer to verify. If verification fails, an\niterative rectification process is employed to ensure the correct answer is\neventually found. This approach achieves new state-of-the-art results on\ndatasets used by prior published research on numeric and algebraic MWPs,\nimproving the previous best results by nearly two percent on average. In\naddition, the approach obtains satisfactory results on trigonometric MWPs, a\ntask not previously attempted to the authors' best knowledge. This study also\nintroduces two new datasets, SVAMPClean and Trig300, to further advance the\ntesting of LLMs' reasoning abilities.", "AI": {"tldr": "A novel method that combines LLM equation generation with symbolic solvers and verification through estimation to solve Math Word Problems, achieving state-of-the-art results.", "motivation": "LLMs struggle with Math Word Problems due to limitations in reasoning and mathematical abilities, despite excelling at other tasks. Current methods need improvement for complex MWPs.", "method": "First prompts LLM to create equations from question decomposition, uses external symbolic solver for answer, then verifies by having LLM estimate the answer and compare with generated answer. Uses iterative rectification if verification fails.", "result": "Achieves new state-of-the-art results on numeric and algebraic MWPs, improving previous best by nearly 2% on average. Also obtains satisfactory results on trigonometric MWPs, a previously unattempted task.", "conclusion": "The proposed approach effectively enhances LLMs' ability to solve complex MWPs through decomposition, symbolic solving, and verification, demonstrating significant improvements across various mathematical problem types."}}
{"id": "2509.18134", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.18134", "abs": "https://arxiv.org/abs/2509.18134", "authors": ["Furan Xie", "Bing Liu", "Li Chai"], "title": "A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization", "comment": null, "summary": "This paper investigates the privacy-preserving distributed optimization\nproblem, aiming to protect agents' private information from potential attackers\nduring the optimization process. Gradient tracking, an advanced technique for\nimproving the convergence rate in distributed optimization, has been applied to\nmost first-order algorithms in recent years. We first reveal the inherent\nprivacy leakage risk associated with gradient tracking. Building upon this\ninsight, we propose a weighted gradient tracking distributed privacy-preserving\nalgorithm, eliminating the privacy leakage risk in gradient tracking using\ndecaying weight factors. Then, we characterize the convergence of the proposed\nalgorithm under time-varying heterogeneous step sizes. We prove the proposed\nalgorithm converges precisely to the optimal solution under mild assumptions.\nFinally, numerical simulations validate the algorithm's effectiveness through a\nclassical distributed estimation problem and the distributed training of a\nconvolutional neural network.", "AI": {"tldr": "This paper addresses privacy risks in gradient tracking for distributed optimization and proposes a weighted gradient tracking algorithm with decaying weights to eliminate privacy leakage while maintaining convergence.", "motivation": "To protect agents' private information from attackers during distributed optimization processes, as gradient tracking techniques used to improve convergence rates inherently create privacy leakage risks.", "method": "Proposes a weighted gradient tracking distributed privacy-preserving algorithm that uses decaying weight factors to eliminate privacy leakage in gradient tracking, with convergence analysis under time-varying heterogeneous step sizes.", "result": "The algorithm converges precisely to the optimal solution under mild assumptions, validated through numerical simulations including distributed estimation problems and CNN training.", "conclusion": "The proposed weighted gradient tracking algorithm successfully eliminates privacy leakage risks while maintaining convergence performance, providing an effective privacy-preserving solution for distributed optimization."}}
{"id": "2509.18633", "categories": ["cs.AI", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2509.18633", "abs": "https://arxiv.org/abs/2509.18633", "authors": ["Yara Mohajerani"], "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents", "comment": "Submitted and accepted to Tackling Climate Change with Machine\n  Learning workshop at NeurIPS 2025. 5 pages, 1 figure. Source code and\n  documentation available at\n  https://github.com/yaramohajerani/spatial-climate-ABM", "summary": "Climate risk assessment requires modelling complex interactions between\nspatially heterogeneous hazards and adaptive economic systems. We present a\nnovel geospatial agent-based model that integrates climate hazard data with\nevolutionary learning for economic agents. Our framework combines Mesa-based\nspatial modelling with CLIMADA climate impact assessment, introducing adaptive\nlearning behaviours that allow firms to evolve strategies for budget\nallocation, pricing, wages, and risk adaptation through fitness-based selection\nand mutation. We demonstrate the framework using riverine flood projections\nunder RCP8.5 until 2100, showing that evolutionary adaptation enables firms to\nconverge with baseline (no hazard) production levels after decades of\ndisruption due to climate stress. Our results reveal systemic risks where even\nagents that are not directly exposed to floods face impacts through supply\nchain disruptions, with the end-of-century average price of goods 5.6% higher\nunder RCP8.5 compared to the baseline. This open-source framework provides\nfinancial institutions and companies with tools to quantify both direct and\ncascading climate risks while evaluating cost-effective adaptation strategies.", "AI": {"tldr": "A geospatial agent-based model integrating climate hazard data with evolutionary learning for economic agents to assess climate risks and adaptation strategies.", "motivation": "Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems.", "method": "Combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviors for firms to evolve strategies through fitness-based selection and mutation.", "result": "Evolutionary adaptation enables firms to converge with baseline production levels after decades of disruption; systemic risks show even non-exposed agents face impacts through supply chain disruptions, with end-of-century prices 5.6% higher under RCP8.5.", "conclusion": "This open-source framework provides tools for financial institutions and companies to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies."}}
{"id": "2509.18135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18135", "abs": "https://arxiv.org/abs/2509.18135", "authors": ["Shaoxun Wang", "Xingjun Zhang", "Qianyang Li", "Jiawei Cao", "Zhendong Tan"], "title": "SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting", "comment": null, "summary": "Inter-series correlations are crucial for accurate multivariate time series\nforecasting, yet these relationships often exhibit complex dynamics across\ndifferent temporal scales. Existing methods are limited in modeling these\nmulti-scale dependencies and struggle to capture their intricate and evolving\nnature. To address this challenge, this paper proposes a novel Static-Dynamic\nGraph Fusion network (SDGF), whose core lies in capturing multi-scale\ninter-series correlations through a dual-path graph structure learning\napproach. Specifically, the model utilizes a static graph based on prior\nknowledge to anchor long-term, stable dependencies, while concurrently\nemploying Multi-level Wavelet Decomposition to extract multi-scale features for\nconstructing an adaptively learned dynamic graph to capture associations at\ndifferent scales. We design an attention-gated module to fuse these two\ncomplementary sources of information intelligently, and a multi-kernel dilated\nconvolutional network is then used to deepen the understanding of temporal\npatterns. Comprehensive experiments on multiple widely used real-world\nbenchmark datasets demonstrate the effectiveness of our proposed model.", "AI": {"tldr": "Proposes SDGF network using static-dynamic graph fusion to capture multi-scale inter-series correlations in multivariate time series forecasting", "motivation": "Existing methods struggle to model complex multi-scale dependencies and evolving inter-series correlations in multivariate time series", "method": "Dual-path graph structure learning with static graph (prior knowledge) and dynamic graph (multi-level wavelet decomposition), fused via attention-gated module and processed with multi-kernel dilated convolutional network", "result": "Comprehensive experiments on real-world benchmark datasets demonstrate the model's effectiveness", "conclusion": "SDGF successfully addresses the challenge of capturing multi-scale inter-series correlations through static-dynamic graph fusion"}}
{"id": "2509.18667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18667", "abs": "https://arxiv.org/abs/2509.18667", "authors": ["Qiao Xiao", "Hong Ting Tsang", "Jiaxin Bai"], "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation", "comment": "16 pages, 2 figures, 4 tables. Submitted to the 2026 18th\n  International Conference on Machine Learning and Computing (ICMLC 2026),\n  under review", "summary": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied\napproach for improving the reasoning, accuracy, and factuality of Large\nLanguage Models. However, many existing graph-based RAG systems overlook the\nhigh cost associated with LLM token usage during graph construction, hindering\nlarge-scale adoption. To address this, we propose TERAG, a simple yet effective\nframework designed to build informative graphs at a significantly lower cost.\nInspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the\nretrieval phase, and we achieve at least 80% of the accuracy of widely used\ngraph-based RAG methods while consuming only 3%-11% of the output tokens.", "AI": {"tldr": "TERAG is a cost-effective graph-based RAG framework that reduces LLM token usage by 89-97% while maintaining 80% accuracy compared to existing methods.", "motivation": "Existing graph-based RAG systems have high LLM token costs during graph construction, which limits large-scale adoption. The goal is to create a more cost-effective solution.", "method": "Proposes TERAG framework inspired by HippoRAG, incorporating Personalized PageRank (PPR) during retrieval phase to build informative graphs with minimal token consumption.", "result": "Achieves at least 80% accuracy of widely used graph-based RAG methods while using only 3%-11% of the output tokens (89-97% reduction in token usage).", "conclusion": "TERAG provides a simple yet effective solution for cost-efficient graph-based RAG, making large-scale adoption more feasible by significantly reducing token costs while maintaining competitive accuracy."}}
{"id": "2509.18136", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18136", "abs": "https://arxiv.org/abs/2509.18136", "authors": ["Suqing Wang", "Zuchao Li", "Luohe Shi", "Bo Du", "Hai Zhao", "Yun Li", "Qianren Wang"], "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development", "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) have achieved remarkable success across various\ndomains, driving significant technological advancements and innovations.\nDespite the rapid growth in model scale and capability, systematic, data-driven\nresearch on how structural configurations affect performance remains scarce. To\naddress this gap, we present a large-scale dataset encompassing diverse\nopen-source LLM structures and their performance across multiple benchmarks.\nLeveraging this dataset, we conduct a systematic, data mining-driven analysis\nto validate and quantify the relationship between structural configurations and\nperformance. Our study begins with a review of the historical development of\nLLMs and an exploration of potential future trends. We then analyze how various\nstructural choices impact performance across benchmarks and further corroborate\nour findings using mechanistic interpretability techniques. By providing\ndata-driven insights into LLM optimization, our work aims to guide the targeted\ndevelopment and application of future models. We will release our dataset at\nhttps://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset", "AI": {"tldr": "This paper presents a large-scale dataset and systematic analysis of how structural configurations affect LLM performance, using data mining and mechanistic interpretability techniques.", "motivation": "Despite rapid growth in LLM capabilities, there's a lack of systematic, data-driven research on how structural configurations impact performance. The authors aim to fill this gap with empirical evidence.", "method": "Created a large-scale dataset of diverse open-source LLM structures and their benchmark performance. Conducted systematic data mining analysis and used mechanistic interpretability techniques to validate findings.", "result": "The study provides data-driven insights into the relationship between structural configurations and LLM performance across multiple benchmarks.", "conclusion": "This work offers guidance for targeted development and application of future LLMs by quantifying how structural choices impact performance, with the dataset being publicly released."}}
{"id": "2509.18681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18681", "abs": "https://arxiv.org/abs/2509.18681", "authors": ["Nicolas Valot", "Louis Fabre", "Benjamin Lesage", "Ammar Mechouche", "Claire Pagetti"], "title": "Implementation of airborne ML models with semantics preservation", "comment": null, "summary": "Machine Learning (ML) may offer new capabilities in airborne systems.\nHowever, as any piece of airborne systems, ML-based systems will be required to\nguarantee their safe operation. Thus, their development will have to be\ndemonstrated to be compliant with the adequate guidance. So far, the European\nUnion Aviation Safety Agency (EASA) has published a concept paper and an\nEUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level\nobjectives to confirm the ML model achieves its intended function and maintains\ntraining performance in the target environment. The paper aims to clarify the\ndifference between an ML model and its corresponding unambiguous description,\nreferred to as the Machine Learning Model Description (MLMD). It then refines\nthe essential notion of semantics preservation to ensure the accurate\nreplication of the model. We apply our contributions to several industrial use\ncases to build and compare several target models.", "AI": {"tldr": "The paper clarifies the distinction between ML models and their unambiguous descriptions (MLMD) and refines semantics preservation for accurate model replication, applied to industrial use cases.", "motivation": "To address the need for safe operation and regulatory compliance of ML-based airborne systems, particularly in meeting EASA and EUROCAE/SAE standards for demonstrating intended function and training performance maintenance.", "method": "The paper defines the concept of Machine Learning Model Description (MLMD) as an unambiguous description of ML models, refines semantics preservation for accurate model replication, and applies these concepts to industrial use cases to build and compare target models.", "result": "The approach provides a framework for ensuring ML models in airborne systems can be accurately described and replicated while maintaining their intended functionality and performance in target environments.", "conclusion": "The clarification between ML models and MLMDs, along with refined semantics preservation concepts, contributes to safer development of ML-based airborne systems that can meet regulatory compliance requirements."}}
{"id": "2509.18137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18137", "abs": "https://arxiv.org/abs/2509.18137", "authors": ["Shaoheng Wang", "Yao Lu", "Yuqi Li", "Yaxin Gao", "Jiaqi Nie", "Shanqing Yu", "Yingli Tian", "Qi Xuan"], "title": "LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods", "comment": null, "summary": "As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation\n(LoRA) can save significant costs in storage and computing, but its strong\nadaptability to a single task is often accompanied by insufficient cross-task\ngeneralization capabilities. To improve this, existing work combines LoRA with\nmixture-of-experts (MoE) to enhance the model's adaptability through expert\nmodules and routing mechanisms. However, existing LoRA-MoE methods lack unified\nstandards in models, datasets, hyperparameters, and evaluation methods, making\nit difficult to conduct fair comparisons between different methods. To this\nend, we proposed a unified benchmark named LoRALib. Specifically, we\nstandardized datasets from $40$ downstream tasks into a unified format,\nfine-tuned them using the same hyperparameters and obtained $680$ LoRA modules\nacross $17$ model architectures. Based on this LoRA library, we conduct\nlarge-scale experiments on $3$ representative LoRA-MoE methods and different\nLoRA selection mechanisms using the open-sourced testing tool OpenCompass.\nExtensive experiments show that LoRAMoE performs best, and that prioritizing\nLoRAs relevant to the target task can further improve the performance of MoE.\nWe hope these findings will inspire future work. Our datasets and LoRA library\nare available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset\nand https://huggingface.co/YaoLuzjut/models.", "AI": {"tldr": "LoRALib is a unified benchmark for LoRA-MoE methods that standardizes datasets, hyperparameters, and evaluation across 40 tasks and 17 models, revealing LoRAMoE as the best performer and showing task-relevant LoRA selection improves MoE performance.", "motivation": "Existing LoRA-MoE methods lack standardized evaluation frameworks, making fair comparisons difficult due to inconsistent models, datasets, hyperparameters, and evaluation methods across different approaches.", "method": "Created LoRALib benchmark by standardizing 40 downstream tasks into unified format, fine-tuning with same hyperparameters to obtain 680 LoRA modules across 17 model architectures, then conducting large-scale experiments on 3 representative LoRA-MoE methods using OpenCompass testing tool.", "result": "Extensive experiments show LoRAMoE performs best among the tested methods, and prioritizing LoRAs relevant to the target task can further improve MoE performance.", "conclusion": "The findings provide standardized benchmarks for fair comparison of LoRA-MoE methods and demonstrate the importance of task-relevant LoRA selection, which should inspire future work in parameter-efficient fine-tuning."}}
{"id": "2509.18690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18690", "abs": "https://arxiv.org/abs/2509.18690", "authors": ["Zhiyu Kan", "Wensheng Gan", "Zhenlian Qi", "Philip S. Yu"], "title": "Advances in Large Language Models for Medicine", "comment": "Preprint. 5 figures, 4 tables", "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years,\nwith large language models (LLMs) emerging as a significant breakthrough. LLMs\nare increasingly making an impact across various industries, with the medical\nfield standing out as the most prominent application area. This paper\nsystematically reviews the up-to-date research progress of LLMs in the medical\nfield, providing an in-depth analysis of training techniques for large medical\nmodels, their adaptation in healthcare settings, related applications, as well\nas their strengths and limitations. Furthermore, it innovatively categorizes\nmedical LLMs into three distinct types based on their training methodologies\nand classifies their evaluation approaches into two categories. Finally, the\nstudy proposes solutions to existing challenges and outlines future research\ndirections based on identified issues in the field of medical LLMs. By\nsystematically reviewing previous and advanced research findings, we aim to\nhighlight the necessity of developing medical LLMs, provide a deeper\nunderstanding of their current state of development, and offer clear guidance\nfor subsequent research.", "AI": {"tldr": "This paper provides a systematic review of large language models (LLMs) in the medical field, analyzing training techniques, healthcare applications, strengths/limitations, and proposing future research directions.", "motivation": "To highlight the necessity of developing medical LLMs, provide understanding of their current state, and offer guidance for subsequent research as AI advances rapidly with LLMs making significant impact in medicine.", "method": "Systematic review of up-to-date research progress, including analysis of training techniques for large medical models, their healthcare adaptation, applications, and innovative categorization of medical LLMs into three types based on training methodologies.", "result": "The study categorizes medical LLMs into three distinct types, classifies evaluation approaches into two categories, and identifies existing challenges in the field.", "conclusion": "The paper proposes solutions to challenges and outlines future research directions, aiming to provide clear guidance for subsequent research in medical LLMs."}}
{"id": "2509.18138", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18138", "abs": "https://arxiv.org/abs/2509.18138", "authors": ["Tiantian Zhang"], "title": "Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts", "comment": null, "summary": "We introduce a new algorithm, \\emph{Rank-Induced Plackett--Luce Mirror\nDescent (RIPLM)}, which leverages the structural equivalence between the\n\\emph{rank benchmark} and the \\emph{distributional benchmark} established in\n\\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert\nidentities, RIPLM updates directly in the \\emph{rank-induced Plackett--Luce\n(PL)} parameterization. This ensures that the algorithm's played distributions\nremain within the class of rank-induced distributions at every round,\npreserving the equivalence with the rank benchmark. To our knowledge, RIPLM is\nthe first algorithm that is both (i) \\emph{rank-faithful} and (ii)\n\\emph{variance-adaptive} in the sleeping experts setting.", "AI": {"tldr": "RIPLM is a new rank-faithful and variance-adaptive algorithm for sleeping experts that operates directly in rank-induced Plackett-Luce parameter space.", "motivation": "To create an algorithm that preserves the structural equivalence between rank and distributional benchmarks while being both rank-faithful and variance-adaptive in sleeping experts settings.", "method": "Uses rank-induced Plackett-Luce mirror descent (RIPLM) that updates directly in rank-induced PL parameterization, ensuring played distributions remain within rank-induced distributions at every round.", "result": "RIPLM is the first algorithm that simultaneously achieves rank-faithfulness and variance-adaptivity in the sleeping experts setting.", "conclusion": "The RIPLM algorithm successfully bridges the gap between rank and distributional benchmarks while maintaining desirable properties for sleeping experts problems."}}
{"id": "2509.18710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18710", "abs": "https://arxiv.org/abs/2509.18710", "authors": ["Yanjie Fu", "Dongjie Wang", "Wangyang Ying", "Xiangliang Zhang", "Huan Liu", "Jian Pei"], "title": "Autonomous Data Agents: A New Opportunity for Smart Data", "comment": null, "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions.", "AI": {"tldr": "DataAgents represent a paradigm shift using autonomous AI agents to transform complex data into actionable knowledge through LLM reasoning, task decomposition, and tool calling.", "motivation": "Data preparation and analysis remain labor-intensive despite growing data complexity, and traditional tools lack the adaptability needed for optimal AI utilization of unstructured data.", "method": "DataAgents integrate LLM reasoning with task decomposition, action reasoning, grounding, and tool calling to autonomously handle data operations like preprocessing, transformation, augmentation, and retrieval.", "result": "DataAgents can dynamically plan workflows and adapt to diverse data tasks at scale, transforming unstructured data into coherent knowledge through automated data operations.", "conclusion": "The report calls for advancing workflow optimization, establishing benchmarks, safeguarding privacy, and developing trustworthy guardrails to prevent malicious actions in DataAgent systems."}}
{"id": "2509.18139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18139", "abs": "https://arxiv.org/abs/2509.18139", "authors": ["Akshay Murthy", "Shawn Sebastian", "Manil Shangle", "Huaduo Wang", "Sopam Dasgupta", "Gopal Gupta"], "title": "Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification", "comment": "7 pages", "summary": "Recently, the demand for Machine Learning (ML) models that can balance\naccuracy, efficiency, and interpreability has grown significantly.\nTraditionally, there has been a tradeoff between accuracy and explainability in\npredictive models, with models such as Neural Networks achieving high accuracy\non complex datasets while sacrificing internal transparency. As such, new\nrule-based algorithms such as FOLD-SE have been developed that provide tangible\njustification for predictions in the form of interpretable rule sets. The\nprimary objective of this study was to compare FOLD-SE and FOLD-R++, both\nrule-based classifiers, in binary classification and evaluate how FOLD-SE\nperforms against XGBoost, a widely used ensemble classifier, when applied to\nmulti-category classification. We hypothesized that because FOLD-SE can\ngenerate a condensed rule set in a more explainable manner, it would lose\nupwards of an average of 3 percent in accuracy and F1 score when compared with\nXGBoost and FOLD-R++ in multiclass and binary classification, respectively. The\nresearch used data collections for classification, with accuracy, F1 scores,\nand processing time as the primary performance measures. Outcomes show that\nFOLD-SE is superior to FOLD-R++ in terms of binary classification by offering\nfewer rules but losing a minor percentage of accuracy and efficiency in\nprocessing time; in tasks that involve multi-category classifications, FOLD-SE\nis more precise and far more efficient compared to XGBoost, in addition to\ngenerating a comprehensible rule set. The results point out that FOLD-SE is a\nbetter choice for both binary tasks and classifications with multiple\ncategories. Therefore, these results demonstrate that rule-based approaches\nlike FOLD-SE can bridge the gap between explainability and performance,\nhighlighting their potential as viable alternatives to black-box models in\ndiverse classification tasks.", "AI": {"tldr": "FOLD-SE, a rule-based classifier, balances accuracy and explainability, outperforming FOLD-R++ in binary classification and XGBoost in multi-category classification while providing interpretable rule sets.", "motivation": "Address the trade-off between accuracy and explainability in ML models by developing rule-based algorithms that offer transparent predictions without significant performance loss.", "method": "Compare FOLD-SE and FOLD-R++ for binary classification, and FOLD-SE against XGBoost for multi-category classification using accuracy, F1 scores, and processing time as metrics.", "result": "FOLD-SE outperforms FOLD-R++ in binary classification with fewer rules and minor accuracy loss; it is more precise and efficient than XGBoost in multi-category tasks while generating interpretable rules.", "conclusion": "Rule-based approaches like FOLD-SE effectively bridge the gap between explainability and performance, making them viable alternatives to black-box models in classification tasks."}}
{"id": "2509.18771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18771", "abs": "https://arxiv.org/abs/2509.18771", "authors": ["Xingkun Yin", "Kaibin Huang", "Dong In Kim", "Hongyang Du"], "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models", "comment": null, "summary": "Scaling model size, training data, and compute power have driven advances in\nlarge language models (LLMs), but these approaches are reaching saturation as\nhuman-generated text is exhausted and further gains diminish. We propose\nexperience scaling, a framework for continuous post-deployment evolution for\nLLMs through autonomous interaction with the environment and collaborative\nsharing of accumulated experience. The framework captures raw interactions,\ndistills them into compact, reusable knowledge, and periodically refines stored\ncontent to preserve relevance and efficiency. We validate the framework in\nsimulated real-world scenarios involving generalization to previously unseen\nbut related tasks, repetitive queries, and over-saturated knowledge stores.\nAcross all settings, experience scaling improves accuracy, sustains performance\nover time, and maintains gains when applied to novel situations. These results\ndemonstrate that structured post-deployment learning can extend LLM\ncapabilities beyond the limits of static human-generated data, offering a\nscalable path for continued intelligence progress.", "AI": {"tldr": "Experience scaling - a framework for continuous post-deployment evolution of LLMs through autonomous environmental interaction and collaborative experience sharing, enabling ongoing capability improvement beyond static training data limits.", "motivation": "Traditional scaling approaches (model size, training data, compute) are reaching saturation as human-generated text is exhausted and gains diminish, requiring new methods for continuous LLM evolution.", "method": "A framework that captures raw interactions, distills them into compact reusable knowledge, and periodically refines stored content to maintain relevance and efficiency through autonomous environmental interaction and collaborative experience sharing.", "result": "Validated in simulated real-world scenarios, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations involving generalization to unseen tasks, repetitive queries, and over-saturated knowledge stores.", "conclusion": "Structured post-deployment learning can extend LLM capabilities beyond static human-generated data limitations, offering a scalable path for continued intelligence progress through experience scaling."}}
{"id": "2509.18140", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18140", "abs": "https://arxiv.org/abs/2509.18140", "authors": ["Iram Wajahat", "Amritpal Singh", "Fazel Keshtkar", "Syed Ahmad Chan Bukhari"], "title": "A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders", "comment": "6 pages, 6 figures", "summary": "Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent\na significant global health burden, disproportionately impacting genetically\npredisposed populations such as the Pima Indians (a Native American tribe from\nsouth central Arizona). This study introduces a novel machine learning (ML)\nframework that integrates predictive modeling with gene-agnostic pathway\nmapping to identify high-risk individuals and uncover potential therapeutic\ntargets. Using the Pima Indian dataset, logistic regression and t-tests were\napplied to identify key predictors of T2DM, yielding an overall model accuracy\nof 78.43%. To bridge predictive analytics with biological relevance, we\ndeveloped a pathway mapping strategy that links identified predictors to\ncritical signaling networks, including insulin signaling, AMPK, and PPAR\npathways. This approach provides mechanistic insights without requiring direct\nmolecular data. Building upon these connections, we propose therapeutic\nstrategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1\nmodulators, and phytochemical, further validated through pathway enrichment\nanalyses. Overall, this framework advances precision medicine by offering\ninterpretable and scalable solutions for early detection and targeted\nintervention in metabolic disorders. The key contributions of this work are:\n(1) development of an ML framework combining logistic regression and principal\ncomponent analysis (PCA) for T2DM risk prediction; (2) introduction of a\ngene-agnostic pathway mapping approach to generate mechanistic insights; and\n(3) identification of novel therapeutic strategies tailored for high-risk\npopulations.", "AI": {"tldr": "A novel machine learning framework combining predictive modeling with gene-agnostic pathway mapping to identify high-risk individuals and therapeutic targets for type 2 diabetes in genetically predisposed populations like Pima Indians.", "motivation": "Metabolic disorders like type 2 diabetes disproportionately affect genetically predisposed populations such as Pima Indians, creating a significant global health burden that requires precision medicine approaches for early detection and targeted intervention.", "method": "Used logistic regression and t-tests on Pima Indian dataset to identify T2DM predictors (78.43% accuracy), combined with pathway mapping strategy linking predictors to insulin signaling, AMPK, and PPAR pathways without requiring molecular data.", "result": "Developed an interpretable ML framework that successfully identified high-risk individuals and proposed therapeutic strategies including dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1 modulators, and phytochemical interventions validated through pathway enrichment analyses.", "conclusion": "The framework advances precision medicine by providing scalable, interpretable solutions for early detection and targeted intervention in metabolic disorders, with key contributions in ML-based risk prediction, gene-agnostic pathway mapping, and population-specific therapeutic strategy identification."}}
{"id": "2509.18787", "categories": ["cs.AI", "C.2.4"], "pdf": "https://arxiv.org/pdf/2509.18787", "abs": "https://arxiv.org/abs/2509.18787", "authors": ["Luca Muscariello", "Vijoy Pandey", "Ramiz Polic"], "title": "The AGNTCY Agent Directory Service: Architecture and Implementation", "comment": null, "summary": "The Agent Directory Service (ADS) is a distributed directory for the\ndiscovery of AI agent capabilities, metadata, and provenance. It leverages\ncontent-addressed storage, hierarchical taxonomies, and cryptographic signing\nto enable efficient, verifiable, and multi-dimensional discovery across\nheterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema\nFramework (OASF), ADS decouples capability indexing from content location\nthrough a two-level mapping realized over a Kademlia-based Distributed Hash\nTable (DHT). It reuses mature OCI / ORAS infrastructure for artifact\ndistribution, integrates Sigstore for provenance, and supports schema-driven\nextensibility for emerging agent modalities (LLM prompt agents, MCP servers,\nA2A-enabled components). This paper formalizes the architectural model,\ndescribes storage and discovery layers, explains security and performance\nproperties, and positions ADS within the broader landscape of emerging agent\nregistry and interoperability initiatives.", "AI": {"tldr": "ADS is a distributed directory service for discovering AI agent capabilities using content-addressed storage, hierarchical taxonomies, and cryptographic signing to enable verifiable discovery across heterogeneous Multi-Agent Systems.", "motivation": "To address the need for efficient, verifiable discovery of AI agent capabilities across diverse Multi-Agent Systems, enabling interoperability and trust in agent interactions.", "method": "Leverages Open Agentic Schema Framework with two-level mapping over Kademlia-based DHT, uses OCI/ORAS infrastructure for artifact distribution, integrates Sigstore for provenance, and supports schema-driven extensibility.", "result": "A formal architectural model with storage and discovery layers that provides security and performance properties for agent registry and interoperability.", "conclusion": "ADS positions itself as a foundational component in the emerging landscape of agent registry and interoperability initiatives, enabling trustworthy discovery across heterogeneous agent systems."}}
{"id": "2509.18141", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18141", "abs": "https://arxiv.org/abs/2509.18141", "authors": ["Yao Zhao", "Haoyue Sun", "Yantian Ding", "Yanxun Xu"], "title": "KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots", "comment": null, "summary": "Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots\nprovides valuable insights for evidence synthesis in clinical research.\nHowever, existing approaches often rely on manual digitization, which is\nerror-prone and lacks scalability. To address these limitations, we develop\nKM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD\ndirectly from KM plots with high accuracy, robustness, and reproducibility.\nKM-GPT integrates advanced image preprocessing, multi-modal reasoning powered\nby GPT-5, and iterative reconstruction algorithms to generate high-quality IPD\nwithout manual input or intervention. Its hybrid reasoning architecture\nautomates the conversion of unstructured information into structured data flows\nand validates data extraction from complex KM plots. To improve accessibility,\nKM-GPT is equipped with a user-friendly web interface and an integrated AI\nassistant, enabling researchers to reconstruct IPD without requiring\nprogramming expertise. KM-GPT was rigorously evaluated on synthetic and\nreal-world datasets, consistently demonstrating superior accuracy. To\nillustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer\nimmunotherapy trials, reconstructing IPD to facilitate evidence synthesis and\nbiomarker-based subgroup analyses. By automating traditionally manual processes\nand providing a scalable, web-based solution, KM-GPT transforms clinical\nresearch by leveraging reconstructed IPD to enable more informed downstream\nanalyses, supporting evidence-based decision-making.", "AI": {"tldr": "KM-GPT is an AI-powered pipeline that automatically reconstructs individual patient data from Kaplan-Meier plots, eliminating manual digitization and enabling scalable evidence synthesis in clinical research.", "motivation": "Existing approaches for reconstructing IPD from KM plots rely on manual digitization, which is error-prone and lacks scalability, limiting evidence synthesis in clinical research.", "method": "KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered by GPT-5, and iterative reconstruction algorithms with a hybrid reasoning architecture that automates conversion of unstructured information into structured data flows.", "result": "KM-GPT demonstrated superior accuracy on both synthetic and real-world datasets and successfully reconstructed IPD for a meta-analysis of gastric cancer immunotherapy trials, facilitating evidence synthesis and biomarker-based subgroup analyses.", "conclusion": "KM-GPT transforms clinical research by automating traditionally manual processes, providing a scalable web-based solution that enables more informed downstream analyses and supports evidence-based decision-making."}}
{"id": "2509.18836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18836", "abs": "https://arxiv.org/abs/2509.18836", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Bounded PCTL Model Checking of Large Language Model Outputs", "comment": "ICTAI 2025", "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification\nmethod to verify the probabilistic computation tree logic (PCTL) properties of\nan LLM text generation process. We empirically show that only a limited number\nof tokens are typically chosen during text generation, which are not always the\nsame. This insight drives the creation of $\\alpha$-$k$-bounded text generation,\nnarrowing the focus to the $\\alpha$ maximal cumulative probability on the\ntop-$k$ tokens at every step of the text generation process. Our verification\nmethod considers an initial string and the subsequent top-$k$ tokens while\naccommodating diverse text quantification methods, such as evaluating text\nquality and biases. The threshold $\\alpha$ further reduces the selected tokens,\nonly choosing those that exceed or meet it in cumulative probability.\nLLMCHECKER then allows us to formally verify the PCTL properties of\n$\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in\nseveral LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our\nknowledge, this is the first time PCTL-based model checking has been used to\ncheck the consistency of the LLM text generation process.", "AI": {"tldr": "LLMCHECKER is a model-checking-based verification method that verifies probabilistic computation tree logic (PCTL) properties of LLM text generation processes using \u03b1-k-bounded text generation.", "motivation": "Current LLM text generation processes lack formal verification methods to ensure consistency and reliability. The observation that only limited tokens are typically chosen during generation, but not always the same ones, motivates the need for formal verification.", "method": "Uses \u03b1-k-bounded text generation that focuses on \u03b1 maximal cumulative probability on top-k tokens at each generation step. Considers initial strings and subsequent top-k tokens while accommodating various text quantification methods like quality and bias evaluation.", "result": "Successfully demonstrated applicability on multiple LLMs including Llama, Gemma, Mistral, Genstruct, and BERT. Provides formal verification of PCTL properties for bounded LLMs.", "conclusion": "This represents the first application of PCTL-based model checking for verifying consistency in LLM text generation processes, offering a formal verification framework for LLM reliability."}}
{"id": "2509.18144", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18144", "abs": "https://arxiv.org/abs/2509.18144", "authors": ["Yubo Yang", "Yichen Zhu", "Bo Jiang"], "title": "AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation", "comment": "9 pages", "summary": "Spatio-temporal data abounds in domain like traffic and environmental\nmonitoring. However, it often suffers from missing values due to sensor\nmalfunctions, transmission failures, etc. Recent years have seen continued\nefforts to improve spatio-temporal data imputation performance. Recently\ndiffusion models have outperformed other approaches in various tasks, including\nspatio-temporal imputation, showing competitive performance. Extracting and\nutilizing spatio-temporal dependencies as conditional information is vital in\ndiffusion-based methods. However, previous methods introduce error accumulation\nin this process and ignore the variability of the dependencies in the noisy\ndata at different diffusion steps. In this paper, we propose AdaSTI (Adaptive\nDependency Model in Diffusion-based Spatio-Temporal Imputation), a novel\nspatio-temporal imputation approach based on conditional diffusion model.\nInside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model\nfor pre-imputation with the imputed result used to extract conditional\ninformation by our designed Spatio-Temporal Conditionalizer (STC)network. We\nalso propose a Noise-Aware Spatio-Temporal (NAST) network with a gated\nattention mechanism to capture the variant dependencies across diffusion steps.\nExtensive experiments on three real-world datasets show that AdaSTI outperforms\nexisting methods in all the settings, with up to 46.4% reduction in imputation\nerror.", "AI": {"tldr": "AdaSTI is a novel spatio-temporal imputation method using conditional diffusion models that addresses error accumulation and dependency variability issues in previous approaches, achieving up to 46.4% error reduction.", "motivation": "Existing diffusion-based spatio-temporal imputation methods suffer from error accumulation when extracting conditional dependencies and ignore how these dependencies vary across different diffusion steps in noisy data.", "method": "Proposes AdaSTI with three key components: BiS4PI network for pre-imputation using bi-directional S4 model, Spatio-Temporal Conditionalizer (STC) network to extract conditional information, and Noise-Aware Spatio-Temporal (NAST) network with gated attention to capture varying dependencies across diffusion steps.", "result": "Extensive experiments on three real-world datasets show AdaSTI outperforms existing methods in all settings, achieving up to 46.4% reduction in imputation error.", "conclusion": "AdaSTI effectively addresses limitations of previous diffusion-based methods by adaptively handling spatio-temporal dependencies across diffusion steps, demonstrating superior performance for spatio-temporal data imputation."}}
{"id": "2509.18846", "categories": ["cs.AI", "I.2.6; I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2509.18846", "abs": "https://arxiv.org/abs/2509.18846", "authors": ["Hong-Jie Dai", "Zheng-Hao Li", "An-Tai Lu", "Bo-Tsz Shain", "Ming-Ta Li", "Tatheer Hussain Mir", "Kuang-Te Wang", "Min-I Su", "Pei-Kang Liu", "Ming-Ju Tsai"], "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning", "comment": "28 Pages, 4 Figures, 2 Tables", "summary": "Accurate International Classification of Diseases (ICD) coding is critical\nfor clinical documentation, billing, and healthcare analytics, yet it remains a\nlabour-intensive and error-prone task. Although large language models (LLMs)\nshow promise in automating ICD coding, their challenges in base model\nselection, input contextualization, and training data redundancy limit their\neffectiveness. We propose a modular framework for ICD-10 Clinical Modification\n(ICD-10-CM) code prediction that addresses these challenges through principled\nmodel selection, redundancy-aware data sampling, and structured input design.\nThe framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce\naggregation to assess and rank open-source LLMs based on their intrinsic\ncomprehension of ICD-10-CM code definitions. We introduced embedding-based\nsimilarity measures, a redundancy-aware sampling strategy to remove\nsemantically duplicated discharge summaries. We leverage structured discharge\nsummaries from Taiwanese hospitals to evaluate contextual effects and examine\nsection-wise content inclusion under universal and section-specific modelling\nparadigms. Experiments across two institutional datasets demonstrate that the\nselected base model after fine-tuning consistently outperforms baseline LLMs in\ninternal and external evaluations. Incorporating more clinical sections\nconsistently improves prediction performance. This study uses open-source LLMs\nto establish a practical and principled approach to ICD-10-CM code prediction.\nThe proposed framework provides a scalable, institution-ready solution for\nreal-world deployment of automated medical coding systems by combining informed\nmodel selection, efficient data refinement, and context-aware prompting.", "AI": {"tldr": "A modular framework for ICD-10-CM code prediction using LLMs with principled model selection, redundancy-aware data sampling, and structured input design to improve automated medical coding accuracy.", "motivation": "ICD coding is labor-intensive and error-prone, and current LLM approaches face challenges in model selection, input contextualization, and training data redundancy that limit their effectiveness in automated medical coding.", "method": "Proposes a framework with LLM-as-judge evaluation using Plackett-Luce aggregation for model selection, embedding-based similarity measures for redundancy-aware sampling, and structured discharge summaries with section-wise content analysis under different modeling paradigms.", "result": "Experiments show the selected fine-tuned base model consistently outperforms baseline LLMs in internal and external evaluations, with performance improving as more clinical sections are incorporated.", "conclusion": "The framework provides a scalable, institution-ready solution for real-world deployment of automated medical coding systems through informed model selection, efficient data refinement, and context-aware prompting."}}
{"id": "2509.18145", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18145", "abs": "https://arxiv.org/abs/2509.18145", "authors": ["Syed Ahmad Chan Bukhari", "Amritpal Singh", "Shifath Hossain", "Iram Wajahat"], "title": "Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records", "comment": "7 pages, 3 Figure", "summary": "Intensive Care Unit (ICU) patients often present with complex, overlapping\nsigns of physiological deterioration that require timely escalation of care.\nTraditional early warning systems, such as SOFA or MEWS, are limited by their\nfocus on single outcomes and fail to capture the multi-dimensional nature of\nclinical decline. This study proposes a multi-label classification framework to\npredict Care Escalation Triggers (CETs), including respiratory failure,\nhemodynamic instability, renal compromise, and neurological deterioration,\nusing the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are\ndefined through rule-based criteria applied to data from hours 24 to 72 (for\nexample, oxygen saturation below 90, mean arterial pressure below 65 mmHg,\ncreatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale\nscore greater than 2). Features are extracted from the first 24 hours and\ninclude vital sign aggregates, laboratory values, and static demographics. We\ntrain and evaluate multiple classification models on a cohort of 85,242 ICU\nstays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation\nmetrics include per-label precision, recall, F1-score, and Hamming loss.\nXGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,\n0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,\noutperforming baseline models. Feature analysis shows that clinically relevant\nparameters such as respiratory rate, blood pressure, and creatinine are the\nmost influential predictors, consistent with the clinical definitions of the\nCETs. The proposed framework demonstrates practical potential for early,\ninterpretable clinical alerts without requiring complex time-series modeling or\nnatural language processing.", "AI": {"tldr": "A multi-label classification framework using XGBoost to predict Care Escalation Triggers (respiratory, hemodynamic, renal, neurological deterioration) from first 24 hours of ICU data, achieving F1-scores of 0.62-0.76 and outperforming traditional early warning systems.", "motivation": "Traditional ICU early warning systems like SOFA or MEWS are limited by single-outcome focus and fail to capture multi-dimensional clinical decline, requiring a more comprehensive approach to predict various types of physiological deterioration.", "method": "Multi-label classification framework using MIMIC-IV database (85,242 ICU stays) with features from first 24 hours (vital sign aggregates, lab values, demographics) to predict CETs defined by rule-based criteria from hours 24-72. XGBoost was the best performing model among multiple classifiers evaluated.", "result": "XGBoost achieved F1-scores: 0.66 (respiratory), 0.72 (hemodynamic), 0.76 (renal), 0.62 (neurologic). Feature analysis showed clinically relevant parameters (respiratory rate, blood pressure, creatinine) were most influential predictors, aligning with clinical CET definitions.", "conclusion": "The framework demonstrates practical potential for early, interpretable clinical alerts without requiring complex time-series modeling or NLP, offering a more comprehensive approach than traditional single-outcome warning systems."}}
{"id": "2509.18849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18849", "abs": "https://arxiv.org/abs/2509.18849", "authors": ["Wenke Huang", "Quan Zhang", "Yiyang Fang", "Jian Liang", "Xuankun Rong", "Huanjin Yao", "Guancheng Wan", "Ke Liang", "Wenwen He", "Mingjun Li", "Leszek Rutkowski", "Mang Ye", "Bo Du", "Dacheng Tao"], "title": "MAPO: Mixed Advantage Policy Optimization", "comment": null, "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach.", "AI": {"tldr": "Proposes Mixed Advantage Policy Optimization (MAPO) to address advantage reversion and mirror problems in GRPO by dynamically reweighting advantage functions based on trajectory certainty.", "motivation": "Existing GRPO methods suffer from advantage reversion and advantage mirror problems that hinder reasonable advantage allocation across query samples.", "method": "Introduces advantage percent deviation for high-certainty trajectories and dynamically reweights advantage functions based on trajectory certainty to adapt to sample-specific characteristics.", "result": "Comparison with state-of-the-art methods and ablation studies validate the effectiveness of MAPO approach.", "conclusion": "MAPO provides an easy but effective strategy to improve advantage allocation in reinforcement learning for foundation models."}}
{"id": "2509.18147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18147", "abs": "https://arxiv.org/abs/2509.18147", "authors": ["Xinyu Mu", "Hui Dou", "Furao Shen", "Jian Zhao"], "title": "ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks", "comment": null, "summary": "Concept-based interpretability for Convolutional Neural Networks (CNNs) aims\nto align internal model representations with high-level semantic concepts, but\nexisting approaches largely overlook the semantic roles of individual filters\nand the dynamic propagation of concepts across layers. To address these\nlimitations, we propose ConceptFlow, a concept-based interpretability framework\nthat simulates the internal \"thinking path\" of a model by tracing how concepts\nemerge and evolve across layers. ConceptFlow comprises two key components: (i)\nconcept attentions, which associate each filter with relevant high-level\nconcepts to enable localized semantic interpretation, and (ii) conceptual\npathways, derived from a concept transition matrix that quantifies how concepts\npropagate and transform between filters. Together, these components offer a\nunified and structured view of internal model reasoning. Experimental results\ndemonstrate that ConceptFlow yields semantically meaningful insights into model\nreasoning, validating the effectiveness of concept attentions and conceptual\npathways in explaining decision behavior. By modeling hierarchical conceptual\npathways, ConceptFlow provides deeper insight into the internal logic of CNNs\nand supports the generation of more faithful and human-aligned explanations.", "AI": {"tldr": "ConceptFlow is a concept-based interpretability framework for CNNs that traces how concepts emerge and evolve across layers through concept attentions and conceptual pathways.", "motivation": "Existing CNN interpretability approaches overlook the semantic roles of individual filters and the dynamic propagation of concepts across layers, limiting understanding of internal model reasoning.", "method": "ConceptFlow uses concept attentions to associate filters with high-level concepts and conceptual pathways derived from a concept transition matrix to trace concept propagation between filters.", "result": "Experimental results show ConceptFlow provides semantically meaningful insights into model reasoning and validates the effectiveness of concept attentions and conceptual pathways.", "conclusion": "ConceptFlow offers deeper insight into CNN internal logic and supports generation of more faithful, human-aligned explanations by modeling hierarchical conceptual pathways."}}
{"id": "2509.18864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18864", "abs": "https://arxiv.org/abs/2509.18864", "authors": ["Yingxin Li", "Jianbo Zhao", "Xueyu Ren", "Jie Tang", "Wangjie You", "Xu Chen", "Kan Zhou", "Chao Feng", "Jiao Ran", "Yuan Meng", "Zhi Wang"], "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling", "comment": null, "summary": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B.", "AI": {"tldr": "ProfileBench is a new benchmark for user profiling using LLMs, and Conf-Profile is a confidence-driven framework that achieves label-free user profiling through two-stage training with confidence-weighted voting and reinforcement learning.", "motivation": "Current user profiling with LLMs lacks comprehensive benchmarks and struggles with label scarcity, heterogeneous/noisy data, and reliability issues.", "method": "Two-stage framework: 1) Synthesize high-quality labels using LLMs with confidence hints, then use confidence-weighted voting and calibration; 2) Distill results into lightweight LLM and enhance reasoning via confidence-guided unsupervised reinforcement learning.", "result": "Conf-Profile significantly improves performance, achieving 13.97 F1 improvement on Qwen3-8B model.", "conclusion": "The proposed confidence-driven framework effectively addresses label scarcity and data reliability issues in user profiling, demonstrating substantial performance gains through the two-stage training approach."}}
{"id": "2509.18150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18150", "abs": "https://arxiv.org/abs/2509.18150", "authors": ["Kean Shi", "Liang Chen", "Haozhe Zhao", "Baobao Chang"], "title": "Sparse Training Scheme for Multimodal LLM", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding\nperformance across a variety of domains. However, training MLLMs is often\ninefficient due to the significantly longer input sequences introduced by\nmultimodal data and the low utilization of inter-layer computations. To address\nthis challenge, we shift the focus to the training process itself and propose a\nnovel training-efficient framework based on sparse representations, termed the\nSparse Training Scheme (STS). This scheme consists of two key components: the\nVisual Token Compressor, which reduces the information load by compressing\nvisual tokens, and the Layer Dynamic Skipper, which mitigates the computational\noverhead by dynamically skipping unnecessary layers in the language model\nduring both forward and backward passes. Our approach is broadly applicable to\ndiverse MLLM architectures and has been extensively evaluated on multiple\nbenchmarks, demonstrating its effectiveness and efficiency.", "AI": {"tldr": "A sparse training scheme (STS) for efficient MLLM training using visual token compression and dynamic layer skipping to reduce computational overhead.", "motivation": "Training Multimodal Large Language Models (MLLMs) is inefficient due to long input sequences from multimodal data and low utilization of inter-layer computations.", "method": "STS framework with Visual Token Compressor to reduce visual token information load and Layer Dynamic Skipper to dynamically skip unnecessary layers during forward/backward passes.", "result": "Extensively evaluated on multiple benchmarks, demonstrating effectiveness and efficiency across diverse MLLM architectures.", "conclusion": "The proposed sparse training scheme provides an efficient training framework for MLLMs while maintaining performance."}}
{"id": "2509.18868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18868", "abs": "https://arxiv.org/abs/2509.18868", "authors": ["Dianxing Zhang", "Wendong Li", "Kani Song", "Jiaye Lu", "Gang Li", "Liuchun Yang", "Sheng Li"], "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution", "comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM\n  memory mechanisms and evaluation", "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.", "AI": {"tldr": "This paper proposes a unified framework for defining, categorizing, and evaluating LLM memory systems, including a taxonomy, evaluation protocols, governance mechanisms, and testable propositions for reproducible research.", "motivation": "To establish a standardized framework for understanding and evaluating LLM memory systems across heterogeneous setups, enabling fair comparisons and systematic governance of memory operations.", "method": "Proposes a four-part memory taxonomy (parametric, contextual, external, procedural/episodic) with a memory quadruple framework, three-setting evaluation protocol, layered evaluation approach, and DMM Gov system for memory governance.", "result": "Develops a comprehensive coordinate system that integrates temporal governance, leakage auditing, uncertainty reporting, and memory updating/forgetting mechanisms with auditable loops.", "conclusion": "The framework provides a reproducible, comparable, and governable system for LLM memory research and deployment, with four testable propositions to guide future work."}}
{"id": "2509.18151", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18151", "abs": "https://arxiv.org/abs/2509.18151", "authors": ["Jindi Lv", "Yuhao Zhou", "Yuxin Tian", "Qing Ye", "Wentao Feng", "Jiancheng Lv"], "title": "HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork", "comment": null, "summary": "Time-intensive performance evaluations significantly impede progress in\nNeural Architecture Search (NAS). To address this, neural predictors leverage\nsurrogate models trained on proxy datasets, allowing for direct performance\npredictions for new architectures. However, these predictors often exhibit poor\ngeneralization due to their limited ability to capture intricate relationships\namong various architectures. In this paper, we propose HyperNAS, a novel neural\npredictor paradigm for enhancing architecture representation learning. HyperNAS\nconsists of two primary components: a global encoding scheme and a shared\nhypernetwork. The global encoding scheme is devised to capture the\ncomprehensive macro-structure information, while the shared hypernetwork serves\nas an auxiliary task to enhance the investigation of inter-architecture\npatterns. To ensure training stability, we further develop a dynamic adaptive\nmulti-task loss to facilitate personalized exploration on the Pareto front.\nExtensive experiments across five representative search spaces, including ViTs,\ndemonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For\ninstance, HyperNAS strikes new state-of-the-art results, with 97.60\\% top-1\naccuracy on CIFAR-10 and 82.4\\% top-1 accuracy on ImageNet, using at least\n5.0$\\times$ fewer samples.", "AI": {"tldr": "HyperNAS is a novel neural predictor paradigm for Neural Architecture Search that enhances architecture representation learning through global encoding and shared hypernetwork components, achieving state-of-the-art results with significantly fewer training samples.", "motivation": "Traditional neural predictors in NAS exhibit poor generalization due to limited ability to capture intricate relationships among various architectures, and time-intensive performance evaluations impede NAS progress.", "method": "HyperNAS consists of two components: a global encoding scheme to capture comprehensive macro-structure information, and a shared hypernetwork as an auxiliary task to enhance investigation of inter-architecture patterns. It uses a dynamic adaptive multi-task loss for training stability.", "result": "HyperNAS achieves state-of-the-art results with 97.60% top-1 accuracy on CIFAR-10 and 82.4% top-1 accuracy on ImageNet, using at least 5.0\u00d7 fewer samples than previous methods.", "conclusion": "HyperNAS demonstrates superior performance across five representative search spaces, particularly in few-shot scenarios, making it an effective solution for efficient neural architecture search."}}
{"id": "2509.18883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18883", "abs": "https://arxiv.org/abs/2509.18883", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chengcheng Han", "Chenhui Yang", "Chi Zhang", "Chong Peng", "Chuyu Zhang", "Cong Chen", "Fengcun Li", "Gang Xu", "Guoyuan Lin", "Hao Jiang", "Hao Liang", "Haomin Fu", "Haoxiang Ma", "Hong Liu", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiahao Liu", "Jiahuan Li", "Jialin Liu", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiaqi Sun", "Jiaqi Zhang", "Jiarong Shi", "Jiawei Yang", "Jingang Wang", "Jinrui Ding", "Jun Kuang", "Jun Xu", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Li Wei", "Liang Shi", "Lin Qiu", "Lingbin Kong", "Lingchuan Liu", "Linsen Guo", "Longfei An", "Mai Xia", "Meng Zhou", "Mengshen Zhu", "Peng Pei", "Pengcheng Jia", "Qi Gu", "Qi Guo", "Qiong Huang", "Quan Chen", "Quanchi Weng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shanglin Lei", "Shuai Du", "Shuaikang Liu", "Shuang Zhou", "Shuhao Hu", "Siyu Xu", "Songshan Gong", "Tao Liang", "Tianhao Hu", "Wei He", "Wei Shi", "Wei Wang", "Wei Wu", "Wei Zhuo", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Xi Su", "Xiangcheng Liu", "Xiangyu Xi", "Xiangzhou Huang", "Xiao Liu", "Xiaochen Jiang", "Xiaowei Shi", "Xiaowen Shi", "Xiaoyu Li", "Xin Chen", "Xinyue Zhao", "Xuan Huang", "Xuemiao Zhang", "Xuezhi Cao", "Xunliang Cai", "Yajie Zhang", "Yang Chen", "Yang Liu", "Yang Liu", "Yang Zheng", "Yaoming Wang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yiyang Li", "Youshao Xiao", "Yuanzhe Lei", "Yuchen Xie", "Yueqing Sun", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunke Zhao", "Yuqing Ding", "Yuwei Jiang", "Zhaohua Yang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhongda Su", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang", "Zongyu Wang", "Zunyuan Yang"], "title": "LongCat-Flash-Thinking Technical Report", "comment": null, "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research.", "AI": {"tldr": "LongCat-Flash-Thinking is a 560B parameter open-source MoE model that achieves state-of-the-art reasoning performance through a novel training process combining CoT cold-start and large-scale RL with domain-parallel optimization.", "motivation": "To create an efficient large-scale reasoning model that excels in both formal and agentic reasoning tasks while being more computationally efficient than existing approaches.", "method": "Uses a cold-start training with long Chain-of-Thought data, followed by domain-parallel training that decouples optimization across STEM, Code, and Agentic domains, then fuses experts into a single model using the DORA RL framework for 3x training speedup.", "result": "Achieves SOTA performance among open-source models on complex reasoning tasks, with 64.5% reduction in token consumption on AIME-25 while maintaining accuracy.", "conclusion": "The model demonstrates exceptional efficiency in agentic reasoning and is released to advance reasoning systems and agentic AI research."}}
{"id": "2509.18152", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18152", "abs": "https://arxiv.org/abs/2509.18152", "authors": ["Zhenyu Qi", "Qing Yu", "Jichen Wang", "Yun-Bo Zhao", "Zerui Li", "Wenjun Lv"], "title": "WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation", "comment": null, "summary": "Well-log interpretation is fundamental for subsurface characterization but\nremains challenged by heterogeneous tool responses, noisy signals, and limited\nlabels. We propose WLFM, a foundation model pretrained on multi-curve logs from\n1200 wells, comprising three stages: tokenization of log patches into\ngeological tokens, self-supervised pretraining with masked-token modeling and\nstratigraphy-aware contrastive learning, and multi-task adaptation with\nfew-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,\nachieving 0.0041 MSE in porosity estimation and 74.13\\% accuracy in lithology\nclassification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\\%\naccuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,\nlearns a reusable geological vocabulary, and reconstructs masked curves with\nreasonable fidelity, though systematic offsets are observed in shallow and\nultra-deep intervals. Although boundary detection is not explicitly evaluated\nhere, clustering analyses suggest strong potential for future extension. These\nresults establish WLFM as a scalable, interpretable, and transferable backbone\nfor geological AI, with implications for multi-modal integration of logs,\nseismic, and textual data.", "AI": {"tldr": "WLFM is a foundation model for well-log interpretation that uses multi-stage pretraining on 1200 wells, achieving state-of-the-art performance in porosity estimation and lithology classification through tokenization, self-supervised learning, and few-shot fine-tuning.", "motivation": "Well-log interpretation faces challenges from heterogeneous tool responses, noisy signals, and limited labeled data, requiring more robust and scalable AI solutions for subsurface characterization.", "method": "Three-stage approach: (1) tokenization of log patches into geological tokens, (2) self-supervised pretraining with masked-token modeling and stratigraphy-aware contrastive learning, (3) multi-task adaptation with few-shot fine-tuning.", "result": "WLFM achieves 0.0041 MSE in porosity estimation and 74.13% accuracy in lithology classification. With fine-tuning (WLFM-Finetune), performance improves to 0.0038 MSE and 78.10% accuracy. The model also exhibits emergent layer-awareness and learns reusable geological vocabulary.", "conclusion": "WLFM establishes a scalable, interpretable, and transferable backbone for geological AI with potential for multi-modal integration of logs, seismic, and textual data, though systematic offsets remain in shallow and ultra-deep intervals."}}
{"id": "2509.18905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18905", "abs": "https://arxiv.org/abs/2509.18905", "authors": ["Songsong Yu", "Yuxin Chen", "Hao Ju", "Lianjie Jia", "Fuxi Zhang", "Shaofei Huang", "Yuhan Wu", "Rundi Cui", "Binghao Ran", "Zaibin Zhang", "Zhedong Zheng", "Zhipeng Zhang", "Yifan Wang", "Lin Song", "Lijun Wang", "Yanwei Li", "Ying Shan", "Huchuan Lu"], "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective", "comment": "a comprehensive visual spatial reasoning evaluation tool, 25 pages,\n  16 figures", "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.", "AI": {"tldr": "This paper presents a systematic investigation of Visual Spatial Reasoning (VSR) in Vision-Language Models, categorizing spatial intelligence into three capability levels and introducing SIBench, a comprehensive benchmark with 20 datasets across 23 tasks.", "motivation": "Visual Spatial Reasoning is a critical human cognitive ability essential for advancing embodied intelligence and autonomous systems, but current VLMs struggle with representing and reasoning over 3D space.", "method": "The study reviews existing VSR methodologies across input modalities, model architectures, training strategies, and reasoning mechanisms, and creates SIBench benchmark with systematic categorization of spatial intelligence capabilities.", "result": "Experiments with state-of-the-art VLMs reveal a significant gap between perception and reasoning - models perform well on basic perceptual tasks but consistently underperform in understanding and planning tasks, especially in numerical estimation, multi-view reasoning, temporal dynamics, and spatial imagination.", "conclusion": "There remain substantial challenges in achieving spatial intelligence, but this study provides both a systematic roadmap and comprehensive benchmark to guide future research in visual spatial reasoning."}}
{"id": "2509.18153", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.18153", "abs": "https://arxiv.org/abs/2509.18153", "authors": ["Hanqun Cao", "Marcelo D. T. Torres", "Jingjie Zhang", "Zijun Gao", "Fang Wu", "Chunbin Gu", "Jure Leskovec", "Yejin Choi", "Cesar de la Fuente-Nunez", "Guangyong Chen", "Pheng-Ann Heng"], "title": "A deep reinforcement learning platform for antibiotic discovery", "comment": "42 pages, 16 figures", "summary": "Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths\nannually by 2050, underscoring the urgent need for new antibiotics. Here we\npresent ApexAmphion, a deep-learning framework for de novo design of\nantibiotics that couples a 6.4-billion-parameter protein language model with\nreinforcement learning. The model is first fine-tuned on curated peptide data\nto capture antimicrobial sequence regularities, then optimised with proximal\npolicy optimization against a composite reward that combines predictions from a\nlearned minimum inhibitory concentration (MIC) classifier with differentiable\nphysicochemical objectives. In vitro evaluation of 100 designed peptides showed\nlow MIC values (nanomolar range in some cases) for all candidates (100% hit\nrate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial\nactivity against at least two clinically relevant bacteria. The lead molecules\nkilled bacteria primarily by potently targeting the cytoplasmic membrane. By\nunifying generation, scoring and multi-objective optimization with deep\nreinforcement learning in a single pipeline, our approach rapidly produces\ndiverse, potent candidates, offering a scalable route to peptide antibiotics\nand a platform for iterative steering toward potency and developability within\nhours.", "AI": {"tldr": "ApexAmphion is a deep learning framework that combines a 6.4B-parameter protein language model with reinforcement learning to design novel antibiotics, achieving 100% hit rate with nanomolar potency in vitro.", "motivation": "Antimicrobial resistance is projected to cause 10 million annual deaths by 2050, creating an urgent need for new antibiotics.", "method": "Fine-tunes a protein language model on peptide data, then uses proximal policy optimization with composite rewards combining MIC predictions and physicochemical objectives.", "result": "100% hit rate with low MIC values (nanomolar range), 99/100 compounds showed broad-spectrum activity against clinically relevant bacteria via membrane targeting.", "conclusion": "The approach provides a scalable platform for rapid generation of diverse, potent peptide antibiotics with iterative optimization capabilities."}}
{"id": "2509.18942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18942", "abs": "https://arxiv.org/abs/2509.18942", "authors": ["Xiao Han", "Zimo Zhao", "Wanyu Wang", "Maolin Wang", "Zitao Liu", "Yi Chang", "Xiangyu Zhao"], "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, con- ventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the lim- itations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outper- forms baseline methods, yielding\nsubstantial gains in task accuracy and resource efficiency. These findings\ndemonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency.", "AI": {"tldr": "DEAL is a novel framework that integrates Low-Rank Adaptation (LoRA) with continuous fine-tuning to address catastrophic forgetting and suboptimal data efficiency in LLM fine-tuning.", "motivation": "Conventional fine-tuning approaches suffer from catastrophic forgetting and poor data efficiency, limiting their real-world applicability despite the importance of fine-tuning for adapting LLMs to specific tasks.", "method": "DEAL integrates LoRA with a continuous fine-tuning strategy, incorporating knowledge retention and adaptive parameter update modules to mitigate limitations of existing FT methods while maintaining efficiency in privacy-preserving settings.", "result": "Experiments on 15 diverse datasets show DEAL consistently outperforms baseline methods with substantial gains in task accuracy and resource efficiency.", "conclusion": "The approach demonstrates potential to advance continual adaptation in LLMs by enhancing task performance while improving resource efficiency."}}
{"id": "2509.18154", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18154", "abs": "https://arxiv.org/abs/2509.18154", "authors": ["Tianyu Yu", "Zefan Wang", "Chongyi Wang", "Fuwei Huang", "Wenshuo Ma", "Zhihui He", "Tianchi Cai", "Weize Chen", "Yuxiang Huang", "Yuanqian Zhao", "Bokai Xu", "Junbo Cui", "Yingjing Xu", "Liqing Ruan", "Luoyuan Zhang", "Hanyu Liu", "Jingkun Tang", "Hongyuan Liu", "Qining Guo", "Wenhao Hu", "Bingxiang He", "Jie Zhou", "Jie Cai", "Ji Qi", "Zonghao Guo", "Chi Chen", "Guoyang Zeng", "Yuxuan Li", "Ganqu Cui", "Ning Ding", "Xu Han", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe", "comment": "Project Website: https://github.com/OpenBMB/MiniCPM-V", "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.", "AI": {"tldr": "MiniCPM-V 4.5 is an 8B parameter multimodal LLM that achieves state-of-the-art performance with exceptional efficiency, surpassing larger proprietary and open-source models while using significantly less GPU memory and inference time.", "motivation": "Address the training and inference efficiency bottlenecks in multimodal LLMs to make them more accessible and scalable, as current models face challenges with computational resource requirements.", "method": "Three core improvements: 1) Unified 3D-Resampler architecture for compact image/video encoding, 2) Unified learning paradigm for document knowledge and text recognition without heavy data engineering, 3) Hybrid reinforcement learning strategy for both short and long reasoning modes.", "result": "Surpasses GPT-4o-latest and Qwen2.5-VL 72B in OpenCompass evaluation. On VideoMME benchmark, achieves SOTA performance among models under 30B size with only 46.7% GPU memory cost and 8.7% inference time of Qwen2.5-VL 7B.", "conclusion": "MiniCPM-V 4.5 demonstrates that strong multimodal performance can be achieved with remarkable efficiency through architectural innovations and optimized training strategies, making advanced MLLMs more accessible."}}
{"id": "2509.18970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18970", "abs": "https://arxiv.org/abs/2509.18970", "authors": ["Xixun Lin", "Yucheng Ning", "Jingwen Zhang", "Yan Dong", "Yilong Liu", "Yongxuan Wu", "Xiaohua Qi", "Nan Sun", "Yanmin Shang", "Pengfei Cao", "Lixin Zou", "Xu Chen", "Chuan Zhou", "Jia Wu", "Shirui Pan", "Bin Wang", "Yanan Cao", "Kai Chen", "Songlin Hu", "Li Guo"], "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions", "comment": null, "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.", "AI": {"tldr": "This paper presents the first comprehensive survey on hallucinations in LLM-based agents, analyzing their workflow, proposing a taxonomy of hallucination types, examining triggering causes, and summarizing mitigation approaches.", "motivation": "LLM-based agents are increasingly deployed in real-world applications but remain vulnerable to hallucinations that undermine system reliability, requiring systematic understanding and consolidation of recent advances.", "method": "The authors analyze the complete workflow of LLM-based agents, propose a new taxonomy of hallucination types occurring at different stages, examine eighteen triggering causes, and review existing studies on mitigation and detection approaches.", "result": "The survey provides a comprehensive framework for understanding agent hallucinations, including their classification, underlying causes, and current mitigation strategies.", "conclusion": "This work aims to inspire further research on addressing hallucinations in LLM-based agents to develop more robust and reliable agent systems."}}
{"id": "2509.18161", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18161", "abs": "https://arxiv.org/abs/2509.18161", "authors": ["William H Patty"], "title": "Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks", "comment": null, "summary": "Activation functions in neural networks are typically selected from a set of\nempirically validated, commonly used static functions such as ReLU, tanh, or\nsigmoid. However, by optimizing the shapes of a network's activation functions,\nwe can train models that are more parameter-efficient and accurate by assigning\nmore optimal activations to the neurons. In this paper, I present and compare 9\ntraining methodologies to explore dual-optimization dynamics in neural networks\nwith parameterized linear B-spline activation functions. The experiments\nrealize up to 94% lower end model error rates in FNNs and 51% lower rates in\nCNNs compared to traditional ReLU-based models. These gains come at the cost of\nadditional development and training complexity as well as end model latency.", "AI": {"tldr": "This paper explores optimizing activation function shapes using parameterized linear B-splines, achieving significant error rate reductions in FNNs (94%) and CNNs (51%) compared to ReLU-based models, though with increased complexity and latency.", "motivation": "Traditional neural networks use static activation functions like ReLU, tanh, or sigmoid, which may not be optimal. By optimizing activation function shapes, we can create more parameter-efficient and accurate models by assigning better activations to neurons.", "method": "The paper presents and compares 9 training methodologies for dual-optimization dynamics in neural networks using parameterized linear B-spline activation functions.", "result": "Experiments show up to 94% lower end model error rates in FNNs and 51% lower rates in CNNs compared to traditional ReLU-based models.", "conclusion": "While optimized activation functions provide substantial accuracy improvements, these gains come at the cost of additional development and training complexity as well as increased end model latency."}}
{"id": "2509.18980", "categories": ["cs.AI", "cs.HC", "cs.IR", "H.3.3; H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18980", "abs": "https://arxiv.org/abs/2509.18980", "authors": ["Maxime Manderlier", "Fabian Lecron", "Olivier Vu Thanh", "Nicolas Gillis"], "title": "From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system", "comment": null, "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.", "AI": {"tldr": "This paper investigates whether LLMs can generate effective explanations from an interpretable recommendation model, using user-centered evaluation with 326 participants to assess explanation quality across multiple dimensions.", "motivation": "Many explainable AI works rely on automatic metrics that fail to capture users' actual needs and perceptions, so the authors aim to adopt a more user-centered approach to evaluate explanation quality.", "method": "The study uses constrained matrix factorization with explicit user type representations, then translates model outputs into natural language explanations using carefully designed LLM prompts. Multiple explanation types are generated from the same model by varying LLM input information.", "result": "All explanation types were generally well received with moderate statistical differences between strategies. User comments provided complementary insights beyond quantitative results.", "conclusion": "LLMs can effectively generate user-facing explanations from interpretable recommendation models, and user-centered evaluation provides valuable insights into explanation quality that automatic metrics may miss."}}
{"id": "2509.18162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18162", "abs": "https://arxiv.org/abs/2509.18162", "authors": ["Meraryslan Meraliyev", "Cemil Turan", "Shirali Kadyrov"], "title": "A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge", "comment": null, "summary": "We study last-mile delivery with one truck and one drone under explicit\nbattery management: the drone flies at twice the truck speed; each sortie must\nsatisfy an endurance budget; after every delivery the drone recharges on the\ntruck before the next launch. We introduce a hybrid reinforcement learning (RL)\nsolver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a\nsmall pointer/attention policy that schedules drone sorties. The policy decodes\nlaunch--serve--rendezvous triplets with hard feasibility masks for endurance\nand post-delivery recharge; a fast, exact timeline simulator enforces\nlaunch/recovery handling and computes the true makespan used by masked\ngreedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and\n$R{=}0.1$, the method achieves an average makespan of \\textbf{5.203}$\\pm$0.093,\nversus \\textbf{5.349}$\\pm$0.038 for ALNS and \\textbf{5.208}$\\pm$0.124 for NN --\ni.e., \\textbf{2.73\\%} better than ALNS on average and within \\textbf{0.10\\%} of\nNN. Per-seed, the RL scheduler never underperforms ALNS on the same instance\nand ties or beats NN on two of three seeds. A decomposition of the makespan\nshows the expected truck--wait trade-off across heuristics; the learned\nscheduler balances both to minimize the total completion time. We provide a\nconfig-first implementation with plotting and significance-test utilities to\nsupport replication.", "AI": {"tldr": "Hybrid RL solver for truck-drone delivery with battery constraints achieves 2.73% better makespan than ALNS and matches neural network performance.", "motivation": "To optimize last-mile delivery with one truck and one drone under explicit battery management constraints, where each drone sortie must satisfy endurance limits and requires recharging between deliveries.", "method": "Hybrid reinforcement learning solver combining ALNS-based truck tour optimization with pointer/attention policy for drone scheduling. Uses feasibility masks for endurance and recharge constraints, with exact timeline simulation for makespan computation.", "result": "Achieves average makespan of 5.203\u00b10.093 vs 5.349\u00b10.038 for ALNS and 5.208\u00b10.124 for NN - 2.73% better than ALNS and within 0.10% of NN. RL scheduler never underperforms ALNS and ties/beats NN on 2/3 seeds.", "conclusion": "The learned scheduler effectively balances truck and wait times to minimize total completion time, demonstrating superior performance over traditional optimization methods while handling complex battery constraints."}}
{"id": "2509.18986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18986", "abs": "https://arxiv.org/abs/2509.18986", "authors": ["Erik Penther", "Michael Grohs", "Jana-Rebecca Rehse"], "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)", "comment": "Short paper at the ML4PM Workshop 2025, held in conjunction with the\n  ICPM 2025 in Montevideo, Uruguay", "summary": "Predictive process monitoring is a sub-domain of process mining which aims to\nforecast the future of ongoing process executions. One common prediction target\nis the remaining time, meaning the time that will elapse until a process\nexecution is completed. In this paper, we compare four different remaining time\nprediction approaches in a real-life outbound warehouse process of a logistics\ncompany in the aviation business. For this process, the company provided us\nwith a novel and original event log with 169,523 traces, which we can make\npublicly available. Unsurprisingly, we find that deep learning models achieve\nthe highest accuracy, but shallow methods like conventional boosting techniques\nachieve competitive accuracy and require significantly fewer computational\nresources.", "AI": {"tldr": "Comparison of four remaining time prediction approaches in a real-life logistics warehouse process using a novel event log with 169,523 traces", "motivation": "To forecast the remaining time until process completion in predictive process monitoring, specifically for logistics operations", "method": "Evaluated four different remaining time prediction approaches including deep learning models and shallow methods like conventional boosting techniques", "result": "Deep learning models achieved highest accuracy, but shallow methods like boosting achieved competitive accuracy with significantly fewer computational resources", "conclusion": "While deep learning provides best accuracy, shallow methods offer competitive performance with better computational efficiency for remaining time prediction in process mining"}}
{"id": "2509.18164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18164", "abs": "https://arxiv.org/abs/2509.18164", "authors": ["Ranfei Chen", "Ming Chen"], "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns", "comment": null, "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture\nfollowing auto regressive models. Their denoising process offers a powerful\ngenerative advantage, but they present significant challenges in learning and\nunderstanding numerically sensitive mathematical and order-sensitive logical\ntasks. Current training methods, including pre-training, fine-tuning, and\nreinforcement learning, focus primarily on improving general knowledge\nretention and reasoning abilities, but lack a comprehensive understanding of\nmathematical and logical patterns. We propose DSFT, a simple yet effective\nDiffusion SFT strategy, by adjusting the masking strategy and loss function,\nguiding models to understand mathematical and logical patterns. This strategy\ncan be flexibly combined with pre-training, reinforcement learning, and other\ntraining methods. Validated on models such as LLaDA and Dream series, we prove\nthat DSFT on small-scale data can achieve improvements of 5-10% and\napproximately 2% on mathematical and logical problems, respectively. This\ninspiring masking approach offers insights for future learning of specific\npatterns, which can be easily and efficiently combined with other training\nmethods and applied to various dLLMs. Our code is publicly available at\nhttps://anonymous.4open.science/r/DSFT-0FFB/", "AI": {"tldr": "DSFT is a Diffusion SFT strategy that improves dLLMs' performance on mathematical and logical tasks by adjusting masking strategy and loss function, achieving 5-10% improvements on math problems and 2% on logical problems.", "motivation": "Current dLLM training methods focus on general knowledge but lack comprehensive understanding of mathematically sensitive and order-sensitive logical tasks, which are challenging for diffusion models.", "method": "Proposes DSFT (Diffusion SFT) strategy with adjusted masking strategy and loss function to guide models in understanding mathematical and logical patterns. Can be combined with pre-training, reinforcement learning, and other methods.", "result": "Validated on LLaDA and Dream series models, DSFT achieves 5-10% improvement on mathematical problems and approximately 2% improvement on logical problems using small-scale data.", "conclusion": "DSFT provides an effective masking approach for learning specific patterns that can be easily combined with other training methods and applied to various dLLMs."}}
{"id": "2509.19030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19030", "abs": "https://arxiv.org/abs/2509.19030", "authors": ["Victoire Herv\u00e9", "Henrik Warpefelt", "Christoph Salge"], "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action", "comment": null, "summary": "Algorithmic evaluation of procedurally generated content struggles to find\nmetrics that align with human experience, particularly for composite artefacts.\nAutomatic decomposition as a possible solution requires concepts that meet a\nrange of properties. To this end, drawing on Games Studies and Game AI\nresearch, we introduce the nested concepts of \\textit{Landmarks},\n\\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the\nartefact's perceivability, evocativeness, and Call to Action, all from a\nplayer-centric perspective. These terms are generic to games and usable across\ngenres. We argue that these entities can be found and evaluated with techniques\ncurrently used in both research and industry, opening a path towards a fully\nautomated decomposition of PCG, and evaluation of the salient sub-components.\nAlthough the work presented here emphasises mixed-initiative PCG and\ncompositional PCG, we believe it applies beyond those domains. With this\napproach, we intend to create a connection between humanities and technical\ngame research and allow for better computational PCG evaluation", "AI": {"tldr": "The paper introduces nested concepts of Landmarks, Monuments, and Beacons for automated decomposition and evaluation of procedurally generated content in games, aiming to bridge humanities and technical game research.", "motivation": "Algorithmic evaluation of procedurally generated content struggles to find metrics that align with human experience, particularly for composite artefacts. Current methods lack player-centric perspectives.", "method": "Drawing on Games Studies and Game AI research, the authors introduce three nested concepts based on perceivability, evocativeness, and Call to Action from a player-centric perspective. These concepts are designed to be generic across game genres and can be evaluated using existing techniques.", "result": "The proposed framework provides a path towards fully automated decomposition of PCG and evaluation of salient sub-components, with applicability beyond mixed-initiative PCG and compositional PCG.", "conclusion": "This approach creates a connection between humanities and technical game research, enabling better computational PCG evaluation through player-centric concepts that can be automatically identified and assessed."}}
{"id": "2509.18166", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18166", "abs": "https://arxiv.org/abs/2509.18166", "authors": ["Xiaoqian Qi", "Haoye Chai", "Yong Li"], "title": "MobiGPT: A Foundation Model for Mobile Wireless Networks", "comment": null, "summary": "With the rapid development of mobile communication technologies, future\nmobile networks will offer vast services and resources for commuting,\nproduction, daily life, and entertainment. Accurate and efficient forecasting\nof mobile data (e.g., cell traffic, user behavior, channel quality) helps\noperators monitor network state changes, orchestrate wireless resources, and\nschedule infrastructure and users, thereby improving supply efficiency and\nservice quality. However, current forecasting paradigms rely on customized\ndesigns with tailored models for exclusive data types. Such approaches increase\ncomplexity and deployment costs under large-scale, heterogeneous networks\ninvolving base stations, users, and channels. In this paper, we design a\nfoundation model for mobile data forecasting, MobiGPT, with a unified structure\ncapable of forecasting three data types: base station traffic, user app usage,\nand channel quality. We propose a soft-prompt learning method to help the model\nunderstand features of different data types, and introduce a temporal masking\nmechanism to guide the model through three forecasting tasks: short-term\nprediction, long-term prediction, and distribution generation, supporting\ndiverse optimization scenarios. Evaluations on real-world datasets with over\n100,000 samples show that MobiGPT achieves accurate multi-type forecasting.\nCompared to existing models, it improves forecasting accuracy by 27.37%,\n20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits\nsuperior zero/few-shot performance in unseen scenarios, with over 21.51%\nimprovement, validating its strong transferability as a foundation model.", "AI": {"tldr": "MobiGPT is a foundation model for mobile data forecasting that can handle multiple data types (base station traffic, user app usage, channel quality) using a unified structure with soft-prompt learning and temporal masking mechanisms.", "motivation": "Current mobile data forecasting approaches require customized models for each data type, increasing complexity and deployment costs in large-scale heterogeneous networks. There's a need for a unified foundation model that can handle multiple forecasting tasks efficiently.", "method": "Proposes MobiGPT with soft-prompt learning to understand different data types and temporal masking mechanism to guide three forecasting tasks: short-term prediction, long-term prediction, and distribution generation.", "result": "Evaluations on real-world datasets with over 100,000 samples show MobiGPT achieves accurate multi-type forecasting with 27.37%, 20.08%, and 7.27% accuracy improvements over existing models, plus superior zero/few-shot performance with over 21.51% improvement in unseen scenarios.", "conclusion": "MobiGPT demonstrates strong generalization and transferability as a foundation model for mobile data forecasting, effectively addressing the limitations of customized approaches while supporting diverse optimization scenarios."}}
{"id": "2509.19058", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19058", "abs": "https://arxiv.org/abs/2509.19058", "authors": ["Kwonho Kim", "Heejeong Nam", "Inwoo Hwang", "Sanghack Lee"], "title": "Towards Causal Representation Learning with Observable Sources as Auxiliaries", "comment": null, "summary": "Causal representation learning seeks to recover latent factors that generate\nobservational data through a mixing function. Needing assumptions on latent\nstructures or relationships to achieve identifiability in general, prior works\noften build upon conditional independence given known auxiliary variables.\nHowever, prior frameworks limit the scope of auxiliary variables to be external\nto the mixing function. Yet, in some cases, system-driving latent factors can\nbe easily observed or extracted from data, possibly facilitating\nidentification. In this paper, we introduce a framework of observable sources\nbeing auxiliaries, serving as effective conditioning variables. Our main\nresults show that one can identify entire latent variables up to subspace-wise\ntransformations and permutations using volume-preserving encoders. Moreover,\nwhen multiple known auxiliary variables are available, we offer a\nvariable-selection scheme to choose those that maximize recoverability of the\nlatent factors given knowledge of the latent causal graph. Finally, we\ndemonstrate the effectiveness of our framework through experiments on synthetic\ngraph and image data, thereby extending the boundaries of current approaches.", "AI": {"tldr": "The paper introduces a framework using observable sources as auxiliary variables for causal representation learning, enabling identification of latent variables up to subspace-wise transformations and permutations with volume-preserving encoders.", "motivation": "Prior works limit auxiliary variables to be external to the mixing function, but system-driving latent factors can sometimes be observed or extracted from data, which could facilitate identification of latent variables.", "method": "Proposes using observable sources as auxiliary variables, employs volume-preserving encoders for identification, and introduces a variable-selection scheme when multiple auxiliary variables are available to maximize recoverability given knowledge of the latent causal graph.", "result": "The framework can identify entire latent variables up to subspace-wise transformations and permutations. Experiments on synthetic graph and image data demonstrate the effectiveness of the approach.", "conclusion": "This work extends the boundaries of current causal representation learning approaches by leveraging observable sources as auxiliary variables, providing a more flexible framework for latent variable identification."}}
{"id": "2509.18169", "categories": ["cs.LG", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18169", "abs": "https://arxiv.org/abs/2509.18169", "authors": ["Hengbo Xiao", "Jingyuan Fan", "Xin Tong", "Jingzhao Zhang", "Chao Lu", "Guannan He"], "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning", "comment": null, "summary": "Complex systems typically rely on high-precision numerical computation to\nsupport decisions, but current large language models (LLMs) cannot yet\nincorporate such computations as an intrinsic and interpretable capability with\nexisting architectures. Mainstream multi-agent approaches can leverage external\nexperts, but inevitably introduce communication overhead and suffer from\ninefficient multimodal emergent capability and limited scalability. To this\nend, we propose PiMoE (Physically-isolated Mixture of Experts), a training and\ninference architecture for integrating computation and reasoning. Instead of\nthe workflow paradigm of tool invocation, PiMoE endogenously integrates\ncomputational capabilities into neural networks after separately training\nexperts, a text-to-computation module, and a router. At inference, the router\ndirects computation and reasoning at the token level, thereby enabling\niterative alternation within a single chain of thought. We evaluate PiMoE on\ntwo reasoning-computation tasks against LLM finetuning and the multi-agent\nsystem approaches. Results show that the PiMoE architecture achieves not only\nhigher accuracy than directly finetuning LLMs but also significant improvements\nin response latency, token usage, and GPU energy consumption compared with\nmainstream multi-agent approaches. PiMoE offers an efficient, interpretable,\nand scalable paradigm for next-generation scientific or industrial intelligent\nsystems.", "AI": {"tldr": "PiMoE is a novel architecture that integrates computational capabilities into neural networks through physically-isolated experts, enabling efficient token-level alternation between computation and reasoning within a single chain of thought.", "motivation": "Current LLMs cannot incorporate high-precision numerical computation as an intrinsic capability, and multi-agent approaches introduce communication overhead and scalability limitations.", "method": "PiMoE trains experts, a text-to-computation module, and a router separately, then integrates them endogenously. At inference, the router directs computation and reasoning at token level for iterative alternation.", "result": "PiMoE achieves higher accuracy than LLM finetuning and significant improvements in latency, token usage, and GPU energy consumption compared to multi-agent approaches.", "conclusion": "PiMoE provides an efficient, interpretable, and scalable paradigm for next-generation scientific and industrial intelligent systems."}}
{"id": "2509.19077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19077", "abs": "https://arxiv.org/abs/2509.19077", "authors": ["Zikang Tian", "Shaohui Peng", "Du Huang", "Jiaming Guo", "Ruizhi Chen", "Rui Zhang", "Xishan Zhang", "Yuxuan Guo", "Zidong Du", "Qi Guo", "Ling Li", "Yewen Pu", "Xing Hu", "Yunji Chen"], "title": "Code Driven Planning with Domain-Adaptive Critic", "comment": null, "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.", "AI": {"tldr": "CoPiC reduces LLM query costs by generating diverse planning programs and using a domain-adaptive critic to select plans aligned with long-term rewards, achieving higher success rates with fewer queries.", "motivation": "Existing LLM-based planners require frequent queries for iterative refinement, which is costly and focuses on short-term feedback rather than long-term rewards.", "method": "Generate diverse high-level planning programs with LLMs, then use a trained domain-adaptive critic to evaluate and select the best plan based on long-term alignment.", "result": "Outperforms AdaPlanner and Reflexion with 23.33% higher success rate and 91.27% reduction in query costs across ALFWorld, NetHack, and StarCraft II.", "conclusion": "CoPiC effectively balances planning quality and efficiency by separating plan generation from evaluation, reducing reliance on costly LLM queries."}}
{"id": "2509.18171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18171", "abs": "https://arxiv.org/abs/2509.18171", "authors": ["Zhanting Zhou", "KaHou Tam", "Zeqin Wu", "Pengzhao Sun", "Jinbo Wang", "Fengli Zhang"], "title": "FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification", "comment": null, "summary": "Federated Graph Learning (FGL) under domain skew -- as observed on platforms\nsuch as \\emph{Twitch Gamers} and multilingual \\emph{Wikipedia} networks --\ndrives client models toward incompatible representations, rendering naive\naggregation both unstable and ineffective. We find that the culprit is not the\nweighting scheme but the \\emph{noisy gradient signal}: empirical analysis of\nbaseline methods suggests that a vast majority of gradient dimensions can be\ndominated by domain-specific variance. We therefore shift focus from\n\"aggregation-first\" to a \\emph{projection-first} strategy that denoises client\nupdates \\emph{before} they are combined. The proposed FedIA framework realises\nthis \\underline{I}mportance-\\underline{A}ware idea through a two-stage,\nplug-and-play pipeline: (i) a server-side top-$\\rho$ mask keeps only the most\ninformative about 5% of coordinates, and (ii) a lightweight\ninfluence-regularised momentum weight suppresses outlier clients. FedIA adds\n\\emph{no extra uplink traffic and only negligible server memory}, making it\nreadily deployable. On both homogeneous (Twitch Gamers) and heterogeneous\n(Wikipedia) graphs, it yields smoother, more stable convergence and higher\nfinal accuracy than nine strong baselines. A convergence sketch further shows\nthat dynamic projection maintains the optimal\n$\\mathcal{O}(\\sigma^{2}/\\sqrt{T})$ rate.", "AI": {"tldr": "FedIA is a federated graph learning framework that addresses domain skew by using a projection-first strategy to denoise client updates before aggregation, achieving stable convergence and higher accuracy with minimal overhead.", "motivation": "Federated Graph Learning under domain skew leads to incompatible client representations, making naive aggregation unstable and ineffective due to noisy gradient signals dominated by domain-specific variance.", "method": "FedIA employs a two-stage pipeline: (1) server-side top-\u03c1 mask to retain only the most informative gradient coordinates (about 5%), and (2) lightweight influence-regularised momentum weight to suppress outlier clients.", "result": "FedIA achieves smoother, more stable convergence and higher final accuracy than nine baselines on both homogeneous (Twitch Gamers) and heterogeneous (Wikipedia) graphs, with no extra uplink traffic and negligible server memory overhead.", "conclusion": "The projection-first strategy effectively addresses domain skew in federated graph learning, maintaining optimal convergence rates while being readily deployable due to minimal resource requirements."}}
{"id": "2509.19236", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19236", "abs": "https://arxiv.org/abs/2509.19236", "authors": ["Chunhao Tian", "Yutong Wang", "Xuebo Liu", "Zhexuan Wang", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration", "comment": "EMNLP 2025 Findings", "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.", "AI": {"tldr": "AgentInit is a novel multi-agent system initialization method that optimizes agent team structure through multi-round interactions, reflections, and balanced team selection using Pareto principles to improve collaboration and system performance.", "motivation": "Existing MAS initialization methods fail to adequately address the collaborative needs of agents in subsequent stages, leading to suboptimal system efficiency and effectiveness.", "method": "AgentInit incorporates multi-round interactions and reflections between agents during generation, uses a Natural Language to Format mechanism for consistency, and applies balanced team selection strategies based on Pareto principles to jointly consider diversity and task relevance.", "result": "AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving performance improvements of up to 1.2x and 1.6x respectively, while significantly reducing token consumption.", "conclusion": "The method demonstrates strong transferability to similar tasks and verifies the effectiveness of its key components, establishing AgentInit as a reliable and adaptable MAS initialization approach."}}
{"id": "2509.18172", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18172", "abs": "https://arxiv.org/abs/2509.18172", "authors": ["Wonjun Bang", "Jongseok Park", "Hongseung Yu", "Kyungmin Bin", "Kyunghan Lee"], "title": "SBVR: Summation of BitVector Representation for Efficient LLM Quantization", "comment": "9 pages, 4 figures", "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime.", "AI": {"tldr": "SBVR is a novel LLM quantization method that uses Gaussian-like code representation with custom CUDA kernels for fast inference, achieving state-of-the-art performance with 2.21x-3.04x speedup over FP16 models.", "motivation": "Existing PTQ methods have limitations: RTN-based methods don't account for LLM weights' Gaussian distribution, while codebook-based methods suffer from inefficient memory access patterns that degrade inference speed due to GPU cache limitations.", "method": "SBVR maps weight values to non-uniform representation points that follow LLM weights' actual distribution, and uses custom CUDA kernels for direct matrix-vector multiplication in SBVR format without decompression.", "result": "SBVR achieves state-of-the-art perplexity and accuracy benchmarks while providing 2.21x-3.04x end-to-end token-generation speedup over naive FP16 models in 4-bit quantization.", "conclusion": "SBVR overcomes limitations of existing PTQ methods by combining distribution-aware quantization with hardware-friendly execution, enabling both high compression accuracy and fast inference speeds."}}
{"id": "2509.19265", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19265", "abs": "https://arxiv.org/abs/2509.19265", "authors": ["Saeed Almheiri", "Rania Hossam", "Mena Attia", "Chenxi Wang", "Preslav Nakov", "Timothy Baldwin", "Fajri Koto"], "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World", "comment": "EMNLP 2025 - Findings", "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.", "AI": {"tldr": "This paper demonstrates that cross-cultural transfer of commonsense reasoning is possible using lightweight alignment methods, showing that even small amounts of culture-specific examples from one Arab country can significantly improve performance in other Arab countries.", "motivation": "LLMs often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. The potential for cross-cultural transfer - using alignment in one culture to improve performance in others - remains underexplored, particularly in the Arab world.", "method": "Used a culturally grounded commonsense reasoning dataset covering 13 Arab countries. Evaluated lightweight alignment methods including in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization.", "result": "Just 12 culture-specific examples from one country improved performance in others by 10% on average within multilingual models. Out-of-culture demonstrations from Indonesia and US contexts matched or surpassed in-culture alignment for MCQ reasoning.", "conclusion": "Efficient cross-cultural alignment is possible and offers a promising approach to adapt LLMs to low-resource cultural settings, demonstrating cultural commonsense transferability beyond the Arab world."}}
{"id": "2509.18173", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18173", "abs": "https://arxiv.org/abs/2509.18173", "authors": ["Hongyi Luo", "Qing Cheng", "Daniel Matos", "Hari Krishna Gadi", "Yanfeng Zhang", "Lu Liu", "Yongliang Wang", "Niclas Zeller", "Daniel Cremers", "Liqiu Meng"], "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route", "comment": "Accepted to EMNLP 2025 (Main). This is the camera-ready/author\n  version", "summary": "Humans can interpret geospatial information through natural language, while\nthe geospatial cognition capabilities of Large Language Models (LLMs) remain\nunderexplored. Prior research in this domain has been constrained by\nnon-quantifiable metrics, limited evaluation datasets and unclear research\nhierarchies. Therefore, we propose a large-scale benchmark and conduct a\ncomprehensive evaluation of the geospatial route cognition of LLMs. We create a\nlarge-scale evaluation dataset comprised of 36000 routes from 12 metropolises\nworldwide. Then, we introduce PathBuilder, a novel tool for converting natural\nlanguage instructions into navigation routes, and vice versa, bridging the gap\nbetween geospatial information and natural language. Finally, we propose a new\nevaluation framework and metrics to rigorously assess 11 state-of-the-art\n(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs\nexhibit limitation to reverse routes: most reverse routes neither return to the\nstarting point nor are similar to the optimal route. Additionally, LLMs face\nchallenges such as low robustness in route generation and high confidence for\ntheir incorrect answers. Code\\ \\&\\ Data available here:\n\\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}", "AI": {"tldr": "A comprehensive benchmark evaluating LLMs' geospatial route cognition capabilities, revealing significant limitations in route reversal tasks despite high confidence in incorrect answers.", "motivation": "Geospatial cognition capabilities of LLMs remain underexplored due to non-quantifiable metrics, limited evaluation datasets, and unclear research hierarchies in prior work.", "method": "Created a large-scale dataset of 36,000 routes from 12 global metropolises, introduced PathBuilder tool for natural language-route conversion, and proposed new evaluation framework to assess 11 SOTA LLMs on route reversal tasks.", "result": "LLMs exhibit significant limitations in reversing routes - most reversed routes neither return to starting points nor match optimal routes, showing low robustness and high confidence in incorrect answers.", "conclusion": "Current LLMs struggle with geospatial route cognition, particularly route reversal, highlighting the need for improved spatial reasoning capabilities in language models."}}
{"id": "2509.18200", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18200", "abs": "https://arxiv.org/abs/2509.18200", "authors": ["Yu Ti Huang"], "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought", "comment": null, "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my\nright\") into allocentric orientations (N/E/S/W). This challenge is particularly\ncritical in indoor or complex facilities where GPS signals are weak and\ndetailed maps are unavailable. While chain-of-thought (CoT) prompting has\nadvanced reasoning in language and vision tasks, its application to multimodal\nspatial orientation remains underexplored. We introduce Conversational\nOrientation Reasoning (COR), a new benchmark designed for Traditional Chinese\nconversational navigation projected from real-world environments, addressing\negocentric-to-allocentric reasoning in non-English and ASR-transcribed\nscenarios. We propose a multimodal chain-of-thought (MCoT) framework, which\nintegrates ASR-transcribed speech with landmark coordinates through a\nstructured three-step reasoning process: (1) extracting spatial relations, (2)\nmapping coordinates to absolute directions, and (3) inferring user orientation.\nA curriculum learning strategy progressively builds these capabilities on\nTaiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of\nresource-constrained settings. Experiments show that MCoT achieves 100%\norientation accuracy on clean transcripts and 98.1% with ASR transcripts,\nsubstantially outperforming unimodal and non-structured baselines. Moreover,\nMCoT demonstrates robustness under noisy conversational conditions, including\nASR recognition errors and multilingual code-switching. The model also\nmaintains high accuracy in cross-domain evaluation and resilience to linguistic\nvariation, domain shift, and referential ambiguity. These findings highlight\nthe potential of structured MCoT spatial reasoning as a path toward\ninterpretable and resource-efficient embodied navigation.", "AI": {"tldr": "The paper introduces COR, a new benchmark for Traditional Chinese conversational navigation, and proposes MCoT, a multimodal chain-of-thought framework that achieves high accuracy in translating egocentric utterances to allocentric orientations.", "motivation": "Address the challenge of translating egocentric utterances to allocentric orientations in indoor/complex facilities where GPS is weak and maps are unavailable, particularly for non-English and ASR-transcribed scenarios.", "method": "Multimodal chain-of-thought (MCoT) framework integrating ASR-transcribed speech with landmark coordinates through a three-step reasoning process: spatial relation extraction, coordinate mapping to absolute directions, and user orientation inference. Uses curriculum learning on Taiwan-LLM-13B-v2.0-Chat.", "result": "MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, outperforming unimodal and non-structured baselines. Demonstrates robustness to ASR errors, multilingual code-switching, and maintains high accuracy in cross-domain evaluation.", "conclusion": "Structured MCoT spatial reasoning shows potential for interpretable and resource-efficient embodied navigation, with strong performance under noisy conversational conditions and resilience to linguistic variation and domain shift."}}
{"id": "2509.18208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18208", "abs": "https://arxiv.org/abs/2509.18208", "authors": ["Boyuan Zhang", "Yingjun Du", "Xiantong Zhen", "Ling Shao"], "title": "Variational Task Vector Composition", "comment": null, "summary": "Task vectors capture how a model changes during fine-tuning by recording the\ndifference between pre-trained and task-specific weights. The composition of\ntask vectors, a key operator in task arithmetic, enables models to integrate\nknowledge from multiple tasks without incurring additional inference costs. In\nthis paper, we propose variational task vector composition, where composition\ncoefficients are taken as latent variables and estimated in a Bayesian\ninference framework. Unlike previous methods that operate at the task level,\nour framework focuses on sample-specific composition. Motivated by the\nobservation of structural redundancy in task vectors, we introduce a\nSpike-and-Slab prior that promotes sparsity and preserves only the most\ninformative components. To further address the high variance and sampling\ninefficiency in sparse, high-dimensional spaces, we develop a gated sampling\nmechanism that constructs a controllable posterior by filtering the composition\ncoefficients based on both uncertainty and importance. This yields a more\nstable and interpretable variational framework by deterministically selecting\nreliable task components, reducing sampling variance while improving\ntransparency and generalization. Experimental results demonstrate that our\nmethod consistently outperforms existing approaches across all datasets by\nselectively leveraging the most reliable and informative components in task\nvectors. These findings highlight the practical value of our approach,\nestablishing a new standard for efficient and effective task vector\ncomposition.", "AI": {"tldr": "Proposes variational task vector composition with Bayesian inference, using Spike-and-Slab priors for sparsity and gated sampling for efficiency, outperforming existing methods.", "motivation": "To enable sample-specific composition of task vectors rather than task-level composition, addressing structural redundancy and improving efficiency.", "method": "Bayesian inference framework with Spike-and-Slab priors for sparsity, combined with gated sampling mechanism to filter composition coefficients based on uncertainty and importance.", "result": "Consistently outperforms existing approaches across all datasets by selectively leveraging the most reliable and informative components.", "conclusion": "Establishes a new standard for efficient and effective task vector composition with improved transparency and generalization."}}
{"id": "2509.18353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18353", "abs": "https://arxiv.org/abs/2509.18353", "authors": ["Jakub Adamczyk", "Jakub Poziemski", "Franciszek Job", "Mateusz Kr\u00f3l", "Maciej Makowski"], "title": "MolPILE - large-scale, diverse dataset for molecular representation learning", "comment": null, "summary": "The size, diversity, and quality of pretraining datasets critically determine\nthe generalization ability of foundation models. Despite their growing\nimportance in chemoinformatics, the effectiveness of molecular representation\nlearning has been hindered by limitations in existing small molecule datasets.\nTo address this gap, we present MolPILE, large-scale, diverse, and rigorously\ncurated collection of 222 million compounds, constructed from 6 large-scale\ndatabases using an automated curation pipeline. We present a comprehensive\nanalysis of current pretraining datasets, highlighting considerable\nshortcomings for training ML models, and demonstrate how retraining existing\nmodels on MolPILE yields improvements in generalization performance. This work\nprovides a standardized resource for model training, addressing the pressing\nneed for an ImageNet-like dataset in molecular chemistry.", "AI": {"tldr": "MolPILE is a large-scale, diverse dataset of 222 million compounds curated from 6 databases to address limitations in existing molecular datasets for foundation model training.", "motivation": "Existing small molecule datasets have limitations that hinder molecular representation learning, creating a need for an ImageNet-like standardized resource in chemoinformatics.", "method": "Constructed MolPILE using an automated curation pipeline from 6 large-scale databases, with comprehensive analysis of current pretraining datasets and retraining existing models on the new dataset.", "result": "Retraining existing models on MolPILE yields improvements in generalization performance, demonstrating the dataset's effectiveness for molecular representation learning.", "conclusion": "MolPILE provides a standardized, high-quality resource that addresses critical gaps in molecular chemistry datasets and enhances foundation model generalization capabilities."}}
{"id": "2509.18362", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18362", "abs": "https://arxiv.org/abs/2509.18362", "authors": ["Yuxuan Cai", "Xiaozhuan Liang", "Xinghua Wang", "Jin Ma", "Haijin Liang", "Jinwen Luo", "Xinyu Zuo", "Lisheng Duan", "Yuyang Yin", "Xi Chen"], "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction", "comment": null, "summary": "As large language models (LLMs) become increasingly powerful, the sequential\nnature of autoregressive generation creates a fundamental throughput bottleneck\nthat limits the practical deployment. While Multi-Token Prediction (MTP) has\ndemonstrated remarkable benefits for model training efficiency and performance,\nits inherent potential for inference acceleration remains largely unexplored.\nThis paper introduces FastMTP, a simple yet effective method that improves\nmulti-step draft quality by aligning MTP training with its inference pattern,\nsignificantly enhancing speculative decoding performance. Our approach\nfine-tunes a single MTP head with position-shared weights on self-distilled\ndata, enabling it to capture dependencies among consecutive future tokens and\nmaintain high acceptance rates across multiple recursive draft steps. By\nintegrating language-aware dynamic vocabulary compression into the MTP head, we\nfurther reduce computational overhead in the drafting process. Experimental\nresults across seven diverse benchmarks demonstrate that FastMTP achieves an\naverage of 2.03x speedup compared to standard next token prediction with\nlossless output quality, outperforming vanilla MTP by 82%. FastMTP requires\nonly lightweight training and seamlessly integrates with existing inference\nframeworks, offering a practical and rapidly deployable solution for\naccelerating LLM inference.", "AI": {"tldr": "FastMTP is a method that improves multi-token prediction training to accelerate LLM inference through better speculative decoding, achieving 2.03x speedup with lossless quality.", "motivation": "Autoregressive generation creates throughput bottlenecks in LLM deployment, and while multi-token prediction helps training efficiency, its inference acceleration potential remains unexplored.", "method": "Fine-tunes a single MTP head with position-shared weights on self-distilled data, integrates language-aware dynamic vocabulary compression, and aligns training with inference patterns to improve multi-step draft quality.", "result": "Achieves average 2.03x speedup across seven benchmarks compared to standard next token prediction, outperforming vanilla MTP by 82% while maintaining lossless output quality.", "conclusion": "FastMTP offers a practical, lightweight training solution that seamlessly integrates with existing inference frameworks for rapidly deployable LLM acceleration."}}
{"id": "2509.18367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18367", "abs": "https://arxiv.org/abs/2509.18367", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data", "comment": null, "summary": "Recent advances in distributed swarm learning (DSL) offer a promising\nparadigm for edge Internet of Things. Such advancements enhance data privacy,\ncommunication efficiency, energy saving, and model scalability. However, the\npresence of non-independent and identically distributed (non-i.i.d.) data pose\na significant challenge for multi-access edge computing, degrading learning\nperformance and diverging training behavior of vanilla DSL. Further, there\nstill lacks theoretical guidance on how data heterogeneity affects model\ntraining accuracy, which requires thorough investigation. To fill the gap, this\npaper first study the data heterogeneity by measuring the impact of non-i.i.d.\ndatasets under the DSL framework. This then motivates a new multi-worker\nselection design for DSL, termed M-DSL algorithm, which works effectively with\ndistributed heterogeneous data. A new non-i.i.d. degree metric is introduced\nand defined in this work to formulate the statistical difference among local\ndatasets, which builds a connection between the measure of data heterogeneity\nand the evaluation of DSL performance. In this way, our M-DSL guides effective\nselection of multiple works who make prominent contributions for global model\nupdates. We also provide theoretical analysis on the convergence behavior of\nour M-DSL, followed by extensive experiments on different heterogeneous\ndatasets and non-i.i.d. data settings. Numerical results verify performance\nimprovement and network intelligence enhancement provided by our M-DSL beyond\nthe benchmarks.", "AI": {"tldr": "This paper proposes M-DSL, a multi-worker selection algorithm for distributed swarm learning that addresses data heterogeneity challenges in edge IoT environments.", "motivation": "Non-i.i.d. data in distributed swarm learning degrades performance and lacks theoretical guidance on how data heterogeneity affects model training accuracy.", "method": "Introduces a new non-i.i.d. degree metric to measure data heterogeneity, develops M-DSL algorithm for effective multi-worker selection based on data statistical differences, and provides theoretical convergence analysis.", "result": "Extensive experiments show M-DSL improves performance and network intelligence beyond benchmarks on various heterogeneous datasets and non-i.i.d. settings.", "conclusion": "M-DSL effectively addresses data heterogeneity challenges in distributed swarm learning through systematic measurement and worker selection, with verified performance improvements."}}
{"id": "2509.18376", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.18376", "abs": "https://arxiv.org/abs/2509.18376", "authors": ["Burouj Armgaan", "Eshan Jain", "Harsh Pandey", "Mahesh Chandran", "Sayan Ranu"], "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability", "comment": "31 pages, 20 figures, NeurIPS 2025 (Oral)", "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.", "AI": {"tldr": "GnnXemplar is a novel global explainer for Graph Neural Networks that identifies representative exemplar nodes and generates natural language rules using LLMs, outperforming existing methods in fidelity, scalability, and interpretability.", "motivation": "Current GNN explanation methods are limited to local explanations, while global explanation methods that characterize entire classes are underdeveloped, especially for large real-world graphs where traditional motif discovery approaches fail.", "method": "GnnXemplar identifies exemplar nodes in the GNN embedding space using a coverage maximization approach over reverse k-nearest neighbors, then employs self-refining prompts with LLMs to generate natural language rules from the exemplars' neighborhoods.", "result": "Experiments across diverse benchmarks show GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.", "conclusion": "GnnXemplar provides an effective global explanation framework for GNNs that addresses limitations of existing methods and offers human-interpretable explanations through exemplar-based reasoning and LLM-generated rules."}}
{"id": "2509.18386", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18386", "abs": "https://arxiv.org/abs/2509.18386", "authors": ["Jonathan Kabala Mbuya", "Dieter Pfoser", "Antonios Anastasopoulos"], "title": "Graph Enhanced Trajectory Anomaly Detection", "comment": null, "summary": "Trajectory anomaly detection is essential for identifying unusual and\nunexpected movement patterns in applications ranging from intelligent\ntransportation systems to urban safety and fraud prevention.\n  Existing methods only consider limited aspects of the trajectory nature and\nits movement space by treating trajectories as sequences of sampled locations,\nwith sampling determined by positioning technology, e.g., GPS, or by high-level\nabstractions such as staypoints. Trajectories are analyzed in Euclidean space,\nneglecting the constraints and connectivity information of the underlying\nmovement network, e.g., road or transit networks.\n  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework\ntightly integrates road network topology, segment semantics, and historical\ntravel patterns to model trajectory data. GETAD uses a Graph Attention Network\nto learn road-aware embeddings that capture both physical attributes and\ntransition behavior, and augments these with graph-based positional encodings\nthat reflect the spatial layout of the road network.\n  A Transformer-based decoder models sequential movement, while a\nmultiobjective loss function combining autoregressive prediction and supervised\nlink prediction ensures realistic and structurally coherent representations.\n  To improve the robustness of anomaly detection, we introduce Confidence\nWeighted Negative Log Likelihood (CW NLL), an anomaly scoring function that\nemphasizes high-confidence deviations.\n  Experiments on real-world and synthetic datasets demonstrate that GETAD\nachieves consistent improvements over existing methods, particularly in\ndetecting subtle anomalies in road-constrained environments. These results\nhighlight the benefits of incorporating graph structure and contextual\nsemantics into trajectory modeling, enabling more precise and context-aware\nanomaly detection.", "AI": {"tldr": "GETAD is a graph-enhanced trajectory anomaly detection framework that integrates road network topology, segment semantics, and historical patterns using Graph Attention Networks and Transformers to detect subtle anomalies in road-constrained environments.", "motivation": "Existing trajectory anomaly detection methods only consider limited aspects of trajectory nature and movement space, treating trajectories as simple location sequences in Euclidean space while neglecting underlying network constraints and connectivity information.", "method": "GETAD uses Graph Attention Network to learn road-aware embeddings with graph-based positional encodings, a Transformer-based decoder for sequential movement modeling, and a multiobjective loss function combining autoregressive prediction and supervised link prediction. It introduces Confidence Weighted Negative Log Likelihood for robust anomaly scoring.", "result": "Experiments on real-world and synthetic datasets show that GETAD achieves consistent improvements over existing methods, particularly in detecting subtle anomalies in road-constrained environments.", "conclusion": "The results highlight the benefits of incorporating graph structure and contextual semantics into trajectory modeling, enabling more precise and context-aware anomaly detection."}}
{"id": "2509.18389", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18389", "abs": "https://arxiv.org/abs/2509.18389", "authors": ["Jiuqi Wang", "Rohan Chandra", "Shangtong Zhang"], "title": "Towards Provable Emergence of In-Context Reinforcement Learning", "comment": "NeurIPS 2025, 28 pages", "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by\nupdating its neural network parameters to adapt its policy to the task.\nRecently, it has been observed that some RL agents can solve a wide range of\nnew out-of-distribution tasks without parameter updates after pretraining on\nsome task distribution. When evaluated in a new task, instead of making\nparameter updates, the pretrained agent conditions its policy on additional\ninput called the context, e.g., the agent's interaction history in the new\ntask. The agent's performance increases as the information in the context\nincreases, with the agent's parameters fixed. This phenomenon is typically\ncalled in-context RL (ICRL). The pretrained parameters of the agent network\nenable the remarkable ICRL phenomenon. However, many ICRL works perform the\npretraining with standard RL algorithms. This raises the central question this\npaper aims to address: Why can the RL pretraining algorithm generate network\nparameters that enable ICRL? We hypothesize that the parameters capable of ICRL\nare minimizers of the pretraining loss. This work provides initial support for\nthis hypothesis through a case study. In particular, we prove that when a\nTransformer is pretrained for policy evaluation, one of the global minimizers\nof the pretraining loss can enable in-context temporal difference learning.", "AI": {"tldr": "This paper investigates why RL pretraining algorithms can generate network parameters that enable in-context reinforcement learning (ICRL), hypothesizing that such parameters are minimizers of the pretraining loss.", "motivation": "The motivation is to understand why standard RL pretraining algorithms can produce network parameters that allow agents to solve new out-of-distribution tasks without parameter updates, simply by conditioning on context like interaction history.", "method": "The authors conduct a case study proving that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.", "result": "The paper provides initial support for the hypothesis that parameters capable of ICRL are indeed minimizers of the pretraining loss, specifically demonstrating this for policy evaluation with Transformers.", "conclusion": "The work establishes theoretical evidence that ICRL capabilities emerge from optimal solutions to the pretraining objective, offering insights into why standard RL pretraining can yield context-adaptive behavior."}}
{"id": "2509.18396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18396", "abs": "https://arxiv.org/abs/2509.18396", "authors": ["Do\u011fay Alt\u0131nel"], "title": "Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules", "comment": "24 pages", "summary": "Deep learning optimizers are optimization algorithms that enable deep neural\nnetworks to learn. The effectiveness of learning is highly dependent on the\noptimizer employed in the training process. Alongside the rapid advancement of\ndeep learning, a wide range of optimizers with different approaches have been\ndeveloped. This study aims to provide a review of various optimizers that have\nbeen proposed and received attention in the literature. From Stochastic\ngradient descent to the most recent ones such as Momentum, AdamW, Sophia, and\nMuon in chronological order, optimizers are examined individually, and their\ndistinctive features are highlighted in the study. The update rule of each\noptimizer is presented in detail, with an explanation of the associated\nconcepts and variables. The techniques applied by these optimizers, their\ncontributions to the optimization process, and their default hyperparameter\nsettings are also discussed. In addition, insights are offered into the open\nchallenges encountered in the optimization of deep learning models. Thus, a\ncomprehensive resource is provided both for understanding the current state of\noptimizers and for identifying potential areas of future development.", "AI": {"tldr": "This paper provides a comprehensive review of deep learning optimizers, examining their evolution from Stochastic Gradient Descent to recent developments like Momentum, AdamW, Sophia, and Muon, highlighting their distinctive features, update rules, and hyperparameter settings.", "motivation": "The effectiveness of deep learning training is highly dependent on the optimizer used, and with the rapid advancement of deep learning, numerous optimizers with different approaches have been developed. This study aims to provide a comprehensive resource for understanding the current state of optimizers and identifying future development areas.", "method": "The study conducts a chronological review of various optimizers, examining each individually and highlighting their distinctive features. It presents detailed update rules, explains associated concepts and variables, discusses techniques applied by these optimizers, their contributions to optimization, and their default hyperparameter settings.", "result": "The paper provides a comprehensive examination of optimizer evolution and characteristics, offering insights into open challenges in deep learning optimization.", "conclusion": "This review serves as a valuable resource for understanding the current state of deep learning optimizers and identifying potential areas for future development in optimization algorithms."}}
{"id": "2509.18408", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.18408", "abs": "https://arxiv.org/abs/2509.18408", "authors": ["Sarwan Ali"], "title": "Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations", "comment": "Accepted to CIKM 2025 as Short paper", "summary": "We present a novel information-preserving Chaos Game Representation (CGR)\nmethod, also called Reverse-CGR (R-CGR), for biological sequence analysis that\naddresses the fundamental limitation of traditional CGR approaches - the loss\nof sequence information during geometric mapping. Our method introduces\ncomplete sequence recovery through explicit path encoding combined with\nrational arithmetic precision control, enabling perfect sequence reconstruction\nfrom stored geometric traces. Unlike purely geometric approaches, our\nreversibility is achieved through comprehensive path storage that maintains\nboth positional and character information at each step. We demonstrate the\neffectiveness of R-CGR on biological sequence classification tasks, achieving\ncompetitive performance compared to traditional sequence-based methods while\nproviding interpretable geometric visualizations. The approach generates\nfeature-rich images suitable for deep learning while maintaining complete\nsequence information through explicit encoding, opening new avenues for\ninterpretable bioinformatics analysis where both accuracy and sequence recovery\nare essential.", "AI": {"tldr": "A novel Chaos Game Representation method called R-CGR that preserves complete sequence information through explicit path encoding and rational arithmetic, enabling perfect sequence reconstruction from geometric traces.", "motivation": "Traditional CGR approaches lose sequence information during geometric mapping, limiting their utility in applications requiring both accurate classification and sequence recovery.", "method": "Introduces complete sequence recovery through explicit path encoding combined with rational arithmetic precision control, maintaining both positional and character information at each step of the geometric mapping.", "result": "Achieves competitive performance on biological sequence classification tasks compared to traditional methods while providing interpretable geometric visualizations and complete sequence recovery.", "conclusion": "R-CGR opens new avenues for interpretable bioinformatics analysis by generating feature-rich images suitable for deep learning while maintaining complete sequence information through explicit encoding."}}
{"id": "2509.18433", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18433", "abs": "https://arxiv.org/abs/2509.18433", "authors": ["Chang Liu", "Ladda Thiamwong", "Yanjie Fu", "Rui Xie"], "title": "Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors", "comment": "Accepted at ICMLA 2025. 8 pages, 6 figures", "summary": "Utilizing offline reinforcement learning (RL) with real-world clinical data\nis getting increasing attention in AI for healthcare. However, implementation\nposes significant challenges. Defining direct rewards is difficult, and inverse\nRL (IRL) struggles to infer accurate reward functions from expert behavior in\ncomplex environments. Offline RL also encounters challenges in aligning learned\npolicies with observed human behavior in healthcare applications. To address\nchallenges in applying offline RL to physical activity promotion for older\nadults at high risk of falls, based on wearable sensor activity monitoring, we\nintroduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse\nReinforcement Learning (KANDI). By leveraging the flexible function\napproximation in Kolmogorov-Arnold Networks, we estimate reward functions by\nlearning free-living environment behavior from low-fall-risk older adults\n(experts), while diffusion-based policies within an Actor-Critic framework\nprovide a generative approach for action refinement and efficiency in offline\nRL. We evaluate KANDI using wearable activity monitoring data in a two-arm\nclinical trial from our Physio-feedback Exercise Program (PEER) study,\nemphasizing its practical application in a fall-risk intervention program to\npromote physical activity among older adults. Additionally, KANDI outperforms\nstate-of-the-art methods on the D4RL benchmark. These results underscore\nKANDI's potential to address key challenges in offline RL for healthcare\napplications, offering an effective solution for activity promotion\nintervention strategies in healthcare.", "AI": {"tldr": "KANDI is a novel offline reinforcement learning method that combines Kolmogorov-Arnold Networks and Diffusion Policies for healthcare applications, specifically for physical activity promotion in older adults at high fall risk.", "motivation": "Offline RL faces challenges in healthcare applications: defining direct rewards is difficult, IRL struggles with complex environments, and aligning policies with human behavior is problematic. The paper addresses these issues for fall-risk intervention in older adults.", "method": "KANDI uses Kolmogorov-Arnold Networks for flexible reward function estimation from expert behavior (low-fall-risk older adults), and diffusion-based policies within an Actor-Critic framework for generative action refinement in offline RL.", "result": "KANDI outperforms state-of-the-art methods on the D4RL benchmark and shows practical effectiveness in a clinical trial for physical activity promotion among older adults at high fall risk.", "conclusion": "KANDI offers an effective solution for healthcare applications, addressing key challenges in offline RL and showing potential for activity promotion intervention strategies in clinical settings."}}
{"id": "2509.18445", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.18445", "abs": "https://arxiv.org/abs/2509.18445", "authors": ["Kangzheng Liu", "Leixin Ma"], "title": "MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems", "comment": "9 pages, 7 figures", "summary": "The simulation of complex physical systems using a discretized mesh is a\ncornerstone of applied mechanics, but traditional numerical solvers are often\ncomputationally prohibitive for many-query tasks. While Graph Neural Networks\n(GNNs) have emerged as powerful surrogate models for mesh-based data, their\nstandard autoregressive application for long-term prediction is often plagued\nby error accumulation and instability. To address this, we introduce\nMeshODENet, a general framework that synergizes the spatial reasoning of GNNs\nwith the continuous-time modeling of Neural Ordinary Differential Equations. We\ndemonstrate the framework's effectiveness and versatility on a series of\nchallenging structural mechanics problems, including one- and two-dimensional\nelastic bodies undergoing large, non-linear deformations. The results\ndemonstrate that our approach significantly outperforms baseline models in\nlong-term predictive accuracy and stability, while achieving substantial\ncomputational speed-ups over traditional solvers. This work presents a powerful\nand generalizable approach for developing data-driven surrogates to accelerate\nthe analysis and modeling of complex structural systems.", "AI": {"tldr": "MeshODENet combines Graph Neural Networks with Neural ODEs to create stable, accurate surrogate models for simulating complex structural mechanics problems, outperforming baseline models in long-term prediction.", "motivation": "Traditional numerical solvers are computationally expensive for many-query tasks, and standard autoregressive GNNs suffer from error accumulation and instability in long-term predictions.", "method": "A framework that integrates GNNs for spatial reasoning with Neural ODEs for continuous-time modeling, applied to 1D and 2D elastic bodies undergoing large non-linear deformations.", "result": "Significantly outperforms baseline models in long-term predictive accuracy and stability while achieving substantial computational speed-ups over traditional solvers.", "conclusion": "MeshODENet provides a powerful and generalizable approach for developing data-driven surrogates to accelerate analysis and modeling of complex structural systems."}}
{"id": "2509.18452", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "D.2.0; G.4; B.8.2"], "pdf": "https://arxiv.org/pdf/2509.18452", "abs": "https://arxiv.org/abs/2509.18452", "authors": ["Anton Lebedev", "Won Kyung Lee", "Soumyadip Ghosh", "Olha I. Yaman", "Vassilis Kalantzis", "Yingdong Lu", "Tomasz Nowicki", "Shashanka Ubaru", "Lior Horesh", "Vassil Alexandrov"], "title": "Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion", "comment": "8 pages, 3 figures, 1 algorithm, 1 table of experiment cases", "summary": "Large, sparse linear systems are pervasive in modern science and engineering,\nand Krylov subspace solvers are an established means of solving them. Yet\nconvergence can be slow for ill-conditioned matrices, so practical deployments\nusually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix\ninversion can generate such preconditioners and accelerate Krylov iterations,\nbut its effectiveness depends on parameters whose optima vary across matrices;\nmanual or grid search is costly. We present an AI-driven framework recommending\nMCMC parameters for a given linear system. A graph neural surrogate predicts\npreconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition\nfunction then chooses the parameter sets most likely to minimise iterations. On\na previously unseen ill-conditioned system, the framework achieves better\npreconditioning with 50\\% of the search budget of conventional methods,\nyielding about a 10\\% reduction in iterations to convergence. These results\nsuggest a route for incorporating MCMC-based preconditioners into large-scale\nsystems.", "AI": {"tldr": "AI-driven framework for optimizing MCMC-based preconditioner parameters to accelerate Krylov subspace solvers for ill-conditioned linear systems", "motivation": "Krylov solvers converge slowly for ill-conditioned matrices, and MCMC-based preconditioning requires parameter tuning that is costly via manual or grid search", "method": "Uses graph neural network surrogate to predict preconditioning speed from matrix A and MCMC parameters, then Bayesian acquisition function selects optimal parameters to minimize iterations", "result": "Achieves better preconditioning with 50% of search budget compared to conventional methods, yielding ~10% reduction in iterations to convergence", "conclusion": "Provides a viable approach for incorporating MCMC-based preconditioners into large-scale systems through automated parameter optimization"}}
{"id": "2509.18457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18457", "abs": "https://arxiv.org/abs/2509.18457", "authors": ["Ebrahim Farahmand", "Reza Rahimi Azghan", "Nooshin Taheri Chatrudi", "Velarie Yaa Ansu-Baidoo", "Eric Kim", "Gautham Krishna Gudur", "Mohit Malu", "Owen Krueger", "Edison Thomaz", "Giulia Pedrielli", "Pavan Turaga", "Hassan Ghasemzadeh"], "title": "GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting", "comment": null, "summary": "This paper proposes GluMind, a transformer-based multimodal framework\ndesigned for continual and long-term blood glucose forecasting. GluMind devises\ntwo attention mechanisms, including cross-attention and multi-scale attention,\nwhich operate in parallel and deliver accurate predictive performance.\nCross-attention effectively integrates blood glucose data with other\nphysiological and behavioral signals such as activity, stress, and heart rate,\naddressing challenges associated with varying sampling rates and their adverse\nimpacts on robust prediction. Moreover, the multi-scale attention mechanism\ncaptures long-range temporal dependencies. To mitigate catastrophic forgetting,\nGluMind incorporates a knowledge retention technique into the transformer-based\nforecasting model. The knowledge retention module not only enhances the model's\nability to retain prior knowledge but also boosts its overall forecasting\nperformance. We evaluate GluMind on the recently released AIREADI dataset,\nwhich contains behavioral and physiological data collected from healthy people,\nindividuals with prediabetes, and those with type 2 diabetes. We examine the\nperformance stability and adaptability of GluMind in learning continuously as\nnew patient cohorts are introduced. Experimental results show that GluMind\nconsistently outperforms other state-of-the-art forecasting models, achieving\napproximately 15% and 9% improvements in root mean squared error (RMSE) and\nmean absolute error (MAE), respectively.", "AI": {"tldr": "GluMind is a transformer-based multimodal framework for continual blood glucose forecasting that uses cross-attention and multi-scale attention mechanisms, achieving 15% RMSE and 9% MAE improvements over state-of-the-art models.", "motivation": "To address challenges in blood glucose forecasting including varying sampling rates of physiological signals, long-range temporal dependencies, and catastrophic forgetting when learning from new patient cohorts.", "method": "Uses transformer architecture with parallel cross-attention (integrates blood glucose with physiological/behavioral signals) and multi-scale attention (captures long-range dependencies), plus knowledge retention module to prevent catastrophic forgetting.", "result": "Evaluated on AIREADI dataset, GluMind consistently outperforms state-of-the-art models with approximately 15% improvement in RMSE and 9% improvement in MAE.", "conclusion": "GluMind demonstrates superior performance and stability for continual blood glucose forecasting across different patient populations, effectively handling multimodal data integration and long-term dependencies."}}
{"id": "2509.18469", "categories": ["cs.LG", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18469", "abs": "https://arxiv.org/abs/2509.18469", "authors": ["Han-Lin Hsieh", "Maryam M. Shanechi"], "title": "Probabilistic Geometric Principal Component Analysis with application to neural data", "comment": "Published at the International Conference on Learning Representations\n  (ICLR) 2025. Code is available at GitHub\n  https://github.com/ShanechiLab/PGPCA.git", "summary": "Dimensionality reduction is critical across various domains of science\nincluding neuroscience. Probabilistic Principal Component Analysis (PPCA) is a\nprominent dimensionality reduction method that provides a probabilistic\napproach unlike the deterministic approach of PCA and serves as a connection\nbetween PCA and Factor Analysis (FA). Despite their power, PPCA and its\nextensions are mainly based on linear models and can only describe the data in\na Euclidean coordinate system. However, in many neuroscience applications, data\nmay be distributed around a nonlinear geometry (i.e., manifold) rather than\nlying in the Euclidean space. We develop Probabilistic Geometric Principal\nComponent Analysis (PGPCA) for such datasets as a new dimensionality reduction\nalgorithm that can explicitly incorporate knowledge about a given nonlinear\nmanifold that is first fitted from these data. Further, we show how in addition\nto the Euclidean coordinate system, a geometric coordinate system can be\nderived for the manifold to capture the deviations of data from the manifold\nand noise. We also derive a data-driven EM algorithm for learning the PGPCA\nmodel parameters. As such, PGPCA generalizes PPCA to better describe data\ndistributions by incorporating a nonlinear manifold geometry. In simulations\nand brain data analyses, we show that PGPCA can effectively model the data\ndistribution around various given manifolds and outperforms PPCA for such data.\nMoreover, PGPCA provides the capability to test whether the new geometric\ncoordinate system better describes the data than the Euclidean one. Finally,\nPGPCA can perform dimensionality reduction and learn the data distribution both\naround and on the manifold. These capabilities make PGPCA valuable for\nenhancing the efficacy of dimensionality reduction for analysis of\nhigh-dimensional data that exhibit noise and are distributed around a nonlinear\nmanifold.", "AI": {"tldr": "PGPCA extends PPCA to handle data distributed around nonlinear manifolds by incorporating geometric coordinates and an EM algorithm for parameter learning, showing superior performance over PPCA for manifold-distributed data.", "motivation": "Traditional PPCA and its extensions are limited to linear models and Euclidean spaces, but neuroscience data often lies on nonlinear manifolds, requiring a probabilistic dimensionality reduction method that can handle manifold geometry.", "method": "Developed Probabilistic Geometric PCA (PGPCA) that incorporates knowledge of a fitted nonlinear manifold, derives geometric coordinate systems to capture deviations from the manifold, and uses a data-driven EM algorithm for parameter learning.", "result": "PGPCA effectively models data distributions around various manifolds, outperforms PPCA for such data, enables testing of geometric vs Euclidean coordinate systems, and performs dimensionality reduction both around and on the manifold.", "conclusion": "PGPCA provides valuable capabilities for enhancing dimensionality reduction efficacy in high-dimensional data with noise distributed around nonlinear manifolds, making it particularly useful for neuroscience applications."}}
{"id": "2509.18470", "categories": ["cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18470", "abs": "https://arxiv.org/abs/2509.18470", "authors": ["Xiaozhou Tan", "Minghui Zhao", "Mattias Cross", "Anton Ragni"], "title": "Discrete-time diffusion-like models for speech synthesis", "comment": null, "summary": "Diffusion models have attracted a lot of attention in recent years. These\nmodels view speech generation as a continuous-time process. For efficient\ntraining, this process is typically restricted to additive Gaussian noising,\nwhich is limiting. For inference, the time is typically discretized, leading to\nthe mismatch between continuous training and discrete sampling conditions.\nRecently proposed discrete-time processes, on the other hand, usually do not\nhave these limitations, may require substantially fewer inference steps, and\nare fully consistent between training/inference conditions. This paper explores\nsome diffusion-like discrete-time processes and proposes some new variants.\nThese include processes applying additive Gaussian noise, multiplicative\nGaussian noise, blurring noise and a mixture of blurring and Gaussian noises.\nThe experimental results suggest that discrete-time processes offer comparable\nsubjective and objective speech quality to their widely popular continuous\ncounterpart, with more efficient and consistent training and inference schemas.", "AI": {"tldr": "This paper proposes discrete-time diffusion processes as alternatives to continuous-time models, addressing limitations like training-inference mismatch and inefficient sampling while maintaining comparable speech quality.", "motivation": "Continuous-time diffusion models have limitations including restrictive additive Gaussian noising, training-inference mismatch due to time discretization, and inefficient sampling requiring many steps. Discrete-time processes offer more flexibility and consistency.", "method": "The paper explores discrete-time diffusion processes including variants with additive Gaussian noise, multiplicative Gaussian noise, blurring noise, and mixed blurring-Gaussian noise, proposing new variants of these discrete-time processes.", "result": "Experimental results show that discrete-time processes achieve comparable subjective and objective speech quality to continuous-time models, while offering more efficient and consistent training and inference.", "conclusion": "Discrete-time diffusion processes are viable alternatives to continuous-time models, providing similar speech quality with improved efficiency and consistency between training and inference conditions."}}
{"id": "2509.18471", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.18471", "abs": "https://arxiv.org/abs/2509.18471", "authors": ["Mariano Tepper", "Ted Willke"], "title": "Individualized non-uniform quantization for vector search", "comment": null, "summary": "Embedding vectors are widely used for representing unstructured data and\nsearching through it for semantically similar items. However, the large size of\nthese vectors, due to their high-dimensionality, creates problems for modern\nvector search techniques: retrieving large vectors from memory/storage is\nexpensive and their footprint is costly. In this work, we present NVQ\n(non-uniform vector quantization), a new vector compression technique that is\ncomputationally and spatially efficient in the high-fidelity regime. The core\nin NVQ is to use novel parsimonious and computationally efficient\nnonlinearities for building non-uniform vector quantizers. Critically, these\nquantizers are \\emph{individually} learned for each indexed vector. Our\nexperimental results show that NVQ exhibits improved accuracy compared to the\nstate of the art with a minimal computational cost.", "AI": {"tldr": "NVQ is a new vector compression technique that uses non-uniform vector quantization with individually learned quantizers for each vector, achieving improved accuracy with minimal computational cost.", "motivation": "Embedding vectors are large due to high-dimensionality, creating problems for vector search techniques as retrieving large vectors from memory/storage is expensive and their footprint is costly.", "method": "Uses novel parsimonious and computationally efficient nonlinearities to build non-uniform vector quantizers that are individually learned for each indexed vector.", "result": "Experimental results show NVQ exhibits improved accuracy compared to state-of-the-art methods.", "conclusion": "NVQ provides a computationally and spatially efficient vector compression technique for high-fidelity regime applications."}}
{"id": "2509.18480", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.18480", "abs": "https://arxiv.org/abs/2509.18480", "authors": ["Yuyang Wang", "Jiarui Lu", "Navdeep Jaitly", "Josh Susskind", "Miguel Angel Bautista"], "title": "SimpleFold: Folding Proteins is Simpler than You Think", "comment": "28 pages, 11 figures, 13 tables", "summary": "Protein folding models have achieved groundbreaking results typically via a\ncombination of integrating domain knowledge into the architectural blocks and\ntraining pipelines. Nonetheless, given the success of generative models across\ndifferent but related problems, it is natural to question whether these\narchitectural designs are a necessary condition to build performant models. In\nthis paper, we introduce SimpleFold, the first flow-matching based protein\nfolding model that solely uses general purpose transformer blocks. Protein\nfolding models typically employ computationally expensive modules involving\ntriangular updates, explicit pair representations or multiple training\nobjectives curated for this specific domain. Instead, SimpleFold employs\nstandard transformer blocks with adaptive layers and is trained via a\ngenerative flow-matching objective with an additional structural term. We scale\nSimpleFold to 3B parameters and train it on approximately 9M distilled protein\nstructures together with experimental PDB data. On standard folding benchmarks,\nSimpleFold-3B achieves competitive performance compared to state-of-the-art\nbaselines, in addition SimpleFold demonstrates strong performance in ensemble\nprediction which is typically difficult for models trained via deterministic\nreconstruction objectives. Due to its general-purpose architecture, SimpleFold\nshows efficiency in deployment and inference on consumer-level hardware.\nSimpleFold challenges the reliance on complex domain-specific architectures\ndesigns in protein folding, opening up an alternative design space for future\nprogress.", "AI": {"tldr": "SimpleFold is a protein folding model that uses standard transformer blocks with flow-matching training, achieving competitive performance without domain-specific architectural designs.", "motivation": "To challenge the necessity of complex domain-specific architectures in protein folding by demonstrating that general-purpose transformer blocks can achieve state-of-the-art results.", "method": "Uses standard transformer blocks with adaptive layers, trained via generative flow-matching objective with structural term. Scaled to 3B parameters on 9M distilled protein structures and PDB data.", "result": "Achieves competitive performance on standard folding benchmarks and shows strong ensemble prediction capabilities. Efficient deployment on consumer hardware.", "conclusion": "SimpleFold demonstrates that complex domain-specific architectures are not necessary for high-performance protein folding, opening alternative design possibilities."}}
{"id": "2509.18483", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18483", "abs": "https://arxiv.org/abs/2509.18483", "authors": ["Abhijit Sen", "Illya V. Lukin", "Kurt Jacobs", "Lev Kaplan", "Andrii G. Sotnikov", "Denys I. Bondar"], "title": "Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints", "comment": null, "summary": "The prediction of quantum dynamical responses lies at the heart of modern\nphysics. Yet, modeling these time-dependent behaviors remains a formidable\nchallenge because quantum systems evolve in high-dimensional Hilbert spaces,\noften rendering traditional numerical methods computationally prohibitive.\nWhile large language models have achieved remarkable success in sequential\nprediction, quantum dynamics presents a fundamentally different challenge:\nforecasting the entire temporal evolution of quantum systems rather than merely\nthe next element in a sequence. Existing neural architectures such as recurrent\nand convolutional networks often require vast training datasets and suffer from\nspurious oscillations that compromise physical interpretability. In this work,\nwe introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)\naugmented with physics-informed loss functions that enforce the Ehrenfest\ntheorems. Our method achieves superior accuracy with significantly less\ntraining data: it requires only 5.4 percent of the samples (200) compared to\nTemporal Convolution Networks (3,700). We further introduce the Chain of KANs,\na novel architecture that embeds temporal causality directly into the model\ndesign, making it particularly well-suited for time series modeling. Our\nresults demonstrate that physics-informed KANs offer a compelling advantage\nover conventional black-box models, maintaining both mathematical rigor and\nphysical consistency while dramatically reducing data requirements.", "AI": {"tldr": "Physics-informed Kolmogorov Arnold Networks (KANs) with Ehrenfest theorem constraints provide superior quantum dynamics prediction using only 5.4% of training data compared to conventional methods.", "motivation": "Quantum dynamical prediction is computationally challenging due to high-dimensional Hilbert spaces, and existing neural networks require large datasets and suffer from spurious oscillations that compromise physical interpretability.", "method": "Introduce Kolmogorov Arnold Networks (KANs) augmented with physics-informed loss functions enforcing Ehrenfest theorems, plus Chain of KANs architecture embedding temporal causality for time series modeling.", "result": "Achieves superior accuracy with only 200 training samples (5.4% of what Temporal Convolution Networks require), maintaining mathematical rigor and physical consistency.", "conclusion": "Physics-informed KANs offer compelling advantages over black-box models by dramatically reducing data requirements while preserving physical interpretability and accuracy."}}
{"id": "2509.18499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18499", "abs": "https://arxiv.org/abs/2509.18499", "authors": ["Rachel Chung", "Pratyush Nidhi Sharma", "Mikko Siponen", "Rohit Vadodaria", "Luke Smith"], "title": "Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models", "comment": "Presented at the Association of Certified Fraud Examiners (ACFE)\n  Research Institute Annual Meeting, Las Vegas, NV, (2024)", "summary": "Money laundering is a critical global issue for financial institutions.\nAutomated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),\ncan be trained to identify illicit transactions in real time. A major issue for\ndeveloping such models is the lack of access to training data due to privacy\nand confidentiality concerns. Synthetically generated data that mimics the\nstatistical properties of real data but preserves privacy and confidentiality\nhas been proposed as a solution. However, training AML models on purely\nsynthetic datasets presents its own set of challenges. This article proposes\nthe use of hybrid datasets to augment the utility of synthetic datasets by\nincorporating publicly available, easily accessible, and real-world features.\nThese additions demonstrate that hybrid datasets not only preserve privacy but\nalso improve model utility, offering a practical pathway for financial\ninstitutions to enhance AML systems.", "AI": {"tldr": "The paper proposes using hybrid datasets combining synthetic and real-world features to improve anti-money laundering (AML) models while preserving privacy.", "motivation": "Financial institutions face challenges in training AML models due to privacy concerns limiting access to real transaction data. Synthetic data alone has limitations for effective model training.", "method": "The article proposes augmenting synthetic datasets with publicly available, easily accessible real-world features to create hybrid datasets that preserve privacy while enhancing model utility.", "result": "Hybrid datasets demonstrate improved model utility compared to purely synthetic datasets while maintaining privacy and confidentiality protections.", "conclusion": "Hybrid datasets offer a practical solution for financial institutions to enhance AML systems by balancing privacy concerns with model effectiveness."}}
{"id": "2509.18521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18521", "abs": "https://arxiv.org/abs/2509.18521", "authors": ["Yuzhen Zhou", "Jiajun Li", "Yusheng Su", "Gowtham Ramesh", "Zilin Zhu", "Xiang Long", "Chenyang Zhao", "Jin Pan", "Xiaodong Yu", "Ze Wang", "Kangrui Du", "Jialian Wu", "Ximeng Sun", "Jiang Liu", "Qiaolin Yu", "Hao Chen", "Zicheng Liu", "Emad Barsoum"], "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation", "comment": null, "summary": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale\npre-trained language models (LLMs). Successive generations, including GPT-o\nseries, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale\nRL training to enhance reasoning and coding capabilities. To meet the\ncommunity's growing RL needs, numerous RL frameworks have been proposed. Most\nof these frameworks primarily rely on inference engines for rollout generation\nand training engines for policy updates. However, RL training remains\ncomputationally expensive, with rollout generation accounting for more than 90%\nof total runtime. In addition, its efficiency is often constrained by the\nlong-tail distribution of rollout response lengths, where a few lengthy\nresponses stall entire batches, leaving GPUs idle and underutilized. As model\nand rollout sizes continue to grow, this bottleneck increasingly limits\nscalability. To address this challenge, we propose Active Partial Rollouts in\nReinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the\nrollout phase, APRIL over-provisions rollout requests, terminates once the\ntarget number of responses is reached, and recycles incomplete responses for\ncontinuation in future steps. This strategy ensures that no rollouts are\ndiscarded while substantially reducing GPU idle time. Experiments show that\nAPRIL improves rollout throughput by at most 44% across commonly used RL\nalgorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%\nhigher final accuracy across tasks. Moreover, APRIL is both framework and\nhardware agnostic, already integrated into the slime RL framework, and\ndeployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies\nsystem-level and algorithmic considerations in proposing APRIL, with the aim of\nadvancing RL training efficiency and inspiring further optimizations in RL\nsystems.", "AI": {"tldr": "APRIL is a novel RL training method that addresses the inefficiency caused by long-tail distribution of rollout response lengths by over-provisioning requests, terminating once target responses are reached, and recycling incomplete responses.", "motivation": "RL training is computationally expensive with rollout generation accounting for over 90% of runtime, and efficiency is constrained by long-tail distribution where a few lengthy responses stall entire batches, leaving GPUs idle.", "method": "APRIL over-provisions rollout requests, terminates once the target number of responses is reached, and recycles incomplete responses for continuation in future steps, ensuring no rollouts are discarded while reducing GPU idle time.", "result": "APRIL improves rollout throughput by up to 44% across RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves up to 8% higher final accuracy across tasks. It's framework and hardware agnostic.", "conclusion": "APRIL unifies system-level and algorithmic considerations to advance RL training efficiency and inspire further optimizations in RL systems."}}
{"id": "2509.18529", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.18529", "abs": "https://arxiv.org/abs/2509.18529", "authors": ["Mingqian Ma"], "title": "Reverse-Complement Consistency for DNA Language Models", "comment": null, "summary": "A fundamental property of DNA is that the reverse complement (RC) of a\nsequence often carries identical biological meaning. However, state-of-the-art\nDNA language models frequently fail to capture this symmetry, producing\ninconsistent predictions for a sequence and its RC counterpart, which\nundermines their reliability. In this work, we introduce Reverse-Complement\nConsistency Regularization (RCCR), a simple and model-agnostic fine-tuning\nobjective that directly penalizes the divergence between a model's prediction\non a sequence and the aligned prediction on its reverse complement. We evaluate\nRCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,\nDNABERT-2) on a wide range of genomic tasks, including sequence classification,\nscalar regression, and profile prediction. Our experiments show that RCCR\nsubstantially improves RC robustness by dramatically reducing prediction flips\nand errors, all while maintaining or improving task accuracy compared to\nbaselines such as RC data augmentation and test-time averaging. By integrating\na key biological prior directly into the learning process, RCCR produces a\nsingle, intrinsically robust, and computationally efficient model fine-tuning\nrecipe for diverse biology tasks.", "AI": {"tldr": "RCCR is a fine-tuning method that enforces reverse-complement consistency in DNA language models by penalizing prediction divergence between sequences and their reverse complements, improving robustness while maintaining accuracy.", "motivation": "DNA language models often fail to capture the fundamental biological symmetry where reverse complements carry identical meaning, leading to inconsistent predictions that undermine model reliability.", "method": "Reverse-Complement Consistency Regularization (RCCR) - a model-agnostic fine-tuning objective that directly penalizes divergence between predictions on sequences and their reverse complements.", "result": "RCCR substantially improves reverse-complement robustness across three DNA model backbones on various genomic tasks, reducing prediction flips and errors while maintaining or improving task accuracy compared to baselines.", "conclusion": "RCCR provides a single, intrinsically robust, and computationally efficient fine-tuning recipe that integrates biological symmetry priors directly into DNA language model learning."}}
{"id": "2509.18542", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18542", "abs": "https://arxiv.org/abs/2509.18542", "authors": ["Qi Wang", "Hanyang Peng", "Yue Yu"], "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating\nlarge parameter sets sparsely, minimizing computational overhead. To circumvent\nthe prohibitive cost of training MoEs from scratch, recent work employs\nupcycling, reusing a single pre-trained dense model by replicating its\nfeed-forward network (FFN) layers into experts. However, this limits expert\ndiversity, as all experts originate from a single pre-trained dense model. This\npaper addresses this limitation by constructing powerful MoE models using\nexperts sourced from multiple identically-architected but disparate pre-trained\nmodels (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact\nthat these source models occupy disparate, dissonant regions of the parameter\nspace, making direct upcycling prone to severe performance degradation. To\novercome this, we propose Symphony-MoE, a novel two-stage framework designed to\nharmonize these models into a single, coherent expert mixture. First, we\nestablish this harmony in a training-free manner: we construct a shared\nbackbone via a layer-aware fusion strategy and, crucially, alleviate parameter\nmisalignment among experts using activation-based functional alignment.\nSubsequently, a single lightweight stage of router training coordinates the\nentire architecture. Experiments demonstrate that our method successfully\nintegrates experts from heterogeneous sources, achieving an MoE model that\nsignificantly surpasses baselines in multi-domain tasks and out-of-distribution\ngeneralization.", "AI": {"tldr": "Symphony-MoE is a novel framework that constructs powerful Mixture-of-Experts models by harmonizing experts from multiple pre-trained models, overcoming parameter misalignment through training-free fusion and functional alignment.", "motivation": "Existing MoE upcycling methods limit expert diversity by using only one pre-trained model. This paper addresses the challenge of integrating experts from multiple disparate pre-trained models that occupy different parameter spaces.", "method": "A two-stage framework: 1) Training-free harmony via layer-aware fusion and activation-based functional alignment to address parameter misalignment; 2) Lightweight router training to coordinate the entire architecture.", "result": "Successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.", "conclusion": "Symphony-MoE enables effective construction of powerful MoE models by harmonizing diverse experts from multiple pre-trained sources, overcoming the limitations of single-source upcycling."}}
{"id": "2509.18552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18552", "abs": "https://arxiv.org/abs/2509.18552", "authors": ["Kiril Bangachev", "Guy Bresler", "Iliyas Noman", "Yury Polyanskiy"], "title": "Global Minimizers of Sigmoid Contrastive Loss", "comment": "Author names listed in alphabetical order. NeurIPS 2025", "summary": "The meta-task of obtaining and aligning representations through contrastive\npretraining is steadily gaining importance since its introduction in CLIP and\nALIGN. In this paper we theoretically explain the advantages of synchronizing\nwith trainable inverse temperature and bias under the sigmoid loss, as\nimplemented in the recent SigLIP and SigLIP2 models of Google DeepMind.\nTemperature and bias can drive the loss function to zero for a rich class of\nconfigurations that we call $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object\nrelated to spherical codes and are parametrized by a margin $\\mathsf{m}$ and\nrelative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of\nconstellations to theoretically justify the success of SigLIP on retrieval, to\nexplain the modality gap present in SigLIP, and to identify the necessary\ndimension for producing high-quality representations. Finally, we propose a\nreparameterization of the sigmoid loss with explicit relative bias, which\nimproves training dynamics in experiments with synthetic data.", "AI": {"tldr": "Theoretical analysis of SigLIP's contrastive learning with trainable temperature and bias, introducing (m, b_rel)-Constellations to explain representation quality and modality gap.", "motivation": "To theoretically explain the advantages of synchronizing temperature and bias in contrastive pretraining models like SigLIP and SigLIP2, and understand why they succeed in retrieval tasks.", "method": "Introduces (m, b_rel)-Constellations as a novel combinatorial object related to spherical codes, parametrized by margin and relative bias. Uses this characterization to analyze representation quality and modality gap.", "result": "Provides theoretical justification for SigLIP's success on retrieval, explains the modality gap, identifies necessary dimensions for high-quality representations, and proposes a reparameterization that improves training dynamics.", "conclusion": "The theoretical framework of (m, b_rel)-Constellations successfully explains SigLIP's performance characteristics and leads to improved training methods through explicit relative bias parameterization."}}
{"id": "2509.18568", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18568", "abs": "https://arxiv.org/abs/2509.18568", "authors": ["Niharika Tewari", "Nguyen Linh Dan Le", "Mujie Liu", "Jing Ren", "Ziqi Xu", "Tabinda Sarwar", "Veeky Baths", "Feng Xia"], "title": "Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia", "comment": null, "summary": "Dementia is a progressive neurodegenerative disorder with multiple\netiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal\ndementia, and vascular dementia. Its clinical and biological heterogeneity\nmakes diagnosis and subtype differentiation highly challenging. Graph Neural\nNetworks (GNNs) have recently shown strong potential in modeling brain\nconnectivity, but their limited robustness, data scarcity, and lack of\ninterpretability constrain clinical adoption. Explainable Graph Neural Networks\n(XGNNs) have emerged to address these barriers by combining graph-based\nlearning with interpretability, enabling the identification of disease-relevant\nbiomarkers, analysis of brain network disruptions, and provision of transparent\ninsights for clinicians. This paper presents the first comprehensive review\ndedicated to XGNNs in dementia research. We examine their applications across\nAlzheimer's disease, Parkinson's disease, mild cognitive impairment, and\nmulti-disease diagnosis. A taxonomy of explainability methods tailored for\ndementia-related tasks is introduced, alongside comparisons of existing models\nin clinical scenarios. We also highlight challenges such as limited\ngeneralizability, underexplored domains, and the integration of Large Language\nModels (LLMs) for early detection. By outlining both progress and open\nproblems, this review aims to guide future work toward trustworthy, clinically\nmeaningful, and scalable use of XGNNs in dementia research.", "AI": {"tldr": "This paper provides the first comprehensive review of Explainable Graph Neural Networks (XGNNs) in dementia research, covering applications across various dementia subtypes and introducing a taxonomy of explainability methods tailored for clinical scenarios.", "motivation": "Dementia's clinical and biological heterogeneity makes diagnosis and subtype differentiation challenging. While GNNs show potential in modeling brain connectivity, their limited robustness, data scarcity, and lack of interpretability constrain clinical adoption. XGNNs address these barriers by combining graph-based learning with interpretability.", "method": "The paper presents a comprehensive review examining XGNN applications across Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and multi-disease diagnosis. It introduces a taxonomy of explainability methods and compares existing models in clinical scenarios.", "result": "The review identifies XGNNs' ability to identify disease-relevant biomarkers, analyze brain network disruptions, and provide transparent insights for clinicians. It also highlights challenges including limited generalizability, underexplored domains, and integration of LLMs for early detection.", "conclusion": "By outlining both progress and open problems, this review aims to guide future work toward trustworthy, clinically meaningful, and scalable use of XGNNs in dementia research, addressing barriers to clinical adoption through improved interpretability and robustness."}}
{"id": "2509.18573", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18573", "abs": "https://arxiv.org/abs/2509.18573", "authors": ["Dong Chen", "Jian Liu", "Chun-Long Chen", "Guo-Wei Wei"], "title": "Interaction Topological Transformer for Multiscale Learning in Porous Materials", "comment": "4 figures, 2 tables", "summary": "Porous materials exhibit vast structural diversity and support critical\napplications in gas storage, separations, and catalysis. However, predictive\nmodeling remains challenging due to the multiscale nature of structure-property\nrelationships, where performance is governed by both local chemical\nenvironments and global pore-network topology. These complexities, combined\nwith sparse and unevenly distributed labeled data, hinder generalization across\nmaterial families. We propose the Interaction Topological Transformer (ITT), a\nunified data-efficient framework that leverages novel interaction topology to\ncapture materials information across multiple scales and multiple levels,\nincluding structural, elemental, atomic, and pairwise-elemental organization.\nITT extracts scale-aware features that reflect both compositional and\nrelational structure within complex porous frameworks, and integrates them\nthrough a built-in Transformer architecture that supports joint reasoning\nacross scales. Trained using a two-stage strategy, i.e., self-supervised\npretraining on 0.6 million unlabeled structures followed by supervised\nfine-tuning, ITT achieves state-of-the-art, accurate, and transferable\npredictions for adsorption, transport, and stability properties. This framework\nprovides a principled and scalable path for learning-guided discovery in\nstructurally and chemically diverse porous materials.", "AI": {"tldr": "Proposes Interaction Topological Transformer (ITT) - a unified framework for predictive modeling of porous materials that captures multi-scale structure-property relationships using interaction topology and Transformer architecture.", "motivation": "Predictive modeling of porous materials is challenging due to multiscale structure-property relationships and sparse labeled data, hindering generalization across material families.", "method": "ITT leverages interaction topology to capture materials information across structural, elemental, atomic, and pairwise-elemental levels. Uses two-stage training: self-supervised pretraining on 0.6M unlabeled structures followed by supervised fine-tuning.", "result": "ITT achieves state-of-the-art, accurate, and transferable predictions for adsorption, transport, and stability properties of porous materials.", "conclusion": "The framework provides a principled and scalable path for learning-guided discovery in structurally and chemically diverse porous materials."}}
{"id": "2509.18584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18584", "abs": "https://arxiv.org/abs/2509.18584", "authors": ["Mingchun Sun", "Rongqiang Zhao", "Jie Liu"], "title": "DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation", "comment": null, "summary": "Diffusion models are the mainstream approach for time series generation\ntasks. However, existing diffusion models for time series generation require\nretraining the entire framework to introduce specific conditional guidance.\nThere also exists a certain degree of distributional bias between the generated\ndata and the real data, which leads to potential model biases in downstream\ntasks. Additionally, the complexity of diffusion models and the latent spaces\nleads to an uninterpretable inference process. To address these issues, we\npropose the data style-guided diffusion model (DS-Diffusion). In the\nDS-Diffusion, a diffusion framework based on style-guided kernels is developed\nto avoid retraining for specific conditions. The time-information based\nhierarchical denoising mechanism (THD) is developed to reduce the\ndistributional bias between the generated data and the real data. Furthermore,\nthe generated samples can clearly indicate the data style from which they\noriginate. We conduct comprehensive evaluations using multiple public datasets\nto validate our approach. Experimental results show that, compared to the\nstate-of-the-art model such as ImagenTime, the predictive score and the\ndiscriminative score decrease by 5.56% and 61.55%, respectively. The\ndistributional bias between the generated data and the real data is further\nreduced, the inference process is also more interpretable. Moreover, by\neliminating the need to retrain the diffusion model, the flexibility and\nadaptability of the model to specific conditions are also enhanced.", "AI": {"tldr": "DS-Diffusion is a novel time series generation model that addresses limitations of existing diffusion models by introducing style-guided kernels and hierarchical denoising to reduce distributional bias and improve interpretability without requiring retraining for specific conditions.", "motivation": "Existing diffusion models for time series generation require retraining for specific conditional guidance, suffer from distributional bias between generated and real data, and have uninterpretable inference processes due to complex latent spaces.", "method": "Proposes DS-Diffusion with: 1) Diffusion framework based on style-guided kernels to avoid retraining for specific conditions, 2) Time-information based hierarchical denoising mechanism (THD) to reduce distributional bias, 3) Clear indication of data style origins for interpretability.", "result": "Compared to state-of-the-art ImagenTime: predictive score decreases by 5.56%, discriminative score decreases by 61.55%. Distributional bias is further reduced, inference process is more interpretable, and model flexibility/adaptability is enhanced without retraining.", "conclusion": "DS-Diffusion effectively addresses key limitations of existing diffusion models for time series generation by reducing distributional bias, improving interpretability, and eliminating the need for retraining, while achieving superior performance metrics."}}
{"id": "2509.18607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18607", "abs": "https://arxiv.org/abs/2509.18607", "authors": ["Qiuhai Zeng", "Sarvesh Rajkumar", "Di Wang", "Narendra Gyanchandani", "Wenbo Yan"], "title": "Reflect before Act: Proactive Error Correction in Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ninteractive decision-making tasks, but existing methods often struggle with\nerror accumulation and lack robust self-correction mechanisms. We introduce\n\"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based\ndecision-making by introducing a critical reflect step prior to taking the next\naction. This approach allows for immediate error correction, ensuring smooth\naction path and adaptibity to environment feedback. We evaluate REBACT on three\ndiverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results\ndemonstrate that REBACT significantly outperforms strong baselines, improving\nsuccess rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld\n(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using\nClaude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's\nperformance improvements are achieved with only a few modification steps,\ndemonstrating its computational efficiency.", "AI": {"tldr": "REBACT introduces a 'reflect before act' approach that adds a reflection step before each action in LLM-based decision-making, enabling immediate error correction and improving success rates across multiple interactive environments.", "motivation": "Existing LLM methods for interactive decision-making struggle with error accumulation and lack robust self-correction mechanisms, leading to suboptimal performance.", "method": "REBACT adds a critical reflect step prior to taking the next action, allowing for immediate error correction and adaptation to environment feedback using Claude3.5-sonnet as the underlying LLM.", "result": "REBACT significantly outperforms baselines with success rate improvements of 24% on WebShop (61%), 6.72% on ALFWorld (98.51%), and 0.5% on TextCraft (99.5%), achieving high performance with few modification steps.", "conclusion": "The reflect-before-act approach effectively enhances LLM decision-making by enabling robust self-correction while maintaining computational efficiency."}}
{"id": "2509.18611", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18611", "abs": "https://arxiv.org/abs/2509.18611", "authors": ["Zituo Chen", "Sili Deng"], "title": "Flow marching for a generative PDE foundation model", "comment": null, "summary": "Pretraining on large-scale collections of PDE-governed spatiotemporal\ntrajectories has recently shown promise for building generalizable models of\ndynamical systems. Yet most existing PDE foundation models rely on\ndeterministic Transformer architectures, which lack generative flexibility for\nmany science and engineering applications. We propose Flow Marching, an\nalgorithm that bridges neural operator learning with flow matching motivated by\nan analysis of error accumulation in physical dynamical systems, and we build a\ngenerative PDE foundation model on top of it. By jointly sampling the noise\nlevel and the physical time step between adjacent states, the model learns a\nunified velocity field that transports a noisy current state toward its clean\nsuccessor, reducing long-term rollout drift while enabling uncertainty-aware\nensemble generations. Alongside this core algorithm, we introduce a\nPhysics-Pretrained Variational Autoencoder (P2VAE) to embed physical states\ninto a compact latent space, and an efficient Flow Marching Transformer (FMT)\nthat combines a diffusion-forcing scheme with latent temporal pyramids,\nachieving up to 15x greater computational efficiency than full-length video\ndiffusion models and thereby enabling large-scale pretraining at substantially\nreduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE\nfamilies and train suites of P2VAEs and FMTs at multiple scales. On downstream\nevaluation, we benchmark on unseen Kolmogorov turbulence with few-shot\nadaptation, demonstrate long-term rollout stability over deterministic\ncounterparts, and present uncertainty-stratified ensemble results, highlighting\nthe importance of generative PDE foundation models for real-world applications.", "AI": {"tldr": "Flow Marching bridges neural operator learning with flow matching to create a generative PDE foundation model that reduces long-term rollout drift while enabling uncertainty-aware ensemble generations.", "motivation": "Existing PDE foundation models rely on deterministic Transformer architectures which lack generative flexibility needed for many science and engineering applications.", "method": "Proposes Flow Marching algorithm that jointly samples noise level and physical time step, learns unified velocity field, uses Physics-Pretrained Variational Autoencoder (P2VAE) for state embedding, and Flow Marching Transformer (FMT) with diffusion-forcing and latent temporal pyramids for efficiency.", "result": "Achieves up to 15x greater computational efficiency than full-length video diffusion models, demonstrates long-term rollout stability over deterministic counterparts, and shows effective few-shot adaptation on unseen Kolmogorov turbulence.", "conclusion": "The approach highlights the importance of generative PDE foundation models for real-world applications, enabling uncertainty-stratified ensemble results and large-scale pretraining at reduced cost."}}
{"id": "2509.18629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18629", "abs": "https://arxiv.org/abs/2509.18629", "authors": ["Abel Gurung", "Joseph Campbell"], "title": "HyperAdapt: Simple High-Rank Adaptation", "comment": null, "summary": "Foundation models excel across diverse tasks, but adapting them to\nspecialized applications often requires fine-tuning, an approach that is memory\nand compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate\nthis by updating only a small subset of weights. In this paper, we introduce\nHyperAdapt, a parameter-efficient fine-tuning method that significantly reduces\nthe number of trainable parameters compared to state-of-the-art methods like\nLoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying\nrow- and column-wise scaling through diagonal matrices, thereby inducing a\nhigh-rank update while requiring only $n+m$ trainable parameters for an $n\n\\times m$ matrix. Theoretically, we establish an upper bound on the rank of\nHyperAdapt's updates, and empirically, we confirm that it consistently induces\nhigh-rank transformations across model layers. Experiments on GLUE, arithmetic\nreasoning, and commonsense reasoning benchmarks with models up to 14B\nparameters demonstrate that HyperAdapt matches or nearly matches the\nperformance of full fine-tuning and state-of-the-art PEFT methods while using\norders of magnitude fewer trainable parameters.", "AI": {"tldr": "HyperAdapt is a parameter-efficient fine-tuning method that uses diagonal matrices for row- and column-wise scaling, achieving high-rank updates with only n+m parameters for an n\u00d7m matrix.", "motivation": "To reduce the memory and computational costs of fine-tuning foundation models while maintaining performance comparable to full fine-tuning.", "method": "HyperAdapt adapts pre-trained weight matrices by applying row- and column-wise scaling through diagonal matrices, requiring only n+m trainable parameters for an n\u00d7m matrix.", "result": "Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters show HyperAdapt matches or nearly matches full fine-tuning and state-of-the-art PEFT methods while using significantly fewer parameters.", "conclusion": "HyperAdapt provides an efficient alternative to full fine-tuning and existing PEFT methods, achieving high performance with minimal parameter updates."}}
{"id": "2509.18653", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18653", "abs": "https://arxiv.org/abs/2509.18653", "authors": ["Paris A. Karakasis", "Nicholas D. Sidiropoulos"], "title": "Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering", "comment": "13 pages, Submitted to IEEE Transactions on Signal Processing", "summary": "We introduce a novel framework for clustering a collection of tall matrices\nbased on their column spaces, a problem we term Subspace Clustering of\nSubspaces (SCoS). Unlike traditional subspace clustering methods that assume\nvectorized data, our formulation directly models each data sample as a matrix\nand clusters them according to their underlying subspaces. We establish\nconceptual links to Subspace Clustering and Generalized Canonical Correlation\nAnalysis (GCCA), and clarify key differences that arise in this more general\nsetting. Our approach is based on a Block Term Decomposition (BTD) of a\nthird-order tensor constructed from the input matrices, enabling joint\nestimation of cluster memberships and partially shared subspaces. We provide\nthe first identifiability results for this formulation and propose scalable\noptimization algorithms tailored to large datasets. Experiments on real-world\nhyperspectral imaging datasets demonstrate that our method achieves superior\nclustering accuracy and robustness, especially under high noise and\ninterference, compared to existing subspace clustering techniques. These\nresults highlight the potential of the proposed framework in challenging\nhigh-dimensional applications where structure exists beyond individual data\nvectors.", "AI": {"tldr": "A novel framework called Subspace Clustering of Subspaces (SCoS) that clusters tall matrices based on their column spaces using Block Term Decomposition of third-order tensors.", "motivation": "Traditional subspace clustering methods assume vectorized data, but many real-world applications have data naturally represented as matrices with underlying subspace structure that should be preserved.", "method": "Constructs a third-order tensor from input matrices and uses Block Term Decomposition to jointly estimate cluster memberships and partially shared subspaces, with scalable optimization algorithms for large datasets.", "result": "Superior clustering accuracy and robustness compared to existing subspace clustering techniques, especially under high noise and interference, as demonstrated on hyperspectral imaging datasets.", "conclusion": "The framework shows strong potential for challenging high-dimensional applications where structure exists beyond individual data vectors, providing the first identifiability results for this formulation."}}
{"id": "2509.18703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18703", "abs": "https://arxiv.org/abs/2509.18703", "authors": ["Jakub Adamczyk"], "title": "Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology", "comment": null, "summary": "This research focuses on rational pesticide design, using graph machine\nlearning to accelerate the development of safer, eco-friendly agrochemicals,\ninspired by in silico methods in drug discovery. With an emphasis on\necotoxicology, the initial contributions include the creation of ApisTox, the\nlargest curated dataset on pesticide toxicity to honey bees. We conducted a\nbroad evaluation of machine learning (ML) models for molecular graph\nclassification, including molecular fingerprints, graph kernels, GNNs, and\npretrained transformers. The results show that methods successful in medicinal\nchemistry often fail to generalize to agrochemicals, underscoring the need for\ndomain-specific models and benchmarks. Future work will focus on developing a\ncomprehensive benchmarking suite and designing ML models tailored to the unique\nchallenges of pesticide discovery.", "AI": {"tldr": "This paper presents a rational pesticide design approach using graph machine learning, creating ApisTox - the largest curated dataset on pesticide toxicity to honey bees, and evaluates various ML models for agrochemical applications.", "motivation": "To accelerate development of safer, eco-friendly agrochemicals by adapting in silico methods from drug discovery to pesticide design, with emphasis on ecotoxicology.", "method": "Created ApisTox dataset and conducted broad evaluation of ML models including molecular fingerprints, graph kernels, GNNs, and pretrained transformers for molecular graph classification.", "result": "Found that methods successful in medicinal chemistry often fail to generalize to agrochemicals, highlighting the need for domain-specific models and benchmarks.", "conclusion": "Future work will focus on developing comprehensive benchmarking suite and designing ML models tailored to pesticide discovery challenges."}}
{"id": "2509.18714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18714", "abs": "https://arxiv.org/abs/2509.18714", "authors": ["Zhenyu Tao", "Wei Xu", "Xiaohu You"], "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications", "comment": "This paper is accepted by the 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "The bisimulation metric (BSM) is a powerful tool for computing state\nsimilarities within a Markov decision process (MDP), revealing that states\ncloser in BSM have more similar optimal value functions. While BSM has been\nsuccessfully utilized in reinforcement learning (RL) for tasks like state\nrepresentation learning and policy exploration, its application to multiple-MDP\nscenarios, such as policy transfer, remains challenging. Prior work has\nattempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis\nof its mathematical properties has limited further theoretical progress. In\nthis work, we formally establish a generalized bisimulation metric (GBSM)\nbetween pairs of MDPs, which is rigorously proven with the three fundamental\nproperties: GBSM symmetry, inter-MDP triangle inequality, and the distance\nbound on identical state spaces. Leveraging these properties, we theoretically\nanalyse policy transfer, state aggregation, and sampling-based estimation in\nMDPs, obtaining explicit bounds that are strictly tighter than those derived\nfrom the standard BSM. Additionally, GBSM provides a closed-form sample\ncomplexity for estimation, improving upon existing asymptotic results based on\nBSM. Numerical results validate our theoretical findings and demonstrate the\neffectiveness of GBSM in multi-MDP scenarios.", "AI": {"tldr": "The paper introduces a generalized bisimulation metric (GBSM) for comparing pairs of MDPs, establishing rigorous mathematical properties and providing tighter theoretical bounds for policy transfer, state aggregation, and sampling-based estimation than standard BSM.", "motivation": "While bisimulation metric (BSM) is effective for single MDP analysis, its application to multiple-MDP scenarios like policy transfer remains challenging due to lack of rigorous mathematical properties when generalized to MDP pairs.", "method": "Formally establish GBSM between pairs of MDPs with proven fundamental properties: symmetry, inter-MDP triangle inequality, and distance bounds on identical state spaces. Use these properties to analyze policy transfer, state aggregation, and sampling-based estimation.", "result": "GBSM provides explicit bounds that are strictly tighter than standard BSM, offers closed-form sample complexity for estimation (improving upon asymptotic BSM results), and numerical validation demonstrates effectiveness in multi-MDP scenarios.", "conclusion": "GBSM successfully generalizes bisimulation metrics to multiple-MDP settings with rigorous mathematical foundations, enabling more effective theoretical analysis and practical applications in multi-MDP reinforcement learning problems."}}
{"id": "2509.18719", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18719", "abs": "https://arxiv.org/abs/2509.18719", "authors": ["Bo Qu", "Zhurong Wang", "Daisuke Yagi", "Zhen Xu", "Yang Zhao", "Yinan Shan", "Frank Zahradnik"], "title": "LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection", "comment": "12 pages, 12 figures, ACL 2025 industry track", "summary": "This paper presents a novel approach to e-commerce payment fraud detection by\nintegrating reinforcement learning (RL) with Large Language Models (LLMs). By\nframing transaction risk as a multi-step Markov Decision Process (MDP), RL\noptimizes risk detection across multiple payment stages. Crafting effective\nreward functions, essential for RL model success, typically requires\nsignificant human expertise due to the complexity and variability in design.\nLLMs, with their advanced reasoning and coding capabilities, are well-suited to\nrefine these functions, offering improvements over traditional methods. Our\napproach leverages LLMs to iteratively enhance reward functions, achieving\nbetter fraud detection accuracy and demonstrating zero-shot capability.\nExperiments with real-world data confirm the effectiveness, robustness, and\nresilience of our LLM-enhanced RL framework through long-term evaluations,\nunderscoring the potential of LLMs in advancing industrial RL applications.", "AI": {"tldr": "This paper introduces a novel e-commerce fraud detection method that combines reinforcement learning (RL) with Large Language Models (LLMs) to optimize risk assessment across payment stages, using LLMs to iteratively refine reward functions for improved accuracy.", "motivation": "Traditional RL models for fraud detection require significant human expertise to craft effective reward functions due to their complexity and variability. LLMs offer advanced reasoning capabilities that can automate and enhance this process.", "method": "The approach frames transaction risk as a multi-step Markov Decision Process (MDP) and uses LLMs to iteratively refine reward functions for RL optimization, enabling better fraud detection across multiple payment stages.", "result": "Experiments with real-world data show the LLM-enhanced RL framework achieves superior fraud detection accuracy, demonstrates zero-shot capability, and proves robust and resilient in long-term evaluations.", "conclusion": "The integration of LLMs with RL presents significant potential for advancing industrial RL applications, particularly in complex domains like e-commerce fraud detection where automated reward function refinement is crucial."}}
{"id": "2509.18744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18744", "abs": "https://arxiv.org/abs/2509.18744", "authors": ["Yuqing Liu"], "title": "Theory of periodic convolutional neural network", "comment": null, "summary": "We introduce a novel convolutional neural network architecture, termed the\n\\emph{periodic CNN}, which incorporates periodic boundary conditions into the\nconvolutional layers. Our main theoretical contribution is a rigorous\napproximation theorem: periodic CNNs can approximate ridge functions depending\non $d-1$ linear variables in a $d$-dimensional input space, while such\napproximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer\nvariables). This result establishes a sharp characterization of the expressive\npower of periodic CNNs. Beyond the theory, our findings suggest that periodic\nCNNs are particularly well-suited for problems where data naturally admits a\nridge-like structure of high intrinsic dimension, such as image analysis on\nwrapped domains, physics-informed learning, and materials science. The work\nthus both expands the mathematical foundation of CNN approximation theory and\nhighlights a class of architectures with surprising and practically relevant\napproximation capabilities.", "AI": {"tldr": "Periodic CNNs with boundary conditions can approximate ridge functions in d-1 dimensions, which is impossible in lower dimensions, establishing their expressive power for high-dimensional ridge-structured data.", "motivation": "To develop CNN architectures that can handle data with ridge-like structures in high dimensions, particularly for applications like image analysis on wrapped domains and physics-informed learning.", "method": "Introducing periodic boundary conditions into convolutional layers and proving a theoretical approximation theorem for ridge functions.", "result": "Periodic CNNs can approximate ridge functions depending on d-1 linear variables in d-dimensional space, but not in lower-dimensional settings, providing a sharp characterization of their expressive power.", "conclusion": "Periodic CNNs expand CNN approximation theory and offer practical advantages for problems with high-dimensional ridge structures in various scientific domains."}}
{"id": "2509.18751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18751", "abs": "https://arxiv.org/abs/2509.18751", "authors": ["Samuel Yoon", "Jongwon Kim", "Juyoung Ha", "Young Myoung Ko"], "title": "MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model", "comment": null, "summary": "Recently reconstruction-based deep models have been widely used for time\nseries anomaly detection, but as their capacity and representation capability\nincrease, these models tend to over-generalize, often reconstructing unseen\nanomalies accurately. Prior works have attempted to mitigate this by\nincorporating a memory architecture that stores prototypes of normal patterns.\nNevertheless, these approaches suffer from high training costs and have yet to\nbe effectively integrated with time series foundation models (TFMs). To address\nthese challenges, we propose \\textbf{MOMEMTO}, a TFM for anomaly detection,\nenhanced with a patch-based memory module to mitigate over-generalization. The\nmemory module is designed to capture representative normal patterns from\nmultiple domains and enables a single model to be jointly fine-tuned across\nmultiple datasets through a multi-domain training strategy. MOMEMTO initializes\nmemory items with latent representations from a pre-trained encoder, organizes\nthem into patch-level units, and updates them via an attention mechanism. We\nevaluate our method using 23 univariate benchmark datasets. Experimental\nresults demonstrate that MOMEMTO, as a single model, achieves higher scores on\nAUC and VUS metrics compared to baseline methods, and further enhances the\nperformance of its backbone TFM, particularly in few-shot learning scenarios.", "AI": {"tldr": "MOMEMTO is a time series foundation model for anomaly detection that uses a patch-based memory module to prevent over-generalization by storing normal patterns from multiple domains, enabling joint fine-tuning across datasets.", "motivation": "Existing reconstruction-based deep models for time series anomaly detection tend to over-generalize and accurately reconstruct anomalies. Memory-based approaches have high training costs and haven't been effectively integrated with time series foundation models.", "method": "Proposes MOMEMTO with a patch-based memory module that captures normal patterns from multiple domains. Memory items are initialized with pre-trained encoder representations, organized into patch-level units, and updated via attention mechanism. Uses multi-domain training strategy for joint fine-tuning.", "result": "Evaluated on 23 univariate benchmark datasets, MOMEMTO achieves higher AUC and VUS metrics compared to baselines and enhances backbone TFM performance, especially in few-shot learning scenarios.", "conclusion": "MOMEMTO effectively addresses over-generalization in time series anomaly detection through its memory architecture and multi-domain training, demonstrating superior performance as a single model across multiple datasets."}}
{"id": "2509.18766", "categories": ["cs.LG", "math.OC", "stat.ML", "62J07, 68T07", "G.3"], "pdf": "https://arxiv.org/pdf/2509.18766", "abs": "https://arxiv.org/abs/2509.18766", "authors": ["Rapha\u00ebl Berthier"], "title": "Diagonal Linear Networks and the Lasso Regularization Path", "comment": "29 pages, 1 figure", "summary": "Diagonal linear networks are neural networks with linear activation and\ndiagonal weight matrices. Their theoretical interest is that their implicit\nregularization can be rigorously analyzed: from a small initialization, the\ntraining of diagonal linear networks converges to the linear predictor with\nminimal 1-norm among minimizers of the training loss. In this paper, we deepen\nthis analysis showing that the full training trajectory of diagonal linear\nnetworks is closely related to the lasso regularization path. In this\nconnection, the training time plays the role of an inverse regularization\nparameter. Both rigorous results and simulations are provided to illustrate\nthis conclusion. Under a monotonicity assumption on the lasso regularization\npath, the connection is exact while in the general case, we show an approximate\nconnection.", "AI": {"tldr": "Diagonal linear networks' training trajectory closely mirrors the lasso regularization path, with training time acting as an inverse regularization parameter.", "motivation": "To deepen the analysis of implicit regularization in diagonal linear networks by connecting their full training trajectory to the lasso regularization path.", "method": "Analyzed the training dynamics of diagonal linear networks with linear activation and diagonal weight matrices, comparing them to the lasso regularization path through both rigorous theoretical analysis and simulations.", "result": "Found that under monotonicity assumptions, the connection between diagonal linear network training and lasso path is exact; in general cases, an approximate connection exists.", "conclusion": "The training trajectory of diagonal linear networks is fundamentally related to the lasso regularization path, with training time inversely related to the regularization parameter."}}
{"id": "2509.18851", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18851", "abs": "https://arxiv.org/abs/2509.18851", "authors": ["Gongrui Nan", "Siye Chen", "Jing Huang", "Mengyu Lu", "Dexun Wang", "Chunmei Xie", "Weiqi Xiong", "Xianzhou Zeng", "Qixuan Zhou", "Yadong Li", "Xingzhong Xu"], "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization", "comment": null, "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)\nacross various tasks. However, GRPO, a representative RLVR algorithm, suffers\nfrom a critical limitation: when all responses within a group are either\nentirely correct or entirely incorrect, the model fails to learn from these\nhomogeneous responses. This is particularly problematic for homogeneously\nincorrect groups, where GRPO's advantage function yields a value of zero,\nleading to null gradients and the loss of valuable learning signals. To\novercome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy\nOptimization), an algorithm designed to convert homogeneous errors into robust\nlearning signals. First, NGRPO introduces Advantage Calibration. This mechanism\nhypothesizes the existence of a virtual maximum-reward sample during advantage\ncalculation, thereby altering the mean and variance of rewards within a group\nand ensuring that the advantages for homogeneously incorrect samples are no\nlonger zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the\nupdate magnitude for positive samples while imposing stricter constraints on\nthat of negative samples. This serves to stabilize the exploration pressure\nintroduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B\ndemonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,\nDAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and\nAIME2025. These results validate NGRPO's ability to learn from homogeneous\nerrors, leading to stable and substantial improvements in mathematical\nreasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.", "AI": {"tldr": "NGRPO addresses GRPO's limitation in handling homogeneous responses by introducing Advantage Calibration and Asymmetric Clipping to convert homogeneous errors into learning signals, achieving superior performance on mathematical reasoning benchmarks.", "motivation": "GRPO fails to learn from homogeneous responses (all correct or all incorrect), especially problematic for homogeneously incorrect groups where advantage function yields zero, causing null gradients and loss of learning signals.", "method": "NGRPO introduces two key mechanisms: 1) Advantage Calibration - hypothesizes a virtual maximum-reward sample to ensure non-zero advantages for homogeneously incorrect samples; 2) Asymmetric Clipping - relaxes positive sample updates while constraining negative sample updates to stabilize exploration.", "result": "Experiments on Qwen2.5-Math-7B show NGRPO significantly outperforms PPO, GRPO, DAPO, and PSR-NSR on MATH500, AMC23, and AIME2025 benchmarks, demonstrating stable and substantial improvements in mathematical reasoning.", "conclusion": "NGRPO effectively converts homogeneous errors into robust learning signals, enabling LLMs to learn from previously unlearnable scenarios and achieve better mathematical reasoning performance."}}
{"id": "2509.18810", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18810", "abs": "https://arxiv.org/abs/2509.18810", "authors": ["Arman Mohammadi", "Mattias Krysander", "Daniel Jung", "Erik Frisk"], "title": "Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems", "comment": null, "summary": "Deep neural networks has been increasingly applied in fault diagnostics,\nwhere it uses historical data\n  to capture systems behavior, bypassing the need for high-fidelity physical\nmodels.\n  However, despite their competence in prediction tasks, these models often\nstruggle with\n  the evaluation of their confidence. This matter is particularly\n  important in consistency-based diagnosis where decision logic is highly\nsensitive to false alarms.\n  To address this challenge, this work presents a diagnostic framework that\nuses\n  ensemble probabilistic machine learning to\n  improve diagnostic characteristics of data driven consistency based diagnosis\n  by quantifying and automating the prediction uncertainty.\n  The proposed method is evaluated across several case studies using both\nablation\n  and comparative analyses, showing consistent improvements across a range of\ndiagnostic metrics.", "AI": {"tldr": "A diagnostic framework using ensemble probabilistic machine learning to quantify prediction uncertainty in data-driven consistency-based fault diagnosis, improving diagnostic metrics by addressing confidence evaluation challenges in deep neural networks.", "motivation": "Deep neural networks for fault diagnostics struggle with confidence evaluation, which is critical in consistency-based diagnosis where decision logic is highly sensitive to false alarms.", "method": "Uses ensemble probabilistic machine learning to quantify and automate prediction uncertainty in data-driven consistency-based diagnosis.", "result": "Evaluated across several case studies using ablation and comparative analyses, showing consistent improvements across a range of diagnostic metrics.", "conclusion": "The proposed framework effectively addresses confidence evaluation challenges in fault diagnostics by leveraging ensemble probabilistic methods to improve diagnostic characteristics."}}
{"id": "2509.18930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18930", "abs": "https://arxiv.org/abs/2509.18930", "authors": ["Alex Schutz", "Victor-Alexandru Darvariu", "Efimia Panagiotaki", "Bruno Lacerda", "Nick Hawes"], "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning", "comment": null, "summary": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks\nto execute classic algorithms by supervised learning. Despite its successes,\nimportant limitations remain: inability to construct valid solutions without\npost-processing and to reason about multiple correct ones, poor performance on\ncombinatorial NP-hard problems, and inapplicability to problems for which\nstrong algorithms are not yet known. To address these limitations, we reframe\nthe problem of learning algorithm trajectories as a Markov Decision Process,\nwhich imposes structure on the solution construction procedure and unlocks the\npowerful tools of imitation and reinforcement learning (RL). We propose the\nGNARL framework, encompassing the methodology to translate problem formulations\nfrom NAR to RL and a learning architecture suitable for a wide range of\ngraph-based problems. We achieve very high graph accuracy results on several\nCLRS-30 problems, performance matching or exceeding much narrower NAR\napproaches for NP-hard problems and, remarkably, applicability even when\nlacking an expert algorithm.", "AI": {"tldr": "The paper introduces GNARL, a framework that reframes neural algorithmic reasoning as a Markov Decision Process to address limitations of supervised learning approaches, enabling better performance on NP-hard problems and applicability without expert algorithms.", "motivation": "Current Neural Algorithmic Reasoning (NAR) approaches have limitations including inability to construct valid solutions without post-processing, poor performance on combinatorial NP-hard problems, and inapplicability when strong algorithms are unknown.", "method": "Proposes GNARL framework that translates NAR problems to RL by modeling algorithm trajectories as Markov Decision Processes, using imitation and reinforcement learning for graph-based problems.", "result": "Achieves very high graph accuracy on CLRS-30 problems, matches or exceeds narrower NAR approaches for NP-hard problems, and works even without expert algorithms.", "conclusion": "Reframing neural algorithmic reasoning as RL via MDPs successfully addresses key limitations of supervised NAR approaches, enabling broader applicability and better performance on challenging problems."}}
{"id": "2509.18811", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.18811", "abs": "https://arxiv.org/abs/2509.18811", "authors": ["Thomas Savary", "Fran\u00e7ois Rozet", "Gilles Louppe"], "title": "Training-Free Data Assimilation with GenCast", "comment": null, "summary": "Data assimilation is widely used in many disciplines such as meteorology,\noceanography, and robotics to estimate the state of a dynamical system from\nnoisy observations. In this work, we propose a lightweight and general method\nto perform data assimilation using diffusion models pre-trained for emulating\ndynamical systems. Our method builds on particle filters, a class of data\nassimilation algorithms, and does not require any further training. As a\nguiding example throughout this work, we illustrate our methodology on GenCast,\na diffusion-based model that generates global ensemble weather forecasts.", "AI": {"tldr": "A lightweight method for data assimilation using pre-trained diffusion models without additional training, applied to weather forecasting.", "motivation": "Data assimilation is crucial in fields like meteorology and robotics for estimating system states from noisy observations, but existing methods can be complex and require training.", "method": "Builds on particle filters using diffusion models pre-trained for dynamical system emulation, demonstrated with GenCast for weather forecasting.", "result": "Proposes a general framework that leverages existing diffusion models for data assimilation tasks.", "conclusion": "The method provides a lightweight, training-free approach to data assimilation using pre-trained diffusion models, with potential applications across various disciplines."}}
{"id": "2509.18949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18949", "abs": "https://arxiv.org/abs/2509.18949", "authors": ["Niccol\u00f2 Rocchi", "Fabio Stella", "Cassio de Campos"], "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach", "comment": "Accepted at ECAI2025 conference, 20 pages, 1 figure", "summary": "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models.", "AI": {"tldr": "This paper introduces credal networks (CN) as a privacy-preserving alternative to Bayesian networks (BN) to protect against tracing attacks while maintaining model utility.", "motivation": "Privacy concerns in publicly released Bayesian networks where tracing attacks can identify whether specific individuals belong to the training data, and existing noise-based protection methods significantly degrade model utility.", "method": "Proposes using credal networks to mask learned Bayesian networks by adapting tracing attack notions and identifying key learning information to conceal. Conducts numerical experiments to analyze privacy gains through CN hyperparameter tuning.", "result": "CNs enable meaningful inferences while safeguarding privacy, providing a way to modulate privacy gains through hyperparameter tuning without introducing noise that degrades utility.", "conclusion": "Credal networks offer a principled, practical, and effective approach for developing privacy-aware probabilistic graphical models that balance privacy protection with model utility."}}
{"id": "2509.18826", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18826", "abs": "https://arxiv.org/abs/2509.18826", "authors": ["Wenlong Lyu", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective", "comment": "39 pages, 20 figures", "summary": "The well-known graph-based clustering methods, including spectral clustering,\nsymmetric non-negative matrix factorization, and doubly stochastic\nnormalization, can be viewed as relaxations of the kernel $k$-means approach.\nHowever, we posit that these methods excessively relax their inherent low-rank,\nnonnegative, doubly stochastic, and orthonormal constraints to ensure numerical\nfeasibility, potentially limiting their clustering efficacy. In this paper,\nguided by our theoretical analyses, we propose \\textbf{Lo}w-\\textbf{R}ank\n\\textbf{D}oubly stochastic clustering (\\textbf{LoRD}), a model that only\nrelaxes the orthonormal constraint to derive a probabilistic clustering\nresults. Furthermore, we theoretically establish the equivalence between\northogonality and block diagonality under the doubly stochastic constraint. By\nintegrating \\textbf{B}lock diagonal regularization into LoRD, expressed as the\nmaximization of the Frobenius norm, we propose \\textbf{B-LoRD}, which further\nenhances the clustering performance. To ensure numerical solvability, we\ntransform the non-convex doubly stochastic constraint into a linear convex\nconstraint through the introduction of a class probability parameter. We\nfurther theoretically demonstrate the gradient Lipschitz continuity of our LoRD\nand B-LoRD enables the proposal of a globally convergent projected gradient\ndescent algorithm for their optimization. Extensive experiments validate the\neffectiveness of our approaches. The code is publicly available at\nhttps://github.com/lwl-learning/LoRD.", "AI": {"tldr": "LoRD is a low-rank doubly stochastic clustering method that relaxes only the orthonormal constraint to produce probabilistic clustering results, with B-LoRD adding block diagonal regularization to enhance performance.", "motivation": "Existing graph-based clustering methods excessively relax constraints (low-rank, nonnegative, doubly stochastic, orthonormal) for numerical feasibility, potentially limiting clustering effectiveness.", "method": "Proposes LoRD (only relaxes orthonormal constraint) and B-LoRD (adds block diagonal regularization via Frobenius norm maximization). Uses projected gradient descent with linear convex constraint transformation for optimization.", "result": "Extensive experiments validate the effectiveness of the approaches. Theoretically establishes equivalence between orthogonality and block diagonality under doubly stochastic constraint.", "conclusion": "The proposed methods provide improved clustering performance by maintaining more constraints than existing approaches while ensuring numerical solvability through proper optimization techniques."}}
{"id": "2509.19017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19017", "abs": "https://arxiv.org/abs/2509.19017", "authors": ["Hazem Dewidar", "Elena Umili"], "title": "Fully Learnable Neural Reward Machines", "comment": null, "summary": "Non-Markovian Reinforcement Learning (RL) tasks present significant\nchallenges, as agents must reason over entire trajectories of state-action\npairs to make optimal decisions. A common strategy to address this is through\nsymbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which\nprovide a structured way to express temporally extended objectives. However,\nthese approaches often rely on restrictive assumptions -- such as the\navailability of a predefined Symbol Grounding (SG) function mapping raw\nobservations to high-level symbolic representations, or prior knowledge of the\ntemporal task. In this work, we propose a fully learnable version of Neural\nReward Machines (NRM), which can learn both the SG function and the automaton\nend-to-end, removing any reliance on prior knowledge. Our approach is therefore\nas easily applicable as classic deep RL (DRL) approaches, while being far more\nexplainable, because of the finite and compact nature of automata. Furthermore,\nwe show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,\nour method outperforms previous approaches based on Recurrent Neural Networks\n(RNNs).", "AI": {"tldr": "Proposes Fully Learnable Neural Reward Machines (FLNRM) that can learn both Symbol Grounding functions and automata end-to-end for non-Markovian RL tasks, outperforming RNN-based approaches while maintaining explainability.", "motivation": "Current approaches for non-Markovian RL rely on restrictive assumptions like predefined Symbol Grounding functions or prior knowledge of temporal tasks, limiting their applicability.", "method": "Develops a fully learnable version of Neural Reward Machines that learns both the Symbol Grounding function mapping raw observations to symbols and the automaton structure end-to-end, integrated with deep reinforcement learning.", "result": "FLNRM outperforms previous approaches based on Recurrent Neural Networks and is as easily applicable as classic deep RL methods while being more explainable due to the finite and compact nature of automata.", "conclusion": "The proposed FLNRM approach successfully removes reliance on prior knowledge while maintaining explainability and achieving superior performance compared to RNN-based methods for non-Markovian reinforcement learning tasks."}}
{"id": "2509.18842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18842", "abs": "https://arxiv.org/abs/2509.18842", "authors": ["Nikolas Chatzis", "Ioannis Kordonis", "Manos Theodosis", "Petros Maragos"], "title": "Shared-Weights Extender and Gradient Voting for Neural Network Expansion", "comment": "5 pages, 3 figures", "summary": "Expanding neural networks during training is a promising way to augment\ncapacity without retraining larger models from scratch. However, newly added\nneurons often fail to adjust to a trained network and become inactive,\nproviding no contribution to capacity growth. We propose the Shared-Weights\nExtender (SWE), a novel method explicitly designed to prevent inactivity of new\nneurons by coupling them with existing ones for smooth integration. In\nparallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based\nmethod for allocating neurons across layers during deep network expansion. Our\nextensive benchmarking on four datasets shows that our method can effectively\nsuppress neuron inactivity and achieve better performance compared to other\nexpanding methods and baselines.", "AI": {"tldr": "SWE and SVoD methods prevent neuron inactivity during neural network expansion by coupling new neurons with existing ones and using gradient-based allocation.", "motivation": "Newly added neurons during neural network expansion often become inactive and fail to contribute to capacity growth, limiting the effectiveness of network expansion methods.", "method": "Shared-Weights Extender (SWE) couples new neurons with existing ones for smooth integration, and Steepest Voting Distributor (SVoD) uses gradient-based allocation to distribute neurons across layers during expansion.", "result": "Extensive benchmarking on four datasets shows the method effectively suppresses neuron inactivity and achieves better performance compared to other expanding methods and baselines.", "conclusion": "The proposed SWE and SVoD methods successfully address the neuron inactivity problem in neural network expansion, enabling effective capacity augmentation without retraining from scratch."}}
{"id": "2509.19063", "categories": ["cs.LG", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2509.19063", "abs": "https://arxiv.org/abs/2509.19063", "authors": ["Przemys\u0142aw Spyra"], "title": "Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training", "comment": null, "summary": "The rising computational and energy demands of deep neural networks (DNNs),\ndriven largely by backpropagation (BP), challenge sustainable AI development.\nThis paper rigorously investigates three BP-free training methods: the\nForward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)\nalgorithms, tracing their progression from foundational concepts to a\ndemonstrably superior solution.\n  A robust comparative framework was established: each algorithm was\nimplemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and\nbenchmarked against an equivalent BP-trained model. Hyperparameters were\noptimized with Optuna, and consistent early stopping criteria were applied\nbased on validation performance, ensuring all models were optimally tuned\nbefore comparison.\n  Results show that MF not only competes with but consistently surpasses BP in\nclassification accuracy on its native MLPs. Its superior generalization stems\nfrom converging to a more favorable minimum in the validation loss landscape,\nchallenging the assumption that global optimization is required for\nstate-of-the-art results. Measured at the hardware level using the NVIDIA\nManagement Library (NVML) API, MF reduces energy consumption by up to 41% and\nshortens training time by up to 34%, translating to a measurably smaller carbon\nfootprint as estimated by CodeCarbon.\n  Beyond this primary result, we present a hardware-level analysis that\nexplains the efficiency gains: exposing FF's architectural inefficiencies,\nvalidating MF's computationally lean design, and challenging the assumption\nthat all BP-free methods are inherently more memory-efficient. By documenting\nthe evolution from FF's conceptual groundwork to MF's synthesis of accuracy and\nsustainability, this work offers a clear, data-driven roadmap for future\nenergy-efficient deep learning.", "AI": {"tldr": "This paper demonstrates that Mono-Forward (MF) algorithm outperforms backpropagation in both accuracy and efficiency, achieving up to 41% energy reduction and 34% faster training while maintaining competitive classification performance.", "motivation": "The rising computational and energy demands of deep neural networks driven by backpropagation challenge sustainable AI development, necessitating more efficient training methods.", "method": "Rigorous comparative analysis of three BP-free methods (Forward-Forward, Cascaded-Forward, and Mono-Forward) using optimized hyperparameters, consistent early stopping, and hardware-level energy measurements via NVML API and CodeCarbon.", "result": "MF consistently surpasses BP in classification accuracy on MLPs, reduces energy consumption by up to 41%, shortens training time by up to 34%, and converges to more favorable validation loss minima.", "conclusion": "MF offers a superior balance of accuracy and sustainability, challenging assumptions about global optimization requirements and providing a roadmap for energy-efficient deep learning."}}
{"id": "2509.19084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19084", "abs": "https://arxiv.org/abs/2509.19084", "authors": ["Asela Hevapathige"], "title": "Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success across\nvarious graph-based tasks. However, they face some fundamental limitations:\nfeature oversmoothing can cause node representations to become\nindistinguishable in deeper networks, they struggle to effectively manage\nheterogeneous relationships where connected nodes differ significantly, and\nthey process entire feature vectors as indivisible units, which limits\nflexibility. We seek to address these limitations. We propose AxelGNN, a novel\nGNN architecture inspired by Axelrod's cultural dissemination model that\naddresses these limitations through a unified framework. AxelGNN incorporates\nsimilarity-gated probabilistic interactions that adaptively promote convergence\nor divergence based on node similarity, implements trait-level copying\nmechanisms for fine-grained feature aggregation at the segment level, and\nmaintains global polarization to preserve node distinctiveness across multiple\nrepresentation clusters. The model's bistable convergence dynamics naturally\nhandle both homophilic and heterophilic graphs within a single architecture.\nExtensive experiments on node classification and influence estimation\nbenchmarks demonstrate that AxelGNN consistently outperforms or matches\nstate-of-the-art GNN methods across diverse graph structures with varying\nhomophily-heterophily characteristics.", "AI": {"tldr": "AxelGNN is a novel Graph Neural Network architecture inspired by Axelrod's cultural dissemination model that addresses fundamental GNN limitations like feature oversmoothing, handling heterogeneous relationships, and fine-grained feature processing through similarity-gated probabilistic interactions and trait-level copying mechanisms.", "motivation": "To overcome fundamental limitations of traditional GNNs including feature oversmoothing in deep networks, ineffective handling of heterogeneous relationships where connected nodes differ significantly, and processing entire feature vectors as indivisible units which limits flexibility.", "method": "AxelGNN incorporates similarity-gated probabilistic interactions that adaptively promote convergence or divergence based on node similarity, implements trait-level copying mechanisms for fine-grained feature aggregation at the segment level, and maintains global polarization to preserve node distinctiveness across multiple representation clusters.", "result": "Extensive experiments on node classification and influence estimation benchmarks demonstrate that AxelGNN consistently outperforms or matches state-of-the-art GNN methods across diverse graph structures with varying homophily-heterophily characteristics.", "conclusion": "AxelGNN provides a unified framework that naturally handles both homophilic and heterophilic graphs within a single architecture, effectively addressing key limitations of traditional GNNs through its bistable convergence dynamics inspired by cultural dissemination models."}}
{"id": "2509.18893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18893", "abs": "https://arxiv.org/abs/2509.18893", "authors": ["Qinhan Hou", "Yilun Zheng", "Xichun Zhang", "Sitao Luan", "Jing Tang"], "title": "Exploring Heterophily in Graph-level Tasks", "comment": "Accectped by NeurIPS 2025 Workshop, New Perspectives in Advancing\n  Graph Machine Learning (NPGML)", "summary": "While heterophily has been widely studied in node-level tasks, its impact on\ngraph-level tasks remains unclear. We present the first analysis of heterophily\nin graph-level learning, combining theoretical insights with empirical\nvalidation. We first introduce a taxonomy of graph-level labeling schemes, and\nfocus on motif-based tasks within local structure labeling, which is a popular\nlabeling scheme. Using energy-based gradient flow analysis, we reveal a key\ninsight: unlike frequency-dominated regimes in node-level tasks, motif\ndetection requires mixed-frequency dynamics to remain flexible across multiple\nspectral components. Our theory shows that motif objectives are inherently\nmisaligned with global frequency dominance, demanding distinct architectural\nconsiderations. Experiments on synthetic datasets with controlled heterophily\nand real-world molecular property prediction support our findings, showing that\nfrequency-adaptive model outperform frequency-dominated models. This work\nestablishes a new theoretical understanding of heterophily in graph-level\nlearning and offers guidance for designing effective GNN architectures.", "AI": {"tldr": "First analysis of heterophily in graph-level learning, revealing that motif-based tasks require mixed-frequency dynamics rather than frequency-dominated approaches used in node-level tasks.", "motivation": "While heterophily has been extensively studied in node-level tasks, its impact on graph-level tasks remains unclear and requires systematic investigation.", "method": "Combined theoretical energy-based gradient flow analysis with empirical validation on synthetic datasets with controlled heterophily and real-world molecular property prediction tasks.", "result": "Motif detection requires mixed-frequency dynamics to remain flexible across multiple spectral components, and frequency-adaptive models outperform frequency-dominated models in graph-level tasks.", "conclusion": "Establishes new theoretical understanding of heterophily in graph-level learning and provides guidance for designing effective GNN architectures that account for mixed-frequency requirements."}}
{"id": "2509.19100", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19100", "abs": "https://arxiv.org/abs/2509.19100", "authors": ["Alexander Robey"], "title": "Algorithms for Adversarially Robust Deep Learning", "comment": "PhD thesis", "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents.", "AI": {"tldr": "This thesis focuses on developing robust deep learning algorithms across three areas: adversarial examples in computer vision, domain generalization, and jailbreaking large language models.", "motivation": "Deep learning models are increasingly used in safety-critical applications, making robustness against adversarial exploitation fundamentally important.", "method": "The research introduces new technical results, training paradigms, and certification algorithms for adversarial examples; develops state-of-the-art domain generalization algorithms for medical imaging, molecular identification, and image classification; and proposes new attacks and defenses for jailbreaking LLMs.", "result": "The work achieves state-of-the-art generalization performance across multiple domains and represents frontier progress in designing robust language-based agents.", "conclusion": "The thesis makes significant contributions to improving the robustness of deep learning models across computer vision and natural language processing applications, addressing critical security challenges in safety-critical systems."}}
{"id": "2509.18904", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18904", "abs": "https://arxiv.org/abs/2509.18904", "authors": ["Zhaoxin Wang", "Handing Wang", "Cong Tian", "Yaochu Jin"], "title": "Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction", "comment": null, "summary": "Federated learning allows multiple participants to collaboratively train a\ncentral model without sharing their private data. However, this distributed\nnature also exposes new attack surfaces. In particular, backdoor attacks allow\nattackers to implant malicious behaviors into the global model while\nmaintaining high accuracy on benign inputs. Existing attacks usually rely on\nfixed patterns or adversarial perturbations as triggers, which tightly couple\nthe main and backdoor tasks. This coupling makes them vulnerable to dilution by\nhonest updates and limits their persistence under federated defenses. In this\nwork, we propose an approach to decouple the backdoor task from the main task\nby dynamically optimizing the backdoor trigger within a min-max framework. The\ninner layer maximizes the performance gap between poisoned and benign samples,\nensuring that the contributions of benign users have minimal impact on the\nbackdoor. The outer process injects the adaptive triggers into the local model.\nWe evaluate our method on both computer vision and natural language tasks, and\ncompare it with six backdoor attack methods under six defense algorithms.\nExperimental results show that our method achieves good attack performance and\ncan be easily integrated into existing backdoor attack techniques.", "AI": {"tldr": "This paper proposes a dynamic trigger optimization approach for backdoor attacks in federated learning that decouples backdoor tasks from main tasks to improve persistence against defenses.", "motivation": "Existing backdoor attacks in federated learning use fixed triggers that tightly couple main and backdoor tasks, making them vulnerable to dilution by honest updates and limited persistence under federated defenses.", "method": "A min-max framework where the inner layer maximizes performance gap between poisoned and benign samples to minimize impact of benign users, and the outer process injects adaptive triggers into the local model.", "result": "Evaluation on computer vision and natural language tasks shows the method achieves good attack performance and outperforms six existing backdoor attack methods under six defense algorithms.", "conclusion": "The proposed dynamic trigger optimization method effectively improves backdoor attack persistence in federated learning and can be easily integrated into existing attack techniques."}}
{"id": "2509.19112", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19112", "abs": "https://arxiv.org/abs/2509.19112", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation", "comment": "Accepted at NeuRIPS2025 Workshop on Structured Probabilistic\n  Inference and Generative Modeling", "summary": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning.", "AI": {"tldr": "CARGO is a scalable multi-label causal discovery method for high-dimensional event sequences that uses pretrained causal Transformers to infer causal graphs and reconstruct global Markov boundaries through adaptive frequency fusion.", "motivation": "Understanding causality in event sequences (like symptoms leading to diseases or error codes causing system failures) is critical but remains challenging in domains like healthcare and vehicle diagnostics, especially with sparse, high-dimensional data.", "method": "CARGO uses two pretrained causal Transformers as foundation models to infer causal graphs per sequence in parallel, then aggregates them using adaptive frequency fusion to reconstruct global Markov boundaries of labels, bypassing expensive full-dataset conditional independence testing.", "result": "On a real-world automotive fault prediction dataset with 29,100 unique event types and 474 imbalanced labels, CARGO demonstrated effective structured reasoning capabilities.", "conclusion": "CARGO provides an efficient probabilistic reasoning approach for causal discovery in high-dimensional event sequences, enabling scalable analysis while avoiding the computational intractability of traditional methods."}}
{"id": "2509.19120", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19120", "abs": "https://arxiv.org/abs/2509.19120", "authors": ["Ferdinand Kahenga", "Antoine Bagula", "Sajal K. Das", "Patrick Sello"], "title": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI", "comment": null, "summary": "Federated Learning (FL) has emerged as a powerful paradigm for\nprivacy-preserving model training, yet deployments in sensitive domains such as\nhealthcare face persistent challenges from non-IID data, client unreliability,\nand adversarial manipulation. This paper introduces FedFiTS, a trust and\nfairness-aware selective FL framework that advances the FedFaSt line by\ncombining fitness-based client election with slotted aggregation. FedFiTS\nimplements a three-phase participation strategy-free-for-all training, natural\nselection, and slotted team participation-augmented with dynamic client\nscoring, adaptive thresholding, and cohort-based scheduling to balance\nconvergence efficiency with robustness. A theoretical convergence analysis\nestablishes bounds for both convex and non-convex objectives under standard\nassumptions, while a communication-complexity analysis shows reductions\nrelative to FedAvg and other baselines. Experiments on diverse datasets-medical\nimaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular\nagricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently\noutperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and\nresilience to poisoning attacks. By integrating trust-aware aggregation with\nfairness-oriented client selection, FedFiTS advances scalable and secure FL,\nmaking it well suited for real-world healthcare and cross-domain deployments.", "AI": {"tldr": "FedFiTS is a trust and fairness-aware selective federated learning framework that improves upon FedFaSt by combining fitness-based client election with slotted aggregation to address challenges in sensitive domains like healthcare.", "motivation": "Federated Learning deployments in sensitive domains face persistent challenges from non-IID data, client unreliability, and adversarial manipulation, requiring more robust and fair frameworks.", "method": "FedFiTS implements a three-phase participation strategy (free-for-all training, natural selection, slotted team participation) with dynamic client scoring, adaptive thresholding, and cohort-based scheduling. It includes theoretical convergence analysis and communication-complexity analysis.", "result": "Experiments on medical imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and agricultural data show FedFiTS consistently outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and resilience to poisoning attacks.", "conclusion": "By integrating trust-aware aggregation with fairness-oriented client selection, FedFiTS advances scalable and secure FL, making it well suited for real-world healthcare and cross-domain deployments."}}
{"id": "2509.19122", "categories": ["cs.LG", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.19122", "abs": "https://arxiv.org/abs/2509.19122", "authors": ["Chunming Ye", "Wenquan Tian", "Yalan Gao", "Songzhou Li"], "title": "Analysis on distribution and clustering of weight", "comment": "14page,16 figures", "summary": "The study on architecture and parameter characteristics remains the hot topic\nin the research of large language models. In this paper we concern with the\ncharacteristics of weight which are used to analyze the correlations and\ndifferences between models. Two kinds of vectors-standard deviation vector and\nclustering vector-are proposed to describe features of models. In the first\ncase, the weights are assumed to follow normal distribution. The standard\ndeviation values of projection matrices are normalized to form\nStandard-Deviation Vector, representing the distribution characteristics of\nmodels. In the second case, the singular values from each weight projection\nmatrix are extracted and grouped by K-Means algorithm. The grouped data with\nthe same type matrix are combined as Clustering Vector to represent the\ncorrelation characteristics of models' weights. The study reveals that these\ntwo vectors can effectively distinguish between different models and clearly\nshow the similarities among models of the same family. Moreover, after\nconducting LoRA fine-tuning with different datasets and models, it is found\nthat the distribution of weights represented by standard deviation vector is\ndirectly influenced by the dataset, but the correlations between different\nweights represented by clustering vector remain unaffected and maintain a high\nconsistency with the pre-trained model.", "AI": {"tldr": "This paper proposes two vector representations (Standard-Deviation Vector and Clustering Vector) to analyze weight characteristics in large language models, showing they can distinguish between different models and reveal dataset influences on weight distribution while maintaining correlation consistency.", "motivation": "To analyze the correlations and differences between large language models by studying their weight characteristics, particularly focusing on how to effectively distinguish different models and understand the similarities among models of the same family.", "method": "Two vector representations are proposed: 1) Standard-Deviation Vector - normalizing standard deviation values of projection matrices assuming normal distribution; 2) Clustering Vector - extracting singular values from weight projection matrices and grouping them using K-Means algorithm. The study also conducts LoRA fine-tuning experiments with different datasets and models.", "result": "The two vectors effectively distinguish between different models and clearly show similarities among models of the same family. Standard-deviation vector distribution is directly influenced by the dataset used in fine-tuning, while clustering vector correlations remain unaffected and maintain high consistency with the pre-trained model.", "conclusion": "The proposed vector representations provide effective tools for analyzing model characteristics, with standard-deviation vectors capturing dataset-influenced distribution patterns and clustering vectors preserving inherent correlation characteristics that remain stable across fine-tuning."}}
{"id": "2509.18962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18962", "abs": "https://arxiv.org/abs/2509.18962", "authors": ["Kirsten K\u00f6bschall", "Sebastian Buschj\u00e4ger", "Raphael Fischer", "Lisa Hartung", "Stefan Kramer"], "title": "Lift What You Can: Green Online Learning with Heterogeneous Ensembles", "comment": null, "summary": "Ensemble methods for stream mining necessitate managing multiple models and\nupdating them as data distributions evolve. Considering the calls for more\nsustainability, established methods are however not sufficiently considerate of\nensemble members' computational expenses and instead overly focus on predictive\ncapabilities. To address these challenges and enable green online learning, we\npropose heterogeneous online ensembles (HEROS). For every training step, HEROS\nchooses a subset of models from a pool of models initialized with diverse\nhyperparameter choices under resource constraints to train. We introduce a\nMarkov decision process to theoretically capture the trade-offs between\npredictive performance and sustainability constraints. Based on this framework,\nwe present different policies for choosing which models to train on incoming\ndata. Most notably, we propose the novel $\\zeta$-policy, which focuses on\ntraining near-optimal models at reduced costs. Using a stochastic model, we\ntheoretically prove that our $\\zeta$-policy achieves near optimal performance\nwhile using fewer resources compared to the best performing policy. In our\nexperiments across 11 benchmark datasets, we find empiric evidence that our\n$\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating\nhighly accurate performance, in some cases even outperforming competitors, and\nsimultaneously being much more resource-friendly.", "AI": {"tldr": "HEROS proposes a heterogeneous online ensemble method that balances predictive performance with computational sustainability by selectively training subsets of models under resource constraints.", "motivation": "Current ensemble methods for stream mining focus too much on predictive capabilities without sufficiently considering computational expenses, creating a need for more sustainable approaches to online learning.", "method": "HEROS uses a Markov decision process framework to model trade-offs between performance and sustainability, with a novel \u03b6-policy that selectively trains near-optimal models from a diverse pool under resource constraints.", "result": "Theoretical analysis proves \u03b6-policy achieves near-optimal performance with fewer resources. Experiments on 11 benchmark datasets show it outperforms competitors in accuracy while being much more resource-friendly.", "conclusion": "HEROS provides a sustainable ensemble method that maintains high predictive performance while significantly reducing computational costs, representing a strong contribution to green online learning."}}
{"id": "2509.19135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19135", "abs": "https://arxiv.org/abs/2509.19135", "authors": ["Wenying Luo", "Zhiyuan Lin", "Wenhao Xu", "Minghao Liu", "Zhi Li"], "title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding", "comment": null, "summary": "Human mobility traces, often recorded as sequences of check-ins, provide a\nunique window into both short-term visiting patterns and persistent lifestyle\nregularities. In this work we introduce GSTM-HMU, a generative spatio-temporal\nframework designed to advance mobility analysis by explicitly modeling the\nsemantic and temporal complexity of human movement. The framework consists of\nfour key innovations. First, a Spatio-Temporal Concept Encoder (STCE)\nintegrates geographic location, POI category semantics, and periodic temporal\nrhythms into unified vector representations. Second, a Cognitive Trajectory\nMemory (CTM) adaptively filters historical visits, emphasizing recent and\nbehaviorally salient events in order to capture user intent more effectively.\nThird, a Lifestyle Concept Bank (LCB) contributes structured human preference\ncues, such as activity types and lifestyle patterns, to enhance\ninterpretability and personalization. Finally, task-oriented generative heads\ntransform the learned representations into predictions for multiple downstream\ntasks. We conduct extensive experiments on four widely used real-world\ndatasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate\nperformance on three benchmark tasks: next-location prediction, trajectory-user\nidentification, and time estimation. The results demonstrate consistent and\nsubstantial improvements over strong baselines, confirming the effectiveness of\nGSTM-HMU in extracting semantic regularities from complex mobility data. Beyond\nraw performance gains, our findings also suggest that generative modeling\nprovides a promising foundation for building more robust, interpretable, and\ngeneralizable systems for human mobility intelligence.", "AI": {"tldr": "GSTM-HMU is a generative spatio-temporal framework that models semantic and temporal complexity of human mobility traces, achieving superior performance on next-location prediction, trajectory-user identification, and time estimation tasks.", "motivation": "To advance mobility analysis by explicitly modeling the semantic and temporal complexity of human movement, capturing both short-term visiting patterns and persistent lifestyle regularities from mobility check-in sequences.", "method": "The framework includes four innovations: Spatio-Temporal Concept Encoder (integrates location, POI semantics, and temporal rhythms), Cognitive Trajectory Memory (filters historical visits for user intent), Lifestyle Concept Bank (adds human preference cues), and task-oriented generative heads for multiple downstream tasks.", "result": "Extensive experiments on four real-world datasets (Gowalla, WeePlace, Brightkite, FourSquare) show consistent and substantial improvements over strong baselines on three benchmark tasks.", "conclusion": "Generative modeling provides a promising foundation for building more robust, interpretable, and generalizable systems for human mobility intelligence, effectively extracting semantic regularities from complex mobility data."}}
{"id": "2509.18964", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18964", "abs": "https://arxiv.org/abs/2509.18964", "authors": ["Xingtu Liu"], "title": "Central Limit Theorems for Asynchronous Averaged Q-Learning", "comment": null, "summary": "This paper establishes central limit theorems for Polyak-Ruppert averaged\nQ-learning under asynchronous updates. We present a non-asymptotic central\nlimit theorem, where the convergence rate in Wasserstein distance explicitly\nreflects the dependence on the number of iterations, state-action space size,\nthe discount factor, and the quality of exploration. In addition, we derive a\nfunctional central limit theorem, showing that the partial-sum process\nconverges weakly to a Brownian motion.", "AI": {"tldr": "Central limit theorems for Polyak-Ruppert averaged Q-learning under asynchronous updates, including non-asymptotic and functional CLTs", "motivation": "To establish rigorous statistical guarantees for Q-learning algorithms, particularly understanding the convergence behavior and distributional properties of Polyak-Ruppert averaging in asynchronous settings", "method": "Develops mathematical proofs for central limit theorems, analyzing the convergence rate in Wasserstein distance and deriving functional CLT showing weak convergence to Brownian motion", "result": "Established non-asymptotic CLT with explicit dependence on iteration count, state-action space size, discount factor, and exploration quality; proved functional CLT showing partial-sum process converges to Brownian motion", "conclusion": "Provides fundamental statistical foundations for Q-learning with Polyak-Ruppert averaging, enabling better understanding of convergence properties and distributional behavior in reinforcement learning algorithms"}}
{"id": "2509.19220", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19220", "abs": "https://arxiv.org/abs/2509.19220", "authors": ["Ferdinand Kahenga", "Antoine Bagula", "Patrick Sello", "Sajal K. Das"], "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity", "comment": null, "summary": "Federated learning in practice must contend with heterogeneous feature\nspaces, severe non-IID data, and scarce labels across clients. We present\nFedFusion, a federated transfer-learning framework that unifies domain\nadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,\nDivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via\nconfidence-filtered pseudo-labels and domain-adaptive transfer, while clients\nmaintain personalised encoders tailored to local data. To preserve global\ncoherence under heterogeneity, FedFusion employs similarity-weighted classifier\ncoupling (with optional cluster-wise averaging), mitigating dominance by\ndata-rich sites and improving minority-client performance. The frugal-labelling\npipeline combines self-/semi-supervised pretext training with selective\nfine-tuning, reducing annotation demands without sharing raw data. Across\ntabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,\nFedFusion consistently outperforms state-of-the-art baselines in accuracy,\nrobustness, and fairness while maintaining comparable communication and\ncomputation budgets. These results show that harmonising personalisation,\ndomain adaptation, and label efficiency is an effective recipe for robust\nfederated learning under real-world constraints.", "AI": {"tldr": "FedFusion is a federated transfer-learning framework that addresses heterogeneous feature spaces, non-IID data, and label scarcity through domain adaptation, frugal labeling, and diversity-aware encoders.", "motivation": "Federated learning faces challenges with heterogeneous feature spaces, severe non-IID data distribution, and scarce labels across clients in real-world scenarios.", "method": "Combines domain adaptation and frugal labeling with diversity-aware encoders (DivEn, DivEn-mix, DivEn-c), uses confidence-filtered pseudo-labels, similarity-weighted classifier coupling, and self-/semi-supervised pretext training with selective fine-tuning.", "result": "Outperforms state-of-the-art baselines in accuracy, robustness, and fairness across tabular and imaging benchmarks under various data regimes while maintaining comparable communication and computation budgets.", "conclusion": "Harmonizing personalization, domain adaptation, and label efficiency provides an effective solution for robust federated learning under real-world constraints."}}
{"id": "2509.18968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18968", "abs": "https://arxiv.org/abs/2509.18968", "authors": ["Zhanglu Yan", "Jiayi Mao", "Qianhui Liu", "Fanfan Li", "Gang Pan", "Tao Luo", "Bowen Zhu", "Weng-Fai Wong"], "title": "Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding", "comment": null, "summary": "Spiking neural networks (SNNs) promise high energy efficiency, particularly\nwith time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting\nat most one spike per neuron. However, such energy advantage is often\nunrealized because inference requires evaluating a temporal decay function and\nsubsequent multiplication with the synaptic weights. This paper challenges this\ncostly approach by repurposing a physical hardware `bug', namely, the natural\nsignal decay in optoelectronic devices, as the core computation of TTFS. We\nfabricated a custom indium oxide optoelectronic synapse, showing how its\nnatural physical decay directly implements the required temporal function. By\ntreating the device's analog output as the fused product of the synaptic weight\nand temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates\nthese expensive digital operations. To use the Otters paradigm in complex\narchitectures like the transformer, which are challenging to train directly due\nto the sparsity issue, we introduce a novel quantized neural network-to-SNN\nconversion algorithm. This complete hardware-software co-design enables our\nmodel to achieve state-of-the-art accuracy across seven GLUE benchmark datasets\nand demonstrates a 1.77$\\times$ improvement in energy efficiency over previous\nleading SNNs, based on a comprehensive analysis of compute, data movement, and\nmemory access costs using energy measurements from a commercial 22nm process.\nOur work thus establishes a new paradigm for energy-efficient SNNs, translating\nfundamental device physics directly into powerful computational primitives. All\ncodes and data are open source.", "AI": {"tldr": "This paper introduces Otters, a hardware-software co-design approach that repurposes natural signal decay in optoelectronic devices as the core computation for time-to-first-spike (TTFS) encoding in spiking neural networks, eliminating expensive digital operations and achieving state-of-the-art energy efficiency.", "motivation": "Current SNNs with TTFS encoding fail to realize their full energy efficiency potential because inference requires costly evaluation of temporal decay functions and multiplication with synaptic weights. The authors aim to eliminate these expensive digital operations by leveraging natural physical phenomena.", "method": "The authors fabricated a custom indium oxide optoelectronic synapse that uses natural physical decay to implement the required temporal function. They treat the device's analog output as the fused product of synaptic weight and temporal decay. For complex architectures like transformers, they developed a novel quantized neural network-to-SNN conversion algorithm to overcome training challenges due to sparsity.", "result": "The Otters paradigm achieved state-of-the-art accuracy across seven GLUE benchmark datasets and demonstrated a 1.77\u00d7 improvement in energy efficiency over previous leading SNNs, based on comprehensive analysis of compute, data movement, and memory access costs using energy measurements from a commercial 22nm process.", "conclusion": "This work establishes a new paradigm for energy-efficient SNNs by translating fundamental device physics directly into computational primitives, showing how physical hardware 'bugs' can be repurposed as core computational elements to achieve significant energy savings."}}
{"id": "2509.18990", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.18990", "abs": "https://arxiv.org/abs/2509.18990", "authors": ["Carson Dudley", "Marisa Eisenberg"], "title": "Learning From Simulators: A Theory of Simulation-Grounded Learning", "comment": null, "summary": "Simulation-Grounded Neural Networks (SGNNs) are predictive models trained\nentirely on synthetic data from mechanistic simulations. They have achieved\nstate-of-the-art performance in domains where real-world labels are limited or\nunobserved, but lack a formal underpinning.\n  We present the foundational theory of simulation-grounded learning. We show\nthat SGNNs implement amortized Bayesian inference under a simulation prior and\nconverge to the Bayes-optimal predictor. We derive generalization bounds under\nmodel misspecification and prove that SGNNs can learn unobservable scientific\nquantities that empirical methods provably cannot. We also formalize a novel\nform of mechanistic interpretability uniquely enabled by SGNNs: by attributing\npredictions to the simulated mechanisms that generated them, SGNNs yield\nposterior-consistent, scientifically grounded explanations.\n  We provide numerical experiments to validate all theoretical predictions.\nSGNNs recover latent parameters, remain robust under mismatch, and outperform\nclassical tools: in a model selection task, SGNNs achieve half the error of AIC\nin distinguishing mechanistic dynamics. These results establish SGNNs as a\nprincipled and practical framework for scientific prediction in data-limited\nregimes.", "AI": {"tldr": "SGNNs implement amortized Bayesian inference under simulation priors, converge to Bayes-optimal predictors, and provide mechanistic interpretability through posterior-consistent explanations.", "motivation": "To establish formal theoretical foundations for Simulation-Grounded Neural Networks (SGNNs) which currently lack formal underpinning despite achieving state-of-the-art performance in data-limited domains.", "method": "Develop theoretical framework showing SGNNs perform amortized Bayesian inference, derive generalization bounds under model misspecification, and formalize mechanistic interpretability through attribution to simulated mechanisms.", "result": "SGNNs recover latent parameters, remain robust under mismatch, and outperform classical tools - achieving half the error of AIC in model selection tasks for distinguishing mechanistic dynamics.", "conclusion": "SGNNs are established as a principled and practical framework for scientific prediction in data-limited regimes, providing both theoretical guarantees and practical advantages over traditional methods."}}
{"id": "2509.18993", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18993", "abs": "https://arxiv.org/abs/2509.18993", "authors": ["Boao Kong", "Junzhu Liang", "Yuxi Liu", "Renjia Deng", "Kun Yuan"], "title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure", "comment": "32 pages", "summary": "Low-rank architectures have become increasingly important for efficient large\nlanguage model (LLM) pre-training, providing substantial reductions in both\nparameter complexity and memory/computational demands. Despite these\nadvantages, current low-rank methods face three critical shortcomings: (1)\ncompromised model performance, (2) considerable computational overhead, and (3)\nlimited activation memory savings. To address these limitations, we propose\nCross-layer Low-Rank residual Network (CR-Net), an innovative\nparameter-efficient framework inspired by our discovery that inter-layer\nactivation residuals possess low-rank properties. CR-Net implements this\ninsight through a dual-path architecture that efficiently reconstructs layer\nactivations by combining previous-layer outputs with their low-rank\ndifferences, thereby maintaining high-rank information with minimal parameters.\nWe further develop a specialized activation recomputation strategy tailored for\nCR-Net that dramatically reduces memory requirements. Extensive pre-training\nexperiments across model scales from 60M to 7B parameters demonstrate that\nCR-Net consistently outperforms state-of-the-art low-rank frameworks while\nrequiring fewer computational resources and less memory.", "AI": {"tldr": "CR-Net is a parameter-efficient framework that uses low-rank residual networks to improve LLM pre-training by maintaining high-rank information with minimal parameters and reduced memory requirements.", "motivation": "Current low-rank methods for LLM pre-training suffer from compromised performance, computational overhead, and limited memory savings. The authors discovered that inter-layer activation residuals have low-rank properties, which can be leveraged for efficiency.", "method": "Proposes Cross-layer Low-Rank residual Network (CR-Net) with a dual-path architecture that reconstructs layer activations by combining previous-layer outputs with their low-rank differences. Includes a specialized activation recomputation strategy for memory reduction.", "result": "Extensive pre-training experiments across model scales (60M to 7B parameters) show CR-Net consistently outperforms state-of-the-art low-rank frameworks while requiring fewer computational resources and less memory.", "conclusion": "CR-Net effectively addresses the limitations of current low-rank methods by leveraging low-rank residual properties, achieving better performance with improved efficiency in computational resources and memory usage."}}
{"id": "2509.18997", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18997", "abs": "https://arxiv.org/abs/2509.18997", "authors": ["Pascal Esser", "Maximilian Fleissner", "Debarghya Ghoshdastidar"], "title": "Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization", "comment": null, "summary": "Representation learning from unlabeled data has been extensively studied in\nstatistics, data science and signal processing with a rich literature on\ntechniques for dimension reduction, compression, multi-dimensional scaling\namong others. However, current deep learning models use new principles for\nunsupervised representation learning that cannot be easily analyzed using\nclassical theories. For example, visual foundation models have found tremendous\nsuccess using self-supervision or denoising/masked autoencoders, which\neffectively learn representations from massive amounts of unlabeled data.\nHowever, it remains difficult to characterize the representations learned by\nthese models and to explain why they perform well for diverse prediction tasks\nor show emergent behavior. To answer these questions, one needs to combine\nmathematical tools from statistics and optimization. This paper provides an\noverview of recent theoretical advances in representation learning from\nunlabeled data and mentions our contributions in this direction.", "AI": {"tldr": "This paper provides a theoretical overview of representation learning from unlabeled data, highlighting the gap between classical statistical methods and modern deep learning approaches, and discusses recent theoretical advances in analyzing these models.", "motivation": "Current deep learning models use new unsupervised representation learning principles that cannot be easily analyzed using classical theories, making it difficult to characterize why they perform well for diverse tasks or show emergent behavior.", "method": "The paper combines mathematical tools from statistics and optimization to provide theoretical analysis of representation learning from unlabeled data, particularly focusing on visual foundation models using self-supervision or denoising/masked autoencoders.", "result": "The paper presents an overview of recent theoretical advances in understanding representation learning from unlabeled data and mentions the authors' contributions to this research direction.", "conclusion": "There is a need for new theoretical frameworks that can explain the success of modern unsupervised representation learning methods, and combining tools from statistics and optimization provides a promising approach for such analysis."}}
{"id": "2509.19018", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19018", "abs": "https://arxiv.org/abs/2509.19018", "authors": ["Teng Xiao", "Zuchao Li", "Lefei Zhang"], "title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment", "comment": null, "summary": "Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge.", "AI": {"tldr": "OmniBridge is a unified multimodal framework that supports vision-language understanding, generation, and retrieval using a language-centric design with lightweight bidirectional latent alignment and decoupled training strategy.", "motivation": "Current multimodal LLM solutions treat tasks in isolation or require expensive training from scratch, leading to high computational costs and limited generalization across modalities.", "method": "Uses pretrained LLMs with lightweight bidirectional latent alignment module and two-stage decoupled training: supervised fine-tuning with latent space alignment, and semantic-guided diffusion training with learnable query embeddings.", "result": "Achieves competitive or state-of-the-art performance across multiple benchmarks in all three tasks (understanding, generation, retrieval).", "conclusion": "Demonstrates effectiveness of latent space alignment for unifying multimodal modeling under a shared representation space, providing a modular and efficient solution."}}
{"id": "2509.19032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19032", "abs": "https://arxiv.org/abs/2509.19032", "authors": ["Kashaf Ul Emaan"], "title": "Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling", "comment": null, "summary": "Detection of credit card fraud is an acute issue of financial security\nbecause transaction datasets are highly lopsided, with fraud cases being only a\ndrop in the ocean. Balancing datasets using the most popular methods of\ntraditional oversampling such as the Synthetic Minority Oversampling Technique\n(SMOTE) generally create simplistic synthetic samples that are not readily\napplicable to complex fraud patterns. Recent industry advances that include\nConditional Tabular Generative Adversarial Networks (CTGAN) and Tabular\nVariational Autoencoders (TVAE) have demonstrated increased efficiency in\ntabular synthesis, yet all these models still exhibit issues with\nhigh-dimensional dependence modelling. Now we will present our hybrid approach\nwhere we use a Generative Adversarial Network (GAN) with a Transformer encoder\nblock to produce realistic fraudulent transactions samples. The GAN\narchitecture allows training realistic generators adversarial, and the\nTransformer allows the model to learn rich feature interactions by\nself-attention. Such a hybrid strategy overcomes the limitations of SMOTE,\nCTGAN, and TVAE by producing a variety of high-quality synthetic minority\nclasses samples. We test our algorithm on the publicly-available Credit Card\nFraud Detection dataset and compare it to conventional and generative\nresampling strategies with a variety of classifiers, such as Logistic\nRegression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and\nSupport Vector Machine (SVM). Findings indicate that our Transformer-based GAN\nshows substantial gains in Recall, F1-score and Area Under the Receiver\nOperating Characteristic Curve (AUC), which indicates that it is effective in\novercoming the severe class imbalance inherent in the task of fraud detection.", "AI": {"tldr": "A hybrid GAN-Transformer approach for generating realistic fraudulent transaction samples to address class imbalance in credit card fraud detection, outperforming traditional methods like SMOTE and other generative models.", "motivation": "Credit card fraud detection faces severe class imbalance issues where fraud cases are extremely rare. Traditional oversampling methods like SMOTE create simplistic synthetic samples that don't capture complex fraud patterns, while existing generative models like CTGAN and TVAE struggle with high-dimensional dependence modeling.", "method": "Proposes a hybrid approach combining Generative Adversarial Network (GAN) with Transformer encoder blocks. The GAN enables adversarial training of realistic generators, while the Transformer's self-attention mechanism learns rich feature interactions to overcome limitations of previous methods.", "result": "Tested on the Credit Card Fraud Detection dataset and compared with conventional and generative resampling strategies using various classifiers (LR, RF, XGBoost, SVM). The Transformer-based GAN showed substantial improvements in Recall, F1-score, and AUC metrics.", "conclusion": "The hybrid GAN-Transformer approach effectively overcomes severe class imbalance in fraud detection by producing high-quality synthetic minority class samples, demonstrating superior performance compared to existing methods."}}
{"id": "2509.19044", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19044", "abs": "https://arxiv.org/abs/2509.19044", "authors": ["Yang Li", "Chenyu Wang", "Tingrui Wang", "Yongwei Wang", "Haonan Li", "Zhunga Liu", "Quan Pan"], "title": "Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks", "comment": null, "summary": "Black-box adversarial attacks remain challenging due to limited access to\nmodel internals. Existing methods often depend on specific network\narchitectures or require numerous queries, resulting in limited\ncross-architecture transferability and high query costs. To address these\nlimitations, we propose JAD, a latent diffusion model framework for black-box\nadversarial attacks. JAD generates adversarial examples by leveraging a latent\ndiffusion model guided by attention maps distilled from both a convolutional\nneural network (CNN) and a Vision Transformer (ViT) models. By focusing on\nimage regions that are commonly sensitive across architectures, this approach\ncrafts adversarial perturbations that transfer effectively between different\nmodel types. This joint attention distillation strategy enables JAD to be\narchitecture-agnostic, achieving superior attack generalization across diverse\nmodels. Moreover, the generative nature of the diffusion framework yields high\nadversarial sample generation efficiency by reducing reliance on iterative\nqueries. Experiments demonstrate that JAD offers improved attack\ngeneralization, generation efficiency, and cross-architecture transferability\ncompared to existing methods, providing a promising and effective paradigm for\nblack-box adversarial attacks.", "AI": {"tldr": "JAD is a latent diffusion model framework for black-box adversarial attacks that uses joint attention distillation from CNN and ViT models to generate architecture-agnostic adversarial examples with improved transferability and efficiency.", "motivation": "Existing black-box adversarial attack methods have limitations including dependency on specific network architectures, high query costs, and poor cross-architecture transferability.", "method": "JAD leverages a latent diffusion model guided by attention maps distilled from both CNN and Vision Transformer models, focusing on commonly sensitive image regions across different architectures.", "result": "Experiments show JAD achieves superior attack generalization, generation efficiency, and cross-architecture transferability compared to existing methods.", "conclusion": "JAD provides a promising and effective paradigm for black-box adversarial attacks by being architecture-agnostic and reducing reliance on iterative queries."}}
{"id": "2509.19078", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19078", "abs": "https://arxiv.org/abs/2509.19078", "authors": ["Jian Xu", "Qibin Zhao", "John Paisley", "Delu Zeng"], "title": "Diffusion Bridge Variational Inference for Deep Gaussian Processes", "comment": null, "summary": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian\nmodeling but pose substantial challenges for posterior inference, especially\nover inducing variables. Denoising diffusion variational inference (DDVI)\naddresses this by modeling the posterior as a time-reversed diffusion from a\nsimple Gaussian prior. However, DDVI's fixed unconditional starting\ndistribution remains far from the complex true posterior, resulting in\ninefficient inference trajectories and slow convergence. In this work, we\npropose Diffusion Bridge Variational Inference (DBVI), a principled extension\nof DDVI that initiates the reverse diffusion from a learnable, data-dependent\ninitial distribution. This initialization is parameterized via an amortized\nneural network and progressively adapted using gradients from the ELBO\nobjective, reducing the posterior gap and improving sample efficiency. To\nenable scalable amortization, we design the network to operate on the inducing\ninputs, which serve as structured, low-dimensional summaries of the dataset and\nnaturally align with the inducing variables' shape. DBVI retains the\nmathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time\nSDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We\nderive a tractable training objective under this formulation and implement DBVI\nfor scalable inference in large-scale DGPs. Across regression, classification,\nand image reconstruction tasks, DBVI consistently outperforms DDVI and other\nvariational baselines in predictive accuracy, convergence speed, and posterior\nquality.", "AI": {"tldr": "DBVI improves upon DDVI by using a learnable, data-dependent initial distribution for diffusion-based variational inference in deep Gaussian processes, leading to better efficiency and performance.", "motivation": "DDVI's fixed unconditional starting distribution is far from the true posterior, causing inefficient inference and slow convergence in deep Gaussian processes.", "method": "DBVI initiates reverse diffusion from an amortized neural network parameterized initial distribution that operates on inducing inputs, using Girsanov-based ELBOs and reverse-time SDEs with a Doob-bridged diffusion process.", "result": "DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality across regression, classification, and image reconstruction tasks.", "conclusion": "DBVI provides a scalable and efficient inference method for deep Gaussian processes by bridging the gap between the initial distribution and true posterior through learnable data-dependent initialization."}}
{"id": "2509.19098", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.19098", "abs": "https://arxiv.org/abs/2509.19098", "authors": ["Adrien Prevost", "Timothee Mathieu", "Odalric-Ambrym Maillard"], "title": "Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning", "comment": null, "summary": "We study the non-contextual multi-armed bandit problem in a transfer learning\nsetting: before any pulls, the learner is given N'_k i.i.d. samples from each\nsource distribution nu'_k, and the true target distributions nu_k lie within a\nknown distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first\nderive a problem-dependent asymptotic lower bound on cumulative regret that\nextends the classical Lai-Robbins result to incorporate the transfer parameters\n(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that\nmatches this new bound in the Gaussian case. Finally, we validate our approach\nvia simulations, showing that KL-UCB-Transfer significantly outperforms the\nno-prior baseline when source and target distributions are sufficiently close.", "AI": {"tldr": "This paper studies transfer learning in multi-armed bandits, where prior knowledge from source distributions helps learn target distributions, and proposes an algorithm that matches theoretical bounds.", "motivation": "To leverage transfer learning in bandit problems by using samples from source distributions to improve learning efficiency when target distributions are similar to sources.", "method": "Developed KL-UCB-Transfer, an index policy that incorporates transfer parameters (distance bounds, sample sizes) and matches the derived asymptotic lower bound for Gaussian distributions.", "result": "Theoretical analysis shows the algorithm achieves optimal regret bounds, and simulations demonstrate significant performance improvements over baseline methods when source and target distributions are close.", "conclusion": "Transfer learning can effectively reduce regret in multi-armed bandits when prior knowledge is available, with KL-UCB-Transfer providing optimal performance guarantees."}}
{"id": "2509.19104", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19104", "abs": "https://arxiv.org/abs/2509.19104", "authors": ["Sharan Sahu", "Martin T. Wells"], "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment", "comment": "70 pages, 9 figures, 3 tables", "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates.", "AI": {"tldr": "DRO-REBEL introduces a unified family of robust RLHF updates using Wasserstein, KL, and \u03c7\u00b2 ambiguity sets to address overoptimization in offline RLHF, achieving optimal parametric rates with practical algorithms.", "motivation": "Existing offline RLHF approaches suffer from overoptimization where models overfit to reward misspecification and drift from preferred behaviors observed during training.", "method": "Uses Fenchel duality to reduce updates to simple relative-reward regression, avoiding PPO-style clipping or auxiliary value networks. Derives practical SGD algorithms for three divergences: gradient regularization (Wasserstein), importance weighting (KL), and fast 1-D dual solve (\u03c7\u00b2).", "result": "Achieves O(n^{-1/4}) estimation bounds with tighter constants than prior approaches, recovering minimax-optimal O(n^{-1/2}) rate via localized Rademacher complexity analysis. Experiments show strong worst-case robustness across unseen preference mixtures, model sizes, and data scales.", "conclusion": "Validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve optimal rates but forfeit coverage, while coverage-guaranteeing radii incur O(n^{-1/4}) rates. \u03c7\u00b2-REBEL shows consistently strong empirical performance."}}
{"id": "2509.19128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19128", "abs": "https://arxiv.org/abs/2509.19128", "authors": ["Alexandre Pich\u00e9", "Ehsan Kamaloo", "Rafael Pardinas", "Dzmitry Bahdanau"], "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio", "comment": null, "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution.", "AI": {"tldr": "PipelineRL introduces concurrent asynchronous data generation and model training with in-flight weight updates to improve hardware efficiency and data freshness for RL-based LLM training.", "motivation": "Current RL methods for LLMs struggle with scaling due to poor hardware utilization and stale off-policy data that harms RL algorithm performance.", "method": "PipelineRL uses concurrent asynchronous data generation and model training with novel in-flight weight updates, allowing LLM generation engine to receive updated model weights with minimal interruption during token sequence generation.", "result": "Experiments on 128 H100 GPUs show PipelineRL achieves ~2x faster learning than conventional RL baselines while maintaining highly on-policy training data.", "conclusion": "PipelineRL provides a superior trade-off between hardware efficiency and data on-policyness, with scalable open-source implementation released."}}
{"id": "2509.19159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19159", "abs": "https://arxiv.org/abs/2509.19159", "authors": ["Qingfeng Lan", "Gautham Vasan", "A. Rupam Mahmood"], "title": "Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions", "comment": "Code release: https://github.com/qlan3/ENN", "summary": "Catastrophic forgetting has remained a significant challenge for efficient\nreinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While\nrecent works have proposed effective methods to mitigate this issue, they\nmainly focus on the algorithmic side. Meanwhile, we do not fully understand\nwhat architectural properties of neural networks lead to catastrophic\nforgetting. This study aims to fill this gap by studying the role of activation\nfunctions in the training dynamics of neural networks and their impact on\ncatastrophic forgetting in reinforcement learning setup. Our study reveals\nthat, besides sparse representations, the gradient sparsity of activation\nfunctions also plays an important role in reducing forgetting. Based on this\ninsight, we propose a new class of activation functions, elephant activation\nfunctions, that can generate both sparse outputs and sparse gradients. We show\nthat by simply replacing classical activation functions with elephant\nactivation functions in the neural networks of value-based algorithms, we can\nsignificantly improve the resilience of neural networks to catastrophic\nforgetting, thus making reinforcement learning more sample-efficient and\nmemory-efficient.", "AI": {"tldr": "This paper studies how activation functions affect catastrophic forgetting in reinforcement learning, revealing that gradient sparsity is key. The authors propose 'elephant activation functions' that create sparse outputs and gradients, significantly improving neural network resilience to forgetting.", "motivation": "Catastrophic forgetting has been a major challenge in reinforcement learning for decades. While recent works focus on algorithmic solutions, there's limited understanding of how neural network architecture properties contribute to forgetting. This study aims to fill this gap by examining activation functions' role.", "method": "The authors analyze activation functions' training dynamics and their impact on catastrophic forgetting. They propose a new class of 'elephant activation functions' that generate both sparse outputs and sparse gradients. These are simply substituted for classical activation functions in value-based RL algorithms.", "result": "Replacing classical activation functions with elephant activation functions significantly improves neural networks' resilience to catastrophic forgetting. This makes reinforcement learning more sample-efficient and memory-efficient.", "conclusion": "Gradient sparsity of activation functions, in addition to sparse representations, plays a crucial role in reducing catastrophic forgetting. The proposed elephant activation functions provide an effective architectural solution to mitigate forgetting in reinforcement learning."}}
{"id": "2509.19189", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19189", "abs": "https://arxiv.org/abs/2509.19189", "authors": ["Binghui Li", "Fengling Chen", "Zixun Huang", "Lean Wang", "Lei Wu"], "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws", "comment": "52 pages, accepted by NeurIPS 2025 as a spotlight paper", "summary": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training.", "AI": {"tldr": "This paper introduces Functional Scaling Law (FSL) to model loss dynamics during LLM training, capturing learning rate schedule effects through a novel convolution-type functional term.", "motivation": "Existing scaling laws focus only on final loss, ignoring training dynamics and learning rate schedule impacts. The authors aim to bridge this gap by studying training process dynamics.", "method": "Uses teacher-student kernel regression with online SGD, develops intrinsic time viewpoint and SDE modeling to derive FSL that characterizes population risk evolution for general learning rate schedules.", "result": "FSL makes learning rate schedule effects fully tractable. Theoretical justification provided for empirical practices: higher-capacity models are more efficient, learning rate decay improves efficiency, WSD schedules outperform direct-decay.", "conclusion": "FSL framework deepens understanding of LLM pre-training dynamics and provides insights for improving large-scale model training, with practical relevance demonstrated across 0.1B to 1B parameter models."}}
{"id": "2509.19197", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19197", "abs": "https://arxiv.org/abs/2509.19197", "authors": ["Abdul-Rauf Nuhu", "Parham Kebria", "Vahid Hemmati", "Benjamin Lartey", "Mahmoud Nabil Mahmoud", "Abdollah Homaifar", "Edward Tunstel"], "title": "A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness", "comment": null, "summary": "Data-driven models, especially deep learning classifiers often demonstrate\ngreat success on clean datasets. Yet, they remain vulnerable to common data\ndistortions such as adversarial and common corruption perturbations. These\nperturbations can significantly degrade performance, thereby challenging the\noverall reliability of the models. Traditional robustness validation typically\nrelies on perturbed test datasets to assess and improve model performance. In\nour framework, however, we propose a validation approach that extracts \"weak\nrobust\" samples directly from the training dataset via local robustness\nanalysis. These samples, being the most susceptible to perturbations, serve as\nan early and sensitive indicator of the model's vulnerabilities. By evaluating\nmodels on these challenging training instances, we gain a more nuanced\nunderstanding of its robustness, which informs targeted performance\nenhancement. We demonstrate the effectiveness of our approach on models trained\nwith CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation\nguided by weak robust samples can drive meaningful improvements in model\nreliability under adversarial and common corruption scenarios.", "AI": {"tldr": "Proposes a validation approach that extracts 'weak robust' samples from training data via local robustness analysis to identify model vulnerabilities early, enabling targeted performance enhancement against adversarial and corruption perturbations.", "motivation": "Deep learning classifiers perform well on clean data but remain vulnerable to adversarial and common corruption perturbations, challenging model reliability. Traditional robustness validation relies on perturbed test datasets, which may not provide early vulnerability detection.", "method": "Extracts 'weak robust' samples directly from training dataset through local robustness analysis. These samples are the most susceptible to perturbations and serve as early indicators of model vulnerabilities. Models are evaluated on these challenging training instances to understand robustness nuances.", "result": "Demonstrated effectiveness on models trained with CIFAR-10, CIFAR-100, and ImageNet. The approach enables meaningful improvements in model reliability under adversarial and common corruption scenarios.", "conclusion": "Robustness validation guided by weak robust samples provides a more nuanced understanding of model vulnerabilities and drives targeted performance enhancement, offering an effective alternative to traditional perturbed test dataset approaches."}}
{"id": "2509.19215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19215", "abs": "https://arxiv.org/abs/2509.19215", "authors": ["Juntong Ni", "Saurabh Kataria", "Shengpu Tang", "Carl Yang", "Xiao Hu", "Wei Jin"], "title": "PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation", "comment": "Accepted at NeurIPS 2025 Workshop on Learning from Time Series for\n  Health", "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, yet\nlarge PPG foundation models remain difficult to deploy on resource-limited\ndevices. We present PPG-Distill, a knowledge distillation framework that\ntransfers both global and local knowledge through prediction-, feature-, and\npatch-level distillation. PPG-Distill incorporates morphology distillation to\npreserve local waveform patterns and rhythm distillation to capture inter-patch\ntemporal structures. On heart rate estimation and atrial fibrillation\ndetection, PPG-Distill improves student performance by up to 21.8% while\nachieving 7X faster inference and reducing memory usage by 19X, enabling\nefficient PPG analysis on wearables", "AI": {"tldr": "PPG-Distill is a knowledge distillation framework that enables efficient PPG analysis on wearable devices by transferring knowledge from large foundation models to smaller student models through multi-level distillation techniques.", "motivation": "Large PPG foundation models are difficult to deploy on resource-limited wearable devices due to computational constraints, creating a need for efficient model compression techniques.", "method": "Uses knowledge distillation with prediction-, feature-, and patch-level distillation. Specifically incorporates morphology distillation for local waveform patterns and rhythm distillation for inter-patch temporal structures.", "result": "Achieves up to 21.8% improvement in student model performance for heart rate estimation and atrial fibrillation detection, with 7X faster inference and 19X memory reduction.", "conclusion": "PPG-Distill enables efficient PPG analysis on wearable devices by significantly improving performance while dramatically reducing computational requirements."}}
{"id": "2509.19222", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19222", "abs": "https://arxiv.org/abs/2509.19222", "authors": ["Julien Delavande", "Regis Pierrard", "Sasha Luccioni"], "title": "Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models", "comment": "10 pages. Accepted as an oral presentation at the NeurIPS 2025\n  NextVid Workshop (San Diego, December 6, 2025)", "summary": "Recent advances in text-to-video (T2V) generation have enabled the creation\nof high-fidelity, temporally coherent clips from natural language prompts. Yet\nthese systems come with significant computational costs, and their energy\ndemands remain poorly understood. In this paper, we present a systematic study\nof the latency and energy consumption of state-of-the-art open-source T2V\nmodels. We first develop a compute-bound analytical model that predicts scaling\nlaws with respect to spatial resolution, temporal length, and denoising steps.\nWe then validate these predictions through fine-grained experiments on\nWAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and\nlinear scaling with the number of denoising steps. Finally, we extend our\nanalysis to six diverse T2V models, comparing their runtime and energy profiles\nunder default settings. Our results provide both a benchmark reference and\npractical insights for designing and deploying more sustainable generative\nvideo systems.", "AI": {"tldr": "This paper presents a systematic study of the computational costs and energy consumption of text-to-video generation models, developing analytical models and validating them through experiments on various T2V systems.", "motivation": "Text-to-video generation systems have significant computational costs, but their energy demands remain poorly understood. The authors aim to provide systematic analysis of latency and energy consumption to enable more sustainable generative video systems.", "method": "Developed a compute-bound analytical model predicting scaling laws for spatial resolution, temporal length, and denoising steps. Validated predictions through fine-grained experiments on WAN2.1-T2V and extended analysis to six diverse T2V models.", "result": "Found quadratic growth in computational costs with spatial and temporal dimensions, and linear scaling with the number of denoising steps. Provided runtime and energy profiles for six T2V models under default settings.", "conclusion": "The study provides benchmark references and practical insights for designing and deploying more sustainable generative video systems by understanding their computational and energy requirements."}}
{"id": "2509.19233", "categories": ["cs.LG", "I.2.0; I.2.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.19233", "abs": "https://arxiv.org/abs/2509.19233", "authors": ["Milad Leyli-abadi", "Antoine Marot", "J\u00e9r\u00f4me Picault"], "title": "Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation", "comment": "Accepted at ECML PKDD ML4SPS 2025 workshop", "summary": "In the context of the energy transition, with increasing integration of\nrenewable sources and cross-border electricity exchanges, power grids are\nencountering greater uncertainty and operational risk. Maintaining grid\nstability under varying conditions is a complex task, and power flow simulators\nare commonly used to support operators by evaluating potential actions before\nimplementation. However, traditional physical solvers, while accurate, are\noften too slow for near real-time use. Machine learning models have emerged as\nfast surrogates, and to improve their adherence to physical laws (e.g.,\nKirchhoff's laws), they are often trained with embedded constraints which are\nalso known as physics-informed or hybrid models. This paper presents an\nablation study to demystify hybridization strategies, ranging from\nincorporating physical constraints as regularization terms or unsupervised\nlosses, and exploring model architectures from simple multilayer perceptrons to\nadvanced graph-based networks enabling the direct optimization of physics\nequations. Using our custom benchmarking pipeline for hybrid models called\nLIPS, we evaluate these models across four dimensions: accuracy, physical\ncompliance, industrial readiness, and out-of-distribution generalization. The\nresults highlight how integrating physical knowledge impacts performance across\nthese criteria. All the implementations are reproducible and provided in the\ncorresponding Github page.", "AI": {"tldr": "This paper presents an ablation study analyzing different hybridization strategies for machine learning models used as fast surrogates for power flow simulation, evaluating their performance across accuracy, physical compliance, industrial readiness, and generalization.", "motivation": "Power grids face increasing uncertainty due to renewable integration and cross-border exchanges, requiring fast simulation tools. Traditional physical solvers are accurate but too slow for real-time use, while ML models need better adherence to physical laws like Kirchhoff's laws.", "method": "The study uses a custom benchmarking pipeline called LIPS to evaluate various hybridization strategies, including physical constraints as regularization terms or unsupervised losses, and different architectures from MLPs to graph-based networks that enable direct optimization of physics equations.", "result": "The results demonstrate how different physical knowledge integration strategies impact performance across four key dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization.", "conclusion": "The study provides insights into effective hybridization strategies for power flow simulation models, with all implementations being reproducible and available on GitHub, contributing to better grid stability management tools."}}
{"id": "2509.19234", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19234", "abs": "https://arxiv.org/abs/2509.19234", "authors": ["Hesam Hosseini", "Ying Cao", "Ali H. Sayed"], "title": "Stability and Generalization of Adversarial Diffusion Training", "comment": null, "summary": "Algorithmic stability is an established tool for analyzing generalization.\nWhile adversarial training enhances model robustness, it often suffers from\nrobust overfitting and an enlarged generalization gap. Although recent work has\nestablished the convergence of adversarial training in decentralized networks,\nits generalization properties remain unexplored. This work presents a\nstability-based generalization analysis of adversarial training under the\ndiffusion strategy for convex losses. We derive a bound showing that the\ngeneralization error grows with both the adversarial perturbation strength and\nthe number of training steps, a finding consistent with single-agent case but\nnovel for decentralized settings. Numerical experiments on logistic regression\nvalidate these theoretical predictions.", "AI": {"tldr": "This paper presents a stability-based generalization analysis of adversarial training in decentralized networks, showing that generalization error increases with adversarial perturbation strength and training steps.", "motivation": "While adversarial training enhances model robustness, it suffers from robust overfitting and enlarged generalization gaps. Although convergence has been established for decentralized adversarial training, its generalization properties remain unexplored.", "method": "The authors use algorithmic stability analysis under the diffusion strategy for convex losses to derive generalization bounds for decentralized adversarial training.", "result": "The theoretical analysis shows that generalization error grows with both adversarial perturbation strength and number of training steps, which is consistent with single-agent cases but novel for decentralized settings. Numerical experiments on logistic regression validate these findings.", "conclusion": "This work provides the first stability-based generalization analysis for adversarial training in decentralized networks, establishing theoretical foundations for understanding generalization properties in distributed robust learning settings."}}
{"id": "2509.19284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19284", "abs": "https://arxiv.org/abs/2509.19284", "authors": ["Yunzhen Feng", "Julia Kempe", "Cheng Zhang", "Parag Jain", "Anthony Hartshorn"], "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT", "comment": null, "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\nremains unclear. While prior work reports gains from lengthening CoTs and\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\nstudies suggest that shorter thinking can outperform longer traces. We\ntherefore conduct a systematic evaluation across ten LRMs on math and\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\nthat both naive CoT lengthening and increased review are associated with\n*lower* accuracy.\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\nprocess quality. We introduce a graph view of CoT to extract structure and\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\nsteps in abandoned branches-that consistently outpredicts length and review\nratio for correctness across models. To probe causality, we design two\ninterventions. First, we rank candidate CoTs by each metric at test time, where\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\nbranches, which significantly improves accuracy, indicating that failed\nbranches bias subsequent reasoning. Taken together, these results characterize\neffective CoTs as those that *fail less* and support *structure-aware*\ntest-time scaling over indiscriminately generating long CoT.", "AI": {"tldr": "This paper challenges the 'longer-is-better' assumption for chain-of-thought reasoning, finding that longer CoTs and increased review actually lower accuracy. The authors introduce a Failed-Step Fraction metric that better predicts correctness and show that removing failed branches improves reasoning performance.", "motivation": "Large reasoning models spend substantial compute on long chain-of-thought traces, but what characterizes an effective CoT remains unclear. Prior work shows conflicting results about CoT length effectiveness, motivating systematic evaluation.", "method": "Systematic evaluation across ten large reasoning models on math and scientific reasoning tasks. Introduces graph view of CoT to extract structure and Failed-Step Fraction metric. Designs two interventions: ranking CoTs by metrics and editing CoTs to remove failed branches.", "result": "Both naive CoT lengthening and increased review are associated with lower accuracy. Failed-Step Fraction consistently outpredicts length and review ratio for correctness. Removing failed branches significantly improves accuracy, indicating failed branches bias subsequent reasoning.", "conclusion": "Effective CoTs are those that fail less, supporting structure-aware test-time scaling over indiscriminately generating long CoT traces. Failed-Step Fraction is a key metric for characterizing reasoning quality."}}
