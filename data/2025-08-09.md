<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 33]
- [cs.LG](#cs.LG) [Total: 97]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
*Chitranshu Harbola,Anupam Purwar*

Main category: cs.AI

TL;DR: The paper introduces an LLM-based system for prescriptive maintenance, combining vibration analysis with multi-agentic generation to provide actionable recommendations, validated on bearing vibration datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance industrial machinery maintenance by moving beyond anomaly detection to offer intelligent, actionable maintenance recommendations.

Method: Integrates LLMs with vibration frequency analysis (BPFO, BPFI, BSF, FTF) and multi-agentic generation for maintenance planning, including fault classification and severity assessment.

Result: Demonstrates effective anomaly detection and contextually relevant maintenance guidance, bridging condition monitoring with actionable plans.

Conclusion: Advances LLM applications in industrial maintenance, offering a scalable framework for prescriptive maintenance across sectors.

Abstract: Industrial machinery maintenance requires timely intervention to prevent
catastrophic failures and optimize operational efficiency. This paper presents
an integrated Large Language Model (LLM)-based intelligent system for
prescriptive maintenance that extends beyond traditional anomaly detection to
provide actionable maintenance recommendations. Building upon our prior LAMP
framework for numerical data analysis, we develop a comprehensive solution that
combines bearing vibration frequency analysis with multi agentic generation for
intelligent maintenance planning. Our approach serializes bearing vibration
data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM
processing, enabling few-shot anomaly detection with high accuracy. The system
classifies fault types (inner race, outer race, ball/roller, cage faults) and
assesses severity levels. A multi-agentic component processes maintenance
manuals using vector embeddings and semantic search, while also conducting web
searches to retrieve comprehensive procedural knowledge and access up-to-date
maintenance practices for more accurate and in-depth recommendations. The
Gemini model then generates structured maintenance recommendations includes
immediate actions, inspection checklists, corrective measures, parts
requirements, and timeline specifications. Experimental validation in bearing
vibration datasets demonstrates effective anomaly detection and contextually
relevant maintenance guidance. The system successfully bridges the gap between
condition monitoring and actionable maintenance planning, providing industrial
practitioners with intelligent decision support. This work advances the
application of LLMs in industrial maintenance, offering a scalable framework
for prescriptive maintenance across machinery components and industrial
sectors.

</details>


### [2] [ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis](https://arxiv.org/abs/2508.04915)
*Huiya Zhao,Yinghao Zhu,Zixiang Wang,Yasha Wang,Junyi Gao,Liantao Ma*

Main category: cs.AI

TL;DR: HealthFlow is a self-evolving AI agent for healthcare that improves strategic planning through meta-level evolution, outperforming existing frameworks.


<details>
  <summary>Details</summary>
Motivation: Current AI agents in healthcare rely on static strategies, limiting their ability to improve strategic planning, which is crucial for complex domains.

Method: HealthFlow uses a meta-level evolution mechanism to autonomously refine high-level problem-solving policies, learning from successes and failures. A new benchmark, EHRFlowBench, is introduced for evaluation.

Result: HealthFlow significantly outperforms state-of-the-art agent frameworks in complex health data analysis tasks.

Conclusion: The work shifts focus from tool-users to self-evolving task-managers, enabling more autonomous and effective AI for scientific discovery.

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [3] [Cognitive Duality for Adaptive Web Agents](https://arxiv.org/abs/2508.05081)
*Jiarun Liu,Chunhong Zhang,Zheng Hu*

Main category: cs.AI

TL;DR: The paper introduces CogniWeb, a modular web agent architecture inspired by human dual-process cognition, combining offline imitation learning and online exploration for efficient and effective web navigation.


<details>
  <summary>Details</summary>
Motivation: Web navigation is a challenging domain for AGI, requiring complex decision-making in dynamic environments. Current methods lack effective integration of offline and online learning paradigms.

Method: The authors propose a dual-process framework (System 1 for fast intuitive behaviors and System 2 for deliberative planning) implemented in CogniWeb, which adapts based on task complexity.

Result: CogniWeb achieves a 43.96% success rate on WebArena with 75% reduction in token usage, demonstrating competitive performance and efficiency.

Conclusion: The dual-process approach bridges the gap between offline and online learning, offering a unified and efficient solution for web navigation tasks.

Abstract: Web navigation represents a critical and challenging domain for evaluating
artificial general intelligence (AGI), demanding complex decision-making within
high-entropy, dynamic environments with combinatorially explosive action
spaces. Current approaches to building autonomous web agents either focus on
offline imitation learning or online exploration, but rarely integrate both
paradigms effectively. Inspired by the dual-process theory of human cognition,
we derive a principled decomposition into fast System 1 and slow System 2
cognitive processes. This decomposition provides a unifying perspective on
existing web agent methodologies, bridging the gap between offline learning of
intuitive reactive behaviors and online acquisition of deliberative planning
capabilities. We implement this framework in CogniWeb, a modular agent
architecture that adaptively toggles between fast intuitive processing and
deliberate reasoning based on task complexity. Our evaluation on WebArena
demonstrates that CogniWeb achieves competitive performance (43.96% success
rate) while maintaining significantly higher efficiency (75% reduction in token
usage).

</details>


### [4] [GeoFlow: Agentic Workflow Automation for Geospatial Tasks](https://arxiv.org/abs/2508.04719)
*Amulya Bhattaram,Justin Chung,Stanley Chung,Ranit Gupta,Janani Ramamoorthy,Kartikeya Gullapalli,Diana Marculescu,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: GeoFlow automates agentic workflows for geospatial tasks, improving success rates and reducing token usage.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks explicit guidance for geospatial API invocation, limiting agent performance.

Method: GeoFlow provides detailed tool-calling objectives to agents for runtime API selection.

Result: Increases success by 6.8% and reduces token usage up to fourfold.

Conclusion: GeoFlow outperforms state-of-the-art methods in efficiency and effectiveness.

Abstract: We present GeoFlow, a method that automatically generates agentic workflows
for geospatial tasks. Unlike prior work that focuses on reasoning decomposition
and leaves API selection implicit, our method provides each agent with detailed
tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow
increases agentic success by 6.8% and reduces token usage by up to fourfold
across major LLM families compared to state-of-the-art approaches.

</details>


### [5] [Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures](https://arxiv.org/abs/2508.05116)
*Peer-Benedikt Degen,Igor Asanov*

Main category: cs.AI

TL;DR: A study evaluates a Socratic AI Tutor for enhancing student research question development, showing improved critical thinking compared to uninstructed AI. It proposes orchestrated multi-agent systems (MAS) for education, discussing implications and scalability.


<details>
  <summary>Details</summary>
Motivation: To explore how dialogic AI can support metacognitive engagement and counter de-skilling narratives in generative AI usage in higher education.

Method: Controlled experiment with 65 pre-service teachers comparing a Socratic AI Tutor to an uninstructed AI chatbot.

Result: Students using the Socratic Tutor reported greater support for critical, independent, and reflective thinking.

Conclusion: The study offers empirical evidence and a conceptual framework for hybrid learning ecosystems integrating human-AI co-agency and pedagogical alignment.

Abstract: Generative AI is no longer a peripheral tool in higher education. It is
rapidly evolving into a general-purpose infrastructure that reshapes how
knowledge is generated, mediated, and validated. This paper presents findings
from a controlled experiment evaluating a Socratic AI Tutor, a large language
model designed to scaffold student research question development through
structured dialogue grounded in constructivist theory. Conducted with 65
pre-service teacher students in Germany, the study compares interaction with
the Socratic Tutor to engagement with an uninstructed AI chatbot. Students
using the Socratic Tutor reported significantly greater support for critical,
independent, and reflective thinking, suggesting that dialogic AI can stimulate
metacognitive engagement and challenging recent narratives of de-skilling due
to generative AI usage. These findings serve as a proof of concept for a
broader pedagogical shift: the use of multi-agent systems (MAS) composed of
specialised AI agents. To conceptualise this, we introduce the notion of
orchestrated MAS, modular, pedagogically aligned agent constellations, curated
by educators, that support diverse learning trajectories through differentiated
roles and coordinated interaction. To anchor this shift, we propose an adapted
offer-and-use model, in which students appropriate instructional offers from
these agents. Beyond technical feasibility, we examine system-level
implications for higher education institutions and students, including funding
necessities, changes to faculty roles, curriculars, competencies and assessment
practices. We conclude with a comparative cost-effectiveness analysis
highlighting the scalability of such systems. In sum, this study contributes
both empirical evidence and a conceptual roadmap for hybrid learning ecosystems
that embed human-AI co-agency and pedagogical alignment.

</details>


### [6] [Who is a Better Player: LLM against LLM](https://arxiv.org/abs/2508.04720)
*Yingjie Zhou,Jiezhang Cao,Farong Wen,Li Xu,Yanwei Jiang,Jun Jia,Ronghui Li,Xiaohong Liu,Yu Zhou,Xiongkuo Min,Jie Guo,Zicheng Zhang,Guangtao Zhai*

Main category: cs.AI

TL;DR: The paper introduces an adversarial benchmarking framework using board games to evaluate LLMs, addressing limitations of Q&A benchmarks. It uses Qi Town, a platform with 5 games and 20 LLM players, employing Elo ratings and PLG for technical evaluation, and PSS for mental fitness. Results show LLMs' adaptability but reveal instability in skill play.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' comprehensive performance beyond Q&A benchmarks by leveraging adversarial board games, which test strategic reasoning and intelligence.

Method: Developed Qi Town, a platform with 5 games and 20 LLM players, using Elo ratings, PLG for technical evaluation, and PSS for mental fitness. Conducted a round-robin tournament for systematic comparison.

Result: LLMs showed optimism and adaptability in adversarial environments but exhibited instability in skill play, as revealed by PLG analysis.

Conclusion: The framework effectively evaluates LLMs' adversarial performance, highlighting their adaptability but also instability, suggesting further research is needed.

Abstract: Adversarial board games, as a paradigmatic domain of strategic reasoning and
intelligence, have long served as both a popular competitive activity and a
benchmark for evaluating artificial intelligence (AI) systems. Building on this
foundation, we propose an adversarial benchmarking framework to assess the
comprehensive performance of Large Language Models (LLMs) through board games
competition, compensating the limitation of data dependency of the mainstream
Question-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a
specialized evaluation platform that supports 5 widely played games and
involves 20 LLM-driven players. The platform employs both the Elo rating system
and a novel Performance Loop Graph (PLG) to quantitatively evaluate the
technical capabilities of LLMs, while also capturing Positive Sentiment Score
(PSS) throughout gameplay to assess mental fitness. The evaluation is
structured as a round-robin tournament, enabling systematic comparison across
players. Experimental results indicate that, despite technical differences,
most LLMs remain optimistic about winning and losing, demonstrating greater
adaptability to high-stress adversarial environments than humans. On the other
hand, the complex relationship between cyclic wins and losses in PLGs exposes
the instability of LLMs' skill play during games, warranting further
explanation and exploration.

</details>


### [7] [Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)](https://arxiv.org/abs/2508.04846)
*Mahdi Nazari Ashani,Ali Asghar Alesheikh,Saba Kazemi,Kimya Kheirkhah,Yasin Mohammadi,Fatemeh Rezaie,Amir Mahdi Manafi,Hedieh Zarkesh*

Main category: cs.AI

TL;DR: The paper compares three methods for autonomous web-based GIS, favoring a client-side fine-tuned small language model (SLM) for its high accuracy and privacy benefits.


<details>
  <summary>Details</summary>
Motivation: Current cloud-based solutions for AWebGIS raise privacy and scalability concerns, prompting exploration of offline alternatives.

Method: Three approaches were tested: cloud-based LLMs, classical ML classifiers, and a client-side fine-tuned SLM (T5-small).

Result: The client-side SLM achieved the highest accuracy (0.93 exact matching, 0.99 Levenshtein similarity, 0.98 ROUGE scores) and reduced server load.

Conclusion: Browser-executable SLMs are feasible for AWebGIS, offering accuracy, privacy, and scalability advantages over cloud-based methods.

Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to
perform geospatial operations from natural language input, providing intuitive,
intelligent, and hands-free interaction. However, most current solutions rely
on cloud-based large language models (LLMs), which require continuous internet
access and raise users' privacy and scalability issues due to centralized
server processing. This study compares three approaches to enabling AWebGIS:
(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)
a semi-automated offline method using classical machine learning classifiers
such as support vector machine and random forest; and (3) a fully autonomous
offline (client-side) method based on a fine-tuned small language model (SLM),
specifically T5-small model, executed in the client's web browser. The third
approach, which leverages SLMs, achieved the highest accuracy among all
methods, with an exact matching accuracy of 0.93, Levenshtein similarity of
0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L
scores of 0.98. Crucially, this client-side computation strategy reduces the
load on backend servers by offloading processing to the user's device,
eliminating the need for server-based inference. These results highlight the
feasibility of browser-executable models for AWebGIS solutions.

</details>


### [8] [Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning](https://arxiv.org/abs/2508.04848)
*Chang Tian,Matthew B. Blaschko,Mingzhe Xing,Xiuxing Li,Yinliang Yue,Marie-Francine Moens*

Main category: cs.AI

TL;DR: RL improves LLM reasoning in ideal settings but fails in non-ideal scenarios like summary inference, noise suppression, and contextual filtering. Current methods don't fully address these deficits.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLM reasoning in realistic, non-ideal scenarios, inspired by human reasoning reliability under imperfect inputs.

Method: Fine-tuned LLMs and an LVLM using RL (policy-gradient), tested on eight datasets across three non-ideal scenarios.

Result: RL boosts performance in ideal settings but declines significantly in non-ideal scenarios, revealing reasoning limitations.

Conclusion: Current LLM reasoning capabilities are overstated; evaluation in non-ideal scenarios is crucial.

Abstract: Reinforcement learning (RL) has become a key technique for enhancing the
reasoning abilities of large language models (LLMs), with policy-gradient
algorithms dominating the post-training stage because of their efficiency and
effectiveness. However, most existing benchmarks evaluate large-language-model
reasoning under idealized settings, overlooking performance in realistic,
non-ideal scenarios. We identify three representative non-ideal scenarios with
practical relevance: summary inference, fine-grained noise suppression, and
contextual filtering. We introduce a new research direction guided by
brain-science findings that human reasoning remains reliable under imperfect
inputs. We formally define and evaluate these challenging scenarios. We
fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM)
using RL with a representative policy-gradient algorithm and then test their
performance on eight public datasets. Our results reveal that while RL
fine-tuning improves baseline reasoning under idealized settings, performance
declines significantly across all three non-ideal scenarios, exposing critical
limitations in advanced reasoning capabilities. Although we propose a
scenario-specific remediation method, our results suggest current methods leave
these reasoning deficits largely unresolved. This work highlights that the
reasoning abilities of large models are often overstated and underscores the
importance of evaluating models under non-ideal scenarios. The code and data
will be released at XXXX.

</details>


### [9] [The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein--Ligand Binding](https://arxiv.org/abs/2508.05006)
*Youzhi Zhang,Yufei Li,Gaofeng Meng,Hongbin Liu,Jiebo Luo*

Main category: cs.AI

TL;DR: A game-theoretic framework (Docking Game) with Loop Self-Play (LoopPlay) improves ligand docking accuracy by 10% over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current multi-task learning models perform poorly in ligand docking due to structural differences between ligands and proteins.

Method: Proposes a two-player game (ligand and protein players) solved by LoopPlay, alternating training in outer and inner loops for mutual adaptation.

Result: LoopPlay achieves ~10% better binding mode prediction than state-of-the-art methods.

Conclusion: The framework enhances molecular docking accuracy, benefiting drug discovery.

Abstract: Molecular docking is a crucial aspect of drug discovery, as it predicts the
binding interactions between small-molecule ligands and protein pockets.
However, current multi-task learning models for docking often show inferior
performance in ligand docking compared to protein pocket docking. This
disparity arises largely due to the distinct structural complexities of ligands
and proteins. To address this issue, we propose a novel game-theoretic
framework that models the protein-ligand interaction as a two-player game
called the Docking Game, with the ligand docking module acting as the ligand
player and the protein pocket docking module as the protein player. To solve
this game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which
alternately trains these players through a two-level loop. In the outer loop,
the players exchange predicted poses, allowing each to incorporate the other's
structural predictions, which fosters mutual adaptation over multiple
iterations. In the inner loop, each player dynamically refines its predictions
by incorporating its own predicted ligand or pocket poses back into its model.
We theoretically show the convergence of LoopPlay, ensuring stable
optimization. Extensive experiments conducted on public benchmark datasets
demonstrate that LoopPlay achieves approximately a 10\% improvement in
predicting accurate binding modes compared to previous state-of-the-art
methods. This highlights its potential to enhance the accuracy of molecular
docking in drug discovery.

</details>


### [10] [Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses](https://arxiv.org/abs/2508.05009)
*Bin Han,Robert Wolfe,Anat Caspi,Bill Howe*

Main category: cs.AI

TL;DR: LLMs show promise for spatial data integration but struggle with macro-scale spatial reasoning. A review-and-refine method improves accuracy, positioning LLMs as a flexible alternative to rule-based approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based and machine learning methods for spatial data integration are limited. LLMs offer a potential solution by leveraging human-like reasoning.

Method: Investigate LLMs' spatial reasoning, adapt a review-and-refine method, and evaluate performance with relevant features.

Result: LLMs perform well with relevant features but struggle with macro-scale reasoning. The review-and-refine method effectively corrects errors.

Conclusion: LLMs are a promising alternative for spatial data integration, with future research directions including post-training and multi-modal methods.

Abstract: We explore the application of large language models (LLMs) to empower domain
experts in integrating large, heterogeneous, and noisy urban spatial datasets.
Traditional rule-based integration methods are unable to cover all edge cases,
requiring manual verification and repair. Machine learning approaches require
collecting and labeling of large numbers of task-specific samples. In this
study, we investigate the potential of LLMs for spatial data integration. Our
analysis first considers how LLMs reason about environmental spatial
relationships mediated by human experience, such as between roads and
sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they
struggle to connect the macro-scale environment with the relevant computational
geometry tasks, often producing logically incoherent responses. But when
provided relevant features, thereby reducing dependence on spatial reasoning,
LLMs are able to generate high-performing results. We then adapt a
review-and-refine method, which proves remarkably effective in correcting
erroneous initial responses while preserving accurate responses. We discuss
practical implications of employing LLMs for spatial data integration in
real-world contexts and outline future research directions, including
post-training, multi-modal integration methods, and support for diverse data
formats. Our findings position LLMs as a promising and flexible alternative to
traditional rule-based heuristics, advancing the capabilities of adaptive
spatial data integration.

</details>


### [11] [MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.05083)
*Dexuan Xu,Jieyi Wang,Zhongyan Chai,Yongzhi Cao,Hanpin Wang,Huamin Zhang,Yu Huang*

Main category: cs.AI

TL;DR: MedMKEB is the first benchmark for evaluating multimodal medical knowledge editing in MLLMs, addressing reliability, generality, and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic benchmarks for updating medical knowledge in multimodal models without retraining.

Method: Built on a medical visual QA dataset, MedMKEB includes tasks like counterfactual correction and adversarial robustness, validated by experts.

Result: Experiments show limitations of current editing approaches, emphasizing the need for specialized strategies.

Conclusion: MedMKEB aims to advance trustworthy and efficient medical knowledge editing in MLLMs.

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly improved medical AI, enabling it to unify the understanding of
visual and textual information. However, as medical knowledge continues to
evolve, it is critical to allow these models to efficiently update outdated or
incorrect information without retraining from scratch. Although textual
knowledge editing has been widely studied, there is still a lack of systematic
benchmarks for multimodal medical knowledge editing involving image and text
modalities. To fill this gap, we present MedMKEB, the first comprehensive
benchmark designed to evaluate the reliability, generality, locality,
portability, and robustness of knowledge editing in medical multimodal large
language models. MedMKEB is built on a high-quality medical visual
question-answering dataset and enriched with carefully constructed editing
tasks, including counterfactual correction, semantic generalization, knowledge
transfer, and adversarial robustness. We incorporate human expert validation to
ensure the accuracy and reliability of the benchmark. Extensive single editing
and sequential editing experiments on state-of-the-art general and medical
MLLMs demonstrate the limitations of existing knowledge-based editing
approaches in medicine, highlighting the need to develop specialized editing
strategies. MedMKEB will serve as a standard benchmark to promote the
development of trustworthy and efficient medical knowledge editing algorithms.

</details>


### [12] [EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search](https://arxiv.org/abs/2508.05113)
*Xinyue Wu,Fan Hu,Shaik Jani Babu,Yi Zhao,Xinfei Guo*

Main category: cs.AI

TL;DR: EasySize is a lightweight gate sizing framework using a finetuned Qwen3-8B model, achieving universal applicability and outperforming existing methods with reduced computational resources.


<details>
  <summary>Details</summary>
Motivation: Analog circuit design is complex and time-consuming, with existing AI methods lacking portability and efficiency across technology nodes.

Method: Combines finetuned Qwen3-8B with dynamic task-specific loss functions and heuristic search (DE and PSO) for efficient gate sizing.

Result: Outperforms AutoCkt on 86.67% of tasks with 96.67% simulation resource reduction, despite training only on 350nm data.

Conclusion: EasySize reduces reliance on human expertise and computational resources, simplifying analog circuit design.

Abstract: Analog circuit design is a time-consuming, experience-driven task in chip
development. Despite advances in AI, developing universal, fast, and stable
gate sizing methods for analog circuits remains a significant challenge. Recent
approaches combine Large Language Models (LLMs) with heuristic search
techniques to enhance generalizability, but they often depend on large model
sizes and lack portability across different technology nodes. To overcome these
limitations, we propose EasySize, the first lightweight gate sizing framework
based on a finetuned Qwen3-8B model, designed for universal applicability
across process nodes, design specifications, and circuit topologies. EasySize
exploits the varying Ease of Attainability (EOA) of performance metrics to
dynamically construct task-specific loss functions, enabling efficient
heuristic search through global Differential Evolution (DE) and local Particle
Swarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned
solely on 350nm node data, EasySize achieves strong performance on 5
operational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology
nodes without additional targeted training, and outperforms AutoCkt, a
widely-used Reinforcement Learning based sizing framework, on 86.67\% of tasks
with more than 96.67\% of simulation resources reduction. We argue that
EasySize can significantly reduce the reliance on human expertise and
computational resources in gate sizing, thereby accelerating and simplifying
the analog circuit design process. EasySize will be open-sourced at a later
date.

</details>


### [13] [Graph-based Event Log Repair](https://arxiv.org/abs/2508.05145)
*Sebastiano Dissegna,Chiara Di Francescomarino,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: The paper proposes a Heterogeneous Graph Neural Network model to reconstruct missing event attributes in Process Mining, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Event logs often have missing data, and existing methods either require process models or use less expressive ML/DL models. Graph Neural Networks offer a more natural representation for trace reconstruction.

Method: Develops a Heterogeneous Graph Neural Network to reconstruct missing event attributes from incomplete traces.

Result: The model shows very good performance in reconstructing all event attributes, outperforming autoencoder-based approaches on synthetic and real logs.

Conclusion: The proposed approach is effective for comprehensive event log reconstruction, addressing limitations of existing model-free methods.

Abstract: The quality of event logs in Process Mining is crucial when applying any form
of analysis to them. In real-world event logs, the acquisition of data can be
non-trivial (e.g., due to the execution of manual activities and related manual
recording or to issues in collecting, for each event, all its attributes), and
often may end up with events recorded with some missing information. Standard
approaches to the problem of trace (or log) reconstruction either require the
availability of a process model that is used to fill missing values by
leveraging different reasoning techniques or employ a Machine Learning/Deep
Learning model to restore the missing values by learning from similar cases. In
recent years, a new type of Deep Learning model that is capable of handling
input data encoded as graphs has emerged, namely Graph Neural Networks. Graph
Neural Network models, and even more so Heterogeneous Graph Neural Networks,
offer the advantage of working with a more natural representation of complex
multi-modal sequences like the execution traces in Process Mining, allowing for
more expressive and semantically rich encodings.
  In this work, we focus on the development of a Heterogeneous Graph Neural
Network model that, given a trace containing some incomplete events, will
return the full set of attributes missing from those events. We evaluate our
work against a state-of-the-art approach leveraging autoencoders on two
synthetic logs and four real event logs, on different types of missing values.
Different from state-of-the-art model-free approaches, which mainly focus on
repairing a subset of event attributes, the proposed approach shows very good
performance in reconstructing all different event attributes.

</details>


### [14] [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
*Zhuohang Jiang,Pangjing Wu,Xu Yuan,Wenqi Fan,Qing Li*

Main category: cs.AI

TL;DR: QA-Dragon improves VQA by dynamically retrieving and combining text and image knowledge, outperforming baselines in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods retrieve from text or images separately, limiting multi-hop reasoning and up-to-date knowledge integration.

Method: QA-Dragon uses a domain router for subject identification and a search router for dynamic retrieval, combining text and image agents.

Result: Achieves significant improvements in accuracy and knowledge overlap, outperforming baselines by 5.06% to 6.35%.

Conclusion: QA-Dragon effectively tackles complex VQA tasks through multimodal, multi-turn, and multi-hop reasoning.

Abstract: Retrieval-Augmented Generation (RAG) has been introduced to mitigate
hallucinations in Multimodal Large Language Models (MLLMs) by incorporating
external knowledge into the generation process, and it has become a widely
adopted approach for knowledge-intensive Visual Question Answering (VQA).
However, existing RAG methods typically retrieve from either text or images in
isolation, limiting their ability to address complex queries that require
multi-hop reasoning or up-to-date factual knowledge. To address this
limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to
identify the query's subject domain for domain-specific reasoning, along with a
search router that dynamically selects optimal retrieval strategies. By
orchestrating both text and image search agents in a hybrid setup, our system
supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle
complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM
Challenge at KDD Cup 2025, where it significantly enhances the reasoning
performance of base models under challenging scenarios. Our framework achieves
substantial improvements in both answer accuracy and knowledge overlap scores,
outperforming baselines by 5.06% on the single-source task, 6.35% on the
multi-source task, and 5.03% on the multi-turn task.

</details>


### [15] [An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication](https://arxiv.org/abs/2508.05267)
*Vítor N. Lourenço,Mohnish Dubey,Yunfei Bai,Audrey Depeige,Vivek Jain*

Main category: cs.AI

TL;DR: A framework combining RDF graph databases and LLMs improves expert identification and communication in large-scale maintenance organizations by processing natural language queries with transparent reasoning.


<details>
  <summary>Details</summary>
Motivation: Traditional communication methods struggle with information overload and slow response times in complex organizational structures.

Method: Uses RDF graph databases and LLMs for natural language query processing, supported by a planning-orchestration architecture for explainable results.

Result: Enables precise audience targeting and efficient communication, maintaining system trust.

Conclusion: The proposed framework effectively addresses communication challenges in large-scale maintenance organizations.

Abstract: In large-scale maintenance organizations, identifying subject matter experts
and managing communications across complex entities relationships poses
significant challenges -- including information overload and longer response
times -- that traditional communication approaches fail to address effectively.
We propose a novel framework that combines RDF graph databases with LLMs to
process natural language queries for precise audience targeting, while
providing transparent reasoning through a planning-orchestration architecture.
Our solution enables communication owners to formulate intuitive queries
combining concepts such as equipment, manufacturers, maintenance engineers, and
facilities, delivering explainable results that maintain trust in the system
while improving communication efficiency across the organization.

</details>


### [16] [A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents](https://arxiv.org/abs/2508.05311)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: A hybrid architecture combining decision trees and LLMs improves reasoning benchmarks and applications like clinical decision support.


<details>
  <summary>Details</summary>
Motivation: To integrate interpretable symbolic reasoning with generative LLMs for robust and general-purpose neuro-symbolic reasoning.

Method: Embeds decision trees and random forests as callable oracles within a multi-agent framework, with LLMs handling abductive reasoning and planning.

Result: Achieves +7.2% entailment consistency on ProofWriter, +5.3% accuracy on GSM8k, and +6.0% abstraction accuracy on ARC.

Conclusion: The architecture provides an interpretable, extensible solution for neuro-symbolic reasoning.

Abstract: We propose a hybrid architecture that integrates decision tree-based symbolic
reasoning with the generative capabilities of large language models (LLMs)
within a coordinated multi-agent framework. Unlike prior approaches that
loosely couple symbolic and neural modules, our design embeds decision trees
and random forests as callable oracles within a unified reasoning system.
Tree-based modules enable interpretable rule inference and causal logic, while
LLM agents handle abductive reasoning, generalization, and interactive
planning. A central orchestrator maintains belief state consistency and
mediates communication across agents and external tools, enabling reasoning
over both structured and unstructured inputs.
  The system achieves strong performance on reasoning benchmarks. On
\textit{ProofWriter}, it improves entailment consistency by +7.2\% through
logic-grounded tree validation. On GSM8k, it achieves +5.3\% accuracy gains in
multistep mathematical problems via symbolic augmentation. On \textit{ARC}, it
boosts abstraction accuracy by +6.0\% through integration of symbolic oracles.
Applications in clinical decision support and scientific discovery show how the
system encodes domain rules symbolically while leveraging LLMs for contextual
inference and hypothesis generation. This architecture offers a robust,
interpretable, and extensible solution for general-purpose neuro-symbolic
reasoning.

</details>


### [17] [The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition](https://arxiv.org/abs/2508.05338)
*Brinnae Bent*

Main category: cs.AI

TL;DR: The paper calls for redefining the term 'agent' in AI to address ambiguity, proposing a framework with clear criteria and multidimensional characterization.


<details>
  <summary>Details</summary>
Motivation: Ambiguity in the term 'agent' hinders research communication, evaluation, and policy development, necessitating a clearer definition.

Method: Historical analysis and contemporary usage patterns inform a framework defining minimum requirements and characterizing systems along multiple dimensions.

Result: A proposed framework clarifies the term 'agent' while preserving its multifaceted nature, aiding research and policy.

Conclusion: The paper recommends terminology standardization and framework adoption to improve clarity, reproducibility, and policy effectiveness.

Abstract: The term 'agent' in artificial intelligence has long carried multiple
interpretations across different subfields. Recent developments in AI
capabilities, particularly in large language model systems, have amplified this
ambiguity, creating significant challenges in research communication, system
evaluation and reproducibility, and policy development. This paper argues that
the term 'agent' requires redefinition. Drawing from historical analysis and
contemporary usage patterns, we propose a framework that defines clear minimum
requirements for a system to be considered an agent while characterizing
systems along a multidimensional spectrum of environmental interaction,
learning and adaptation, autonomy, goal complexity, and temporal coherence.
This approach provides precise vocabulary for system description while
preserving the term's historically multifaceted nature. After examining
potential counterarguments and implementation challenges, we provide specific
recommendations for moving forward as a field, including suggestions for
terminology standardization and framework adoption. The proposed approach
offers practical tools for improving research clarity and reproducibility while
supporting more effective policy development.

</details>


### [18] [NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making](https://arxiv.org/abs/2508.05344)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.AI

TL;DR: NomicLaw is a multi-agent simulation where LLMs engage in collaborative law-making, revealing their social reasoning and persuasive abilities in legal and ethical dilemmas.


<details>
  <summary>Details</summary>
Motivation: To empirically understand LLM behavior in open-ended, multi-agent settings, particularly in legal and ethical deliberation, which remains underexplored.

Method: NomicLaw uses structured simulations where LLMs propose rules, justify them, and vote on peer proposals. Voting patterns and strategic language are analyzed.

Result: LLMs form alliances, betray trust, and adapt rhetoric to influence decisions, showcasing latent social reasoning and persuasive capabilities.

Conclusion: The study highlights LLMs' potential for autonomous negotiation and legislation, offering insights for future AI system design.

Abstract: Recent advancements in large language models (LLMs) have extended their
capabilities from basic text processing to complex reasoning tasks, including
legal interpretation, argumentation, and strategic interaction. However,
empirical understanding of LLM behavior in open-ended, multi-agent settings
especially those involving deliberation over legal and ethical dilemmas remains
limited. We introduce NomicLaw, a structured multi-agent simulation where LLMs
engage in collaborative law-making, responding to complex legal vignettes by
proposing rules, justifying them, and voting on peer proposals. We
quantitatively measure trust and reciprocity via voting patterns and
qualitatively assess how agents use strategic language to justify proposals and
influence outcomes. Experiments involving homogeneous and heterogeneous LLM
groups demonstrate how agents spontaneously form alliances, betray trust, and
adapt their rhetoric to shape collective decisions. Our results highlight the
latent social reasoning and persuasive capabilities of ten open-source LLMs and
provide insights into the design of future AI systems capable of autonomous
negotiation, coordination and drafting legislation in legal settings.

</details>


### [19] [Minimal Model Reasoning in Description Logics: Don't Try This at Home!](https://arxiv.org/abs/2508.05350)
*Federica Di Stefano,Quentin Manière,Magdalena Ortiz,Mantas Šimkus*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reasoning with minimal models has always been at the core of many knowledge
representation techniques, but we still have only a limited understanding of
this problem in Description Logics (DLs). Minimization of some selected
predicates, letting the remaining predicates vary or be fixed, as proposed in
circumscription, has been explored and exhibits high complexity. The case of
`pure' minimal models, where the extension of all predicates must be minimal,
has remained largely uncharted. We address this problem in popular DLs and
obtain surprisingly negative results: concept satisfiability in minimal models
is undecidable already for $\mathcal{EL}$. This undecidability also extends to
a very restricted fragment of tuple-generating dependencies. To regain
decidability, we impose acyclicity conditions on the TBox that bring the
worst-case complexity below double exponential time and allow us to establish a
connection with the recently studied pointwise circumscription; we also derive
results in data complexity. We conclude with a brief excursion to the DL-Lite
family, where a positive result was known for DL-Lite$_{\text{core}}$, but our
investigation establishes ExpSpace-hardness already for its extension
DL-Lite$_{\text{horn}}$.

</details>


### [20] [StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models](https://arxiv.org/abs/2508.05383)
*Xiangxiang Zhang,Jingxuan Wei,Donghong Zhong,Qi Chen,Caijun Jia,Cheng Tan,Jinming Gu,Xiaobo Qin,Zhiping Liu,Liang Hu,Tong Sun,Yuchen Wu,Zewei Sun,Chenwei Lou,Hua Zheng,Tianyang Zhan,Changbao Wang,Shuangzhi Wu,Zefa Lin,Chang Guo,Sihang Yuan,Riwei Chen,Shixiong Zhao,Yingping Zhang,Gaowei Wu,Bihui Yu,Jiahui Wu,Zhehui Zhao,Qianqian Liu,Ruofeng Tang,Xingyue Huang,Bing Zhao,Mengyang Zhang,Youqiang Zhou*

Main category: cs.AI

TL;DR: StructVRM introduces fine-grained, verifiable rewards for multimodal models, improving performance on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with partial correctness in multi-question reasoning tasks due to coarse binary rewards.

Method: StructVRM uses a model-based verifier for sub-question-level feedback, assessing semantic and mathematical equivalence.

Result: Seed-StructVRM achieves state-of-the-art performance on six benchmarks and a new STEM-Bench.

Conclusion: Structured, verifiable rewards effectively advance multimodal models in complex reasoning domains.

Abstract: Existing Vision-Language Models often struggle with complex, multi-question
reasoning tasks where partial correctness is crucial for effective learning.
Traditional reward mechanisms, which provide a single binary score for an
entire response, are too coarse to guide models through intricate problems with
multiple sub-parts. To address this, we introduce StructVRM, a method that
aligns multimodal reasoning with Structured and Verifiable Reward Models. At
its core is a model-based verifier trained to provide fine-grained,
sub-question-level feedback, assessing semantic and mathematical equivalence
rather than relying on rigid string matching. This allows for nuanced, partial
credit scoring in previously intractable problem formats. Extensive experiments
demonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM,
achieves state-of-the-art performance on six out of twelve public multimodal
benchmarks and our newly curated, high-difficulty STEM-Bench. The success of
StructVRM validates that training with structured, verifiable rewards is a
highly effective approach for advancing the capabilities of multimodal models
in complex, real-world reasoning domains.

</details>


### [21] [An Explainable Machine Learning Framework for Railway Predictive Maintenance using Data Streams from the Metro Operator of Portugal](https://arxiv.org/abs/2508.05388)
*Silvia García-Méndez,Francisco de Arriba-Pérez,Fátima Leal,Bruno Veloso,Benedita Malheiro,Juan Carlos Burguillo-Rial*

Main category: cs.AI

TL;DR: A real-time predictive maintenance solution for Intelligent Transportation Systems using an online processing pipeline with pre-processing, ML classification, and explainability, achieving high accuracy and F-measure.


<details>
  <summary>Details</summary>
Motivation: To enhance railway predictive maintenance by enabling real-time fault prediction with explainability, improving service availability and safety.

Method: Implements an online processing pipeline with sample pre-processing, incremental ML classification, and explainability modules.

Result: Achieves over 98% F-measure and 99% accuracy on the MetroPT dataset, handling class imbalance and noise effectively.

Conclusion: The pipeline is methodologically sound and practically applicable for proactive maintenance in railway operations, enabling swift decision-making.

Abstract: This work contributes to a real-time data-driven predictive maintenance
solution for Intelligent Transportation Systems. The proposed method implements
a processing pipeline comprised of sample pre-processing, incremental
classification with Machine Learning models, and outcome explanation. This
novel online processing pipeline has two main highlights: (i) a dedicated
sample pre-processing module, which builds statistical and frequency-related
features on the fly, and (ii) an explainability module. This work is the first
to perform online fault prediction with natural language and visual
explainability. The experiments were performed with the MetroPT data set from
the metro operator of Porto, Portugal. The results are above 98 % for F-measure
and 99 % for accuracy. In the context of railway predictive maintenance,
achieving these high values is crucial due to the practical and operational
implications of accurate failure prediction. In the specific case of a high
F-measure, this ensures that the system maintains an optimal balance between
detecting the highest possible number of real faults and minimizing false
alarms, which is crucial for maximizing service availability. Furthermore, the
accuracy obtained enables reliability, directly impacting cost reduction and
increased safety. The analysis demonstrates that the pipeline maintains high
performance even in the presence of class imbalance and noise, and its
explanations effectively reflect the decision-making process. These findings
validate the methodological soundness of the approach and confirm its practical
applicability for supporting proactive maintenance decisions in real-world
railway operations. Therefore, by identifying the early signs of failure, this
pipeline enables decision-makers to understand the underlying problems and act
accordingly swiftly.

</details>


### [22] [DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning](https://arxiv.org/abs/2508.05405)
*Xinrun Xu,Pi Bu,Ye Wang,Börje F. Karlsson,Ziming Wang,Tengtao Song,Qi Zhu,Jun Song,Zhiming Ding,Bo Zheng*

Main category: cs.AI

TL;DR: DeepPHY is a benchmark framework to evaluate VLMs' understanding of physical principles, revealing their struggles in predictive control despite strong perceptual abilities.


<details>
  <summary>Details</summary>
Motivation: VLMs lack attention to detail and precise action planning in dynamic environments, necessitating a systematic evaluation of their physical reasoning.

Method: DeepPHY uses simulated environments with varying difficulty and fine-grained metrics to assess VLMs' physical reasoning.

Result: State-of-the-art VLMs fail to translate descriptive physical knowledge into precise predictive control.

Conclusion: DeepPHY highlights VLMs' limitations in physical reasoning, suggesting a need for improved models for complex tasks.

Abstract: Although Vision Language Models (VLMs) exhibit strong perceptual abilities
and impressive visual reasoning, they struggle with attention to detail and
precise action planning in complex, dynamic environments, leading to subpar
performance. Real-world tasks typically require complex interactions, advanced
spatial reasoning, long-term planning, and continuous strategy refinement,
usually necessitating understanding the physics rules of the target scenario.
However, evaluating these capabilities in real-world scenarios is often
prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel
benchmark framework designed to systematically evaluate VLMs' understanding and
reasoning about fundamental physical principles through a series of challenging
simulated environments. DeepPHY integrates multiple physical reasoning
environments of varying difficulty levels and incorporates fine-grained
evaluation metrics. Our evaluation finds that even state-of-the-art VLMs
struggle to translate descriptive physical knowledge into precise, predictive
control.

</details>


### [23] [Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation](https://arxiv.org/abs/2508.05427)
*Kartar Kumar Lohana Tharwani,Rajesh Kumar,Sumita,Numan Ahmed,Yong Tang*

Main category: cs.AI

TL;DR: LLMs are transforming organic synthesis by proposing routes, predicting outcomes, and automating experiments, aided by integration with other technologies. Challenges include biases and safety, but community efforts aim to democratize access.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs are revolutionizing organic synthesis by automating tasks and integrating with other technologies, while addressing challenges like bias and safety.

Method: Survey of milestones in LLM applications, coupling with graph neural networks, quantum calculations, and spectroscopy, alongside community initiatives like open benchmarks and federated learning.

Result: LLMs enable faster, greener, data-driven chemistry but face limitations like biased datasets and opaque reasoning.

Conclusion: LLMs, combined with AI and automation, promise rapid, reliable molecular innovation, with human oversight ensured through community efforts.

Abstract: Large language models (LLMs) are beginning to reshape how chemists plan and
run reactions in organic synthesis. Trained on millions of reported
transformations, these text-based models can propose synthetic routes, forecast
reaction outcomes and even instruct robots that execute experiments without
human supervision. Here we survey the milestones that turned LLMs from
speculative tools into practical lab partners. We show how coupling LLMs with
graph neural networks, quantum calculations and real-time spectroscopy shrinks
discovery cycles and supports greener, data-driven chemistry. We discuss
limitations, including biased datasets, opaque reasoning and the need for
safety gates that prevent unintentional hazards. Finally, we outline community
initiatives open benchmarks, federated learning and explainable interfaces that
aim to democratize access while keeping humans firmly in control. These
advances chart a path towards rapid, reliable and inclusive molecular
innovation powered by artificial intelligence and automation.

</details>


### [24] [Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI](https://arxiv.org/abs/2508.05432)
*Krzysztof Janowicz,Zilong Liu,Gengchen Mai,Zhangyu Wang,Ivan Majic,Alexandra Fortacz,Grant McKenzie,Song Gao*

Main category: cs.AI

TL;DR: The paper discusses the geographic variability in AI alignment, emphasizing the need for context-aware approaches due to cultural, political, and legal differences across regions.


<details>
  <summary>Details</summary>
Motivation: Current AI alignment methods often ignore geographic and cultural differences, leading to inappropriate or biased outcomes. The paper aims to address this gap.

Method: The paper reviews geographic research problems, suggests future work topics, and outlines methods for assessing alignment sensitivity.

Result: AI alignment must account for spatio-temporal context to avoid globally inconsistent or biased outputs, as demonstrated by examples like text-to-image models and Google Maps.

Conclusion: The paper calls for urgent development of spatio-temporally aware alignment methods to ensure AI systems respect regional norms and realities.

Abstract: AI (super) alignment describes the challenge of ensuring (future) AI systems
behave in accordance with societal norms and goals. While a quickly evolving
literature is addressing biases and inequalities, the geographic variability of
alignment remains underexplored. Simply put, what is considered appropriate,
truthful, or legal can differ widely across regions due to cultural norms,
political realities, and legislation. Alignment measures applied to AI/ML
workflows can sometimes produce outcomes that diverge from statistical
realities, such as text-to-image models depicting balanced gender ratios in
company leadership despite existing imbalances. Crucially, some model outputs
are globally acceptable, while others, e.g., questions about Kashmir, depend on
knowing the user's location and their context. This geographic sensitivity is
not new. For instance, Google Maps renders Kashmir's borders differently based
on user location. What is new is the unprecedented scale and automation with
which AI now mediates knowledge, expresses opinions, and represents geographic
reality to millions of users worldwide, often with little transparency about
how context is managed. As we approach Agentic AI, the need for
spatio-temporally aware alignment, rather than one-size-fits-all approaches, is
increasingly urgent. This paper reviews key geographic research problems,
suggests topics for future work, and outlines methods for assessing alignment
sensitivity.

</details>


### [25] [Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?](https://arxiv.org/abs/2508.05464)
*Matteo Prandi,Vincenzo Suriani,Federico Pierucci,Marcello Galisai,Daniele Nardi,Piercosma Bisconti*

Main category: cs.AI

TL;DR: The paper introduces Bench-2-CoP, a framework to evaluate AI benchmarks against the EU AI Act's requirements, revealing significant gaps in assessing systemic risks like loss-of-control scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the lack of alignment between current AI benchmarks and regulatory needs, particularly systemic risks highlighted by the EU AI Act.

Method: Uses LLM-as-judge analysis to map 194,955 benchmark questions against the EU AI Act's taxonomy of capabilities and propensities.

Result: Finds major misalignment: benchmarks focus on behavioral propensities (e.g., hallucination, bias) but neglect critical functional capabilities (e.g., loss-of-control scenarios).

Conclusion: Highlights the need for policymakers and developers to refine evaluation tools to address systemic risks and ensure safer, compliant AI.

Abstract: The rapid advancement of General Purpose AI (GPAI) models necessitates robust
evaluation frameworks, especially with emerging regulations like the EU AI Act
and its associated Code of Practice (CoP). Current AI evaluation practices
depend heavily on established benchmarks, but these tools were not designed to
measure the systemic risks that are the focus of the new regulatory landscape.
This research addresses the urgent need to quantify this "benchmark-regulation
gap." We introduce Bench-2-CoP, a novel, systematic framework that uses
validated LLM-as-judge analysis to map the coverage of 194,955 questions from
widely-used benchmarks against the EU AI Act's taxonomy of model capabilities
and propensities. Our findings reveal a profound misalignment: the evaluation
ecosystem is overwhelmingly focused on a narrow set of behavioral propensities,
such as "Tendency to hallucinate" (53.7% of the corpus) and "Discriminatory
bias" (28.9%), while critical functional capabilities are dangerously
neglected. Crucially, capabilities central to loss-of-control scenarios,
including evading human oversight, self-replication, and autonomous AI
development, receive zero coverage in the entire benchmark corpus. This
translates to a near-total evaluation gap for systemic risks like "Loss of
Control" (0.4% coverage) and "Cyber Offence" (0.8% coverage). This study
provides the first comprehensive, quantitative analysis of this gap, offering
critical insights for policymakers to refine the CoP and for developers to
build the next generation of evaluation tools, ultimately fostering safer and
more compliant AI.

</details>


### [26] [Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?](https://arxiv.org/abs/2508.05474)
*Burak Can Kaplan,Hugo Cesar De Castro Carneiro,Stefan Wermter*

Main category: cs.AI

TL;DR: A small, efficient LLM is used to generate diverse ERC datasets, improving classifier robustness and performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address scarcity and bias in ERC data by leveraging LLMs for synthetic dataset generation.

Method: Employ a resource-efficient LLM to create six diverse ERC datasets, supplementing existing benchmarks.

Result: Generated datasets enhance ERC classifier robustness and yield significant performance improvements.

Conclusion: Synthetic datasets effectively address ERC data limitations and improve model performance.

Abstract: Emotion recognition in conversations (ERC) focuses on identifying emotion
shifts within interactions, representing a significant step toward advancing
machine intelligence. However, ERC data remains scarce, and existing datasets
face numerous challenges due to their highly biased sources and the inherent
subjectivity of soft labels. Even though Large Language Models (LLMs) have
demonstrated their quality in many affective tasks, they are typically
expensive to train, and their application to ERC tasks--particularly in data
generation--remains limited. To address these challenges, we employ a small,
resource-efficient, and general-purpose LLM to synthesize ERC datasets with
diverse properties, supplementing the three most widely used ERC benchmarks. We
generate six novel datasets, with two tailored to enhance each benchmark. We
evaluate the utility of these datasets to (1) supplement existing datasets for
ERC classification, and (2) analyze the effects of label imbalance in ERC. Our
experimental results indicate that ERC classifier models trained on the
generated datasets exhibit strong robustness and consistently achieve
statistically significant performance improvements on existing ERC benchmarks.

</details>


### [27] [InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities](https://arxiv.org/abs/2508.05496)
*Shuo Cai,Su Lu,Qi Zhou,Kejing Yang,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: InfiAlign is a scalable post-training framework combining SFT and DPO to enhance LLM reasoning, using efficient data selection to reduce resource needs while improving performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLM reasoning capabilities post-training is resource-intensive; InfiAlign aims to improve scalability and sample efficiency.

Method: Integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) and uses a data selection pipeline for high-quality alignment.

Result: Achieves performance comparable to larger models with 12% of training data, with notable gains in mathematical reasoning (3.89% improvement on AIME benchmarks).

Conclusion: InfiAlign offers a scalable, data-efficient solution for aligning large reasoning models, demonstrating strong generalization across tasks.

Abstract: Large language models (LLMs) have exhibited impressive reasoning abilities on
a wide range of complex tasks. However, enhancing these capabilities through
post-training remains resource intensive, particularly in terms of data and
computational cost. Although recent efforts have sought to improve sample
efficiency through selective data curation, existing methods often rely on
heuristic or task-specific strategies that hinder scalability. In this work, we
introduce InfiAlign, a scalable and sample-efficient post-training framework
that integrates supervised fine-tuning (SFT) with Direct Preference
Optimization (DPO) to align LLMs for enhanced reasoning. At the core of
InfiAlign is a robust data selection pipeline that automatically curates
high-quality alignment data from open-source reasoning datasets using
multidimensional quality metrics. This pipeline enables significant performance
gains while drastically reducing data requirements and remains extensible to
new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model
achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only
approximately 12% of the training data, and demonstrates strong generalization
across diverse reasoning tasks. Additional improvements are obtained through
the application of DPO, with particularly notable gains in mathematical
reasoning tasks. The model achieves an average improvement of 3.89% on AIME
24/25 benchmarks. Our results highlight the effectiveness of combining
principled data selection with full-stage post-training, offering a practical
solution for aligning large reasoning models in a scalable and data-efficient
manner. The model checkpoints are available at
https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.

</details>


### [28] [GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning](https://arxiv.org/abs/2508.05498)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.AI

TL;DR: GRAIL is a framework combining LLMs with graph retrieval for structured knowledge, improving accuracy and F1 scores by 21.01% and 22.43% respectively.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods struggle with structured knowledge like graphs, facing precision and redundancy issues.

Method: GRAIL uses LLM-guided exploration, path filtering, and a two-stage training process for dynamic reasoning.

Result: Achieves 21.01% accuracy and 22.43% F1 improvement on knowledge graph QA datasets.

Conclusion: GRAIL effectively balances precision and conciseness in graph retrieval, enhancing reasoning performance.

Abstract: Large Language Models (LLMs) integrated with Retrieval-Augmented Generation
(RAG) techniques have exhibited remarkable performance across a wide range of
domains. However, existing RAG approaches primarily operate on unstructured
data and demonstrate limited capability in handling structured knowledge such
as knowledge graphs. Meanwhile, current graph retrieval methods fundamentally
struggle to capture holistic graph structures while simultaneously facing
precision control challenges that manifest as either critical information gaps
or excessive redundant connections, collectively undermining reasoning
performance. To address this challenge, we propose GRAIL: Graph-Retrieval
Augmented Interactive Learning, a framework designed to interact with
large-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL
integrates LLM-guided random exploration with path filtering to establish a
data synthesis pipeline, where a fine-grained reasoning trajectory is
automatically generated for each task. Based on the synthesized data, we then
employ a two-stage training process to learn a policy that dynamically decides
the optimal actions at each reasoning step. The overall objective of
precision-conciseness balance in graph retrieval is decoupled into fine-grained
process-supervised rewards to enhance data efficiency and training stability.
In practical deployment, GRAIL adopts an interactive retrieval paradigm,
enabling the model to autonomously explore graph paths while dynamically
balancing retrieval breadth and precision. Extensive experiments have shown
that GRAIL achieves an average accuracy improvement of 21.01% and F1
improvement of 22.43% on three knowledge graph question-answering datasets. Our
source code and datasets is available at https://github.com/Changgeww/GRAIL.

</details>


### [29] [Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation](https://arxiv.org/abs/2508.05508)
*Roshita Bhonsle,Rishav Dutta,Sneha Vavilapalli,Harsh Seth,Abubakarr Jaye,Yapei Chang,Mukund Rungta,Emmanuel Aboah Boateng,Sadid Hasan,Ehi Nosakhare,Soundar Srinivasan*

Main category: cs.AI

TL;DR: A generalizable, modular framework is proposed for evaluating agent task completion, focusing on step-by-step reasoning and outperforming existing methods like LLM-as-a-Judge.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods overlook step-by-step reasoning and are domain-specific, limiting their applicability.

Method: The framework decomposes tasks into sub-tasks, validates each step, and aggregates results for a final verdict.

Result: The framework achieves 4.76% and 10.52% higher alignment accuracy with human evaluations compared to GPT-4o.

Conclusion: The proposed framework shows promise for general-purpose agent evaluation.

Abstract: The increasing adoption of foundation models as agents across diverse domains
necessitates a robust evaluation framework. Current methods, such as
LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step
reasoning that drives agentic decision-making. Meanwhile, existing
Agent-as-a-Judge systems, where one agent evaluates another's task completion,
are typically designed for narrow, domain-specific settings. To address this
gap, we propose a generalizable, modular framework for evaluating agent task
completion independent of the task domain. The framework emulates human-like
evaluation by decomposing tasks into sub-tasks and validating each step using
available information, such as the agent's output and reasoning. Each module
contributes to a specific aspect of the evaluation process, and their outputs
are aggregated to produce a final verdict on task completion. We validate our
framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA
and BigCodeBench. Our Judge Agent predicts task success with closer agreement
to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,
respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This
demonstrates the potential of our proposed general-purpose evaluation
framework.

</details>


### [30] [Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program](https://arxiv.org/abs/2508.05513)
*Meryem Yilmaz Soylu,Adrian Gallard,Jeonghyun Lee,Gayane Grigoryan,Rushil Desai,Stephen Harmon*

Main category: cs.AI

TL;DR: LORI is an AI tool using NLP and large language models (RoBERTa, LLAMA) to analyze leadership skills in letters of recommendation, achieving high accuracy (F1: 91.6%).


<details>
  <summary>Details</summary>
Motivation: Manual review of LORs is time-consuming; LORI automates leadership skill assessment to streamline admissions and enhance candidate evaluation.

Method: Uses NLP and large language models (RoBERTa, LLAMA) to detect leadership attributes like teamwork, communication, and innovation.

Result: RoBERTa model achieves 91.6% F1, 92.4% precision, and 91.6% recall, demonstrating strong consistency.

Conclusion: LORI improves graduate admissions by automating leadership skill assessment, ensuring comprehensive candidate evaluation.

Abstract: Letters of recommendation (LORs) provide valuable insights into candidates'
capabilities and experiences beyond standardized test scores. However,
reviewing these text-heavy materials is time-consuming and labor-intensive. To
address this challenge and support the admission committee in providing
feedback for students' professional growth, our study introduces LORI: LOR
Insights, a novel AI-based detection tool for assessing leadership skills in
LORs submitted by online master's program applicants. By employing natural
language processing and leveraging large language models using RoBERTa and
LLAMA, we seek to identify leadership attributes such as teamwork,
communication, and innovation. Our latest RoBERTa model achieves a weighted F1
score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong
level of consistency in our test data. With the growing importance of
leadership skills in the STEM sector, integrating LORI into the graduate
admissions process is crucial for accurately assessing applicants' leadership
capabilities. This approach not only streamlines the admissions process but
also automates and ensures a more comprehensive evaluation of candidates'
capabilities.

</details>


### [31] [MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media](https://arxiv.org/abs/2508.05557)
*Rui Lu,Jinhe Bi,Yunpu Ma,Feng Xiao,Yuntao Du,Yijun Tian*

Main category: cs.AI

TL;DR: MV-Debate, a multi-view agent debate framework, improves harmful content detection on social media by combining diverse interpretive perspectives and dynamic reflection.


<details>
  <summary>Details</summary>
Motivation: Social media's multimodal nature makes harmful intent (e.g., sarcasm, hate speech) hard to detect due to cross-modal contradictions and cultural shifts.

Method: MV-Debate uses four debate agents (surface analyst, deep reasoner, modality contrast, social contextualist) for iterative debate and reflection under a reflection-gain criterion.

Result: Outperforms single-model and multi-agent baselines on three benchmark datasets.

Conclusion: Multi-agent debate shows promise for reliable social intent detection in safety-critical online contexts.

Abstract: Social media has evolved into a complex multimodal environment where text,
images, and other signals interact to shape nuanced meanings, often concealing
harmful intent. Identifying such intent, whether sarcasm, hate speech, or
misinformation, remains challenging due to cross-modal contradictions, rapid
cultural shifts, and subtle pragmatic cues. To address these challenges, we
propose MV-Debate, a multi-view agent debate framework with dynamic reflection
gating for unified multimodal harmful content detection. MV-Debate assembles
four complementary debate agents, a surface analyst, a deep reasoner, a
modality contrast, and a social contextualist, to analyze content from diverse
interpretive perspectives. Through iterative debate and reflection, the agents
refine responses under a reflection-gain criterion, ensuring both accuracy and
efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate
significantly outperforms strong single-model and existing multi-agent debate
baselines. This work highlights the promise of multi-agent debate in advancing
reliable social intent detection in safety-critical online contexts.

</details>


### [32] [The Missing Reward: Active Inference in the Era of Experience](https://arxiv.org/abs/2508.05619)
*Bo Wen*

Main category: cs.AI

TL;DR: Active Inference (AIF) can bridge the grounded-agency gap in AI by replacing external rewards with intrinsic free energy minimization, enabling autonomous learning and alignment with human values.


<details>
  <summary>Details</summary>
Motivation: Current AI systems rely heavily on human-engineered rewards and face scalability issues, hindering progress toward autonomous intelligence.

Method: Proposes integrating AIF with Large Language Models to create agents that learn from self-generated data while minimizing free energy.

Result: AIF offers a unified Bayesian framework for balancing exploration and exploitation, aligning AI with human values.

Conclusion: AIF provides a scalable and principled approach to autonomous AI, addressing the grounded-agency gap and reducing reliance on human reward engineering.

Abstract: This paper argues that Active Inference (AIF) provides a crucial foundation
for developing autonomous AI agents capable of learning from experience without
continuous human reward engineering. As AI systems begin to exhaust
high-quality training data and rely on increasingly large human workforces for
reward design, the current paradigm faces significant scalability challenges
that could impede progress toward genuinely autonomous intelligence. The
proposal for an ``Era of Experience,'' where agents learn from self-generated
data, is a promising step forward. However, this vision still depends on
extensive human engineering of reward functions, effectively shifting the
bottleneck from data curation to reward curation. This highlights what we
identify as the \textbf{grounded-agency gap}: the inability of contemporary AI
systems to autonomously formulate, adapt, and pursue objectives in response to
changing circumstances. We propose that AIF can bridge this gap by replacing
external reward signals with an intrinsic drive to minimize free energy,
allowing agents to naturally balance exploration and exploitation through a
unified Bayesian objective. By integrating Large Language Models as generative
world models with AIF's principled decision-making framework, we can create
agents that learn efficiently from experience while remaining aligned with
human values. This synthesis offers a compelling path toward AI systems that
can develop autonomously while adhering to both computational and physical
constraints.

</details>


### [33] [Simulating Human-Like Learning Dynamics with LLM-Empowered Agents](https://arxiv.org/abs/2508.05622)
*Yu Yuan,Lili Zhao,Wei Chen,Guangting Zheng,Kai Zhang,Mengdi Zhang,Qi Liu*

Main category: cs.AI

TL;DR: LearnerAgent, a multi-agent framework using LLMs, simulates human learning dynamics with psychologically grounded profiles, revealing insights into cognitive growth, learner behavior, and LLM limitations.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of capturing learning dynamics, tracking progress, and providing explainability in human learning behavior research.

Method: Introduces LearnerAgent, a multi-agent framework with psychologically grounded learner profiles (Deep, Surface, Lazy, General) and tracks learning progress through weekly knowledge acquisition, monthly strategic choices, tests, and peer interaction.

Result: 1) Deep Learner shows sustained cognitive growth; Surface Learner's shallow knowledge is diagnosed. 2) Learner behaviors align with profiles. 3) Self-concept evolves realistically. 4) Base LLM defaults to a 'diligent but brittle Surface Learner.'

Conclusion: LearnerAgent effectively simulates real learning scenarios, providing deeper insights into LLM behavior and human-like learning dynamics.

Abstract: Capturing human learning behavior based on deep learning methods has become a
major research focus in both psychology and intelligent systems. Recent
approaches rely on controlled experiments or rule-based models to explore
cognitive processes. However, they struggle to capture learning dynamics, track
progress over time, or provide explainability. To address these challenges, we
introduce LearnerAgent, a novel multi-agent framework based on Large Language
Models (LLMs) to simulate a realistic teaching environment. To explore
human-like learning dynamics, we construct learners with psychologically
grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free
General Learner to inspect the base LLM's default behavior. Through weekly
knowledge acquisition, monthly strategic choices, periodic tests, and peer
interaction, we can track the dynamic learning progress of individual learners
over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis
reveals that only Deep Learner achieves sustained cognitive growth. Our
specially designed "trap questions" effectively diagnose Surface Learner's
shallow knowledge. 2) The behavioral and cognitive patterns of distinct
learners align closely with their psychological profiles. 3) Learners'
self-concept scores evolve realistically, with the General Learner developing
surprisingly high self-efficacy despite its cognitive limitations. 4)
Critically, the default profile of base LLM is a "diligent but brittle Surface
Learner"-an agent that mimics the behaviors of a good student but lacks true,
generalizable understanding. Extensive simulation experiments demonstrate that
LearnerAgent aligns well with real scenarios, yielding more insightful findings
about LLMs' behavior.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [NAEx: A Plug-and-Play Framework for Explaining Network Alignment](https://arxiv.org/abs/2508.04731)
*Shruti Saxena,Arijit Khan,Joydeep Chandra*

Main category: cs.LG

TL;DR: NAEx is a model-agnostic framework for explaining network alignment models by identifying influential subgraphs and features, ensuring interpretability and trust.


<details>
  <summary>Details</summary>
Motivation: Limited interpretability of network alignment models hinders trust, especially in high-stakes domains.

Method: NAEx jointly parameterizes graph structures and feature spaces using learnable masks and optimizes for faithful explanations.

Result: NAEx effectively explains alignment decisions and integrates with four NA models, demonstrating efficiency on benchmarks.

Conclusion: NAEx enhances interpretability and trust in network alignment models through faithful and efficient explanations.

Abstract: Network alignment (NA) identifies corresponding nodes across multiple
networks, with applications in domains like social networks, co-authorship, and
biology. Despite advances in alignment models, their interpretability remains
limited, making it difficult to understand alignment decisions and posing
challenges in building trust, particularly in high-stakes domains. To address
this, we introduce NAEx, a plug-and-play, model-agnostic framework that
explains alignment models by identifying key subgraphs and features influencing
predictions. NAEx addresses the key challenge of preserving the joint
cross-network dependencies on alignment decisions by: (1) jointly
parameterizing graph structures and feature spaces through learnable edge and
feature masks, and (2) introducing an optimization objective that ensures
explanations are both faithful to the original predictions and enable
meaningful comparisons of structural and feature-based similarities between
networks. NAEx is an inductive framework that efficiently generates NA
explanations for previously unseen data. We introduce evaluation metrics
tailored to alignment explainability and demonstrate NAEx's effectiveness and
efficiency on benchmark datasets by integrating it with four representative NA
models.

</details>


### [35] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: LumiGen is an LVLM-enhanced iterative framework improving T2I generation by integrating fine-grained control and semantic consistency through prompt enhancement and visual feedback.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models struggle with complex instructions, fine-grained control, and semantic consistency, prompting the need for an enhanced framework.

Method: LumiGen uses an Intelligent Prompt Parsing & Augmentation (IPPA) module and an Iterative Visual Feedback & Refinement (IVFR) module for closed-loop optimization.

Result: LumiGen achieves a superior score of 3.08 on LongBench-T2I, excelling in text rendering and pose expression.

Conclusion: LVLM integration in LumiGen significantly improves T2I generation quality and controllability.

Abstract: Text-to-Image (T2I) generation has made significant advancements with
diffusion models, yet challenges persist in handling complex instructions,
ensuring fine-grained content control, and maintaining deep semantic
consistency. Existing T2I models often struggle with tasks like accurate text
rendering, precise pose generation, or intricate compositional coherence.
Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in cross-modal understanding and instruction following. We propose
LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I
model performance, particularly in areas requiring fine-grained control,
through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an
Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt
enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which
acts as a "visual critic" to iteratively correct and optimize generated images.
Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a
superior average score of 3.08, outperforming state-of-the-art baselines.
Notably, our framework demonstrates significant improvements in critical
dimensions such as text rendering and pose expression, validating the
effectiveness of LVLM integration for more controllable and higher-quality
image generation.

</details>


### [36] [MissMecha: An All-in-One Python Package for Studying Missing Data Mechanisms](https://arxiv.org/abs/2508.04740)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: MissMecha is a Python toolkit for simulating, visualizing, and evaluating missing data under MCAR, MAR, and MNAR assumptions, supporting both numerical and categorical features.


<details>
  <summary>Details</summary>
Motivation: Incomplete data is a common issue in real-world datasets, but existing tools are limited in handling heterogeneous tabular data and missing mechanisms.

Method: MissMecha provides a unified platform with features like visual diagnostics, MCAR testing, and type-aware imputation evaluation metrics.

Result: The toolkit supports mechanism-aware studies and offers a comprehensive solution for data quality research, benchmarking, and education.

Conclusion: MissMecha addresses the limitations of existing tools by providing a versatile and open-source solution for working with incomplete data.

Abstract: Incomplete data is a persistent challenge in real-world datasets, often
governed by complex and unobservable missing mechanisms. Simulating missingness
has become a standard approach for understanding its impact on learning and
analysis. However, existing tools are fragmented, mechanism-limited, and
typically focus only on numerical variables, overlooking the heterogeneous
nature of real-world tabular data. We present MissMecha, an open-source Python
toolkit for simulating, visualizing, and evaluating missing data under MCAR,
MAR, and MNAR assumptions. MissMecha supports both numerical and categorical
features, enabling mechanism-aware studies across mixed-type tabular datasets.
It includes visual diagnostics, MCAR testing utilities, and type-aware
imputation evaluation metrics. Designed to support data quality research,
benchmarking, and education,MissMecha offers a unified platform for researchers
and practitioners working with incomplete data.

</details>


### [37] [MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling](https://arxiv.org/abs/2508.05492)
*Jifan Gao,Mahmudur Rahman,John Caskey,Madeline Oguss,Ann O'Rourke,Randy Brown,Anne Stey,Anoop Mayampurath,Matthew M. Churpek,Guanhua Chen,Majid Afshar*

Main category: cs.LG

TL;DR: MoMA is a novel architecture using multiple LLM agents to integrate multimodal EHR data for clinical predictions, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Multimodal EHR data offers richer insights but integrating diverse modalities for clinical predictions is challenging due to high data requirements.

Method: MoMA uses specialist LLM agents to convert non-textual data into textual summaries, an aggregator agent to unify them, and a predictor agent for clinical predictions.

Result: MoMA outperforms current methods in accuracy and flexibility across three real-world prediction tasks.

Conclusion: MoMA effectively integrates multimodal EHR data, enhancing clinical prediction accuracy and adaptability.

Abstract: Multimodal electronic health record (EHR) data provide richer, complementary
insights into patient health compared to single-modality data. However,
effectively integrating diverse data modalities for clinical prediction
modeling remains challenging due to the substantial data requirements. We
introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed
to leverage multiple large language model (LLM) agents for clinical prediction
tasks using multimodal EHR data. MoMA employs specialized LLM agents
("specialist agents") to convert non-textual modalities, such as medical images
and laboratory results, into structured textual summaries. These summaries,
together with clinical notes, are combined by another LLM ("aggregator agent")
to generate a unified multimodal summary, which is then used by a third LLM
("predictor agent") to produce clinical predictions. Evaluating MoMA on three
prediction tasks using real-world datasets with different modality combinations
and prediction settings, MoMA outperforms current state-of-the-art methods,
highlighting its enhanced accuracy and flexibility across various tasks.

</details>


### [38] [Edge-Assisted Collaborative Fine-Tuning for Multi-User Personalized Artificial Intelligence Generated Content (AIGC)](https://arxiv.org/abs/2508.04745)
*Nan Li,Wanting Yang,Marie Siew,Zehui Xiong,Binbin Chen,Shiwen Mao,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: A novel cluster-aware hierarchical federated aggregation framework is proposed to enhance personalized content generation in edge-AIGC, addressing privacy, efficiency, and scalability challenges.


<details>
  <summary>Details</summary>
Motivation: Existing edge-AIGC applications lack efficiency and scalability, and cloud-based solutions fail to address privacy risks and communication costs in multi-user scenarios.

Method: The framework uses Low-Rank Adaptation (LoRA) for local fine-tuning, clusters clients by task similarity, and employs intra-cluster aggregation and inter-cluster knowledge interaction for hybrid content generation.

Result: The framework accelerates convergence and maintains viability for scalable multi-user personalized AIGC services under edge constraints.

Conclusion: The proposed solution effectively balances personalization, privacy, and efficiency in edge-AIGC scenarios.

Abstract: Diffusion models (DMs) have emerged as powerful tools for high-quality
content generation, yet their intensive computational requirements for
inference pose challenges for resource-constrained edge devices. Cloud-based
solutions aid in computation but often fall short in addressing privacy risks,
personalization efficiency, and communication costs in multi-user edge-AIGC
scenarios. To bridge this gap, we first analyze existing edge-AIGC applications
in personalized content synthesis, revealing their limitations in efficiency
and scalability. We then propose a novel cluster-aware hierarchical federated
aggregation framework. Based on parameter-efficient local fine-tuning via
Low-Rank Adaptation (LoRA), the framework first clusters clients based on the
similarity of their uploaded task requirements, followed by an intra-cluster
aggregation for enhanced personalization at the server-side. Subsequently, an
inter-cluster knowledge interaction paradigm is implemented to enable
hybrid-style content generation across diverse clusters.Building upon federated
learning (FL) collaboration, our framework simultaneously trains personalized
models for individual users at the devices and a shared global model enhanced
with multiple LoRA adapters on the server,enabling efficient edge inference;
meanwhile, all prompts for clustering and inference are encoded prior to
transmission, thereby further mitigating the risk of plaintext leakage. Our
evaluations demonstrate that the framework achieves accelerated convergence
while maintaining practical viability for scalable multi-user personalized AIGC
services under edge constraints.

</details>


### [39] [A Foundational Multi-Modal Model for Few-Shot Learning](https://arxiv.org/abs/2508.04746)
*Pengtao Dang,Tingbo Guo,Sha Cao,Chi Zhang*

Main category: cs.LG

TL;DR: The paper introduces a Large Multi-Modal Model (LMMM) framework (M3F) and a dataset (M3FD) to improve few-shot learning (FSL) in data-scarce scientific domains, outperforming traditional meta-learning methods.


<details>
  <summary>Details</summary>
Motivation: FSL is essential in fields with limited data, but existing methods struggle with generalization. This work aims to leverage multi-modal data and LMMMs to enhance FSL performance.

Method: The authors created M3FD, a diverse dataset with 10K+ few-shot samples, and developed M3F, a modular LMMM framework for FSL. The model is fine-tuned on M3FD to improve generalization.

Result: M3F outperforms conventional meta-learning methods on FSL tasks, demonstrating the feasibility of LMMMs for real-world scientific applications.

Conclusion: The M3F framework and M3FD dataset provide a scalable, unified solution for FSL in data-scarce domains, lowering barriers to adoption and promoting reproducibility.

Abstract: Few-shot learning (FSL) is a machine learning paradigm that aims to
generalize models from a small number of labeled examples, typically fewer than
10 per class. FSL is particularly crucial in biomedical, environmental,
materials, and mechanical sciences, where samples are limited and data
collection is often prohibitively costly, time-consuming, or ethically
constrained. In this study, we present an innovative approach to FSL by
demonstrating that a Large Multi-Modal Model (LMMM), trained on a set of
independent tasks spanning diverse domains, task types, and input modalities,
can substantially improve the generalization of FSL models, outperforming
models based on conventional meta-learning on tasks of the same type. To
support this, we first constructed a Multi-Modal Model Few-shot Dataset (M3FD,
over 10K+ few-shot samples), which includes 2D RGB images, 2D/3D medical scans,
tabular and time-course datasets, from which we manually curated FSL tasks such
as classification. We further introduced M3F (Multi-Modal Model for Few-shot
learning framework), a novel Large Multi-Modal Model framework tailored for
data-constrained scientific applications. M3F supports a wide range of
scientific data types through a modular pipeline. By fine-tuning the model on
M3FD, M3F improves model performance, making LMMM feasible for real-world FSL
deployment. The source code is located at https://github.com/ptdang1001/M3F. To
democratize access to complex FSL data and promote reproducibility for public
usage, M3FD is paired with a flexible and user-friendly tool that enables
efficient querying, task-specific sampling, and preprocessing. Together, our
dataset and framework offer a unified, scalable solution that significantly
lowers the barrier to applying LMMMs in data-scarce scientific domains.

</details>


### [40] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: AttriLens-Mol is a reinforcement learning framework for molecular property prediction with LLMs, improving performance and interpretability by guiding reasoning with attribute-based rewards.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for molecular property prediction rely on human-crafted prompts and verbose reasoning, lacking relevance. AttriLens-Mol aims to enhance reasoning by eliciting relevant molecular attributes.

Method: The framework uses three rewards: format (structured output), count (avoid irrelevant attributes), and rationality (verified relatedness). It trains 7B-size models on 4,000 samples.

Result: AttriLens-Mol outperforms supervised fine-tuning and advanced models (GPT-3.5, GPT-4o, etc.) on in-distribution and out-of-distribution datasets. Extracted attributes also improve interpretable decision tree models.

Conclusion: AttriLens-Mol effectively elicits relevant molecular attributes, enhancing both performance and interpretability in property prediction.

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [41] [PA-RNet: Perturbation-Aware Reasoning Network for Multimodal Time Series Forecasting](https://arxiv.org/abs/2508.04750)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.LG

TL;DR: PA-RNet is a robust multimodal forecasting framework addressing textual noise in time series data, featuring perturbation-aware modules and cross-modal attention for improved performance.


<details>
  <summary>Details</summary>
Motivation: Textual data in multimodal time series often contains noise or ambiguity, degrading model performance, which existing methods overlook.

Method: PA-RNet uses a perturbation-aware projection module and cross-modal attention to filter noise while preserving semantic meaning, supported by theoretical guarantees.

Result: PA-RNet outperforms state-of-the-art baselines across diverse domains and temporal settings.

Conclusion: PA-RNet effectively handles textual noise, enhancing robustness and generalization in multimodal time series forecasting.

Abstract: In real-world applications, multimodal time series data often suffer from
interference, especially in the textual modality. Existing methods for
multimodal time series forecasting often neglect the inherent perturbations
within textual data, where irrelevant, noisy, or ambiguous content can
significantly degrade model performance, particularly when the noise exhibits
varying intensity or stems from structural inconsistencies. To address this
challenge, we propose PA-RNet (Perturbation-Aware Reasoning Network for
Multimodal Time Series Forecasting), a robust multimodal forecasting framework.
PA-RNet features a perturbation-aware projection module and a cross-modal
attention mechanism to effectively separate noise from the textual embeddings
while maintaining semantically meaningful representations, thereby enhancing
the model's generalization ability. Theoretically, we establish the Lipschitz
continuity of PA-RNet with respect to textual inputs and prove that the
proposed perturbation module can reduce expected prediction error, offering
strong guarantees of stability under noisy conditions. Furthermore, we
introduce a textual perturbation pipeline that can be seamlessly incorporated
into existing multimodal time series forecasting tasks, allowing for systematic
evaluation of the model's robustness in the presence of varying levels of
textual noise. Extensive experiments across diverse domains and temporal
settings demonstrate that PA-RNet consistently outperforms state-of-the-art
baselines.

</details>


### [42] [InfoQ: Mixed-Precision Quantization via Global Information Flow](https://arxiv.org/abs/2508.04753)
*Mehmet Emre Akbulut,Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Manuel Roveri*

Main category: cs.LG

TL;DR: InfoQ is a training-free framework for mixed-precision quantization (MPQ) that evaluates layer sensitivity by measuring mutual information changes, achieving better accuracy and efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MPQ methods rely on costly search algorithms or local heuristics, missing global quantization effects. InfoQ addresses this by focusing on layer impact on network-wide information flow.

Method: InfoQ quantizes layers at different bit-widths, measures mutual information changes in subsequent layers via a single forward pass, and formulates bit-width allocation as an integer linear programming problem.

Result: InfoQ improves accuracy by up to 1% for MobileNetV2 and ResNet18 on ImageNet at high compression rates (14X and 10.66X) and uses significantly less data than state-of-the-art methods.

Conclusion: InfoQ offers a superior trade-off between search time and accuracy, demonstrating the importance of global sensitivity metrics in MPQ.

Abstract: Mixed-precision quantization (MPQ) is crucial for deploying deep neural
networks on resource-constrained devices, but finding the optimal bit-width for
each layer represents a complex combinatorial optimization problem. Current
state-of-the-art methods rely on computationally expensive search algorithms or
local sensitivity heuristic proxies like the Hessian, which fail to capture the
cascading global effects of quantization error. In this work, we argue that the
quantization sensitivity of a layer should not be measured by its local
properties, but by its impact on the information flow throughout the entire
network. We introduce InfoQ, a novel framework for MPQ that is training-free in
the bit-width search phase. InfoQ assesses layer sensitivity by quantizing each
layer at different bit-widths and measuring, through a single forward pass, the
resulting change in mutual information in the subsequent layers. This
quantifies how much each layer quantization impacts the network information
flow. The resulting scores are used to formulate bit-width allocation as an
integer linear programming problem, which is solved efficiently to minimize
total sensitivity under a given budget (e.g., model size or BitOps). Our
retraining-free search phase provides a superior search-time/accuracy trade-off
(using two orders of magnitude less data compared to state-of-the-art methods
such as LIMPQ), while yielding up to a 1% accuracy improvement for MobileNetV2
and ResNet18 on ImageNet at high compression rates (14X and 10.66X).

</details>


### [43] [Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle](https://arxiv.org/abs/2508.04755)
*Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: LLMs show promise as zero-shot dynamic insulin dosing agents, matching or outperforming trained RL agents in stable cases, but exhibit limitations like aggressive dosing and reasoning errors, requiring careful integration into clinical workflows.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of LLMs in automating clinical decision-making for dynamic insulin dosing without extensive training, comparing their performance to trained RL agents.

Method: Open-source LLMs were tested as insulin dosing agents in a Type 1 diabetes simulator, using zero-shot inference and compared to small neural network-based RL agents (SRAs).

Result: LLMs achieved comparable or superior performance to SRAs in stable cohorts but showed issues like aggressive dosing and reasoning errors.

Conclusion: LLMs can be integrated into clinical workflows cautiously, with targeted prompt engineering and hybrid approaches combining linguistic reasoning and physiological modeling.

Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold
promise for automating complex clinical decision-making, yet their practical
deployment remains hindered by the intensive engineering required to inject
clinical knowledge and ensure patient safety. Recent advancements in large
language models (LLMs) suggest a complementary approach, where implicit prior
knowledge and clinical heuristics are naturally embedded through linguistic
prompts without requiring environment-specific training. In this study, we
rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in
silico Type 1 diabetes simulator, comparing their zero-shot inference
performance against small neural network-based RL agents (SRAs) explicitly
trained for the task. Our results indicate that carefully designed zero-shot
prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or
superior clinical performance relative to extensively trained SRAs,
particularly in stable patient cohorts. However, LLMs exhibit notable
limitations, such as overly aggressive insulin dosing when prompted with
chain-of-thought (CoT) reasoning, highlighting critical failure modes including
arithmetic hallucination, temporal misinterpretation, and inconsistent clinical
logic. Incorporating explicit reasoning about latent clinical states (e.g.,
meals) yielded minimal performance gains, underscoring the current model's
limitations in capturing complex, hidden physiological dynamics solely through
textual inference. Our findings advocate for cautious yet optimistic
integration of LLMs into clinical workflows, emphasising the necessity of
targeted prompt engineering, careful validation, and potentially hybrid
approaches that combine linguistic reasoning with structured physiological
modelling to achieve safe, robust, and clinically effective decision-support
systems.

</details>


### [44] [Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration](https://arxiv.org/abs/2508.04780)
*Lin Jiang,Dahai Yu,Rongchao Xu,Tian Tang,Guang Wang*

Main category: cs.LG

TL;DR: EPOPR is a predict-then-optimize framework for equitable power restoration, reducing outage duration by 3.60% and inequity by 14.19%.


<details>
  <summary>Details</summary>
Motivation: Current power restoration methods are inequitable, as disadvantaged communities submit fewer requests, leading to prolonged outages.

Method: EPOPR combines Equity-Conformalized Quantile Regression for repair prediction and Spatial-Temporal Attentional RL for equitable decision-making.

Result: EPOPR reduces average outage duration by 3.60% and inequity by 14.19%.

Conclusion: EPOPR successfully balances efficiency and equity in power restoration.

Abstract: The increasing frequency of extreme weather events, such as hurricanes,
highlights the urgent need for efficient and equitable power system
restoration. Many electricity providers make restoration decisions primarily
based on the volume of power restoration requests from each region. However,
our data-driven analysis reveals significant disparities in request submission
volume, as disadvantaged communities tend to submit fewer restoration requests.
This disparity makes the current restoration solution inequitable, leaving
these communities vulnerable to extended power outages. To address this, we aim
to propose an equity-aware power restoration strategy that balances both
restoration efficiency and equity across communities. However, achieving this
goal is challenging for two reasons: the difficulty of predicting repair
durations under dataset heteroscedasticity, and the tendency of reinforcement
learning agents to favor low-uncertainty actions, which potentially undermine
equity. To overcome these challenges, we design a predict-then-optimize
framework called EPOPR with two key components: (1) Equity-Conformalized
Quantile Regression for uncertainty-aware repair duration prediction, and (2)
Spatial-Temporal Attentional RL that adapts to varying uncertainty levels
across regions for equitable decision-making. Experimental results show that
our EPOPR effectively reduces the average power outage duration by 3.60% and
decreases inequity between different communities by 14.19% compared to
state-of-the-art baselines.

</details>


### [45] [Federated Continual Recommendation](https://arxiv.org/abs/2508.04792)
*Jaehyung Lim,Wonbin Kweon,Woojoo Kim,Junyoung Kim,Seongjin Choi,Dongha Kim,Hwanjo Yu*

Main category: cs.LG

TL;DR: The paper introduces F3CRec, a framework combining Federated Learning (FL) and Continual Learning (CL) for privacy-preserving recommendations in non-stationary data streams.


<details>
  <summary>Details</summary>
Motivation: Address the gap between Federated Recommendation (FedRec) and Continual Learning Recommendation (CLRec) by integrating both to handle evolving user preferences while preserving privacy.

Method: Proposes F3CRec with Adaptive Replay Memory (client-side) and Item-wise Temporal Mean (server-side) to balance knowledge retention and adaptation.

Result: F3CRec outperforms existing methods in maintaining recommendation quality over time in a federated setting.

Conclusion: F3CRec effectively bridges FedRec and CLRec, offering a robust solution for privacy-preserving recommendations in dynamic environments.

Abstract: The increasing emphasis on privacy in recommendation systems has led to the
adoption of Federated Learning (FL) as a privacy-preserving solution, enabling
collaborative training without sharing user data. While Federated
Recommendation (FedRec) effectively protects privacy, existing methods struggle
with non-stationary data streams, failing to maintain consistent recommendation
quality over time. On the other hand, Continual Learning Recommendation (CLRec)
methods address evolving user preferences but typically assume centralized data
access, making them incompatible with FL constraints. To bridge this gap, we
introduce Federated Continual Recommendation (FCRec), a novel task that
integrates FedRec and CLRec, requiring models to learn from streaming data
while preserving privacy. As a solution, we propose F3CRec, a framework
designed to balance knowledge retention and adaptation under the strict
constraints of FCRec. F3CRec introduces two key components: Adaptive Replay
Memory on the client side, which selectively retains past preferences based on
user-specific shifts, and Item-wise Temporal Mean on the server side, which
integrates new knowledge while preserving prior information. Extensive
experiments demonstrate that F3CRec outperforms existing approaches in
maintaining recommendation quality over time in a federated environment.

</details>


### [46] [HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing](https://arxiv.org/abs/2508.04811)
*Lin Jiang,Yu Yang,Guang Wang*

Main category: cs.LG

TL;DR: HCRide, a human-centered ride-hailing system, balances passenger fairness and driver preference while maintaining system efficiency using a novel multi-agent reinforcement learning algorithm (Habic).


<details>
  <summary>Details</summary>
Motivation: Existing systems prioritize operator revenue, often neglecting passenger and driver satisfaction. This work aims to address this gap by designing a fair and preference-aware system.

Method: Developed HCRide with Habic, a multi-agent reinforcement learning algorithm featuring a competition mechanism, dynamic Actor network, and Bi-Critic network.

Result: HCRide improved system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% in real-world datasets.

Conclusion: HCRide successfully balances efficiency, fairness, and driver preference, demonstrating its effectiveness in ride-hailing services.

Abstract: Order dispatch systems play a vital role in ride-hailing services, which
directly influence operator revenue, driver profit, and passenger experience.
Most existing work focuses on improving system efficiency in terms of operator
revenue, which may cause a bad experience for both passengers and drivers.
Hence, in this work, we aim to design a human-centered ride-hailing system by
considering both passenger fairness and driver preference without compromising
the overall system efficiency. However, it is nontrivial to achieve this target
due to the potential conflicts between passenger fairness and driver preference
since optimizing one may sacrifice the other. To address this challenge, we
design HCRide, a Human-Centered Ride-hailing system based on a novel
multi-agent reinforcement learning algorithm called Harmonization-oriented
Actor-Bi-Critic (Habic), which includes three major components (i.e., a
multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic
network) to optimize system efficiency and passenger fairness with driver
preference consideration. We extensively evaluate our HCRide using two
real-world ride-hailing datasets from Shenzhen and New York City. Experimental
results show our HCRide effectively improves system efficiency by 2.02%,
fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art
baselines.

</details>


### [47] [Unified Flow Matching for Long Horizon Event Forecasting](https://arxiv.org/abs/2508.04843)
*Xiao Shou*

Main category: cs.LG

TL;DR: Proposes a non-autoregressive flow matching framework for modeling long-horizon marked event sequences, improving accuracy and efficiency over autoregressive and diffusion-based methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and error accumulation of autoregressive models in long-range forecasting of marked event sequences.

Method: Introduces a unified flow matching framework for joint modeling of inter-event times and event types using continuous and discrete flow matching.

Result: Demonstrates significant improvements in accuracy and generation efficiency on six real-world benchmarks.

Conclusion: The framework effectively models long-horizon event sequences without sequential decoding, outperforming existing methods.

Abstract: Modeling long horizon marked event sequences is a fundamental challenge in
many real-world applications, including healthcare, finance, and user behavior
modeling. Existing neural temporal point process models are typically
autoregressive, predicting the next event one step at a time, which limits
their efficiency and leads to error accumulation in long-range forecasting. In
this work, we propose a unified flow matching framework for marked temporal
point processes that enables non-autoregressive, joint modeling of inter-event
times and event types, via continuous and discrete flow matching. By learning
continuous-time flows for both components, our method generates coherent long
horizon event trajectories without sequential decoding. We evaluate our model
on six real-world benchmarks and demonstrate significant improvements over
autoregressive and diffusion-based baselines in both accuracy and generation
efficiency.

</details>


### [48] [Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection](https://arxiv.org/abs/2508.04845)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: A multi-stage intrusion detection framework for CAN protocol combines unsupervised anomaly detection and supervised graph learning, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The CAN protocol lacks built-in security, making it vulnerable to cyber-attacks, necessitating robust intrusion detection.

Method: Uses a Variational Graph Autoencoder (VGAE) for anomaly detection and a Knowledge-Distilled Graph Attention Network (KD-GAT) for attack classification, encoding CAN traffic as graph sequences.

Result: Achieves 96% parameter reduction and 16.2% average F1-score improvement over existing methods, excelling on imbalanced datasets.

Conclusion: The framework is effective for securing CAN networks, offering competitive performance and efficiency.

Abstract: The Controller Area Network (CAN) protocol is a standard for in-vehicle
communication but remains susceptible to cyber-attacks due to its lack of
built-in security. This paper presents a multi-stage intrusion detection
framework leveraging unsupervised anomaly detection and supervised graph
learning tailored for automotive CAN traffic. Our architecture combines a
Variational Graph Autoencoder (VGAE) for structural anomaly detection with a
Knowledge-Distilled Graph Attention Network (KD-GAT) for robust attack
classification. CAN bus activity is encoded as graph sequences to model
temporal and relational dependencies. The pipeline applies VGAE-based selective
undersampling to address class imbalance, followed by GAT classification with
optional score-level fusion. The compact student GAT achieves 96% parameter
reduction compared to the teacher model while maintaining strong predictive
performance. Experiments on six public CAN intrusion datasets--Car-Hacking,
Car-Survival, and can-train-and-test--demonstrate competitive accuracy and
efficiency, with average improvements of 16.2% in F1-score over existing
methods, particularly excelling on highly imbalanced datasets with up to 55%
F1-score improvements.

</details>


### [49] [Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos](https://arxiv.org/abs/2508.04853)
*Haoyu Zhang,Shihao Zhang,Ian Colbert,Rayan Saab*

Main category: cs.LG

TL;DR: The paper provides the first quantitative error bounds for OPTQ (GPTQ) and Qronos PTQ algorithms, analyzing their iterative procedures and offering theoretical justification for practical design choices.


<details>
  <summary>Details</summary>
Motivation: Despite OPTQ's widespread use in PTQ for deep neural networks, it lacks rigorous theoretical guarantees. This paper aims to fill that gap by deriving error bounds for OPTQ and Qronos.

Method: The authors analyze OPTQ's iterative quantization process, derive non-asymptotic 2-norm error bounds, and extend the analysis to Qronos, including stochastic variants.

Result: Theoretical bounds are established for OPTQ and Qronos, justifying heuristic design choices and providing guidance for parameter selection. Stronger infinity-norm bounds are derived for stochastic variants.

Conclusion: The paper bridges the gap between theory and practice in PTQ, offering insights into OPTQ and Qronos, and supporting their empirical advantages with rigorous analysis.

Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the
memory and compute costs of modern deep neural networks, including large
language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as
GPTQ-has emerged as a leading method due to its computational efficiency and
strong empirical performance. Despite its widespread adoption, however, OPTQ
lacks rigorous quantitative theoretical guarantees. This paper presents the
first quantitative error bounds for both deterministic and stochastic variants
of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ
algorithm. We analyze how OPTQ's iterative procedure induces quantization error
and derive non-asymptotic 2-norm error bounds that depend explicitly on the
calibration data and a regularization parameter that OPTQ uses. Our analysis
provides theoretical justification for several practical design choices,
including the widely used heuristic of ordering features by decreasing norm, as
well as guidance for selecting the regularization parameter. For the stochastic
variant, we establish stronger infinity-norm error bounds, which enable control
over the required quantization alphabet and are particularly useful for
downstream layers and nonlinearities. Finally, we extend our analysis to
Qronos, providing new theoretical bounds, for both its deterministic and
stochastic variants, that help explain its empirical advantages.

</details>


### [50] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: Agnostics is a language-agnostic post-training pipeline for LLMs that improves performance on low-resource languages by judging code behavior externally, eliminating per-language engineering.


<details>
  <summary>Details</summary>
Motivation: Low-resource languages lack sufficient pre-training data and require extensive per-language post-training setups, hindering LLM performance.

Method: Agnostics uses an LLM to rewrite unit tests into I/O format, configures a verifier for compilation/execution, and applies RL with verifiable rewards.

Result: Agnostics improves Qwen-3 4B to rival larger models, scales to diverse model families, and sets new SOTA pass@1 results on MultiPL-E and LiveCodeBench.

Conclusion: Agnostics simplifies RL post-training for any language, with released datasets and configurations for easy adoption.

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>


### [51] [Hilbert Neural Operator: Operator Learning in the Analytic Signal Domain](https://arxiv.org/abs/2508.04882)
*Saman Pordanesh,Pejman Shahsavari,Hossein Ghadjari*

Main category: cs.LG

TL;DR: The paper introduces the Hilbert Neural Operator (HNO), a new neural operator architecture that leverages the Hilbert transform to enhance learning of PDE solution operators by explicitly incorporating instantaneous amplitude and phase information.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators like FNO rely on Fourier transforms, which assume periodicity and lack phase-sensitive features. HNO aims to address these limitations by integrating signal processing insights.

Method: HNO maps input signals to their analytic representation via the Hilbert transform, making amplitude and phase explicit. It then applies spectral convolutions to this transformed representation.

Result: HNO is hypothesized to outperform existing methods for causal, phase-sensitive, and non-stationary systems, though empirical validation is implied but not detailed here.

Conclusion: HNO offers a theoretically grounded alternative to FNO, with potential advantages in handling non-periodic and phase-sensitive PDE problems.

Abstract: Neural operators have emerged as a powerful, data-driven paradigm for
learning solution operators of partial differential equations (PDEs).
State-of-the-art architectures, such as the Fourier Neural Operator (FNO), have
achieved remarkable success by performing convolutions in the frequency domain,
making them highly effective for a wide range of problems. However, this method
has some limitations, including the periodicity assumption of the Fourier
transform. In addition, there are other methods of analysing a signal, beyond
phase and amplitude perspective, and provide us with other useful information
to learn an effective network. We introduce the \textbf{Hilbert Neural Operator
(HNO)}, a new neural operator architecture to address some advantages by
incorporating a strong inductive bias from signal processing. HNO operates by
first mapping the input signal to its analytic representation via the Hilbert
transform, thereby making instantaneous amplitude and phase information
explicit features for the learning process. The core learnable operation -- a
spectral convolution -- is then applied to this Hilbert-transformed
representation. We hypothesize that this architecture enables HNO to model
operators more effectively for causal, phase-sensitive, and non-stationary
systems. We formalize the HNO architecture and provide the theoretical
motivation for its design, rooted in analytic signal theory.

</details>


### [52] [Gaussian mixture layers for neural networks](https://arxiv.org/abs/2508.04883)
*Sinho Chewi,Philippe Rigollet,Yuling Yan*

Main category: cs.LG

TL;DR: The paper explores implementing dynamics directly over probability measures in neural networks, using Gaussian mixture models and Wasserstein gradient flows to create a new Gaussian mixture (GM) layer.


<details>
  <summary>Details</summary>
Motivation: To investigate whether dynamics can be directly implemented over probability measures, advancing beyond the mean-field theory for neural networks.

Method: Uses Gaussian mixture models and Wasserstein gradient flows to derive training dynamics, introducing GM layers into neural architectures.

Result: GM layers achieve comparable test performance to traditional two-layer networks and exhibit distinct behavior even in the mean-field regime.

Conclusion: The GM layer offers a novel approach with unique dynamics, validated through experiments, expanding the toolkit for neural network design.

Abstract: The mean-field theory for two-layer neural networks considers infinitely wide
networks that are linearly parameterized by a probability measure over the
parameter space. This nonparametric perspective has significantly advanced both
the theoretical and conceptual understanding of neural networks, with
substantial efforts made to validate its applicability to networks of moderate
width. In this work, we explore the opposite direction, investigating whether
dynamics can be directly implemented over probability measures. Specifically,
we employ Gaussian mixture models as a flexible and expressive parametric
family of distributions together with the theory of Wasserstein gradient flows
to derive training dynamics for such measures. Our approach introduces a new
type of layer -- the Gaussian mixture (GM) layer -- that can be integrated into
neural network architectures. As a proof of concept, we validate our proposal
through experiments on simple classification tasks, where a GM layer achieves
test performance comparable to that of a two-layer fully connected network.
Furthermore, we examine the behavior of these dynamics and demonstrate
numerically that GM layers exhibit markedly different behavior compared to
classical fully connected layers, even when the latter are large enough to be
considered in the mean-field regime.

</details>


### [53] [Uncertainty Quantification for Surface Ozone Emulators using Deep Learning](https://arxiv.org/abs/2508.04885)
*Kelsey Doerksen,Yuliya Marchetti,Steven Lu,Kevin Bowman,James Montgomery,Kazuyuki Miyazaki,Yarin Gal,Freddie Kalaitzis*

Main category: cs.LG

TL;DR: A deep learning-based U-Net architecture is used to predict surface ozone residuals with uncertainty quantification, improving bias correction for air pollution models.


<details>
  <summary>Details</summary>
Motivation: Air pollution, particularly surface ozone, is a global health hazard, but traditional models lack interpretability and practicality for human-health scales.

Method: An uncertainty-aware U-Net with Bayesian and quantile regression methods predicts MOMO-Chem model's surface ozone residuals, tested in North America and Europe.

Result: The method effectively quantifies uncertainty and identifies optimal ground stations for bias correction, while evaluating land-use impact.

Conclusion: The approach enhances interpretability and decision-making for policy and public health measures in air pollution modeling.

Abstract: Air pollution is a global hazard, and as of 2023, 94\% of the world's
population is exposed to unsafe pollution levels. Surface Ozone (O3), an
important pollutant, and the drivers of its trends are difficult to model, and
traditional physics-based models fall short in their practical use for scales
relevant to human-health impacts. Deep Learning-based emulators have shown
promise in capturing complex climate patterns, but overall lack the
interpretability necessary to support critical decision making for policy
changes and public health measures. We implement an uncertainty-aware U-Net
architecture to predict the Multi-mOdel Multi-cOnstituent Chemical data
assimilation (MOMO-Chem) model's surface ozone residuals (bias) using Bayesian
and quantile regression methods. We demonstrate the capability of our
techniques in regional estimation of bias in North America and Europe for June
2019. We highlight the uncertainty quantification (UQ) scores between our two
UQ methodologies and discern which ground stations are optimal and sub-optimal
candidates for MOMO-Chem bias correction, and evaluate the impact of land-use
information in surface ozone residual modeling.

</details>


### [54] [Leveraging Deep Learning for Physical Model Bias of Global Air Quality Estimates](https://arxiv.org/abs/2508.04886)
*Kelsey Doerksen,Yuliya Marchetti,Kevin Bowman,Steven Lu,James Montgomery,Yarin Gal,Freddie Kalaitzis,Kazuyuki Miyazaki*

Main category: cs.LG

TL;DR: The paper addresses the challenge of modeling surface ozone, a key air pollutant, using a 2D CNN to estimate model bias, improving accuracy over traditional methods and aiding policy decisions.


<details>
  <summary>Details</summary>
Motivation: Air pollution causes millions of premature deaths, but modeling surface ozone at human-relevant scales remains difficult, limiting the use of physics-based models.

Method: A 2D Convolutional Neural Network (CNN) is used to estimate surface ozone model residuals (bias), incorporating high-resolution satellite land use data.

Result: The CNN outperforms traditional machine learning in capturing model residuals, particularly in North America and Europe, and improves understanding of ozone bias drivers.

Conclusion: The approach enhances scientific understanding of ozone bias at urban scales, offering potential for better environmental policy.

Abstract: Air pollution is the world's largest environmental risk factor for human
disease and premature death, resulting in more than 6 million permature deaths
in 2019. Currently, there is still a challenge to model one of the most
important air pollutants, surface ozone, particularly at scales relevant for
human health impacts, with the drivers of global ozone trends at these scales
largely unknown, limiting the practical use of physics-based models. We employ
a 2D Convolutional Neural Network based architecture that estimate surface
ozone MOMO-Chem model residuals, referred to as model bias. We demonstrate the
potential of this technique in North America and Europe, highlighting its
ability better to capture physical model residuals compared to a traditional
machine learning method. We assess the impact of incorporating land use
information from high-resolution satellite imagery to improve model estimates.
Importantly, we discuss how our results can improve our scientific
understanding of the factors impacting ozone bias at urban scales that can be
used to improve environmental policy.

</details>


### [55] [Retrieval-Augmented Water Level Forecasting for Everglades](https://arxiv.org/abs/2508.04888)
*Rahuul Rangaraj,Jimeng Shi,Rajendra Paudel,Giri Narasimhan,Yanzhao Wu*

Main category: cs.LG

TL;DR: The paper introduces Retrieval-Augmented Forecasting (RAF) for water level prediction in hydrology, improving accuracy by leveraging historical data without retraining.


<details>
  <summary>Details</summary>
Motivation: Accurate water level forecasting is vital for ecosystem management, but current deep learning models struggle with generalization and adaptation in hydrology.

Method: Proposes RAF, a framework that retrieves historically similar hydrological episodes to enhance forecasting. Compares similarity-based and mutual information-based RAF methods.

Result: RAF significantly improves water level forecasting accuracy in the Everglades.

Conclusion: RAF shows promise for hydrology and encourages adaptive AI adoption in ecosystem management.

Abstract: Accurate water level forecasting is crucial for managing ecosystems such as
the Everglades, a subtropical wetland vital for flood mitigation, drought
management, water resource planning, and biodiversity conservation. While
recent advances in deep learning, particularly time series foundation models,
have demonstrated success in general-domain forecasting, their application in
hydrology remains underexplored. Furthermore, they often struggle to generalize
across diverse unseen datasets and domains, due to the lack of effective
mechanisms for adaptation. To address this gap, we introduce
Retrieval-Augmented Forecasting (RAF) into the hydrology domain, proposing a
framework that retrieves historically analogous multivariate hydrological
episodes to enrich the model input before forecasting. By maintaining an
external archive of past observations, RAF identifies and incorporates relevant
patterns from historical data, thereby enhancing contextual awareness and
predictive accuracy without requiring the model for task-specific retraining or
fine-tuning. Furthermore, we explore and compare both similarity-based and
mutual information-based RAF methods. We conduct a comprehensive evaluation on
real-world data from the Everglades, demonstrating that the RAF framework
yields substantial improvements in water level forecasting accuracy. This study
highlights the potential of RAF approaches in environmental hydrology and paves
the way for broader adoption of adaptive AI methods by domain experts in
ecosystem management. The code and data are available at
https://github.com/rahuul2992000/WaterRAF.

</details>


### [56] [Honest and Reliable Evaluation and Expert Equivalence Testing of Automated Neonatal Seizure Detection](https://arxiv.org/abs/2508.04899)
*Jovana Kljajic,John M. O'Toole,Robert Hogan,Tamara Skoric*

Main category: cs.LG

TL;DR: The paper evaluates performance metrics for neonatal seizure detection models, proposing best practices to address biases and inconsistencies. It highlights the superiority of Matthews and Pearson's correlation coefficients under class imbalance and recommends specific reporting standards for clinical validation.


<details>
  <summary>Details</summary>
Motivation: Current evaluation practices for neonatal seizure detection models are inconsistent and biased, leading to unreliable AI performance claims. This study aims to improve metric reliability and comparability.

Method: The study assessed standard metrics, consensus strategies, and human-expert equivalence tests using real and synthetic seizure annotations, varying class imbalance, inter-rater agreement, and rater numbers.

Result: Matthews and Pearson's correlation coefficients performed best under class imbalance. Consensus strategies were sensitive to rater numbers and agreement levels. The multi-rater Turing test with Fleiss k effectively captured expert-level AI performance.

Conclusion: The study recommends reporting balanced metrics, sensitivity, specificity, PPV, NPV, and multi-rater Turing test results with Fleiss k on held-out validation sets to ensure reliable AI evaluation for clinical adoption.

Abstract: Reliable evaluation of machine learning models for neonatal seizure detection
is critical for clinical adoption. Current practices often rely on inconsistent
and biased metrics, hindering model comparability and interpretability.
Expert-level claims about AI performance are frequently made without rigorous
validation, raising concerns about their reliability. This study aims to
systematically evaluate common performance metrics and propose best practices
tailored to the specific challenges of neonatal seizure detection. Using real
and synthetic seizure annotations, we assessed standard performance metrics,
consensus strategies, and human-expert level equivalence tests under varying
class imbalance, inter-rater agreement, and number of raters. Matthews and
Pearson's correlation coefficients outperformed the area under the curve in
reflecting performance under class imbalance. Consensus types are sensitive to
the number of raters and agreement level among them. Among human-expert level
equivalence tests, the multi-rater Turing test using Fleiss k best captured
expert-level AI performance. We recommend reporting: (1) at least one balanced
metric, (2) Sensitivity, specificity, PPV and NPV, (3) Multi-rater Turing test
results using Fleiss k, and (4) All the above on held-out validation set. This
proposed framework provides an important prerequisite to clinical validation by
enabling a thorough and honest appraisal of AI methods for neonatal seizure
detection.

</details>


### [57] [Sensitivity of Stability: Theoretical & Empirical Analysis of Replicability for Adaptive Data Selection in Transfer Learning](https://arxiv.org/abs/2508.04901)
*Prabhav Singh,Jessica Sorrell*

Main category: cs.LG

TL;DR: The paper analyzes replicability in transfer learning, introducing a measure called selection sensitivity ($\Delta_Q$) to quantify the trade-off between adaptation effectiveness and consistency. It shows that replicability failure increases with sensitivity and decreases with sample size, validated through experiments on MultiNLI. Highly adaptive strategies perform better but are less replicable, while pretraining mitigates replicability issues.


<details>
  <summary>Details</summary>
Motivation: To understand the reliability of transfer learning adaptations, especially with dynamic data selection strategies, and to quantify the trade-off between performance and replicability.

Method: Theoretical framework introducing selection sensitivity ($\Delta_Q$) and empirical analysis on MultiNLI using six adaptive selection strategies.

Result: Highly adaptive strategies improve performance but increase replicability failure rates, while less adaptive methods keep failure rates low. Pretraining reduces failure rates by up to 30%.

Conclusion: The study provides guidelines for balancing performance and replicability in transfer learning and emphasizes the need for replicability-aware system design.

Abstract: The widespread adoption of transfer learning has revolutionized machine
learning by enabling efficient adaptation of pre-trained models to new domains.
However, the reliability of these adaptations remains poorly understood,
particularly when using adaptive data selection strategies that dynamically
prioritize training examples. We present a comprehensive theoretical and
empirical analysis of replicability in transfer learning, introducing a
mathematical framework that quantifies the fundamental trade-off between
adaptation effectiveness and result consistency. Our key contribution is the
formalization of selection sensitivity ($\Delta_Q$), a measure that captures
how adaptive selection strategies respond to perturbations in training data. We
prove that replicability failure probability: the likelihood that two
independent training runs produce models differing in performance by more than
a threshold, increases quadratically with selection sensitivity while
decreasing exponentially with sample size. Through extensive experiments on the
MultiNLI corpus using six adaptive selection strategies - ranging from uniform
sampling to gradient-based selection - we demonstrate that this theoretical
relationship holds precisely in practice. Our results reveal that highly
adaptive strategies like gradient-based and curriculum learning achieve
superior task performance but suffer from high replicability failure rates,
while less adaptive approaches maintain failure rates below 7%. Crucially, we
show that source domain pretraining provides a powerful mitigation mechanism,
reducing failure rates by up to 30% while preserving performance gains. These
findings establish principled guidelines for practitioners to navigate the
performance-replicability trade-off and highlight the need for
replicability-aware design in modern transfer learning systems.

</details>


### [58] [Advancing Hate Speech Detection with Transformers: Insights from the MetaHate](https://arxiv.org/abs/2508.04913)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: The paper explores transformer-based models for hate speech detection, achieving top performance with fine-tuned ELECTRA on the MetaHate dataset, while identifying challenges like sarcasm and coded language.


<details>
  <summary>Details</summary>
Motivation: Hate speech on social media has severe real-world impacts, necessitating robust automated detection methods. Existing deep learning approaches have limitations, prompting the exploration of transformer models.

Method: The study evaluates transformer models (BERT, RoBERTa, GPT-2, ELECTRA) on the MetaHate dataset (1.2M samples), with fine-tuning for hate speech detection.

Result: Fine-tuned ELECTRA achieved the highest F1 score (0.8980). Challenges include sarcasm, coded language, and label noise.

Conclusion: Transformer models, especially ELECTRA, are effective for hate speech detection, though challenges remain in handling nuanced language.

Abstract: Hate speech is a widespread and harmful form of online discourse,
encompassing slurs and defamatory posts that can have serious social,
psychological, and sometimes physical impacts on targeted individuals and
communities. As social media platforms such as X (formerly Twitter), Facebook,
Instagram, Reddit, and others continue to facilitate widespread communication,
they also become breeding grounds for hate speech, which has increasingly been
linked to real-world hate crimes. Addressing this issue requires the
development of robust automated methods to detect hate speech in diverse social
media environments. Deep learning approaches, such as vanilla recurrent neural
networks (RNNs), long short-term memory (LSTM), and convolutional neural
networks (CNNs), have achieved good results, but are often limited by issues
such as long-term dependencies and inefficient parallelization. This study
represents the comprehensive exploration of transformer-based models for hate
speech detection using the MetaHate dataset--a meta-collection of 36 datasets
with 1.2 million social media samples. We evaluate multiple state-of-the-art
transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with
fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We
also analyze classification errors, revealing challenges with sarcasm, coded
language, and label noise.

</details>


### [59] [ALScope: A Unified Toolkit for Deep Active Learning](https://arxiv.org/abs/2508.04937)
*Chenkai Wu,Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Gang Liu,Wray Buntine,Lan Du*

Main category: cs.LG

TL;DR: ALScope is a new Deep Active Learning (DAL) platform for classification tasks, integrating diverse datasets and algorithms to evaluate performance under varied conditions like distribution shifts and data imbalance.


<details>
  <summary>Details</summary>
Motivation: The lack of a unified platform for fair and systematic evaluation of DAL algorithms under diverse real-world challenges (e.g., distribution shifts, data imbalance) motivated the development of ALScope.

Method: ALScope integrates 10 datasets from CV and NLP, and 21 DAL algorithms, supporting flexible configuration of experimental factors like OOD sample ratio and class imbalance. Extensive experiments were conducted under various settings.

Result: Findings show DAL performance varies by domain and task, with room for improvement in non-standard scenarios (imbalanced, open-set). Some algorithms perform well but are time-consuming.

Conclusion: ALScope enables comprehensive DAL evaluation, highlighting performance variability and the need for further research in challenging scenarios.

Abstract: Deep Active Learning (DAL) reduces annotation costs by selecting the most
informative unlabeled samples during training. As real-world applications
become more complex, challenges stemming from distribution shifts (e.g.,
open-set recognition) and data imbalance have gained increasing attention,
prompting the development of numerous DAL algorithms. However, the lack of a
unified platform has hindered fair and systematic evaluation under diverse
conditions. Therefore, we present a new DAL platform ALScope for classification
tasks, integrating 10 datasets from computer vision (CV) and natural language
processing (NLP), and 21 representative DAL algorithms, including both
classical baselines and recent approaches designed to handle challenges such as
distribution shifts and data imbalance. This platform supports flexible
configuration of key experimental factors, ranging from algorithm and dataset
choices to task-specific factors like out-of-distribution (OOD) sample ratio,
and class imbalance ratio, enabling comprehensive and realistic evaluation. We
conduct extensive experiments on this platform under various settings. Our
findings show that: (1) DAL algorithms' performance varies significantly across
domains and task settings; (2) in non-standard scenarios such as imbalanced and
open-set settings, DAL algorithms show room for improvement and require further
investigation; and (3) some algorithms achieve good performance, but require
significantly longer selection time.

</details>


### [60] [REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2508.04946)
*Nameer Hirschkind,Joseph Liu,Mahesh Kumar Nandwana,Xiao Yu*

Main category: cs.LG

TL;DR: REINA, a novel loss function for Simultaneous Speech Translation (SimulST), optimizes the tradeoff between translation quality and latency by waiting for more input only when it provides useful information. It achieves SOTA results and improves efficiency by 21%.


<details>
  <summary>Details</summary>
Motivation: SimulST systems struggle with balancing translation quality and latency. The goal is to optimize this tradeoff by leveraging information theory principles.

Method: Introduces REINA, a loss function derived from information theory, to train an adaptive policy using a non-streaming translation model. Applied to French, Spanish, and German translations.

Result: Achieves state-of-the-art streaming results and improves the latency/quality tradeoff by up to 21% compared to prior methods.

Conclusion: REINA effectively optimizes SimulST systems, demonstrating significant improvements in both quality and latency.

Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while
simultaneously emitting translated text or speech. Such systems face the
significant challenge of balancing translation quality and latency. We
introduce a strategy to optimize this tradeoff: wait for more input only if you
gain information by doing so. Based on this strategy, we present Regularized
Entropy INformation Adaptation (REINA), a novel loss to train an adaptive
policy using an existing non-streaming translation model. We derive REINA from
information theory principles and show that REINA helps push the reported
Pareto frontier of the latency/quality tradeoff over prior works. Utilizing
REINA, we train a SimulST model on French, Spanish and German, both from and
into English. Training on only open source or synthetically generated data, we
achieve state-of-the-art (SOTA) streaming results for models of comparable
size. We also introduce a metric for streaming efficiency, quantitatively
showing REINA improves the latency/quality trade-off by as much as 21% compared
to prior approaches, normalized against non-streaming baseline BLEU scores.

</details>


### [61] [Self-Error Adjustment: Theory and Practice of Balancing Individual Performance and Diversity in Ensemble Learning](https://arxiv.org/abs/2508.04948)
*Rui Zou*

Main category: cs.LG

TL;DR: Proposes Self-Error Adjustment (SEA), a novel ensemble learning framework for precise control over accuracy-diversity trade-off, outperforming traditional methods like NCL.


<details>
  <summary>Details</summary>
Motivation: Traditional ensemble methods lack precise control over the accuracy-diversity trade-off, and NCL has loose theoretical bounds and limited adjustment range.

Method: SEA decomposes ensemble errors into individual performance and diversity terms, introducing an adjustable parameter for fine control.

Result: SEA outperforms baseline methods, offers broader adjustment range, and has tighter theoretical bounds.

Conclusion: SEA provides flexible adjustment and superior performance, validated by empirical results.

Abstract: Ensemble learning boosts performance by aggregating predictions from multiple
base learners. A core challenge is balancing individual learner accuracy with
diversity. Traditional methods like Bagging and Boosting promote diversity
through randomness but lack precise control over the accuracy-diversity
trade-off. Negative Correlation Learning (NCL) introduces a penalty to manage
this trade-off but suffers from loose theoretical bounds and limited adjustment
range. To overcome these limitations, we propose a novel framework called
Self-Error Adjustment (SEA), which decomposes ensemble errors into two distinct
components: individual performance terms, representing the self-error of each
base learner, and diversity terms, reflecting interactions among learners. This
decomposition allows us to introduce an adjustable parameter into the loss
function, offering precise control over the contribution of each component,
thus enabling finer regulation of ensemble performance. Compared to NCL and its
variants, SEA provides a broader range of effective adjustments and more
consistent changes in diversity. Furthermore, we establish tighter theoretical
bounds for adjustable ensemble methods and validate them through empirical
experiments. Experimental results on several public regression and
classification datasets demonstrate that SEA consistently outperforms baseline
methods across all tasks. Ablation studies confirm that SEA offers more
flexible adjustment capabilities and superior performance in fine-tuning
strategies.

</details>


### [62] [Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization](https://arxiv.org/abs/2508.04950)
*Wei Liu,Anweshit Panda,Ujwal Pandey,Christopher Brissette,Yikang Shen,George M. Slota,Naigang Wang,Jie Chen,Yangyang Xu*

Main category: cs.LG

TL;DR: The paper proposes two compressed decentralized algorithms for nonconvex stochastic optimization, combining momentum and message-compression techniques to achieve fast convergence and reduced communication costs. The methods are theoretically proven effective and outperform state-of-the-art approaches in training DNNs and Transformers.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of consensus error, compression error, and momentum gradient bias in decentralized optimization, while maintaining the benefits of momentum acceleration and compressed communication.

Method: Two algorithms are designed: (1) a compressed decentralized adaptive method for bounded gradients, and (2) a compressed decentralized heavy-ball method with gradient tracking for data heterogeneity. Both use momentum and compression techniques.

Result: Both methods achieve optimal convergence rates, linear speedup, and topology-independent parameters within error tolerance. They outperform existing methods in training DNNs and Transformers.

Conclusion: The proposed algorithms effectively combine momentum and compression, offering superior performance in decentralized nonconvex stochastic optimization.

Abstract: In this paper, we design two compressed decentralized algorithms for solving
nonconvex stochastic optimization under two different scenarios. Both
algorithms adopt a momentum technique to achieve fast convergence and a
message-compression technique to save communication costs. Though momentum
acceleration and compressed communication have been used in literature, it is
highly nontrivial to theoretically prove the effectiveness of their composition
in a decentralized algorithm that can maintain the benefits of both sides,
because of the need to simultaneously control the consensus error, the
compression error, and the bias from the momentum gradient.
  For the scenario where gradients are bounded, our proposal is a compressed
decentralized adaptive method. To the best of our knowledge, this is the first
decentralized adaptive stochastic gradient method with compressed
communication. For the scenario of data heterogeneity without bounded
gradients, our proposal is a compressed decentralized heavy-ball method, which
applies a gradient tracking technique to address the challenge of data
heterogeneity. Notably, both methods achieve an optimal convergence rate, and
they can achieve linear speed up and adopt topology-independent algorithmic
parameters within a certain regime of the user-specified error tolerance.
Superior empirical performance is observed over state-of-the-art methods on
training deep neural networks (DNNs) and Transformers.

</details>


### [63] [MENDR: Manifold Explainable Neural Data Representations](https://arxiv.org/abs/2508.04956)
*Matthew Chen,Micky Nnamdi,Justin Shao,Andrew Hornback,Hongyun Huang,Ben Tamo,Yishan Zhong,Benoit Marteau,Wenqi Shi,May Dongmei Wang*

Main category: cs.LG

TL;DR: MENDR, a novel EEG foundation model, enhances interpretability and performance using wavelet-based representations and Riemannian Manifold Transformer architecture.


<details>
  <summary>Details</summary>
Motivation: Current EEG foundation models lack transparency and overlook deterministic signal processing features, hindering clinical integration.

Method: MENDR uses wavelet packet transforms and a Riemannian Manifold Transformer to learn symmetric positive definite matrix embeddings from EEG data.

Result: MENDR achieves near state-of-the-art performance with fewer parameters and supports interpretable EEG signal reconstruction.

Conclusion: MENDR offers efficient, interpretable, and clinically viable EEG analysis, addressing transparency and performance gaps.

Abstract: Foundation models for electroencephalography (EEG) signals have recently
demonstrated success in learning generalized representations of EEGs,
outperforming specialized models in various downstream tasks. However, many of
these models lack transparency in their pretraining dynamics and offer limited
insight into how well EEG information is preserved within their embeddings. For
successful clinical integration, EEG foundation models must ensure transparency
in pretraining, downstream fine-tuning, and the interpretability of learned
representations. Current approaches primarily operate in the temporal domain,
overlooking advancements in digital signal processing that enable the
extraction of deterministic and traceable features, such as wavelet-based
representations. We propose MENDR (Manifold Explainable Neural Data
Representations), a filter bank-based EEG foundation model built on a novel
Riemannian Manifold Transformer architecture to resolve these issues. MENDR
learns symmetric positive definite matrix embeddings of EEG signals and is
pretrained on a large corpus comprising over 4,000 hours of EEG data,
decomposed via discrete wavelet packet transforms into multi-resolution
coefficients. MENDR significantly enhances interpretability by visualizing
symmetric positive definite embeddings as geometric ellipsoids and supports
accurate reconstruction of EEG signals from learned embeddings. Evaluations
across multiple clinical EEG tasks demonstrate that MENDR achieves near
state-of-the-art performance with substantially fewer parameters, underscoring
its potential for efficient, interpretable, and clinically applicable EEG
analysis.

</details>


### [64] [RCUKF: Data-Driven Modeling Meets Bayesian Estimation](https://arxiv.org/abs/2508.04985)
*Kumar Anurag,Kasra Azizi,Francesco Sorrentino,Wenbin Wan*

Main category: cs.LG

TL;DR: Proposes RCUKF, combining reservoir computing and unscented Kalman filtering for accurate modeling of complex systems.


<details>
  <summary>Details</summary>
Motivation: Challenges in obtaining reliable process models for complex systems.

Method: Integrates data-driven modeling (reservoir computing) with Bayesian estimation (unscented Kalman filter).

Result: Demonstrated effectiveness on benchmark problems and real-time vehicle trajectory estimation.

Conclusion: RCUKF provides a robust solution for modeling complex systems where traditional models fail.

Abstract: Accurate modeling is crucial in many engineering and scientific applications,
yet obtaining a reliable process model for complex systems is often
challenging. To address this challenge, we propose a novel framework, reservoir
computing with unscented Kalman filtering (RCUKF), which integrates data-driven
modeling via reservoir computing (RC) with Bayesian estimation through the
unscented Kalman filter (UKF). The RC component learns the nonlinear system
dynamics directly from data, serving as a surrogate process model in the UKF
prediction step to generate state estimates in high-dimensional or chaotic
regimes where nominal mathematical models may fail. Meanwhile, the UKF
measurement update integrates real-time sensor data to correct potential drift
in the data-driven model. We demonstrate RCUKF effectiveness on well-known
benchmark problems and a real-time vehicle trajectory estimation task in a
high-fidelity simulation environment.

</details>


### [65] [Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.04999)
*Menghua Jiang,Yuxia Lin,Baoliang Chen,Haifeng Hu,Yuncheng Jiang,Sijie Mai*

Main category: cs.LG

TL;DR: The paper proposes MMCI, a causal intervention model for multimodal sentiment analysis, to address spurious correlations and improve generalization by disentangling causal and shortcut features.


<details>
  <summary>Details</summary>
Motivation: Existing MSA methods suffer from spurious correlations, leading to reliance on statistical shortcuts rather than true causal relationships, which harms generalization.

Method: MMCI models multimodal inputs as a multi-relational graph, uses attention to disentangle causal and shortcut features, and applies backdoor adjustment to combine them dynamically.

Result: Experiments on standard MSA and OOD datasets show MMCI effectively suppresses biases and improves performance.

Conclusion: MMCI enhances generalization in MSA by addressing spurious correlations through causal intervention.

Abstract: Multimodal sentiment analysis (MSA) aims to understand human emotions by
integrating information from multiple modalities, such as text, audio, and
visual data. However, existing methods often suffer from spurious correlations
both within and across modalities, leading models to rely on statistical
shortcuts rather than true causal relationships, thereby undermining
generalization. To mitigate this issue, we propose a Multi-relational
Multimodal Causal Intervention (MMCI) model, which leverages the backdoor
adjustment from causal theory to address the confounding effects of such
shortcuts. Specifically, we first model the multimodal inputs as a
multi-relational graph to explicitly capture intra- and inter-modal
dependencies. Then, we apply an attention mechanism to separately estimate and
disentangle the causal features and shortcut features corresponding to these
intra- and inter-modal relations. Finally, by applying the backdoor adjustment,
we stratify the shortcut features and dynamically combine them with the causal
features to encourage MMCI to produce stable predictions under distribution
shifts. Extensive experiments on several standard MSA datasets and
out-of-distribution (OOD) test sets demonstrate that our method effectively
suppresses biases and improves performance.

</details>


### [66] [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004)
*Chengsong Huang,Wenhao Yu,Xiaoyang Wang,Hongming Zhang,Zongxia Li,Ruosen Li,Jiaxin Huang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: R-Zero is an autonomous framework for self-evolving LLMs, eliminating the need for human-curated data by having two models (Challenger and Solver) co-evolve through interaction, improving reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing LLM training relies on human-curated tasks, limiting progress toward super-intelligence. R-Zero aims to overcome this by enabling fully autonomous self-improvement.

Method: R-Zero uses two models (Challenger and Solver) that interact: the Challenger proposes tasks at the Solver's capability edge, and the Solver solves them, creating a self-improving curriculum.

Result: R-Zero boosts reasoning capabilities, e.g., improving Qwen3-4B-Base by +6.49 on math and +7.54 on general-domain benchmarks.

Conclusion: R-Zero demonstrates a scalable, autonomous approach for advancing LLMs beyond human-curated limitations, enhancing reasoning performance.

Abstract: Self-evolving Large Language Models (LLMs) offer a scalable path toward
super-intelligence by autonomously generating, refining, and learning from
their own experiences. However, existing methods for training such models still
rely heavily on vast human-curated tasks and labels, typically via fine-tuning
or reinforcement learning, which poses a fundamental bottleneck to advancing AI
systems toward capabilities beyond human intelligence. To overcome this
limitation, we introduce R-Zero, a fully autonomous framework that generates
its own training data from scratch. Starting from a single base LLM, R-Zero
initializes two independent models with distinct roles, a Challenger and a
Solver. These models are optimized separately and co-evolve through
interaction: the Challenger is rewarded for proposing tasks near the edge of
the Solver capability, and the Solver is rewarded for solving increasingly
challenging tasks posed by the Challenger. This process yields a targeted,
self-improving curriculum without any pre-existing tasks and labels.
Empirically, R-Zero substantially improves reasoning capability across
different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on
math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.

</details>


### [67] [SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models](https://arxiv.org/abs/2508.05015)
*Dai Do,Manh Nguyen,Svetha Venkatesh,Hung Le*

Main category: cs.LG

TL;DR: SPaRFT is a self-paced learning framework for LLMs that optimizes data selection and timing, achieving high accuracy with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Current methods for training LLMs require extensive data and compute, limiting scalability and generalizability for smaller models.

Method: SPaRFT uses cluster-based data reduction and a multi-armed bandit to optimize data selection and allocation based on model performance.

Result: SPaRFT matches or outperforms state-of-the-art baselines using up to 100x fewer samples.

Conclusion: Performance-driven training curricula can unlock strong reasoning in LLMs with minimal resources.

Abstract: Large language models (LLMs) have shown strong reasoning capabilities when
fine-tuned with reinforcement learning (RL). However, such methods require
extensive data and compute, making them impractical for smaller models. Current
approaches to curriculum learning or data selection are largely
heuristic-driven or demand extensive computational resources, limiting their
scalability and generalizability. We propose \textbf{SPaRFT}, a self-paced
learning framework that enables efficient learning based on the capability of
the model being trained through optimizing which data to use and when. First,
we apply \emph{cluster-based data reduction} to partition training data by
semantics and difficulty, extracting a compact yet diverse subset that reduces
redundancy. Then, a \emph{multi-armed bandit} treats data clusters as arms,
optimized to allocate training samples based on model current performance.
Experiments across multiple reasoning benchmarks show that SPaRFT achieves
comparable or better accuracy than state-of-the-art baselines while using up to
\(100\times\) fewer samples. Ablation studies and analyses further highlight
the importance of both data clustering and adaptive selection. Our results
demonstrate that carefully curated, performance-driven training curricula can
unlock strong reasoning abilities in LLMs with minimal resources.

</details>


### [68] [Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality](https://arxiv.org/abs/2508.05025)
*Zhehan Qu,Tianyi Hu,Christian Fronk,Maria Gorlatova*

Main category: cs.LG

TL;DR: The paper explores how AR systems in CPR training can compromise situational awareness (SA) due to cognitive tunneling. It introduces an AR app for CPR feedback, evaluates SA using eye tracking, and proposes FixGraphPool, a graph neural network, to predict SA with high accuracy.


<details>
  <summary>Details</summary>
Motivation: AR systems enhance task performance but risk reducing situational awareness in safety-critical scenarios like CPR, where responders must stay vigilant to unpredictable hazards.

Method: Developed an AR app for CPR feedback on Magic Leap 2, conducted a user study with simulated incidents, and used eye tracking to analyze SA. Proposed FixGraphPool, a graph neural network, to model gaze data for SA prediction.

Result: Higher SA correlated with greater saccadic amplitude/velocity and fewer fixations on virtual content. FixGraphPool achieved 83.0% accuracy in predicting SA, outperforming other models.

Conclusion: Eye tracking is valuable for SA modeling in AR, and FixGraphPool shows promise for designing safer AR systems by maintaining situational awareness.

Abstract: Augmented Reality (AR) systems, while enhancing task performance through
real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on
virtual content that compromises situational awareness (SA) in safety-critical
scenarios. This paper investigates SA in AR-guided cardiopulmonary
resuscitation (CPR), where responders must balance effective compressions with
vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR
app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth
and rate) and conducted a user study with simulated unexpected incidents (e.g.,
bleeding) to evaluate SA, in which SA metrics were collected via observation
and questionnaires administered during freeze-probe events. Eye tracking
analysis revealed that higher SA levels were associated with greater saccadic
amplitude and velocity, and with reduced proportion and frequency of fixations
on virtual content. To predict SA, we propose FixGraphPool, a graph neural
network that structures gaze events (fixations, saccades) into spatiotemporal
graphs, effectively capturing dynamic attentional patterns. Our model achieved
83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and
state-of-the-art time-series models by leveraging domain knowledge and
spatial-temporal information encoded in ET data. These findings demonstrate the
potential of eye tracking for SA modeling in AR and highlight its utility in
designing AR systems that ensure user safety and situational awareness.

</details>


### [69] [Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting](https://arxiv.org/abs/2508.05059)
*Jinhyeok Jang,Jaehong Kim,Jung Uk Kim*

Main category: cs.LG

TL;DR: The paper introduces KNOW prediction, a method to enhance pre-trained weights by leveraging structured forgetting and its inversion, outperforming traditional fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To improve pre-trained weights by encapsulating more knowledge beyond the given dataset, addressing limitations in data-scarce scenarios.

Method: Uses structured forgetting and its inversion, modeled via meta-learning (KNOWN hyper-model), to predict knowledge-enriched weights.

Result: KNOW prediction consistently outperforms naive fine-tuning and simple weight prediction across diverse datasets and architectures.

Conclusion: The work reinterprets forgetting dynamics to enhance knowledge transfer in deep learning, offering a novel strategy for weight optimization.

Abstract: Pre-trained weights have become a cornerstone of modern deep learning,
enabling efficient knowledge transfer and improving downstream task
performance, especially in data-scarce scenarios. However, a fundamental
question remains: how can we obtain better pre-trained weights that encapsulate
more knowledge beyond the given dataset? In this work, we introduce
\textbf{KNowledge Overflowed Weights (KNOW)} prediction, a novel strategy that
leverages structured forgetting and its inversion to synthesize
knowledge-enriched weights. Our key insight is that sequential fine-tuning on
progressively downsized datasets induces a structured forgetting process, which
can be modeled and reversed to recover knowledge as if trained on a larger
dataset. We construct a dataset of weight transitions governed by this
controlled forgetting and employ meta-learning to model weight prediction
effectively. Specifically, our \textbf{KNowledge Overflowed Weights Nowcaster
(KNOWN)} acts as a hyper-model that learns the general evolution of weights and
predicts enhanced weights with improved generalization. Extensive experiments
across diverse datasets and architectures demonstrate that KNOW prediction
consistently outperforms Na\"ive fine-tuning and simple weight prediction,
leading to superior downstream performance. Our work provides a new perspective
on reinterpreting forgetting dynamics to push the limits of knowledge transfer
in deep learning.

</details>


### [70] [TANGO: Graph Neural Dynamics via Learned Energy and Tangential Flows](https://arxiv.org/abs/2508.05070)
*Moshe Eliasof,Eldad Haber,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: TANGO is a graph representation learning framework using dynamical systems, combining energy-based dynamics and tangential evolution for stable and flexible feature learning.


<details>
  <summary>Details</summary>
Motivation: To address challenges in graph learning, such as oversquashing and ill-conditioned energy regions, by ensuring stability and flexibility in feature evolution.

Method: Uses a learnable Lyapunov function for energy reduction and a tangential component via message passing to evolve features while maintaining energy.

Result: Achieves strong performance in node and graph classification/regression tasks, mitigating oversquashing.

Conclusion: TANGO effectively combines energy functions and tangential flows for robust graph neural networks.

Abstract: We introduce TANGO -- a dynamical systems inspired framework for graph
representation learning that governs node feature evolution through a learned
energy landscape and its associated descent dynamics. At the core of our
approach is a learnable Lyapunov function over node embeddings, whose gradient
defines an energy-reducing direction that guarantees convergence and stability.
To enhance flexibility while preserving the benefits of energy-based dynamics,
we incorporate a novel tangential component, learned via message passing, that
evolves features while maintaining the energy value. This decomposition into
orthogonal flows of energy gradient descent and tangential evolution yields a
flexible form of graph dynamics, and enables effective signal propagation even
in flat or ill-conditioned energy regions, that often appear in graph learning.
Our method mitigates oversquashing and is compatible with different graph
neural network backbones. Empirically, TANGO achieves strong performance across
a diverse set of node and graph classification and regression benchmarks,
demonstrating the effectiveness of jointly learned energy functions and
tangential flows for graph neural networks.

</details>


### [71] [ULU: A Unified Activation Function](https://arxiv.org/abs/2508.05073)
*Simin Huo*

Main category: cs.LG

TL;DR: ULU is a non-monotonic, piecewise activation function outperforming ReLU and Mish, with an adaptive variant AULU and a new LIB metric for inductive bias measurement.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing activation functions like ReLU and Mish by introducing a flexible, non-monotonic function that treats positive and negative inputs differently.

Method: Proposes ULU, a piecewise activation function, and its adaptive variant AULU with learnable parameters. Introduces the LIB metric for measuring inductive bias.

Result: ULU and AULU outperform ReLU and Mish in image classification and object detection tasks.

Conclusion: ULU and AULU offer superior performance and adaptability, with the LIB metric providing a quantitative measure of inductive bias.

Abstract: We propose \textbf{ULU}, a novel non-monotonic, piecewise activation function
defined as $\{f(x;\alpha_1),x<0; f(x;\alpha_2),x>=0 \}$, where
$f(x;\alpha)=0.5x(tanh(\alpha x)+1),\alpha >0$. ULU treats positive and
negative inputs differently. Extensive experiments demonstrate ULU
significantly outperforms ReLU and Mish across image classification and object
detection tasks. Its variant Adaptive ULU (\textbf{AULU}) is expressed as
$\{f(x;\beta_1^2),x<0; f(x;\beta_2^2),x>=0 \}$, where $\beta_1$ and $\beta_2$
are learnable parameters, enabling it to adapt its response separately for
positive and negative inputs. Additionally, we introduce the LIB (Like
Inductive Bias) metric from AULU to quantitatively measure the inductive bias
of the model.

</details>


### [72] [Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning](https://arxiv.org/abs/2508.05077)
*Luai Abuelsamen,Temitope Lukman Adebanjo*

Main category: cs.LG

TL;DR: The paper explores how multimodal imitation learning benefits from statistical learning theory, showing tighter generalization bounds and better optimization landscapes compared to unimodal approaches.


<details>
  <summary>Details</summary>
Motivation: To understand why multimodal policies (e.g., RGB-D, proprioception, language) outperform unimodal ones by analyzing their theoretical foundations.

Method: Analyzes multimodal learning theory, connecting empirical results (e.g., PerAct, CLIPort) to concepts like Rademacher complexity, PAC learning, and information theory.

Result: Properly integrated multimodal policies achieve superior performance with tighter generalization bounds and favorable optimization landscapes.

Conclusion: Multimodal architectures, when integrated correctly, offer theoretical and empirical advantages over unimodal approaches in imitation learning.

Abstract: This paper examines the theoretical foundations of multimodal imitation
learning through the lens of statistical learning theory. We analyze how
multimodal perception (RGB-D, proprioception, language) affects sample
complexity and optimization landscapes in imitation policies. Building on
recent advances in multimodal learning theory, we show that properly integrated
multimodal policies can achieve tighter generalization bounds and more
favorable optimization landscapes than their unimodal counterparts. We provide
a comprehensive review of theoretical frameworks that explain why multimodal
architectures like PerAct and CLIPort achieve superior performance, connecting
these empirical results to fundamental concepts in Rademacher complexity, PAC
learning, and information theory.

</details>


### [73] [Integrated Influence: Data Attribution with Baseline](https://arxiv.org/abs/2508.05089)
*Linxiao Yang,Xinyu Gu,Liang Sun*

Main category: cs.LG

TL;DR: Proposes Integrated Influence, a data attribution method with a baseline approach, addressing limitations of LOO-based methods and improving reliability.


<details>
  <summary>Details</summary>
Motivation: Existing data attribution methods, like LOO-based ones, lack collective influence analysis and counterfactual explanations, limiting transparency.

Method: Introduces Integrated Influence, which uses a baseline dataset, degenerates data to baseline, and accumulates sample influence during the process.

Result: Outperforms existing methods in data attribution and mislabeled example identification, with a theoretical framework linking it to popular methods like influence functions.

Conclusion: Integrated Influence enhances data attribution reliability and flexibility, offering a broader theoretical and practical improvement over current methods.

Abstract: As an effective approach to quantify how training samples influence test
sample, data attribution is crucial for understanding data and model and
further enhance the transparency of machine learning models. We find that
prevailing data attribution methods based on leave-one-out (LOO) strategy
suffer from the local-based explanation, as these LOO-based methods only
perturb a single training sample, and overlook the collective influence in the
training set. On the other hand, the lack of baseline in many data attribution
methods reduces the flexibility of the explanation, e.g., failing to provide
counterfactual explanations. In this paper, we propose Integrated Influence, a
novel data attribution method that incorporates a baseline approach. Our method
defines a baseline dataset, follows a data degeneration process to transition
the current dataset to the baseline, and accumulates the influence of each
sample throughout this process. We provide a solid theoretical framework for
our method, and further demonstrate that popular methods, such as influence
functions, can be viewed as special cases of our approach. Experimental results
show that Integrated Influence generates more reliable data attributions
compared to existing methods in both data attribution task and mislablled
example identification task.

</details>


### [74] [Cold Start Active Preference Learning in Socio-Economic Domains](https://arxiv.org/abs/2508.05090)
*Mojtaba Fayaz-Bakhsh,Danial Ataee,MohammadAmin Fazli*

Main category: cs.LG

TL;DR: The paper introduces a cold-start active preference learning framework using PCA for initial pseudo-labels and active learning for refinement, outperforming standard methods in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: The cold-start problem in active preference learning hinders performance when no initial labeled data is available, especially in domains like social systems and economics where labeled data is scarce and noisy.

Method: The framework uses PCA for self-supervised pre-training to generate pseudo-labels, followed by an active learning loop with a simulated noisy oracle for refinement.

Result: Experiments show the method outperforms standard active learning, achieving higher accuracy with fewer labeled pairs.

Conclusion: The framework effectively mitigates the cold-start problem, improving sample efficiency and applicability in data-constrained environments.

Abstract: Active preference learning is a powerful paradigm for efficiently modeling
preferences, yet it suffers from the cold-start problem: a significant drop in
performance when no initial labeled data is available. This challenge is
particularly acute in computational social systems and economic analysis, where
labeled data is often scarce, expensive, and subject to expert noise. To
address this gap, we propose a novel framework for cold-start active preference
learning. Our method initiates the learning process through a self-supervised
pre-training phase, utilizing Principal Component Analysis (PCA) to derive
initial pseudo-labels from the data's inherent structure, thereby creating a
cold-start model without any initial oracle interaction. Subsequently, the
model is refined through an active learning loop that strategically queries a
simulated noisy oracle for labels. We conduct extensive experiments on diverse
datasets from different domains, including financial credibility, career
success rate, and socio-economic status. The results demonstrate that our
cold-start approach outperforms standard active learning strategies that begin
from a blank slate, achieving higher accuracy with substantially fewer labeled
pairs. Our framework offers a practical and effective solution to mitigate the
cold-start problem, enhancing the sample efficiency and applicability of
preference learning in data-constrained environments. We release our code at
https://github.com/Dan-A2/cold-start-preference-learning

</details>


### [75] [Learning from Similarity-Confidence and Confidence-Difference](https://arxiv.org/abs/2508.05108)
*Tomoya Tate,Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: A novel WSL framework leverages multiple weak supervision signals (similarity-confidence and confidence-difference) for improved performance when labeled data is scarce. It introduces unbiased risk estimators and a risk correction approach, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate labeling is challenging, and labeled data is often limited. Existing WSL methods use single weak supervision, missing potential complementary signals.

Method: Proposes SconfConfDiff Classification, integrating two weak labels. Derives two unbiased risk estimators (convex combination and new interaction-based) and a risk correction approach.

Result: The method achieves optimal convergence rates, mitigates overfitting, and is robust against label noise and inaccurate class priors. Experiments show superior performance over baselines.

Conclusion: The framework effectively combines multiple weak supervision signals, offering a practical solution for limited labeled data scenarios with theoretical and empirical validation.

Abstract: In practical machine learning applications, it is often challenging to assign
accurate labels to data, and increasing the number of labeled instances is
often limited. In such cases, Weakly Supervised Learning (WSL), which enables
training with incomplete or imprecise supervision, provides a practical and
effective solution. However, most existing WSL methods focus on leveraging a
single type of weak supervision. In this paper, we propose a novel WSL
framework that leverages complementary weak supervision signals from multiple
relational perspectives, which can be especially valuable when labeled data is
limited. Specifically, we introduce SconfConfDiff Classification, a method that
integrates two distinct forms of weaklabels: similarity-confidence and
confidence-difference, which are assigned to unlabeled data pairs. To implement
this method, we derive two types of unbiased risk estimators for
classification: one based on a convex combination of existing estimators, and
another newly designed by modeling the interaction between two weak labels. We
prove that both estimators achieve optimal convergence rates with respect to
estimation error bounds. Furthermore, we introduce a risk correction approach
to mitigate overfitting caused by negative empirical risk, and provide
theoretical analysis on the robustness of the proposed method against
inaccurate class prior probability and label noise. Experimental results
demonstrate that the proposed method consistently outperforms existing
baselines across a variety of settings.

</details>


### [76] [Exploring Superior Function Calls via Reinforcement Learning](https://arxiv.org/abs/2508.05118)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Yicheng Chen,Cunyin Peng,Jinjie GU,Chenyi Zhuang*

Main category: cs.LG

TL;DR: A novel reinforcement learning framework improves function calling in LLMs by addressing exploration, reasoning, and parameter verification, achieving 86.02% accuracy.


<details>
  <summary>Details</summary>
Motivation: Current training methods for LLMs in function calling lack robust reasoning and exploration, limiting real-world deployment.

Method: A two-stage data pipeline with iterative LLM evaluation and AST validation, combined with strategic entropy-based exploration in reinforcement learning.

Result: Achieves 86.02% accuracy on the Berkeley Function Calling Leaderboard, outperforming standard methods by up to 6%.

Conclusion: The framework enhances function calling performance, especially for code-pretrained models, and will be released for community use.

Abstract: Function calling capabilities are crucial for deploying Large Language Models
in real-world applications, yet current training approaches fail to develop
robust reasoning strategies. Supervised fine-tuning produces models that rely
on superficial pattern matching, while standard reinforcement learning methods
struggle with the complex action space of structured function calls. We present
a novel reinforcement learning framework designed to enhance group relative
policy optimization through strategic entropy based exploration specifically
tailored for function calling tasks. Our approach addresses three critical
challenges in function calling: insufficient exploration during policy
learning, lack of structured reasoning in chain-of-thought generation, and
inadequate verification of parameter extraction. Our two-stage data preparation
pipeline ensures high-quality training samples through iterative LLM evaluation
and abstract syntax tree validation. Extensive experiments on the Berkeley
Function Calling Leaderboard demonstrate that this framework achieves
state-of-the-art performance among open-source models with 86.02\% overall
accuracy, outperforming standard GRPO by up to 6\% on complex multi-function
scenarios. Notably, our method shows particularly strong improvements on
code-pretrained models, suggesting that structured language generation
capabilities provide an advantageous starting point for reinforcement learning
in function calling tasks. We will release all the code, models and dataset to
benefit the community.

</details>


### [77] [HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation](https://arxiv.org/abs/2508.05135)
*Thinh Nguyen,Trung Phan,Binh T. Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: The paper introduces HFedDG, a hierarchical federated learning approach addressing domain shift by integrating domain generalization, and proposes HFedATM for improved performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional FL and HFL face scalability and domain shift issues, limiting model performance on unseen data.

Method: Proposes HFedATM, which aligns models using Filter-wise Optimal Transport Alignment and merges them with Shrinkage-aware Regularized Mean Aggregation.

Result: HFedATM outperforms FedDG baselines, maintains efficiency, and achieves tighter generalization bounds.

Conclusion: HFedATM effectively addresses domain shift in HFL, enhancing performance and stability.

Abstract: Federated Learning (FL) is a decentralized approach where multiple clients
collaboratively train a shared global model without sharing their raw data.
Despite its effectiveness, conventional FL faces scalability challenges due to
excessive computational and communication demands placed on a single central
server as the number of participating devices grows. Hierarchical Federated
Learning (HFL) addresses these issues by distributing model aggregation tasks
across intermediate nodes (stations), thereby enhancing system scalability and
robustness against single points of failure. However, HFL still suffers from a
critical yet often overlooked limitation: domain shift, where data
distributions vary significantly across different clients and stations,
reducing model performance on unseen target domains. While Federated Domain
Generalization (FedDG) methods have emerged to improve robustness to domain
shifts, their integration into HFL frameworks remains largely unexplored. In
this paper, we formally introduce Hierarchical Federated Domain Generalization
(HFedDG), a novel scenario designed to investigate domain shift within
hierarchical architectures. Specifically, we propose HFedATM, a hierarchical
aggregation method that first aligns the convolutional filters of models from
different stations through Filter-wise Optimal Transport Alignment and
subsequently merges aligned models using a Shrinkage-aware Regularized Mean
Aggregation. Our extensive experimental evaluations demonstrate that HFedATM
significantly boosts the performance of existing FedDG baselines across
multiple datasets and maintains computational and communication efficiency.
Moreover, theoretical analyses indicate that HFedATM achieves tighter
generalization error bounds compared to standard hierarchical averaging,
resulting in faster convergence and stable training behavior.

</details>


### [78] [Deep Neural Networks with General Activations: Super-Convergence in Sobolev Norms](https://arxiv.org/abs/2508.05141)
*Yahong Yang,Juncai He*

Main category: cs.LG

TL;DR: Deep neural networks with general activation functions achieve superior approximation rates in Sobolev spaces, outperforming traditional methods like finite element and spectral methods, termed 'super-convergence.'


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in error-estimation theory for neural-network-based PDE solutions and demonstrate their superior accuracy over classical numerical methods.

Method: Analysis of deep fully-connected neural networks with general activation functions in Sobolev spaces, measuring errors in the $W^{m,p}$-norm for $m < n$.

Result: Deep networks exhibit super-convergence, surpassing classical methods in approximating weak PDE solutions.

Conclusion: The work provides a unified theoretical foundation for neural networks in scientific computing, closing a significant gap in PDE approximation theory.

Abstract: This paper establishes a comprehensive approximation result for deep
fully-connected neural networks with commonly-used and general activation
functions in Sobolev spaces $W^{n,\infty}$, with errors measured in the
$W^{m,p}$-norm for $m < n$ and $1\le p \le \infty$. The derived rates surpass
those of classical numerical approximation techniques, such as finite element
and spectral methods, exhibiting a phenomenon we refer to as
\emph{super-convergence}. Our analysis shows that deep networks with general
activations can approximate weak solutions of partial differential equations
(PDEs) with superior accuracy compared to traditional numerical methods at the
approximation level. Furthermore, this work closes a significant gap in the
error-estimation theory for neural-network-based approaches to PDEs, offering a
unified theoretical foundation for their use in scientific computing.

</details>


### [79] [PSEO: Optimizing Post-hoc Stacking Ensemble Through Hyperparameter Tuning](https://arxiv.org/abs/2508.05144)
*Beicheng Xu,Wei Liu,Keyao Ding,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: PSEO is a framework optimizing post-hoc stacking ensembles in AutoML, outperforming 16 methods with a top average test rank of 2.96.


<details>
  <summary>Details</summary>
Motivation: Current CASH methods in AutoML use fixed ensemble strategies, lacking adaptability to task specifics. PSEO aims to address this gap.

Method: PSEO selects base models via binary quadratic programming, balances diversity and performance, and optimizes multi-layer stacking with a hyperparameter search.

Result: PSEO achieves the best average test rank (2.96) on 80 datasets, surpassing existing AutoML and ensemble methods.

Conclusion: PSEO effectively optimizes post-hoc ensembles, demonstrating superior performance and adaptability in AutoML tasks.

Abstract: The Combined Algorithm Selection and Hyperparameter Optimization (CASH)
problem is fundamental in Automated Machine Learning (AutoML). Inspired by the
success of ensemble learning, recent AutoML systems construct post-hoc
ensembles for final predictions rather than relying on the best single model.
However, while most CASH methods conduct extensive searches for the optimal
single model, they typically employ fixed strategies during the ensemble phase
that fail to adapt to specific task characteristics. To tackle this issue, we
propose PSEO, a framework for post-hoc stacking ensemble optimization. First,
we conduct base model selection through binary quadratic programming, with a
trade-off between diversity and performance. Furthermore, we introduce two
mechanisms to fully realize the potential of multi-layer stacking. Finally,
PSEO builds a hyperparameter space and searches for the optimal post-hoc
ensemble strategy within it. Empirical results on 80 public datasets show that
\sys achieves the best average test rank (2.96) among 16 methods, including
post-hoc designs in recent AutoML systems and state-of-the-art ensemble
learning methods.

</details>


### [80] [Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation](https://arxiv.org/abs/2508.05154)
*Rishabh Gaur,Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: The paper introduces domain-driven metrics for evaluating RL-based agent-based models (ABMs) and rational ABMs (RABMs), addressing challenges in performance assessment due to system complexity and lack of standardized metrics.


<details>
  <summary>Details</summary>
Motivation: The complexity and stochasticity of ABMs/RABMs, along with the absence of standardized metrics for RL algorithms, motivate the development of domain-driven evaluation metrics.

Method: The study develops domain-driven RL metrics, building on existing state-of-the-art metrics, and applies them to a rational ABM case study involving pandemic behaviors like masking, vaccination, and lockdown.

Result: Results demonstrate the effectiveness of domain-driven rewards combined with traditional and advanced metrics in various simulation scenarios, such as differential mask availability.

Conclusion: The proposed domain-driven metrics enhance the evaluation of RL-based ABMs/RABMs, providing a more nuanced and practical assessment framework.

Abstract: For the development and optimization of agent-based models (ABMs) and
rational agent-based models (RABMs), optimization algorithms such as
reinforcement learning are extensively used. However, assessing the performance
of RL-based ABMs and RABMS models is challenging due to the complexity and
stochasticity of the modeled systems, and the lack of well-standardized metrics
for comparing RL algorithms. In this study, we are developing domain-driven
metrics for RL, while building on state-of-the-art metrics. We demonstrate our
``Domain-driven-RL-metrics'' using policy optimization on a rational ABM
disease modeling case study to model masking behavior, vaccination, and
lockdown in a pandemic. Our results show the use of domain-driven rewards in
conjunction with traditional and state-of-the-art metrics for a few different
simulation scenarios such as the differential availability of masks.

</details>


### [81] [Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models](https://arxiv.org/abs/2508.05165)
*Mason Nakamura,Saaduddin Mahmud,Kyle H. Wray,Hamed Zamani,Shlomo Zilberstein*

Main category: cs.LG

TL;DR: HIA (Heuristic-Guided Inference-time Alignment) is a tuning-free, black-box-compatible method that balances alignment quality and computational cost for LLMs, outperforming existing baselines under the same inference budget.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with user preferences is costly in terms of fine-tuning or inference, necessitating a balance between alignment quality and computational expense.

Method: HIA uses a lightweight prompt optimizer, heuristic reward models, and two-stage filtering to reduce inference calls while maintaining alignment quality.

Result: HIA outperforms best-of-N sampling, beam search, and greedy search on real-world datasets (HelpSteer and ComPRed) under the same inference budget, even with low budgets (1-2 queries).

Conclusion: HIA offers a practical, scalable solution for personalized LLM deployment by efficiently balancing alignment quality and computational cost.

Abstract: Aligning LLMs with user preferences is crucial for real-world use but often
requires costly fine-tuning or expensive inference, forcing trade-offs between
alignment quality and computational cost. Existing inference-time methods
typically ignore this balance, focusing solely on the optimized policy's
performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a
tuning-free, black-box-compatible approach that uses a lightweight prompt
optimizer, heuristic reward models, and two-stage filtering to reduce inference
calls while preserving alignment quality. On real-world prompt datasets,
HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and
greedy search baselines in multi-objective, goal-conditioned tasks under the
same inference budget. We also find that HIA is effective under low-inference
budgets with as little as one or two response queries, offering a practical
solution for scalable, personalized LLM deployment.

</details>


### [82] [pFedDSH: Enabling Knowledge Transfer in Personalized Federated Learning through Data-free Sub-Hypernetwork](https://arxiv.org/abs/2508.05157)
*Thinh Nguyen,Le Huy Khiem,Van-Tuan Tran,Khoa D Doan,Nitesh V Chawla,Kok-Seng Wong*

Main category: cs.LG

TL;DR: The paper introduces pFedDSH, a framework for dynamic client onboarding in Personalized Federated Learning, using hypernetworks and data-free replay to maintain performance and privacy.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in existing pFL methods that assume static client participation, the paper tackles challenges in dynamic environments where new clients join incrementally.

Method: Proposes pFedDSH, leveraging a central hypernetwork with batch-specific masks and data-free replay for knowledge transfer and stability.

Result: Outperforms state-of-the-art pFL and Federated Continual Learning baselines on CIFAR-10, CIFAR-100, and Tiny-ImageNet, showing robust performance and adaptation.

Conclusion: pFedDSH effectively handles dynamic client onboarding, ensuring performance stability for existing clients and efficient adaptation for new ones.

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, offering a significant privacy
benefit. However, most existing Personalized Federated Learning (pFL) methods
assume a static client participation, which does not reflect real-world
scenarios where new clients may continuously join the federated system (i.e.,
dynamic client onboarding). In this paper, we explore a practical scenario in
which a new batch of clients is introduced incrementally while the learning
task remains unchanged. This dynamic environment poses various challenges,
including preserving performance for existing clients without retraining and
enabling efficient knowledge transfer between client batches. To address these
issues, we propose Personalized Federated Data-Free Sub-Hypernetwork (pFedDSH),
a novel framework based on a central hypernetwork that generates personalized
models for each client via embedding vectors. To maintain knowledge stability
for existing clients, pFedDSH incorporates batch-specific masks, which activate
subsets of neurons to preserve knowledge. Furthermore, we introduce a data-free
replay strategy motivated by DeepInversion to facilitate backward transfer,
enhancing existing clients' performance without compromising privacy. Extensive
experiments conducted on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate
that pFedDSH outperforms the state-of-the-art pFL and Federated Continual
Learning baselines in our investigation scenario. Our approach achieves robust
performance stability for existing clients, as well as adaptation for new
clients and efficient utilization of neural resources.

</details>


### [83] [FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance](https://arxiv.org/abs/2508.05201)
*Mengao Zhang,Jiayu Fu,Tanya Warrier,Yuwen Wang,Tianhui Tan,Ke-wei Huang*

Main category: cs.LG

TL;DR: The paper addresses hallucination in LLMs for finance, proposing a framework to evaluate and mitigate it using a context-aware masked span prediction task on financial documents.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs poses risks in finance due to the need for precise numerical accuracy and context-dependent data, which current benchmarks lack.

Method: Develops a scalable framework with automated dataset creation (masking strategy), a new hallucination evaluation dataset from S&P 500 reports, and evaluates LLMs on financial tabular data.

Result: Identifies intrinsic hallucination patterns in LLMs, providing a methodology for reliable financial LLM evaluation.

Conclusion: The framework advances trustworthy financial Generative AI by addressing hallucination risks in LLMs.

Abstract: Hallucination remains a critical challenge for deploying Large Language
Models (LLMs) in finance. Accurate extraction and precise calculation from
tabular data are essential for reliable financial analysis, since even minor
numerical errors can undermine decision-making and regulatory compliance.
Financial applications have unique requirements, often relying on
context-dependent, numerical, and proprietary tabular data that existing
hallucination benchmarks rarely capture. In this study, we develop a rigorous
and scalable framework for evaluating intrinsic hallucinations in financial
LLMs, conceptualized as a context-aware masked span prediction task over
real-world financial documents. Our main contributions are: (1) a novel,
automated dataset creation paradigm using a masking strategy; (2) a new
hallucination evaluation dataset derived from S&P 500 annual reports; and (3) a
comprehensive evaluation of intrinsic hallucination patterns in
state-of-the-art LLMs on financial tabular data. Our work provides a robust
methodology for in-house LLM evaluation and serves as a critical step toward
building more trustworthy and reliable financial Generative AI systems.

</details>


### [84] [S$^2$M-Former: Spiking Symmetric Mixing Branchformer for Brain Auditory Attention Detection](https://arxiv.org/abs/2508.05164)
*Jiaqi Wang,Zhengyu Ma,Xiongri Shen,Chenlin Zhou,Leilei Zhao,Han Zhang,Yi Zhong,Siqi Cai,Zhenxi Song,Zhiguo Zhang*

Main category: cs.LG

TL;DR: S$^2$M-Former is a spiking symmetric mixing framework for EEG-based auditory attention detection, offering energy efficiency and high performance.


<details>
  <summary>Details</summary>
Motivation: Current EEG-based AAD lacks synergistic frameworks to leverage complementary EEG features under energy constraints.

Method: Proposes S$^2$M-Former with a spike-driven symmetric architecture and lightweight 1D token sequences to reduce parameters and power consumption.

Result: Achieves 5.8× energy reduction, 14.7× parameter reduction, and comparable SOTA decoding accuracy on three benchmarks.

Conclusion: S$^2$M-Former is a promising low-power, high-performance solution for AAD tasks.

Abstract: Auditory attention detection (AAD) aims to decode listeners' focus in complex
auditory environments from electroencephalography (EEG) recordings, which is
crucial for developing neuro-steered hearing devices. Despite recent
advancements, EEG-based AAD remains hindered by the absence of synergistic
frameworks that can fully leverage complementary EEG features under
energy-efficiency constraints. We propose S$^2$M-Former, a novel spiking
symmetric mixing framework to address this limitation through two key
innovations: i) Presenting a spike-driven symmetric architecture composed of
parallel spatial and frequency branches with mirrored modular design,
leveraging biologically plausible token-channel mixers to enhance complementary
learning across branches; ii) Introducing lightweight 1D token sequences to
replace conventional 3D operations, reducing parameters by 14.7$\times$. The
brain-inspired spiking architecture further reduces power consumption,
achieving a 5.8$\times$ energy reduction compared to recent ANN methods, while
also surpassing existing SNN baselines in terms of parameter efficiency and
performance. Comprehensive experiments on three AAD benchmarks (KUL, DTU and
AV-GC-AAD) across three settings (within-trial, cross-trial and cross-subject)
demonstrate that S$^2$M-Former achieves comparable state-of-the-art (SOTA)
decoding accuracy, making it a promising low-power, high-performance solution
for AAD tasks.

</details>


### [85] [Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction](https://arxiv.org/abs/2508.05210)
*Saddam Hussain Khan*

Main category: cs.LG

TL;DR: A hybrid deep learning model combining LSTM, Transformer encoders, TS-Mixer, and attention mechanisms outperforms traditional methods in predicting Rate of Penetration (ROP) with high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Accurate ROP prediction is challenging due to complex drilling data dynamics, and existing models fail to capture intricate relationships.

Method: Proposes a hybrid architecture integrating LSTM, Transformer encoders, TS-Mixer blocks, and attention mechanisms to model temporal dependencies and feature interactions.

Result: Achieves an R-squared score of 0.9988 and MAPE of 1.447%, outperforming benchmarks.

Conclusion: The hybrid model enables reliable real-time ROP prediction, advancing intelligent drilling optimization.

Abstract: The Rate of Penetration (ROP) is crucial for optimizing drilling operations;
however, accurately predicting it is hindered by the complex, dynamic, and
high-dimensional nature of drilling data. Traditional empirical, physics-based,
and basic machine learning models often fail to capture intricate temporal and
contextual relationships, resulting in suboptimal predictions and limited
real-time utility. To address this gap, we propose a novel hybrid deep learning
architecture integrating Long Short-Term Memory (LSTM) networks, Transformer
encoders, Time-Series Mixer (TS-Mixer) blocks, and attention mechanisms to
synergistically model temporal dependencies, static feature interactions,
global context, and dynamic feature importance. Evaluated on a real-world
drilling dataset, our model outperformed benchmarks (standalone LSTM, TS-Mixer,
and simpler hybrids) with an R-squared score of 0.9988 and a Mean Absolute
Percentage Error of 1.447%, as measured by standard regression metrics
(R-squared, MAE, RMSE, MAPE). Model interpretability was ensured using SHAP and
LIME, while actual vs. predicted curves and bias checks confirmed accuracy and
fairness across scenarios. This advanced hybrid approach enables reliable
real-time ROP prediction, paving the way for intelligent, cost-effective
drilling optimization systems with significant operational impact.

</details>


### [86] [Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models](https://arxiv.org/abs/2508.05260)
*Zhouyao Qian,Yang Chen,Baodian Li,Shuyi Zhang,Zhen Tian,Gongsen Wang,Tianyue Gu,Xinyu Zhou,Huilin Chen,Xinyi Li,Hao Zhu,Shuyao Zhang,Zongheng Li,Siyuan Wang*

Main category: cs.LG

TL;DR: A hybrid LSTM-RF model outperforms standalone LSTM and RF models in predicting marine chlorophyll concentration, offering improved accuracy for ecological monitoring.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of marine chlorophyll concentration is vital for ecosystem health and red tide warnings, addressing limitations of single models in time-series and nonlinear feature analysis.

Method: The LSTM-RF hybrid model combines LSTM and RF, trained with multi-source ocean data (temperature, salinity, dissolved oxygen). Standardized treatment and sliding window techniques enhance accuracy.

Result: The hybrid model achieves R²=0.5386, MSE=0.005806, and MAE=0.057147, outperforming standalone LSTM (R²=0.0208) and RF (R²=0.4934).

Conclusion: The LSTM-RF model provides an innovative, high-accuracy solution for predicting marine ecological variables, improving upon single-model approaches.

Abstract: Marine chlorophyll concentration is an important indicator of ecosystem
health and carbon cycle strength, and its accurate prediction is crucial for
red tide warning and ecological response. In this paper, we propose a LSTM-RF
hybrid model that combines the advantages of LSTM and RF, which solves the
deficiencies of a single model in time-series modelling and nonlinear feature
portrayal. Trained with multi-source ocean data(temperature, salinity,
dissolved oxygen, etc.), the experimental results show that the LSTM-RF model
has an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test
set, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2
=0.4934) alone , respectively. The standardised treatment and sliding window
approach improved the prediction accuracy of the model and provided an
innovative solution for high-frequency prediction of marine ecological
variables.

</details>


### [87] [Near Optimal Inference for the Best-Performing Algorithm](https://arxiv.org/abs/2508.05173)
*Amichai Painsky*

Main category: cs.LG

TL;DR: The paper introduces a novel framework for subset selection in multinomial distributions to identify the best-performing machine learning algorithm from benchmark data, improving upon existing methods with both asymptotic and finite-sample schemes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reliably select the best-performing machine learning algorithm from benchmark data, especially when performance differences are marginal.

Method: The problem is formulated as subset selection for multinomial distributions. The paper proposes a novel framework with asymptotic and finite-sample schemes.

Result: The proposed schemes significantly outperform current methods, with matching lower bounds demonstrating their favorable performance.

Conclusion: The introduced framework effectively addresses the subset selection problem, providing robust solutions for identifying top-performing algorithms.

Abstract: Consider a collection of competing machine learning algorithms. Given their
performance on a benchmark of datasets, we would like to identify the best
performing algorithm. Specifically, which algorithm is most likely to rank
highest on a future, unseen dataset. A natural approach is to select the
algorithm that demonstrates the best performance on the benchmark. However, in
many cases the performance differences are marginal and additional candidates
may also be considered. This problem is formulated as subset selection for
multinomial distributions. Formally, given a sample from a countable alphabet,
our goal is to identify a minimal subset of symbols that includes the most
frequent symbol in the population with high confidence. In this work, we
introduce a novel framework for the subset selection problem. We provide both
asymptotic and finite-sample schemes that significantly improve upon currently
known methods. In addition, we provide matching lower bounds, demonstrating the
favorable performance of our proposed schemes.

</details>


### [88] [FlowState: Sampling Rate Invariant Time Series Forecasting](https://arxiv.org/abs/2508.05287)
*Lars Graf,Thomas Ortner,Stanisław Woźniak,Angeliki Pantazi*

Main category: cs.LG

TL;DR: FlowState introduces a novel time series foundation model (TSFM) with a state space model encoder and functional basis decoder, enabling continuous-time modeling and dynamic time-scale adjustment, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing TSFMs struggle with generalization across varying lengths, adaptability to sampling rates, and computational inefficiency. FlowState aims to address these challenges.

Method: FlowState uses a state space model (SSM) based encoder and a functional basis decoder for continuous-time modeling and dynamic time-scale adjustment. It also includes an efficient pretraining strategy.

Result: FlowState outperforms state-of-the-art models on GIFT-ZS and Chronos-ZS benchmarks, despite being smaller. It also adapts online to varying input sampling rates.

Conclusion: FlowState is a highly efficient and adaptable TSFM, setting new benchmarks in time series forecasting with its innovative architecture and pretraining strategy.

Abstract: Foundation models (FMs) have transformed natural language processing, but
their success has not yet translated to time series forecasting. Existing time
series foundation models (TSFMs), often based on transformer variants, struggle
with generalization across varying context and target lengths, lack
adaptability to different sampling rates, and are computationally inefficient.
We introduce FlowState, a novel TSFM architecture that addresses these
challenges through two key innovations: a state space model (SSM) based encoder
and a functional basis decoder. This design enables continuous-time modeling
and dynamic time-scale adjustment, allowing FlowState to inherently generalize
across all possible temporal resolutions, and dynamically adjust the
forecasting horizons. In contrast to other state-of-the-art TSFMs, which
require training data across all possible sampling rates to memorize patterns
at each scale, FlowState inherently adapts its internal dynamics to the input
scale, enabling smaller models, reduced data requirements, and improved
efficiency. We further propose an efficient pretraining strategy that improves
robustness and accelerates training. Despite being the smallest model,
FlowState outperforms all other models and is state-of-the-art for the GIFT-ZS
and the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness of
its components, and we demonstrate its unique ability to adapt online to
varying input sampling rates.

</details>


### [89] [Human Activity Recognition from Smartphone Sensor Data for Clinical Trials](https://arxiv.org/abs/2508.05175)
*Stefania Russo,Rafał Klimas,Marta Płonka,Hugo Le Gall,Sven Holm,Dimitar Stanev,Florian Lipsmeier,Mattia Zanon,Lito Kriara*

Main category: cs.LG

TL;DR: A ResNet-based HAR model detects gait and everyday activities with high accuracy and robustness across smartphone wear locations, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To develop a HAR model with minimal overhead for detecting gait and everyday activities, applicable to both healthy individuals and people with multiple sclerosis.

Method: The model uses ResNet architecture, trained and evaluated on smartphone sensor data from healthy controls and PwMS, incorporating datasets from GaitLab, Roche, and public sources.

Result: Achieved 98.4%-99.6% accuracy for gait detection and 96.2% for everyday activities, outperforming state-of-the-art models, especially across 9 wear locations (2.8%-9.0% improvement).

Conclusion: The HAR model is highly accurate and robust, suitable for practical applications in activity recognition.

Abstract: We developed a ResNet-based human activity recognition (HAR) model with
minimal overhead to detect gait versus non-gait activities and everyday
activities (walking, running, stairs, standing, sitting, lying, sit-to-stand
transitions). The model was trained and evaluated using smartphone sensor data
from adult healthy controls (HC) and people with multiple sclerosis (PwMS) with
Expanded Disability Status Scale (EDSS) scores between 0.0-6.5. Datasets
included the GaitLab study (ISRCTN15993728), an internal Roche dataset, and
publicly available data sources (training only). Data from 34 HC and 68 PwMS
(mean [SD] EDSS: 4.7 [1.5]) were included in the evaluation. The HAR model
showed 98.4% and 99.6% accuracy in detecting gait versus non-gait activities in
the GaitLab and Roche datasets, respectively, similar to a comparative
state-of-the-art ResNet model (99.3% and 99.4%). For everyday activities, the
proposed model not only demonstrated higher accuracy than the state-of-the-art
model (96.2% vs 91.9%; internal Roche dataset) but also maintained high
performance across 9 smartphone wear locations (handbag, shopping bag,
crossbody bag, backpack, hoodie pocket, coat/jacket pocket, hand, neck, belt),
outperforming the state-of-the-art model by 2.8% - 9.0%. In conclusion, the
proposed HAR model accurately detects everyday activities and shows high
robustness to various smartphone wear locations, demonstrating its practical
applicability.

</details>


### [90] [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](https://arxiv.org/abs/2508.05310)
*Jelle Luijkx,Zlatan Ajanović,Laura Ferranti,Jens Kober*

Main category: cs.LG

TL;DR: ASkDAgger reduces human teaching effort in interactive imitation learning by leveraging novice plans and uncertainty, using SAG, FIER, and PIER to optimize feedback and replay.


<details>
  <summary>Details</summary>
Motivation: Human teaching effort is a bottleneck in interactive imitation learning. Existing methods don't utilize novice plans, which contain valuable information.

Method: Introduces ASkDAgger with S-Aware Gating (SAG), Foresight Interactive Experience Replay (FIER), and Prioritized Interactive Experience Replay (PIER) to optimize feedback and replay.

Result: Validated in language-conditioned manipulation tasks, ASkDAgger reduces queries, improves generalization, and speeds adaptation.

Conclusion: ASkDAgger effectively balances query frequency with failure incidence, enhancing interactive imitation learning.

Abstract: Human teaching effort is a significant bottleneck for the broader
applicability of interactive imitation learning. To reduce the number of
required queries, existing methods employ active learning to query the human
teacher only in uncertain, risky, or novel situations. However, during these
queries, the novice's planned actions are not utilized despite containing
valuable information, such as the novice's capabilities, as well as
corresponding uncertainty levels. To this end, we allow the novice to say: "I
plan to do this, but I am uncertain." We introduce the Active Skill-level Data
Aggregation (ASkDAgger) framework, which leverages teacher feedback on the
novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating
threshold to track sensitivity, specificity, or a minimum success rate; (2)
Foresight Interactive Experience Replay (FIER), which recasts valid and
relabeled novice action plans into demonstrations; and (3) Prioritized
Interactive Experience Replay (PIER), which prioritizes replay based on
uncertainty, novice success, and demonstration age. Together, these components
balance query frequency with failure incidence, reduce the number of required
demonstration annotations, improve generalization, and speed up adaptation to
changing domains. We validate the effectiveness of ASkDAgger through
language-conditioned manipulation tasks in both simulation and real-world
environments. Code, data, and videos are available at
https://askdagger.github.io.

</details>


### [91] [Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference](https://arxiv.org/abs/2508.05190)
*Luis Mandl,Dibyajyoti Nayak,Tim Ricken,Somdatta Goswami*

Main category: cs.LG

TL;DR: PITI-DeepONet improves long-term accuracy in solving time-dependent PDEs by learning time-derivative operators and integrating them with classical schemes, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods (FR and AR) fail to capture causal dependencies and suffer from error accumulation, limiting long-term accuracy in PDE solutions.

Method: Introduces PITI-DeepONet, a dual-output architecture trained via physics-informed or hybrid objectives, learning time-derivative operators and integrating them with classical time-stepping.

Result: Significant error reductions: 84% (vs. FR) and 79% (vs. AR) for heat equation; 87% (vs. FR) and 98% (vs. AR) for Burgers equation; 42% (vs. FR) and 89% (vs. AR) for Allen-Cahn equation.

Conclusion: PITI-DeepONet enables more reliable long-term integration of complex PDEs, outperforming traditional methods.

Abstract: Accurately modeling and inferring solutions to time-dependent partial
differential equations (PDEs) over extended horizons remains a core challenge
in scientific machine learning. Traditional full rollout (FR) methods, which
predict entire trajectories in one pass, often fail to capture the causal
dependencies and generalize poorly outside the training time horizon.
Autoregressive (AR) approaches, evolving the system step by step, suffer from
error accumulation, limiting long-term accuracy. These shortcomings limit the
long-term accuracy and reliability of both strategies. To address these issues,
we introduce the Physics-Informed Time-Integrated Deep Operator Network
(PITI-DeepONet), a dual-output architecture trained via fully physics-informed
or hybrid physics- and data-driven objectives to ensure stable, accurate
long-term evolution well beyond the training horizon. Instead of forecasting
future states, the network learns the time-derivative operator from the current
state, integrating it using classical time-stepping schemes to advance the
solution in time. Additionally, the framework can leverage residual monitoring
during inference to estimate prediction quality and detect when the system
transitions outside the training domain. Applied to benchmark problems,
PITI-DeepONet shows improved accuracy over extended inference time horizons
when compared to traditional methods. Mean relative $\mathcal{L}_2$ errors
reduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation;
by 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and
by 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation.
By moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for
more reliable, long-term integration of complex, time-dependent PDEs.

</details>


### [92] [Optimal Corpus Aware Training for Neural Machine Translation](https://arxiv.org/abs/2508.05364)
*Yi-Hsiu Liao,Cheng Shen,Brenda,Yang*

Main category: cs.LG

TL;DR: OCAT improves CAT by fine-tuning only corpus-related parameters, boosting accuracy and resilience to overfitting, with notable gains in translation tasks.


<details>
  <summary>Details</summary>
Motivation: CAT's reliance on pre-defined high-quality data is error-prone and inefficient. OCAT addresses this by optimizing corpus-aware training.

Method: OCAT fine-tunes a CAT pre-trained model by freezing most parameters and tuning only corpus-related ones.

Result: +3.6 and +1.8 chrF improvements in WMT23 English to Chinese and German tasks, outperforming vanilla training and matching other fine-tuning methods.

Conclusion: OCAT is lightweight, effective, and less sensitive to hyperparameters, making it a robust alternative to existing techniques.

Abstract: Corpus Aware Training (CAT) leverages valuable corpus metadata during
training by injecting corpus information into each training example, and has
been found effective in the literature, commonly known as the "tagging"
approach. Models trained with CAT inherently learn the quality, domain and
nuance between corpora directly from data, and can easily switch to different
inference behavior. To achieve the best evaluation, CAT models pre-define a
group of high quality data before training starts which can be error-prone and
inefficient. In this work, we propose Optimal Corpus Aware Training (OCAT),
which fine-tunes a CAT pre-trained model by freezing most of the model
parameters and only tuning small set of corpus-related parameters. We show that
OCAT is lightweight, resilient to overfitting, and effective in boosting model
accuracy. We use WMT23 English to Chinese and English to German translation
tasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively,
over vanilla training. Furthermore, our approach is on-par or slightly better
than other state-of-the-art fine-tuning techniques while being less sensitive
to hyperparameter settings.

</details>


### [93] [Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms](https://arxiv.org/abs/2508.05387)
*Jie Xiao,Shaoduo Gan,Changyuan Fan,Qingnan Ren,Alfred Long,Yuchen Zhang,Rymon Yu,Eric Yang,Lynn Ai*

Main category: cs.LG

TL;DR: Echo decouples RL-based post-training for LLMs into separate inference and training phases, improving efficiency with lightweight sync protocols and heterogeneous hardware.


<details>
  <summary>Details</summary>
Motivation: Serial context switching in co-located RL systems violates SPMD assumptions, reducing efficiency.

Method: Echo uses sequential pull and asynchronous push-pull protocols to synchronize inference and training swarms.

Result: Echo matches co-located baselines in convergence and reward while utilizing decentralized hardware.

Conclusion: Decentralized, heterogeneous resources can achieve datacentre-grade performance for large-scale RL in LLMs.

Abstract: Modern RL-based post-training for large language models (LLMs) co-locate
trajectory sampling and policy optimisation on the same GPU cluster, forcing
the system to switch between inference and training workloads. This serial
context switching violates the single-program-multiple-data (SPMD) assumption
underlying today's distributed training systems. We present Echo, the RL system
that cleanly decouples these two phases across heterogeneous "inference" and
"training" swarms while preserving statistical efficiency. Echo introduces two
lightweight synchronization protocols: a sequential pull mode that refreshes
sampler weights on every API call for minimal bias, and an asynchronous
push-pull mode that streams version-tagged rollouts through a replay buffer to
maximise hardware utilisation. Training three representative RL workloads with
Qwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,
Echo matches a fully co-located Verl baseline in convergence speed and final
reward while off-loading trajectory generation to commodity edge hardware.
These promising results demonstrate that large-scale RL for LLMs could achieve
datacentre-grade performance using decentralised, heterogeneous resources.

</details>


### [94] [Bidding-Aware Retrieval for Multi-Stage Consistency in Online Advertising](https://arxiv.org/abs/2508.05206)
*Bin Liu,Yunfei Liu,Ziru Xu,Zhaoyu Zhou,Zhi Kou,Yeqiu Yang,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: The paper proposes Bidding-Aware Retrieval (BAR) to address inconsistency in online advertising systems by integrating ad bid values into retrieval, improving revenue and advertiser outcomes.


<details>
  <summary>Details</summary>
Motivation: The inconsistency between retrieval and ranking stages in online advertising systems, exacerbated by auto-bidding, leads to sub-optimal revenue and advertiser results.

Method: BAR incorporates bid signals via monotonicity-constrained learning and multi-task distillation, with asynchronous near-line inference for real-time updates and task-attentive refinement for feature disentanglement.

Result: BAR achieved a 4.32% revenue increase and 22.2% impression lift in Alibaba's platform.

Conclusion: BAR effectively resolves multi-stage inconsistency, enhancing platform revenue and advertiser performance.

Abstract: Online advertising systems typically use a cascaded architecture to manage
massive requests and candidate volumes, where the ranking stages allocate
traffic based on eCPM (predicted CTR $\times$ Bid). With the increasing
popularity of auto-bidding strategies, the inconsistency between the
computationally sensitive retrieval stage and the ranking stages becomes more
pronounced, as the former cannot access precise, real-time bids for the vast ad
corpus. This discrepancy leads to sub-optimal platform revenue and advertiser
outcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a
model-based retrieval framework that addresses multi-stage inconsistency by
incorporating ad bid value into the retrieval scoring function. The core
innovation is Bidding-Aware Modeling, incorporating bid signals through
monotonicity-constrained learning and multi-task distillation to ensure
economically coherent representations, while Asynchronous Near-Line Inference
enables real-time updates to the embedding for market responsiveness.
Furthermore, the Task-Attentive Refinement module selectively enhances feature
interactions to disentangle user interest and commercial value signals.
Extensive offline experiments and full-scale deployment across Alibaba's
display advertising platform validated BAR's efficacy: 4.32% platform revenue
increase with 22.2% impression lift for positively-operated advertisements.

</details>


### [95] [Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees](https://arxiv.org/abs/2508.05441)
*Zuyuan Zhang,Arnob Ghosh,Tian Lan*

Main category: cs.LG

TL;DR: The paper introduces two novel MCTS methods, CVaR-MCTS and W-MCTS, to address tail-risk in decision-making, providing rigorous safety guarantees and improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing MCTS methods lack rigorous tail-safety guarantees, risking adverse outcomes in high-stake scenarios.

Method: Proposes CVaR-MCTS for explicit tail-risk control and W-MCTS to address estimation bias using Wasserstein ambiguity sets.

Result: Both methods outperform baselines, achieving robust tail-risk guarantees with better rewards and stability.

Conclusion: The proposed methods effectively enhance safety and performance in MCTS for high-risk scenarios.

Abstract: Making decisions with respect to just the expected returns in Monte Carlo
Tree Search (MCTS) cannot account for the potential range of high-risk, adverse
outcomes associated with a decision. To this end, safety-aware MCTS often
consider some constrained variants -- by introducing some form of mean risk
measures or hard cost thresholds. These approaches fail to provide rigorous
tail-safety guarantees with respect to extreme or high-risk outcomes (denoted
as tail-risk), potentially resulting in serious consequence in high-stake
scenarios. This paper addresses the problem by developing two novel solutions.
We first propose CVaR-MCTS, which embeds a coherent tail risk measure,
Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter
$\alpha$ achieves explicit tail-risk control over the expected loss in the
"worst $(1-\alpha)\%$ scenarios." Second, we further address the estimation
bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or
W-MCTS) by introducing a first-order Wasserstein ambiguity set
$\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to
characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety
guarantees for both CVaR-MCTS and W-MCTS and establish their regret.
Evaluations on diverse simulated environments demonstrate that our proposed
methods outperform existing baselines, effectively achieving robust tail-risk
guarantees with improved rewards and stability.

</details>


### [96] [EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting](https://arxiv.org/abs/2508.05454)
*Wei Li,Zixin Wang,Qizheng Sun,Qixiang Gao,Fenglei Yang*

Main category: cs.LG

TL;DR: EnergyPatchTST, a Patch Time Series Transformer extension for energy forecasting, improves accuracy by 7-12% with multi-scale feature extraction, probability prediction, future variable integration, and pre-training.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for energy time series prediction struggle with multi-scale dynamics and data irregularity.

Method: Proposes EnergyPatchTST with multi-scale feature extraction, Monte Carlo-based uncertainty estimation, future variable integration, and pre-training.

Result: Outperforms other methods, reducing prediction error by 7-12% and providing reliable uncertainty estimation.

Conclusion: EnergyPatchTST offers a robust solution for energy time series prediction, enhancing accuracy and reliability.

Abstract: Accurate and reliable energy time series prediction is of great significance
for power generation planning and allocation. At present, deep learning time
series prediction has become the mainstream method. However, the multi-scale
time dynamics and the irregularity of real data lead to the limitations of the
existing methods. Therefore, we propose EnergyPatchTST, which is an extension
of the Patch Time Series Transformer specially designed for energy forecasting.
The main innovations of our method are as follows: (1) multi-scale feature
extraction mechanism to capture patterns with different time resolutions; (2)
probability prediction framework to estimate uncertainty through Monte Carlo
elimination; (3) integration path of future known variables (such as
temperature and wind conditions); And (4) Pre-training and Fine-tuning examples
to enhance the performance of limited energy data sets. A series of experiments
on common energy data sets show that EnergyPatchTST is superior to other
commonly used methods, the prediction error is reduced by 7-12%, and reliable
uncertainty estimation is provided, which provides an important reference for
time series prediction in the energy field.

</details>


### [97] [DFW: A Novel Weighting Scheme for Covariate Balancing and Treatment Effect Estimation](https://arxiv.org/abs/2508.05215)
*Ahmad Saeed Khan,Erik Schaffernicht,Johannes Andreas Stork*

Main category: cs.LG

TL;DR: Proposes Deconfounding Factor Weighting (DFW) to improve causal effect estimation by stabilizing weights and better balancing covariates compared to traditional methods like IPW.


<details>
  <summary>Details</summary>
Motivation: Selection bias in observational data leads to imbalanced covariate distributions, and existing propensity score methods (e.g., IPW) suffer from instability and inaccuracy.

Method: DFW leverages a deconfounding factor to construct stable weights, prioritizing less confounded samples and bounding weights to improve covariate balance.

Result: DFW outperforms IPW and CBPS in covariate balancing and treatment effect estimation, as shown in experiments on real-world and synthetic datasets.

Conclusion: DFW provides a robust alternative to traditional propensity score methods, enhancing causal inference by addressing instability and imbalance issues.

Abstract: Estimating causal effects from observational data is challenging due to
selection bias, which leads to imbalanced covariate distributions across
treatment groups. Propensity score-based weighting methods are widely used to
address this issue by reweighting samples to simulate a randomized controlled
trial (RCT). However, the effectiveness of these methods heavily depends on the
observed data and the accuracy of the propensity score estimator. For example,
inverse propensity weighting (IPW) assigns weights based on the inverse of the
propensity score, which can lead to instable weights when propensity scores
have high variance-either due to data or model misspecification-ultimately
degrading the ability of handling selection bias and treatment effect
estimation. To overcome these limitations, we propose Deconfounding Factor
Weighting (DFW), a novel propensity score-based approach that leverages the
deconfounding factor-to construct stable and effective sample weights. DFW
prioritizes less confounded samples while mitigating the influence of highly
confounded ones, producing a pseudopopulation that better approximates a RCT.
Our approach ensures bounded weights, lower variance, and improved covariate
balance.While DFW is formulated for binary treatments, it naturally extends to
multi-treatment settings, as the deconfounding factor is computed based on the
estimated probability of the treatment actually received by each sample.
Through extensive experiments on real-world benchmark and synthetic datasets,
we demonstrate that DFW outperforms existing methods, including IPW and CBPS,
in both covariate balancing and treatment effect estimation.

</details>


### [98] [Task complexity shapes internal representations and robustness in neural networks](https://arxiv.org/abs/2508.05463)
*Robert Jankowski,Filippo Radicchi,M. Ángeles Serrano,Marián Boguñá,Santo Fortunato*

Main category: cs.LG

TL;DR: The paper introduces data-agnostic probes to study how task difficulty affects neural network representations, revealing insights into robustness and topology, with implications for model compression and interpretability.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural network representations are shaped by input data complexity and task difficulty, given their black-box nature.

Method: Uses five probes (pruning, binarization, noise injection, sign flipping, bipartite network randomization) on MLPs, analyzed as signed bipartite graphs, for easy vs. hard tasks on MNIST and Fashion-MNIST.

Result: Hard-task models collapse under binarization, while easy-task models remain robust. Pruning reveals phase transitions, noise can enhance accuracy, and sign structure alone can maintain performance.

Conclusion: The performance gap between full-precision and binarized/shuffled networks measures task complexity, highlighting the importance of signed bipartite topology for practical model strategies.

Abstract: Neural networks excel across a wide range of tasks, yet remain black boxes.
In particular, how their internal representations are shaped by the complexity
of the input data and the problems they solve remains obscure. In this work, we
introduce a suite of five data-agnostic probes-pruning, binarization, noise
injection, sign flipping, and bipartite network randomization-to quantify how
task difficulty influences the topology and robustness of representations in
multilayer perceptrons (MLPs). MLPs are represented as signed, weighted
bipartite graphs from a network science perspective. We contrast easy and hard
classification tasks on the MNIST and Fashion-MNIST datasets. We show that
binarizing weights in hard-task models collapses accuracy to chance, whereas
easy-task models remain robust. We also find that pruning low-magnitude edges
in binarized hard-task models reveals a sharp phase-transition in performance.
Moreover, moderate noise injection can enhance accuracy, resembling a
stochastic-resonance effect linked to optimal sign flips of small-magnitude
weights. Finally, preserving only the sign structure-instead of precise weight
magnitudes-through bipartite network randomizations suffices to maintain high
accuracy. These phenomena define a model- and modality-agnostic measure of task
complexity: the performance gap between full-precision and binarized or
shuffled neural network performance. Our findings highlight the crucial role of
signed bipartite topology in learned representations and suggest practical
strategies for model compression and interpretability that align with task
complexity.

</details>


### [99] [ML-based Short Physical Performance Battery future score prediction based on questionnaire data](https://arxiv.org/abs/2508.05222)
*Marcin Kolakowski,Seif Ben Bader*

Main category: cs.LG

TL;DR: The paper predicts older adults' SPPB scores at a 4-year horizon using ML models, with XGBoost performing best (MAE: 0.79). Feature selection improved results (MAE: 0.82).


<details>
  <summary>Details</summary>
Motivation: To intervene early in slowing physical deterioration in older adults by predicting SPPB scores.

Method: Tested ML algorithms (Random Forest, XGBoost, Linear Regression, dense and TabNet neural networks) on questionnaire data. Used Shapley values for feature selection.

Result: XGBoost achieved the best performance (MAE: 0.79). Feature selection yielded slightly higher MAE (0.82).

Conclusion: XGBoost is effective for predicting SPPB scores, and feature selection maintains performance with fewer features.

Abstract: Effective slowing down of older adults\' physical capacity deterioration
requires intervention as soon as the first symptoms surface. In this paper, we
analyze the possibility of predicting the Short Physical Performance Battery
(SPPB) score at a four-year horizon based on questionnaire data. The ML
algorithms tested included Random Forest, XGBoost, Linear Regression, dense and
TabNet neural networks. The best results were achieved for the XGBoost (mean
absolute error of 0.79 points). Based on the Shapley values analysis, we
selected smaller subsets of features (from 10 to 20) and retrained the XGBoost
regressor, achieving a mean absolute error of 0.82.

</details>


### [100] [Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning](https://arxiv.org/abs/2508.05224)
*Mirko Konstantin,Anirban Mukhopadhyay*

Main category: cs.LG

TL;DR: Proposes LIGHTYEAR, a decentralized P2P FL framework using agreement scores for personalized, robust model updates.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of centralized FL like single-point failure, poor personalization, and vulnerability to adversarial clients.

Method: Uses P2P topology with agreement scores on local validation sets to select trustworthy updates, plus regularization for stability.

Result: Outperforms centralized and existing P2P FL methods, especially in adversarial and heterogeneous settings.

Conclusion: LIGHTYEAR offers a scalable, personalized, and robust alternative to traditional FL architectures.

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data privacy by keeping data local.
Traditional FL approaches rely on a centralized, star-shaped topology, where a
central server aggregates model updates from clients. However, this
architecture introduces several limitations, including a single point of
failure, limited personalization, and poor robustness to distribution shifts or
vulnerability to malfunctioning clients. Moreover, update selection in
centralized FL often relies on low-level parameter differences, which can be
unreliable when client data is not independent and identically distributed, and
offer clients little control. In this work, we propose a decentralized,
peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P
topology to enable each client to identify and aggregate a personalized set of
trustworthy and beneficial updates.This framework is the Local Inference Guided
Aggregation for Heterogeneous Training Environments to Yield Enhancement
Through Agreement and Regularization (LIGHTYEAR). Central to our method is an
agreement score, computed on a local validation set, which quantifies the
semantic alignment of incoming updates in the function space with respect to
the clients reference model. Each client uses this score to select a tailored
subset of updates and performs aggregation with a regularization term that
further stabilizes the training. Our empirical evaluation across two datasets
shows that the proposed approach consistently outperforms both centralized
baselines and existing P2P methods in terms of client-level performance,
particularly under adversarial and heterogeneous conditions.

</details>


### [101] [Tractable Sharpness-Aware Learning of Probabilistic Circuits](https://arxiv.org/abs/2508.05537)
*Hrithik Suresh,Sahil Sidheekh,Vishnu Shreeram M. P,Sriraam Natarajan,Narayanan C. Krishnan*

Main category: cs.LG

TL;DR: The paper addresses overfitting in Probabilistic Circuits (PCs) by proposing a Hessian-based regularizer, improving generalization by guiding PCs toward flatter minima.


<details>
  <summary>Details</summary>
Motivation: Overfitting in PCs, especially with limited data, is analyzed from a log-likelihood-landscape perspective, identifying sharp optima as the cause.

Method: A Hessian-based regularizer is introduced, leveraging the tractable trace of the Hessian in PCs to compute sharpness proxies efficiently.

Result: The method consistently guides PCs to flatter minima, enhancing generalization performance on synthetic and real-world datasets.

Conclusion: The proposed regularizer effectively mitigates overfitting in PCs, offering simple updates for EM and seamless integration with gradient-based methods.

Abstract: Probabilistic Circuits (PCs) are a class of generative models that allow
exact and tractable inference for a wide range of queries. While recent
developments have enabled the learning of deep and expressive PCs, this
increased capacity can often lead to overfitting, especially when data is
limited. We analyze PC overfitting from a log-likelihood-landscape perspective
and show that it is often caused by convergence to sharp optima that generalize
poorly. Inspired by sharpness aware minimization in neural networks, we propose
a Hessian-based regularizer for training PCs. As a key contribution, we show
that the trace of the Hessian of the log-likelihood-a sharpness proxy that is
typically intractable in deep neural networks-can be computed efficiently for
PCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer
that yields simple closed-form parameter updates for EM, and integrates
seamlessly with gradient based learning methods. Experiments on synthetic and
real-world datasets demonstrate that our method consistently guides PCs toward
flatter minima, improves generalization performance.

</details>


### [102] [Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs](https://arxiv.org/abs/2508.05232)
*Feifan Xia,Mingyang Liao,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.LG

TL;DR: Cross-LoRA enables transferring LoRA modules between diverse LLMs without additional data, using subspace alignment and projection.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of traditional PEFT methods like LoRA, which are tied to specific base model architectures.

Method: Introduces Cross-LoRA with LoRA-Align (subspace alignment via SVD) and LoRA-Shift (projection of aligned subspaces).

Result: Achieves up to 5.26% performance gain over base models and matches directly trained LoRA adapters.

Conclusion: Cross-LoRA is a lightweight, data-free solution for efficient adaptation across heterogeneous LLMs.

Abstract: Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are
tightly coupled with the base model architecture, which constrains their
applicability across heterogeneous pretrained large language models (LLMs). To
address this limitation, we introduce Cross-LoRA, a data-free framework for
transferring LoRA modules between diverse base models without requiring
additional training data. Cross-LoRA consists of two key components: (a)
LoRA-Align, which performs subspace alignment between source and target base
models through rank-truncated singular value decomposition (SVD) and
Frobenius-optimal linear transformation, ensuring compatibility under dimension
mismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project
source LoRA weight updates into the target model parameter space. Both
components are data-free, training-free, and enable lightweight adaptation on a
commodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that
Cross-LoRA achieves relative gains of up to 5.26% over base models. Across
other commonsense reasoning benchmarks, Cross-LoRA maintains performance
comparable to that of directly trained LoRA adapters.

</details>


### [103] [Adapting Vision-Language Models Without Labels: A Comprehensive Survey](https://arxiv.org/abs/2508.05547)
*Hao Dong,Lijun Sheng,Jian Liang,Ran He,Eleni Chatzi,Olga Fink*

Main category: cs.LG

TL;DR: A survey on unsupervised adaptation methods for Vision-Language Models (VLMs), proposing a taxonomy and analyzing four key paradigms to enhance performance without labeled data.


<details>
  <summary>Details</summary>
Motivation: VLMs show strong generalization but underperform in specific tasks without adaptation. Unsupervised methods are needed for data efficiency.

Method: Proposes a taxonomy of four paradigms (Data-Free Transfer, Unsupervised Domain Transfer, Episodic Test-Time Adaptation, Online Test-Time Adaptation) and reviews methodologies and benchmarks.

Result: Systematic overview of unsupervised VLM adaptation, highlighting methodologies and benchmarks.

Conclusion: Identifies open challenges and future directions, with a maintained literature repository.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable generalization
capabilities across a wide range of tasks. However, their performance often
remains suboptimal when directly applied to specific downstream scenarios
without task-specific adaptation. To enhance their utility while preserving
data efficiency, recent research has increasingly focused on unsupervised
adaptation methods that do not rely on labeled data. Despite the growing
interest in this area, there remains a lack of a unified, task-oriented survey
dedicated to unsupervised VLM adaptation. To bridge this gap, we present a
comprehensive and structured overview of the field. We propose a taxonomy based
on the availability and nature of unlabeled visual data, categorizing existing
approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised
Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),
and Online Test-Time Adaptation (streaming data). Within this framework, we
analyze core methodologies and adaptation strategies associated with each
paradigm, aiming to establish a systematic understanding of the field.
Additionally, we review representative benchmarks across diverse applications
and highlight open challenges and promising directions for future research. An
actively maintained repository of relevant literature is available at
https://github.com/tim-learn/Awesome-LabelFree-VLMs.

</details>


### [104] [MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs](https://arxiv.org/abs/2508.05257)
*Xiaodong Chen,Mingming Ha,Zhenzhong Lan,Jing Zhang,Jianguo Li*

Main category: cs.LG

TL;DR: The paper introduces Mixture-of-Basis-Experts (MoBE), a novel method for compressing large MoE-based LLMs with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the high memory requirements and accuracy drops in existing MoE compression methods.

Method: Decomposes expert matrices into unique and shared basis components, minimizing reconstruction error.

Result: MoBE reduces parameters by 24%-30% with only 1%-2% accuracy drop, outperforming prior methods.

Conclusion: MoBE offers an efficient solution for compressing large MoE models while preserving accuracy.

Abstract: The Mixture-of-Experts (MoE) architecture has become a predominant paradigm
for scaling large language models (LLMs). Despite offering strong performance
and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and
Kimi-K2-Instruct present serious challenges due to substantial memory
requirements in deployment. While recent works have explored MoE compression to
address this issue, existing methods often suffer from considerable accuracy
drops (e.g., 7-14% relatively) even at modest compression rates. This paper
introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model
compression while incurring minimal accuracy drops. Specifically, each up/gate
matrix in an expert is decomposed via a rank decomposition as W = AB, where
matrix A is unique to each expert. The relatively larger matrix B is further
re-parameterized as a linear combination of basis matrices {Bi} shared across
all experts within a given MoE layer. The factorization is learned by
minimizing the reconstruction error relative to the original weight matrices.
Experiments demonstrate that MoBE achieves notably lower accuracy drops
compared to prior works. For instance, MoBE can reduce the parameter counts of
Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by
24%-30% with only 1%-2% accuracy drop (about 2% drops when measured
relatively).

</details>


### [105] [Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models](https://arxiv.org/abs/2508.05581)
*Guilherme Seidyo Imai Aldeia,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: LLMs can generate interpretable computable phenotypes (CPs) for clinical use, with iterative refinement improving accuracy and reducing training needs.


<details>
  <summary>Details</summary>
Motivation: Explore LLMs' potential for generating accurate and concise CPs to enhance clinical decision support for hypertension patients.

Method: Propose a synthesize, execute, debug, instruct strategy for iterative CP refinement using LLMs and data-driven feedback.

Result: LLMs with iterative learning produce reasonably accurate CPs, nearing state-of-the-art ML performance with fewer training examples.

Conclusion: LLMs show promise for scalable CP generation in clinical settings, offering interpretability and efficiency.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities for
medical question answering and programming, but their potential for generating
interpretable computable phenotypes (CPs) is under-explored. In this work, we
investigate whether LLMs can generate accurate and concise CPs for six clinical
phenotypes of varying complexity, which could be leveraged to enable scalable
clinical decision support to improve care for patients with hypertension. In
addition to evaluating zero-short performance, we propose and test a
synthesize, execute, debug, instruct strategy that uses LLMs to generate and
iteratively refine CPs using data-driven feedback. Our results show that LLMs,
coupled with iterative learning, can generate interpretable and reasonably
accurate programs that approach the performance of state-of-the-art ML methods
while requiring significantly fewer training examples.

</details>


### [106] [Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle](https://arxiv.org/abs/2508.05612)
*Linghao Zhu,Yiran Guan,Dingkang Liang,Jianzhong Ju,Zhenbo Luo,Bin Qin,Jian Luan,Yuliang Liu,Xiang Bai*

Main category: cs.LG

TL;DR: Shuffle-R1 improves RL fine-tuning for MLLMs by addressing Advantage Collapsing and Rollout Silencing through dynamic trajectory sampling and batch shuffling.


<details>
  <summary>Details</summary>
Motivation: Current RL pipelines for MLLMs suffer from inefficiencies due to Advantage Collapsing and Rollout Silencing, leading to suboptimal learning.

Method: Proposes Pairwise Trajectory Sampling and Advantage-based Trajectory Shuffle to enhance gradient signal and rollout exposure.

Result: Outperforms RL baselines on reasoning benchmarks with minimal overhead.

Conclusion: Data-centric adaptations like Shuffle-R1 are crucial for efficient RL training in MLLMs.

Abstract: Reinforcement learning (RL) has emerged as an effective post-training
paradigm for enhancing the reasoning capabilities of multimodal large language
model (MLLM). However, current RL pipelines often suffer from training
inefficiencies caused by two underexplored issues: Advantage Collapsing, where
most advantages in a batch concentrate near zero, and Rollout Silencing, where
the proportion of rollouts contributing non-zero gradients diminishes over
time. These issues lead to suboptimal gradient updates and hinder long-term
learning efficiency. To address these issues, we propose Shuffle-R1, a simple
yet principled framework that improves RL fine-tuning efficiency by dynamically
restructuring trajectory sampling and batch composition. It introduces (1)
Pairwise Trajectory Sampling, which selects high-contrast trajectories with
large advantages to improve gradient signal quality, and (2) Advantage-based
Trajectory Shuffle, which increases exposure of valuable rollouts through
informed batch reshuffling. Experiments across multiple reasoning benchmarks
show that our framework consistently outperforms strong RL baselines with
minimal overhead. These results highlight the importance of data-centric
adaptations for more efficient RL training in MLLM.

</details>


### [107] [TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution](https://arxiv.org/abs/2508.05616)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.LG

TL;DR: TrajEvo uses LLMs and evolutionary algorithms to design trajectory prediction heuristics, outperforming traditional and deep learning methods, especially in OOD scenarios.


<details>
  <summary>Details</summary>
Motivation: Improving accuracy, generalizability, and explainability in trajectory prediction for safety-critical domains like robotics and autonomous vehicles.

Method: TrajEvo employs an evolutionary algorithm with Cross-Generation Elite Sampling and a Statistics Feedback Loop to refine heuristics using LLMs.

Result: TrajEvo outperforms existing heuristic and deep learning methods, particularly in OOD generalization.

Conclusion: TrajEvo advances automated, fast, explainable, and generalizable trajectory prediction heuristics.

Abstract: Trajectory prediction is a critical task in modeling human behavior,
especially in safety-critical domains such as social robotics and autonomous
vehicle navigation. Traditional heuristics based on handcrafted rules often
lack accuracy and generalizability. Although deep learning approaches offer
improved performance, they typically suffer from high computational cost,
limited explainability, and, importantly, poor generalization to
out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a
framework that leverages Large Language Models (LLMs) to automatically design
trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to
generate and refine prediction heuristics from past trajectory data. We propose
two key innovations: Cross-Generation Elite Sampling to encourage population
diversity, and a Statistics Feedback Loop that enables the LLM to analyze and
improve alternative predictions. Our evaluations demonstrate that TrajEvo
outperforms existing heuristic methods across multiple real-world datasets, and
notably surpasses both heuristic and deep learning methods in generalizing to
an unseen OOD real-world dataset. TrajEvo marks a promising step toward the
automated design of fast, explainable, and generalizable trajectory prediction
heuristics. We release our source code to facilitate future research at
https://github.com/ai4co/trajevo.

</details>


### [108] [RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders](https://arxiv.org/abs/2508.05289)
*Zhongheng Yang,Aijia Sun,Yushang Zhao,Yinuo Yang,Dannier Li,Chengrui Zhou*

Main category: cs.LG

TL;DR: The paper proposes using RLHF to align LLM-based CRS with user preferences by leveraging implicit feedback signals, outperforming traditional methods in accuracy and satisfaction.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised fine-tuning fails to capture implicit user feedback like dwell time or sentiment, limiting CRS performance.

Method: Uses RLHF with a reward model trained on weakly-labeled engagement data and optimizes the LLM via PPO, modeling conversational state transitions.

Result: RLHF-fine-tuned models show better top-k recommendation accuracy, coherence, and user satisfaction on synthetic and real-world datasets.

Conclusion: Implicit signal alignment via RLHF enables scalable and user-adaptive CRS design.

Abstract: Conversational recommender systems (CRS) based on Large Language Models
(LLMs) need to constantly be aligned to the user preferences to provide
satisfying and context-relevant item recommendations. The traditional
supervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell
time, sentiment polarity, or engagement patterns. In this paper, we share a
fine-tuning solution using human feedback reinforcement learning (RLHF) to
maximize implied user feedback (IUF) in a multi-turn recommendation context. We
specify a reward model $R_{\phi}$ learnt on weakly-labelled engagement
information and maximize user-centric utility by optimizing the foundational
LLM M_{\theta} through a proximal policy optimization (PPO) approach. The
architecture models conversational state transitions $s_t \to a_t \to s_{t
+1}$, where the action $a_t$ is associated with LLM-generated item suggestions
only on condition of conversation history in the past. The evaluation across
synthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that
our RLHF-fine-tuned models can perform better in terms of top-$k$
recommendation accuracy, coherence, and user satisfaction compared to
(arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give
up This paper shows that implicit signal alignment can be efficient in
achieving scalable and user-adaptive design of CRS.

</details>


### [109] [Optimal Growth Schedules for Batch Size and Learning Rate in SGD that Reduce SFO Complexity](https://arxiv.org/abs/2508.05297)
*Hikaru Umeda,Hideaki Iiduka*

Main category: cs.LG

TL;DR: The paper explores optimal batch-size and learning-rate scheduling in deep learning to balance efficiency and convergence, reducing computational bottlenecks.


<details>
  <summary>Details</summary>
Motivation: The growth of deep learning models has created computational challenges, with naive hyperparameter scheduling harming efficiency and generalization.

Method: The study uses stochastic first-order oracle (SFO) complexity to derive and validate optimal growth schedules for batch size and learning rate.

Result: Theoretical and experimental results show improved efficiency and convergence with the proposed schedules.

Conclusion: The findings provide scalable and efficient guidelines for large-batch training in deep learning.

Abstract: The unprecedented growth of deep learning models has enabled remarkable
advances but introduced substantial computational bottlenecks. A key factor
contributing to training efficiency is batch-size and learning-rate scheduling
in stochastic gradient methods. However, naive scheduling of these
hyperparameters can degrade optimization efficiency and compromise
generalization. Motivated by recent theoretical insights, we investigated how
the batch size and learning rate should be increased during training to balance
efficiency and convergence. We analyzed this problem on the basis of stochastic
first-order oracle (SFO) complexity, defined as the expected number of gradient
evaluations needed to reach an $\epsilon$-approximate stationary point of the
empirical loss. We theoretically derived optimal growth schedules for the batch
size and learning rate that reduce SFO complexity and validated them through
extensive experiments. Our results offer both theoretical insights and
practical guidelines for scalable and efficient large-batch training in deep
learning.

</details>


### [110] [Adaptive Batch Size and Learning Rate Scheduler for Stochastic Gradient Descent Based on Minimization of Stochastic First-order Oracle Complexity](https://arxiv.org/abs/2508.05302)
*Hikaru Umeda,Hideaki Iiduka*

Main category: cs.LG

TL;DR: The paper explores how adjusting batch size and learning rate in SGD, based on critical batch size theory, improves convergence speed.


<details>
  <summary>Details</summary>
Motivation: To enhance SGD efficiency by leveraging theoretical insights on critical batch size and adaptive scheduling.

Method: Introduces an adaptive joint scheduler for batch size and learning rate, using full gradient norm decay during training.

Result: Experiments show faster convergence compared to existing schedulers.

Conclusion: Adaptive scheduling based on critical batch size theory effectively accelerates SGD.

Abstract: The convergence behavior of mini-batch stochastic gradient descent (SGD) is
highly sensitive to the batch size and learning rate settings. Recent
theoretical studies have identified the existence of a critical batch size that
minimizes stochastic first-order oracle (SFO) complexity, defined as the
expected number of gradient evaluations required to reach a stationary point of
the empirical loss function in a deep neural network. An adaptive scheduling
strategy is introduced to accelerate SGD that leverages theoretical findings on
the critical batch size. The batch size and learning rate are adjusted on the
basis of the observed decay in the full gradient norm during training.
Experiments using an adaptive joint scheduler based on this strategy
demonstrated improved convergence speed compared with that of existing
schedulers.

</details>


### [111] [Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning](https://arxiv.org/abs/2508.05316)
*Yue Duan,Taicai Chen,Lei Qi,Yinghuan Shi*

Main category: cs.LG

TL;DR: USP is a framework for semi-supervised continual learning (SSCL) that improves learning plasticity, unlabeled learning, and memory stability through three strategies: FSR, DCP, and CUD.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of SSCL by synergistically enhancing learning plasticity, unlabeled learning, and memory stability, reducing annotation costs and managing continual data arrival.

Method: USP employs three strategies: (1) FSR for learning plasticity, (2) DCP for unlabeled learning, and (3) CUD for memory stability.

Result: USP outperforms prior SSCL methods, achieving up to 5.94% higher accuracy in the last task.

Conclusion: USP effectively addresses SSCL challenges, validating its superiority over existing methods.

Abstract: Semi-supervised continual learning (SSCL) seeks to leverage both labeled and
unlabeled data in a sequential learning setup, aiming to reduce annotation
costs while managing continual data arrival. SSCL introduces complex
challenges, including ensuring effective unlabeled learning (UL), while
balancing memory stability (MS) and learning plasticity (LP). Previous SSCL
efforts have typically focused on isolated aspects of the three, while this
work presents USP, a divide-and-conquer framework designed to synergistically
enhance these three aspects: (1) Feature Space Reservation (FSR) strategy for
LP, which constructs reserved feature locations for future classes by shaping
old classes into an equiangular tight frame; (2) Divide-and-Conquer
Pseudo-labeling (DCP) approach for UL, which assigns reliable pseudo-labels
across both high- and low-confidence unlabeled data; and (3)
Class-mean-anchored Unlabeled Distillation (CUD) for MS, which reuses DCP's
outputs to anchor unlabeled data to stable class means for distillation to
prevent forgetting. Comprehensive evaluations show USP outperforms prior SSCL
methods, with gains up to 5.94% in the last accuracy, validating its
effectiveness. The code is available at https://github.com/NJUyued/USP4SSCL.

</details>


### [112] [Latent Preference Bandits](https://arxiv.org/abs/2508.05367)
*Newton Mwai,Emil Carlsson,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: The paper proposes relaxing the assumptions of latent bandits to require only a model of action preference ordering, improving performance when reward scales vary.


<details>
  <summary>Details</summary>
Motivation: Learning from scratch is costly for personalization tasks with limited decision points. Latent bandits reduce exploration time but require accurate joint distribution models, which may not exist.

Method: A posterior-sampling algorithm is introduced, requiring only a model of action preference ordering in each latent state.

Result: The algorithm performs competitively with latent bandits when reward distributions are well-specified and outperforms them when reward scales differ.

Conclusion: Relaxing assumptions to focus on preference ordering improves adaptability and performance in scenarios with varying reward scales.

Abstract: Bandit algorithms are guaranteed to solve diverse sequential decision-making
problems, provided that a sufficient exploration budget is available. However,
learning from scratch is often too costly for personalization tasks where a
single individual faces only a small number of decision points. Latent bandits
offer substantially reduced exploration times for such problems, given that the
joint distribution of a latent state and the rewards of actions is known and
accurate. In practice, finding such a model is non-trivial, and there may not
exist a small number of latent states that explain the responses of all
individuals. For example, patients with similar latent conditions may have the
same preference in treatments but rate their symptoms on different scales. With
this in mind, we propose relaxing the assumptions of latent bandits to require
only a model of the \emph{preference ordering} of actions in each latent state.
This allows problem instances with the same latent state to vary in their
reward distributions, as long as their preference orderings are equal. We give
a posterior-sampling algorithm for this problem and demonstrate that its
empirical performance is competitive with latent bandits that have full
knowledge of the reward distribution when this is well-specified, and
outperforms them when reward scales differ between instances with the same
latent state.

</details>


### [113] [NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning](https://arxiv.org/abs/2508.05404)
*Wenjie Huo,Katinka Wolter*

Main category: cs.LG

TL;DR: Proposes NT-ML, a defense against backdoor attacks in DNNs, using non-target label training and mutual learning to purify models.


<details>
  <summary>Details</summary>
Motivation: DNNs are vulnerable to backdoor attacks; existing defenses may not handle advanced attacks effectively.

Method: NT-ML involves retraining with non-target labels and mutual learning between teacher (clean data accuracy) and student (poisoned data confidence) models.

Result: NT-ML defends against 6 backdoor attacks with few clean samples, outperforming 5 state-of-the-art defenses.

Conclusion: NT-ML is an effective and efficient defense mechanism against advanced backdoor attacks.

Abstract: Recent studies have shown that deep neural networks (DNNs) are vulnerable to
backdoor attacks, where a designed trigger is injected into the dataset,
causing erroneous predictions when activated. In this paper, we propose a novel
defense mechanism, Non-target label Training and Mutual Learning (NT-ML), which
can successfully restore the poisoned model under advanced backdoor attacks. NT
aims to reduce the harm of poisoned data by retraining the model with the
outputs of the standard training. At this stage, a teacher model with high
accuracy on clean data and a student model with higher confidence in correct
prediction on poisoned data are obtained. Then, the teacher and student can
learn the strengths from each other through ML to obtain a purified student
model. Extensive experiments show that NT-ML can effectively defend against 6
backdoor attacks with a small number of clean samples, and outperforms 5
state-of-the-art backdoor defenses.

</details>


### [114] [Cumulative Learning Rate Adaptation: Revisiting Path-Based Schedules for SGD and Adam](https://arxiv.org/abs/2508.05408)
*Asma Atamna,Tom Maus,Fabian Kievelitz,Tobias Glasmachers*

Main category: cs.LG

TL;DR: The paper examines adaptive learning rate mechanisms in deep learning, focusing on a 2017 path-based method. It identifies inconsistencies in its application to Adam and proposes a corrected variant, benchmarking it against SGD and Adam with and without adaptation.


<details>
  <summary>Details</summary>
Motivation: To evaluate the practical utility of adaptive learning rate mechanisms, especially in dynamic loss landscapes, and address inconsistencies in existing methods like the 2017 path-based approach for Adam.

Method: Revisits a cumulative path-based adaptation scheme, proposes a corrected variant for Adam, and benchmarks it against SGD, Adam, and a recent alternative method.

Result: The corrected variant better aligns with Adam's dynamics, and benchmarking clarifies when adaptive learning rate strategies are beneficial.

Conclusion: Adaptive learning rate mechanisms can be practical, but their effectiveness depends on the optimizer and problem context, with the proposed variant improving consistency for Adam.

Abstract: The learning rate is a crucial hyperparameter in deep learning, with its
ideal value depending on the problem and potentially changing during training.
In this paper, we investigate the practical utility of adaptive learning rate
mechanisms that adjust step sizes dynamically in response to the loss
landscape. We revisit a cumulative path-based adaptation scheme proposed in
2017, which adjusts the learning rate based on the discrepancy between the
observed path length, computed as a time-discounted sum of normalized gradient
steps, and the expected length of a random walk. While the original approach
offers a compelling intuition, we show that its adaptation mechanism for Adam
is conceptually inconsistent due to the optimizer's internal preconditioning.
We propose a corrected variant that better reflects Adam's update dynamics. To
assess the practical value of online learning rate adaptation, we benchmark SGD
and Adam, with and without cumulative adaptation, and compare them to a recent
alternative method. Our results aim to clarify when and why such adaptive
strategies offer practical benefits.

</details>


### [115] [MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow](https://arxiv.org/abs/2508.05411)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Main category: cs.LG

TL;DR: A causality-aware framework for molecular generation from text, combining a Causality-Aware Transformer and Variational Mean Flow, outperforms baselines in quality, diversity, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ensuring high-quality, diverse molecular generation from text with fast inference, which existing methods struggle with.

Method: Proposes a Causality-Aware Transformer (CAT) for joint encoding of molecular graphs and text, and a Variational Mean Flow (VMF) framework for efficient, expressive latent modeling.

Result: Achieves higher novelty (74.5%), diversity (70.3%), and 100% validity, with computational efficiency (1-5 NFEs).

Conclusion: The framework effectively balances quality, diversity, and speed, outperforming state-of-the-art methods.

Abstract: Molecular generation conditioned on textual descriptions is a fundamental
task in computational chemistry and drug discovery. Existing methods often
struggle to simultaneously ensure high-quality, diverse generation and fast
inference. In this work, we propose a novel causality-aware framework that
addresses these challenges through two key innovations. First, we introduce a
Causality-Aware Transformer (CAT) that jointly encodes molecular graph tokens
and text instructions while enforcing causal dependencies during generation.
Second, we develop a Variational Mean Flow (VMF) framework that generalizes
existing flow-based methods by modeling the latent space as a mixture of
Gaussians, enhancing expressiveness beyond unimodal priors. VMF enables
efficient one-step inference while maintaining strong generation quality and
diversity. Extensive experiments on four standard molecular benchmarks
demonstrate that our model outperforms state-of-the-art baselines, achieving
higher novelty (up to 74.5\%), diversity (up to 70.3\%), and 100\% validity
across all datasets. Moreover, VMF requires only one number of function
evaluation (NFE) during conditional generation and up to five NFEs for
unconditional generation, offering substantial computational efficiency over
diffusion-based methods.

</details>


### [116] [Echo State Networks for Bitcoin Time Series Prediction](https://arxiv.org/abs/2508.05416)
*Mansi Sharma,Enrico Sartor,Marc Cavazza,Helmut Prendinger*

Main category: cs.LG

TL;DR: The paper explores Echo State Networks (ESNs) for forecasting cryptocurrency prices during extreme volatility, outperforming other methods and showing robustness in chaotic periods.


<details>
  <summary>Details</summary>
Motivation: Forecasting stock and cryptocurrency prices is difficult due to volatility and non-stationarity. ESNs have shown promise in modeling such data, but their application to cryptocurrencies, especially during chaos, is underexplored.

Method: The study uses ESNs for cryptocurrency forecasting and includes chaos analysis via the Lyapunov exponent to evaluate performance during volatile periods.

Result: ESNs outperform existing machine learning methods, particularly in chaotic conditions, aligning with Lyapunov exponent analysis.

Conclusion: ESNs are robust and effective for cryptocurrency forecasting, especially during extreme volatility, making them a superior choice over traditional methods.

Abstract: Forecasting stock and cryptocurrency prices is challenging due to high
volatility and non-stationarity, influenced by factors like economic changes
and market sentiment. Previous research shows that Echo State Networks (ESNs)
can effectively model short-term stock market movements, capturing nonlinear
patterns in dynamic data. To the best of our knowledge, this work is among the
first to explore ESNs for cryptocurrency forecasting, especially during extreme
volatility. We also conduct chaos analysis through the Lyapunov exponent in
chaotic periods and show that our approach outperforms existing machine
learning methods by a significant margin. Our findings are consistent with the
Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods
and excel under high chaos compared to Boosting and Na\"ive methods.

</details>


### [117] [Negative Binomial Variational Autoencoders for Overdispersed Latent Modeling](https://arxiv.org/abs/2508.05423)
*Yixuan Zhang,Wenxin Zhang,Hua Jiang,Quyu Kong,Feng Zhou*

Main category: cs.LG

TL;DR: NegBio-VAE extends VAEs by using a negative binomial distribution for spike count modeling, improving accuracy by addressing overdispersion in neural activity.


<details>
  <summary>Details</summary>
Motivation: Current VAEs, like Poisson-VAE, inadequately model neural spike variability due to rigid Poisson constraints (equal mean and variance).

Method: Introduces NegBio-VAE with a negative binomial distribution, two ELBO optimization schemes, and differentiable reparameterization strategies.

Result: Empirical results show significant gains in reconstruction fidelity by modeling overdispersion.

Conclusion: NegBio-VAE effectively captures neural variability, outperforming Poisson-based models.

Abstract: Biological neurons communicate through spike trains, discrete, irregular
bursts of activity that exhibit variability far beyond the modeling capacity of
conventional variational autoencoders (VAEs). Recent work, such as the
Poisson-VAE, makes a biologically inspired move by modeling spike counts using
the Poisson distribution. However, they impose a rigid constraint: equal mean
and variance, which fails to reflect the true stochastic nature of neural
activity. In this work, we challenge this constraint and introduce NegBio-VAE,
a principled extension of the VAE framework that models spike counts using the
negative binomial distribution. This shift grants explicit control over
dispersion, unlocking a broader and more accurate family of neural
representations. We further develop two ELBO optimization schemes and two
differentiable reparameterization strategies tailored to the negative binomial
setting. By introducing one additional dispersion parameter, NegBio-VAE
generalizes the Poisson latent model to a negative binomial formulation.
Empirical results demonstrate this minor yet impactful change leads to
significant gains in reconstruction fidelity, highlighting the importance of
explicitly modeling overdispersion in spike-like activations.

</details>


### [118] [Federated Multi-Objective Learning with Controlled Pareto Frontiers](https://arxiv.org/abs/2508.05424)
*Jiansheng Rao,Jiayi Li,Zhizhi Gong,Soummya Kar,Haoxuan Li*

Main category: cs.LG

TL;DR: CR-FMOL introduces a federated multi-objective learning framework with a preference-cone constraint to ensure client-wise Pareto optimality, improving fairness in federated learning.


<details>
  <summary>Details</summary>
Motivation: Address the issue of FedAvg under-serving minority clients in federated learning by enforcing client-wise Pareto optimality.

Method: Uses conically-regularised FMOL with preference-cone constraints and solves a cone-constrained Pareto-MTL sub-problem after local FMGDA/FSMGDA steps.

Result: Enhances client fairness on non-IID benchmarks, with comparable accuracy to FedAvg given sufficient training rounds.

Conclusion: CR-FMOL is a promising approach for fair federated learning, though early-stage performance may lag slightly.

Abstract: Federated learning (FL) is a widely adopted paradigm for privacy-preserving
model training, but FedAvg optimise for the majority while under-serving
minority clients. Existing methods such as federated multi-objective learning
(FMOL) attempts to import multi-objective optimisation (MOO) into FL. However,
it merely delivers task-wise Pareto-stationary points, leaving client fairness
to chance. In this paper, we introduce Conically-Regularised FMOL (CR-FMOL),
the first federated MOO framework that enforces client-wise Pareto optimality
through a novel preference-cone constraint. After local federated
multi-gradient descent averaging (FMGDA) / federated stochastic multi-gradient
descent averaging (FSMGDA) steps, each client transmits its aggregated
task-loss vector as an implicit preference; the server then solves a
cone-constrained Pareto-MTL sub-problem centred at the uniform vector,
producing a descent direction that is Pareto-stationary for every client within
its cone. Experiments on non-IID benchmarks show that CR-FMOL enhances client
fairness, and although the early-stage performance is slightly inferior to
FedAvg, it is expected to achieve comparable accuracy given sufficient training
rounds.

</details>


### [119] [Group Causal Policy Optimization for Post-Training Large Language Models](https://arxiv.org/abs/2508.05428)
*Ziyin Gu,Jingyao Wang,Ran Zuo,Chuxiong Sun,Zeen Song,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: GCPO improves upon GRPO by incorporating causal structure into optimization, enhancing prediction quality and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Specialized domains require targeted post-training for LLMs, and GRPO's independence assumption overlooks semantic interactions among candidate responses.

Method: Introduces a Structural Causal Model (SCM) to reveal hidden dependencies, leading to causally informed reward adjustment and KL regularization in GCPO.

Result: GCPO consistently outperforms GRPO and other methods across reasoning benchmarks.

Conclusion: Integrating causal structure into optimization (GCPO) addresses GRPO's limitations and improves performance.

Abstract: Recent advances in large language models (LLMs) have broadened their
applicability across diverse tasks, yet specialized domains still require
targeted post training. Among existing methods, Group Relative Policy
Optimization (GRPO) stands out for its efficiency, leveraging groupwise
relative rewards while avoiding costly value function learning. However, GRPO
treats candidate responses as independent, overlooking semantic interactions
such as complementarity and contradiction. To address this challenge, we first
introduce a Structural Causal Model (SCM) that reveals hidden dependencies
among candidate responses induced by conditioning on a final integrated output
forming a collider structure. Then, our causal analysis leads to two insights:
(1) projecting responses onto a causally informed subspace improves prediction
quality, and (2) this projection yields a better baseline than query only
conditioning. Building on these insights, we propose Group Causal Policy
Optimization (GCPO), which integrates causal structure into optimization
through two key components: a causally informed reward adjustment and a novel
KL regularization term that aligns the policy with a causally projected
reference distribution. Comprehensive experimental evaluations demonstrate that
GCPO consistently surpasses existing methods, including GRPO across multiple
reasoning benchmarks.

</details>


### [120] [Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search](https://arxiv.org/abs/2508.05433)
*Qinglong Hu,Xialiang Tong,Mingxuan Yuan,Fei Liu,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: MLES introduces a programmatic policy discovery method using multimodal LLMs and evolutionary search, balancing performance and interpretability for control tasks.


<details>
  <summary>Details</summary>
Motivation: Deep reinforcement learning lacks interpretability, hindering trust and deployment in safety-critical tasks. MLES aims to address this gap.

Method: MLES combines multimodal LLMs as policy generators with evolutionary mechanisms and visual feedback-driven behavior analysis for optimization.

Result: MLES matches PPO in performance and efficiency while providing transparent, human-aligned policies and scalable solutions.

Conclusion: MLES is a promising approach for interpretable control policy discovery, overcoming domain-specific language limitations and enabling knowledge transfer.

Abstract: Interpretability and high performance are essential goals in designing
control policies, particularly for safety-critical tasks. Deep reinforcement
learning has greatly enhanced performance, yet its inherent lack of
interpretability often undermines trust and hinders real-world deployment. This
work addresses these dual challenges by introducing a novel approach for
programmatic policy discovery, called Multimodal Large Language Model-assisted
Evolutionary Search (MLES). MLES utilizes multimodal large language models as
policy generators, combining them with evolutionary mechanisms for automatic
policy optimization. It integrates visual feedback-driven behavior analysis
within the policy generation process to identify failure patterns and
facilitate targeted improvements, enhancing the efficiency of policy discovery
and producing adaptable, human-aligned policies. Experimental results show that
MLES achieves policy discovery capabilities and efficiency comparable to
Proximal Policy Optimization (PPO) across two control tasks, while offering
transparent control logic and traceable design processes. This paradigm
overcomes the limitations of predefined domain-specific languages, facilitates
knowledge transfer and reuse, and is scalable across various control tasks.
MLES shows promise as a leading approach for the next generation of
interpretable control policy discovery.

</details>


### [121] [Competing Risks: Impact on Risk Estimation and Algorithmic Fairness](https://arxiv.org/abs/2508.05435)
*Vincent Jeanselme,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: The paper highlights the bias and disparities caused by treating competing risks as censoring in survival analysis, proposing a framework to quantify and address these issues.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked consequences of misclassifying competing risks as censoring, which leads to biased survival estimates and amplifies disparities.

Method: Develops a theoretical framework to quantify the error from misclassifying competing risks as censoring and examines its impact on predictive performance and fairness.

Result: Demonstrates that ignoring competing risks introduces substantial bias, overestimates risk, and exacerbates disparities, especially in high-risk groups.

Conclusion: Practitioners must account for competing risks to improve accuracy, reduce disparities, and inform better decisions in survival analysis.

Abstract: Accurate time-to-event prediction is integral to decision-making, informing
medical guidelines, hiring decisions, and resource allocation. Survival
analysis, the quantitative framework used to model time-to-event data, accounts
for patients who do not experience the event of interest during the study
period, known as censored patients. However, many patients experience events
that prevent the observation of the outcome of interest. These competing risks
are often treated as censoring, a practice frequently overlooked due to a
limited understanding of its consequences. Our work theoretically demonstrates
why treating competing risks as censoring introduces substantial bias in
survival estimates, leading to systematic overestimation of risk and,
critically, amplifying disparities. First, we formalize the problem of
misclassifying competing risks as censoring and quantify the resulting error in
survival estimates. Specifically, we develop a framework to estimate this error
and demonstrate the associated implications for predictive performance and
algorithmic fairness. Furthermore, we examine how differing risk profiles
across demographic groups lead to group-specific errors, potentially
exacerbating existing disparities. Our findings, supported by an empirical
analysis of cardiovascular management, demonstrate that ignoring competing
risks disproportionately impacts the individuals most at risk of these events,
potentially accentuating inequity. By quantifying the error and highlighting
the fairness implications of the common practice of considering competing risks
as censoring, our work provides a critical insight into the development of
survival models: practitioners must account for competing risks to improve
accuracy, reduce disparities in risk assessment, and better inform downstream
decisions.

</details>


### [122] [Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes](https://arxiv.org/abs/2508.05469)
*Zachary Robertson,Sanmi Koyejo*

Main category: cs.LG

TL;DR: The paper introduces gaming-resistant mechanisms for evaluating AI systems without ground truth, using f-mutual information measures. These mechanisms outperform current practices in robustness and discrimination.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating AI systems when ground truth is unavailable, ensuring resistance to gaming and maintaining output quality.

Method: Exploits the connection between gaming resistance and output quality, using f-mutual information measures under natural conditions. Empirical tests span ten domains.

Result: Information-theoretic mechanisms achieve perfect discrimination between faithful and strategic agents, with 10-100x better robustness to adversarial manipulation. Performance peaks at a 10:1 compression ratio.

Conclusion: The proposed mechanisms are uniquely gaming-resistant and effective, offering superior robustness and discrimination in AI evaluation without ground truth.

Abstract: We develop mechanisms for evaluating AI systems without ground truth by
exploiting a connection between gaming resistance and output quality. The data
processing inequality ensures post-hoc attempts to game a metric degrades both
information content and task performance. We prove that f-mutual information
measures are the unique gaming resistant mechanisms under natural conditions,
with the overseer acting as an agent. While Shannon mutual information faces
exponential sample complexity, bounded measures like total variation distance
remain tractable. Empirically, across ten domains from translation to peer
review, all information-theoretic mechanisms achieve perfect discrimination (d
> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit
systematic evaluation inversion, preferring fabricated content over accurate
summaries. Our mechanisms show 10-100x better robustness to adversarial
manipulation than current practices. We also find performance follows an
inverted-U curve with compression ratio, peaking at 10:1 where agent responses
exhibit optimal information diversity (3 effective dimensions), giving a
bias-variance perspective on when our approach is expected to be most
effective.

</details>


### [123] [Prediction of Survival Outcomes under Clinical Presence Shift: A Joint Neural Network Architecture](https://arxiv.org/abs/2508.05472)
*Vincent Jeanselme,Glen Martin,Matthew Sperrin,Niels Peek,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: The paper proposes a multi-task recurrent neural network to model clinical presence in EHRs, improving prediction model performance and transportability.


<details>
  <summary>Details</summary>
Motivation: Clinical presence in EHRs impacts outcomes but is often overlooked, limiting model performance and transportability.

Method: A multi-task recurrent neural network jointly models inter-observation time, missingness, and survival outcomes.

Result: The method improves performance and transportability in mortality prediction on the MIMIC-III dataset.

Conclusion: Incorporating clinical presence enhances model performance and transportability in clinical prediction tasks.

Abstract: Electronic health records arise from the complex interaction between patients
and the healthcare system. This observation process of interactions, referred
to as clinical presence, often impacts observed outcomes. When using electronic
health records to develop clinical prediction models, it is standard practice
to overlook clinical presence, impacting performance and limiting the
transportability of models when this interaction evolves. We propose a
multi-task recurrent neural network that jointly models the inter-observation
time and the missingness processes characterising this interaction in parallel
to the survival outcome of interest. Our work formalises the concept of
clinical presence shift when the prediction model is deployed in new settings
(e.g. different hospitals, regions or countries), and we theoretically justify
why the proposed joint modelling can improve transportability under changes in
clinical presence. We demonstrate, in a real-world mortality prediction task in
the MIMIC-III dataset, how the proposed strategy improves performance and
transportability compared to state-of-the-art prediction models that do not
incorporate the observation process. These results emphasise the importance of
leveraging clinical presence to improve performance and create more
transportable clinical prediction models.

</details>


### [124] [Parameter-free entropy-regularized multi-view clustering with hierarchical feature selection](https://arxiv.org/abs/2508.05504)
*Kristina P. Sinaga,Sara Colantonio,Miin-Shen Yang*

Main category: cs.LG

TL;DR: The paper introduces two parameter-free algorithms, AMVFCM-U and AAMVFCM-U, for multi-view clustering, addressing challenges like high-dimensional data and cross-view integration. The methods use entropy regularization and SNR-based feature weighting for adaptive consensus, with AAMVFCM-U adding hierarchical dimensionality reduction. Results show significant improvements in efficiency and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Multi-view clustering struggles with heterogeneous data, high-dimensional features, and lack of principled cross-view integration. Traditional methods require manual tuning and lack adaptive mechanisms.

Method: Two algorithms are proposed: AMVFCM-U uses entropy regularization and SNR-based feature weighting for adaptive consensus. AAMVFCM-U extends this with hierarchical dimensionality reduction via adaptive thresholding.

Result: The methods outperform 15 state-of-the-art techniques, achieving up to 97% computational efficiency, reducing dimensionality to 0.45% of original size, and identifying optimal view combinations.

Conclusion: The proposed framework provides a unified, parameter-free solution for multi-view clustering, offering adaptive consensus, efficiency, and superior performance.

Abstract: Multi-view clustering faces critical challenges in automatically discovering
patterns across heterogeneous data while managing high-dimensional features and
eliminating irrelevant information. Traditional approaches suffer from manual
parameter tuning and lack principled cross-view integration mechanisms. This
work introduces two complementary algorithms: AMVFCM-U and AAMVFCM-U, providing
a unified parameter-free framework. Our approach replaces fuzzification
parameters with entropy regularization terms that enforce adaptive cross-view
consensus. The core innovation employs signal-to-noise ratio based
regularization ($\delta_j^h = \frac{\bar{x}_j^h}{(\sigma_j^h)^2}$) for
principled feature weighting with convergence guarantees, coupled with
dual-level entropy terms that automatically balance view and feature
contributions. AAMVFCM-U extends this with hierarchical dimensionality
reduction operating at feature and view levels through adaptive thresholding
($\theta^{h^{(t)}} = \frac{d_h^{(t)}}{n}$). Evaluation across five diverse
benchmarks demonstrates superiority over 15 state-of-the-art methods. AAMVFCM-U
achieves up to 97% computational efficiency gains, reduces dimensionality to
0.45% of original size, and automatically identifies critical view combinations
for optimal pattern discovery.

</details>


### [125] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: X-VFL is a new Vertical Federated Learning framework addressing non-aligned data and locally independent inference via Cross Completion and Decision Subspace Alignment, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: VFL faces challenges with aligned data samples and joint inference; X-VFL aims to handle missing features and enable local inference.

Method: X-VFL introduces Cross Completion (XCom) for feature reconstruction and Decision Subspace Alignment (DS-Align) for local inference.

Result: X-VFL achieves 15% and 43% accuracy improvements on CIFAR-10 and MIMIC-III datasets, respectively.

Conclusion: X-VFL is effective for scenarios with missing features and local inference, validated by superior performance.

Abstract: Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.

</details>


### [126] [Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$](https://arxiv.org/abs/2508.05571)
*Feiyu Wang,Guoan Wang,Yihao Zhang,Shengfan Wang,Weitao Li,Bokai Huang,Shimao Chen,Zihan Jiang,Rui Xu,Tong Yang*

Main category: cs.LG

TL;DR: Fairy±i introduces a 2-bit quantization framework for complex-valued LLMs, surpassing the accuracy ceiling of full-precision models by leveraging complex domain advantages and enabling efficient inference.


<details>
  <summary>Details</summary>
Motivation: Current QAT research focuses on minimizing quantization error without surpassing the full-precision accuracy ceiling. This work aims to break this ceiling by improving full-precision models and quantizing them efficiently.

Method: The method maps weights to the fourth roots of unity (±1, ±i), creating a symmetric 2-bit representation. It enables multiplication-free inference using additions and element swaps.

Result: Fairy±i outperforms existing 2-bit quantization methods in PPL and downstream tasks while maintaining storage and compute efficiency.

Conclusion: This work pioneers a new direction for highly accurate and practical LLMs under extreme low-bit constraints.

Abstract: Quantization-Aware Training (QAT) integrates quantization into the training
loop, enabling LLMs to learn robust low-bit representations, and is widely
recognized as one of the most promising research directions. All current QAT
research focuses on minimizing quantization error on full-precision models,
where the full-precision accuracy acts as an upper bound (accuracy ceiling). No
existing method has even attempted to surpass this ceiling. To break this
ceiling, we propose a new paradigm: raising the ceiling (full-precision model),
and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$,
the first 2-bit quantization framework for complex-valued LLMs. Specifically,
our method leverages the representational advantages of the complex domain to
boost full-precision accuracy. We map weights to the fourth roots of unity
$\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically
optimal 2-bit representation. Importantly, each quantized weight has either a
zero real or imaginary part, enabling multiplication-free inference using only
additions and element swaps. Experimental results show that Fairy$\pm i$
outperforms the ceiling of existing 2-bit quantization approaches in terms of
both PPL and downstream tasks, while maintaining strict storage and compute
efficiency. This work opens a new direction for building highly accurate and
practical LLMs under extremely low-bit constraints.

</details>


### [127] [Enhancing PyKEEN with Multiple Negative Sampling Solutions for Knowledge Graph Embedding Models](https://arxiv.org/abs/2508.05587)
*Claudia d'Amato,Ivan Diliso,Nicola Fanizzi,Zafar Saeed*

Main category: cs.LG

TL;DR: The paper introduces an extension for PyKEEN to integrate advanced negative sampling strategies for knowledge graph embeddings, improving performance and flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing libraries for knowledge graph embeddings lack advanced negative sampling strategies, limiting performance and customization.

Method: Developed a modular extension for PyKEEN with static and dynamic negative samplers, ensuring compatibility with existing workflows.

Result: Empirical study shows the extension enhances performance in link prediction tasks and offers insights for better strategy design.

Conclusion: The extension improves PyKEEN's capabilities, enabling more effective and customizable knowledge graph embedding methods.

Abstract: Embedding methods have become popular due to their scalability on link
prediction and/or triple classification tasks on Knowledge Graphs. Embedding
models are trained relying on both positive and negative samples of triples.
However, in the absence of negative assertions, these must be usually
artificially generated using various negative sampling strategies, ranging from
random corruption to more sophisticated techniques which have an impact on the
overall performance. Most of the popular libraries for knowledge graph
embedding, support only basic such strategies and lack advanced solutions. To
address this gap, we deliver an extension for the popular KGE framework PyKEEN
that integrates a suite of several advanced negative samplers (including both
static and dynamic corruption strategies), within a consistent modular
architecture, to generate meaningful negative samples, while remaining
compatible with existing PyKEEN -based workflows and pipelines. The developed
extension not only enhancesPyKEEN itself but also allows for easier and
comprehensive development of embedding methods and/or for their customization.
As a proof of concept, we present a comprehensive empirical study of the
developed extensions and their impact on the performance (link prediction
tasks) of different embedding methods, which also provides useful insights for
the design of more effective strategies

</details>


### [128] [Optimizing IoT Threat Detection with Kolmogorov-Arnold Networks (KANs)](https://arxiv.org/abs/2508.05591)
*Natalia Emelianova,Carlos Kamienski,Ronaldo C. Prati*

Main category: cs.LG

TL;DR: Kolmogorov-Arnold Networks (KANs) outperform traditional MLPs and match state-of-the-art models like Random Forest and XGBoost for IoT intrusion detection, with added interpretability.


<details>
  <summary>Details</summary>
Motivation: The rise in IoT cyberattacks necessitates better intrusion detection methods.

Method: Using KANs with learnable activation functions for intrusion detection in IoT networks.

Result: KANs achieve competitive accuracy and superior interpretability compared to traditional models.

Conclusion: KANs are a promising alternative for IoT intrusion detection due to their performance and interpretability.

Abstract: The exponential growth of the Internet of Things (IoT) has led to the
emergence of substantial security concerns, with IoT networks becoming the
primary target for cyberattacks. This study examines the potential of
Kolmogorov-Arnold Networks (KANs) as an alternative to conventional machine
learning models for intrusion detection in IoT networks. The study demonstrates
that KANs, which employ learnable activation functions, outperform traditional
MLPs and achieve competitive accuracy compared to state-of-the-art models such
as Random Forest and XGBoost, while offering superior interpretability for
intrusion detection in IoT networks.

</details>


### [129] [Non-omniscient backdoor injection with a single poison sample: Proving the one-poison hypothesis for linear regression and linear classification](https://arxiv.org/abs/2508.05600)
*Thorsten Peinemann,Paula Arnold,Sebastian Berndt,Thomas Eisenbarth,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: The paper explores the 'one-poison hypothesis,' showing that a single poison sample can inject a backdoor into ML models without harming benign task performance, proven for linear models.


<details>
  <summary>Details</summary>
Motivation: To determine the minimal poison data required for successful backdoor attacks, addressing gaps in prior work.

Method: Formulates the one-poison hypothesis, proves it for linear regression/classification, and validates with experiments.

Result: A single poison sample can achieve zero backdooring-error with negligible impact on benign performance.

Conclusion: The one-poison hypothesis holds for linear models, demonstrating efficient backdoor attacks with minimal data.

Abstract: Backdoor injection attacks are a threat to machine learning models that are
trained on large data collected from untrusted sources; these attacks enable
attackers to inject malicious behavior into the model that can be triggered by
specially crafted inputs. Prior work has established bounds on the success of
backdoor attacks and their impact on the benign learning task, however, an open
question is what amount of poison data is needed for a successful backdoor
attack. Typical attacks either use few samples, but need much information about
the data points or need to poison many data points.
  In this paper, we formulate the one-poison hypothesis: An adversary with one
poison sample and limited background knowledge can inject a backdoor with zero
backdooring-error and without significantly impacting the benign learning task
performance. Moreover, we prove the one-poison hypothesis for linear regression
and linear classification. For adversaries that utilize a direction that is
unused by the benign data distribution for the poison sample, we show that the
resulting model is functionally equivalent to a model where the poison was
excluded from training. We build on prior work on statistical backdoor learning
to show that in all other cases, the impact on the benign learning task is
still limited. We also validate our theoretical results experimentally with
realistic benchmark data sets.

</details>


### [130] [On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification](https://arxiv.org/abs/2508.05629)
*Yongliang Wu,Yizhou Zhou,Zhou Ziheng,Yingzhe Peng,Xinyu Ye,Xinting Hu,Wenbo Zhu,Lu Qi,Ming-Hsuan Yang,Xu Yang*

Main category: cs.LG

TL;DR: Proposes Dynamic Fine-Tuning (DFT) to improve SFT for LLMs by dynamically rescaling gradients, enhancing generalization and outperforming standard SFT.


<details>
  <summary>Details</summary>
Motivation: Addresses limited generalization of SFT compared to RL, revealing problematic reward structures in standard SFT gradients.

Method: Introduces DFT, a method to stabilize gradient updates by dynamically rescaling the objective function with token probabilities.

Result: DFT significantly outperforms standard SFT in benchmarks and shows competitive results in offline RL.

Conclusion: DFT bridges theory and practice, advancing SFT performance with a simple yet effective solution.

Abstract: We present a simple yet theoretically motivated improvement to Supervised
Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited
generalization compared to reinforcement learning (RL). Through mathematical
analysis, we reveal that standard SFT gradients implicitly encode a problematic
reward structure that may severely restrict the generalization capabilities of
model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing
gradient updates for each token by dynamically rescaling the objective function
with the probability of this token. Remarkably, this single-line code change
significantly outperforms standard SFT across multiple challenging benchmarks
and base models, demonstrating greatly improved generalization. Additionally,
our approach shows competitive results in offline RL settings, offering an
effective yet simpler alternative. This work bridges theoretical insight and
practical solutions, substantially advancing SFT performance. The code will be
available at https://github.com/yongliang-wu/DFT.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [131] [BTPG-max: Achieving Local Maximal Bidirectional Pairs for Bidirectional Temporal Plan Graphs](https://arxiv.org/abs/2508.04849)
*Yifan Su,Rishi Veerapaneni,Jiaoyang Li*

Main category: cs.MA

TL;DR: BTPG-max improves Multi-Agent Path Finding (MAPF) by optimizing bidirectional dependencies for better delay robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing infeasibility of strict timestep adherence in real systems due to delays, leading to collisions.

Method: Proposes BTPG-max algorithm to find more bidirectional pairs in Temporal Plan Graphs (TPG), ensuring local optimality.

Result: BTPG-max constructs BTPGs with more bidirectional edges, better anytime behavior, and enhanced delay robustness.

Conclusion: BTPG-max advances MAPF solutions by optimizing bidirectional dependencies for real-world delay scenarios.

Abstract: Multi-Agent Path Finding (MAPF) requires computing collision-free paths for
multiple agents in shared environment. Most MAPF planners assume that each
agent reaches a specific location at a specific timestep, but this is
infeasible to directly follow on real systems where delays often occur. To
address collisions caused by agents deviating due to delays, the Temporal Plan
Graph (TPG) was proposed, which converts a MAPF time dependent solution into a
time independent set of inter-agent dependencies. Recently, a Bidirectional TPG
(BTPG) was proposed which relaxed some dependencies into ``bidirectional pairs"
and improved efficiency of agents executing their MAPF solution with delays.
Our work improves upon this prior work by designing an algorithm, BPTG-max,
that finds more bidirectional pairs. Our main theoretical contribution is in
designing the BTPG-max algorithm is locally optimal, i.e. which constructs a
BTPG where no additional bidirectional pairs can be added. We also show how in
practice BTPG-max leads to BTPGs with significantly more bidirectional edges,
superior anytime behavior, and improves robustness to delays.

</details>


### [132] [Congestion Mitigation Path Planning for Large-Scale Multi-Agent Navigation in Dense Environments](https://arxiv.org/abs/2508.05253)
*Takuro Kato,Keisuke Okumura,Yoko Sasaki,Naoya Yokomachi*

Main category: cs.MA

TL;DR: The paper introduces congestion mitigation path planning (CMPP) to address local congestion in multi-agent systems by embedding congestion into the cost function, improving navigation efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance navigation efficiency in high-density environments where autonomous agents move simultaneously, mitigating local congestion is critical.

Method: CMPP uses a flow-based multiplicative penalty on sparse graph vertices, with two solvers: an exact mixed-integer nonlinear programming solver for small instances and a scalable two-layer search algorithm (A-CMTS) for large-scale instances.

Result: Empirical studies show CMPP reduces local congestion and improves system throughput in discrete- and continuous-space scenarios.

Conclusion: CMPP significantly enhances multi-agent system performance, making it suitable for real-world applications like logistics and autonomous-vehicle operations.

Abstract: In high-density environments where numerous autonomous agents move
simultaneously in a distributed manner, streamlining global flows to mitigate
local congestion is crucial to maintain overall navigation efficiency. This
paper introduces a novel path-planning problem, congestion mitigation path
planning (CMPP), which embeds congestion directly into the cost function,
defined by the usage of incoming edges along agents' paths. CMPP assigns a
flow-based multiplicative penalty to each vertex of a sparse graph, which grows
steeply where frequently-traversed paths intersect, capturing the intuition
that congestion intensifies where many agents enter the same area from
different directions. Minimizing the total cost yields a set of coarse-level,
time-independent routes that autonomous agents can follow while applying their
own local collision avoidance. We formulate the problem and develop two
solvers: (i) an exact mixed-integer nonlinear programming solver for small
instances, and (ii) a scalable two-layer search algorithm, A-CMTS, which
quickly finds suboptimal solutions for large-scale instances and iteratively
refines them toward the optimum. Empirical studies show that augmenting
state-of-the-art collision-avoidance planners with CMPP significantly reduces
local congestion and enhances system throughput in both discrete- and
continuous-space scenarios. These results indicate that CMPP improves the
performance of multi-agent systems in real-world applications such as logistics
and autonomous-vehicle operations.

</details>
