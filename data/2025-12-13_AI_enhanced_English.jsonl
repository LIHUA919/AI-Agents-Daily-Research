{"id": "2512.09939", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09939", "abs": "https://arxiv.org/abs/2512.09939", "authors": ["Stella C. Dong"], "title": "Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)", "comment": null, "summary": "Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.\n  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.\n  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.\n  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.", "AI": {"tldr": "A multi-agent simulation framework (R-CMASP) for reinsurance decision-making that combines simulator-coupled dynamics, role-specialized agents, and normative constraints to improve stability and efficiency over deterministic approaches.", "motivation": "Reinsurance decision-making involves distributed information, partial observability, and regulatory constraints that cannot be handled by deterministic automation, requiring more flexible multi-agent approaches.", "method": "Proposes R-CMASP framework extending stochastic games with simulator-coupled dynamics, role-specialized agents with structured communication, and normative feasibility constraints.", "result": "LLM-based agents in calibrated environment show improved stability, reduced pricing variance, better capital efficiency, and higher clause interpretation accuracy compared to baselines.", "conclusion": "Regulated decision environments are best modeled as norm-governed, simulator-coupled multi-agent systems rather than deterministic automation."}}
{"id": "2512.10078", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10078", "abs": "https://arxiv.org/abs/2512.10078", "authors": ["Jingyao Ren", "Eric Ewing", "T. K. Satish Kumar", "Sven Koenig", "Nora Ayanian"], "title": "Empirical Hardness in Multi-Agent Pathfinding: Research Challenges and Opportunities", "comment": "Published in AAMAS-25", "summary": "Multi-agent pathfinding (MAPF) is the problem of finding collision-free paths for a team of agents on a map. Although MAPF is NP-hard, the hardness of solving individual instances varies significantly, revealing a gap between theoretical complexity and actual hardness. This paper outlines three key research challenges in MAPF empirical hardness to understand such phenomena. The first challenge, known as algorithm selection, is determining the best-performing algorithms for a given instance. The second challenge is understanding the key instance features that affect MAPF empirical hardness, such as structural properties like phase transition and backbone/backdoor. The third challenge is how to leverage our knowledge of MAPF empirical hardness to effectively generate hard MAPF instances or diverse benchmark datasets. This work establishes a foundation for future empirical hardness research and encourages deeper investigation into these promising and underexplored areas.", "AI": {"tldr": "MAPF empirical hardness varies significantly despite NP-hard complexity; paper identifies three key research challenges: algorithm selection, understanding instance features affecting hardness, and generating hard benchmark instances.", "motivation": "There is a gap between theoretical complexity and actual hardness in MAPF problems, with significant variation in solving difficulty across instances that needs systematic investigation.", "method": "Outlines three research challenges for understanding MAPF empirical hardness phenomena.", "result": "Establishes foundation for future empirical hardness research in MAPF.", "conclusion": "Identifies promising underexplored areas for deeper investigation into MAPF empirical hardness."}}
{"id": "2512.10166", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10166", "abs": "https://arxiv.org/abs/2512.10166", "authors": ["Khushiyant"], "title": "Emergent Collective Memory in Decentralized Multi-Agent AI Systems", "comment": "23 pages, 4 figures", "summary": "We demonstrate how collective memory emerges in decentralized multi-agent systems through the interplay between individual agent memory and environmental trace communication. Our agents maintain internal memory states while depositing persistent environmental traces, creating a spatially distributed collective memory without centralized control. Comprehensive validation across five environmental conditions (20x20 to 50x50 grids, 5-20 agents, 50 runs per configuration) reveals a critical asymmetry: individual memory alone provides 68.7% performance improvement over no-memory baselines (1563.87 vs 927.23, p < 0.001), while environmental traces without memory fail completely. This demonstrates that memory functions independently but traces require cognitive infrastructure for interpretation. Systematic density-sweep experiments (rho in [0.049, 0.300], up to 625 agents) validate our theoretical phase transition prediction. On realistic large grids (30x30, 50x50), stigmergic coordination dominates above rho ~ 0.20, with traces outperforming memory by 36-41% on composite metrics despite lower food efficiency. The experimental crossover confirms the predicted critical density rho_c = 0.230 within 13% error.", "AI": {"tldr": "Study shows how collective memory emerges in multi-agent systems through combined individual memory and environmental traces, with environmental dominance at high densities.", "motivation": "To understand how collective memory emerges in decentralized multi-agent systems through the interplay of individual memory and environmental trace communication.", "method": "Used decentralized multi-agent systems with internal memory and persistent environmental traces on various grid sizes (20x20 to 50x50) with 5-20 agents across 50 runs per configuration, plus systematic density-sweep experiments with up to 625 agents.", "result": "Individual memory alone provides 68.7% performance improvement over no-memory baselines, while environmental traces without memory fail. At high densities (rho > 0.20), traces outperform memory by 36-41% despite lower food efficiency, validating theoretical phase transition prediction.", "conclusion": "Memory functions independently but environmental traces require cognitive infrastructure for interpretation; stigmergic coordination dominates at high densities, confirming predicted critical density."}}
{"id": "2512.10610", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10610", "abs": "https://arxiv.org/abs/2512.10610", "authors": ["Xiaopei Tan", "Muyang Fan"], "title": "Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing", "comment": null, "summary": "We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation.", "AI": {"tldr": "A concurrent routing framework integrating LLMs with live traffic data that enables dynamic route planning without stopping agents, reducing intersection wait times to just 0.75 seconds under high traffic.", "motivation": "Traditional approaches require agents to stop for deliberation, causing traffic delays. The need for continuous movement and real-time adaptation in dynamic traffic environments motivates this research.", "method": "Uses a non-blocking asynchronous architecture with Unity coroutines and a request manager. Environment is a weighted undirected graph with live congestion metrics updated continuously by agents for shared perception.", "result": "LLM-driven agents successfully adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding while maintaining real-time performance with very low decision latency.", "conclusion": "Provides a reproducible framework for future research in adaptive routing and multi-agent cooperation, demonstrating effective real-time LLM integration in traffic systems."}}
{"id": "2512.09947", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09947", "abs": "https://arxiv.org/abs/2512.09947", "authors": ["Fuyan Ou", "Siqi Ai", "Yulin Hu"], "title": "HGC-Herd: Efficient Heterogeneous Graph Condensation via Representative Node Herding", "comment": "8 pages, 2 figures", "summary": "Heterogeneous graph neural networks (HGNNs) have demonstrated strong capability in modeling complex semantics across multi-type nodes and relations. However, their scalability to large-scale graphs remains challenging due to structural redundancy and high-dimensional node features. Existing graph condensation approaches, such as GCond, are primarily developed for homogeneous graphs and rely on gradient matching, resulting in considerable computational, memory, and optimization overhead. We propose HGC-Herd, a training-free condensation framework that generates compact yet informative heterogeneous graphs while maintaining both semantic and structural fidelity. HGC-Herd integrates lightweight feature propagation to encode multi-hop relational context and employs a class-wise herding mechanism to identify representative nodes per class, producing balanced and discriminative subsets for downstream learning tasks. Extensive experiments on ACM, DBLP, and Freebase validate that HGC-Herd attains comparable or superior accuracy to full-graph training while markedly reducing both runtime and memory consumption. These results underscore its practical value for efficient and scalable heterogeneous graph representation learning.", "AI": {"tldr": "HGC-Herd is a training-free graph condensation framework for heterogeneous graphs that creates compact graphs while maintaining accuracy, reducing runtime and memory usage.", "motivation": "Existing graph condensation methods like GCond are designed for homogeneous graphs and suffer from high computational costs due to gradient matching.", "method": "Uses lightweight feature propagation to encode multi-hop context and class-wise herding to select representative nodes per class.", "result": "Achieved comparable or better accuracy than full-graph training on ACM, DBLP, and Freebase datasets with significantly lower runtime and memory consumption.", "conclusion": "HGC-Herd provides an efficient and scalable solution for heterogeneous graph representation learning with practical value."}}
{"id": "2512.09931", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09931", "abs": "https://arxiv.org/abs/2512.09931", "authors": ["Akaash Chatterjee", "Suman Kundu"], "title": "ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples", "comment": "5 pages, 1 Figure", "summary": "Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.", "AI": {"tldr": "ExaCraft is an AI system that generates personalized learning examples by adapting to learners' dynamic context through real-time behavior analysis and user profiles.", "motivation": "Existing educational AI tools lack personalization and don't adapt to learners' changing understanding, struggles, or skill development.", "method": "Uses Google Gemini AI and Python Flask API with Chrome extension, combining user profiles with real-time learner behavior analysis to generate culturally relevant examples.", "result": "Developed ExaCraft system that adapts to five key learning context aspects: struggle indicators, mastery patterns, topic progression history, session boundaries, and learning progression signals.", "conclusion": "ExaCraft demonstrates ability to evolve examples from basic to advanced implementations based on learner needs and progression patterns."}}
{"id": "2512.10355", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10355", "abs": "https://arxiv.org/abs/2512.10355", "authors": ["Hyunsung Kim", "Sangwoo Seo", "Hoyoung Choi", "Tom Boomstra", "Jinsung Yoon", "Chanyoung Park"], "title": "Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks", "comment": null, "summary": "Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.", "AI": {"tldr": "Developed DEFCON, a framework using Graph Attention Networks to quantify soccer defenders' contributions by evaluating their impact on Expected Possession Value, showing strong correlations with market values.", "motivation": "The motivation is to overcome the challenge of evaluating defensive performance in soccer, as effective defending often involves preventing dangerous opportunities before they arise, which existing methods focused on visible on-ball actions fail to capture.", "method": "The method involves estimating success probabilities and expected values of attacking options using Graph Attention Networks. DEFCON calculates Expected Possession Value (EPV) before and after each defensive action and assigns credits to defenders based on their impact on the opponent's EPV.", "result": "The results show that DEFCON's aggregated player credits have strong positive correlations with market valuations. The framework also enables various practical applications, including in-game timelines, spatial analyses, and attacker-defender interaction summaries.", "conclusion": "The paper concludes that DEFCON effectively addresses the limitations of existing defensive evaluation methods by providing a more accurate measurement of defenders' contributions through EPV changes. It suggests that DEFCON's outputs can be used by clubs and analysts to better assess defensive performance and make informed decisions in recruitment, strategy, and performance analysis."}}
{"id": "2512.09972", "categories": ["cs.LG", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.09972", "abs": "https://arxiv.org/abs/2512.09972", "authors": ["Kesheng Chen", "Wenjian Luo", "Zhenqian Zhu", "Yamin Hu", "Yiya Xi"], "title": "BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization", "comment": null, "summary": "Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the \"curse of dimensionality,\" rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.", "AI": {"tldr": "BAMBO automatically constructs high-quality LLM Pareto sets via optimized block partitioning (1D clustering) and evolutionary optimization (qEHVI), outperforming prior merging methods.", "motivation": "Current LLM merging methods fail to construct optimal Pareto sets: model-level is too coarse/generates suboptimal solutions; layer-level suffers from intractable dimensionality.", "method": "Proposes BAMBO framework with Hybrid Optimal Block Partitioning (1D clustering via dynamic programming) and evolutionary loop optimization using qEHVI.", "result": "BAMBO constructs a superior, more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse constraints.", "conclusion": "BAMBO successfully addresses the limitations of existing merging techniques by balancing granularity and dimensionality for efficient Pareto set construction in LLMs."}}
{"id": "2512.09932", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09932", "abs": "https://arxiv.org/abs/2512.09932", "authors": ["Maya Grace Torii", "Takahito Murakami", "Shuka Koseki", "Yoichi Ochiai"], "title": "Suzume-chan: Your Personal Navigator as an Embodied Information Hub", "comment": "3 pages, 1 figure, This study will demonstrate at WISS 2025", "summary": "Access to expert knowledge often requires real-time human communication. Digital tools improve access to information but rarely create the sense of connection needed for deep understanding. This study addresses this issue using Social Presence Theory, which explains how a feeling of \"being together\" enhances communication. An \"Embodied Information Hub\" is proposed as a new way to share knowledge through physical and conversational interaction. The prototype, Suzume-chan, is a small, soft AI agent running locally with a language model and retrieval-augmented generation (RAG). It learns from spoken explanations and responds through dialogue, reducing psychological distance and making knowledge sharing warmer and more human-centered.", "AI": {"tldr": "Physical AI agent enhanced by social presence improves knowledge sharing through interactive dialogue.", "motivation": "Current digital tools improve information access but lack the sense of connection needed for deep understanding. There is a need for systems that create social presence ('being together') to enhance knowledge sharing.", "method": "Development of a prototype called Suzume-chan\u2014a small, soft AI agent running locally with language model and retrieval-augmented generation (RAG). It learns from spoken explanations and responds through dialogue.", "result": "The prototype reduces psychological distance and makes knowledge sharing warmer and more human-centered through interactive dialogue and physical embodiment.", "conclusion": "The Embodied Information Hub, exemplified by prototype Suzume-chan, successfully creates a warmer, more human-centered knowledge sharing experience by combining physical embodiment and conversational interaction to foster social presence."}}
{"id": "2512.10016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10016", "abs": "https://arxiv.org/abs/2512.10016", "authors": ["Marvin Alles", "Xingyuan Zhang", "Patrick van der Smagt", "Philip Becker-Ehmck"], "title": "Latent Action World Models for Control with Unlabeled Trajectories", "comment": null, "summary": "Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.", "AI": {"tldr": "A latent-action world model that learns from both action-labeled and action-free data by creating a shared latent action representation, enabling more efficient world model training with significantly fewer labeled samples.", "motivation": "Standard world models rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. Humans combine direct interaction with action-free experience, suggesting a need for models that can learn from heterogeneous data.", "method": "Introduce latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns control signals with actions inferred from passive observations, enabling training on large-scale unlabeled trajectories with only a small set of labeled ones.", "result": "On DeepMind Control Suite, achieves strong performance using about 10x fewer action-labeled samples than purely action-conditioned baselines.", "conclusion": "Latent actions enable training on both passive and interactive data, making world models learn more efficiently by bridging offline RL (action-conditioned) and action-free training domains."}}
{"id": "2512.09935", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09935", "abs": "https://arxiv.org/abs/2512.09935", "authors": ["Chih-Han Chen", "Chen-Han Tsai", "Yu-Shao Peng"], "title": "Exploring Health Misinformation Detection with Multi-Agent Debate", "comment": null, "summary": "Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.", "AI": {"tldr": "Two-stage framework combining agreement score prediction and multi-agent debate for health misinformation detection, outperforming baseline methods.", "motivation": "Fact-checking health claims is critical due to online misinformation proliferation; effective verification requires quality evidence retrieval and rigorous reasoning.", "method": "First stage uses LLMs to evaluate retrieved articles and compute aggregated agreement score; second stage initiates multi-agent debate if score falls below threshold to synthesize conflicting evidence.", "result": "Experimental results show superior performance compared to baseline methods.", "conclusion": "Combining automated scoring with collaborative reasoning adds value for complex verification tasks."}}
{"id": "2512.10032", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.10032", "abs": "https://arxiv.org/abs/2512.10032", "authors": ["Jan Marco Ruiz de Vargas", "Kirtan Padh", "Niki Kilbertus"], "title": "Cluster-Dags as Powerful Background Knowledge For Causal Discovery", "comment": "23 pages, 5 figures", "summary": "Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.", "AI": {"tldr": "Causal discovery using Cluster-DAGs as prior knowledge improves performance.", "motivation": "Current causal discovery methods struggle with high-dimensional data and complex dependencies.", "method": "Developed Cluster-PC and Cluster-FCI algorithms incorporating Cluster-DAGs as flexible priors.", "result": "Cluster-PC and Cluster-FCI outperform baseline methods without prior knowledge on simulated data.", "conclusion": "Cluster-DAGs provide effective warm-start frameworks for causal discovery in various settings."}}
{"id": "2512.09944", "categories": ["cs.AI", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.09944", "abs": "https://arxiv.org/abs/2512.09944", "authors": ["Moein Heidari", "Mohammad Amin Roohi", "Armin Khosravi", "Ilker Hacihaliloglu"], "title": "Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting", "comment": null, "summary": "Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.", "AI": {"tldr": "Echo-CoPilot is a multi-view, multi-task agent that uses LLM orchestration of specialized echocardiography tools to automate full-study interpretation, outperforming existing models on MIMIC-EchoQA benchmark.", "motivation": "Echocardiography interpretation is cognitively demanding and performed manually, while existing foundation models operate in isolation without unified clinical assessment.", "method": "ReAct-style loop where LLM decomposes clinician queries, invokes specialized tools for view recognition, segmentation, measurement, disease prediction, and report synthesis, integrating outputs into guideline-aware answers.", "result": "Achieves 50.8% accuracy on MIMIC-EchoQA benchmark, outperforming general-purpose and biomedical video vision-language models. Successfully handles borderline clinical cases using quantitative measurements and physiologic context.", "conclusion": "Echo-CoPilot demonstrates effective automation of comprehensive echocardiography interpretation through LLM-orchestrated tool integration, showing promise for clinical decision support near diagnostic thresholds."}}
{"id": "2512.10033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10033", "abs": "https://arxiv.org/abs/2512.10033", "authors": ["Sarwan Ali"], "title": "Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation", "comment": null, "summary": "Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $\u03ba=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.", "AI": {"tldr": "HB-SGE is a robust first-order optimization method combining heavy-ball momentum with synthetic gradient extrapolation, providing stable convergence on ill-conditioned and non-convex problems where NAG and standard momentum fail.", "motivation": "Classical accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation.", "method": "HB-SGE estimates future gradient directions using local Taylor approximations instead of accumulating historical gradients, combining heavy-ball momentum with predictive gradient extrapolation.", "result": "HB-SGE converges in 119 iterations on ill-conditioned quadratics (\u03ba=50) where SGD and NAG diverge, and converges in 2,718 iterations on the non-convex Rosenbrock function where classical momentum methods diverge within 10 steps.", "conclusion": "HB-SGE provides a robust alternative to NAG with stable convergence across diverse landscapes, offering speedup over SGD while maintaining O(d) memory overhead and standard momentum hyperparameters."}}
{"id": "2512.09976", "categories": ["cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.09976", "abs": "https://arxiv.org/abs/2512.09976", "authors": ["Alexis Kafantaris"], "title": "Fuzzy Hierarchical Multiplex", "comment": "11 pages, 2 figures, 1 double figure, 1 table, 12 references. This will be part of my PhD dissertation and it is a White paper-theoretical framewor. As is, it s meant for a basis that will be later used to further developed an FHM. It might not be math-logic related and I am willing to change it, I just felt that it belonged to mathematical modeling. Yours truly, AK", "summary": "A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.", "AI": {"tldr": "Proposes a white-theoretical extension of FCM causality for fuzzy optimization, focusing on service process design in information transmission.", "motivation": "To enhance service optimization in information transmission by creating a framework that maps data dynamics into metrics and examines logical implication and concept hierarchy.", "method": "Extends FCM causality using a multiplex approach to analyze dynamics, logical implication, and hierarchy of concepts mathematically.", "result": "Develops a white-theoretical framework exemplified for service process design, with a thorough analysis of FHM following logical steps.", "conclusion": "The framework provides a foundational extension to FCM for fuzzy optimization, applicable to optimizing service processes in information transmission."}}
{"id": "2512.10040", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.10040", "abs": "https://arxiv.org/abs/2512.10040", "authors": ["Skyler Wu", "Aymen Echarghaoui"], "title": "Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs", "comment": "Working paper. 13 pages, 4 figures", "summary": "Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.", "AI": {"tldr": "MRPO enhances DPO by using multiple reference models, but current weight-setting methods are unreliable. The paper introduces four statistically sound weighting strategies that outperform existing methods, yet surprisingly finds single-reference DPO often works better.", "motivation": "Current MRPO methods set reference weights in an ad-hoc and statistically unsound way, leading to unreliable performance. There's a need for more rigorous weighting strategies.", "method": "Proposed four new weighting strategies: two offline methods using validation signal, one online method with sliding-window estimator, and one online method using Thompson Sampling as a K-armed bandit. Experiments conducted with Qwen2.5-0.5B policy model and 7 reference models from various families.", "result": "All four new strategies outperformed current MRPO weighting methods on UltraFeedback and SafeRLHF datasets. However, single-reference DPO using 6 out of 7 references consistently outperformed all multiple-reference approaches.", "conclusion": "While the new weighting strategies improve MRPO performance, the finding that single-reference DPO often works better questions the practical value of multiple-reference approaches, suggesting simpler methods may be more effective."}}
{"id": "2512.10004", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10004", "abs": "https://arxiv.org/abs/2512.10004", "authors": ["Sha Li", "Ayush Sadekar", "Nathan Self", "Yiqi Su", "Lars Andersland", "Mira Chaplin", "Annabel Zhang", "Hyoju Yang", "James B Henderson", "Krista Wigginton", "Linsey Marr", "T. M. Murali", "Naren Ramakrishnan"], "title": "Exploring LLMs for Scientific Information Extraction Using The SciEx Framework", "comment": null, "summary": "Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.", "AI": {"tldr": "SciEx presents a modular framework for extracting scientific information from papers using LLMs, addressing challenges like long documents, multi-modal content, and changing ontologies through decoupled components.", "motivation": "Existing LLM-based extraction tools struggle with scientific literature due to long contexts, multi-modal data, and rapidly changing data schemas, making adaptation difficult.", "method": "The framework decouples PDF parsing, multi-modal retrieval, extraction, and aggregation into modular components, allowing flexible integration of models and strategies.", "result": "Evaluation across three scientific topics shows SciEx can extract fine-grained information accurately and consistently, highlighting LLM pipeline strengths and limitations.", "conclusion": "SciEx offers a practical, extensible solution for automated scientific data extraction, though current LLM-based approaches still have limitations."}}
{"id": "2512.10042", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10042", "abs": "https://arxiv.org/abs/2512.10042", "authors": ["Jongmin Lee", "Meiqi Sun", "Pieter Abbeel"], "title": "SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation", "comment": "ICLR 2025", "summary": "In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.", "AI": {"tldr": "SEMDICE is an off-policy algorithm that learns a state entropy maximizing policy from arbitrary datasets for unsupervised RL pre-training, showing superior performance and downstream adaptation efficiency.", "motivation": "Unsupervised pre-training in RL needs methods that can learn effective prior policies without task rewards, particularly for state entropy maximization (SEM) which requires handling arbitrary off-policy data.", "method": "SEMDICE directly optimizes the policy within the space of stationary distributions to compute a Markov state-entropy-maximizing policy from off-policy datasets.", "result": "SEMDICE outperforms baseline algorithms in maximizing state entropy and achieves the best adaptation efficiency for downstream tasks among SEM-based methods.", "conclusion": "SEMDICE provides a principled solution for SEM in unsupervised RL pre-training, effectively leveraging off-policy data for improved downstream task performance."}}
{"id": "2512.10034", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.10034", "abs": "https://arxiv.org/abs/2512.10034", "authors": ["Salom\u00e9 Guilbert", "Cassandra Masschelein", "Jeremy Goumaz", "Bohdan Naida", "Philippe Schwaller"], "title": "DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations", "comment": null, "summary": "Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.", "AI": {"tldr": "DynaMate is an automated multi-agent framework that designs and executes complete molecular dynamics workflows for proteins and protein-ligand systems, including binding affinity calculations. It addresses MD setup complexity through modular agents with self-correcting capabilities.", "motivation": "Force field-based MD simulations are crucial but technically complex to set up, creating barriers for widespread use in drug discovery and protein engineering. Current agentic LLMs haven't successfully automated protein-ligand MD workflows.", "method": "DynaMate uses a modular multi-agent framework with three specialized modules that interact to plan experiments, perform simulations, and analyze results. It integrates dynamic tool use, web search, PaperQA, and self-correcting behavior.", "result": "Evaluation across twelve benchmark systems showed DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful protein-ligand interaction analyses.", "conclusion": "The framework enables standardized, scalable, and efficient molecular modeling pipelines for future biomolecular and drug design applications."}}
{"id": "2512.10043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10043", "abs": "https://arxiv.org/abs/2512.10043", "authors": ["Jo\u00e3o Lucas Luz Lima Sarcinelli", "Diego Furtado Silva"], "title": "Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition", "comment": null, "summary": "Large Language Models (LLMs) excel in many Natural Language Processing (NLP) tasks through in-context learning but often under-perform in Named Entity Recognition (NER), especially for lower-resource languages like Portuguese. While open-weight LLMs enable local deployment, no single model dominates all tasks, motivating ensemble approaches. However, existing LLM ensembles focus on text generation or classification, leaving NER under-explored. In this context, this work proposes a novel three-step ensemble pipeline for zero-shot NER using similarly capable, locally run LLMs. Our method outperforms individual LLMs in four out of five Portuguese NER datasets by leveraging a heuristic to select optimal model combinations with minimal annotated data. Moreover, we show that ensembles obtained on different source datasets generally outperform individual LLMs in cross-dataset configurations, potentially eliminating the need for annotated data for the current task. Our work advances scalable, low-resource, and zero-shot NER by effectively combining multiple small LLMs without fine-tuning. Code is available at https://github.com/Joao-Luz/local-llm-ner-ensemble.", "AI": {"tldr": "Novel LLM ensemble pipeline for zero-shot NER improves performance for Portuguese using multiple small models without fine-tuning.", "motivation": "Large Language Models underperform in Named Entity Recognition for lower-resource languages, and while open-weight LLMs enable local deployment, no single model dominates all tasks. Existing LLM ensembles focus on text generation or classification, leaving NER under-explored.", "method": "Novel three-step ensemble pipeline for zero-shot NER using similarly capable, locally run LLMs with a heuristic to select optimal model combinations requiring minimal annotated data.", "result": "Outperforms individual LLMs in four out of five Portuguese NER datasets. Ensembles obtained on different source datasets generally outperform individual LLMs in cross-dataset configurations, potentially eliminating need for annotated data for current task.", "conclusion": "Ensemble of similarly capable LLMs using a novel three-step pipeline improves zero-shot Named Entity Recognition for lower-resource languages like Portuguese, outperforming individual models and offering a scalable, low-resource solution without fine-tuning."}}
