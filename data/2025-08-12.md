<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 65]
- [cs.LG](#cs.LG) [Total: 144]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: The paper introduces a CUDA-accelerated framework for simulating the fishing card game Pasur, using CFR to compute near-Nash equilibria and efficient memory management techniques.


<details>
  <summary>Details</summary>
Motivation: Pasur's complex rules and large game tree present unique challenges, requiring innovative solutions for efficient computation and memory usage.

Method: The framework decomposes the game tree into game states and inherited scores, uses PyTorch CUDA tensors, and applies a round-by-round backward training strategy.

Result: The approach constructs a game tree with over 10^9 nodes and computes near-Nash equilibria, later training a tree-based model for gameplay predictions.

Conclusion: The framework is scalable and can be extended to other reinforcement learning problems with decomposable action trees, like strategy games or financial decisions.

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [2] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: SciLink is an AI framework designed to foster serendipitous discoveries in materials research by linking experimental data, novelty assessment, and simulations.


<details>
  <summary>Details</summary>
Motivation: Modern labs prioritize efficiency over unplanned findings, missing serendipitous discoveries. SciLink aims to bridge this gap.

Method: Uses hybrid AI: ML for data analysis and LLMs for reasoning, converting raw data into falsifiable claims scored for novelty.

Result: Demonstrated versatility in atomic-resolution and hyperspectral data, integrating human guidance and proposing follow-up experiments.

Conclusion: SciLink enhances efficiency while cultivating serendipity, bridging automated experimentation and open-ended exploration.

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [3] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: IRL-VLA introduces a three-stage framework combining imitation learning, inverse reinforcement learning, and PPO for close-loop autonomous driving, achieving top performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models in autonomous driving face challenges like suboptimal open-loop imitation learning and inefficiencies in close-loop training due to simulation gaps.

Method: A three-stage approach: (1) pretrain VLA policy via imitation learning, (2) build a lightweight reward world model via inverse reinforcement learning, (3) enhance planning with PPO-based reinforcement learning.

Result: Achieves state-of-the-art performance in NAVSIM v2 and 1st runner up in CVPR2025 Autonomous Grand Challenge.

Conclusion: IRL-VLA accelerates VLA research by addressing close-loop training challenges and improving performance in autonomous driving.

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [4] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: CountQA is a new benchmark for evaluating object counting in Multimodal Large Language Models (MLLMs), revealing their poor performance (42.9% accuracy) in high-density, cluttered scenes.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack fundamental object counting skills, limiting their real-world reliability. Existing benchmarks are inadequate for testing this capability in complex scenarios.

Method: Introduces CountQA, a benchmark with 1,500 question-answer pairs featuring real-world images with high object density, clutter, and occlusion. Evaluates 15 MLLMs on this benchmark.

Result: Top-performing MLLM achieves only 42.9% accuracy, with performance worsening as object counts increase.

Conclusion: CountQA addresses a critical gap in MLLM evaluation, aiming to improve their numerical and spatial awareness for real-world applications.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [5] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: K-Dense Analyst, a hierarchical multi-agent system, outperforms top LLMs in bioinformatics by 6.3% using a dual-loop architecture for autonomous analysis.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap between data generation and scientific insights in bioinformatics due to the limitations of LLMs in iterative, tool-integrated workflows.

Method: Introduces K-Dense Analyst, a system with specialized agents for decomposing objectives into executable tasks in secure environments.

Result: Achieves 29.2% accuracy on BixBench, surpassing GPT-5 by 6.3% and improving Gemini 2.5 Pro's baseline by nearly 27%.

Conclusion: Autonomous scientific reasoning requires purpose-built systems, not just enhanced LLMs, to bridge high-level objectives and computational execution.

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [6] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: The paper explores how Formal Concept Analysis (FCA) can be leveraged for variability analysis by identifying key properties of FCA and demonstrating their application in interpreting variability information.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of applying FCA's mathematically oriented properties to variability-related tasks, which is not straightforward due to its foundational literature.

Method: The paper gathers and analyzes essential properties of FCA, focusing on their relevance and application in variability analysis.

Result: The study identifies key FCA properties useful for variability analysis and shows how they can interpret variability information in conceptual structures.

Conclusion: The paper successfully bridges part of the gap in applying FCA to variability analysis by clarifying its properties and their practical use.

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [7] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: A novel multi-agent LLM framework for summarizing enterprise data outperforms traditional methods in faithfulness, relevance, and insight quality.


<details>
  <summary>Details</summary>
Motivation: Traditional table-to-text models struggle with hierarchical structures and context-aware deltas in business reporting.

Method: Multi-agent pipeline for slicing, variance detection, context construction, and LLM-based generation.

Result: Achieves 83% faithfulness, superior coverage of changes, and high relevance scores (4.4/5).

Conclusion: The framework excels in handling subtle trade-offs and outperforms baseline approaches.

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [8] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: A pixel-based trajectory calibration assistant for zero-shot CTMM improves accuracy by 16.8% without region-specific training, using transferable geospatial knowledge and spatial-temporal awareness.


<details>
  <summary>Details</summary>
Motivation: Current CTMM methods rely on region-specific data, limiting adaptability to new areas. Zero-shot CTMM aims to overcome this by leveraging transferable features and handling positioning errors.

Method: Proposes a pixel-based calibration assistant, a Gaussian mixture model in VAE for knowledge sharing, a spatial-temporal awareness module, and a constrained path-finding algorithm.

Result: Outperforms existing methods by 16.8% in zero-shot CTMM.

Conclusion: The approach enables high-accuracy CTMM without region-specific training, enhancing adaptability and reducing positioning errors.

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [9] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: The paper introduces a method to reduce the number of rules in rule-based knowledge graph completion by discovering rule contexts and using probabilistic circuits, achieving competitive performance with fewer rules.


<details>
  <summary>Details</summary>
Motivation: Rule-based methods for knowledge graph completion are explainable but require many rules, which can hinder explainability. The goal is to improve efficiency without sacrificing performance.

Method: The approach discovers rule contexts from training data and uses probabilistic circuits to distribute probabilities over these contexts, reducing the number of rules needed.

Result: Achieves a 70-96% reduction in rules, outperforms baselines by up to 31x, and preserves 91% of peak performance with minimal rule sets. Validated on 8 benchmark datasets.

Conclusion: The method efficiently reduces rule sets while maintaining performance, grounded in probabilistic logic semantics, and has broader implications for probabilistic reasoning.

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [10] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: A survey on self-evolving AI agents, reviewing techniques, frameworks, and domain-specific strategies, with emphasis on evaluation, safety, and ethics.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of static AI agents by exploring adaptive, self-evolving systems for dynamic environments.

Method: Introduces a unified framework with four key components (System Inputs, Agent System, Environment, Optimisers) and reviews techniques targeting these.

Result: Comprehensive review of self-evolving techniques, domain-specific strategies, and critical considerations like safety and ethics.

Conclusion: Provides a foundation for developing adaptive, autonomous, lifelong agentic systems.

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [11] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR introduces a differentiable rule learning method with expressive syntax, outperforming existing techniques in knowledge graph tasks.


<details>
  <summary>Details</summary>
Motivation: Existing ILP methods assume chain-like rule structures, limiting performance and interpretability.

Method: GLIDR uses a differentiable message passing algorithm to learn expressive rules with features like branches and cycles.

Result: GLIDR outperforms existing rule learning methods and competes with embedding methods, showing robustness to noise.

Conclusion: GLIDR's rules retain predictive power and can be integrated with neural networks for end-to-end learning.

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [12] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: EMPATHIA is a multi-agent AI framework for refugee integration, addressing cultural, emotional, and ethical dimensions, achieving 87.4% validation convergence.


<details>
  <summary>Details</summary>
Motivation: Current AI approaches focus narrowly on employment, neglecting cultural and ethical aspects critical for long-term refugee success.

Method: EMPATHIA uses three modules (SEED, RISE, THRIVE) with specialized agents for transparent, interpretable recommendations, tested on UN Kakuma dataset.

Result: Achieved 87.4% validation convergence and explainable assessments across five host countries.

Conclusion: EMPATHIA balances competing values, supports human-AI collaboration, and offers a generalizable framework for AI-driven allocation tasks.

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [13] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: ParBalans extends Balans by adding parallelization to improve MIP solving performance, showing competitive results against Gurobi.


<details>
  <summary>Details</summary>
Motivation: Parallelization is key for solving complex MIP problems efficiently, but Balans's parallel potential hasn't been fully explored.

Method: Introduces ParBalans, combining solver-level and algorithmic-level parallelism for MIPs.

Result: ParBalans performs competitively against Gurobi, especially on hard benchmarks.

Conclusion: ParBalans effectively leverages parallelism to enhance MIP solving, proving its viability for challenging instances.

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [14] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: The paper proposes a self-organizing UAV network framework using GDPO and SG-based incentives to ensure reliable connectivity and covert communication.


<details>
  <summary>Details</summary>
Motivation: The increasing use of UAV networks in sensitive applications demands reliable and covert communication, challenged by dynamic mobility and exposure risks.

Method: Combines Graph Diffusion-based Policy Optimization (GDPO) for dynamic topology generation with a Stackelberg Game (SG)-based incentive mechanism to guide UAV cooperation.

Result: The framework shows effectiveness in model convergence, topology generation quality, and covert communication enhancement.

Conclusion: The proposed framework successfully addresses connectivity and covert communication challenges in UAV networks.

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [15] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: FEAT is an AI framework for forensic death investigations, outperforming existing AI systems and matching human expert accuracy.


<details>
  <summary>Details</summary>
Motivation: Address workforce shortages and diagnostic variability in forensic cause-of-death determination, especially in high-volume systems like China's.

Method: Multi-agent AI framework with a domain-adapted LLM, integrating task decomposition, evidence analysis, iterative refinement, and conclusion synthesis.

Result: Outperformed state-of-the-art AI systems, achieved high expert concordance, and detected subtle evidentiary nuances.

Conclusion: FEAT offers scalable, consistent death certification with expert-level rigor, advancing equitable access to reliable medicolegal services.

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


### [16] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: Ultra-low-bit LLM models (1/1.58/2-bit) match full-precision performance. This work optimizes microkernels for CPUs, integrating them into PyTorch-TPP, achieving 2.2x speedup over SOTA runtimes and 7x over 16-bit models.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored computational efficiency of SOTA inference runtimes for ultra-low-bit LLM models, enabling efficient deployment on resource-constrained devices.

Method: Design and implement 1-bit and 2-bit microkernels optimized for modern CPUs, integrate them into PyTorch-TPP, and evaluate end-to-end inference performance.

Result: Achieves up to 2.2x speedup over bitnet.cpp and 7x over 16-bit models, advancing LLM inference efficiency for AI PCs and edge devices.

Conclusion: The optimized runtime enables efficient deployment of ultra-low-bit LLM models, enhancing performance for resource-constrained environments.

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [17] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: A modular prompting framework for safer, adaptive LLM use, grounded in human learning theory, improves scaffolding and adaptivity in tasks like education and gaming.


<details>
  <summary>Details</summary>
Motivation: To enhance the safety and adaptability of LLMs in dynamic, user-centered tasks by leveraging human learning theory (ZPD).

Method: Combines natural language boundary prompts with a control schema using fuzzy scaffolding logic and adaptation rules, avoiding fine-tuning or external orchestration.

Result: Outperforms standard prompting in scaffolding quality, adaptivity, and instructional alignment in simulated tutoring and other domains.

Conclusion: The framework offers a reusable, interpretable method for goal-aligned LLM behavior in uncertain or evolving contexts.

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [18] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: A novel framework using natural language and reinforcement learning to automate optimal viewpoint selection for volumetric data exploration.


<details>
  <summary>Details</summary>
Motivation: Challenges in selecting optimal viewpoints for volumetric data exploration, especially for non-experts, drive the need for an intuitive, automated solution.

Method: The framework encodes volumetric blocks, uses CLIP Score for semantic guidance, and employs reinforcement learning to navigate and select viewpoints aligning with user intent.

Result: Automated viewpoint selection improves efficiency and interpretability of volumetric data navigation.

Conclusion: The proposed method enhances volumetric data exploration by leveraging natural language and semantic cues for effective viewpoint selection.

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [19] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: The paper proposes shifting from vision-centered to language-centered remote sensing interpretation, using LLMs as a cognitive hub inspired by Global Workspace Theory. It addresses challenges like multimodal representation and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: Current vision-centered models lack capabilities in multi-modal reasoning and semantic abstraction. Integrating LLMs can unify understanding and decision-making in remote sensing.

Method: Proposes a language-centered framework using LLMs as a cognitive hub, addressing challenges like unified multimodal representation and knowledge association.

Result: A framework is constructed for language-centered remote sensing interpretation, with identified technical challenges and solutions.

Conclusion: The work lays a foundation for cognition-driven geospatial analysis, suggesting future research in adaptive alignment, trustworthy reasoning, and autonomous interaction.

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [20] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: The paper introduces MACA, a method for multi-level credit assignment in cooperative MARL, using counterfactual reasoning and attention to improve performance in complex tasks like Starcraft.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of credit assignment in MARL, where rewards are shared among agents with diverse and overlapping contributions.

Method: Multi-level Advantage Credit Assignment (MACA) integrates advantage functions for individual, joint, and correlated actions, using an attention-based framework.

Result: MACA outperforms other methods in Starcraft tasks, showing efficacy in complex credit assignment.

Conclusion: MACA effectively handles multi-level credit assignment, enhancing cooperative MARL performance.

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [21] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: MDK12-Bench is introduced as a large-scale benchmark for evaluating multimodal large language models (MLLMs) using real-world K-12 exams, addressing limitations of current benchmarks. It includes dynamic evaluation and knowledge-point analysis to improve model robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for MLLMs lack scale, coverage, and structured knowledge, leading to static evaluations. MDK12-Bench aims to provide a comprehensive, dynamic, and knowledge-driven assessment.

Method: MDK12-Bench is built from K-12 exams across six disciplines, featuring 141K instances and 6,225 knowledge points. It includes dynamic evaluation with unfamiliar shifts and KP-RAG for knowledge-driven reasoning.

Result: Current MLLMs show limitations in handling difficulty levels, temporal shifts, contextual shifts, and knowledge-driven reasoning. The benchmark highlights areas for improvement.

Conclusion: MDK12-Bench offers a robust framework for evaluating MLLMs, guiding enhancements in model robustness, interpretability, and AI-assisted education.

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [22] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: The paper introduces MP-Bench, a large-scale dataset for severe weather prediction, and MMLM, a multimodal model addressing challenges in AI-driven weather forecasting.


<details>
  <summary>Details</summary>
Motivation: Current weather forecasting relies on manual interpretation, which is subjective and burdensome. AI offers potential but faces challenges like data scarcity and high-dimensional data alignment.

Method: Developed MP-Bench dataset and MMLM model with adaptive fusion modules for 4D meteorological data.

Result: MMLM performs exceptionally well on MP-Bench, demonstrating effectiveness in severe weather prediction.

Conclusion: The work advances AI-driven weather forecasting, with public release of code and dataset.

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [23] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: The paper introduces pushdown reward machines (pdRMs), an extension of reward machines, to handle more complex behaviors in reinforcement learning by recognizing deterministic context-free languages.


<details>
  <summary>Details</summary>
Motivation: To enhance the expressiveness of reward machines (RMs) in reinforcement learning (RL) by enabling them to recognize and reward temporally extended behaviors representable in deterministic context-free languages.

Method: Proposes pdRMs based on deterministic pushdown automata, introduces two variants of pdRM-based policies (full stack vs. top k symbols), and provides theoretical and experimental validation.

Result: Theoretical results confirm pdRMs' expressive power and space complexity, while experiments demonstrate successful training of agents for tasks in deterministic context-free languages.

Conclusion: pdRMs extend the capabilities of RMs, improving RL sample efficiency for more complex behaviors, with practical applications validated through experiments.

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [24] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: The paper introduces Distributed Guided Local Search (DGLS) to improve local search in DCOPs by addressing issues in GDBA, showing superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Local search in DCOPs often converges to poor optima, and GDBA's empirical benefits are marginal. The paper aims to enhance performance by addressing GDBA's shortcomings.

Method: Proposes DGLS with adaptive violation conditions, penalty evaporation, and synchronized updates to improve GDBA.

Result: DGLS outperforms baselines, especially on structured problems, with significant margins (3.77%--66.3%).

Conclusion: DGLS effectively addresses GDBA's issues and demonstrates superior performance in DCOPs.

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [25] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: CRAMF is a framework for improving LLM-based autoformalization by retrieving formal definitions of mathematical concepts, addressing challenges like model hallucination and semantic gaps.


<details>
  <summary>Details</summary>
Motivation: Manual formalization in ITPs is labor-intensive and requires expertise; automated solutions face issues like model hallucination and semantic gaps.

Method: CRAMF retrieves formal definitions from Mathlib4, uses contextual query augmentation, and employs a dual-channel hybrid retrieval strategy.

Result: CRAMF improves translation accuracy by up to 62.1% (avg 29.9%) on benchmarks like miniF2F and ProofNet.

Conclusion: CRAMF effectively enhances autoformalization by grounding LLMs with retrieved formal definitions, addressing key challenges in the field.

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [26] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: The paper explores Transformer-based models for multimodal learning in crop yield prediction, emphasizing interpretability using self-attention mechanisms and comparing methods like Attention Rollout (AR) and Generic Attention (GA) against Shapley Value Sampling (SVS).


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in complex multimodal learning architectures, particularly in agriculture, by leveraging Transformer-based models for explainability.

Method: Uses self-attention mechanisms (AR, GA) and proposes Weighted Modality Activation (WMA) for feature and modality attributions, comparing them with SVS. Evaluates on large datasets with four input modalities.

Result: Transformer-based models outperform convolutional and recurrent networks, with higher R2 scores. AR provides more reliable temporal attributions than GA and SVS. Modality attributions vary across methods.

Conclusion: Transformer-based models enhance interpretability and performance in multimodal crop yield prediction, with AR being a robust method for temporal attributions.

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [27] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: The paper argues against using LLMs like ChatGPT to simulate human psychology in research, showing conceptual and empirical flaws in this approach.


<details>
  <summary>Details</summary>
Motivation: To caution against the growing trend of using LLMs as substitutes for human participants in psychological studies, highlighting their unreliability.

Method: Provides conceptual arguments and empirical evidence, including tests with the CENTAUR model, to show discrepancies between LLM and human responses.

Result: LLMs exhibit notable inconsistencies in responses to slight wording changes and vary widely among models, proving unreliable for simulating human psychology.

Conclusion: LLMs should not replace human participants in psychology studies; they must be validated against human responses for each new application.

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [28] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: The paper introduces DatasetResearch, a benchmark for evaluating AI agents' ability to discover and synthesize datasets, revealing significant gaps in current capabilities.


<details>
  <summary>Details</summary>
Motivation: The shift from computational power to data availability as the bottleneck in AI development, with many valuable datasets hidden across repositories, prompts the need for autonomous dataset discovery.

Method: The study evaluates AI agents using DatasetResearch, a tri-dimensional framework, on 208 real-world demands across knowledge and reasoning tasks.

Result: Advanced systems achieve only 22% on the challenging DatasetResearch-pro subset, highlighting limitations in dataset discovery.

Conclusion: The benchmark sets a baseline for future AI systems, emphasizing the need for improved capabilities in autonomous dataset discovery.

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [29] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: MASteer is an end-to-end framework for trustworthiness repair in LLMs, using representation engineering with automated sample generation and adaptive steering strategies, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing repair methods (SFT, RLHF) are costly and slow, while prompt engineering lacks robustness. Representation engineering offers a lightweight alternative but lacks automation.

Method: MASteer combines AutoTester (multi-agent sample generation) and AutoRepairer (adaptive steering strategies) for automated, context-aware repair.

Result: MASteer improves metrics by 15.36% on LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, maintaining general model capabilities.

Conclusion: MASteer provides scalable, efficient trustworthiness repair with strong robustness and practical value.

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [30] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse is a modular framework for distributed ML inference with targeted cryptographic verification, reducing costs by focusing on key subcomputations.


<details>
  <summary>Details</summary>
Motivation: To minimize trust and costs in distributed ML inference by avoiding full-model circuitization and enabling flexible verification.

Method: Uses verifiable 'slices' of subcomputations, enforced via audit, replication, or incentives, with support for multiple proving systems.

Result: Empirical evaluation shows performance in memory usage, runtime, and circuit behavior under sliced and unsliced setups.

Conclusion: DSperse offers scalable, flexible verification strategies for diverse ML deployment needs.

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [31] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: A framework using active inference models decision-making in embodied agents, leveraging biologically inspired systems for explainable AI.


<details>
  <summary>Details</summary>
Motivation: To understand purposeful behavior in autonomous agents and develop safe, efficient, and explainable AI systems.

Method: Proposes an active inference framework with experiment-informed generative models, tested in a simulated game-play environment.

Result: Demonstrates learning in agents, highlighting memory-based learning and predictive planning in decision-making.

Conclusion: Contributes to explainable AI with a scalable, biologically grounded approach to understanding agent behavior.

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [32] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: The paper explores alternative techniques for hitting set optimization in the implicit hitting set (IHS) framework, comparing pseudo-Boolean reasoning and stochastic local search to traditional integer programming. It highlights trade-offs between efficiency and reliability, with exact PB reasoning proving competitive and offering correctness certificates.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional integer programming in IHS, such as numerical instability, and to explore more reliable and efficient alternatives like pseudo-Boolean reasoning.

Method: The study evaluates different algorithmic techniques for hitting set optimization, including pseudo-Boolean reasoning and stochastic local search, comparing them to commercial IP solvers.

Result: Exact PB reasoning is competitive with numerically exact IP solvers and avoids correctness issues. PB reasoning also enables correctness certificates for IHS computations.

Conclusion: While commercial IP solvers remain effective, PB reasoning offers a reliable and competitive alternative for HS computations, with added benefits like correctness certification.

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [33] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: MultiMedEdit is the first benchmark for evaluating knowledge editing (KE) in multimodal medical tasks, addressing gaps in current methods and providing metrics for reliability, generality, and locality.


<details>
  <summary>Details</summary>
Motivation: Existing KE methods lack focus on multimodal medical scenarios, which require integrating updated knowledge with visual reasoning for safe clinical decisions.

Method: Proposes MultiMedEdit, a framework for KE in clinical multimodal tasks, with a three-dimensional metric suite and cross-paradigm comparisons.

Result: Current KE methods struggle with generalization and long-tail reasoning in clinical workflows, with trade-offs in efficiency for real-world deployment.

Conclusion: MultiMedEdit highlights limitations of current KE approaches and lays groundwork for future clinically robust techniques.

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [34] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: The paper explores the limitations of Large Language Models (LLMs) in moderating nuanced content like hate speech and biases, proposing a framework and a fine-tuned model, SafePhi, which outperforms existing moderators.


<details>
  <summary>Details</summary>
Motivation: AI systems, especially LLMs, need safer and more reliable moderation due to their errors in moral reasoning and bias detection, despite their advanced capabilities.

Method: Developed an experimental framework using SOTA models to evaluate LLMs on a diverse benchmark dataset (49 categories) and introduced SafePhi, a QLoRA fine-tuned version of Phi-4.

Result: SafePhi achieved a Macro F1 score of 0.89, outperforming OpenAI Moderator (0.77) and Llama Guard (0.74), but identified critical underperformance areas in LLMs.

Conclusion: The study underscores the need for more diverse data and human-in-the-loop approaches to improve LLM robustness and explainability in moderation tasks.

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [35] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: A Feedback-Driven Decision Support System (DSS) with incremental retraining improves student performance prediction accuracy by 10.7% RMSE reduction, adapting to new data like post-intervention outcomes.


<details>
  <summary>Details</summary>
Motivation: Static machine learning models in education lack adaptability to new data, necessitating a dynamic system for continuous improvement and timely academic intervention.

Method: Proposes a closed-loop DSS with LightGBM-based regressor and incremental retraining, featuring a Flask web interface and SHAP for explainability.

Result: 10.7% reduction in RMSE after retraining, with improved prediction accuracy and upward adjustments for intervened students.

Conclusion: The system advances educational analytics by transforming static predictors into adaptive, human-centered, and responsive AI, suitable for LMS integration.

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [36] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: EndoAgent is a memory-guided AI agent for endoscopic analysis, integrating iterative reasoning and adaptive tool selection, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing AI methods lack unified coordination for multi-step clinical workflows in endoscopy, and AI agents' potential in this domain is underexplored.

Method: EndoAgent uses a dual-memory design for logical coherence (short-term tracking) and enhanced reasoning (long-term learning), integrating expert tools in a unified loop.

Result: EndoAgent outperforms general and medical multimodal models, demonstrating strong flexibility and reasoning.

Conclusion: EndoAgent advances endoscopic AI by combining memory-guided reasoning with adaptive tool use, validated by a new benchmark (EndoAgentBench).

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [37] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: The paper formalizes LLMs as probabilistic Turing machines, proving illusions are inevitable, but proposes two escape routes: RAGs as oracle machines and continuous learning via neural game theory.


<details>
  <summary>Details</summary>
Motivation: To address the core obstacle of illusions in LLMs for reliable deployment.

Method: Constructs a 'computational necessity hierarchy', proves inevitability of illusions, and introduces RAGs and continuous learning as solutions.

Result: Proves illusions are inevitable but provides formal theories for RAGs and continuous learning as escape routes.

Conclusion: Proposes practical solutions to mitigate LLM illusions, enhancing their reliability.

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [38] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: The paper introduces Comp-Comp, an iterative benchmarking framework for domain-specific LLMs, emphasizing comprehensiveness and compactness over scaling laws, and validates it with XUBench in academia.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs rely heavily on scaling laws, neglecting the impact of corpus and QA set design on precision and recall in specific domains.

Method: Proposes Comp-Comp, a framework balancing comprehensiveness (semantic recall) and compactness (precision) for corpus and QA set construction, validated via a case study in academia.

Result: Developed XUBench, a large-scale closed-domain benchmark, demonstrating Comp-Comp's effectiveness in academic settings.

Conclusion: Comp-Comp offers a scalable and extensible alternative to scaling laws for domain-specific benchmark construction, applicable beyond academia.

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [39] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: Pentest-R1 is a reinforcement learning framework that enhances LLMs for penetration testing, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with error handling, reasoning, and autonomous task execution in penetration testing.

Method: Uses a two-stage reinforcement learning pipeline: offline RL on real-world walkthroughs and online RL in a CTF environment.

Result: Achieves 24.2% success on AutoPenBench and 15.0% on Cybench, outperforming most models.

Conclusion: The synergy of offline and online RL is key to Pentest-R1's success.

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [40] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: Invert4TVG enhances Temporal Video Grounding by integrating inversion tasks (Verb Completion, Action Recognition, Video Description) to improve both localization accuracy and semantic action understanding, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current TVG methods overfit to temporal IoU, compromising semantic action understanding, which is crucial for robust TVG.

Method: Introduces Invert4TVG with three inversion tasks (Verb Completion, Action Recognition, Video Description) integrated via reinforcement learning.

Result: Achieves a 7.1% improvement in R1@0.7 on Charades-STA compared to Time-R1.

Conclusion: Invert4TVG strengthens semantic understanding and localization accuracy, setting a new benchmark for TVG.

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [41] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: The paper evaluates BERTopic and NMF for topic modeling in strategic plan development for government organizations, showing BERTopic's superior performance.


<details>
  <summary>Details</summary>
Motivation: To leverage GAI for automating strategic plan development in government, overcoming regulatory challenges.

Method: BERTopic and NMF are trained on GAO reports to generate topics, which are compared against Vision Elements of a strategic plan.

Result: Both models generate themes matching 100% of evaluated elements, with BERTopic achieving stronger correlations.

Conclusion: BERTopic is more effective for this application, with potential to impact a multi-billion dollar industry and aid federal government compliance.

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [42] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: The paper proposes a framework for integrating large language models (LLMs) with multi-agent decision-making, focusing on prompt engineering, memory architectures, multi-modal processing, and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs' capabilities for clear communication and coordination in multi-agent settings, addressing social dilemmas and game-theoretic challenges.

Method: Systematic framework integrating LLMs with multi-agent algorithms, using advanced prompt engineering, memory architectures, multi-modal processing, and fine-tuning.

Result: Evaluated through ablation studies in game settings with social dilemmas, demonstrating the framework's effectiveness.

Conclusion: The proposed framework successfully enhances LLMs for multi-agent collaboration, offering practical integration strategies.

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [43] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: A new agentic approach using a pure ReAct-based Python coding agent successfully translates natural language problem descriptions into formal constraint models, solving all 101 CP-Bench problems without fixed workflows.


<details>
  <summary>Details</summary>
Motivation: Automating the translation of natural language to constraint models is challenging due to the need for deep expertise and limitations of fixed workflows in prior approaches.

Method: A general-purpose Python coding agent based on ReAct, using a persistent IPython kernel for stateful execution and iterative development, with domain expertise injected via a project prompt.

Result: The agent solved all 101 problems in the CP-Bench benchmark, demonstrating effectiveness without specialized architectures or predefined workflows.

Conclusion: Constraint modeling benefits from combining general coding tools and prompt-encoded domain expertise, not specialized architectures or fixed workflows.

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [44] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: The paper introduces a harness enabling local LLMs to play Diplomacy without fine-tuning, using optimized game state representation and tooling for analysis.


<details>
  <summary>Details</summary>
Motivation: Diplomacy's complexity and high variance made it prohibitive for study; this work aims to democratize evaluation of strategic reasoning in LLMs.

Method: Data-driven iteration optimized textual game state representation, enabling 24B models to complete matches. Tooling for hypothesis testing and Critical State Analysis was developed.

Result: Larger models performed best, but smaller ones were adequate. Insights into persuasion, aggressive playstyles, and model performance were gained.

Conclusion: The harness eliminates fine-tuning needs, democratizing LLM evaluation and revealing natural strategic reasoning capabilities in widely used models.

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [45] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: The paper introduces MCPToolBench++, a benchmark for evaluating LLMs' ability to use MCP tools, addressing gaps in datasets, diverse response formats, and context window limitations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack comprehensive datasets and struggle with diverse MCP tool response formats and LLM context limitations.

Method: Proposes MCPToolBench++, a large-scale benchmark built on 4k+ MCP servers from 40+ categories, evaluating single- and multi-step tool calls.

Result: Evaluated SOTA LLMs with agentic abilities on the benchmark and reported performance.

Conclusion: MCPToolBench++ addresses evaluation challenges for LLMs using MCP tools, providing a standardized benchmark.

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [46] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: A novel covert semantic communication framework uses a jammer to protect transmitted semantic information from eavesdropping, optimizing transmission and power via a reinforcement learning algorithm.


<details>
  <summary>Details</summary>
Motivation: To prevent attackers from eavesdropping on semantic information (image meaning) transmitted over time slots, ensuring privacy and transmission quality.

Method: A prioritised sampling assisted twin delayed deep deterministic policy gradient algorithm optimizes semantic information and transmit power without server-jammer communication.

Result: Improves privacy by 77.8% and transmission quality by 14.3% compared to traditional methods.

Conclusion: The proposed framework and algorithm effectively enhance privacy and semantic transmission quality in covert communication.

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [47] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: HGMF improves tool selection for LLMs by hierarchically pruning irrelevant options using Gaussian Mixture Models, enhancing accuracy and reducing latency.


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting the correct tool from large, hierarchical libraries for LLMs due to limited context windows and noise from irrelevant options.

Method: HGMF maps queries and tools into a semantic space, clusters and filters tools hierarchically using GMMs, and produces a high-relevance candidate set.

Result: HGMF significantly improves tool selection accuracy and reduces inference latency in experiments.

Conclusion: HGMF is scalable and effective for large-scale tool libraries, addressing the tool selection challenge for LLMs.

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [48] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: ThinkTuning is a GRPO-based interactive training method where a teacher model guides a student model through feedback, improving reasoning capabilities by 3.85% on average over zero-shot baselines.


<details>
  <summary>Details</summary>
Motivation: Current RL methods don't instill new reasoning abilities in LLMs but only draw out existing behaviors. The goal is to train models lacking such abilities to develop them.

Method: ThinkTuning uses a teacher-student framework where the teacher provides corrective feedback during rollouts, reshaping the student's reasoning.

Result: Improvements of 3.85% on average over zero-shot baselines, with specific gains on MATH-500 (2.08%), AIME (2.23%), and GPQA-Diamond (3.99%) over vanilla-GRPO.

Conclusion: ThinkTuning effectively enhances reasoning in student models through teacher-guided feedback, outperforming baselines.

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [49] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: Multimodal AI integrates diverse data streams for better poultry welfare monitoring, with feature-level fusion being optimal. Challenges include sensor fragility and costs, addressed by new tools (DTS, DRI) and a modular deployment framework.


<details>
  <summary>Details</summary>
Motivation: Traditional welfare checks are subjective and limited; multimodal AI can provide deeper, data-driven insights into laying hen welfare.

Method: Uses multimodal AI (visual, acoustic, environmental, physiological data) with feature-level fusion. Introduces DTS and DRI for evaluation and proposes a modular deployment framework.

Result: Feature-level fusion balances robustness and performance. New tools (DTS, DRI) and framework address adoption barriers like sensor fragility and generalizability.

Conclusion: Multimodal AI enables proactive, precision-driven welfare systems, combining productivity with ethical animal care.

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [50] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav introduces a modular, skill-based framework for Vision-and-Language Navigation (VLN), improving generalization and performance on benchmarks like R2R and GSA-R2R.


<details>
  <summary>Details</summary>
Motivation: Current VLN methods struggle with generalization, especially in unseen scenarios requiring complex reasoning.

Method: Decomposes navigation into atomic skills (e.g., Vertical Movement) and uses a VLM-based router to dynamically select specialized agents.

Result: Achieves state-of-the-art performance on R2R and strong generalization on GSA-R2R.

Conclusion: SkillNav's modular approach enhances VLN agents' interpretability and adaptability.

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [51] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: DiMuST is a POI recommendation model using disentangled representation learning to align spatial-temporal transitions and social relationships, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing POI recommendation models misalign spatial-temporal transitions, causing redundancy and reduced interpretability.

Method: DiMuST uses a Disentangled variational multiplex graph Auto-Encoder (DAE) to disentangle shared/private distributions, fuses shared features via PoE, and denoises private features with contrastive constraints.

Result: DiMuST outperforms existing methods on two datasets across multiple metrics.

Conclusion: DiMuST effectively captures spatial-temporal transitions while preserving their intrinsic correlations, improving POI recommendations.

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [52] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: A multi-agent framework reduces private info leakage in LLMs by decomposing privacy tasks, outperforming single-agent baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns in LLMs processing multi-source data (e.g., meetings with private/public info) is challenging.

Method: Introduces a multi-agent framework for specialized subtasks (extraction, classification) with iterative validation.

Result: Reduces private info leakage by 18% (ConfAIde) and 19% (PrivacyLens) with GPT-4o, preserving public content fidelity.

Conclusion: Principled information-flow design in multi-agent systems shows promise for contextual privacy with LLMs.

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [53] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: The paper proposes Ethics2Vec, an extension of the Anything2vec approach, to map AI decision-making strategies to vector representations for ethical alignment with human values.


<details>
  <summary>Details</summary>
Motivation: The challenge of aligning AI systems with human ethical values, especially when dealing with incommensurable criteria like human life and cost, necessitates a common metric space.

Method: Extends the Anything2vec approach to ethics, mapping agent decision-making strategies to multivariate vectors for comparison and alignment assessment.

Result: Introduces Ethics2Vec for binary decision-making and extends it to automatic control laws, like in self-driving cars.

Conclusion: Ethics2Vec provides a scalable method to assess and align AI behavior with human ethical values, addressing the alignment problem.

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [54] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: Transformers struggle with planning tasks due to problem symmetries. A novel contrastive learning method makes them symmetry-aware, improving performance in plan-generation and heuristic-prediction.


<details>
  <summary>Details</summary>
Motivation: Transformers lack inductive bias for planning tasks, leading to poor extrapolation from easy to hard problems due to problem symmetries.

Method: Proposed a contrastive learning objective to make transformers symmetry-aware, combined with architectural improvements.

Result: Effective and efficient training for plan-generation and heuristic-prediction, outperforming PlanGPT.

Conclusion: Symmetry-aware training addresses transformer limitations in automated planning.

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [55] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: The paper introduces optimal robust best-effort (ORBE) policies for robust MDPs (RMDPs), refining policy selection by maximizing worst-case and non-adversarial expected returns.


<details>
  <summary>Details</summary>
Motivation: Existing RMDP methods focus on worst-case performance, but multiple optimal policies may differ under non-adversarial conditions, necessitating a refined selection criterion.

Method: Proposes ORBE policies, inspired by game theory, and provides an algorithm to compute them with minimal overhead compared to robust value iteration.

Result: ORBE policies exist, their structure is characterized, and numerical experiments confirm feasibility.

Conclusion: ORBE policies offer a principled way to break ties among optimal robust policies, enhancing RMDP solutions.

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [56] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: A Knowledge Graph aids first responders by providing AI-driven treatment recommendations in emergency situations.


<details>
  <summary>Details</summary>
Motivation: The increasing need for rapid, personalized healthcare in emergencies requires innovative tools to assist first responders.

Method: The paper introduces a Knowledge Graph as a central knowledge representation, leveraging AI for situation pre-recognition and treatment recommendations.

Result: The Knowledge Graph enables intelligent, timely medical recommendations for first responders.

Conclusion: The proposed system enhances emergency care by integrating AI-driven knowledge management for optimized treatment.

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [57] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: X-evolve is a novel method combining LLMs and EAs to evolve solution spaces, reducing LLM call costs and improving optimization efficiency across hard problems.


<details>
  <summary>Details</summary>
Motivation: Current methods evolve individual solutions, incurring high LLM call costs. X-evolve aims to reduce costs and enhance exploration by evolving solution spaces.

Method: X-evolve uses LLMs to generate tunable programs defining solution spaces, then employs a score-based search algorithm to explore these spaces efficiently.

Result: Demonstrated efficacy in three problems: improved cap set bounds, larger independent sets in graph theory, and better heuristics for bin packing.

Conclusion: Evolving solution spaces enhances search effectiveness, enabling high-dimensional problem-solving previously deemed prohibitive.

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


### [58] [Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots](https://arxiv.org/abs/2508.07941)
*Olivier Poulet,Frédéric Guinand,François Guérin*

Main category: cs.AI

TL;DR: A method using LSTM for short-term position prediction and DQN for collision risk anticipation reduces collisions in constrained robot environments.


<details>
  <summary>Details</summary>
Motivation: To anticipate and reduce collision risks between robots moving without communication or identifiers in constrained environments.

Method: Uses LSTM to predict robot positions and dynamically adjusts DQN rewards to anticipate collisions.

Result: Significant decrease in collisions and improved stability, even with low sampling frequency (1 Hz).

Conclusion: The method is computationally efficient and suitable for embedded systems.

Abstract: This article proposes a collision risk anticipation method based on
short-term prediction of the agents position. A Long Short-Term Memory (LSTM)
model, trained on past trajectories, is used to estimate the next position of
each robot. This prediction allows us to define an anticipated collision risk
by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.
The approach is tested in a constrained environment, where two robots move
without communication or identifiers. Despite a limited sampling frequency (1
Hz), the results show a significant decrease of the collisions number and a
stability improvement. The proposed method, which is computationally
inexpensive, appears particularly attractive for implementation on embedded
systems.

</details>


### [59] [Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths](https://arxiv.org/abs/2508.08001)
*Rui Yao,Qi Chai,Jinhai Yao,Siyuan Li,Junhao Chen,Qi Zhang,Hao Wang*

Main category: cs.AI

TL;DR: The paper proposes an LLM-based, uncertainty-aware framework to decode Fedspeak, improving policy stance classification and model reliability.


<details>
  <summary>Details</summary>
Motivation: Fedspeak encodes implicit policy signals, and automating its interpretation is crucial for financial forecasting and policy analysis.

Method: The framework incorporates domain-specific reasoning and a dynamic uncertainty decoding module to enhance semantic representation and prediction confidence.

Result: The framework achieves state-of-the-art performance in policy stance analysis, with perceptual uncertainty correlating positively with model error rates.

Conclusion: The proposed method effectively deciphers Fedspeak, offering reliable and accurate policy stance classification.

Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a communication tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an LLM-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.

</details>


### [60] [Fitting Description Logic Ontologies to ABox and Query Examples](https://arxiv.org/abs/2508.08007)
*Maurice Funk,Marvin Grosser,Carsten Lutz*

Main category: cs.AI

TL;DR: The paper studies ontology fitting for querying, analyzing complexity for AQs, CQs, and UCQs in ALC and ALCI.


<details>
  <summary>Details</summary>
Motivation: To address the problem of fitting an ontology to satisfy positive and negative query examples in ontology-mediated querying.

Method: Uses description logics ALC and ALCI, analyzing fitting problems for AQs, CQs, and UCQs.

Result: Fitting is CONP for AQs and full CQs, and 2EXPTIME-complete for CQs and UCQs.

Conclusion: The study provides computational complexity results for ontology fitting across query languages.

Abstract: We study a fitting problem inspired by ontology-mediated querying: given a
collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash
q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for
all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as
ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be
${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

</details>


### [61] [AdaptFlow: Adaptive Workflow Optimization via Meta-Learning](https://arxiv.org/abs/2508.08053)
*Runchuan Zhu,Bowen Jiang,Lingrui Mei,Fangkai Yang,Lu Wang,Haoxiang Gao,Fengshuo Bai,Pu Zhao,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: AdaptFlow is a meta-learning framework for LLM workflows, enabling rapid adaptation to diverse tasks via language-guided modifications, outperforming manual and automated baselines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM workflows rely on static templates, limiting adaptability and scalability. AdaptFlow aims to address this by learning generalizable workflow initializations.

Method: Uses a bi-level optimization scheme: inner loop refines workflows for subtasks with LLM feedback; outer loop updates shared initialization for cross-task performance.

Result: Outperforms baselines in QA, code generation, and math reasoning, achieving state-of-the-art results with strong generalization.

Conclusion: AdaptFlow demonstrates effective generalization to unseen tasks, offering a scalable and adaptable solution for LLM workflows.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in agentic workflows, which are structured sequences of LLM invocations
intended to solve complex tasks. However, existing approaches often rely on
static templates or manually designed workflows, which limit adaptability to
diverse tasks and hinder scalability. We propose AdaptFlow, a natural
language-based meta-learning framework inspired by model-agnostic meta-learning
(MAML). AdaptFlow learns a generalizable workflow initialization that enables
rapid subtask-level adaptation. It employs a bi-level optimization scheme: the
inner loop refines the workflow for a specific subtask using LLM-generated
feedback, while the outer loop updates the shared initialization to perform
well across tasks. This setup allows AdaptFlow to generalize effectively to
unseen tasks by adapting the initialized workflow through language-guided
modifications. Evaluated across question answering, code generation, and
mathematical reasoning benchmarks, AdaptFlow consistently outperforms both
manually crafted and automatically searched baselines, achieving
state-of-the-art results with strong generalization across tasks and models.
The source code and data are available at
https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.

</details>


### [62] [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](https://arxiv.org/abs/2508.08075)
*Meishen He,Wenjun Ma,Jiao Wang,Huijun Yue,Xiaoma Fan*

Main category: cs.AI

TL;DR: The paper proposes FNBT, a method for open-world information fusion under uncertainty, addressing heterogeneous frames in data fusion.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with heterogeneous frames from diverse data sources, leading to unsatisfactory fusion results.

Method: FNBT introduces a criterion for open-world tasks, extends frames to accommodate heterogeneity, and uses a full negation mechanism for mass function transformation.

Result: FNBT satisfies theoretical properties (invariance, heritability, conflict elimination) and outperforms in classification tasks, resolving Zadeh's counterexample.

Conclusion: FNBT effectively handles open-world information fusion, validated by theoretical and empirical results.

Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field
of information fusion under uncertainty. Most existing research focuses on
combining evidence within the same frame of discernment. However, in real-world
scenarios, trained algorithms or data often originate from different regions or
organizations, where data silos are prevalent. As a result, using different
data sources or models to generate basic probability assignments may lead to
heterogeneous frames, for which traditional fusion methods often yield
unsatisfactory results. To address this challenge, this study proposes an
open-world information fusion method, termed Full Negation Belief
Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a
criterion is introduced to determine whether a given fusion task belongs to the
open-world setting. Then, by extending the frames, the method can accommodate
elements from heterogeneous frames. Finally, a full negation mechanism is
employed to transform the mass functions, so that existing combination rules
can be applied to the transformed mass functions for such information fusion.
Theoretically, the proposed method satisfies three desirable properties, which
are formally proven: mass function invariance, heritability, and essential
conflict elimination. Empirically, FNBT demonstrates superior performance in
pattern classification tasks on real-world datasets and successfully resolves
Zadeh's counterexample, thereby validating its practical effectiveness.

</details>


### [63] [TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork](https://arxiv.org/abs/2508.08115)
*Pranav Pushkar Mishra,Mohammad Arvan,Mohan Zalake*

Main category: cs.AI

TL;DR: TeamMedAgents integrates human teamwork components into LLM-based medical decision-making, improving performance in 7 out of 8 medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: To translate human teamwork theories into computational multi-agent systems for better medical decision-making.

Method: Operationalizes six teamwork components (e.g., leadership, trust) in a modular architecture, evaluated across eight medical benchmarks.

Result: Consistent improvements in 7/8 datasets, with optimal teamwork configurations varying by task complexity and domain.

Conclusion: TeamMedAgents advances collaborative AI by systematically applying human teamwork theories to multi-agent systems in critical domains.

Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (LLMs). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop communication, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.

</details>


### [64] [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127)
*Rui Miao,Yixin Liu,Yili Wang,Xu Shen,Yue Tan,Yiwei Dai,Shirui Pan,Xin Wang*

Main category: cs.AI

TL;DR: BlindGuard is an unsupervised defense method for LLM-based multi-agent systems, detecting malicious agents without labeled data by analyzing interaction patterns and using noise injection and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing supervised defense methods rely on labeled malicious agents, making them impractical for real-world scenarios. BlindGuard aims to provide a generalizable and practical solution without such dependencies.

Method: BlindGuard uses a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns. It employs a corruption-guided detector with directional noise injection and contrastive learning, trained solely on normal agent behaviors.

Result: BlindGuard effectively detects diverse attack types (prompt injection, memory poisoning, tool attack) across various MAS communication patterns, outperforming supervised baselines in generalizability.

Conclusion: BlindGuard offers a practical, unsupervised solution for securing LLM-based multi-agent systems, demonstrating strong performance and generalizability without requiring attack-specific labels.

Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various communication patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.

</details>


### [65] [From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework](https://arxiv.org/abs/2508.08147)
*Yunkai Hu,Tianqiao Zhao,Meng Yue*

Main category: cs.AI

TL;DR: A novel LLM-assisted agent converts natural-language power system descriptions into solver-ready formulations, ensuring feasible and optimal solutions by integrating domain-aware prompts, validation, and iterative repair.


<details>
  <summary>Details</summary>
Motivation: Direct LLM use for power system optimization often yields infeasible or suboptimal results due to lack of numerical precision and constraint handling. The goal is to bridge high-level descriptions with executable models for efficient decision-making.

Method: The pipeline combines domain-aware prompts with an LLM, enforces feasibility through validation and repair, and generates solver-ready formulations. The unit commitment problem serves as a case study.

Result: The agent produces optimal or near-optimal schedules with objective costs, showing that solver coupling with validation enhances reliability.

Conclusion: Combining AI with optimization frameworks efficiently bridges problem descriptions and executable models, improving energy system decision-making.

Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent
that automatically converts natural-language descriptions of power system
optimization scenarios into compact, solver-ready formulations and generates
corresponding solutions. In contrast to approaches that rely solely on LLM to
produce solutions directly, the proposed method focuses on discovering a
mathematically compatible formulation that can be efficiently solved by
off-the-shelf optimization solvers. Directly using LLMs to produce solutions
often leads to infeasible or suboptimal results, as these models lack the
numerical precision and constraint-handling capabilities of established
optimization solvers. The pipeline integrates a domain-aware prompt and schema
with an LLM, enforces feasibility through systematic validation and iterative
repair, and returns both solver-ready models and user-facing results. Using the
unit commitment problem as a representative case study, the agent produces
optimal or near-optimal schedules along with the associated objective costs.
Results demonstrate that coupling the solver with task-specific validation
significantly enhances solution reliability. This work shows that combining AI
with established optimization frameworks bridges high-level problem
descriptions and executable mathematical models, enabling more efficient
decision-making in energy systems

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems](https://arxiv.org/abs/2508.06539)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: The paper proposes a geometric theory of survival, framing it as an emergent property of biological state space curvature and flow, rather than a supervised learning task.


<details>
  <summary>Details</summary>
Motivation: To move beyond traditional supervised survival modeling by grounding survival in biophysical and geometric principles.

Method: Develops Self-Organizing Survival Manifolds (SOSM), using geodesic curvature minimization to align prognosis with geometric flow stability.

Result: Theoretical proofs show survival-aligned trajectories emerge under biologically plausible conditions, linking survival to thermodynamics and optimal transport.

Conclusion: Survival is reframed as a geometric phase transition, offering a universal, label-free foundation bridging machine learning, biophysics, and geometry.

Abstract: Survival is traditionally modeled as a supervised learning task, reliant on
curated outcome labels and fixed covariates. This work rejects that premise. It
proposes that survival is not an externally annotated target but a geometric
consequence: an emergent property of the curvature and flow inherent in
biological state space. We develop a theory of Self-Organizing Survival
Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature
geodesic flows on latent manifolds shaped by internal biological constraints. A
survival energy functional based on geodesic curvature minimization is
introduced and shown to induce structures where prognosis aligns with geometric
flow stability. We derive discrete and continuous formulations of the objective
and prove theoretical results demonstrating the emergence and convergence of
survival-aligned trajectories under biologically plausible conditions. The
framework draws connections to thermodynamic efficiency, entropy flow, Ricci
curvature, and optimal transport, grounding survival modeling in physical law.
Health, disease, aging, and death are reframed as geometric phase transitions
in the manifold's structure. This theory offers a universal, label-free
foundation for modeling survival as a property of form, not annotation-bridging
machine learning, biophysics, and the geometry of life itself.

</details>


### [67] [Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering](https://arxiv.org/abs/2508.06574)
*Fatemeh Moradi,Mehran Tarif,Mohammadhossein Homaei*

Main category: cs.LG

TL;DR: A two-phase learning framework combines unsupervised anomaly detection (Isolation Forest) and semi-supervised refinement (self-training SVM) to detect supply chain fraud effectively, achieving an F1-score of 0.817.


<details>
  <summary>Details</summary>
Motivation: Fraud detection in supply chains is challenging due to complexity, class imbalance, and limited labeled data. Traditional methods struggle with these issues.

Method: Phase 1: Isolation Forest for unsupervised anomaly detection. Phase 2: Self-training SVM for semi-supervised refinement using labeled and pseudo-labeled data.

Result: Achieves an F1-score of 0.817 with a false positive rate below 3.0% on the DataCo Smart Supply Chain Dataset.

Conclusion: The framework is effective for supply chain fraud detection but has limitations like concept drift and lacks comparison with deep learning methods.

Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the
complexity of global networks and the scarcity of labeled data. Traditional
detection methods often struggle with class imbalance and limited supervision,
reducing their effectiveness in real-world applications. This paper proposes a
novel two-phase learning framework to address these challenges. In the first
phase, the Isolation Forest algorithm performs unsupervised anomaly detection
to identify potential fraud cases and reduce the volume of data requiring
further analysis. In the second phase, a self-training Support Vector Machine
(SVM) refines the predictions using both labeled and high-confidence
pseudo-labeled samples, enabling robust semi-supervised learning. The proposed
method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive
real-world supply chain dataset with fraud indicators. It achieves an F1-score
of 0.817 while maintaining a false positive rate below 3.0%. These results
demonstrate the effectiveness and efficiency of combining unsupervised
pre-filtering with semi-supervised refinement for supply chain fraud detection
under real-world constraints, though we acknowledge limitations regarding
concept drift and the need for comparison with deep learning approaches.

</details>


### [68] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: PANAMA, a novel algorithm using Priority Asymmetry for Network Aware MARL, improves multi-agent path finding in Digital Twin ecosystems, outperforming benchmarks in accuracy, speed, and scalability.


<details>
  <summary>Details</summary>
Motivation: The need for efficient data-sharing and robust algorithms in scaling robotics and automated systems within Digital Twin ecosystems drives this research.

Method: PANAMA employs a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures for network-aware multi-agent reinforcement learning.

Result: PANAMA demonstrates superior pathfinding performance in accuracy, speed, and scalability compared to existing benchmarks.

Conclusion: PANAMA bridges network-aware decision-making and multi-agent coordination, advancing the synergy between Digital Twins, wireless networks, and AI-driven automation.

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [69] [GFlowNets for Learning Better Drug-Drug Interaction Representations](https://arxiv.org/abs/2508.06576)
*Azmine Toushik Wasi*

Main category: cs.LG

TL;DR: A framework combining GFlowNet and VGAE is proposed to address severe class imbalance in drug-drug interaction prediction by generating synthetic samples for rare classes, improving model balance and performance.


<details>
  <summary>Details</summary>
Motivation: Drug-drug interaction datasets suffer from severe class imbalance, with rare but critical interactions underrepresented, leading to poor predictive model performance on infrequent cases.

Method: The proposed framework integrates Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE) to generate synthetic samples for rare interaction classes.

Result: The approach enhances predictive performance across all interaction types, ensuring better clinical reliability.

Conclusion: The combination of GFlowNet and VGAE effectively addresses class imbalance in DDI prediction, improving model performance for rare interactions.

Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.

</details>


### [70] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: The paper explores selecting instance-specific experts from a pool for human-AI collaboration, using conformal prediction sets to improve classification performance. A greedy algorithm is introduced for subset selection, outperforming naive methods and achieving near-optimal results on real datasets.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on single-expert scenarios, but multiple experts can enhance classification. The study aims to identify conditions where multiple experts benefit from conformal sets and improve performance.

Method: A greedy algorithm is proposed to select relevant subsets of expert predictions for each instance, leveraging conformal prediction sets.

Result: The greedy algorithm outperforms naive methods, achieving near-optimal subsets and improved classification performance on CIFAR-10H and ImageNet-16H datasets.

Conclusion: The study demonstrates the effectiveness of instance-specific expert selection in human-AI collaboration, with the greedy algorithm offering a practical solution for enhanced classification.

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


### [71] [Hypergraph Neural Network with State Space Models for Node Classification](https://arxiv.org/abs/2508.06587)
*A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: HGMN, a novel hypergraph neural network with a state space model, integrates role-aware representations into GNNs, outperforming traditional methods in node classification tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs overlook role-based characteristics, limiting expressive node representations. Existing role-based methods are unsupervised and suboptimal for downstream tasks.

Method: HGMN uses hypergraph construction (node degree and neighborhood levels) and a learnable mamba transformer to combine role-based and adjacency-based representations. It includes hypergraph convolution layers and a residual network to prevent over-smoothing.

Result: HGMN significantly outperforms state-of-the-art GNN methods on node classification tasks across multiple datasets.

Conclusion: HGMN effectively combines role-based and adjacency features, offering enriched node representations and versatility for graph-based learning.

Abstract: In recent years, graph neural networks (GNNs) have gained significant
attention for node classification tasks on graph-structured data. However,
traditional GNNs primarily focus on adjacency relationships between nodes,
often overlooking the rich role-based characteristics that are crucial for
learning more expressive node representations. Existing methods for capturing
role-based features are largely unsupervised and fail to achieve optimal
performance in downstream tasks. To address these limitations, we propose a
novel hypergraph neural network with state space model (HGMN) that effectively
integrates role-aware representations into GNNs and the state space model. HGMN
utilizes hypergraph construction techniques to model higher-order relationships
and combines role-based and adjacency-based representations through a learnable
mamba transformer mechanism. By leveraging two distinct hypergraph construction
methods-based on node degree and neighborhood levels, it strengthens the
connections among nodes with similar roles, enhancing the model's
representational power. Additionally, the inclusion of hypergraph convolution
layers enables the model to capture complex dependencies within hypergraph
structures. To mitigate the over-smoothing problem inherent in deep GNNs, we
incorporate a residual network, ensuring improved stability and better feature
propagation across layers. Extensive experiments conducted on one newly
introduced dataset and four benchmark datasets demonstrate the superiority of
HGMN. The model achieves significant performance improvements on node
classification tasks compared to state-of-the-art GNN methods. These results
highlight HGMN's ability to provide enriched node representations by
effectively embedding role-based features alongside adjacency information,
making it a versatile and powerful tool for a variety of graph-based learning
applications.

</details>


### [72] [Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning](https://arxiv.org/abs/2508.06588)
*Zian Zhai,Fan Li,Xingyu Tan,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.LG

TL;DR: The paper addresses codebook collapse in graph VQ, proposing RGVQ, a framework using graph topology and feature similarity to enhance token diversity and performance.


<details>
  <summary>Details</summary>
Motivation: Codebook collapse in graph VQ limits token expressiveness and generalization, yet remains underexplored.

Method: RGVQ integrates graph topology and feature similarity as regularization, uses Gumbel-Softmax for soft assignments, and adds structure-aware contrastive regularization.

Result: RGVQ improves codebook utilization and boosts performance in graph VQ backbones across tasks.

Conclusion: RGVQ enables more expressive and transferable graph token representations by mitigating codebook collapse.

Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for
learning discrete representations of graph-structured data. However, a
fundamental challenge, i.e., codebook collapse, remains underexplored in the
graph domain, significantly limiting the expressiveness and generalization of
graph tokens.In this paper, we present the first empirical study showing that
codebook collapse consistently occurs when applying VQ to graph data, even with
mitigation strategies proposed in vision or language domains. To understand why
graph VQ is particularly vulnerable to collapse, we provide a theoretical
analysis and identify two key factors: early assignment imbalances caused by
redundancy in graph features and structural patterns, and self-reinforcing
optimization loops in deterministic VQ. To address these issues, we propose
RGVQ, a novel framework that integrates graph topology and feature similarity
as explicit regularization signals to enhance codebook utilization and promote
token diversity. RGVQ introduces soft assignments via Gumbel-Softmax
reparameterization, ensuring that all codewords receive gradient updates. In
addition, RGVQ incorporates a structure-aware contrastive regularization to
penalize the token co-assignments among similar node pairs. Extensive
experiments demonstrate that RGVQ substantially improves codebook utilization
and consistently boosts the performance of state-of-the-art graph VQ backbones
across multiple downstream tasks, enabling more expressive and transferable
graph token representations.

</details>


### [73] [A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis](https://arxiv.org/abs/2508.06589)
*Xinglin Zhao,Yanwen Wang,Xiaobo Liu,Yanrong Hao,Rui Cao,Xin Wen*

Main category: cs.LG

TL;DR: A federated learning framework for neuroimaging CAD systems improves diagnostic accuracy by addressing subtype heterogeneity and data variability.


<details>
  <summary>Details</summary>
Motivation: To tackle low reproducibility in small-sample studies and confounding heterogeneity in large-scale datasets for neuroimaging CAD systems.

Method: Proposes a dynamic navigation module for routing samples to suitable local models and a meta-integration module for unified diagnostic outputs.

Result: Achieved 74.06% average accuracy across sites, outperforming traditional methods in handling subtype heterogeneity.

Conclusion: The framework enhances reliability and reproducibility in neuroimaging CAD, benefiting personalized medicine and clinical decision-making.

Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing
neuroimaging data for neurological and psychiatric disorders. However,
small-sample studies suffer from low reproducibility, while large-scale
datasets introduce confounding heterogeneity due to multiple disease subtypes
being labeled under a single category. To address these challenges, we propose
a novel federated learning framework tailored for neuroimaging CAD systems. Our
approach includes a dynamic navigation module that routes samples to the most
suitable local models based on latent subtype representations, and a
meta-integration module that combines predictions from heterogeneous local
models into a unified diagnostic output. We evaluated our framework using a
comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100
healthy controls across multiple study cohorts. Experimental results
demonstrate significant improvements in diagnostic accuracy and robustness
compared to traditional methods. Specifically, our framework achieved an
average accuracy of 74.06\% across all tested sites, showcasing its
effectiveness in handling subtype heterogeneity and enhancing model
generalizability. Ablation studies further confirmed the importance of both the
dynamic navigation and meta-integration modules in improving performance. By
addressing data heterogeneity and subtype confounding, our framework advances
reliable and reproducible neuroimaging CAD systems, offering significant
potential for personalized medicine and clinical decision-making in neurology
and psychiatry.

</details>


### [74] [LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference](https://arxiv.org/abs/2508.07221)
*Po-Han Lee,Yu-Cheng Lin,Chan-Tung Ku,Chan Hsu,Pei-Cing Huang,Ping-Hsun Wu,Yihuang Kang*

Main category: cs.LG

TL;DR: Proposes LLM-based agents for automated confounder discovery in causal ML, reducing human dependency and improving treatment effect estimation.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of unmeasured confounding and structural bias in causal ML, aiming to reduce reliance on domain experts and improve scalability.

Method: Integrates LLM-based agents into causal ML pipelines for automated confounder discovery and subgroup analysis, leveraging their reasoning capabilities.

Result: Enhances treatment effect estimation robustness, narrows confidence intervals, and uncovers unrecognized biases in real-world medical datasets.

Conclusion: LLM-based agents provide scalable, trustworthy, and semantically aware causal inference.

Abstract: Estimating individualized treatment effects from observational data presents
a persistent challenge due to unmeasured confounding and structural bias.
Causal Machine Learning (causal ML) methods, such as causal trees and doubly
robust estimators, provide tools for estimating conditional average treatment
effects. These methods have limited effectiveness in complex real-world
environments due to the presence of latent confounders or those described in
unstructured formats. Moreover, reliance on domain experts for confounder
identification and rule interpretation introduces high annotation cost and
scalability concerns. In this work, we proposed Large Language Model-based
agents for automated confounder discovery and subgroup analysis that integrate
agents into the causal ML pipeline to simulate domain expertise. Our framework
systematically performs subgroup identification and confounding structure
discovery by leveraging the reasoning capabilities of LLM-based agents, which
reduces human dependency while preserving interpretability. Experiments on
real-world medical datasets show that our proposed approach enhances treatment
effect estimation robustness by narrowing confidence intervals and uncovering
unrecognized confounding biases. Our findings suggest that LLM-based agents
offer a promising path toward scalable, trustworthy, and semantically aware
causal inference.

</details>


### [75] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: A framework integrating generative AI with multi-disciplinary literature to design bioinspired materials, validated by real-world experiments.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in applying LLMs to discipline-specific experimental science, especially in multi-disciplinary fields like materials science.

Method: Combines fine-tuned models (BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and Hierarchical Sampling to extract insights and design experiments.

Result: Successfully fabricated a novel pollen-based adhesive with tunable properties, validating the AI-assisted approach.

Conclusion: AI-assisted ideation can effectively drive real-world materials design and enhance human-AI collaboration.

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [76] [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)
*Kyle O'Brien,Stephen Casper,Quentin Anthony,Tomek Korbak,Robert Kirk,Xander Davies,Ishan Mishra,Geoffrey Irving,Yarin Gal,Stella Biderman*

Main category: cs.LG

TL;DR: Filtering dual-use topics from training data enhances resistance to adversarial attacks in open-weight AI systems, outperforming existing methods, but context-based risks remain.


<details>
  <summary>Details</summary>
Motivation: Open-weight AI systems are vulnerable to tampering attacks, and current safety methods are insufficient. The paper explores data filtering as a tamper-resistant safeguard.

Method: A multi-stage pipeline for scalable data filtering is introduced, and 6.9B-parameter models are pretrained to test resistance to adversarial fine-tuning.

Result: Filtered models show strong resistance to adversarial attacks (up to 10,000 steps) and outperform baselines, but context-based risks persist.

Conclusion: Pretraining data curation is a promising defense layer for open-weight AI, though defense-in-depth is needed due to context-based vulnerabilities.

Abstract: Open-weight AI systems offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to tampering attacks which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of adversarial fine-tuning. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable
data filtering and show that it offers a tractable and effective method for
minimizing biothreat proxy knowledge in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a defense-in-depth approach. Overall, these findings help to establish
pretraining data curation as a promising layer of defense for open-weight AI
systems.

</details>


### [77] [Local Diffusion Models and Phases of Data Distributions](https://arxiv.org/abs/2508.06614)
*Fangjun Hu,Guangkuo Liu,Yifan Zhang,Xun Gao*

Main category: cs.LG

TL;DR: The paper introduces a new perspective on data distribution phases in diffusion models, proposing local denoisers for efficiency and identifying critical phase transitions where global methods are necessary.


<details>
  <summary>Details</summary>
Motivation: Ordinary diffusion models ignore local spatial structure in data, leading to computationally expensive global score functions. This work aims to leverage local operations for efficiency.

Method: The authors define data distribution phases connected by local operations, identify phase transitions in denoising, and prove an information-theoretic bound on local denoiser fidelity. Numerical experiments validate the approach.

Result: Local denoisers are effective except near phase transitions, where global methods are needed. This simplifies diffusion model architectures.

Conclusion: The work suggests more efficient diffusion models using local networks and opens new research directions in generative AI and physics-inspired neural networks.

Abstract: As a class of generative artificial intelligence frameworks inspired by
statistical physics, diffusion models have shown extraordinary performance in
synthesizing complicated data distributions through a denoising process
gradually guided by score functions. Real-life data, like images, is often
spatially structured in low-dimensional spaces. However, ordinary diffusion
models ignore this local structure and learn spatially global score functions,
which are often computationally expensive. In this work, we introduce a new
perspective on the phases of data distributions, which provides insight into
constructing local denoisers with reduced computational costs. We define two
distributions as belonging to the same data distribution phase if they can be
mutually connected via spatially local operations such as local denoisers.
Then, we show that the reverse denoising process consists of an early trivial
phase and a late data phase, sandwiching a rapid phase transition where local
denoisers must fail. To diagnose such phase transitions, we prove an
information-theoretic bound on the fidelity of local denoisers based on
conditional mutual information, and conduct numerical experiments in a
real-world dataset. This work suggests simpler and more efficient architectures
of diffusion models: far from the phase transition point, we can use small
local neural networks to compute the score function; global neural networks are
only necessary around the narrow time interval of phase transitions. This
result also opens up new directions for studying phases of data distributions,
the broader science of generative artificial intelligence, and guiding the
design of neural networks inspired by physics concepts.

</details>


### [78] [Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations](https://arxiv.org/abs/2508.07722)
*Pietro Talli,Federico Mason,Federico Chiariotti,Andrea Zanella*

Main category: cs.LG

TL;DR: HR3L is a novel architecture for training RL agents over lossy or delayed wireless networks, improving efficiency and adaptability without gradient exchange.


<details>
  <summary>Details</summary>
Motivation: Training RL agents over non-ideal wireless networks is challenging due to partial/intermittent state updates, and existing solutions are computationally heavy.

Method: HR3L uses a transmitter-receiver pair to encode/decode environment representations, avoiding gradient exchange for faster training.

Result: HR3L outperforms baselines in sample efficiency and adapts to various communication issues like packet loss and delays.

Conclusion: HR3L offers a robust and efficient solution for remote RL training in imperfect wireless environments.

Abstract: In this work, we address the problem of training Reinforcement Learning (RL)
agents over communication networks. The RL paradigm requires the agent to
instantaneously perceive the state evolution to infer the effects of its
actions on the environment. This is impossible if the agent receives state
updates over lossy or delayed wireless systems and thus operates with partial
and intermittent information. In recent years, numerous frameworks have been
proposed to manage RL with imperfect feedback; however, they often offer
specific solutions with a substantial computational burden. To address these
limits, we propose a novel architecture, named Homomorphic Robust Remote
Reinforcement Learning (HR3L), that enables the training of remote RL agents
exchanging observations across a non-ideal wireless channel. HR3L considers two
units: the transmitter, which encodes meaningful representations of the
environment, and the receiver, which decodes these messages and performs
actions to maximize a reward signal. Importantly, HR3L does not require the
exchange of gradient information across the wireless channel, allowing for
quicker training and a lower communication overhead than state-of-the-art
solutions. Experimental results demonstrate that HR3L significantly outperforms
baseline methods in terms of sample efficiency and adapts to different
communication scenarios, including packet losses, delayed transmissions, and
capacity limitations.

</details>


### [79] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: The paper proposes a generalized scaling law for large language models, applicable to both dense and sparse architectures, addressing the challenge of optimizing model size and resource allocation.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of language models and their computational costs has created a need for efficient training techniques, but existing scaling laws are architecture-specific.

Method: The authors revisit existing scaling laws and introduce a generalized scaling law framework for both dense and sparse models, evaluating its effectiveness through comparison.

Result: The proposed scaling law demonstrates effectiveness in unifying the framework for diverse architectures.

Conclusion: The generalized scaling law provides a versatile solution for optimizing large language model training across different architectures.

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [80] [Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels](https://arxiv.org/abs/2508.06622)
*Jeremiah Birrell,Reza Ebrahimi*

Main category: cs.LG

TL;DR: ANTIDOTE is a new objective for learning under noisy labels, using adversarial training to reduce noisy sample influence, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning with noisy labels, which is common in real-world data or adversarial settings.

Method: Defines objectives via relaxation over an information-divergence neighborhood, reformulated as adversarial training with computational efficiency.

Result: Effectively reduces noisy label influence, showing behavior akin to forgetting noisy samples, and outperforms comparable methods.

Conclusion: ANTIDOTE is a practical and efficient solution for noisy label learning, adaptable to various noise types.

Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy
labels which are defined in terms of a relaxation over an
information-divergence neighborhood. Using convex duality, we provide a
reformulation as an adversarial training method that has similar computational
cost to training with standard cross-entropy loss. We show that our approach
adaptively reduces the influence of the samples with noisy labels during
learning, exhibiting a behavior that is analogous to forgetting those samples.
ANTIDOTE is effective in practical environments where label noise is inherent
in the training data or where an adversary can alter the training labels.
Extensive empirical evaluations on different levels of symmetric, asymmetric,
human annotation, and real-world label noise show that ANTIDOTE outperforms
leading comparable losses in the field and enjoys a time complexity that is
very close to that of the standard cross entropy loss.

</details>


### [81] [Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record](https://arxiv.org/abs/2508.06627)
*Mosbah Aouad,Anirudh Choudhary,Awais Farooq,Steven Nevers,Lusine Demirkhanyan,Bhrandon Harris,Suguna Pappu,Christopher Gondi,Ravishankar Iyer*

Main category: cs.LG

TL;DR: A multimodal approach using EHR data improves early detection of pancreatic cancer, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Early detection of pancreatic cancer is challenging due to lack of symptoms and biomarkers.

Method: Combines neural controlled differential equations, pretrained language models, recurrent networks, and cross-attention for multimodal data integration.

Result: Achieves 6.5% to 15.5% AUC improvement over state-of-the-art methods and identifies new biomarkers.

Conclusion: The proposed method enhances early PDAC detection and reveals novel risk-associated factors.

Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and
early detection remains a major clinical challenge due to the absence of
specific symptoms and reliable biomarkers. In this work, we propose a new
multimodal approach that integrates longitudinal diagnosis code histories and
routinely collected laboratory measurements from electronic health records to
detect PDAC up to one year prior to clinical diagnosis. Our method combines
neural controlled differential equations to model irregular lab time series,
pretrained language models and recurrent networks to learn diagnosis code
trajectory representations, and cross-attention mechanisms to capture
interactions between the two modalities. We develop and evaluate our approach
on a real-world dataset of nearly 4,700 patients and achieve significant
improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.
Furthermore, our model identifies diagnosis codes and laboratory panels
associated with elevated PDAC risk, including both established and new
biomarkers. Our code is available at
https://github.com/MosbahAouad/EarlyPDAC-MML.

</details>


### [82] [Using Imperfect Synthetic Data in Downstream Inference Tasks](https://arxiv.org/abs/2508.06635)
*Yewon Byun,Shantanu Gupta,Zachary C. Lipton,Rachel Leah Childers,Bryan Wilder*

Main category: cs.LG

TL;DR: A new estimator using generalized method of moments is introduced to combine synthetic data from large language models with real data for statistically valid conclusions in computational social science.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of integrating synthetic data from large language models with real data to produce valid statistical conclusions.

Method: Introduce a hyperparameter-free estimator based on generalized method of moments, leveraging interactions between synthetic and real data moment residuals.

Result: Empirical validation shows improved estimates of target parameters in computational social science tasks.

Conclusion: The proposed estimator offers a principled way to combine synthetic and real data, enhancing statistical validity in limited data regimes.

Abstract: Predictions and generations from large language models are increasingly being
explored as an aid to computational social science and human subject research
in limited data regimes. While previous technical work has explored the
potential to use model-predicted labels for unlabeled data in a principled
manner, there is increasing interest in using large language models to generate
entirely new synthetic samples (also termed as synthetic simulations), such as
in responses to surveys. However, it is not immediately clear by what means
practitioners can combine such data with real data and yet produce
statistically valid conclusions upon them. In this work, we introduce a new
estimator based on generalized method of moments, providing a
hyperparameter-free solution with strong theoretical guarantees to address the
challenge at hand. Surprisingly, we find that interactions between the moment
residuals of synthetic data and those of real data can improve estimates of the
target parameter. We empirically validate the finite-sample performance of our
estimator across different regression tasks in computational social science
applications, demonstrating large empirical gains.

</details>


### [83] [Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series](https://arxiv.org/abs/2508.06638)
*Muyan Anna Li,Aditi Gautam*

Main category: cs.LG

TL;DR: The paper introduces two adaptive thresholding frameworks, SCS and MACS, for anomaly detection in nonstationary time series data, outperforming traditional methods in F1-score on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional static thresholds fail in nonstationary environments due to regime shifts and concept drift, necessitating adaptive solutions.

Method: Proposes Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence Segments (MACS), leveraging statistical online learning and segmentation for local adaptation.

Result: Experiments on Wafer Manufacturing datasets show significant F1-score improvements over percentile and rolling quantile methods.

Conclusion: Adaptive thresholds like SCS and MACS enable reliable, interpretable, and timely anomaly detection in evolving distributions.

Abstract: As time series data become increasingly prevalent in domains such as
manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt
to nonstationary environments where statistical properties shift over time.
Traditional static thresholds are easily rendered obsolete by regime shifts,
concept drift, or multi-scale changes. To address these challenges, we
introduce and empirically evaluate two novel adaptive thresholding frameworks:
Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence
Segments (MACS). Both leverage statistical online learning and segmentation
principles for local, contextually sensitive adaptation, maintaining guarantees
on false alarm rates even under evolving distributions. Our experiments across
Wafer Manufacturing benchmark datasets show significant F1-score improvement
compared to traditional percentile and rolling quantile approaches. This work
demonstrates that robust, statistically principled adaptive thresholds enable
reliable, interpretable, and timely detection of diverse real-world anomalies.

</details>


### [84] [Fractal Language Modelling by Universal Sequence Maps (USM)](https://arxiv.org/abs/2508.06641)
*Jonas S Almeida,Daniel E Russ,Susana Vinga,Ines Duarte,Lee Mason,Praphulla Bhawsar,Aaron Ge,Arlindo Oliveira,Jeya Balaji Balasubramanian*

Main category: cs.LG

TL;DR: The paper introduces Universal Sequence Maps (USM) for bijective encoding of symbolic sequences, resolving seeding biases and revealing USM's efficient convergence to steady-state embeddings.


<details>
  <summary>Details</summary>
Motivation: To explore encoding procedures for symbolic sequences using USM, addressing the need for contextual retention in neural network modeling.

Method: USM uses forward and backward Chaos Game Representations (CGR) projected into the frequency domain (FCGR), enabling Chebyshev distance and k-mer frequency computation.

Result: Resolved seeding biases, reconciled numeric positioning with sequence identity, and revealed USM's efficient convergence to steady-state embeddings.

Conclusion: USM is effective for genomic sequences and scalable to alphabets of arbitrary cardinality.

Abstract: Motivation: With the advent of Language Models using Transformers,
popularized by ChatGPT, there is a renewed interest in exploring encoding
procedures that numerically represent symbolic sequences at multiple scales and
embedding dimensions. The challenge that encoding addresses is the need for
mechanisms that uniquely retain contextual information about the succession of
individual symbols, which can then be modeled by nonlinear formulations such as
neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively
encode symbolic sequences onto embedded numerical spaces. USM is composed of
two Chaos Game Representations (CGR), iterated forwardly and backwardly, that
can be projected into the frequency domain (FCGR). The corresponding USM
coordinates can be used to compute a Chebyshev distance metric as well as k-mer
frequencies, without having to recompute the embedded numeric coordinates, and,
paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal
Sequence Maps (USM) by resolving seeding biases affecting the iterated process.
The resolution had two results, the first expected, the second an intriguing
outcome: 1) full reconciliation of numeric positioning with sequence identity;
and 2) uncovering the nature of USM as an efficient numeric process converging
towards a steady state sequence embedding solution. We illustrate these results
for genomic sequences because of the convenience of a planar representation
defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,
the application to alphabet of arbitrary cardinality was found to be
straightforward.

</details>


### [85] [Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN](https://arxiv.org/abs/2508.06647)
*Andrey Sidorenko,Paul Tiwald*

Main category: cs.LG

TL;DR: TabularARGN is a neural network for generating high-quality synthetic tabular data, balancing privacy and utility effectively.


<details>
  <summary>Details</summary>
Motivation: Traditional anonymization techniques often fail to preserve privacy, necessitating better synthetic data generation methods.

Method: Uses a discretization-based auto-regressive neural network (TabularARGN) for efficient and high-fidelity synthetic data generation.

Result: Competitive in statistical similarity, machine learning utility, and detection robustness; robust against membership-inference attacks.

Conclusion: TabularARGN offers a robust and efficient solution for synthetic data generation with a strong privacy-utility balance.

Abstract: Synthetic data generation has become essential for securely sharing and
analyzing sensitive data sets. Traditional anonymization techniques, however,
often fail to adequately preserve privacy. We introduce the Tabular
Auto-Regressive Generative Network (TabularARGN), a neural network architecture
specifically designed for generating high-quality synthetic tabular data. Using
a discretization-based auto-regressive approach, TabularARGN achieves high data
fidelity while remaining computationally efficient. We evaluate TabularARGN
against existing synthetic data generation methods, showing competitive results
in statistical similarity, machine learning utility, and detection robustness.
We further perform an in-depth privacy evaluation using systematic
membership-inference attacks, highlighting the robustness and effective
privacy-utility balance of our approach.

</details>


### [86] [In-Context Reinforcement Learning via Communicative World Models](https://arxiv.org/abs/2508.06659)
*Fernando Martinez-Lopez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: CORAL introduces a two-agent framework (IA and CA) for in-context RL, using emergent communication to improve generalization and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: RL agents often overfit to training environments, limiting generalization. CORAL aims to enhance in-context RL by learning transferable representations.

Method: CORAL decouples representation learning (IA) from control (CA). IA pre-trains as a world model, using Causal Influence Loss to shape communication. CA interprets IA's messages for task-solving.

Result: CORAL improves sample efficiency and enables zero-shot adaptation in unseen sparse-reward environments.

Conclusion: Learning transferable communicative representations via CORAL effectively enhances RL generalization and adaptation.

Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks
and contexts without updating their parameters, mainly because their learned
representations and policies are overfit to the specifics of their training
environments. To boost agents' in-context RL (ICRL) ability, this work
formulates ICRL as a two-agent emergent communication problem and introduces
CORAL (Communicative Representation for Adaptive RL), a framework that learns a
transferable communicative context by decoupling latent representation learning
from control. In CORAL, an Information Agent (IA) is pre-trained as a world
model on a diverse distribution of tasks. Its objective is not to maximize task
reward, but to build a world model and distill its understanding into concise
messages. The emergent communication protocol is shaped by a novel Causal
Influence Loss, which measures the effect that the message has on the next
action. During deployment, the previously trained IA serves as a fixed
contextualizer for a new Control Agent (CA), which learns to solve tasks by
interpreting the provided communicative context. Our experiments demonstrate
that this approach enables the CA to achieve significant gains in sample
efficiency and successfully perform zero-shot adaptation with the help of
pre-trained IA in entirely unseen sparse-reward environments, validating the
efficacy of learning a transferable communicative representation.

</details>


### [87] [Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.06663)
*Yuan-Hung Chao,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: Integration of KANs into GNNs (KGAT, KSGC, KAPPNP) improves node classification accuracy. A multi-teacher knowledge amalgamation framework further enhances performance.


<details>
  <summary>Details</summary>
Motivation: To address scalability and efficiency limitations of GNNs by leveraging KANs' strong nonlinear expressiveness and efficient inference.

Method: Integrate KANs into GNN architectures (GAT, SGC, APPNP) and use a multi-teacher knowledge amalgamation framework to distill knowledge into a graph-independent KAN student model.

Result: Proposed models (KGAT, KSGC, KAPPNP) improve node classification accuracy, and knowledge amalgamation boosts student model performance.

Conclusion: KANs enhance GNN expressiveness and enable efficient, graph-free inference.

Abstract: Graph Neural Networks (GNNs) have shown strong performance on
graph-structured data, but their reliance on graph connectivity often limits
scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent
architecture with learnable univariate functions, offer strong nonlinear
expressiveness and efficient inference. In this work, we integrate KANs into
three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new
models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge
amalgamation framework, where knowledge from multiple KAN-based GNNs is
distilled into a graph-independent KAN student model. Experiments on benchmark
datasets show that the proposed models improve node classification accuracy,
and the knowledge amalgamation approach significantly boosts student model
performance. Our findings highlight the potential of KANs for enhancing GNN
expressiveness and for enabling efficient, graph-free inference.

</details>


### [88] [Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation](https://arxiv.org/abs/2508.06676)
*Chia-Hsun Lu,Guan-Jhih Wu,Ya-Chi Ho,Chih-Ya Shen*

Main category: cs.LG

TL;DR: A novel watermarking method, DCT-AW, is proposed for Kolmogorov-Arnold Networks (KAN) to protect intellectual property by embedding watermarks via discrete cosine transform, ensuring robustness against attacks.


<details>
  <summary>Details</summary>
Motivation: Protecting intellectual property in machine learning, especially for advanced models like KAN, which are not well-supported by existing watermarking methods.

Method: DCT-AW embeds watermarks by perturbing activation outputs in KAN using discrete cosine transform, ensuring task independence.

Result: DCT-AW minimally impacts model performance and shows superior robustness against attacks like fine-tuning, pruning, and retraining.

Conclusion: DCT-AW is an effective and robust watermarking solution tailored for KAN, addressing the unique challenges posed by its architecture.

Abstract: With the increasing importance of protecting intellectual property in machine
learning, watermarking techniques have gained significant attention. As
advanced models are increasingly deployed in domains such as social network
analysis, the need for robust model protection becomes even more critical.
While existing watermarking methods have demonstrated effectiveness for
conventional deep neural networks, they often fail to adapt to the novel
architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable
activation functions. KAN holds strong potential for modeling complex
relationships in network-structured data. However, their unique design also
introduces new challenges for watermarking. Therefore, we propose a novel
watermarking method, Discrete Cosine Transform-based Activation Watermarking
(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of
KAN, our method embeds watermarks by perturbing activation outputs using
discrete cosine transform, ensuring compatibility with diverse tasks and
achieving task independence. Experimental results demonstrate that DCT-AW has a
small impact on model performance and provides superior robustness against
various watermark removal attacks, including fine-tuning, pruning, and
retraining after pruning.

</details>


### [89] [Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select](https://arxiv.org/abs/2508.06692)
*Md. Akmol Masud,Md Abrar Jahin,Mahmud Hasan*

Main category: cs.LG

TL;DR: HeteRo-Select improves FL training stability and accuracy by smartly selecting clients based on usefulness, fairness, update speed, and data variety, outperforming Oort.


<details>
  <summary>Details</summary>
Motivation: FL suffers from instability due to diverse client data; existing methods like Oort drop accuracy in later stages.

Method: HeteRo-Select uses a step-by-step scoring system for client selection, considering multiple factors, with theoretical convergence guarantees.

Result: On CIFAR-10 with label skew, HeteRo-Select achieves higher peak (74.75%) and final (72.76%) accuracy with minimal stability drop (1.99%) vs Oort.

Conclusion: HeteRo-Select is a reliable solution for heterogeneous FL, backed by theory and empirical results.

Abstract: Federated Learning (FL) is a machine learning technique that often suffers
from training instability due to the diverse nature of client data. Although
utility-based client selection methods like Oort are used to converge by
prioritizing high-loss clients, they frequently experience significant drops in
accuracy during later stages of training. We propose a theoretical
HeteRo-Select framework designed to maintain high performance and ensure
long-term training stability. We provide a theoretical analysis showing that
when client data is very different (high heterogeneity), choosing a smart
subset of client participation can reduce communication more effectively
compared to full participation. Our HeteRo-Select method uses a clear,
step-by-step scoring system that considers client usefulness, fairness, update
speed, and data variety. It also shows convergence guarantees under strong
regularization. Our experimental results on the CIFAR-10 dataset under
significant label skew ($\alpha=0.1$) support the theoretical findings. The
HeteRo-Select method performs better than existing approaches in terms of peak
accuracy, final accuracy, and training stability. Specifically, HeteRo-Select
achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a
minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak
accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability
drop of $2.73\%$. The theoretical foundations and empirical performance in our
study make HeteRo-Select a reliable solution for real-world heterogeneous FL
problems.

</details>


### [90] [CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations](https://arxiv.org/abs/2508.06704)
*Hager Radi Abdelwahed,Mélisande Teng,Robin Zbinden,Laura Pollock,Hugo Larochelle,Devis Tuia,David Rolnick*

Main category: cs.LG

TL;DR: CISO is a deep learning-based method for species distribution modeling that incorporates incomplete biotic data alongside environmental variables, improving predictive performance.


<details>
  <summary>Details</summary>
Motivation: Species distribution models often overlook biotic interactions due to sparse and inconsistent co-occurrence data, limiting their accuracy.

Method: CISO uses deep learning to condition predictions on flexible, incomplete species observations and environmental variables, tested on plant, bird, and butterfly datasets.

Result: CISO outperforms other methods by improving predictions when partial biotic data is included and combining observations from multiple datasets.

Conclusion: CISO is a promising tool for ecological research, enhancing species distribution predictions by integrating incomplete biotic information.

Abstract: Species distribution models (SDMs) are widely used to predict species'
geographic distributions, serving as critical tools for ecological research and
conservation planning. Typically, SDMs relate species occurrences to
environmental variables representing abiotic factors, such as temperature,
precipitation, and soil properties. However, species distributions are also
strongly influenced by biotic interactions with other species, which are often
overlooked. While some methods partially address this limitation by
incorporating biotic interactions, they often assume symmetrical pairwise
relationships between species and require consistent co-occurrence data. In
practice, species observations are sparse, and the availability of information
about the presence or absence of other species varies significantly across
locations. To address these challenges, we propose CISO, a deep learning-based
method for species distribution modeling Conditioned on Incomplete Species
Observations. CISO enables predictions to be conditioned on a flexible number
of species observations alongside environmental variables, accommodating the
variability and incompleteness of available biotic data. We demonstrate our
approach using three datasets representing different species groups: sPlotOpen
for plants, SatBird for birds, and a new dataset, SatButterfly, for
butterflies. Our results show that including partial biotic information
improves predictive performance on spatially separate test sets. When
conditioned on a subset of species within the same dataset, CISO outperforms
alternative methods in predicting the distribution of the remaining species.
Furthermore, we show that combining observations from multiple datasets can
improve performance. CISO is a promising ecological tool, capable of
incorporating incomplete biotic information and identifying potential
interactions between species from disparate taxa.

</details>


### [91] [Analysis of Schedule-Free Nonconvex Optimization](https://arxiv.org/abs/2508.06743)
*Connor Brown*

Main category: cs.LG

TL;DR: The paper introduces a robust Lyapunov framework for analyzing the Schedule-Free (SF) method in nonconvex optimization, providing horizon-agnostic convergence guarantees under minimal assumptions.


<details>
  <summary>Details</summary>
Motivation: Classical first-order methods require step-sizes dependent on the total horizon, which is often unknown. The SF method avoids this but lacks nonconvex analysis without strong assumptions.

Method: A Lyapunov framework is developed to analyze SF under $L$-smoothness and lower-boundedness, simplifying proofs to a single-step descent inequality.

Result: Horizon-agnostic bounds are derived: $O(1/\log T)$, $O(\log T/T)$, and $O(T^{-(1-\alpha)})$. PEP experiments suggest tighter rates.

Conclusion: The work extends SF's guarantees to nonconvex optimization and opens avenues for optimal nonconvex rates.

Abstract: First-order methods underpin most large-scale learning algorithms, yet their
classical convergence guarantees hinge on carefully scheduled step-sizes that
depend on the total horizon $T$, which is rarely known in advance. The
Schedule-Free (SF) method promises optimal performance with hyperparameters
that are independent of $T$ by interpolating between Polyak--Ruppert averaging
and momentum, but nonconvex analysis of SF has been limited or reliant on
strong global assumptions. We introduce a robust Lyapunov framework that, under
only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step
descent inequality. This yields horizon-agnostic bounds in the nonconvex
setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a
linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for
polynomial averaging. We complement these proofs with Performance Estimation
Problem (PEP) experiments that numerically validate our rates and suggest that
our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to
$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex
optimization and charts future directions for optimal nonconvex rates.

</details>


### [92] [Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning](https://arxiv.org/abs/2508.06765)
*Xingke Yang,Liang Li,Sicong Li,Liwei Guan,Hao Wang,Xiaoqi Qi,Jiang Liu,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: Fed MobiLLM enables efficient federated fine-tuning of LLMs on mobile devices by offloading heavy computations to a server, reducing on-device overhead and improving speed.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous mobile devices face challenges in federated LLM fine-tuning due to high computational/memory demands and slow synchronization.

Method: Uses server-assisted federated side-tuning: devices perform lightweight forward propagation, upload activations; server trains a shared side-network asynchronously.

Result: Achieves 95.2% computation reduction, 93.2% lower communication costs, and 5.1x faster convergence.

Conclusion: Fed MobiLLM is effective for practical LLM adaptation on diverse mobile devices.

Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated LLM FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across
mobile devices with diverse computing/communication speeds and local model
architectures. In particular, Fed MobiLLM implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone LLMs, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in communication costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
LLM adaptation over heterogeneous mobile devices.

</details>


### [93] [Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift](https://arxiv.org/abs/2508.06776)
*Amit Pandey*

Main category: cs.LG

TL;DR: Zero-Direction probing (ZDP) detects model drift via null directions of transformer activations without task labels, offering theoretical guarantees and metrics.


<details>
  <summary>Details</summary>
Motivation: To detect model drift without relying on task labels or output evaluations, leveraging null directions of transformer activations.

Method: Proposes ZDP framework, proves theorems (Variance-Leak, Fisher Null-Conservation, Rank-Leak), and derives Spectral Null-Leakage (SNL) metric with non-asymptotic bounds.

Result: Theoretical guarantees on drift detection via null spaces and Fisher geometry, with a-priori thresholds under a Gaussian null model.

Conclusion: Monitoring null spaces of activations provides testable guarantees for detecting representational changes in models.

Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for
detecting model drift from null directions of transformer activations without
task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the
Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound
for low-rank updates, and (iv) a logarithmic-regret guarantee for online
null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with
non-asymptotic tail bounds and a concentration inequality, yielding a-priori
thresholds for drift under a Gaussian null model. These results show that
monitoring right/left null spaces of layer activations and their Fisher
geometry provides concrete, testable guarantees on representational change.

</details>


### [94] [PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)
*Noel Teku,Fengwei Tian,Payel Bhattacharjee,Souradip Chakraborty,Amrit Singh Bedi,Ravi Tandon*

Main category: cs.LG

TL;DR: PROPS is a multi-stage privacy-preserving alignment framework for LLMs, offering better utility than DP-SGD and RR while maintaining high privacy for human preference labels.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns in LLM alignment by ensuring human feedback doesn't reveal personal values or traits, while avoiding excessive privacy that degrades model utility.

Method: PROPS (PROgressively Private Self-alignment), a multi-stage framework where privately aligned models in earlier stages label data for later stages.

Result: PROPS achieves up to 3x higher win-rates than DP-SGD and 2.5x higher than RR for the same privacy budget.

Conclusion: PROPS effectively balances privacy and utility in LLM alignment, outperforming existing methods.

Abstract: Alignment is a key step in developing Large Language Models (LLMs) using
human feedback to ensure adherence to human values and societal norms.
Dependence on human feedback raises privacy concerns about how much a labeler's
preferences may reveal about their personal values, beliefs, and personality
traits. Existing approaches, such as Differentially Private SGD (DP-SGD),
provide rigorous privacy guarantees by privatizing gradients during fine-tuning
and alignment but can provide more privacy than necessary as human preferences
are tied only to labels of (prompt, response) pairs and can degrade model
utility. This work focuses on LLM alignment with preference-level privacy,
which preserves the privacy of preference labels provided by humans. We propose
PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving
alignment framework where privately aligned models in previous stages can serve
as labelers for supplementing training data in the subsequent stages of
alignment. We present theoretical guarantees for PROPS as well as comprehensive
validation using multiple models (Pythia and GPT) and datasets (AlpacaEval,
Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over
existing methods while still providing high privacy. For the same privacy
budget, alignment via PROPS can achieve up to 3x higher win-rates compared to
DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based
alignment.

</details>


### [95] [Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning](https://arxiv.org/abs/2508.06784)
*Junjing Zheng,Chengliang Song,Weidong Jiang,Xinyu Zhang*

Main category: cs.LG

TL;DR: MA-NTAE introduces a non-linear Tucker autoencoder for high-dimensional tensor data, addressing limitations of MLP-based autoencoders and tensor networks by combining tensor decomposition with flexible per-mode encoding.


<details>
  <summary>Details</summary>
Motivation: High-dimensional tensor data challenges self-supervised learning due to computational inefficiency and limited non-linear learning in existing methods.

Method: MA-NTAE generalizes Tucker decomposition to a non-linear framework using a Pick-and-Unfold strategy for per-mode encoding, integrating tensor structural priors.

Result: MA-NTAE shows linear computational complexity growth with tensor order and outperforms standard autoencoders and tensor networks in compression and clustering tasks.

Conclusion: MA-NTAE effectively addresses high-dimensional tensor challenges, offering scalable and superior performance for higher-order, higher-dimensional data.

Abstract: High-dimensional data, particularly in the form of high-order tensors,
presents a major challenge in self-supervised learning. While MLP-based
autoencoders (AE) are commonly employed, their dependence on flattening
operations exacerbates the curse of dimensionality, leading to excessively
large model sizes, high computational overhead, and challenging optimization
for deep structural feature capture. Although existing tensor networks
alleviate computational burdens through tensor decomposition techniques, most
exhibit limited capability in learning non-linear relationships. To overcome
these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder
(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear
framework and employs a Pick-and-Unfold strategy, facilitating flexible
per-mode encoding of high-order tensors via recursive unfold-encode-fold
operations, effectively integrating tensor structural priors. Notably, MA-NTAE
exhibits linear growth in computational complexity with tensor order and
proportional growth with mode dimensions. Extensive experiments demonstrate
MA-NTAE's performance advantages over standard AE and current tensor networks
in compression and clustering tasks, which become increasingly pronounced for
higher-order, higher-dimensional tensors.

</details>


### [96] [Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities](https://arxiv.org/abs/2508.06800)
*Rui Liu,Haolin Zuo,Zheng Lian,Hongyu Yuan,Qi Fan*

Main category: cs.LG

TL;DR: HARDY-MER is a novel framework for multimodal emotion recognition that dynamically adjusts training focus on hard samples by evaluating reconstruction difficulty and cross-modal mutual information.


<details>
  <summary>Details</summary>
Motivation: Existing methods for missing modalities in MER fail to address varying reconstruction difficulty, limiting performance on hard samples.

Method: HARDY-MER uses a Multi-view Hardness Evaluation to quantify sample difficulty and a Retrieval-based Dynamic Curriculum Learning strategy to adjust training focus.

Result: HARDY-MER outperforms existing methods in missing-modality scenarios on benchmark datasets.

Conclusion: The proposed framework effectively handles hard samples in MER, improving performance in missing-modality cases.

Abstract: Missing modalities have recently emerged as a critical research direction in
multimodal emotion recognition (MER). Conventional approaches typically address
this issue through missing modality reconstruction. However, these methods fail
to account for variations in reconstruction difficulty across different
samples, consequently limiting the model's ability to handle hard samples
effectively. To overcome this limitation, we propose a novel Hardness-Aware
Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates
in two key stages: first, it estimates the hardness level of each sample, and
second, it strategically emphasizes hard samples during training to enhance
model performance on these challenging instances. Specifically, we first
introduce a Multi-view Hardness Evaluation mechanism that quantifies
reconstruction difficulty by considering both Direct Hardness (modality
reconstruction errors) and Indirect Hardness (cross-modal mutual information).
Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy
that dynamically adjusts the training curriculum by retrieving samples with
similar semantic information and balancing the learning focus between easy and
hard instances. Extensive experiments on benchmark datasets demonstrate that
HARDY-MER consistently outperforms existing methods in missing-modality
scenarios. Our code will be made publicly available at
https://github.com/HARDY-MER/HARDY-MER.

</details>


### [97] [Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation](https://arxiv.org/abs/2508.06806)
*Xiao Huang,Xu Liu,Enze Zhang,Tong Yu,Shuai Li*

Main category: cs.LG

TL;DR: The paper introduces Classifier-Free Diffusion Generation (CFDG) for Offline-to-online RL, improving data augmentation by reducing the gap between offline and online data distributions. It outperforms existing methods by 15% on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing data augmentation methods in Offline-to-online RL fail to fully bridge the gap between offline and online data distributions, limiting performance.

Method: Proposes CFDG, a classifier-free guidance diffusion approach, to enhance generation quality and align generated data with online data using reweighting.

Result: CFDG achieves a 15% average improvement over existing methods on D4RL benchmarks like MuJoCo and AntMaze.

Conclusion: CFDG is a versatile and effective data augmentation method for Offline-to-online RL, compatible with existing algorithms and significantly boosting performance.

Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online
fine-tuning on an offline pre-trained policy to minimize costly online
interactions. Existing work used offline datasets to generate data that conform
to the online data distribution for data augmentation. However, generated data
still exhibits a gap with the online data, limiting overall performance. To
address this, we propose a new data augmentation approach, Classifier-Free
Diffusion Generation (CFDG). Without introducing additional classifier training
overhead, CFDG leverages classifier-free guidance diffusion to significantly
enhance the generation quality of offline and online data with different
distributions. Additionally, it employs a reweighting method to enable more
generated data to align with the online data, enhancing performance while
maintaining the agent's stability. Experimental results show that CFDG
outperforms replaying the two data types or using a standard diffusion model to
generate new data. Our method is versatile and can be integrated with existing
offline-to-online RL algorithms. By implementing CFDG to popular methods IQL,
PEX and APL, we achieve a notable 15% average improvement in empirical
performance on the D4RL benchmark such as MuJoCo and AntMaze.

</details>


### [98] [Technical Report: Full-Stack Fine-Tuning for the Q Programming Language](https://arxiv.org/abs/2508.06813)
*Brendan R. Hogan,Will Brown,Adel Boyarsky,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: The paper presents an open-source approach to adapt large language models (LLMs) for the Q programming language, addressing the challenge of under-represented tasks. It introduces a Leetcode-style evaluation dataset, benchmarks models, and trains reasoning and non-reasoning models, achieving significant performance improvements over frontier models like Claude Opus-4 and GPT-4.1.


<details>
  <summary>Details</summary>
Motivation: Specialized applications, especially in niche languages like Q, are challenging for general-purpose LLMs due to their under-representation on the Internet. The work aims to bridge this gap.

Method: The approach includes dataset construction, pretraining, supervised fine-tuning, and reinforcement learning for models based on the Qwen-2.5 series, spanning five parameter sizes.

Result: The best model achieves 59% pass@1 accuracy on the Q benchmark, outperforming Claude Opus-4 by 29.5% and GPT-4.1. All models, including the smallest (1.5B), surpass GPT-4.1.

Conclusion: The methodology is broadly applicable and provides a blueprint for adapting LLMs to other under-represented tasks, including those with subjective evaluation criteria.

Abstract: Even though large language models are becoming increasingly capable, it is
still unreasonable to expect them to excel at tasks that are under-represented
on the Internet. Leveraging LLMs for specialized applications, particularly in
niche programming languages and private domains, remains challenging and
largely unsolved. In this work, we address this gap by presenting a
comprehensive, open-source approach for adapting LLMs to the Q programming
language, a popular tool in quantitative finance that is much less present on
the Internet compared to Python, C, Java, and other ``mainstream" languages and
is therefore not a strong suit of general-purpose AI models. We introduce a new
Leetcode style evaluation dataset for Q, benchmark major frontier models on the
dataset, then do pretraining, supervised fine tuning, and reinforcement
learning to train a suite of reasoning and non-reasoning models based on the
Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our
best model achieves a pass@1 accuracy of 59 percent on our Q benchmark,
surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.
Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.
In addition to releasing models, code, and data, we provide a detailed
blueprint for dataset construction, model pretraining, supervised fine-tuning,
and reinforcement learning. Our methodology is broadly applicable, and we
discuss how these techniques can be extended to other tasks, including those
where evaluation may rely on soft or subjective signals.

</details>


### [99] [Who's the Evil Twin? Differential Auditing for Undesired Behavior](https://arxiv.org/abs/2508.06827)
*Ishwar Balappanawar,Venkata Hasith Vattikuti,Greta Kintzley,Ronan Azimi-Mancel,Satvik Golechha*

Main category: cs.LG

TL;DR: The paper explores detecting hidden behaviors in neural networks by framing it as an adversarial game between red and blue teams, testing various detection methods with promising results for adversarial-attack-based techniques.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in detecting hidden harmful behaviors in neural networks with minimal prior knowledge and potential adversarial obfuscation.

Method: The red team trains two models (one benign, one compromised), while the blue team tries to identify the compromised model using strategies like Gaussian noise analysis, model diffing, and adversarial attacks.

Result: Adversarial-attack-based methods achieved 100% accuracy with hints, while other techniques varied. LLM auditing required hints about undesired distributions.

Conclusion: The findings contribute to better audit designs, with open-sourced auditing games and models to aid further research.

Abstract: Detecting hidden behaviors in neural networks poses a significant challenge
due to minimal prior knowledge and potential adversarial obfuscation. We
explore this problem by framing detection as an adversarial game between two
teams: the red team trains two similar models, one trained solely on benign
data and the other trained on data containing hidden harmful behavior, with the
performance of both being nearly indistinguishable on the benign dataset. The
blue team, with limited to no information about the harmful behaviour, tries to
identify the compromised model. We experiment using CNNs and try various blue
team strategies, including Gaussian noise analysis, model diffing, integrated
gradients, and adversarial attacks under different levels of hints provided by
the red team. Results show high accuracy for adversarial-attack-based methods
(100\% correct prediction, using hints), which is very promising, whilst the
other techniques yield more varied performance. During our LLM-focused rounds,
we find that there are not many parallel methods that we could apply from our
study with CNNs. Instead, we find that effective LLM auditing methods require
some hints about the undesired distribution, which can then used in standard
black-box and open-weight methods to probe the models further and reveal their
misalignment. We open-source our auditing games (with the model and data) and
hope that our findings contribute to designing better audits.

</details>


### [100] [Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.06871)
*Aleksandar Todorov,Juan Cardenas-Cartagena,Rafael F. Cunha,Marco Zullich,Matthia Sabatelli*

Main category: cs.LG

TL;DR: Sparsification methods (GMP and SET) improve plasticity and performance in multi-task reinforcement learning (MTRL), outperforming dense baselines and other plasticity interventions.


<details>
  <summary>Details</summary>
Motivation: Plasticity loss in deep reinforcement learning, especially in MTRL, hinders adaptability to diverse tasks.

Method: Evaluated GMP and SET across MTRL architectures (shared backbone, Mixture of Experts, Mixture of Orthogonal Experts) on benchmarks, comparing with dense baselines and other methods.

Result: GMP and SET mitigate plasticity degradation (e.g., neuron dormancy, representational collapse) and often enhance multi-task performance.

Conclusion: Dynamic sparsification is a robust tool for improving plasticity and adaptability in MTRL systems, though its effectiveness depends on context.

Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a
critical challenge in deep reinforcement learning. We examine this issue in
multi-task reinforcement learning (MTRL), where higher representational
flexibility is crucial for managing diverse and potentially conflicting task
demands. We systematically explore how sparsification methods, particularly
Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance
plasticity and consequently improve performance in MTRL agents. We evaluate
these approaches across distinct MTRL architectures (shared backbone, Mixture
of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,
comparing against dense baselines, and a comprehensive range of alternative
plasticity-inducing or regularization methods. Our results demonstrate that
both GMP and SET effectively mitigate key indicators of plasticity degradation,
such as neuron dormancy and representational collapse. These plasticity
improvements often correlate with enhanced multi-task performance, with sparse
agents frequently outperforming dense counterparts and achieving competitive
results against explicit plasticity interventions. Our findings offer insights
into the interplay between plasticity, network sparsity, and MTRL designs,
highlighting dynamic sparsification as a robust but context-sensitive tool for
developing more adaptable MTRL systems.

</details>


### [101] [Conformal Prediction and Trustworthy AI](https://arxiv.org/abs/2508.06885)
*Anthony Bellotti,Xindi Zhao*

Main category: cs.LG

TL;DR: Conformal predictors provide reliable set predictions with guaranteed confidence, aiding trustworthy AI by addressing uncertainty, generalization risk, and bias.


<details>
  <summary>Details</summary>
Motivation: To explore conformal prediction's role in trustworthy AI beyond its known validity, tackling issues like generalization risk and governance.

Method: Review of conformal prediction's applications, including experiments and examples for calibration and bias mitigation.

Result: Demonstrates conformal prediction's effectiveness in uncertainty quantification and bias identification.

Conclusion: Conformal prediction is a valuable tool for trustworthy AI, extending beyond marginal validity to address broader challenges.

Abstract: Conformal predictors are machine learning algorithms developed in the 1990's
by Gammerman, Vovk, and their research team, to provide set predictions with
guaranteed confidence level. Over recent years, they have grown in popularity
and have become a mainstream methodology for uncertainty quantification in the
machine learning community. From its beginning, there was an understanding that
they enable reliable machine learning with well-calibrated uncertainty
quantification. This makes them extremely beneficial for developing trustworthy
AI, a topic that has also risen in interest over the past few years, in both
the AI community and society more widely. In this article, we review the
potential for conformal prediction to contribute to trustworthy AI beyond its
marginal validity property, addressing problems such as generalization risk and
AI governance. Experiments and examples are also provided to demonstrate its
use as a well-calibrated predictor and for bias identification and mitigation.

</details>


### [102] [QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting](https://arxiv.org/abs/2508.06915)
*Shichao Ma,Zhengyang Zhou,Qihe Huang,Binwu Wang,Kuo Yang,Huan Li,Yang Wang*

Main category: cs.LG

TL;DR: QuiZSF integrates retrieval-augmented generation (RAG) with time series pre-trained models (TSPMs) to enhance zero-shot forecasting, achieving top performance in most settings.


<details>
  <summary>Details</summary>
Motivation: Zero-shot time-series forecasting (ZSF) is challenging in data-scarce scenarios, and existing TSPMs lack dynamic external knowledge integration.

Method: QuiZSF combines scalable retrieval (ChronoRAG Base), multi-grained feature learning (MSIL), and dual-branch model alignment (MCC) for TSPMs.

Result: QuiZSF ranks Top1 in 75% (Non-LLM) and 87.5% (LLM) of prediction settings, with high efficiency.

Conclusion: QuiZSF effectively bridges RAG and TSPMs, improving ZSF performance and adaptability.

Abstract: Time series forecasting has become increasingly important to empower diverse
applications with streaming data. Zero-shot time-series forecasting (ZSF),
particularly valuable in data-scarce scenarios, such as domain transfer or
forecasting under extreme conditions, is difficult for traditional models to
deal with. While time series pre-trained models (TSPMs) have demonstrated
strong performance in ZSF, they often lack mechanisms to dynamically
incorporate external knowledge. Fortunately, emerging retrieval-augmented
generation (RAG) offers a promising path for injecting such knowledge on
demand, yet they are rarely integrated with TSPMs. To leverage the strengths of
both worlds, we introduce RAG into TSPMs to enhance zero-shot time series
forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series
Forecaster), a lightweight and modular framework that couples efficient
retrieval with representation learning and model adaptation for ZSF.
Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)
for scalable time-series storage and domain-aware retrieval, introduce a
Multi-grained Series Interaction Learner (MSIL) to extract fine- and
coarse-grained relational features, and develop a dual-branch Model Cooperation
Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM
based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM
based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and
87.5% of prediction settings, while maintaining high efficiency in memory and
inference time.

</details>


### [103] [Class Unbiasing for Generalization in Medical Diagnosis](https://arxiv.org/abs/2508.06943)
*Lishi Zuo,Man-Wai Mak,Lu Yi,Youzhi Tu*

Main category: cs.LG

TL;DR: The paper addresses class-feature bias in medical diagnosis models, proposing a class-unbiased model (Cls-unbias) with a class-wise inequality loss and group distributionally robust optimization to improve generalization.


<details>
  <summary>Details</summary>
Motivation: To mitigate class-feature bias and class imbalance in medical diagnosis models, which can lead to biased performance and poor generalization.

Method: Proposes a class-wise inequality loss for balanced contributions from positive and negative classes and a class-weighted training objective (group distributionally robust optimization) to upweight underperforming classes.

Result: Empirical results on synthetic and real-world datasets show the method effectively reduces class-feature bias and class imbalance, improving model generalization.

Conclusion: The proposed Cls-unbias method successfully mitigates bias and imbalance, enhancing model performance and generalization.

Abstract: Medical diagnosis might fail due to bias. In this work, we identified
class-feature bias, which refers to models' potential reliance on features that
are strongly correlated with only a subset of classes, leading to biased
performance and poor generalization on other classes. We aim to train a
class-unbiased model (Cls-unbias) that mitigates both class imbalance and
class-feature bias simultaneously. Specifically, we propose a class-wise
inequality loss which promotes equal contributions of classification loss from
positive-class and negative-class samples. We propose to optimize a class-wise
group distributionally robust optimization objective-a class-weighted training
objective that upweights underperforming classes-to enhance the effectiveness
of the inequality loss under class imbalance. Through synthetic and real-world
datasets, we empirically demonstrate that class-feature bias can negatively
impact model performance. Our proposed method effectively mitigates both
class-feature bias and class imbalance, thereby improving the model's
generalization ability.

</details>


### [104] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: AMFT is a single-stage algorithm that dynamically balances SFT and RL for LLM fine-tuning, achieving state-of-the-art performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the suboptimal trade-offs and catastrophic forgetting in the traditional two-stage SFT-RL pipeline for LLM fine-tuning.

Method: Introduces AMFT, which uses a meta-gradient adaptive weight controller to dynamically balance SFT and RL rewards, optimizing long-term task performance.

Result: AMFT achieves state-of-the-art results on benchmarks like mathematical reasoning and vision-language navigation, with superior OOD generalization.

Conclusion: AMFT offers a principled and effective paradigm for LLM alignment, with stability and sample efficiency driven by its meta-learning controller.

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [105] [BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity](https://arxiv.org/abs/2508.06953)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: BoRA improves LoRA by using block matrix multiplication and diagonal matrices to enhance rank efficiency with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: LoRA's performance improves with higher rank but increases parameters; BoRA aims to achieve better rank efficiency without excessive parameter growth.

Method: BoRA partitions LoRA matrices into blocks, introduces diagonal matrices for diversity, and concatenates block products to increase rank.

Result: BoRA outperforms LoRA, increasing rank by a factor of block count with minimal additional parameters, validated by experiments.

Conclusion: BoRA is a scalable, efficient alternative to LoRA, enhancing performance with fewer parameters.

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). It approximates the update of a
pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two
low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and
$B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$
can raise the rank of LoRA weights (i.e., $BA$), which typically improves
fine-tuning performance but also significantly increases the number of
trainable parameters. In this paper, we propose Block Diversified Low-Rank
Adaptation (BoRA), which improves the rank of LoRA weights with a small number
of additional parameters. Specifically, BoRA treats the product $BA$ as a block
matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along
the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and
$B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the
concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the
diversity of different block products, BoRA introduces a unique diagonal matrix
$\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication,
resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal
matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only
requiring $b^2r$ additional parameters. Extensive experiments across multiple
datasets and models demonstrate the superiority of BoRA, and ablation studies
further validate its scalability.

</details>


### [106] [Can Multitask Learning Enhance Model Explainability?](https://arxiv.org/abs/2508.06966)
*Hiba Najjar,Bushra Alshbib,Andreas Dengel*

Main category: cs.LG

TL;DR: The paper proposes using multitask learning with satellite data modalities as additional targets to improve model interpretability and performance, while avoiding data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between complexity and interpretability in multimodal learning networks for remote sensing.

Method: Leveraging modalities as additional targets in multitask learning, using satellite data's rich information content.

Result: The approach avoids data scarcity, maintains performance, and explains errors via auxiliary tasks. Demonstrated on segmentation, classification, and regression tasks.

Conclusion: Multitask learning with modalities as targets enhances interpretability and performance in remote sensing models.

Abstract: Remote sensing provides satellite data in diverse types and formats. The
usage of multimodal learning networks exploits this diversity to improve model
performance, except that the complexity of such networks comes at the expense
of their interpretability. In this study, we explore how modalities can be
leveraged through multitask learning to intrinsically explain model behavior.
In particular, instead of additional inputs, we use certain modalities as
additional targets to be predicted along with the main task. The success of
this approach relies on the rich information content of satellite data, which
remains as input modalities. We show how this modeling context provides
numerous benefits: (1) in case of data scarcity, the additional modalities do
not need to be collected for model inference at deployment, (2) the model
performance remains comparable to the multimodal baseline performance, and in
some cases achieves better scores, (3) prediction errors in the main task can
be explained via the model behavior in the auxiliary task(s). We demonstrate
the efficiency of our approach on three datasets, including segmentation,
classification, and regression tasks. Code available at
git.opendfki.de/hiba.najjar/mtl_explainability/.

</details>


### [107] [Structure-Preserving Digital Twins via Conditional Neural Whitney Forms](https://arxiv.org/abs/2508.06981)
*Brooks Kinch,Benjamin Shaffer,Elizabeth Armstrong,Michael Meehan,John Hewson,Nathaniel Trask*

Main category: cs.LG

TL;DR: A framework for real-time digital twins using reduced finite element models with conditional attention mechanisms, ensuring numerical stability and conservation laws.


<details>
  <summary>Details</summary>
Motivation: To enable real-time calibration and closed-loop inference for digital twins, addressing data sparsity and complex geometries.

Method: Uses conditional attention mechanisms within finite element exterior calculus (FEEC) to learn reduced bases and nonlinear conservation laws.

Result: Achieves accurate predictions on complex problems (e.g., turbulence, battery thermal runaway) with sparse data, offering a 3.1x10^8 speedup over LES.

Conclusion: The framework supports non-invasive integration with conventional finite element methods, enabling real-time digital twin applications.

Abstract: We present a framework for constructing real-time digital twins based on
structure-preserving reduced finite element models conditioned on a latent
variable Z. The approach uses conditional attention mechanisms to learn both a
reduced finite element basis and a nonlinear conservation law within the
framework of finite element exterior calculus (FEEC). This guarantees numerical
well-posedness and exact preservation of conserved quantities, regardless of
data sparsity or optimization error. The conditioning mechanism supports
real-time calibration to parametric variables, allowing the construction of
digital twins which support closed loop inference and calibration to sensor
data. The framework interfaces with conventional finite element machinery in a
non-invasive manner, allowing treatment of complex geometries and integration
of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,
and a complex battery thermal runaway problem. The method achieves accurate
predictions on complex geometries with sparse data (25 LES simulations),
including capturing the transition to turbulence and achieving real-time
inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source
implementation is available on GitHub.

</details>


### [108] [Discovery Learning accelerates battery design evaluation](https://arxiv.org/abs/2508.06985)
*Jiawei Zhang,Yifei Zhang,Baozhao Yi,Yao Ren,Qi Jiao,Hanyu Bai,Weiran Jiang,Ziyou Song*

Main category: cs.LG

TL;DR: Discovery Learning (DL) integrates active, physics-guided, and zero-shot learning to rapidly evaluate battery designs without prototyping, reducing time and energy costs by 98% and 95%, respectively.


<details>
  <summary>Details</summary>
Motivation: Battery innovation is bottlenecked by high time and energy costs for prototyping and testing. Existing data-driven methods lack efficiency and require labeled data.

Method: DL combines active learning, physics-guided learning, and zero-shot learning in a human-like reasoning loop, leveraging historical data to predict battery lifetimes without additional labeling.

Result: DL achieved 7.2% test error in predicting cycle life for unobserved designs, saving 98% time and 95% energy compared to industrial practices.

Conclusion: DL accelerates battery development by extracting insights from historical designs, advancing data-driven modeling for scientific and engineering innovation.

Abstract: Fast and reliable validation of novel designs in complex physical systems
such as batteries is critical to accelerating technological innovation.
However, battery research and development remain bottlenecked by the
prohibitively high time and energy costs required to evaluate numerous new
design candidates, particularly in battery prototyping and life testing.
Despite recent progress in data-driven battery lifetime prediction, existing
methods require labeled data of target designs to improve accuracy and cannot
make reliable predictions until after prototyping, thus falling far short of
the efficiency needed to enable rapid feedback for battery design. Here, we
introduce Discovery Learning (DL), a scientific machine-learning paradigm that
integrates active learning, physics-guided learning, and zero-shot learning
into a human-like reasoning loop, drawing inspiration from learning theories in
educational psychology. DL can learn from historical battery designs and
actively reduce the need for prototyping, thus enabling rapid lifetime
evaluation for unobserved material-design combinations without requiring
additional data labeling. To test DL, we present 123 industrial-grade
large-format lithium-ion pouch cells, spanning eight material-design
combinations and diverse cycling protocols. Trained solely on public datasets
of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting
the average cycle life under unknown device variability. This results in
savings of 98% in time and 95% in energy compared to industrial practices. This
work highlights the potential of uncovering insights from historical designs to
inform and accelerate the development of next-generation battery technologies.
DL represents a key advance toward efficient data-driven modeling and helps
realize the promise of machine learning for accelerating scientific discovery
and engineering innovation.

</details>


### [109] [UniMove: A Unified Model for Multi-city Human Mobility Prediction](https://arxiv.org/abs/2508.06986)
*Chonghua Han,Yuan Yuan,Yukun Liu,Jingtao Ding,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: UniMove is a unified model for multi-city human mobility prediction, addressing universal spatial representation and heterogeneous mobility patterns, improving accuracy by 10.2%.


<details>
  <summary>Details</summary>
Motivation: Human mobility prediction is challenging due to randomness, non-uniform time intervals, and city heterogeneity. Existing models require city-specific training.

Method: Proposes a dual-tower architecture (trajectory-location) and MoE Transformer blocks for adaptive pattern handling.

Result: UniMove improves prediction accuracy by 10.2% and enables joint multi-city training.

Conclusion: UniMove advances foundational modeling for human mobility with a unified architecture.

Abstract: Human mobility prediction is vital for urban planning, transportation
optimization, and personalized services. However, the inherent randomness,
non-uniform time intervals, and complex patterns of human mobility, compounded
by the heterogeneity introduced by varying city structures, infrastructure, and
population densities, present significant challenges in modeling. Existing
solutions often require training separate models for each city due to distinct
spatial representations and geographic coverage. In this paper, we propose
UniMove, a unified model for multi-city human mobility prediction, addressing
two challenges: (1) constructing universal spatial representations for
effective token sharing across cities, and (2) modeling heterogeneous mobility
patterns from varying city characteristics. We propose a trajectory-location
dual-tower architecture, with a location tower for universal spatial encoding
and a trajectory tower for sequential mobility modeling. We also design MoE
Transformer blocks to adaptively select experts to handle diverse movement
patterns. Extensive experiments across multiple datasets from diverse cities
demonstrate that UniMove truly embodies the essence of a unified model. By
enabling joint training on multi-city data with mutual data enhancement, it
significantly improves mobility prediction accuracy by over 10.2\%. UniMove
represents a key advancement toward realizing a true foundational model with a
unified architecture for human mobility. We release the implementation at
https://github.com/tsinghua-fib-lab/UniMove/.

</details>


### [110] [A Comparative Study of Feature Selection in Tsetlin Machines](https://arxiv.org/abs/2508.06991)
*Vojtech Halenka,Ole-Christoffer Granmo,Lei Jiao,Per-Arne Andersen*

Main category: cs.LG

TL;DR: The paper evaluates feature selection (FS) methods for Tsetlin machines (TMs), including classical and novel techniques, showing TM-internal scorers perform well and retain interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of tools for estimating feature importance in Tsetlin machines, the paper explores and benchmarks various FS techniques.

Method: Adapts and evaluates FS techniques like filter, embedded methods, SHAP, LIME, and novel TM-specific scorers, tested on 12 datasets using ROAR and ROAD protocols.

Result: TM-internal scorers perform competitively, retain interpretability, and reveal feature interactions. Simpler TM-specific scorers are computationally efficient.

Conclusion: The study establishes a baseline for FS in TMs and encourages development of specialized interpretability techniques.

Abstract: Feature Selection (FS) is crucial for improving model interpretability,
reducing complexity, and sometimes for enhancing accuracy. The recently
introduced Tsetlin machine (TM) offers interpretable clause-based learning, but
lacks established tools for estimating feature importance. In this paper, we
adapt and evaluate a range of FS techniques for TMs, including classical filter
and embedded methods as well as post-hoc explanation methods originally
developed for neural networks (e.g., SHAP and LIME) and a novel family of
embedded scorers derived from TM clause weights and Tsetlin automaton (TA)
states. We benchmark all methods across 12 datasets, using evaluation
protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias
(ROAD), to assess causal impact. Our results show that TM-internal scorers not
only perform competitively but also exploit the interpretability of clauses to
reveal interacting feature patterns. Simpler TM-specific scorers achieve
similar accuracy retention at a fraction of the computational cost. This study
establishes the first comprehensive baseline for FS in TM and paves the way for
developing specialized TM-specific interpretability techniques.

</details>


### [111] [TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations](https://arxiv.org/abs/2508.07016)
*Jianfei Wu,Wenmian Yang,Bingning Liu,Weijia Jia*

Main category: cs.LG

TL;DR: The paper introduces TLCCSP, a framework for time series forecasting that leverages time-lagged cross-correlations using SSDTW and contrastive learning, significantly improving accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models often ignore time-lagged cross-correlations between sequences, which are vital for capturing complex temporal relationships in forecasting.

Method: Proposes TLCCSP with SSDTW to capture lagged correlations and a contrastive learning encoder to approximate SSDTW distances efficiently.

Result: TLCCSP reduces MSE by up to 21.29% on weather, finance, and real estate datasets and cuts SSDTW computational time by ~99%.

Conclusion: TLCCSP effectively enhances forecasting accuracy and scalability, making it suitable for real-time applications.

Abstract: Time series forecasting is critical across various domains, such as weather,
finance and real estate forecasting, as accurate forecasts support informed
decision-making and risk mitigation. While recent deep learning models have
improved predictive capabilities, they often overlook time-lagged
cross-correlations between related sequences, which are crucial for capturing
complex temporal relationships. To address this, we propose the Time-Lagged
Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances
forecasting accuracy by effectively integrating time-lagged cross-correlated
sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)
algorithm to capture lagged correlations and a contrastive learning-based
encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets
demonstrate the effectiveness of our framework. On the weather dataset, SSDTW
reduces mean squared error (MSE) by 16.01% compared with single-sequence
methods, while the contrastive learning encoder (CLE) further decreases MSE by
17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE
reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by
21.29% and 8.62%, respectively. Additionally, the contrastive learning approach
decreases SSDTW computational time by approximately 99%, ensuring scalability
and real-time applicability across multiple time series forecasting tasks.

</details>


### [112] [From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving](https://arxiv.org/abs/2508.07029)
*Antonio Guillen-Perez*

Main category: cs.LG

TL;DR: The paper addresses the brittleness of Behavioral Cloning (BC) in autonomous driving by proposing a pipeline combining BC with Offline Reinforcement Learning (CQL), achieving significantly better performance.


<details>
  <summary>Details</summary>
Motivation: BC policies are brittle and suffer from compounding errors, making them unreliable for long-horizon driving tasks. The goal is to develop a more robust approach using offline RL.

Method: Developed advanced BC baselines (including a Transformer-based model) and applied CQL to the same data and architecture, using a carefully engineered reward function.

Result: The CQL agent outperformed the best BC baseline with a 3.2x higher success rate and 7.4x lower collision rate in large-scale evaluations.

Conclusion: Offline RL (CQL) is essential for learning robust, long-horizon driving policies from static expert data.

Abstract: Learning robust driving policies from large-scale, real-world datasets is a
central challenge in autonomous driving, as online data collection is often
unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward
approach to imitation learning, policies trained with BC are notoriously
brittle and suffer from compounding errors in closed-loop execution. This work
presents a comprehensive pipeline and a comparative study to address this
limitation. We first develop a series of increasingly sophisticated BC
baselines, culminating in a Transformer-based model that operates on a
structured, entity-centric state representation. While this model achieves low
imitation loss, we show that it still fails in long-horizon simulations. We
then demonstrate that by applying a state-of-the-art Offline Reinforcement
Learning algorithm, Conservative Q-Learning (CQL), to the same data and
architecture, we can learn a significantly more robust policy. Using a
carefully engineered reward function, the CQL agent learns a conservative value
function that enables it to recover from minor errors and avoid
out-of-distribution states. In a large-scale evaluation on 1,000 unseen
scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a
3.2x higher success rate and a 7.4x lower collision rate than the strongest BC
baseline, proving that an offline RL approach is critical for learning robust,
long-horizon driving policies from static expert data.

</details>


### [113] [A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling](https://arxiv.org/abs/2508.07032)
*Tiantian He,Keyue Jiang,An Zhao,Anna Schroder,Elinor Thompson,Sonja Soskic,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.LG

TL;DR: The paper proposes a stage-aware Mixture of Experts (MoE) framework to model neurodegenerative disease progression, addressing challenges like scarce longitudinal data and complex pathological mechanisms.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in modeling neurodegenerative disease progression due to irregular data and fixed mechanisms in traditional models.

Method: Introduces IGND-MoE, combining inhomogeneous graph neural diffusion (IGND) and localized neural reaction modules with time-dependent expert weighting.

Result: The model provides stage-specific insights, showing graph-related processes dominate early stages while other mechanisms take over later.

Conclusion: IGND-MoE offers a flexible, principled approach to understanding disease progression, aligning with clinical literature.

Abstract: The long-term progression of neurodegenerative diseases is commonly
conceptualized as a spatiotemporal diffusion process that consists of a graph
diffusion process across the structural brain connectome and a localized
reaction process within brain regions. However, modeling this progression
remains challenging due to 1) the scarcity of longitudinal data obtained
through irregular and infrequent subject visits and 2) the complex interplay of
pathological mechanisms across brain regions and disease stages, where
traditional models assume fixed mechanisms throughout disease progression. To
address these limitations, we propose a novel stage-aware Mixture of Experts
(MoE) framework that explicitly models how different contributing mechanisms
dominate at different disease stages through time-dependent expert
weighting.Data-wise, we utilize an iterative dual optimization method to
properly estimate the temporal position of individual observations,
constructing a co hort-level progression trajectory from irregular snapshots.
Model-wise, we enhance the spatial component with an inhomogeneous graph neural
diffusion model (IGND) that allows diffusivity to vary based on node states and
time, providing more flexible representations of brain networks. We also
introduce a localized neural reaction module to capture complex dynamics beyond
standard processes.The resulting IGND-MoE model dynamically integrates these
components across temporal states, offering a principled way to understand how
stage-specific pathological mechanisms contribute to progression. The
stage-wise weights yield novel clinical insights that align with literature,
suggesting that graph-related processes are more influential at early stages,
while other unknown physical processes become dominant later on.

</details>


### [114] [Differentiable Adaptive Kalman Filtering via Optimal Transport](https://arxiv.org/abs/2508.07037)
*Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin*

Main category: cs.LG

TL;DR: OTAKNet is an online solution for noise-statistics drift in learning-based adaptive Kalman filtering, using optimal transport for adaptation without retraining or labels.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based methods degrade under unobserved noise-statistics drift caused by environmental factors like wind or electromagnetic interference.

Method: OTAKNet connects state estimates to drift via one-step predictive measurement likelihood and uses optimal transport for online adaptation.

Result: OTAKNet outperforms classical model-based and offline learning-based methods, especially with limited training data, on synthetic and real-world datasets.

Conclusion: OTAKNet effectively addresses noise-statistics drift online, enhancing the robustness of learning-based filtering in dynamic environments.

Abstract: Learning-based filtering has demonstrated strong performance in non-linear
dynamical systems, particularly when the statistics of noise are unknown.
However, in real-world deployments, environmental factors, such as changing
wind conditions or electromagnetic interference, can induce unobserved
noise-statistics drift, leading to substantial degradation of learning-based
methods. To address this challenge, we propose OTAKNet, the first online
solution to noise-statistics drift within learning-based adaptive Kalman
filtering. Unlike existing learning-based methods that perform offline
fine-tuning using batch pointwise matching over entire trajectories, OTAKNet
establishes a connection between the state estimate and the drift via one-step
predictive measurement likelihood, and addresses it using optimal transport.
This leverages OT's geometry - aware cost and stable gradients to enable fully
online adaptation without ground truth labels or retraining. We compare OTAKNet
against classical model-based adaptive Kalman filtering and offline
learning-based filtering. The performance is demonstrated on both synthetic and
real-world NCLT datasets, particularly under limited training data.

</details>


### [115] [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054)
*Ziqi Zhang,Ali Shahin Shamsabadi,Hanxiao Lu,Yifeng Cai,Hamed Haddadi*

Main category: cs.LG

TL;DR: The paper examines privacy risks in Knowledge Distillation (KD) for Large Language Models (LLMs), showing that all tested KD techniques transfer membership and memorization risks from teacher to student models, with varying severity.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in KD, where student models may inherit private data risks from teacher models trained on sensitive data.

Method: Systematic evaluation of six KD techniques across seven NLP tasks, using GPT-2, LLAMA-2, and OPT teacher models and various student sizes.

Result: All KD techniques transfer privacy risks, but severity varies. Key components like KD objectives and training data impact risks. Memorization and membership risks often disagree.

Conclusion: Privacy risks in KD are pervasive but differ by technique and model block, highlighting the need for risk-aware KD approaches.

Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high
computational demands of Large Language Models (LLMs) by transferring knowledge
from a large ''teacher'' to a smaller ''student'' model. However, students may
inherit the teacher's privacy when the teacher is trained on private data. In
this work, we systematically characterize and investigate membership and
memorization privacy risks inherent in six LLM KD techniques. Using
instruction-tuning settings that span seven NLP tasks, together with three
teacher model families (GPT-2, LLAMA-2, and OPT), and various size student
models, we demonstrate that all existing LLM KD approaches carry membership and
memorization privacy risks from the teacher to its students. However, the
extent of privacy risks varies across different KD techniques. We
systematically analyse how key LLM KD components (KD objective functions,
student training data and NLP tasks) impact such privacy risks. We also
demonstrate a significant disagreement between memorization and membership
privacy risks of LLM KD techniques. Finally, we characterize per-block privacy
risk and demonstrate that the privacy risk varies across different blocks by a
large margin.

</details>


### [116] [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
*Stanley Ngugi*

Main category: cs.LG

TL;DR: The paper introduces a 'unlearn-then-learn' strategy using PEFT and $IA^3$ to update LLMs' knowledge, achieving high accuracy for new facts while minimizing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with updating conflicting knowledge, leading to resistance and forgetting. The paper aims to address these issues with a targeted approach.

Method: A two-stage strategy: first localizing conflicting knowledge circuits, then using PEFT ($IA^3$) to unlearn and relearn. Evaluated on microsoft/Phi-3-mini-4k-instruct.

Result: Achieved 98.50% accuracy for new facts, 96.00% forget rate for conflicting facts, and 72.00% F_control accuracy, reducing catastrophic forgetting.

Conclusion: The strategy advances precise, localized, and safe knowledge management in LLMs, with 'soft forgetting' enhancing control and safety.

Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates,
especially when new information conflicts with deeply embedded facts. Such
conflicting factual edits often lead to two critical issues: resistance to
adopting the new fact and severe catastrophic forgetting of unrelated
knowledge. This paper introduces and evaluates a novel "unlearn-then-learn"
strategy for precise knowledge editing in LLMs, leveraging the
parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting
and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach
is powered by an initial circuit localization phase that identifies and targets
the specific internal components responsible for encoding the conflicting fact.
Through a rigorous experimental methodology on
microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically
informed two-stage approach achieves near-perfect accuracy (98.50%) for the
new, modulated fact while simultaneously effectively suppressing the original
conflicting fact (96.00% forget rate). Critically, our strategy exhibits
unprecedented localization (72.00% F_control accuracy), dramatically mitigating
catastrophic forgetting observed in direct fine-tuning approaches (which showed
as low as ~20% F_control accuracy), a direct benefit of our targeted
interpretability-guided intervention. Furthermore, qualitative analysis reveals
a nuanced mechanism of "soft forgetting," where original knowledge is
suppressed from default retrieval but remains latent and conditionally
accessible, enhancing model safety and control. These findings represent a
significant advancement towards precise, localized, and safe knowledge
management in compact LLMs.

</details>


### [117] [Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework](https://arxiv.org/abs/2508.07085)
*N Harshit,K Mounvik*

Main category: cs.LG

TL;DR: A hybrid framework combining Transformers and Autoencoders is proposed for early and sensitive online concept drift detection, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Concept drift in machine learning reduces model performance, and existing detection methods are reactive and lack sensitivity for early detection.

Method: A hybrid framework with Transformers and Autoencoders, plus a Trust Score methodology incorporating statistical, reconstruction, uncertainty, and trend metrics.

Result: The proposed model detects drift earlier and more sensitively than baselines, validated on a time-sequenced airline dataset with synthetic drift.

Conclusion: The framework reliably monitors concept drift, offering improved sensitivity and interpretability for real-time applications.

Abstract: In applied machine learning, concept drift, which is either gradual or abrupt
changes in data distribution, can significantly reduce model performance.
Typical detection methods,such as statistical tests or reconstruction-based
models,are generally reactive and not very sensitive to early detection. Our
study proposes a hybrid framework consisting of Transformers and Autoencoders
to model complex temporal dynamics and provide online drift detection. We
create a distinct Trust Score methodology, which includes signals on (1)
statistical and reconstruction-based drift metrics, more specifically, PSI,
JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,
and (4) trend of classifier error aligned with the combined metrics defined by
the Trust Score. Using a time sequenced airline passenger data set with
synthetic drift, our proposed model allows for a better detection of drift
using as a whole and at different detection thresholds for both sensitivity and
interpretability compared to baseline methods and provides a strong pipeline
for drift detection in real time for applied machine learning. We evaluated
performance using a time-sequenced airline passenger dataset having the
gradually injected stimulus of drift in expectations,e.g. permuted ticket
prices in later batches, broken into 10 time segments [1].In the data, our
results support that the Transformation-Autoencoder detected drift earlier and
with more sensitivity than the autoencoders commonly used in the literature,
and provided improved modeling over more error rates and logical violations.
Therefore, a robust framework was developed to reliably monitor concept drift.

</details>


### [118] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: The paper introduces Second-Order MeanFlow, an extension of MeanFlow that incorporates average acceleration fields, proving its feasibility, expressivity, and scalable implementation for efficient sampling.


<details>
  <summary>Details</summary>
Motivation: To enhance generative modelling by extending MeanFlow with second-order dynamics (average acceleration) for richer dynamics and practical efficiency.

Method: Theoretical study of Second-Order MeanFlow, proving consistency conditions, analyzing expressivity via circuit complexity, and deriving scalable implementation criteria using fast attention approximations.

Result: Second-Order MeanFlow supports stable one-step sampling, is implementable in the TC0 class, and allows efficient attention approximations with poly(n) error in near-quadratic time.

Conclusion: The work provides a theoretical foundation for high-order flow matching models, combining expressive dynamics with practical sampling efficiency.

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [119] [BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation](https://arxiv.org/abs/2508.07106)
*Yiran Huang,Amirhossein Nouranizadeh,Christine Ahrends,Mengjia Xu*

Main category: cs.LG

TL;DR: BrainATCL is an unsupervised framework for adaptive temporal brain connectivity learning in fMRI data, outperforming conventional GNNs in tasks like functional link prediction and age estimation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional GNNs in capturing long-range temporal dependencies in dynamic fMRI data, which is crucial for understanding brain connectivity dynamics.

Method: Proposes BrainATCL, a nonparametric framework with adaptive lookback windows and a GINE-Mamba2 backbone for spatial-temporal representation learning, incorporating brain structure and function-informed edge attributes.

Result: Demonstrates superior performance in functional link prediction and age estimation, with strong generalization, including cross-session scenarios.

Conclusion: BrainATCL effectively models dynamic brain connectivity, offering insights into transient neural states and potential applications in behavior and neuropsychiatric disease research.

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely
used to study human brain activity. fMRI signals in areas across the brain
transiently synchronise and desynchronise their activity in a highly structured
manner, even when an individual is at rest. These functional connectivity
dynamics may be related to behaviour and neuropsychiatric disease. To model
these dynamics, temporal brain connectivity representations are essential, as
they reflect evolving interactions between brain regions and provide insight
into transient neural states and network reconfigurations. However,
conventional graph neural networks (GNNs) often struggle to capture long-range
temporal dependencies in dynamic fMRI data. To address this challenge, we
propose BrainATCL, an unsupervised, nonparametric framework for adaptive
temporal brain connectivity learning, enabling functional link prediction and
age estimation. Our method dynamically adjusts the lookback window for each
snapshot based on the rate of newly added edges. Graph sequences are
subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal
representations of dynamic functional connectivity in resting-state fMRI data
of 1,000 participants from the Human Connectome Project. To further improve
spatial modeling, we incorporate brain structure and function-informed edge
attributes, i.e., the left/right hemispheric identity and subnetwork membership
of brain regions, enabling the model to capture biologically meaningful
topological patterns. We evaluate our BrainATCL on two tasks: functional link
prediction and age estimation. The experimental results demonstrate superior
performance and strong generalization, including in cross-session prediction
scenarios.

</details>


### [120] [Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning](https://arxiv.org/abs/2508.07114)
*Atakan Azakli,Bernd Stelzer*

Main category: cs.LG

TL;DR: A new ML method improves prediction precision and discriminative power in challenging hypotheses testing, reducing errors and leveraging MIL for better performance.


<details>
  <summary>Details</summary>
Motivation: To enhance ML model precision and discriminative power in difficult prediction scenarios, particularly where state-of-the-art classifiers struggle.

Method: Proposes a Multiple Instance Learning (MIL) approach, analyzing its scaling behavior with instance numbers and applying it to constrain Wilson coefficients in SMEFT using LHC data.

Result: Demonstrates MIL's superior predictive power and potential to extract maximum Fisher Information from datasets under specific conditions.

Conclusion: MIL offers significant advantages in precision and discriminative power for complex hypotheses testing, with practical applications in physics.

Abstract: In this work, we propose a new machine learning (ML) methodology to obtain
more precise predictions for some parameters of interest in a given hypotheses
testing problem. Our proposed method also allows ML models to have more
discriminative power in cases where it is extremely challenging for
state-of-the-art classifiers to have any level of accurate predictions. This
method can also allow us to systematically decrease the error from ML models in
their predictions. In this paper, we provide a mathematical motivation why
Multiple Instance Learning (MIL) would have more predictive power over their
single-instance counterparts. We support our theoretical claims by analyzing
the behavior of the MIL models through their scaling behaviors with respect to
the number of instances on which the model makes predictions. As a concrete
application, we constrain Wilson coefficients of the Standard Model Effective
Field Theory (SMEFT) using kinematic information from subatomic particle
collision events at the Large Hadron Collider (LHC). We show that under certain
circumstances, it might be possible to extract the theoretical maximum Fisher
Information latent in a dataset.

</details>


### [121] [From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context](https://arxiv.org/abs/2508.07117)
*Peyman Baghershahi,Gregoire Fournier,Pranav Nyati,Sourav Medya*

Main category: cs.LG

TL;DR: LOGIC is a lightweight framework using LLMs to generate interpretable explanations for GNN predictions by aligning GNN internals with human reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing GNN explanation methods struggle with interpretability, especially for text-attributed graphs.

Method: LOGIC projects GNN node embeddings into LLM space, using hybrid prompts to generate natural language explanations and subgraphs.

Result: LOGIC achieves a balance between fidelity and sparsity, improving human-centric metrics like insightfulness.

Conclusion: LOGIC advances LLM-based explainability in graph learning by bridging GNN representations and human understanding.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (LLMs) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
LLM embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the LLM to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and sparsity, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for LLM-based
explainability in graph learning by aligning GNN internals with human
reasoning.

</details>


### [122] [Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2508.07122)
*Zhihao Xue,Yun Zi,Nia Qi,Ming Gong,Yujun Zou*

Main category: cs.LG

TL;DR: A spatiotemporal graph neural network predicts performance in distributed backend systems by modeling service call structures and runtime features, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address forecasting challenges in distributed backend systems with multi-level service call structures by integrating spatiotemporal modeling.

Method: Combines graph convolutional networks for dependency extraction and gated recurrent networks for temporal dynamics, enhanced with time encoding for non-stationary sequences.

Result: Outperforms existing methods in MAE, RMSE, and R2 metrics, showing robustness under varying loads and structural complexities.

Conclusion: The model is effective for backend service performance management, demonstrating practical potential.

Abstract: This paper proposes a spatiotemporal graph neural network-based performance
prediction algorithm to address the challenge of forecasting performance
fluctuations in distributed backend systems with multi-level service call
structures. The method abstracts system states at different time slices into a
sequence of graph structures. It integrates the runtime features of service
nodes with the invocation relationships among services to construct a unified
spatiotemporal modeling framework. The model first applies a graph
convolutional network to extract high-order dependency information from the
service topology. Then it uses a gated recurrent network to capture the dynamic
evolution of performance metrics over time. A time encoding mechanism is also
introduced to enhance the model's ability to represent non-stationary temporal
sequences. The architecture is trained in an end-to-end manner, optimizing the
multi-layer nested structure to achieve high-precision regression of future
service performance metrics. To validate the effectiveness of the proposed
method, a large-scale public cluster dataset is used. A series of
multi-dimensional experiments are designed, including variations in time
windows and concurrent load levels. These experiments comprehensively evaluate
the model's predictive performance and stability. The experimental results show
that the proposed model outperforms existing representative methods across key
metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying
load intensities and structural complexities. These results demonstrate the
model's practical potential for backend service performance management tasks.

</details>


### [123] [Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning](https://arxiv.org/abs/2508.07126)
*Zhengran Ji,Boyuan Chen*

Main category: cs.LG

TL;DR: Pref-GUIDE transforms noisy real-time scalar feedback into preference-based data to improve reward model learning in online reinforcement learning, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Human feedback is essential when task objectives are hard to specify, but scalar feedback is noisy and inconsistent, limiting reward model accuracy.

Method: Pref-GUIDE converts scalar feedback into preference-based data, using short-window comparisons (Individual) and population consensus (Voting) to enhance robustness.

Result: Pref-GUIDE outperforms scalar-feedback baselines and even expert-designed dense rewards in three challenging environments.

Conclusion: Pref-GUIDE provides a scalable, principled method for leveraging human input in online reinforcement learning by reframing feedback as structured preferences.

Abstract: Training reinforcement learning agents with human feedback is crucial when
task objectives are difficult to specify through dense reward functions. While
prior methods rely on offline trajectory comparisons to elicit human
preferences, such data is unavailable in online learning scenarios where agents
must adapt on the fly. Recent approaches address this by collecting real-time
scalar feedback to guide agent behavior and train reward models for continued
learning after human feedback becomes unavailable. However, scalar feedback is
often noisy and inconsistent, limiting the accuracy and generalization of
learned rewards. We propose Pref-GUIDE, a framework that transforms real-time
scalar feedback into preference-based data to improve reward model learning for
continual policy training. Pref-GUIDE Individual mitigates temporal
inconsistency by comparing agent behaviors within short windows and filtering
ambiguous feedback. Pref-GUIDE Voting further enhances robustness by
aggregating reward models across a population of users to form consensus
preferences. Across three challenging environments, Pref-GUIDE significantly
outperforms scalar-feedback baselines, with the voting variant exceeding even
expert-designed dense rewards. By reframing scalar feedback as structured
preferences with population feedback, Pref-GUIDE offers a scalable and
principled approach for harnessing human input in online reinforcement
learning.

</details>


### [124] [How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?](https://arxiv.org/abs/2508.07127)
*Niranjana Arun Menon,Iqra Farooq,Yulong Li,Sara Ahmed,Yutong Xie,Muhammad Awais,Imran Razzak*

Main category: cs.LG

TL;DR: The paper explores using fine-tuned LLMs to predict cardiovascular diseases (CVD) and SNPs linked to CVD risk from genomic data, leveraging Chain of Thought reasoning for improved clinical deductions.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular disease prediction is complex due to multifactorial causes and noisy genomic data. LLMs offer potential to extract meaningful insights from such data.

Method: Fine-tuned LLMs analyze genetic markers from high-throughput genomic profiling, using Chain of Thought reasoning to predict diseases and SNPs.

Result: LLMs show promise in learning latent biological relationships and improving early detection and risk assessment for CVD.

Conclusion: LLMs can advance personalized cardiac care by enhancing disease prediction and clinical deductions from genomic data.

Abstract: Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned LLMs to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how LLMs can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of LLMs in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.

</details>


### [125] [A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs](https://arxiv.org/abs/2508.07134)
*Lu Chenggang*

Main category: cs.LG

TL;DR: A novel method for globally optimal semi-NMF is proposed, outperforming existing NMF and semi-NMF methods in reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing semi-NMF algorithms are iterative, non-convex, and prone to local minima, necessitating a better solution.

Method: Uses an orthogonal decomposition from the scatter matrix of input data to achieve a globally optimal solution under the Frobenius norm.

Result: Achieves lower reconstruction error than standard NMF for nonnegative input matrices and recovers NMF structure in low-rank cases.

Conclusion: The method provides theoretical guarantees and empirical advantages, offering a new perspective on matrix factorization.

Abstract: Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical
Nonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain
both positive and negative entries, making it suitable for decomposing data
with mixed signs. However, most existing semi-NMF algorithms are iterative,
non-convex, and prone to local minima. In this paper, we propose a novel method
that yields a globally optimal solution to the semi-NMF problem under the
Frobenius norm, through an orthogonal decomposition derived from the scatter
matrix of the input data. We rigorously prove that our solution attains the
global minimum of the reconstruction error. Furthermore, we demonstrate that
when the input matrix is nonnegative, our method often achieves lower
reconstruction error than standard NMF algorithms, although unfortunately the
basis matrix may not satisfy nonnegativity. In particular, in low-rank cases
such as rank 1 or 2, our solution reduces exactly to a nonnegative
factorization, recovering the NMF structure. We validate our approach through
experiments on both synthetic data and the UCI Wine dataset, showing that our
method consistently outperforms existing NMF and semi-NMF methods in terms of
reconstruction accuracy. These results confirm that our globally optimal,
non-iterative formulation offers both theoretical guarantees and empirical
advantages, providing a new perspective on matrix factorization in optimization
and data analysis.

</details>


### [126] [A Stable and Principled Loss Function for Direct Language Model Alignment](https://arxiv.org/abs/2508.07137)
*Yuandong Tan*

Main category: cs.LG

TL;DR: The paper critiques Direct Preference Optimization (DPO) for its theoretical misalignment and instability, proposing a novel loss function derived from RLHF optimality to ensure stability and better alignment.


<details>
  <summary>Details</summary>
Motivation: To address the instability and reward hacking issues in DPO by aligning the loss function with RLHF optimality.

Method: Proposes a new loss function targeting a finite logits difference, validated through theoretical analysis and fine-tuning a Qwen2.5-7B model.

Result: Shows significant win-rate improvements over DPO and competitive performance against larger models like Llama-3.1-8B.

Conclusion: The proposed method offers a more stable and effective alternative to DPO for aligning LLMs with human preferences.

Abstract: The alignment of large language models (LLMs) with human preferences is
commonly achieved through Reinforcement Learning from Human Feedback (RLHF).
Direct Preference Optimization (DPO) simplified this paradigm by establishing a
direct mapping between the optimal policy and a reward function, eliminating
the need for an explicit reward model. However, we argue that the DPO loss
function is theoretically misaligned with its own derivation, as it promotes
the indefinite maximization of a logits difference, which can lead to training
instability and reward hacking. In this paper, we propose a novel loss function
derived directly from the RLHF optimality condition. Our proposed loss targets
a specific, finite value for the logits difference, which is dictated by the
underlying reward, rather than its maximization. We provide a theoretical
analysis, including a gradient-based comparison, to demonstrate that our method
avoids the large gradients that plague DPO when the probability of dispreferred
responses approaches zero. This inherent stability prevents reward hacking and
leads to more effective alignment. We validate our approach by fine-tuning a
Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO
baseline and achieving competitive performance against larger models like
Llama-3.1-8B.

</details>


### [127] [Strategic Incentivization for Locally Differentially Private Federated Learning](https://arxiv.org/abs/2508.07138)
*Yashwant Krishna Pagoti,Arunesh Sinha,Shamik Sural*

Main category: cs.LG

TL;DR: The paper models the privacy-accuracy trade-off in Federated Learning (FL) as a game, introducing a token-based incentivization mechanism to balance client privacy and global model accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the degradation in global model accuracy caused by Local Differential Privacy (LDP) noise in FL, while ensuring client privacy.

Method: Propose a game-theoretic approach where the server incentivizes clients to add less noise via tokens, and clients balance privacy and accuracy.

Result: Strategic analysis and experiments show the effectiveness of the token-based mechanism in managing the trade-off.

Conclusion: The token-based incentivization mechanism successfully balances privacy and accuracy in FL.

Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning
model by sharing gradient information, instead of raw data, with a server over
multiple rounds. To address the possibility of information leakage in spite of
sharing only the gradients, Local Differential Privacy (LDP) is often used. In
LDP, clients add a selective amount of noise to the gradients before sending
the same to the server. Although such noise addition protects the privacy of
clients, it leads to a degradation in global model accuracy. In this paper, we
model this privacy-accuracy trade-off as a game, where the sever incentivizes
the clients to add a lower degree of noise for achieving higher accuracy, while
the clients attempt to preserve their privacy at the cost of a potential loss
in accuracy. A token based incentivization mechanism is introduced in which the
quantum of tokens credited to a client in an FL round is a function of the
degree of perturbation of its gradients. The client can later access a newly
updated global model only after acquiring enough tokens, which are to be
deducted from its balance. We identify the players, their actions and payoff,
and perform a strategic analysis of the game. Extensive experiments were
carried out to study the impact of different parameters.

</details>


### [128] [SGD Convergence under Stepsize Shrinkage in Low-Precision Training](https://arxiv.org/abs/2508.07142)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: The paper analyzes the impact of low-precision training on SGD convergence, showing gradient shrinkage and quantization noise slow convergence but still allow it under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how low-precision training affects SGD convergence due to gradient shrinkage and additive noise.

Method: Modeling low-precision SGD as gradient shrinkage and analyzing its convergence under smoothness and bounded-variance assumptions.

Result: Low-precision SGD converges but at a reduced rate due to shrinkage and has an increased error floor from quantization noise.

Conclusion: Low-precision training slows SGD convergence but remains viable under theoretical conditions.

Abstract: Low-precision training has become essential for reducing the computational
and memory costs of large-scale deep learning. However, quantization of
gradients introduces both magnitude shrinkage and additive noise, which can
alter the convergence behavior of stochastic gradient descent (SGD). In this
work, we study the convergence of SGD under a gradient shrinkage model, where
each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by
zero-mean quantization noise. We show that this shrinkage is equivalent to
replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$,
which slows convergence when $q_{\min} < 1$. Under standard smoothness and
bounded-variance assumptions, we prove that low-precision SGD still converges,
but at a reduced rate determined by $q_{\min}$, and with an increased
asymptotic error floor due to quantization noise. We theoretically analyze how
reduced numerical precision slows down training by modeling it as gradient
shrinkage in the standard SGD convergence framework.

</details>


### [129] [What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains](https://arxiv.org/abs/2508.07208)
*Chanakya Ekbote,Marco Bondaschi,Nived Rajaraman,Jason D. Lee,Michael Gastpar,Ashok Vardhan Makkuva,Paul Pu Liang*

Main category: cs.LG

TL;DR: A two-layer transformer with one head per layer can represent any conditional k-gram, showing shallow architectures can achieve strong in-context learning (ICL) capabilities.


<details>
  <summary>Details</summary>
Motivation: To determine if a two-layer single-head transformer can represent any kth-order Markov process, addressing gaps in understanding transformer depth and Markov order for ICL.

Method: Theoretical analysis and simplified variant experiments on learning dynamics for first-order Markov chains.

Result: Proves a two-layer transformer with one head per layer can represent any conditional k-gram, tightens the understanding of transformer depth and Markov order for ICL.

Conclusion: Shallow transformers can exhibit strong ICL capabilities, deepening understanding of transformer-based ICL for structured sequence modeling.

Abstract: In-context learning (ICL) is a hallmark capability of transformers, through
which trained models learn to adapt to new tasks by leveraging information from
the input context. Prior work has shown that ICL emerges in transformers due to
the presence of special circuits called induction heads. Given the equivalence
between induction heads and conditional k-grams, a recent line of work modeling
sequential inputs as Markov processes has revealed the fundamental impact of
model depth on its ICL capabilities: while a two-layer transformer can
efficiently represent a conditional 1-gram model, its single-layer counterpart
cannot solve the task unless it is exponentially large. However, for higher
order Markov sources, the best known constructions require at least three
layers (each with a single attention head) - leaving open the question: can a
two-layer single-head transformer represent any kth-order Markov process? In
this paper, we precisely address this and theoretically show that a two-layer
transformer with one head per layer can indeed represent any conditional
k-gram. Thus, our result provides the tightest known characterization of the
interplay between transformer depth and Markov order for ICL. Building on this,
we further analyze the learning dynamics of our two-layer construction,
focusing on a simplified variant for first-order Markov chains, illustrating
how effective in-context representations emerge during training. Together,
these results deepen our current understanding of transformer-based ICL and
illustrate how even shallow architectures can surprisingly exhibit strong ICL
capabilities on structured sequence modeling tasks.

</details>


### [130] [Neural Bridge Processes](https://arxiv.org/abs/2508.07220)
*Jian Xu,Yican Liu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: Neural Bridge Processes (NBPs) improve stochastic function modeling by dynamically anchoring inputs to the diffusion trajectory, ensuring endpoint coherence and stronger gradients.


<details>
  <summary>Details</summary>
Motivation: Traditional models like GPs and NPs have limitations in scalability, flexibility, and capturing multi-modal distributions. NDPs, while expressive, suffer from weak input coupling and semantic mismatch.

Method: NBPs reformulate the forward kernel to explicitly depend on inputs, enforcing a constrained diffusion path that terminates at the supervised target.

Result: NBPs outperform baselines on synthetic data, EEG signal regression, and image regression tasks.

Conclusion: NBPs enhance performance and theoretical consistency for structured prediction tasks using DDPM-style bridge sampling.

Abstract: Learning stochastic functions from partially observed context-target pairs is
a fundamental problem in probabilistic modeling. Traditional models like
Gaussian Processes (GPs) face scalability issues with large datasets and assume
Gaussianity, limiting their applicability. While Neural Processes (NPs) offer
more flexibility, they struggle with capturing complex, multi-modal target
distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a
learned diffusion process but rely solely on conditional signals in the
denoising network, resulting in weak input coupling from an unconditional
forward process and semantic mismatch at the diffusion endpoint. In this work,
we propose Neural Bridge Processes (NBPs), a novel method for modeling
stochastic functions where inputs x act as dynamic anchors for the entire
diffusion trajectory. By reformulating the forward kernel to explicitly depend
on x, NBP enforces a constrained path that strictly terminates at the
supervised target. This approach not only provides stronger gradient signals
but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG
signal regression and image regression tasks, achieving substantial
improvements over baselines. These results underscore the effectiveness of
DDPM-style bridge sampling in enhancing both performance and theoretical
consistency for structured prediction tasks.

</details>


### [131] [EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning](https://arxiv.org/abs/2508.07224)
*Ananda Prakash Verma*

Main category: cs.LG

TL;DR: EDGE is a misconception-aware adaptive learning framework with four stages: Evaluate, Diagnose, Generate, and Exercise. It integrates psychometrics, cognitive diagnostics, contrastive item generation, and scheduling, introducing EdgeScore for readiness assessment.


<details>
  <summary>Details</summary>
Motivation: To address misconceptions in adaptive learning by unifying psychometrics, cognitive diagnostics, and principled scheduling for more effective learning interventions.

Method: EDGE combines IRT/Bayesian models, misconception discovery, contrastive item generation, and restless bandit scheduling. It introduces EdgeScore and a near-optimal index policy.

Result: Theoretical proofs show EdgeScore's monotonicity and Lipschitz continuity, and counterfactual items reduce misconceptions faster than standard practice under certain conditions.

Conclusion: EDGE provides a theoretical and implementable framework for misconception-aware adaptive learning, with empirical validation left for future work.

Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning
framework composed of four stages: Evaluate (ability and state estimation),
Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual
item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies
psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics
(misconception discovery from distractor patterns and response latencies),
contrastive item generation (minimal perturbations that invalidate learner
shortcuts while pre-serving psychometric validity), and principled scheduling
(a restless bandit approximation to spaced retrieval). We formalize a composite
readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,
and derive an index policy that is near-optimal under mild assumptions on
forgetting and learning gains. We further establish conditions under which
counterfactual items provably reduce the posterior probability of a targeted
misconception faster than standard practice. The paper focuses on theory and
implementable pseudocode; empirical study is left to future work.

</details>


### [132] [Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation](https://arxiv.org/abs/2508.07243)
*Chu Zhao,Eneng Yang,Yizhou Dang,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.LG

TL;DR: CNSDiff improves recommendation performance by mitigating false hard negatives caused by environmental confounders, using a diffusion-based method and causal regularization.


<details>
  <summary>Details</summary>
Motivation: Heuristic negative sampling can introduce false hard negatives due to biases in candidate pools, harming model generalization.

Method: Proposes CNSDiff, which synthesizes negative samples via conditional diffusion and uses causal regularization to reduce confounder influence.

Result: Achieves 13.96% average improvement in OOD recommendation tasks over baselines.

Conclusion: CNSDiff effectively reduces false hard negatives and enhances robustness in OOD scenarios.

Abstract: Heuristic negative sampling enhances recommendation performance by selecting
negative samples of varying hardness levels from predefined candidate pools to
guide the model toward learning more accurate decision boundaries. However, our
empirical and theoretical analyses reveal that unobserved environmental
confounders (e.g., exposure or popularity biases) in candidate pools may cause
heuristic sampling methods to introduce false hard negatives (FHNS). These
misleading samples can encourage the model to learn spurious correlations
induced by such confounders, ultimately compromising its generalization ability
under distribution shifts. To address this issue, we propose a novel method
named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing
negative samples in the latent space via a conditional diffusion process,
CNSDiff avoids the bias introduced by predefined candidate pools and thus
reduces the likelihood of generating FHNS. Moreover, it incorporates a causal
regularization term to explicitly mitigate the influence of environmental
confounders during the negative sampling process, leading to robust negatives
that promote out-of-distribution (OOD) generalization. Comprehensive
experiments under four representative distribution shift scenarios demonstrate
that CNSDiff achieves an average improvement of 13.96% across all evaluation
metrics compared to state-of-the-art baselines, verifying its effectiveness and
robustness in OOD recommendation tasks.

</details>


### [133] [Policy Newton methods for Distortion Riskmetrics](https://arxiv.org/abs/2508.07249)
*Soumen Pachal,Mizhaan Prajit Maniyar,Prashanth L. A*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of risk-sensitive control in a reinforcement learning
(RL) framework. In particular, we aim to find a risk-optimal policy by
maximizing the distortion riskmetric (DRM) of the discounted reward in a finite
horizon Markov decision process (MDP). DRMs are a rich class of risk measures
that include several well-known risk measures as special cases. We derive a
policy Hessian theorem for the DRM objective using the likelihood ratio method.
Using this result, we propose a natural DRM Hessian estimator from sample
trajectories of the underlying MDP. Next, we present a cubic-regularized policy
Newton algorithm for solving this problem in an on-policy RL setting using
estimates of the DRM gradient and Hessian. Our proposed algorithm is shown to
converge to an $\epsilon$-second-order stationary point ($\epsilon$-SOSP) of
the DRM objective, and this guarantee ensures the escaping of saddle points.
The sample complexity of our algorithms to find an $ \epsilon$-SOSP is
$\mathcal{O}(\epsilon^{-3.5})$. Our experiments validate the theoretical
findings. To the best of our knowledge, our is the first work to present
convergence to an $\epsilon$-SOSP of a risk-sensitive objective, while existing
works in the literature have either shown convergence to a first-order
stationary point of a risk-sensitive objective, or a SOSP of a risk-neutral
one.

</details>


### [134] [PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets](https://arxiv.org/abs/2508.07253)
*Bartlomiej Chybowski,Shima Abdullateef,Hollan Haule,Alfredo Gonzalez-Sulser,Javier Escudero*

Main category: cs.LG

TL;DR: An open-source machine-learning framework for robust, generalizable seizure detection across diverse EEG datasets, achieving high performance and cross-dataset transferability.


<details>
  <summary>Details</summary>
Motivation: Manual EEG interpretation for seizure detection is time-consuming, and existing machine learning methods lack real-world applicability due to dataset-specific optimizations.

Method: The framework includes automated pre-processing, a majority voting mechanism, and cross-dataset evaluation. Models are trained and tested on two distinct EEG datasets.

Result: High within-dataset performance (AUC ~0.90) and strong cross-dataset generalization (AUC ~0.62-0.77) were achieved, with minor post-processing improvements.

Conclusion: The framework offers a reproducible, clinically viable solution for seizure detection, complementing expert interpretation and accelerating adoption in diverse settings.

Abstract: Reliable seizure detection is critical for diagnosing and managing epilepsy,
yet clinical workflows remain dependent on time-consuming manual EEG
interpretation. While machine learning has shown promise, existing approaches
often rely on dataset-specific optimisations, limiting their real-world
applicability and reproducibility. Here, we introduce an innovative,
open-source machine-learning framework that enables robust and generalisable
seizure detection across varied clinical datasets. We evaluate our approach on
two publicly available EEG datasets that differ in patient populations and
electrode configurations. To enhance robustness, the framework incorporates an
automated pre-processing pipeline to standardise data and a majority voting
mechanism, in which multiple models independently assess each second of EEG
before reaching a final decision. We train, tune, and evaluate models within
each dataset, assessing their cross-dataset transferability. Our models achieve
high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and
0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets
despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models
trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)
without any post-processing. Furthermore, a mild post-processing improved the
within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset
results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the
potential of, and essential considerations for, deploying our framework in
diverse clinical settings. By making our methodology fully reproducible, we
provide a foundation for advancing clinically viable, dataset-agnostic seizure
detection systems. This approach has the potential for widespread adoption,
complementing rather than replacing expert interpretation, and accelerating
clinical integration.

</details>


### [135] [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.LG

TL;DR: The paper reviews influence functions in deep learning for data attribution, covering theory, algorithmic advances, and applications like mislabel detection, while addressing challenges for real-world use.


<details>
  <summary>Details</summary>
Motivation: To understand how training data influences model predictions for interpretability, debugging, and accountability.

Method: Uses influence functions for efficient first-order approximation of data impact without retraining, focusing on inverse-Hessian-vector product estimation.

Result: Evaluates effectiveness for data attribution and mislabel detection, highlighting algorithmic improvements.

Conclusion: Identifies challenges and future directions to leverage influence functions in large-scale deep learning.

Abstract: The goal of data attribution is to trace the model's predictions through the
learning algorithm and back to its training data. thereby identifying the most
influential training samples and understanding how the model's behavior leads
to particular predictions. Understanding how individual training examples
influence a model's predictions is fundamental for machine learning
interpretability, data debugging, and model accountability. Influence
functions, originating from robust statistics, offer an efficient, first-order
approximation to estimate the impact of marginally upweighting or removing a
data point on a model's learned parameters and its subsequent predictions,
without the need for expensive retraining. This paper comprehensively reviews
the data attribution capability of influence functions in deep learning. We
discuss their theoretical foundations, recent algorithmic advances for
efficient inverse-Hessian-vector product estimation, and evaluate their
effectiveness for data attribution and mislabel detection. Finally,
highlighting current challenges and promising directions for unleashing the
huge potential of influence functions in large-scale, real-world deep learning
scenarios.

</details>


### [136] [When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective](https://arxiv.org/abs/2508.07299)
*Lin-Han Jia,Si-Yu Han,Wen-Chao Hu,Jie-Jing Shao,Wen-Da Wei,Zhi Zhou,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: The paper unifies neuro-symbolic (Nesy) and semi/self-supervised learning (SSL) theories, proposing metrics to predict pretext task effectiveness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between heuristic-based pretext task selection and theory-driven approaches in SSL and Nesy learning.

Method: Extends Nesy theory to unreliable knowledge (assumptions), identifies three key factors (learnability, reliability, completeness), and proposes operational schemes.

Result: High correlation between predicted and actual performance validates the theory and evaluation method.

Conclusion: The framework provides a theory-based approach to pretext task selection, improving SSL and Nesy learning outcomes.

Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models
by enabling them to satisfy knowledge, while semi/self-supervised learning
(SSL) improves the target task performance by designing unsupervised pretext
tasks for unlabeled data to make models satisfy corresponding assumptions. We
extend the Nesy theory based on reliable knowledge to the scenario of
unreliable knowledge (i.e., assumptions), thereby unifying the theoretical
frameworks of SSL and Nesy. Through rigorous theoretical analysis, we
demonstrate that, in theory, the impact of pretext tasks on target performance
hinges on three factors: knowledge learnability with respect to the model,
knowledge reliability with respect to the data, and knowledge completeness with
respect to the target. We further propose schemes to operationalize these
theoretical metrics, and thereby develop a method that can predict the
effectiveness of pretext tasks in advance. This will change the current status
quo in practical applications, where the selections of unsupervised tasks are
heuristic-based rather than theory-based, and it is difficult to evaluate the
rationality of unsupervised pretext task selection before testing the model on
the target task. In experiments, we verify a high correlation between the
predicted performance-estimated using minimal data-and the actual performance
achieved after large-scale semi-supervised or self-supervised learning, thus
confirming the validity of the theory and the effectiveness of the evaluation
method.

</details>


### [137] [Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative](https://arxiv.org/abs/2508.07329)
*Tuo Zhang,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.LG

TL;DR: Proposes an efficient MoE edge deployment scheme using Hessian-Aware Quantization (HAQ) and CPU-GPU collaboration to address quantization accuracy and memory issues.


<details>
  <summary>Details</summary>
Motivation: Deploying large language models (LLMs) on resource-constrained edge devices is challenging due to quantization accuracy degradation and memory limitations in MoE architectures.

Method: Uses Hessian-Aware Quantization (HAQ) for 8-bit joint quantization of activations and weights, and designs a CPU-GPU collaborative inference mechanism for efficient expert module deployment.

Result: Achieves near full-precision model accuracy, reduces GPU memory usage by ~60%, and significantly lowers inference latency on models like OPT and Mixtral 8*7B.

Conclusion: The proposed scheme effectively addresses MoE deployment challenges, balancing accuracy, memory efficiency, and performance.

Abstract: With the breakthrough progress of large language models (LLMs) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through sparse
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.

</details>


### [138] [Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants](https://arxiv.org/abs/2508.07333)
*Yuhao Liu,Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: The paper analyzes finite-time convergence guarantees for numerical schemes in stochastic interpolants, providing error bounds for Euler and Heun's methods and optimizing computational schedules.


<details>
  <summary>Details</summary>
Motivation: Stochastic interpolants are promising for generative modeling, but lack rigorous finite-time convergence guarantees for practical numerical implementations.

Method: The study focuses on analyzing finite-time error bounds in total variation distance for Euler and Heun's methods applied to ODEs from stochastic interpolants. It also optimizes computational schedules.

Result: Novel finite-time error bounds are established, and numerical experiments validate the theoretical findings.

Conclusion: The work provides rigorous convergence guarantees and optimized schedules, enhancing the practical utility of stochastic interpolants in generative modeling.

Abstract: Stochastic interpolants offer a robust framework for continuously
transforming samples between arbitrary data distributions, holding significant
promise for generative modeling. Despite their potential, rigorous finite-time
convergence guarantees for practical numerical schemes remain largely
unexplored. In this work, we address the finite-time convergence analysis of
numerical implementations for ordinary differential equations (ODEs) derived
from stochastic interpolants. Specifically, we establish novel finite-time
error bounds in total variation distance for two widely used numerical
integrators: the first-order forward Euler method and the second-order Heun's
method. Furthermore, our analysis on the iteration complexity of specific
stochastic interpolant constructions provides optimized schedules to enhance
computational efficiency. Our theoretical findings are corroborated by
numerical experiments, which validate the derived error bounds and complexity
analyses.

</details>


### [139] [ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis](https://arxiv.org/abs/2508.07345)
*Samiha Afaf Neha,Abir Ahammed Bhuiyan,Md. Ishrak Khan*

Main category: cs.LG

TL;DR: ProteoKnight, an image-based encoding method, improves PVP classification accuracy and evaluates prediction uncertainty using Monte Carlo Dropout.


<details>
  <summary>Details</summary>
Motivation: Accurate PVP prediction is vital for genomic studies, but existing methods lack spatial efficiency. ProteoKnight addresses this gap.

Method: Adapts DNA-Walk for protein sequences, uses pre-trained CNNs for classification, and employs MCD for uncertainty analysis.

Result: Achieves 90.8% binary classification accuracy; multi-class accuracy is lower. Uncertainty varies by protein class and length.

Conclusion: ProteoKnight outperforms FCGR, offering robust PVP predictions and identifying low-confidence cases.

Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is
essential for genomic studies due to their crucial role as structural elements
in bacteriophages. Computational tools, particularly machine learning, have
emerged for annotating phage protein sequences from high-throughput sequencing.
However, effective annotation requires specialized sequence encodings. Our
paper introduces ProteoKnight, a new image-based encoding method that addresses
spatial constraints in existing techniques, yielding competitive performance in
PVP classification using pre-trained convolutional neural networks.
Additionally, our study evaluates prediction uncertainty in binary PVP
classification through Monte Carlo Dropout (MCD). \textbf{Methods:}
ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,
incorporating pixel colors and adjusting walk distances to capture intricate
protein features. Encoded sequences were classified using multiple pre-trained
CNNs. Variance and entropy measures assessed prediction uncertainty across
proteins of various classes and lengths. \textbf{Results:} Our experiments
achieved 90.8% accuracy in binary classification, comparable to
state-of-the-art methods. Multi-class classification accuracy remains
suboptimal. Our uncertainty analysis unveils variability in prediction
confidence influenced by protein class and sequence length.
\textbf{Conclusions:} Our study surpasses frequency chaos game representation
(FCGR) by introducing novel image encoding that mitigates spatial information
loss limitations. Our classification technique yields accurate and robust PVP
predictions while identifying low-confidence predictions.

</details>


### [140] [Intrinsic training dynamics of deep neural networks](https://arxiv.org/abs/2508.07370)
*Sibylle Marcotte,Gabriel Peyré,Rémi Gribonval*

Main category: cs.LG

TL;DR: The paper explores whether gradient-based training in high-dimensional spaces can be simplified into lower-dimensional structures (implicit bias), focusing on intrinsic gradient flows and their relation to conservation laws. It provides criteria for this property and applies it to ReLU and linear networks, generalizing balanced initializations.


<details>
  <summary>Details</summary>
Motivation: To understand if high-dimensional gradient flows can be reduced to simpler, lower-dimensional dynamics, aiding theoretical insights into deep learning training.

Method: The study introduces an intrinsic dynamic property, links it to conservation laws, and derives a criterion based on kernel inclusion. It applies this to ReLU and linear networks, including relaxed balanced initializations.

Result: For ReLU networks, the flow can be rewritten as intrinsic dynamics in lower dimensions. For linear networks, relaxed balanced initializations are shown to be necessary for dimensionality reduction.

Conclusion: The paper provides theoretical tools to analyze gradient flows in deep learning, demonstrating dimensionality reduction possibilities under specific conditions, with implications for understanding implicit bias.

Abstract: A fundamental challenge in the theory of deep learning is to understand
whether gradient-based training in high-dimensional parameter spaces can be
captured by simpler, lower-dimensional structures, leading to so-called
implicit bias. As a stepping stone, we study when a gradient flow on a
high-dimensional variable $\theta$ implies an intrinsic gradient flow on a
lower-dimensional variable $z = \phi(\theta)$, for an architecture-related
function $\phi$. We express a so-called intrinsic dynamic property and show how
it is related to the study of conservation laws associated with the
factorization $\phi$. This leads to a simple criterion based on the inclusion
of kernels of linear maps which yields a necessary condition for this property
to hold. We then apply our theory to general ReLU networks of arbitrary depth
and show that, for any initialization, it is possible to rewrite the flow as an
intrinsic dynamic in a lower dimension that depends only on $z$ and the
initialization, when $\phi$ is the so-called path-lifting. In the case of
linear networks with $\phi$ the product of weight matrices, so-called balanced
initializations are also known to enable such a dimensionality reduction; we
generalize this result to a broader class of {\em relaxed balanced}
initializations, showing that, in certain configurations, these are the
\emph{only} initializations that ensure the intrinsic dynamic property.
Finally, for the linear neural ODE associated with the limit of infinitely deep
linear networks, with relaxed balanced initialization, we explicitly express
the corresponding intrinsic dynamics.

</details>


### [141] [Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems](https://arxiv.org/abs/2508.07392)
*Nikita Puchkin,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: The paper explores generative modeling and unpaired image-to-image translation using Schrödinger bridges and stochastic optimal control, focusing on scenarios with only i.i.d. samples from initial and target distributions. It introduces a risk function and derives generalization bounds for empirical risk minimizers.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of transforming an initial density to a target one optimally, particularly when only i.i.d. samples are available, making it applicable to generative modeling and image translation.

Method: The approach uses an Ornstein-Uhlenbeck process as a reference, estimates the Schrödinger potential, and introduces a risk function based on Kullback-Leibler divergence between couplings. It analyzes generalization bounds for empirical risk minimizers in Gaussian mixture scenarios.

Result: The method achieves near-fast convergence rates, up to logarithmic factors, in favorable scenarios, leveraging the mixing properties of the Ornstein-Uhlenbeck process. Numerical experiments validate the approach.

Conclusion: The proposed framework effectively addresses generative modeling and image translation with theoretical guarantees and practical performance, demonstrating its potential in real-world applications.

Abstract: Modern methods of generative modelling and unpaired image-to-image
translation based on Schr\"odinger bridges and stochastic optimal control
theory aim to transform an initial density to a target one in an optimal way.
In the present paper, we assume that we only have access to i.i.d. samples from
initial and final distributions. This makes our setup suitable for both
generative modelling and unpaired image-to-image translation. Relying on the
stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as
the reference one and estimate the corresponding Schr\"odinger potential.
Introducing a risk function as the Kullback-Leibler divergence between
couplings, we derive tight bounds on generalization ability of an empirical
risk minimizer in a class of Schr\"odinger potentials including Gaussian
mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we
almost achieve fast rates of convergence up to some logarithmic factors in
favourable scenarios. We also illustrate performance of the suggested approach
with numerical experiments.

</details>


### [142] [Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs](https://arxiv.org/abs/2508.07395)
*Behnoush Khavari,Mehran Shakerinava,Jayesh Khullar,Jerry Huang,François Rivest,Siamak Ravanbakhsh,Sarath Chandar*

Main category: cs.LG

TL;DR: Combining input-independent and non-negative SSMs fails to solve parity tasks, requiring input-dependent transitions and negative eigenvalues for state-tracking.


<details>
  <summary>Details</summary>
Motivation: Address the lack of state-tracking capability in LRNN models like S4D, Mamba, and DeltaNet due to time-invariant or restricted transition matrices.

Method: Investigate multilayer SSMs with diagonal transition matrices to test if combining input-independent and non-negative SSMs can solve parity tasks.

Result: Combining these SSMs still fails to solve parity, indicating the need for input-dependent transitions and negative eigenvalues.

Conclusion: A recurrence layer must be input-dependent and include negative eigenvalues for effective state-tracking, as supported by experiments with S4D and Mamba layers.

Abstract: Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack
state-tracking capability due to either time-invariant transition matrices or
restricted eigenvalue ranges. To address this, input-dependent transition
matrices, particularly those that are complex or non-triangular, have been
proposed to enhance SSM performance on such tasks. While existing theorems
demonstrate that both input-independent and non-negative SSMs are incapable of
solving simple state-tracking tasks, such as parity, regardless of depth, they
do not explore whether combining these two types in a multilayer SSM could
help. We investigate this question for efficient SSMs with diagonal transition
matrices and show that such combinations still fail to solve parity. This
implies that a recurrence layer must both be input-dependent and include
negative eigenvalues. Our experiments support this conclusion by analyzing an
SSM model that combines S4D and Mamba layers.

</details>


### [143] [Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors](https://arxiv.org/abs/2508.07400)
*Mohamad Louai Shehab,Alperen Tercan,Necmiye Ozay*

Main category: cs.LG

TL;DR: The paper addresses the challenge of recovering time-varying reward functions from optimal policies or demonstrations in max entropy reinforcement learning, proposing efficient algorithms based on two priors: sparsity and linear feature combinations.


<details>
  <summary>Details</summary>
Motivation: Reward recovery is ill-posed without assumptions, but many applications have parsimonious rewards with prior information, motivating the use of structured priors.

Method: Two priors are considered: 1) rewards change infrequently (sparsity), solved via polynomial-time sparsification, and 2) rewards as linear combinations of few features, addressed via rank minimization with convex relaxations.

Result: Efficient algorithms are developed for both priors, demonstrating accurate reward recovery and generalizability in examples.

Conclusion: The proposed methods effectively solve reward identification under structured priors, offering practical solutions for real-world applications.

Abstract: In this paper, we consider the problem of recovering time-varying reward
functions from either optimal policies or demonstrations coming from a max
entropy reinforcement learning problem. This problem is highly ill-posed
without additional assumptions on the underlying rewards. However, in many
applications, the rewards are indeed parsimonious, and some prior information
is available. We consider two such priors on the rewards: 1) rewards are mostly
constant and they change infrequently, 2) rewards can be represented by a
linear combination of a small number of feature functions. We first show that
the reward identification problem with the former prior can be recast as a
sparsification problem subject to linear constraints. Moreover, we give a
polynomial-time algorithm that solves this sparsification problem exactly.
Then, we show that identifying rewards representable with the minimum number of
features can be recast as a rank minimization problem subject to linear
constraints, for which convex relaxations of rank can be invoked. In both
cases, these observations lead to efficient optimization-based reward
identification algorithms. Several examples are given to demonstrate the
accuracy of the recovered rewards as well as their generalizability.

</details>


### [144] [Lightning Prediction under Uncertainty: DeepLight with Hazy Loss](https://arxiv.org/abs/2508.07428)
*Md Sultanul Arifin,Abu Nowshed Sakib,Yeasir Rayhan,Tanzima Hashem*

Main category: cs.LG

TL;DR: DeepLight, a deep learning model, improves lightning prediction by leveraging multi-source data and addressing spatial uncertainty, outperforming existing methods by 18%-30% in ETS.


<details>
  <summary>Details</summary>
Motivation: Lightning risks are increasing due to climate change, and current prediction models are limited in capturing spatial dynamics and uncertainty, while relying heavily on costly NWP systems.

Method: DeepLight uses a dual-encoder architecture with multi-branch convolution to process radar reflectivity, cloud properties, and historical data, and introduces a Hazy Loss function to handle spatio-temporal uncertainty.

Result: DeepLight achieves an 18%-30% improvement in Equitable Threat Score (ETS) compared to state-of-the-art methods.

Conclusion: DeepLight is a robust solution for lightning prediction, addressing key limitations of existing models and improving accuracy.

Abstract: Lightning, a common feature of severe meteorological conditions, poses
significant risks, from direct human injuries to substantial economic losses.
These risks are further exacerbated by climate change. Early and accurate
prediction of lightning would enable preventive measures to safeguard people,
protect property, and minimize economic losses. In this paper, we present
DeepLight, a novel deep learning architecture for predicting lightning
occurrences. Existing prediction models face several critical limitations: they
often struggle to capture the dynamic spatial context and inherent uncertainty
of lightning events, underutilize key observational data, such as radar
reflectivity and cloud properties, and rely heavily on Numerical Weather
Prediction (NWP) systems, which are both computationally expensive and highly
sensitive to parameter settings. To overcome these challenges, DeepLight
leverages multi-source meteorological data, including radar reflectivity, cloud
properties, and historical lightning occurrences through a dual-encoder
architecture. By employing multi-branch convolution techniques, it dynamically
captures spatial correlations across varying extents. Furthermore, its novel
Hazy Loss function explicitly addresses the spatio-temporal uncertainty of
lightning by penalizing deviations based on proximity to true events, enabling
the model to better learn patterns amidst randomness. Extensive experiments
show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over
state-of-the-art methods, establishing it as a robust solution for lightning
prediction.

</details>


### [145] [Unsupervised operator learning approach for dissipative equations via Onsager principle](https://arxiv.org/abs/2508.07440)
*Zhipeng Chang,Zhenye Wen,Xiaofei Zhao*

Main category: cs.LG

TL;DR: DOOL is an unsupervised operator learning method for dissipative equations, avoiding costly labeled data by minimizing the Rayleighian functional via the Onsager variational principle.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on expensive supervised training with simulation data, limiting efficiency and scalability.

Method: DOOL uses the Onsager variational principle to train a deep operator network without labeled data, employing a spatiotemporal decoupling strategy for efficiency.

Result: Numerical experiments show DOOL outperforms supervised methods like DeepONet and MIONet, with extensions to second-order wave models.

Conclusion: DOOL offers a computationally efficient, unsupervised alternative for solving dissipative equations, validated by superior performance.

Abstract: Existing operator learning methods rely on supervised training with
high-fidelity simulation data, introducing significant computational cost. In
this work, we propose the deep Onsager operator learning (DOOL) method, a novel
unsupervised framework for solving dissipative equations. Rooted in the Onsager
variational principle (OVP), DOOL trains a deep operator network by directly
minimizing the OVP-defined Rayleighian functional, requiring no labeled data,
and then proceeds in time explicitly through conservation/change laws for the
solution. Another key innovation here lies in the spatiotemporal decoupling
strategy: the operator's trunk network processes spatial coordinates
exclusively, thereby enhancing training efficiency, while integrated external
time stepping enables temporal extrapolation. Numerical experiments on typical
dissipative equations validate the effectiveness of the DOOL method, and
systematic comparisons with supervised DeepONet and MIONet demonstrate its
enhanced performance. Extensions are made to cover the second-order wave models
with dissipation that do not directly follow OVP.

</details>


### [146] [Stackelberg Coupling of Online Representation Learning and Reinforcement Learning](https://arxiv.org/abs/2508.07452)
*Fernando Martinez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: SCORER framework improves RL performance by structuring perception-control interaction as a Stackelberg game, avoiding complex auxiliary objectives.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of learning effective features from sparse rewards without decoupling or naive end-to-end learning.

Method: Introduces SCORER, modeling perception-control interaction as a Stackelberg game with a two-timescale algorithm for equilibrium.

Result: Improves sample efficiency and final performance in benchmark tasks with DQN variants.

Conclusion: Principled algorithmic design of perception-control dynamics can enhance RL performance without added complexity.

Abstract: Integrated, end-to-end learning of representations and policies remains a
cornerstone of deep reinforcement learning (RL). However, to address the
challenge of learning effective features from a sparse reward signal, recent
trends have shifted towards adding complex auxiliary objectives or fully
decoupling the two processes, often at the cost of increased design complexity.
This work proposes an alternative to both decoupling and naive end-to-end
learning, arguing that performance can be significantly improved by structuring
the interaction between distinct perception and control networks with a
principled, game-theoretic dynamic. We formalize this dynamic by introducing
the Stackelberg Coupled Representation and Reinforcement Learning (SCORER)
framework, which models the interaction between perception and control as a
Stackelberg game. The perception network (leader) strategically learns features
to benefit the control network (follower), whose own objective is to minimize
its Bellman error. We approximate the game's equilibrium with a practical
two-timescale algorithm. Applied to standard DQN variants on benchmark tasks,
SCORER improves sample efficiency and final performance. Our results show that
performance gains can be achieved through principled algorithmic design of the
perception-control dynamic, without requiring complex auxiliary objectives or
architectures.

</details>


### [147] [Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten](https://arxiv.org/abs/2508.07458)
*Wei Qian,Chenxu Zhao,Yangyi Li,Wenqian Ye,Mengdi Huai*

Main category: cs.LG

TL;DR: The paper introduces a new class of malicious unlearning attacks targeting predictive uncertainties in deep learning models, showing their effectiveness over traditional label-focused attacks.


<details>
  <summary>Details</summary>
Motivation: The study addresses unexplored vulnerabilities in predictive uncertainties due to malicious unlearning attacks, given the rise of machine unlearning for data privacy.

Method: The authors propose novel optimization frameworks for malicious unlearning attacks and test them in black-box scenarios.

Result: Experiments reveal the attacks are more effective at manipulating uncertainties than traditional label attacks, and existing defenses fail against them.

Conclusion: The work highlights a critical gap in current defenses and calls for new solutions to protect predictive uncertainties from malicious unlearning.

Abstract: Currently, various uncertainty quantification methods have been proposed to
provide certainty and probability estimates for deep learning models' label
predictions. Meanwhile, with the growing demand for the right to be forgotten,
machine unlearning has been extensively studied as a means to remove the impact
of requested sensitive data from a pre-trained model without retraining the
model from scratch. However, the vulnerabilities of such generated predictive
uncertainties with regard to dedicated malicious unlearning attacks remain
unexplored. To bridge this gap, for the first time, we propose a new class of
malicious unlearning attacks against predictive uncertainties, where the
adversary aims to cause the desired manipulations of specific predictive
uncertainty results. We also design novel optimization frameworks for our
attacks and conduct extensive experiments, including black-box scenarios.
Notably, our extensive experiments show that our attacks are more effective in
manipulating predictive uncertainties than traditional attacks that focus on
label misclassifications, and existing defenses against conventional attacks
are ineffective against our attacks.

</details>


### [148] [MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification](https://arxiv.org/abs/2508.07465)
*Tiantian Yang,Zhiqian Chen*

Main category: cs.LG

TL;DR: MOTGNN is a novel framework for binary disease classification using multi-omics data, outperforming baselines by 5-10% in accuracy and offering interpretability.


<details>
  <summary>Details</summary>
Motivation: High dimensionality and complex interactions in multi-omics data challenge predictive modeling, necessitating an interpretable and accurate solution.

Method: MOTGNN uses XGBoost for omics-specific graph construction, modality-specific GNNs for representation learning, and a deep feedforward network for cross-omics integration.

Result: MOTGNN achieves superior performance (5-10% higher accuracy, ROC-AUC, and F1-score) and handles class imbalance well (87.2% vs. 33.4% F1).

Conclusion: MOTGNN enhances predictive accuracy and interpretability in multi-omics disease modeling, with efficient sparse graphs and biomarker insights.

Abstract: Integrating multi-omics data, such as DNA methylation, mRNA expression, and
microRNA (miRNA) expression, offers a comprehensive view of the biological
mechanisms underlying disease. However, the high dimensionality and complex
interactions among omics layers present major challenges for predictive
modeling. We propose Multi-Omics integration with Tree-generated Graph Neural
Network (MOTGNN), a novel and interpretable framework for binary disease
classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform
omics-specific supervised graph construction, followed by modality-specific
Graph Neural Networks (GNNs) for hierarchical representation learning, and a
deep feedforward network for cross-omics integration. On three real-world
disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in
accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance
(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains
computational efficiency through sparse graphs (2.1-2.8 edges per node) and
provides built-in interpretability, revealing both top-ranked biomarkers and
the relative contributions of each omics modality. These results highlight
MOTGNN's potential to improve both predictive accuracy and interpretability in
multi-omics disease modeling.

</details>


### [149] [Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications](https://arxiv.org/abs/2508.07473)
*Zijian Liu*

Main category: cs.LG

TL;DR: The paper analyzes Online Convex Optimization (OCO) under heavy-tailed gradient noise, showing classical algorithms like Online Gradient Descent achieve optimal regret bounds without modifications like gradient clipping.


<details>
  <summary>Details</summary>
Motivation: Existing OCO results assume finite variance gradients, but heavy-tailed gradients (finite p-th moment, p ∈ (1,2]) are understudied. The work aims to address this gap.

Method: The study revisits classical OCO algorithms (e.g., Online Gradient Descent) in heavy-tailed settings, analyzing their performance without algorithmic changes.

Result: Optimal regret bounds are proven for these algorithms in heavy-tailed scenarios, even without knowing p. Applications include nonsmooth nonconvex optimization under heavy-tailed noise.

Conclusion: Classical OCO algorithms are robust to heavy-tailed noise without extra steps, enabling broader applications like nonsmooth nonconvex optimization.

Abstract: In Online Convex Optimization (OCO), when the stochastic gradient has a
finite variance, many algorithms provably work and guarantee a sublinear
regret. However, limited results are known if the gradient estimate has a heavy
tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th
central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this
work examines different old algorithms for OCO (e.g., Online Gradient Descent)
in the more challenging heavy-tailed setting. Under the standard bounded domain
assumption, we establish new regrets for these classical methods without any
algorithmic modification. Remarkably, these regret bounds are fully optimal in
all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting
that OCO with heavy tails can be solved effectively without any extra operation
(e.g., gradient clipping). Our new results have several applications. A
particularly interesting one is the first provable convergence result for
nonsmooth nonconvex optimization under heavy-tailed noise without gradient
clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and
extend our ideas to optimistic algorithms to handle different cases
simultaneously.

</details>


### [150] [N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting](https://arxiv.org/abs/2508.07490)
*Ricardo Matos,Luis Roque,Vitor Cerqueira*

Main category: cs.LG

TL;DR: N-BEATS-MOE extends N-BEATS with a Mixture-of-Experts layer, improving adaptability and interpretability for time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Enhance N-BEATS by adapting to diverse time series characteristics and improving interpretability through expert relevance identification.

Method: Introduces a Mixture-of-Experts layer with dynamic block weighting via a gating network.

Result: Outperforms benchmarks on heterogeneous datasets, showing consistent improvements.

Conclusion: N-BEATS-MOE advances time series forecasting with better adaptability and interpretability.

Abstract: Deep learning approaches are increasingly relevant for time series
forecasting tasks. Methods such as N-BEATS, which is built on stacks of
multilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on
benchmark datasets and competitions. N-BEATS is also more interpretable
relative to other deep learning approaches, as it decomposes forecasts into
different time series components, such as trend and seasonality. In this work,
we present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts
(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a
gating network which allows the model to better adapt to the characteristics of
each time series. We also hypothesize that the gating mechanism provides
additional interpretability by identifying which expert is most relevant for
each series. We evaluate our method across 12 benchmark datasets against
several approaches, achieving consistent improvements on several datasets,
especially those composed of heterogeneous time series.

</details>


### [151] [Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach](https://arxiv.org/abs/2508.07505)
*Yueyang Quan,Chang Wang,Shengjie Zhai,Minghong Fang,Zhuqing Liu*

Main category: cs.LG

TL;DR: Proposes DPMixSGD, a privacy-preserving algorithm for decentralized min-max optimization, ensuring convergence and privacy despite added noise.


<details>
  <summary>Details</summary>
Motivation: Address privacy risks in decentralized min-max optimization due to sensitive data exposure, while maintaining convergence performance.

Method: Develops DPMixSGD, a differential privacy-based algorithm, building on STORM-based decentralized min-max solutions.

Result: Proves noise in gradients doesn't hinder convergence; provides privacy guarantees and validates with experiments.

Conclusion: DPMixSGD effectively balances privacy and performance in non-convex decentralized min-max optimization.

Abstract: Decentralized min-max optimization allows multi-agent systems to
collaboratively solve global min-max optimization problems by facilitating the
exchange of model updates among neighboring agents, eliminating the need for a
central server. However, sharing model updates in such systems carry a risk of
exposing sensitive data to inference attacks, raising significant privacy
concerns. To mitigate these privacy risks, differential privacy (DP) has become
a widely adopted technique for safeguarding individual data. Despite its
advantages, implementing DP in decentralized min-max optimization poses
challenges, as the added noise can hinder convergence, particularly in
non-convex scenarios with complex agent interactions in min-max optimization
problems. In this work, we propose an algorithm called DPMixSGD (Differential
Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving
algorithm specifically designed for non-convex decentralized min-max
optimization. Our method builds on the state-of-the-art STORM-based algorithm,
one of the fastest decentralized min-max solutions. We rigorously prove that
the noise added to local gradients does not significantly compromise
convergence performance, and we provide theoretical bounds to ensure privacy
guarantees. To validate our theoretical findings, we conduct extensive
experiments across various tasks and models, demonstrating the effectiveness of
our approach.

</details>


### [152] [FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction](https://arxiv.org/abs/2508.07518)
*Sichen Zhao,Wei Shao,Jeffrey Chan,Ziqi Xu,Flora Salim*

Main category: cs.LG

TL;DR: FairDRL-ST is a novel framework using disentangled representation learning to address fairness in spatio-temporal predictions, particularly in mobility demand forecasting, without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: Biased predictions in spatio-temporal applications can disproportionately disadvantage certain groups, reinforcing inequalities and undermining ethical AI deployment in public services.

Method: The framework leverages adversarial and disentangled representation learning to separate sensitive attributes, achieving fairness unsupervisedly.

Result: Applied to real-world urban mobility datasets, FairDRL-ST closes fairness gaps while maintaining competitive predictive performance.

Conclusion: FairDRL-ST effectively balances fairness and accuracy in spatio-temporal predictions, offering a practical solution for ethical AI deployment in urban computing.

Abstract: As deep spatio-temporal neural networks are increasingly utilised in urban
computing contexts, the deployment of such methods can have a direct impact on
users of critical urban infrastructure, such as public transport, emergency
services, and traffic management systems. While many spatio-temporal methods
focus on improving accuracy, fairness has recently gained attention due to
growing evidence that biased predictions in spatio-temporal applications can
disproportionately disadvantage certain demographic or geographic groups,
thereby reinforcing existing socioeconomic inequalities and undermining the
ethical deployment of AI in public services. In this paper, we propose a novel
framework, FairDRL-ST, based on disentangled representation learning, to
address fairness concerns in spatio-temporal prediction, with a particular
focus on mobility demand forecasting. By leveraging adversarial learning and
disentangled representation learning, our framework learns to separate
attributes that contain sensitive information. Unlike existing methods that
enforce fairness through supervised learning, which may lead to
overcompensation and degraded performance, our framework achieves fairness in
an unsupervised manner with minimal performance loss. We apply our framework to
real-world urban mobility datasets and demonstrate its ability to close
fairness gaps while delivering competitive predictive performance compared to
state-of-the-art fairness-aware methods.

</details>


### [153] [Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning](https://arxiv.org/abs/2508.07536)
*Tasfiq E. Alam,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.LG

TL;DR: A physics-informed multimodal CNN with late fusion integrates vibration and motor current signals, outperforming baselines in bearing fault classification under variable conditions. Transfer learning strategies further enhance generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing domain shifts in rotating machinery reliability by improving accuracy and interpretability of fault classification under variable operating conditions.

Method: Proposes a physics-informed multimodal CNN with late fusion, incorporating a physics-based loss function and evaluating transfer learning strategies (TSFT, LAS, HFR).

Result: Achieves higher accuracy (up to 98%), reduced false classifications, and improved robustness, with LAS performing best in generalization.

Conclusion: Integrating domain knowledge with data-driven learning enhances robustness and generalizability for real-world industrial fault diagnosis.

Abstract: Accurate and interpretable bearing fault classification is critical for
ensuring the reliability of rotating machinery, particularly under variable
operating conditions where domain shifts can significantly degrade model
performance. This study proposes a physics-informed multimodal convolutional
neural network (CNN) with a late fusion architecture, integrating vibration and
motor current signals alongside a dedicated physics-based feature extraction
branch. The model incorporates a novel physics-informed loss function that
penalizes physically implausible predictions based on characteristic bearing
fault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency
Inner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive
experiments on the Paderborn University dataset demonstrate that the proposed
physics-informed approach consistently outperforms a non-physics-informed
baseline, achieving higher accuracy, reduced false classifications, and
improved robustness across multiple data splits. To address performance
degradation under unseen operating conditions, three transfer learning (TL)
strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy
(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS
yields the best generalization, with additional performance gains when combined
with physics-informed modeling. Validation on the KAIST bearing dataset
confirms the framework's cross-dataset applicability, achieving up to 98
percent accuracy. Statistical hypothesis testing further verifies significant
improvements (p < 0.01) in classification performance. The proposed framework
demonstrates the potential of integrating domain knowledge with data-driven
learning to achieve robust, interpretable, and generalizable fault diagnosis
for real-world industrial applications.

</details>


### [154] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: A study on optimizing remote inference accuracy by scheduling two modalities dynamically to minimize ML model error, using an index-based threshold policy proven optimal for non-monotonic, non-additive AoI functions and heterogeneous transmission times.


<details>
  <summary>Details</summary>
Motivation: Fresh sensor features are critical for real-time inference, but limited network resources make timely delivery from all modalities infeasible.

Method: Develop an index-based threshold policy where the scheduler switches modalities when the current modality's index exceeds a shared threshold.

Result: The policy reduces inference error by up to 55% compared to round-robin and random policies.

Conclusion: The study demonstrates how optimizing task-oriented AoI functions can improve remote inference accuracy.

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [155] [Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning](https://arxiv.org/abs/2508.07556)
*Stephan Rabanser*

Main category: cs.LG

TL;DR: The paper explores uncertainty estimation in ML for safer, more reliable systems, focusing on selective prediction. It introduces lightweight methods, analyzes privacy-uncertainty trade-offs, decomposes error sources, and addresses adversarial manipulation.


<details>
  <summary>Details</summary>
Motivation: To enhance the safety and trustworthiness of ML systems in high-stakes domains by improving uncertainty estimation, particularly in selective prediction.

Method: Proposes a post-hoc abstention method using model training trajectories, studies privacy-uncertainty trade-offs, decomposes selective classification errors, and designs defenses against adversarial manipulation.

Result: Achieves state-of-the-art selective prediction performance, robust uncertainty under differential privacy, interpretable error analysis, and effective adversarial defenses.

Conclusion: Advances reliable ML by improving uncertainty estimation, enabling models to abstain when uncertain, and safeguarding against adversarial attacks.

Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes
domains where reliability is paramount. This thesis investigates how
uncertainty estimation can enhance the safety and trustworthiness of ML,
focusing on selective prediction -- where models abstain when confidence is
low.
  We first show that a model's training trajectory contains rich uncertainty
signals that can be exploited without altering its architecture or loss. By
ensembling predictions from intermediate checkpoints, we propose a lightweight,
post-hoc abstention method that works across tasks, avoids the cost of deep
ensembles, and achieves state-of-the-art selective prediction performance.
Crucially, this approach is fully compatible with differential privacy (DP),
allowing us to study how privacy noise affects uncertainty quality. We find
that while many methods degrade under DP, our trajectory-based approach remains
robust, and we introduce a framework for isolating the privacy-uncertainty
trade-off. Next, we then develop a finite-sample decomposition of the selective
classification gap -- the deviation from the oracle accuracy-coverage curve --
identifying five interpretable error sources and clarifying which interventions
can close the gap. This explains why calibration alone cannot fix ranking
errors, motivating methods that improve uncertainty ordering. Finally, we show
that uncertainty signals can be adversarially manipulated to hide errors or
deny service while maintaining high accuracy, and we design defenses combining
calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating,
and safeguarding uncertainty estimation, enabling models that not only make
accurate predictions -- but also know when to say "I do not know".

</details>


### [156] [Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression](https://arxiv.org/abs/2508.07571)
*Xingwu Chen,Miao Lu,Beining Wu,Difan Zou*

Main category: cs.LG

TL;DR: The paper explores how increased test-time computation, like generating more thoughts or sampling answers, improves language model performance. It bridges practical inference and theoretical analysis by studying randomness and sampling in in-context linear regression.


<details>
  <summary>Details</summary>
Motivation: To understand and improve language model inference by connecting practical techniques (e.g., sampling) with theoretical transformer analysis.

Method: Uses a framework simulating language model decoding via noise injection and binary coefficient sampling in in-context linear regression.

Result: Empirical results support the framework, showing potential insights into real-world language model inference behaviors.

Conclusion: The study provides a theoretical foundation for analyzing inference techniques, offering new perspectives on practical language model performance.

Abstract: Using more test-time computation during language model inference, such as
generating more intermediate thoughts or sampling multiple candidate answers,
has proven effective in significantly improving model performance. This paper
takes an initial step toward bridging the gap between practical language model
inference and theoretical transformer analysis by incorporating randomness and
sampling. We focus on in-context linear regression with continuous/binary
coefficients, where our framework simulates language model decoding through
noise injection and binary coefficient sampling. Through this framework, we
provide detailed analyses of widely adopted inference techniques. Supported by
empirical results, our theoretical framework and analysis demonstrate the
potential for offering new insights into understanding inference behaviors in
real-world language models.

</details>


### [157] [When and how can inexact generative models still sample from the data manifold?](https://arxiv.org/abs/2508.07581)
*Nisha Chandramoorthy,Adriaan de Clercq*

Main category: cs.LG

TL;DR: The paper investigates why some dynamical generative models remain robust in sample generation despite learning errors, focusing on the alignment of Lyapunov vectors with the data manifold's tangent spaces.


<details>
  <summary>Details</summary>
Motivation: To understand the phenomenon where generated samples stay on the data distribution's support despite errors in score functions or drift vector fields.

Method: Uses dynamical systems and perturbation analysis of probability flow, examining Lyapunov vectors' alignment with the data manifold's tangent spaces.

Result: Infinitesimal learning errors affect predicted density only on the data manifold, and alignment of Lyapunov vectors ensures robustness.

Conclusion: The alignment condition is efficient to compute and provides theoretical guarantees for robustness in various dynamical generative models.

Abstract: A curious phenomenon observed in some dynamical generative models is the
following: despite learning errors in the score function or the drift vector
field, the generated samples appear to shift \emph{along} the support of the
data distribution but not \emph{away} from it. In this work, we investigate
this phenomenon of \emph{robustness of the support} by taking a dynamical
systems approach on the generating stochastic/deterministic process. Our
perturbation analysis of the probability flow reveals that infinitesimal
learning errors cause the predicted density to be different from the target
density only on the data manifold for a wide class of generative models.
Further, what is the dynamical mechanism that leads to the robustness of the
support? We show that the alignment of the top Lyapunov vectors (most sensitive
infinitesimal perturbation directions) with the tangent spaces along the
boundary of the data manifold leads to robustness and prove a sufficient
condition on the dynamics of the generating process to achieve this alignment.
Moreover, the alignment condition is efficient to compute and, in practice, for
robust generative models, automatically leads to accurate estimates of the
tangent bundle of the data manifold. Using a finite-time linear perturbation
analysis on samples paths as well as probability flows, our work complements
and extends existing works on obtaining theoretical guarantees for generative
models from a stochastic analysis, statistical learning and uncertainty
quantification points of view. Our results apply across different dynamical
generative models, such as conditional flow-matching and score-based generative
models, and for different target distributions that may or may not satisfy the
manifold hypothesis.

</details>


### [158] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: Klear-Reasoner is a high-performance reasoning model with detailed training workflow insights, achieving top scores on benchmarks like AIME and LiveCodeBench.


<details>
  <summary>Details</summary>
Motivation: Addressing reproducibility issues in high-performance inference models due to incomplete training details.

Method: Uses long Chain-of-Thought supervised fine-tuning (long CoT SFT) and reinforcement learning (RL), with proposed Gradient-Preserving clipping Policy Optimization (GPPO).

Result: Scores 90.5% on AIME 2024, 83.2% on AIME 2025, 66.0% on LiveCodeBench V5, and 58.1% on LiveCodeBench V6.

Conclusion: High-quality data and GPPO enhance reasoning, making Klear-Reasoner a robust model for complex tasks.

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [159] [Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo](https://arxiv.org/abs/2508.07631)
*Advait Parulekar,Litu Rout,Karthikeyan Shanmugam,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: The paper addresses posterior sampling in score-based generative models, proposing a tractable method to sample from distributions close to the posterior under minimal assumptions.


<details>
  <summary>Details</summary>
Motivation: Despite the intractability of exact posterior sampling under computational hardness assumptions, empirical success in tasks like image super-resolution motivates exploring approximate solutions.

Method: The paper frames posterior sampling as a 'tilting' problem, biasing a distribution towards measurements, and shows tractable sampling under minimal assumptions.

Result: The method produces samples close to the noised prior's posterior in KL divergence and the true posterior in Fisher divergence, ensuring consistency with measurements and prior.

Conclusion: This work provides the first formal results for approximate posterior sampling in polynomial time, bridging theory and empirical success.

Abstract: We study the problem of posterior sampling in the context of score based
generative models. We have a trained score network for a prior $p(x)$, a
measurement model $p(y|x)$, and are tasked with sampling from the posterior
$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)
under well-accepted computational hardness assumptions. Despite this, popular
algorithms for tasks such as image super-resolution, stylization, and
reconstruction enjoy empirical success. Rather than establishing distributional
assumptions or restricted settings under which exact posterior sampling is
tractable, we view this as a more general "tilting" problem of biasing a
distribution towards a measurement. Under minimal assumptions, we show that one
can tractably sample from a distribution that is simultaneously close to the
posterior of a noised prior in KL divergence and the true posterior in Fisher
divergence. Intuitively, this combination ensures that the resulting sample is
consistent with both the measurement and the prior. To the best of our
knowledge these are the first formal results for (approximate) posterior
sampling in polynomial time.

</details>


### [160] [Attribution Explanations for Deep Neural Networks: A Theoretical Perspective](https://arxiv.org/abs/2508.07636)
*Huiqi Deng,Hongbin Pei,Quanshi Zhang,Mengnan Du*

Main category: cs.LG

TL;DR: The paper discusses challenges in evaluating the faithfulness of attribution methods for explaining DNNs and highlights recent theoretical advances to address these issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address unresolved concerns about whether attribution methods accurately reflect input contributions to DNN decisions, undermining their reliability.

Method: The paper reviews three key directions: theoretical unification, theoretical rationale, and theoretical evaluation of attribution methods.

Result: Recent theoretical advances provide a framework for systematic comparison, clarification of method foundations, and rigorous evaluation of faithfulness.

Conclusion: The paper concludes with insights into improving theoretical understanding, method selection, and future research directions for attribution methods.

Abstract: Attribution explanation is a typical approach for explaining deep neural
networks (DNNs), inferring an importance or contribution score for each input
variable to the final output. In recent years, numerous attribution methods
have been developed to explain DNNs. However, a persistent concern remains
unresolved, i.e., whether and which attribution methods faithfully reflect the
actual contribution of input variables to the decision-making process. The
faithfulness issue undermines the reliability and practical utility of
attribution explanations. We argue that these concerns stem from three core
challenges. First, difficulties arise in comparing attribution methods due to
their unstructured heterogeneity, differences in heuristics, formulations, and
implementations that lack a unified organization. Second, most methods lack
solid theoretical underpinnings, with their rationales remaining absent,
ambiguous, or unverified. Third, empirically evaluating faithfulness is
challenging without ground truth. Recent theoretical advances provide a
promising way to tackle these challenges, attracting increasing attention. We
summarize these developments, with emphasis on three key directions: (i)
Theoretical unification, which uncovers commonalities and differences among
methods, enabling systematic comparisons; (ii) Theoretical rationale,
clarifying the foundations of existing methods; (iii) Theoretical evaluation,
rigorously proving whether methods satisfy faithfulness principles. Beyond a
comprehensive review, we provide insights into how these studies help deepen
theoretical understanding, inform method selection, and inspire new attribution
methods. We conclude with a discussion of promising open problems for further
work.

</details>


### [161] [Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs](https://arxiv.org/abs/2508.07637)
*Guanqun Ma,David Lenz,Hanqi Guo,Tom Peterka,Bei Wang*

Main category: cs.LG

TL;DR: The paper introduces a framework for extracting complex topological features directly from continuous implicit models like MFA, avoiding discrete representations.


<details>
  <summary>Details</summary>
Motivation: To enable direct extraction of topological features (contours, Jacobi sets, ridge-valley graphs) from continuous implicit models, enhancing data analysis and visualization.

Method: Proposes a framework to extract features from multivariate functional approximation (MFA) models using function values and high-order derivatives, without discretization.

Result: Demonstrates successful extraction of topological features from MFA models, generalizable to other continuous implicit models.

Conclusion: Establishes foundational steps for topological data analysis and visualization on implicit continuous models.

Abstract: Implicit continuous models, such as functional models and implicit neural
networks, are an increasingly popular method for replacing discrete data
representations with continuous, high-order, and differentiable surrogates.
These models offer new perspectives on the storage, transfer, and analysis of
scientific data. In this paper, we introduce the first framework to directly
extract complex topological features -- contours, Jacobi sets, and ridge-valley
graphs -- from a type of continuous implicit model known as multivariate
functional approximation (MFA). MFA replaces discrete data with continuous
piecewise smooth functions. Given an MFA model as the input, our approach
enables direct extraction of complex topological features from the model,
without reverting to a discrete representation of the model. Our work is easily
generalizable to any continuous implicit model that supports the queries of
function values and high-order derivatives. Our work establishes the building
blocks for performing topological data analysis and visualization on implicit
continuous models.

</details>


### [162] [Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals](https://arxiv.org/abs/2508.07638)
*Jia Zhang,Yao Liu,Chen-Xi Zhang,Yi Liu,Yi-Xuan Jin,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: The paper introduces Direct Multi-Preference Optimization (DMPO) to address noise and conflicts in fine-grained preference data, proposing a data selection principle based on Preference Divergence (PD) for efficient training.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with diverse human values requires handling noisy and conflicting fine-grained preference data, which existing methods like DPO struggle with.

Method: Derives the DMPO objective, identifies PD to quantify conflicts, and formulates a data selection principle using high-consensus data (negative PD values).

Result: Achieves over 10% improvement on the UltraFeedback dataset, enhancing training efficiency and robustness without holistic preference annotations.

Conclusion: The PD selection method effectively leverages fine-grained preferences for robust LLM alignment, outperforming existing approaches.

Abstract: Aligning Large Language Models (LLMs) with diverse human values requires
moving beyond a single holistic "better-than" preference criterion. While
collecting fine-grained, aspect-specific preference data is more reliable and
scalable, existing methods like Direct Preference Optimization (DPO) struggle
with the severe noise and conflicts inherent in such aggregated datasets. In
this paper, we tackle this challenge from a data-centric perspective. We first
derive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a
key Preference Divergence (PD) term that quantifies inter-aspect preference
conflicts. Instead of using this term for direct optimization, we leverage it
to formulate a novel, theoretically-grounded data selection principle. Our
principle advocates for selecting a subset of high-consensus data-identified by
the most negative PD values-for efficient DPO training. We prove the optimality
of this strategy by analyzing the loss bounds of the DMPO objective in the
selection problem. To operationalize our approach, we introduce practical
methods of PD term estimation and length bias mitigation, thereby proposing our
PD selection method. Evaluation on the UltraFeedback dataset with three varying
conflict levels shows that our simple yet effective strategy achieves over 10%
relative improvement against both the standard holistic preference and a
stronger oracle using aggregated preference signals, all while boosting
training efficiency and obviating the need for intractable holistic preference
annotating, unlocking the potential of robust LLM alignment via fine-grained
preference signals.

</details>


### [163] [Multi-Turn Jailbreaks Are Simpler Than They Seem](https://arxiv.org/abs/2508.07646)
*Xiaoxue Yang,Jaeha Lee,Anna-Katharina Dick,Jasper Timm,Fei Xie,Diogo Cruz*

Main category: cs.LG

TL;DR: Multi-turn jailbreak attacks on LLMs are as effective as resampling single-turn attacks, with success rates over 70%. Attack success correlates among similar models, and higher reasoning effort increases attack success.


<details>
  <summary>Details</summary>
Motivation: To analyze the effectiveness of multi-turn jailbreak attacks on state-of-the-art LLMs and challenge the perceived sophistication of such attacks.

Method: Empirical analysis using the StrongREJECT benchmark on models like GPT-4, Claude, and Gemini variants, accounting for attacker learning from model refusals.

Result: Multi-turn attacks are no more sophisticated than resampling single-turn attacks, with success rates exceeding 70%. Attack success is correlated among similar models, and higher reasoning effort increases attack success.

Conclusion: The findings highlight vulnerabilities in AI safety evaluation and the need for better jailbreak-resistant system designs.

Abstract: While defenses against single-turn jailbreak attacks on Large Language Models
(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent
vulnerability, often achieving success rates exceeding 70% against models
optimized for single-turn protection. This work presents an empirical analysis
of automated multi-turn jailbreak attacks across state-of-the-art models
including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.
Our findings challenge the perceived sophistication of multi-turn attacks: when
accounting for the attacker's ability to learn from how models refuse harmful
requests, multi-turn jailbreaking approaches are approximately equivalent to
simply resampling single-turn attacks multiple times. Moreover, attack success
is correlated among similar models, making it easier to jailbreak newly
released ones. Additionally, for reasoning models, we find surprisingly that
higher reasoning effort often leads to higher attack success rates. Our results
have important implications for AI safety evaluation and the design of
jailbreak-resistant systems. We release the source code at
https://github.com/diogo-cruz/multi_turn_simpler

</details>


### [164] [Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning](https://arxiv.org/abs/2508.07659)
*Hyeon-Ju Jeon,Jeon-Ho Kang,In-Hyuk Kwon,O-Joun Lee*

Main category: cs.LG

TL;DR: The study improves atmospheric state forecasting by using STGNNs with adaptive edge sampling to handle dynamic spatial correlations between Earth observations and NWP grid points, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance forecasting accuracy by addressing the dynamic spatial correlations between Earth observations and atmospheric states, which conventional NWP systems struggle with.

Method: Employed spatiotemporal graph neural networks (STGNNs) with adaptive edge sampling to regulate structure learning, preventing information loss and over-smoothing.

Result: Outperformed existing STGNN models, even in high-variability regions, using real-world data from East Asia.

Conclusion: The proposed method effectively handles dynamic spatial correlations, improving forecasting accuracy beyond traditional and structure-learning STGNN approaches.

Abstract: This study aims to discover spatial correlations between Earth observations
and atmospheric states to improve the forecasting accuracy of global
atmospheric state estimation, which are usually conducted using conventional
numerical weather prediction (NWP) systems and is the beginning of weather
forecasting. NWP systems predict future atmospheric states at fixed locations,
which are called NWP grid points, by analyzing previous atmospheric states and
newly acquired Earth observations without fixed locations. Thus, surrounding
meteorological context and the changing locations of the observations make
spatial correlations between atmospheric states and observations over time. To
handle complicated spatial correlations, which change dynamically, we employ
spatiotemporal graph neural networks (STGNNs) with structure learning. However,
structure learning has an inherent limitation that this can cause structural
information loss and over-smoothing problem by generating excessive edges. To
solve this problem, we regulate edge sampling by adaptively determining node
degrees and considering the spatial distances between NWP grid points and
observations. We validated the effectiveness of the proposed method by using
real-world atmospheric state and observation data from East Asia. Even in areas
with high atmospheric variability, the proposed method outperformed existing
STGNN models with and without structure learning.

</details>


### [165] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: GLiClass is a novel method for sequence classification, combining efficiency and accuracy for zero-shot and few-shot learning, while addressing limitations of generative LLMs, cross-encoders, and embedding-based approaches.


<details>
  <summary>Details</summary>
Motivation: Modern AI systems need efficient, accurate, and adaptable classification methods, especially for dynamic requirements and large datasets, but existing approaches (generative LLMs, cross-encoders, embeddings) have significant drawbacks.

Method: GLiClass adapts the GLiNER architecture for sequence classification and uses proximal policy optimization (PPO) for multi-label classification in data-sparse conditions or human feedback scenarios.

Result: GLiClass achieves strong accuracy and efficiency, comparable to embedding-based methods, while maintaining flexibility for zero-shot and few-shot learning.

Conclusion: GLiClass offers a promising solution for classification tasks, balancing efficiency, accuracy, and adaptability, and addresses key limitations of existing methods.

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [166] [AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting](https://arxiv.org/abs/2508.07668)
*Hyobin Park,Jinwook Jung,Minseok Seo,Hyunsoo Choi,Deukjae Cho,Sekil Park,Dong-Geol Choi*

Main category: cs.LG

TL;DR: AIS-LLM is a novel framework integrating AIS data with a large language model (LLM) to simultaneously perform trajectory prediction, anomaly detection, and collision risk assessment, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches address maritime traffic tasks individually, lacking holistic analysis. AIS-LLM aims to integrate these tasks for better maritime management.

Method: AIS-LLM combines a Time-Series Encoder, LLM-based Prompt Encoder, Cross-Modality Alignment Module, and Multi-Task Decoder to process AIS data and textual prompts.

Result: AIS-LLM outperforms existing methods in trajectory prediction, anomaly detection, and risk assessment, enabling integrative task analysis.

Conclusion: AIS-LLM offers intelligent and efficient maritime traffic management by unifying multiple tasks and generating situation summaries.

Abstract: With the increase in maritime traffic and the mandatory implementation of the
Automatic Identification System (AIS), the importance and diversity of maritime
traffic analysis tasks based on AIS data, such as vessel trajectory prediction,
anomaly detection, and collision risk assessment, is rapidly growing. However,
existing approaches tend to address these tasks individually, making it
difficult to holistically consider complex maritime situations. To address this
limitation, we propose a novel framework, AIS-LLM, which integrates time-series
AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series
Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a
Cross-Modality Alignment Module for semantic alignment between time-series data
and textual prompts, and an LLM-based Multi-Task Decoder. This architecture
enables the simultaneous execution of three key tasks: trajectory prediction,
anomaly detection, and risk assessment of vessel collisions within a single
end-to-end system. Experimental results demonstrate that AIS-LLM outperforms
existing methods across individual tasks, validating its effectiveness.
Furthermore, by integratively analyzing task outputs to generate situation
summaries and briefings, AIS-LLM presents the potential for more intelligent
and efficient maritime traffic management.

</details>


### [167] [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/abs/2508.07675)
*Xutong Liu,Baran Atalar,Xiangxiang Dai,Jinhang Zuo,Siwei Wang,John C. S. Lui,Wei Chen,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: A learning-based framework for semantic cache eviction in LLMs addresses scalability challenges by optimizing cache usage under unknown query and cost distributions.


<details>
  <summary>Details</summary>
Motivation: High inference costs of LLMs and inefficiencies of traditional caching methods necessitate a principled approach to semantic caching.

Method: Develops offline optimization and online learning algorithms for semantic cache eviction, accounting for unknown query and cost distributions.

Result: Proposed algorithms outperform baselines on synthetic datasets, demonstrating efficiency and adaptability.

Conclusion: The framework provides a scalable, theoretically grounded solution for semantic caching in LLMs.

Abstract: Large Language Models (LLMs) are revolutionizing how users interact with
information systems, yet their high inference cost poses serious scalability
and sustainability challenges. Caching inference responses, allowing them to be
retrieved without another forward pass through the LLM, has emerged as one
possible solution. Traditional exact-match caching, however, overlooks the
semantic similarity between queries, leading to unnecessary recomputation.
Semantic caching addresses this by retrieving responses based on semantic
similarity, but introduces a fundamentally different cache eviction problem:
one must account for mismatch costs between incoming queries and cached
responses. Moreover, key system parameters, such as query arrival probabilities
and serving costs, are often unknown and must be learned over time. Existing
semantic caching methods are largely ad-hoc, lacking theoretical foundations
and unable to adapt to real-world uncertainty. In this paper, we present a
principled, learning-based framework for semantic cache eviction under unknown
query and cost distributions. We formulate both offline optimization and online
learning variants of the problem, and develop provably efficient algorithms
with state-of-the-art guarantees. We also evaluate our framework on a synthetic
dataset, showing that our proposed algorithms perform matching or superior
performance compared with baselines.

</details>


### [168] [Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks](https://arxiv.org/abs/2508.07676)
*Chenchen Lin,Xuehe Wang*

Main category: cs.LG

TL;DR: A socially-aware FL mechanism quantifies indirect privacy leakage via multi-hop propagation, using a Stackelberg game for incentive optimization and mean-field estimation for privacy risk. It improves client utilities and server costs while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Privacy loss in FL depends on multi-hop social interactions, requiring a mechanism to quantify and mitigate indirect leakage while incentivizing participation.

Method: Proposes a two-stage Stackelberg game for server-client interaction, mean-field estimator for privacy risk, and derives Nash Equilibrium.

Result: Outperforms baselines in client utilities, server costs, and model performance, achieving near-optimal social welfare.

Conclusion: The mechanism effectively balances privacy, incentives, and performance in socially-connected FL.

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, thereby enhancing privacy and
facilitating collaboration among clients connected via social networks.
However, these social connections introduce privacy externalities: a client's
privacy loss depends not only on its privacy protection strategy but also on
the privacy decisions of others, propagated through the network via multi-hop
interactions. In this work, we propose a socially-aware privacy-preserving FL
mechanism that systematically quantifies indirect privacy leakage through a
multi-hop propagation model. We formulate the server-client interaction as a
two-stage Stackelberg game, where the server, as the leader, optimizes
incentive policies, and clients, as followers, strategically select their
privacy budgets, which determine their privacy-preserving levels by controlling
the magnitude of added noise. To mitigate information asymmetry in networked
privacy estimation, we introduce a mean-field estimator to approximate the
average external privacy risk. We theoretically prove the existence and
convergence of the fixed point of the mean-field estimator and derive
closed-form expressions for the Stackelberg Nash Equilibrium. Despite being
designed from a client-centric incentive perspective, our mechanism achieves
approximately-optimal social welfare, as revealed by Price of Anarchy (PoA)
analysis. Experiments on diverse datasets demonstrate that our approach
significantly improves client utilities and reduces server costs while
maintaining model performance, outperforming both Social-Agnostic (SA)
baselines and methods that account for social externalities.

</details>


### [169] [MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation](https://arxiv.org/abs/2508.07681)
*Yooseok Lim,ByoungJun Jeon,Seong-A Park,Jisoo Lee,Sae Won Choi,Chang Wook Jeong,Ho-Geol Ryu,Hongyeol Lee,Hyun-Lim Yang*

Main category: cs.LG

TL;DR: MORE-CLEAR uses multimodal offline RL with LLMs to improve sepsis management by integrating clinical notes and structured data, outperforming single-modal approaches.


<details>
  <summary>Details</summary>
Motivation: Early sepsis detection and optimal management are critical, but existing RL methods lack comprehensive patient understanding due to reliance on structured data alone.

Method: MORE-CLEAR leverages LLMs for semantic extraction from clinical notes, uses gated fusion and cross-modal attention for dynamic data integration, and validates on MIMIC-III, MIMIC-IV, and a private dataset.

Result: MORE-CLEAR significantly improves survival rates and policy performance compared to single-modal RL methods.

Conclusion: This is the first multimodal offline RL approach using LLMs for sepsis management, offering a more comprehensive patient understanding and potential for better treatment outcomes.

Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ
dysfunction, making early detection and optimal management critical. Previous
reinforcement learning (RL) approaches to sepsis management rely primarily on
structured data, such as lab results or vital signs, and on a dearth of a
comprehensive understanding of the patient's condition. In this work, we
propose a Multimodal Offline REinforcement learning for Clinical notes
Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis
control in intensive care units. MORE-CLEAR employs pre-trained large-scale
language models (LLMs) to facilitate the extraction of rich semantic
representations from clinical notes, preserving clinical context and improving
patient state representation. Gated fusion and cross-modal attention allow
dynamic weight adjustment in the context of time and the effective integration
of multimodal data. Extensive cross-validation using two public (MIMIC-III and
MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly
improves estimated survival rate and policy performance compared to
single-modal RL approaches. To our knowledge, this is the first to leverage LLM
capabilities within a multimodal offline RL for better state representation in
medical applications. This approach can potentially expedite the treatment and
management of sepsis by enabling reinforcement learning models to propose
enhanced actions based on a more comprehensive understanding of patient
conditions.

</details>


### [170] [Semantic-Enhanced Time-Series Forecasting via Large Language Models](https://arxiv.org/abs/2508.07697)
*Hao Liu,Chun Yang,Zhang xiaoxing,Xiaobin Zhu*

Main category: cs.LG

TL;DR: The paper proposes a Semantic-Enhanced LLM (SE-LLM) to bridge the modality gap between linguistic knowledge and time series data, improving forecasting performance by embedding periodicity and anomalies into semantic space.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based time series forecasting methods focus on token-level alignment, limiting semantic representation due to the intrinsic modality gap between language and time series patterns.

Method: SE-LLM enhances token embedding by leveraging time series periodicity and anomalies, and introduces a plugin module in self-attention to model both long-term and short-term dependencies. The LLM is frozen to reduce computational costs.

Result: SE-LLM outperforms state-of-the-art methods in experiments.

Conclusion: The proposed SE-LLM effectively adapts LLMs to time series forecasting by addressing modality gaps and dependency modeling, achieving superior performance.

Abstract: Time series forecasting plays a significant role in finance, energy,
meteorology, and IoT applications. Recent studies have leveraged the
generalization capabilities of large language models (LLMs) to adapt to time
series forecasting, achieving promising performance. However, existing studies
focus on token-level modal alignment, instead of bridging the intrinsic
modality gap between linguistic knowledge structures and time series data
patterns, greatly limiting the semantic representation. To address this issue,
we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent
periodicity and anomalous characteristics of time series to embed into the
semantic space to enhance the token embedding. This process enhances the
interpretability of tokens for LLMs, thereby activating the potential of LLMs
for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel
at capturing long-range dependencies but are weak at modeling short-term
anomalies in time-series data. Hence, we propose a plugin module embedded
within self-attention that models long-term and short-term dependencies to
effectively adapt LLMs to time-series analysis. Our approach freezes the LLM
and reduces the sequence dimensionality of tokens, greatly reducing
computational consumption. Experiments demonstrate the superiority performance
of our SE-LLM against the state-of-the-art (SOTA) methods.

</details>


### [171] [Energy Consumption in Parallel Neural Network Training](https://arxiv.org/abs/2508.07706)
*Philipp Huber,David Li,Juan Pedro Gutiérrez Hermosillo Muriedas,Deifilia Kieckhefen,Markus Götz,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: The paper examines how parallelization in neural network training affects energy consumption, showing linear scaling with GPU hours but varying factors based on model and hardware.


<details>
  <summary>Details</summary>
Motivation: Address the overlooked impact of parallelization on energy consumption in neural network training, given the rising demand for computational resources.

Method: Conducted scaling experiments with ResNet50 and FourCastNet, varying GPU count, global batch size, and local batch size to evaluate performance, training time, and energy use.

Result: Energy consumption scales linearly with GPU hours, but scaling factors vary by model and hardware, influenced by samples and gradient updates per GPU hour.

Conclusion: The study highlights the complexity of scaling neural network training and provides insights for sustainable AI research.

Abstract: The increasing demand for computational resources of training neural networks
leads to a concerning growth in energy consumption. While parallelization has
enabled upscaling model and dataset sizes and accelerated training, its impact
on energy consumption is often overlooked. To close this research gap, we
conducted scaling experiments for data-parallel training of two models,
ResNet50 and FourCastNet, and evaluated the impact of parallelization
parameters, i.e., GPU count, global batch size, and local batch size, on
predictive performance, training time, and energy consumption. We show that
energy consumption scales approximately linearly with the consumed resources,
i.e., GPU hours; however, the respective scaling factor differs substantially
between distinct model trainings and hardware, and is systematically influenced
by the number of samples and gradient updates per GPU hour. Our results shed
light on the complex interplay of scaling up neural network training and can
inform future developments towards more sustainable AI research.

</details>


### [172] [Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer](https://arxiv.org/abs/2508.07710)
*Jingya Wang,Xin Deng,Wenjie Wei,Dehao Zhang,Shuai Wang,Qian Sun,Jieyuan Zhang,Hanwen Liu,Ning Xie,Malu Zhang*

Main category: cs.LG

TL;DR: Proposes a training-free ANN-to-SNN conversion framework for Transformers using Multi-basis Exponential Decay (MBE) neurons, achieving near-lossless accuracy with low latency.


<details>
  <summary>Details</summary>
Motivation: Existing ANN-to-SNN conversion methods for Transformers struggle with nonlinear operations and require fine-tuning, limiting efficiency.

Method: Introduces MBE neurons with exponential decay and multi-basis encoding to approximate nonlinear operations without modifying pre-trained ANN weights.

Result: Achieves near-lossless conversion accuracy across tasks (CV, NLU, NLG) and architectures (ViT, RoBERTa, GPT-2) with low latency.

Conclusion: The framework enables efficient, scalable deployment of Spiking Transformers in real-world applications.

Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a
promising approach for constructing energy-efficient Transformer architectures.
Compared to directly trained Spiking Transformers, ANN-to-SNN conversion
methods bypass the high training costs. However, existing methods still suffer
from notable limitations, failing to effectively handle nonlinear operations in
Transformer architectures and requiring additional fine-tuning processes for
pre-trained ANNs. To address these issues, we propose a high-performance and
training-free ANN-to-SNN conversion framework tailored for Transformer
architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)
neuron, which employs an exponential decay strategy and multi-basis encoding
method to efficiently approximate various nonlinear operations. It removes the
requirement for weight modifications in pre-trained ANNs. Extensive experiments
across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures
(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless
conversion accuracy with significantly lower latency. This provides a promising
pathway for the efficient and scalable deployment of Spiking Transformers in
real-world applications.

</details>


### [173] [Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information](https://arxiv.org/abs/2508.07713)
*Jinghan Yang,Jiayu Weng*

Main category: cs.LG

TL;DR: A mutual information-based framework filters noisy or mislabeled data by quantifying input-label dependencies, improving model accuracy by up to 15%.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks can memorize corrupted labels, but real-world datasets often contain both label and input noise, necessitating robust data selection methods.

Method: The framework computes each sample's pointwise contribution to mutual information, identifying low-quality samples (low contributions).

Result: On MNIST with synthetic noise, the method effectively filters noise, improving classification accuracy by up to 15% under label corruption.

Conclusion: The mutual information-based approach robustly filters corrupted data while preserving valid samples, enhancing model performance.

Abstract: Deep neural networks can memorize corrupted labels, making data quality
critical for model performance, yet real-world datasets are frequently
compromised by both label noise and input noise. This paper proposes a mutual
information-based framework for data selection under hybrid noise scenarios
that quantifies statistical dependencies between inputs and labels. We compute
each sample's pointwise contribution to the overall mutual information and find
that lower contributions indicate noisy or mislabeled instances. Empirical
validation on MNIST with different synthetic noise settings demonstrates that
the method effectively filters low-quality samples. Under label corruption,
training on high-MI samples improves classification accuracy by up to 15\%
compared to random sampling. Furthermore, the method exhibits robustness to
benign input modifications, preserving semantically valid data while filtering
truly corrupted samples.

</details>


### [174] [Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning](https://arxiv.org/abs/2508.07738)
*Jialu Zhou,Dianxi Shi,Shaowu Yang,Xinyu Wei,Mingyue Yang,Leqian Li,Mengzhu Wang,Chunping Qiu*

Main category: cs.LG

TL;DR: The paper proposes TRGE, a Two-Level Routing Grouped Mixture-of-Experts method, to address catastrophic and forward forgetting in Multi-Domain Continual Learning (MDCL) by dynamically expanding CLIP and using MLLMs for task identification.


<details>
  <summary>Details</summary>
Motivation: MDCL faces challenges like catastrophic forgetting and forward forgetting due to shifting class sets and distributions. PEFT methods struggle with these issues, necessitating a more robust solution.

Method: TRGE dynamically expands CLIP, assigns expert groups per task, uses intra-group routing to avoid overfitting, and employs inter-group routing for task collaboration. MLLMs generate task identifiers, and outputs are fused for unseen samples.

Result: TRGE outperforms advanced methods with fewer trainable parameters in extensive experiments.

Conclusion: TRGE effectively mitigates forgetting in MDCL, enhancing performance and collaboration across tasks.

Abstract: Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential
tasks with shifting class sets and distribution. Despite the
Parameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual
heterogeneity, they still suffer from catastrophic forgetting and forward
forgetting. To address these challenges, we propose a Two-Level Routing Grouped
Mixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the
pre-trained CLIP model, assigning specific expert group for each task to
mitigate catastrophic forgetting. With the number of experts continually grows
in this process, TRGE maintains the static experts count within the group and
introduces the intra-group router to alleviate routing overfitting caused by
the increasing routing complexity. Meanwhile, we design an inter-group routing
policy based on task identifiers and task prototype distance, which dynamically
selects relevant expert groups and combines their outputs to enhance inter-task
collaboration. Secondly, to get the correct task identifiers, we leverage
Multimodal Large Language Models (MLLMs) which own powerful multimodal
comprehension capabilities to generate semantic task descriptions and recognize
the correct task identifier. Finally, to mitigate forward forgetting, we
dynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE
adapter based on training progress, leveraging both pre-trained and learned
knowledge. Through extensive experiments across various settings, our method
outperforms other advanced methods with fewer trainable parameters.

</details>


### [175] [A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory](https://arxiv.org/abs/2508.07746)
*Fengdi Che*

Main category: cs.LG

TL;DR: The paper surveys theoretical insights in offline RL, focusing on conditions like function representation and data coverage, challenges, and practical implications for algorithm design.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical advances and practical algorithm design in offline RL, highlighting inherent challenges and limitations.

Method: Analyzes theoretical conditions (function representation, data coverage), counterexamples, and techniques to mitigate challenges.

Result: Identifies conditions sufficient for offline RL and underscores limitations, guiding future algorithm development.

Conclusion: Theoretical insights reveal offline RL's inherent hardness and the need for novel solutions when conditions are unmet.

Abstract: Offline reinforcement learning (RL) aims to optimize the return given a fixed
dataset of agent trajectories without additional interactions with the
environment. While algorithm development has progressed rapidly, significant
theoretical advances have also been made in understanding the fundamental
challenges of offline RL. However, bridging these theoretical insights with
practical algorithm design remains an ongoing challenge. In this survey, we
explore key intuitions derived from theoretical work and their implications for
offline RL algorithms.
  We begin by listing the conditions needed for the proofs, including function
representation and data coverage assumptions. Function representation
conditions tell us what to expect for generalization, and data coverage
assumptions describe the quality requirement of the data. We then examine
counterexamples, where offline RL is not solvable without an impractically
large amount of data. These cases highlight what cannot be achieved for all
algorithms and the inherent hardness of offline RL. Building on techniques to
mitigate these challenges, we discuss the conditions that are sufficient for
offline RL. These conditions are not merely assumptions for theoretical proofs,
but they also reveal the limitations of these algorithms and remind us to
search for novel solutions when the conditions cannot be satisfied.

</details>


### [176] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: GRAO (Group Relative Alignment Optimization) combines SFT and RL strengths for better language model alignment, outperforming baselines like SFT, DPO, PPO, and GRPO.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of SFT (constrained by offline policy) and RL (low sample efficiency and dependency on high-quality models) in language model alignment.

Method: GRAO integrates SFT and RL with multi-sample generation, Group Direct Alignment Loss, and reference-aware parameter updates.

Result: Achieves 57.70%, 17.65%, 7.95%, and 5.18% improvements over SFT, DPO, PPO, and GRPO baselines, respectively.

Conclusion: GRAO offers a theoretically grounded and empirically validated framework for efficient language model alignment.

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [177] [Sparse Probabilistic Graph Circuits](https://arxiv.org/abs/2508.07763)
*Martin Rektoris,Milan Papež,Václav Šmídl,Tomáš Pevný*

Main category: cs.LG

TL;DR: Sparse Probabilistic Graph Circuits (SPGCs) are introduced to address scalability issues in tractable generative models for graphs, reducing complexity from O(n²) to O(n + m) while maintaining exact inference and performance.


<details>
  <summary>Details</summary>
Motivation: Deep generative models (DGMs) for graphs are intractable due to non-linearities, and existing Probabilistic Graph Circuits (PGCs) are inefficient for sparse graphs. SPGCs aim to solve this scalability problem.

Method: SPGCs operate directly on sparse graph representations, reducing complexity to O(n + m) and enabling exact probabilistic inference.

Result: SPGCs improve memory efficiency and inference speed while matching the performance of intractable DGMs in key metrics, demonstrated in de novo drug design.

Conclusion: SPGCs offer a scalable and efficient solution for tractable probabilistic inference in graph generative models, particularly beneficial for sparse graphs.

Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive
power thanks to very efficient and scalable neural networks. However, these
networks contain non-linearities that prevent analytical computation of many
standard probabilistic inference queries, i.e., these DGMs are considered
\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)
address this issue by enabling \emph{tractable} probabilistic inference, they
operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for
graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue,
we introduce Sparse PGCs, a new class of tractable generative models that
operate directly on sparse graph representation, reducing the complexity to
$\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the
context of de novo drug design, we empirically demonstrate that SPGCs retain
exact inference capabilities, improve memory efficiency and inference speed,
and match the performance of intractable DGMs in key metrics.

</details>


### [178] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: PAMA is a scalable algorithm for multi-objective alignment in LLMs, reducing complexity from O(n^2*d) to O(n) and enabling efficient optimization.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods like RLHF optimize for a single reward, failing to capture diverse human preferences, limiting LLM adaptability.

Method: PAMA transforms multi-objective RLHF into convex optimization with a closed-form solution, ensuring scalability and efficiency.

Result: PAMA achieves robust alignment across models (125M to 7B parameters), converging to Pareto stationary points.

Conclusion: PAMA offers a practical, efficient solution for MOA, enabling adaptable LLM deployments aligned with diverse human values.

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [179] [Topological Feature Compression for Molecular Graph Neural Networks](https://arxiv.org/abs/2508.07807)
*Rahul Khorana*

Main category: cs.LG

TL;DR: A novel Graph Neural Network (GNN) architecture combines higher-order topological signals with standard molecular features, achieving superior performance in accuracy and robustness across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Balancing predictive accuracy, interpretability, and computational efficiency in molecular representation learning remains a challenge.

Method: Introduces a GNN architecture integrating compressed higher-order topological signals with standard molecular features to capture global geometric information.

Result: Demonstrates best performance in accuracy and robustness across small-molecule and complex material datasets.

Conclusion: The proposed GNN architecture effectively addresses the challenge of extracting chemical insight while maintaining interpretability and efficiency.

Abstract: Recent advances in molecular representation learning have produced highly
effective encodings of molecules for numerous cheminformatics and
bioinformatics tasks. However, extracting general chemical insight while
balancing predictive accuracy, interpretability, and computational efficiency
remains a major challenge. In this work, we introduce a novel Graph Neural
Network (GNN) architecture that combines compressed higher-order topological
signals with standard molecular features. Our approach captures global
geometric information while preserving computational tractability and
human-interpretable structure. We evaluate our model across a range of
benchmarks, from small-molecule datasets to complex material datasets, and
demonstrate superior performance using a parameter-efficient architecture. We
achieve the best performing results in both accuracy and robustness across
almost all benchmarks. We open source all code \footnote{All code and results
can be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.

</details>


### [180] [EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning](https://arxiv.org/abs/2508.07809)
*Huanyu Liu,Jia Li,Chang Yu,Taozhi Chen,Yihong Dong,Lecheng Wang,Hu XiaoLong,Ge Li*

Main category: cs.LG

TL;DR: EvoCoT is a self-evolving curriculum learning framework for improving LLM reasoning by optimizing two-stage CoT reasoning, enabling stable learning from hard problems under sparse rewards.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of sparse rewards and exploration bottlenecks in RLVR for LLMs, which limits learning efficiency and reasoning improvement.

Method: EvoCoT uses self-generated and verified CoT trajectories to constrain and gradually expand the exploration space, allowing LLMs to learn from initially unsolved hard problems.

Result: EvoCoT improves reasoning capability without external CoT supervision, solves previously unsolved problems, and is compatible with various RL fine-tuning methods.

Conclusion: EvoCoT offers a scalable and effective solution for enhancing LLM reasoning through controlled exploration and self-evolving curriculum learning.

Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (LLMs) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes sparse, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger LLMs for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
LLMs to stably learn from initially unsolved hard problems under sparse
rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables LLMs to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.

</details>


### [181] [Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow](https://arxiv.org/abs/2508.07841)
*Carlo Cena,Mauro Martini,Marcello Chiaberge*

Main category: cs.LG

TL;DR: The paper explores using Physics-Informed Neural Networks (PINNs) for spacecraft attitude control, showing they outperform purely data-driven methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional MPC relies on accurate physics models, which can be incomplete or expensive. Machine learning offers flexibility but struggles with generalization and stability.

Method: PINNs are compared with data-driven approaches using Real NVP neural networks with self-attention, trained on simulated data from Basilisk.

Result: PINN-based models reduce mean relative error by 27.08% and improve control accuracy and robustness by up to 42.86%.

Conclusion: Incorporating physics into neural networks enhances performance and stability in spacecraft attitude control, making PINNs a superior choice over purely data-driven methods.

Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model
Predictive Control (MPC) has emerged as a powerful strategy for these tasks,
relying on accurate models of the system dynamics to optimize control actions
over a prediction horizon. In scenarios where physics models are incomplete,
difficult to derive, or computationally expensive, machine learning offers a
flexible alternative by learning the system behavior directly from data.
However, purely data-driven models often struggle with generalization and
stability, especially when applied to inputs outside their training domain. To
address these limitations, we investigate the benefits of incorporating
Physics-Informed Neural Networks (PINNs) into the learning of spacecraft
attitude dynamics, comparing their performance with that of purely data-driven
approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network
architecture with a self-attention mechanism, we trained several models on
simulated data generated with the Basilisk simulator. Two training strategies
were considered: a purely data-driven baseline and a physics-informed variant
to improve robustness and stability. Our results demonstrate that the inclusion
of physics-based information significantly enhances the performance in terms of
the mean relative error of the best architectures found by 27.08%. These
advantages are particularly evident when the learned models are integrated into
an MPC framework, where PINN-based models consistently outperform their purely
data-driven counterparts in terms of control accuracy and robustness, yielding
improvements of up to 42.86% in performance stability error and increased
robustness-to-noise.

</details>


### [182] [Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant](https://arxiv.org/abs/2508.07887)
*Sabrina Namazova,Alessandra Brondetta,Younes Strittmatter,Matthew Nassar,Sebastian Musslick*

Main category: cs.LG

TL;DR: Centaur, an LLM fine-tuned on human data, shows promise as a participant simulator but falls short in generating human-like behavior, limiting its reliability for cognitive science prototyping.


<details>
  <summary>Details</summary>
Motivation: To evaluate Centaur's potential as a participant simulator for cognitive science, given its predictive accuracy and generative capabilities.

Method: Review core criteria for a participant simulator and assess Centaur's performance against human data.

Result: Centaur has strong predictive accuracy but its generative behavior systematically diverges from human data.

Conclusion: Centaur is a step toward predicting human behavior but isn't yet a reliable participant simulator or accurate cognitive model.

Abstract: Simulators have revolutionized scientific practice across the natural
sciences. By generating data that reliably approximate real-world phenomena,
they enable scientists to accelerate hypothesis testing and optimize
experimental designs. This is perhaps best illustrated by AlphaFold, a
Nobel-prize winning simulator in chemistry that predicts protein structures
from amino acid sequences, enabling rapid prototyping of molecular
interactions, drug targets, and protein functions. In the behavioral sciences,
a reliable participant simulator - a system capable of producing human-like
behavior across cognitive tasks - would represent a similarly transformative
advance. Recently, Binz et al. introduced Centaur, a large language model (LLM)
fine-tuned on human data from 160 experiments, proposing its use not only as a
model of cognition but also as a participant simulator for "in silico
prototyping of experimental studies", e.g., to advance automated cognitive
science. Here, we review the core criteria for a participant simulator and
assess how well Centaur meets them. Although Centaur demonstrates strong
predictive accuracy, its generative behavior - a critical criterion for a
participant simulator - systematically diverges from human data. This suggests
that, while Centaur is a significant step toward predicting human behavior, it
does not yet meet the standards of a reliable participant simulator or an
accurate model of cognition.

</details>


### [183] [Score Augmentation for Diffusion Models](https://arxiv.org/abs/2508.07926)
*Liang Hou,Yuan Gao,Boyuan Jiang,Xin Tao,Qi Yan,Renjie Liao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.LG

TL;DR: ScoreAug, a novel data augmentation framework for diffusion models, mitigates overfitting by transforming noisy data and aligning with denoising mechanisms, outperforming baselines on benchmarks like CIFAR-10 and ImageNet.


<details>
  <summary>Details</summary>
Motivation: Addressing overfitting in diffusion models, especially in data-limited scenarios, by proposing a tailored augmentation method.

Method: ScoreAug applies transformations to noisy data, requiring the denoiser to predict augmented targets, creating an equivariant learning objective.

Result: Significant performance improvements on benchmarks, effective overfitting mitigation, and stable convergence.

Conclusion: ScoreAug is a robust solution for overfitting in diffusion models, compatible with traditional augmentation for further gains.

Abstract: Diffusion models have achieved remarkable success in generative modeling.
However, this study confirms the existence of overfitting in diffusion model
training, particularly in data-limited regimes. To address this challenge, we
propose Score Augmentation (ScoreAug), a novel data augmentation framework
specifically designed for diffusion models. Unlike conventional augmentation
approaches that operate on clean data, ScoreAug applies transformations to
noisy data, aligning with the inherent denoising mechanism of diffusion.
Crucially, ScoreAug further requires the denoiser to predict the augmentation
of the original target. This design establishes an equivariant learning
objective, enabling the denoiser to learn scores across varied denoising
spaces, thereby realizing what we term score augmentation. We also
theoretically analyze the relationship between scores in different spaces under
general transformations. In experiments, we extensively validate ScoreAug on
multiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with
results demonstrating significant performance improvements over baselines.
Notably, ScoreAug effectively mitigates overfitting across diverse scenarios,
such as varying data scales and model capacities, while exhibiting stable
convergence properties. Another advantage of ScoreAug over standard data
augmentation lies in its ability to circumvent data leakage issues under
certain conditions. Furthermore, we show that ScoreAug can be synergistically
combined with traditional data augmentation techniques to achieve additional
performance gains.

</details>


### [184] [Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting](https://arxiv.org/abs/2508.07927)
*Amal Saadallah,Abdulaziz Al-Ademi*

Main category: cs.LG

TL;DR: A novel framework improves DNN performance for time series forecasting in non-stationary environments by adapting and selecting specialized models based on clustered patterns and concept drift detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of evolving patterns in non-stationary time series forecasting.

Method: Offline training of a base DNN, clustering dominant patterns, fine-tuning specialized models, and deploying them via similarity matching. Includes concept drift detection.

Result: Significant performance gains across various DNN architectures, including those in GluonTS.

Conclusion: The framework is generalizable and effective for enhancing forecasting accuracy in dynamic environments.

Abstract: Time series forecasting poses significant challenges in non-stationary
environments where underlying patterns evolve over time. In this work, we
propose a novel framework that enhances deep neural network (DNN) performance
by leveraging specialized model adaptation and selection. Initially, a base DNN
is trained offline on historical time series data. A reserved validation subset
is then segmented to extract and cluster the most dominant patterns within the
series, thereby identifying distinct regimes. For each identified cluster, the
base DNN is fine-tuned to produce a specialized version that captures unique
pattern characteristics. At inference, the most recent input is matched against
the cluster centroids, and the corresponding fine-tuned version is deployed
based on the closest similarity measure. Additionally, our approach integrates
a concept drift detection mechanism to identify and adapt to emerging patterns
caused by non-stationary behavior. The proposed framework is generalizable
across various DNN architectures and has demonstrated significant performance
gains on both traditional DNNs and recent advanced architectures implemented in
the GluonTS library.

</details>


### [185] [Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters](https://arxiv.org/abs/2508.07952)
*Richard J. Fawley,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: SHARK is a feature-weighted clustering algorithm using Shapley values to quantify feature relevance without extra parameters, outperforming existing methods in noisy settings.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering assumes equal feature importance, which fails in high-dimensional or noisy data. Feature weighting methods often need tuning, so SHARK avoids this.

Method: SHARK uses Shapley values to decompose the $k$-means objective into per-feature contributions, iteratively re-weighting features by their Shapley relevance.

Result: SHARK matches or outperforms other methods, showing robustness and accuracy, especially in noisy scenarios.

Conclusion: SHARK provides a parameter-free, efficient, and effective solution for feature-weighted clustering, validated by experiments.

Abstract: Clustering algorithms often assume all features contribute equally to the
data structure, an assumption that usually fails in high-dimensional or noisy
settings. Feature weighting methods can address this, but most require
additional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a
feature-weighted clustering algorithm motivated by the use of Shapley values
from cooperative game theory to quantify feature relevance, which requires no
additional parameters beyond those in $k$-means. We prove that the $k$-means
objective can be decomposed into a sum of per-feature Shapley values, providing
an axiomatic foundation for unsupervised feature relevance and reducing Shapley
computation from exponential to polynomial time. SHARK iteratively re-weights
features by the inverse of their Shapley contribution, emphasising informative
dimensions and down-weighting irrelevant ones. Experiments on synthetic and
real-world data sets show that SHARK consistently matches or outperforms
existing methods, achieving superior robustness and accuracy, particularly in
scenarios where noise may be present. Software:
https://github.com/rickfawley/shark.

</details>


### [186] [WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2508.07970)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Tingfeng Xian,Haoqiang Hong,Boqi Chen,Haotao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: WeChat-YATT is a scalable RLHF training framework addressing controller scalability and workflow inefficiencies, improving throughput and GPU utilization.


<details>
  <summary>Details</summary>
Motivation: Challenges in scaling RLHF to complex workflows and dynamic workloads, including controller limitations and resource inefficiencies.

Method: Introduces WeChat-YATT with a parallel controller model and dynamic resource placement for efficient orchestration.

Result: Achieves higher throughput and better GPU utilization, successfully deployed in WeChat.

Conclusion: WeChat-YATT effectively addresses scalability and efficiency in RLHF training, proven in real-world applications.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent
paradigm for training large language models and multimodal systems. Despite
notable advances enabled by existing RLHF training frameworks, significant
challenges remain in scaling to complex multimodal workflows and adapting to
dynamic workloads. In particular, current systems often encounter limitations
related to controller scalability when managing large models, as well as
inefficiencies in orchestrating intricate RLHF pipelines, especially in
scenarios that require dynamic sampling and resource allocation. In this paper,
we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,
scalable, and balanced RLHF training framework specifically designed to address
these challenges. WeChat-YATT features a parallel controller programming model
that enables flexible and efficient orchestration of complex RLHF workflows,
effectively mitigating the bottlenecks associated with centralized controller
architectures and facilitating scalability in large-scale data scenarios. In
addition, we propose a dynamic placement schema that adaptively partitions
computational resources and schedules workloads, thereby significantly reducing
hardware idle time and improving GPU utilization under variable training
conditions. We evaluate WeChat-YATT across a range of experimental scenarios,
demonstrating that it achieves substantial improvements in throughput compared
to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been
successfully deployed to train models supporting WeChat product features for a
large-scale user base, underscoring its effectiveness and robustness in
real-world applications.

</details>


### [187] [A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation](https://arxiv.org/abs/2508.08002)
*Hongxin Yu,Yibing Wang,Fengyue Jin,Meng Zhang,Anni Chen*

Main category: cs.LG

TL;DR: The paper introduces a physics-informed deep operator network (PI-DeepONet) for real-time freeway traffic state estimation (TSE), extending it with 2-D data support, nonlinear layers, attention mechanisms, and adaptive parameter identification, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: To improve TSE accuracy by combining model-driven and data-driven approaches, leveraging the strengths of both through an advanced neural network architecture.

Method: Extended PI-DeepONet architecture with 2-D data input, nonlinear expansion, attention mechanisms, and adaptive parameter identification, evaluated on NGSIM and urban expressway data.

Result: Outperformed four baseline TSE methods, achieving high-precision flow and mean speed estimations.

Conclusion: The extended PI-DeepONet architecture effectively enhances TSE accuracy, demonstrating its potential for real-world applications.

Abstract: Traffic state estimation (TSE) falls methodologically into three categories:
model-driven, data-driven, and model-data dual-driven. Model-driven TSE relies
on macroscopic traffic flow models originated from hydrodynamics. Data-driven
TSE leverages historical sensing data and employs statistical models or machine
learning methods to infer traffic state. Model-data dual-driven traffic state
estimation attempts to harness the strengths of both aspects to achieve more
accurate TSE. From the perspective of mathematical operator theory, TSE can be
viewed as a type of operator that maps available measurements of inerested
traffic state into unmeasured traffic state variables in real time. For the
first time this paper proposes to study real-time freeway TSE in the idea of
physics-informed deep operator network (PI-DeepONet), which is an
operator-oriented architecture embedding traffic flow models based on deep
neural networks. The paper has developed an extended architecture from the
original PI-DeepONet. The extended architecture is featured with: (1) the
acceptance of 2-D data input so as to support CNN-based computations; (2) the
introduction of a nonlinear expansion layer, an attention mechanism, and a MIMO
mechanism; (3) dedicated neural network design for adaptive identification of
traffic flow model parameters. A traffic state estimator built on the basis of
this extended PI-DeepONet architecture was evaluated with respect to a short
freeway stretch of NGSIM and a large-scale urban expressway in China, along
with other four baseline TSE methods. The evaluation results demonstrated that
this novel TSE method outperformed the baseline methods with high-precision
estimation results of flow and mean speed.

</details>


### [188] [Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP](https://arxiv.org/abs/2508.08005)
*Xiang Li,Shanshan Wang,Chenglong Xiao*

Main category: cs.LG

TL;DR: A learning-based framework combining traditional ML and graph neural networks is proposed for algorithm selection in the Maximum Clique Problem (MCP), with RF as a strong baseline and GAT-MLP as an effective dual-channel model.


<details>
  <summary>Details</summary>
Motivation: No single algorithm performs best for MCP across all instances, and there's a lack of research on algorithm selection for MCP.

Method: Constructed a labeled dataset using four exact MCP algorithms on diverse graphs, evaluated four classifiers (SVM, RF, DT, KNN), and developed GAT-MLP, a dual-channel model combining GAT and MLP.

Result: RF performed consistently well; GAT-MLP showed strong and consistent performance, with connectivity and topology being key predictors.

Conclusion: Dual-channel architectures and graph neural networks are effective for combinatorial algorithm selection in MCP.

Abstract: Extensive experiments and prior studies show that no single maximum clique
algorithm consistently performs best across all instances, highlighting the
importance of selecting suitable algorithms based on instance features. Through
an extensive analysis of relevant studies, it is found that there is a lack of
research work concerning algorithm selection oriented toward the Maximum Clique
Problem (MCP). In this work, we propose a learning-based framework that
integrates both traditional machine learning and graph neural networks to
address this gap. We construct a labeled dataset by running four exact MCP
algorithms on a diverse collection of graph instances, accompanied by
structural and global statistical features extracted from each graph. We first
evaluate four conventional classifiers: Support Vector Machine (SVM), Random
Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple
dataset variants. Experimental results show that RF consistently shows strong
performance across metrics and dataset variants, making it a reliable baseline.
In addition, feature importance analysis indicates that connectivity and
topological structure are strong predictors of algorithm performance. Building
on these findings, we develop a dual-channel model named GAT-MLP, which
combines a Graph Attention Network (GAT) for local structural encoding with a
Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model
shows strong and consistent performance across all metrics. Our results
highlight the effectiveness of dual-channel architectures and the promise of
graph neural networks in combinatorial algorithm selection.

</details>


### [189] [Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks](https://arxiv.org/abs/2508.08013)
*Mohamad Assaad,Zeinab Nehme,Merouane Debbah*

Main category: cs.LG

TL;DR: Two communication-efficient Federated Learning methods reduce overhead by using scalar values and simultaneous user transmissions, leveraging channel info without extra CSI resources.


<details>
  <summary>Details</summary>
Motivation: FL faces high communication overhead in wireless systems, prompting the need for efficient methods.

Method: Two approaches: zero-order optimization with two-point gradient estimator and first-order gradient computation, leveraging channel info and supporting asynchronous devices.

Result: Convergence guarantees and performance bounds are rigorously derived for both methods.

Conclusion: The proposed methods effectively reduce communication overhead in FL while maintaining performance.

Abstract: Federated Learning (FL) is an emerging learning framework that enables edge
devices to collaboratively train ML models without sharing their local data. FL
faces, however, a significant challenge due to the high amount of information
that must be exchanged between the devices and the aggregator in the training
phase, which can exceed the limited capacity of wireless systems. In this
paper, two communication-efficient FL methods are considered where
communication overhead is reduced by communicating scalar values instead of
long vectors and by allowing high number of users to send information
simultaneously. The first approach employs a zero-order optimization technique
with two-point gradient estimator, while the second involves a first-order
gradient computation strategy. The novelty lies in leveraging channel
information in the learning algorithms, eliminating hence the need for
additional resources to acquire channel state information (CSI) and to remove
its impact, as well as in considering asynchronous devices. We provide a
rigorous analytical framework for the two methods, deriving convergence
guarantees and establishing appropriate performance bounds.

</details>


### [190] [BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models](https://arxiv.org/abs/2508.08040)
*Maozhen Zhang,Mengnan Zhao,Bo Wang*

Main category: cs.LG

TL;DR: BadPromptFL is a backdoor attack targeting prompt-based federated learning in multimodal models, achieving high success rates without altering model parameters.


<details>
  <summary>Details</summary>
Motivation: The security risks of prompt-based aggregation in federated multimodal learning are unexplored, leaving vulnerabilities unaddressed.

Method: Compromised clients optimize local backdoor triggers and prompt embeddings, injecting poisoned prompts into global aggregation.

Result: Achieves >90% attack success rates with minimal visibility and limited client participation.

Conclusion: Highlights critical robustness concerns for prompt-based federated learning in real-world deployments.

Abstract: Prompt-based tuning has emerged as a lightweight alternative to full
fine-tuning in large vision-language models, enabling efficient adaptation via
learned contextual prompts. This paradigm has recently been extended to
federated learning settings (e.g., PromptFL), where clients collaboratively
train prompts under data privacy constraints. However, the security
implications of prompt-based aggregation in federated multimodal learning
remain largely unexplored, leaving a critical attack surface unaddressed. In
this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack
targeting prompt-based federated learning in multimodal contrastive models. In
BadPromptFL, compromised clients jointly optimize local backdoor triggers and
prompt embeddings, injecting poisoned prompts into the global aggregation
process. These prompts are then propagated to benign clients, enabling
universal backdoor activation at inference without modifying model parameters.
Leveraging the contextual learning behavior of CLIP-style architectures,
BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal
visibility and limited client participation. Extensive experiments across
multiple datasets and aggregation protocols validate the effectiveness,
stealth, and generalizability of our attack, raising critical concerns about
the robustness of prompt-based federated learning in real-world deployments.

</details>


### [191] [Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles](https://arxiv.org/abs/2508.08034)
*Roksana Yahyaabadi,Ghazal Farhani,Taufiq Rahman,Soodeh Nikan,Abdullah Jirjees,Fadi Araji*

Main category: cs.LG

TL;DR: A scalable data-driven method using powertrain features and machine learning accurately predicts power consumption for ICE, EV, and HEV platforms, with low errors and robust performance.


<details>
  <summary>Details</summary>
Motivation: Traditional power consumption prediction methods are impractical for large-scale use, necessitating a scalable, data-driven approach.

Method: Uses powertrain dynamic feature sets with machine learning (traditional and deep neural networks) to estimate power consumption.

Result: ICE models achieved high accuracy (errors under 3%), while Transformer and LSTM models excelled for EVs and HEVs (errors below 4.1% and 2.1%).

Conclusion: The method is effective across vehicle types, with EVs and HEVs requiring robust models due to higher dataset variability.

Abstract: Accurate power consumption prediction is crucial for improving efficiency and
reducing environmental impact, yet traditional methods relying on specialized
instruments or rigid physical models are impractical for large-scale,
real-world deployment. This study introduces a scalable data-driven method
using powertrain dynamic feature sets and both traditional machine learning and
deep neural networks to estimate instantaneous and cumulative power consumption
in internal combustion engine (ICE), electric vehicle (EV), and hybrid electric
vehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with
mean absolute error and root mean squared error on the order of $10^{-3}$, and
cumulative errors under 3%. Transformer and long short-term memory models
performed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,
respectively. Results confirm the approach's effectiveness across vehicles and
models. Uncertainty analysis revealed greater variability in EV and HEV
datasets than ICE, due to complex power management, emphasizing the need for
robust models for advanced powertrains.

</details>


### [192] [On Understanding of the Dynamics of Model Capacity in Continual Learning](https://arxiv.org/abs/2508.08052)
*Supriyo Chakraborty,Krishnan Raghavan*

Main category: cs.LG

TL;DR: The paper introduces CLEMC to address the stability-plasticity dilemma in continual learning, showing it's non-stationary and task-dependent.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of balancing stability and plasticity in neural networks during continual learning.

Method: Develops a difference equation to model NN-task-optimization interplay and validates with experiments across various architectures.

Result: Effective capacity and stability-plasticity balance are non-stationary, diminishing with differing task distributions.

Conclusion: CLEMC provides insights into continual learning dynamics, applicable across diverse NN architectures.

Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN)
capacity-its ability to represent tasks-is a fundamental challenge in continual
learning (CL). Within this context, we introduce CL's effective model capacity
(CLEMC) that characterizes the dynamic behavior of the stability-plasticity
balance point. We develop a difference equation to model the evolution of the
interplay between the NN, task data, and optimization procedure. We then
leverage CLEMC to demonstrate that the effective capacity-and, by extension,
the stability-plasticity balance point is inherently non-stationary. We show
that regardless of the NN architecture or optimization method, a NN's ability
to represent new tasks diminishes when incoming task distributions differ from
previous ones. We conduct extensive experiments to support our theoretical
findings, spanning a range of architectures-from small feedforward network and
convolutional networks to medium-sized graph neural networks and
transformer-based large language models with millions of parameters.

</details>


### [193] [C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction](https://arxiv.org/abs/2508.08071)
*Yunqing Li,Zixiang Tang,Jiaying Zhuang,Zhenyu Yang,Farhad Ameri,Jianbang Zhang*

Main category: cs.LG

TL;DR: PMGraph is a benchmark for supply-chain graphs, and C-MAG is a two-stage architecture improving link prediction by fusing multimodal data.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to capture complex supply-chain data like capabilities, certifications, and multimodal profiles, necessitating better solutions.

Method: C-MAG aligns and aggregates textual and visual attributes into group embeddings, then propagates them through a hetero-graph using multiscale message passing.

Result: C-MAG enhances link prediction accuracy and handles noisy real-world data effectively.

Conclusion: PMGraph and C-MAG address gaps in supply-chain data analysis, offering improved accuracy and practical fusion guidelines.

Abstract: Connecting an ever-expanding catalogue of products with suitable
manufacturers and suppliers is critical for resilient, efficient global supply
chains, yet traditional methods struggle to capture complex capabilities,
certifications, geographic constraints, and rich multimodal data of real-world
manufacturer profiles. To address these gaps, we introduce PMGraph, a public
benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking
8,888 manufacturers, over 70k products, more than 110k manufacturer-product
edges, and over 29k product images. Building on this benchmark, we propose the
Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first
aligns and aggregates textual and visual attributes into intermediate group
embeddings, then propagates them through a manufacturer-product hetero-graph
via multiscale message passing to enhance link prediction accuracy. C-MAG also
provides practical guidelines for modality-aware fusion, preserving predictive
performance in noisy, real-world settings.

</details>


### [194] [Grid2Guide: A* Enabled Small Language Model for Indoor Navigation](https://arxiv.org/abs/2508.08100)
*Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: Grid2Guide combines A* search and a Small Language Model (SLM) to generate human-readable indoor navigation instructions without external signals or infrastructure.


<details>
  <summary>Details</summary>
Motivation: Indoor navigation is challenging without external signals or infrastructure, requiring a lightweight, interpretable solution.

Method: The framework uses a binary occupancy matrix from indoor maps, applies A* for optimal pathfinding, and transforms steps into natural language via an SLM.

Result: Experiments show accurate, timely navigation guidance, validating the lightweight, infrastructure-free approach.

Conclusion: Grid2Guide is an effective solution for real-time indoor navigation, balancing accuracy and user interpretability.

Abstract: Reliable indoor navigation remains a significant challenge in complex
environments, particularly where external positioning signals and dedicated
infrastructures are unavailable. This research presents Grid2Guide, a hybrid
navigation framework that combines the A* search algorithm with a Small
Language Model (SLM) to generate clear, human-readable route instructions. The
framework first conducts a binary occupancy matrix from a given indoor map.
Using this matrix, the A* algorithm computes the optimal path between origin
and destination, producing concise textual navigation steps. These steps are
then transformed into natural language instructions by the SLM, enhancing
interpretability for end users. Experimental evaluations across various indoor
scenarios demonstrate the method's effectiveness in producing accurate and
timely navigation guidance. The results validate the proposed approach as a
lightweight, infrastructure-free solution for real-time indoor navigation
support.

</details>


### [195] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: The paper introduces a transfer learning-based predictive process monitoring (PPM) technique to enable organizations with limited event data to implement PPM effectively by transferring knowledge from similar processes.


<details>
  <summary>Details</summary>
Motivation: Existing PPM techniques require substantial event data or resources, limiting accessibility for some organizations. This paper addresses this gap by leveraging transfer learning.

Method: The proposed technique uses transfer learning to apply knowledge from one business process to another, tested in real-life IT service management use cases.

Result: Experiments show that knowledge transfer between similar processes, even across organizations, enables effective PPM in resource-scarce contexts.

Conclusion: The technique allows organizations to utilize PPM without extensive data by transferring pre-trained models within or across organizational boundaries.

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [196] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: The paper presents a vision-based indoor navigation system combining ResNet-50 for localization and an LLM for navigation, achieving high accuracy and demonstrating scalability.


<details>
  <summary>Details</summary>
Motivation: Indoor navigation is challenging due to unreliable GPS and complex environments. The study aims to provide a scalable, infrastructure-free solution.

Method: Uses ResNet-50 for vision-based localization and an LLM for navigation, tested in realistic office corridors.

Result: 96% localization accuracy; 75% navigation accuracy with ChatGPT, though limited by zero-shot reasoning and inference time.

Conclusion: The approach shows promise for scalable indoor navigation in resource-constrained settings.

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


### [197] [MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing](https://arxiv.org/abs/2508.08122)
*Mingrong Lin,Ke Deng,Zhengyang Wu,Zetao Zheng,Jie Li*

Main category: cs.LG

TL;DR: The paper introduces memoryKT, a knowledge tracing model using a temporal variational autoencoder to simulate memory dynamics, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge tracing models often overlook personalized forgetting patterns and other memory processes (encoding, storage, retrieval).

Method: Proposes memoryKT, a model with a three-stage process: learning memory features, reconstructing feedback, and embedding a personalized forgetting module.

Result: Outperforms state-of-the-art baselines on four public datasets.

Conclusion: memoryKT effectively models the full memory cycle, enhancing individual difference perception and performance.

Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery
from their historical interactions. Simulating students' memory states is a
promising approach to enhance both the performance and interpretability of
knowledge tracing models. Memory consists of three fundamental processes:
encoding, storage, and retrieval. Although forgetting primarily manifests
during the storage stage, most existing studies rely on a single,
undifferentiated forgetting mechanism, overlooking other memory processes as
well as personalized forgetting patterns. To address this, this paper proposes
memoryKT, a knowledge tracing model based on a novel temporal variational
autoencoder. The model simulates memory dynamics through a three-stage process:
(i) Learning the distribution of students' knowledge memory features, (ii)
Reconstructing their exercise feedback, while (iii) Embedding a personalized
forgetting module within the temporal workflow to dynamically modulate memory
storage strength. This jointly models the complete encoding-storage-retrieval
cycle, significantly enhancing the model's perception capability for individual
differences. Extensive experiments on four public datasets demonstrate that our
proposed approach significantly outperforms state-of-the-art baselines.

</details>


### [198] [ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring](https://arxiv.org/abs/2508.08073)
*Dimitris Tsaras,Xing Li,Lei Chen,Zhiyao Xie,Mingxuan Yuan*

Main category: cs.LG

TL;DR: A classifier-based approach speeds up logic optimization by 3.9x by pruning unsuccessful cuts, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: High computational demands and inefficiency (98% failure rate) of conventional logic optimization operators like refactor.

Method: Uses a classifier to preemptively prune unsuccessful cuts, avoiding unnecessary resynthesis operations.

Result: Achieves a 3.9x speedup in logic optimization compared to state-of-the-art ABC implementation.

Conclusion: The classifier-based method significantly improves efficiency in logic optimization.

Abstract: In electronic design automation, logic optimization operators play a crucial
role in minimizing the gate count of logic circuits. However, their computation
demands are high. Operators such as refactor conventionally form iterative cuts
for each node, striving for a more compact representation - a task which often
fails 98% on average. Prior research has sought to mitigate computational cost
through parallelization. In contrast, our approach leverages a classifier to
prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis
operations. Experiments on the refactor operator using the EPFL benchmark suite
and 10 large industrial designs demonstrate that this technique can speedup
logic optimization by 3.9x on average compared with the state-of-the-art ABC
implementation.

</details>


### [199] [MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08137)
*Pravallika Abbineni,Saoud Aldowaish,Colin Liechty,Soroosh Noorzad,Ali Ghazizadeh,Morteza Fayazi*

Main category: cs.LG

TL;DR: MuaLLM is an open-source multimodal LLM agent for circuit design, combining RAG and adaptive vector databases for efficient, scalable assistance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in circuit design literature review due to rapid research influx, inconsistent data, and complex optimization.

Method: Uses a hybrid RAG framework with adaptive vector databases and ReAct workflow for iterative reasoning and retrieval.

Result: Achieves 90.1% recall on RAG-250 and 86.8% accuracy on Reas-100, with 10x lower cost and 1.6x speed.

Conclusion: MuaLLM offers scalable, efficient circuit design assistance, overcoming traditional limitations.

Abstract: Conducting a comprehensive literature review is crucial for advancing circuit
design methodologies. However, the rapid influx of state-of-the-art research,
inconsistent data representation, and the complexity of optimizing circuit
design objectives make this task significantly challenging. In this paper, we
propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for
circuit design assistance that integrates a hybrid Retrieval-Augmented
Generation (RAG) framework with an adaptive vector database of circuit design
research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +
Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step
information retrieval. It functions as a question-answering design assistant,
capable of interpreting complex queries and providing reasoned responses
grounded in circuit literature. Its multimodal capabilities enable processing
of both textual and visual data, facilitating more efficient and comprehensive
analysis. The system dynamically adapts using intelligent search tools,
automated document retrieval from the internet, and real-time database updates.
Unlike conventional approaches constrained by model context limits, MuaLLM
decouples retrieval from inference, enabling scalable reasoning over
arbitrarily large corpora. At the maximum context length supported by standard
LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining
the same accuracy. This allows rapid, no-human-in-the-loop database generation,
overcoming the bottleneck of simulation-based dataset creation for circuits. To
evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval
and citation performance, and Reasoning-100 (Reas-100), focused on multistep
reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%
accuracy on Reas-100.

</details>


### [200] [Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles](https://arxiv.org/abs/2508.08080)
*Cas Oude Hoekstra,Floris den Hengst*

Main category: cs.LG

TL;DR: Symbolic Quantile Regression (SQR) extends Symbolic Regression (SR) to predict conditional quantiles, offering interpretable models that perform comparably to black-box methods while maintaining transparency.


<details>
  <summary>Details</summary>
Motivation: Current SR methods focus on average outcomes, but estimating relationships at other distribution points (e.g., median or extremes) is crucial for high-stakes applications.

Method: Introduces SQR to predict conditional quantiles using SR, evaluated against transparent and black-box models.

Result: SQR outperforms transparent models and matches black-box performance without losing interpretability. Demonstrated in an airline fuel usage case study.

Conclusion: SQR is effective for predicting conditional quantiles and understanding feature influences across the target distribution.

Abstract: Symbolic Regression (SR) is a well-established framework for generating
interpretable or white-box predictive models. Although SR has been successfully
applied to create interpretable estimates of the average of the outcome, it is
currently not well understood how it can be used to estimate the relationship
between variables at other points in the distribution of the target variable.
Such estimates of e.g. the median or an extreme value provide a fuller picture
of how predictive variables affect the outcome and are necessary in
high-stakes, safety-critical application domains. This study introduces
Symbolic Quantile Regression (SQR), an approach to predict conditional
quantiles with SR. In an extensive evaluation, we find that SQR outperforms
transparent models and performs comparably to a strong black-box baseline
without compromising transparency. We also show how SQR can be used to explain
differences in the target distribution by comparing models that predict extreme
and central outcomes in an airline fuel usage case study. We conclude that SQR
is suitable for predicting conditional quantiles and understanding interesting
feature influences at varying quantiles.

</details>


### [201] [Neural Logic Networks for Interpretable Classification](https://arxiv.org/abs/2508.08172)
*Vincent Perreault,Katsumi Inoue,Richard Labib,Alain Hertz*

Main category: cs.LG

TL;DR: Neural Logic Networks are generalized with NOT operations and biases to improve interpretability and performance in Boolean networks discovery.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in traditional neural networks by developing a logical and probabilistic model.

Method: Generalize Neural Logic Networks with NOT operations and biases, propose a factorized IF-THEN rule structure, and modify the learning algorithm.

Result: Improved state-of-the-art in Boolean networks discovery and learned interpretable rules, demonstrated in medical field applications.

Conclusion: The method enhances interpretability and performance, making it valuable for fields like medicine where transparency is crucial.

Abstract: Traditional neural networks have an impressive classification performance,
but what they learn cannot be inspected, verified or extracted. Neural Logic
Networks on the other hand have an interpretable structure that enables them to
learn a logical mechanism relating the inputs and outputs with AND and OR
operations. We generalize these networks with NOT operations and biases that
take into account unobserved data and develop a rigorous logical and
probabilistic modeling in terms of concept combinations to motivate their use.
We also propose a novel factorized IF-THEN rule structure for the model as well
as a modified learning algorithm. Our method improves the state-of-the-art in
Boolean networks discovery and is able to learn relevant, interpretable rules
in tabular classification, notably on an example from the medical field where
interpretability has tangible value.

</details>


### [202] [Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation](https://arxiv.org/abs/2508.08087)
*Amir Ali Panahi,Daniel Luder,Billy Wu,Gregory Offer,Dirk Uwe Sauer,Weihan Li*

Main category: cs.LG

TL;DR: The paper benchmarks three operator-learning surrogates (DeepONets, FNOs, PE-FNO) for lithium-ion battery digital twins, highlighting PE-FNO's superior speed, accuracy, and parametric flexibility.


<details>
  <summary>Details</summary>
Motivation: To achieve high-fidelity, sub-millisecond digital twins of lithium-ion batteries for real-time management and design.

Method: Benchmarked DeepONets, FNOs, and proposed PE-FNO on simulated battery trajectories under various current loads and SOC ranges.

Result: PE-FNO is 200x faster than traditional solvers, maintains low errors (1.7 mV voltage, 1% concentration), and generalizes to varying parameters.

Conclusion: PE-FNO meets demands for real-time battery applications, outperforming conventional surrogates and enabling high-speed, high-fidelity digital twins.

Abstract: Reliable digital twins of lithium-ion batteries must achieve high physical
fidelity with sub-millisecond speed. In this work, we benchmark three
operator-learning surrogates for the Single Particle Model (SPM): Deep Operator
Networks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed
parameter-embedded Fourier Neural Operator (PE-FNO), which conditions each
spectral layer on particle radius and solid-phase diffusivity. Models are
trained on simulated trajectories spanning four current families (constant,
triangular, pulse-train, and Gaussian-random-field) and a full range of
State-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates
constant-current behaviour but struggles with more dynamic loads. The basic FNO
maintains mesh invariance and keeps concentration errors below 1 %, with
voltage mean-absolute errors under 1.7 mV across all load types. Introducing
parameter embedding marginally increases error, but enables generalisation to
varying radii and diffusivities. PE-FNO executes approximately 200 times faster
than a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse
tasks are explored in a parameter estimation task with Bayesian optimisation,
recovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute
percentage error, respectively, and 0.5918 percentage points higher error in
comparison with classical methods. These results pave the way for neural
operators to meet the accuracy, speed and parametric flexibility demands of
real-time battery management, design-of-experiments and large-scale inference.
PE-FNO outperforms conventional neural surrogates, offering a practical path
towards high-speed and high-fidelity electrochemical digital twins.

</details>


### [203] [Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)
*Tong Yang,Yu Huang,Yingbin Liang,Yuejie Chi*

Main category: cs.LG

TL;DR: The paper explores how transformers learn symbolic multi-step reasoning tasks, focusing on path-finding in trees, and provides theoretical insights into their training dynamics and generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanisms behind transformers' ability to perform multi-step reasoning tasks, particularly how they learn through chain-of-thought processes.

Method: Analyzes backward and forward reasoning tasks in path-finding using one-layer transformers, grounded in gradient descent dynamics.

Result: Trained one-layer transformers can solve both tasks with generalization guarantees, with attention heads specializing and coordinating autonomously.

Conclusion: Shallow transformers can solve complex reasoning tasks when structured with intermediate steps, offering insights into the emergence of reasoning abilities.

Abstract: Transformers have demonstrated remarkable capabilities in multi-step
reasoning tasks. However, understandings of the underlying mechanisms by which
they acquire these abilities through training remain limited, particularly from
a theoretical standpoint. This work investigates how transformers learn to
solve symbolic multi-step reasoning problems through chain-of-thought
processes, focusing on path-finding in trees. We analyze two intertwined tasks:
a backward reasoning task, where the model outputs a path from a goal node to
the root, and a more complex forward reasoning task, where the model implements
two-stage reasoning by first identifying the goal-to-root path and then
reversing it to produce the root-to-goal path. Our theoretical analysis,
grounded in the dynamics of gradient descent, shows that trained one-layer
transformers can provably solve both tasks with generalization guarantees to
unseen trees. In particular, our multi-phase training dynamics for forward
reasoning elucidate how different attention heads learn to specialize and
coordinate autonomously to solve the two subtasks in a single autoregressive
path. These results provide a mechanistic explanation of how trained
transformers can implement sequential algorithmic procedures. Moreover, they
offer insights into the emergence of reasoning abilities, suggesting that when
tasks are structured to take intermediate chain-of-thought steps, even shallow
multi-head transformers can effectively solve problems that would otherwise
require deeper architectures.

</details>


### [204] [NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection](https://arxiv.org/abs/2508.08124)
*Guanghao Jin,Yuan Liang,Yihan Ma,Jingpei Wu,Guoyang Liu*

Main category: cs.LG

TL;DR: NeuroDx-LM is a large-scale EEG model for neurological disorder detection, addressing challenges like limited labeled data and suboptimal performance. It uses Selective Temporal-Frequency Embedding and Progressive Feature-Aware Training, achieving state-of-the-art results on CHB-MIT and Schizophrenia datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of EEG-based large-scale models in clinical applications, such as limited labeled data and performance issues.

Method: Proposes NeuroDx-LM with Selective Temporal-Frequency Embedding and a two-stage Progressive Feature-Aware Training strategy.

Result: Achieves state-of-the-art performance in EEG-based seizure and schizophrenia detection on CHB-MIT and Schizophrenia datasets.

Conclusion: NeuroDx-LM demonstrates the potential of EEG-based large-scale models for clinical applications, with code available for further use.

Abstract: Large-scale models pre-trained on Electroencephalography (EEG) have shown
promise in clinical applications such as neurological disorder detection.
However, the practical deployment of EEG-based large-scale models faces
critical challenges such as limited labeled EEG data and suboptimal performance
in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel
large-scale model specifically designed for detecting EEG-based neurological
disorders. Our key contributions include (i) a Selective Temporal-Frequency
Embedding mechanism that adaptively captures complex temporal and spectral
patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy
that refines feature representation in a two-stage process. In the first stage,
our model learns the fundamental discriminative features of EEG activities; in
the second stage, the model further extracts more specialized fine-grained
features for accurate diagnostic performance. We evaluated NeuroDx-LM on the
CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in
EEG-based seizure and schizophrenia detection, respectively. These results
demonstrate the great potential of EEG-based large-scale models to advance
clinical applicability. Our code is available at
https://github.com/LetItBe12345/NeuroDx-LM.

</details>


### [205] [OFAL: An Oracle-Free Active Learning Framework](https://arxiv.org/abs/2508.08126)
*Hadi Khorsand,Vahid Pourahmadi*

Main category: cs.LG

TL;DR: OFAL introduces an oracle-free active learning method using neural network uncertainty to generate informative samples, improving model accuracy without relying on an oracle.


<details>
  <summary>Details</summary>
Motivation: Labeling data with an oracle is costly and complex, especially with large unlabeled datasets. Avoiding reliance on an oracle is highly beneficial.

Method: OFAL uses Monte Carlo Dropouts to approximate Bayesian Neural Networks and a variational autoencoder to generate uncertain samples from confident ones.

Result: The method enhances active learning by creating informative samples, improving model accuracy.

Conclusion: OFAL successfully integrates with other active learning methods, offering a cost-effective alternative to oracle-dependent approaches.

Abstract: In the active learning paradigm, using an oracle to label data has always
been a complex and expensive task, and with the emersion of large unlabeled
data pools, it would be highly beneficial If we could achieve better results
without relying on an oracle. This research introduces OFAL, an oracle-free
active learning scheme that utilizes neural network uncertainty. OFAL uses the
model's own uncertainty to transform highly confident unlabeled samples into
informative uncertain samples. First, we start with separating and quantifying
different parts of uncertainty and introduce Monte Carlo Dropouts as an
approximation of the Bayesian Neural Network model. Secondly, by adding a
variational autoencoder, we go on to generate new uncertain samples by stepping
toward the uncertain part of latent space starting from a confidence seed
sample. By generating these new informative samples, we can perform active
learning and enhance the model's accuracy. Lastly, we try to compare and
integrate our method with other widely used active learning sampling methods.

</details>


### [206] [FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks](https://arxiv.org/abs/2508.08151)
*Moses Openja,Paolo Arcaini,Foutse Khomh,Fuyuki Ishikawa*

Main category: cs.LG

TL;DR: FairFLRep is an automated fairness-aware technique for identifying and correcting bias-inducing neurons in DNNs, improving fairness while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: DNNs can reflect and amplify biases from training data, leading to unfair decisions, especially in high-stakes applications. Existing methods struggle to efficiently address this issue.

Method: FairFLRep adjusts neuron weights linked to sensitive attributes (e.g., race, gender) by analyzing input-output relationships to correct disparities in predictive quality parity.

Result: FairFLRep outperforms existing methods in fairness improvement and efficiency, as shown in evaluations on image and tabular datasets.

Conclusion: FairFLRep effectively addresses bias in DNNs by integrating fairness considerations in fault localization and repair, offering a practical solution for fairer AI systems.

Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our
daily lives, including high-stakes decision-making applications that impact
individuals. However, these systems reflect and amplify bias from the data used
during training and testing, potentially resulting in biased behavior and
inaccurate decisions. For instance, having different misclassification rates
between white and black sub-populations. However, effectively and efficiently
identifying and correcting biased behavior in DNNs is a challenge. This paper
introduces FairFLRep, an automated fairness-aware fault localization and repair
technique that identifies and corrects potentially bias-inducing neurons in DNN
classifiers. FairFLRep focuses on adjusting neuron weights associated with
sensitive attributes, such as race or gender, that contribute to unfair
decisions. By analyzing the input-output relationships within the network,
FairFLRep corrects neurons responsible for disparities in predictive quality
parity. We evaluate FairFLRep on four image classification datasets using two
DNN classifiers, and four tabular datasets with a DNN model. The results show
that FairFLRep consistently outperforms existing methods in improving fairness
while preserving accuracy. An ablation study confirms the importance of
considering fairness during both fault localization and repair stages. Our
findings also show that FairFLRep is more efficient than the baseline
approaches in repairing the network.

</details>


### [207] [Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets](https://arxiv.org/abs/2508.08159)
*Cem Ata Baykara,Saurav Raj Pandey,Ali Burak Ünal,Harlin Lee,Mete Akgün*

Main category: cs.LG

TL;DR: The paper explores Federated Learning (FL) for epileptic seizure prediction using EEG data across diverse datasets, addressing data heterogeneity and privacy. It proposes Random Subset Aggregation to improve fairness and performance, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: To develop accurate, generalizable seizure prediction models across clinical sites while addressing privacy and data heterogeneity challenges.

Method: Uses FL with privacy-preserving global normalization and introduces Random Subset Aggregation, where clients train on fixed-size random subsets per round.

Result: Random Subset Aggregation improves performance on underrepresented datasets (e.g., 81.7% accuracy on Helsinki) and achieves 77.1% macro-average accuracy.

Conclusion: Balanced FL approaches like Random Subset Aggregation enable robust, fair, and generalizable seizure prediction systems in heterogeneous multi-hospital settings.

Abstract: Developing accurate and generalizable epileptic seizure prediction models
from electroencephalography (EEG) data across multiple clinical sites is
hindered by patient privacy regulations and significant data heterogeneity
(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving
framework for collaborative training, but standard aggregation methods like
Federated Averaging (FedAvg) can be biased by dominant datasets in
heterogeneous settings. This paper investigates FL for seizure prediction using
a single EEG channel across four diverse public datasets (Siena, CHB-MIT,
Helsinki, NCH), representing distinct patient populations (adult, pediatric,
neonate) and recording conditions. We implement privacy-preserving global
normalization and propose a Random Subset Aggregation strategy, where each
client trains on a fixed-size random subset of its data per round, ensuring
equal contribution during aggregation. Our results show that locally trained
models fail to generalize across sites, and standard weighted FedAvg yields
highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on
Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation
significantly improves performance on under-represented clients (accuracy
increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior
macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,
demonstrating a more robust and fair global model. This work highlights the
potential of balanced FL approaches for building effective and generalizable
seizure prediction systems in realistic, heterogeneous multi-hospital
environments while respecting data privacy.

</details>


### [208] [Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion](https://arxiv.org/abs/2508.08216)
*Nicole Lai-Tan,Xiao Gu,Marios G. Philiastides,Fani Deligianni*

Main category: cs.LG

TL;DR: ITSA improves BCI generalizability for personalized music-based motor rehabilitation by addressing EEG variability and movement artifacts.


<details>
  <summary>Details</summary>
Motivation: Overcoming inter-subject EEG variability and movement artifacts to enhance BCI generalizability for motor rehabilitation.

Method: Proposes ITSA with subject-specific recentering, distribution matching, and supervised rotational alignment, combined with RCSP and Riemannian geometry.

Result: Significant performance improvements across subjects and conditions, with parallel fusion outperforming sequential.

Conclusion: ITSA enhances BCI generalizability and robustness, promising for personalized motor rehabilitation.

Abstract: Personalised music-based interventions offer a powerful means of supporting
motor rehabilitation by dynamically tailoring auditory stimuli to provide
external timekeeping cues, modulate affective states, and stabilise gait
patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for
adapting these interventions across individuals. However, inter-subject
variability in EEG signals, further compounded by movement-induced artefacts
and motor planning differences, hinders the generalisability of BCIs and
results in lengthy calibration processes. We propose Individual Tangent Space
Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific
recentering, distribution matching, and supervised rotational alignment to
enhance cross-subject generalisation. Our hybrid architecture fuses Regularised
Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and
sequential configurations, improving class separability while maintaining the
geometric structure of covariance matrices for robust statistical computation.
Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant
performance improvements across subjects and conditions. The parallel fusion
approach shows the greatest enhancement over its sequential counterpart, with
robust performance maintained across varying data conditions and electrode
configurations. The code will be made publicly available at the time of
publication.

</details>


### [209] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: A systematic review of RL techniques for LLM reasoning, addressing challenges like lack of standardization and inconsistent results, and proposing a minimalist combination of techniques for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented understanding and inconsistent experimental settings in RL for LLM reasoning, and provide clear guidelines for practitioners.

Method: Rigorous reproductions and isolated evaluations of RL techniques within a unified framework, analyzing mechanisms, scenarios, and principles.

Result: A minimalist combination of two techniques outperforms others like GRPO and DAPO, unlocking critic-free policy learning with vanilla PPO loss.

Conclusion: The study offers practical guidelines and a reliable roadmap for selecting RL techniques in LLM reasoning, demonstrating the effectiveness of a simple yet powerful combination.

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [210] [Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.06863)
*Hamidreza Asadian-Rad,Hossein Soleimani,Shahrokh Farahmand*

Main category: cs.MA

TL;DR: A decentralized approach using deep reinforcement learning (DRL) with graph attention layers (GAT) and experience sharing (EPS-PPO) is proposed for UAV trajectory and user assignment in MEC, outperforming existing methods like MADDPG.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of non-convex optimization, user mobility, and communication bottlenecks in centralized UAV-MEC systems, a fully decentralized solution is advocated.

Method: UAVs use local observations and neighbor communication, employing GAT and EPS-PPO for trajectory optimization without global knowledge.

Result: The approach outperforms MADDPG and other methods, showing better performance with local communications only.

Conclusion: Decentralized DRL with GAT and EPS-PPO is effective for UAV-MEC, offering scalability, flexibility, and robustness.

Abstract: Unmanned aerial vehicles (UAVs) have been recently utilized in multi-access
edge computing (MEC) as edge servers. It is desirable to design UAVs'
trajectories and user to UAV assignments to ensure satisfactory service to the
users and energy efficient operation simultaneously. The posed optimization
problem is challenging to solve because: (i) The formulated problem is
non-convex, (ii) Due to the mobility of ground users, their future positions
and channel gains are not known in advance, (iii) Local UAVs' observations
should be communicated to a central entity that solves the optimization
problem. The (semi-) centralized processing leads to communication overhead,
communication/processing bottlenecks, lack of flexibility and scalability, and
loss of robustness to system failures. To simultaneously address all these
limitations, we advocate a fully decentralized setup with no centralized
entity. Each UAV obtains its local observation and then communicates with its
immediate neighbors only. After sharing information with neighbors, each UAV
determines its next position via a locally run deep reinforcement learning
(DRL) algorithm. None of the UAVs need to know the global communication graph.
Two main components of our proposed solution are (i) Graph attention layers
(GAT), and (ii) Experience and parameter sharing proximal policy optimization
(EPS-PPO). Our proposed approach eliminates all the limitations of
semi-centralized MADRL methods such as MAPPO and MA deep deterministic policy
gradient (MADDPG), while guaranteeing a better performance than independent
local DRLs such as in IPPO. Numerical results reveal notable performance gains
in several different criteria compared to the existing MADDPG algorithm,
demonstrating the potential for offering a better performance, while utilizing
local communications only.

</details>


### [211] [A Survey on Agentic Service Ecosystems: Measurement, Analysis, and Optimization](https://arxiv.org/abs/2508.07343)
*Xuwen Zhang,Xiao Xue,Xia Xie,Qun Ma,Xiangning Yu,Deyu Zhou,Yifan Wang,Ming Zhang*

Main category: cs.MA

TL;DR: The paper proposes a framework to analyze swarm intelligence emergence in Agentic Service Ecosystems, addressing gaps in current research through measurement, analysis, and optimization steps.


<details>
  <summary>Details</summary>
Motivation: Traditional methods are inadequate for the complexity of autonomous agents in Agentic Service Ecosystems, and current research lacks a unified approach to swarm intelligence emergence.

Method: The paper introduces a three-step framework (measurement, analysis, optimization) to study swarm intelligence emergence, reviewing existing technologies and their limitations.

Result: The framework provides theoretical support and practical methods for understanding and optimizing swarm intelligence in these ecosystems.

Conclusion: The proposed framework addresses research gaps and offers actionable insights for real-world applications in Agentic Service Ecosystems.

Abstract: The Agentic Service Ecosystem consists of heterogeneous autonomous agents
(e.g., intelligent machines, humans, and human-machine hybrid systems) that
interact through resource exchange and service co-creation. These agents, with
distinct behaviors and motivations, exhibit autonomous perception, reasoning,
and action capabilities, which increase system complexity and make traditional
linear analysis methods inadequate. Swarm intelligence, characterized by
decentralization, self-organization, emergence, and dynamic adaptability,
offers a novel theoretical lens and methodology for understanding and
optimizing such ecosystems. However, current research, owing to fragmented
perspectives and cross-ecosystem differences, fails to comprehensively capture
the complexity of swarm-intelligence emergence in agentic contexts. The lack of
a unified methodology further limits the depth and systematic treatment of the
research. This paper proposes a framework for analyzing the emergence of swarm
intelligence in Agentic Service Ecosystems, with three steps: measurement,
analysis, and optimization, to reveal the cyclical mechanisms and quantitative
criteria that foster emergence. By reviewing existing technologies, the paper
analyzes their strengths and limitations, identifies unresolved challenges, and
shows how this framework provides both theoretical support and actionable
methods for real-world applications.

</details>


### [212] [Retrieval-Augmented Multi-Agent System for Rapid Statement of Work Generation](https://arxiv.org/abs/2508.07569)
*Amulya Suravarjhula,Rashi Chandrashekhar Agrawal,Sakshi Jayesh Patel,Rahul Gupta*

Main category: cs.MA

TL;DR: An AI-driven system automates the drafting of Statements of Work (SOW), making it faster, more accurate, and legally compliant.


<details>
  <summary>Details</summary>
Motivation: The manual drafting of SOWs is slow, complex, and error-prone, necessitating a more efficient solution.

Method: The system uses three AI agents: one drafts the SOW, another checks legal compliance, and the third handles formatting.

Result: The system drafts SOWs in under three minutes with high accuracy, outperforming manual methods.

Conclusion: AI can streamline legal and business processes, reducing risks and saving time.

Abstract: Drafting a Statement of Work (SOW) is a vital part of business and legal
projects. It outlines key details like deliverables, timelines,
responsibilities, and legal terms. However, creating these documents is often a
slow and complex process. It usually involves multiple people, takes several
days, and leaves room for errors or outdated content. This paper introduces a
new AI-driven automation system that makes the entire SOW drafting process
faster, easier, and more accurate. Instead of relying completely on humans, the
system uses three intelligent components or 'agents' that each handle a part of
the job. One agent writes the first draft, another checks if everything is
legally correct, and the third agent formats the document and ensures
everything is in order. Unlike basic online tools that just fill in templates,
this system understands the meaning behind the content and customizes the SOW
to match the needs of the project. It also checks legal compliance and
formatting so that users can trust the result. The system was tested using real
business examples. It was able to create a full SOW in under three minutes,
compared to several hours or days using manual methods. It also performed well
in accuracy and quality, showing that it can reduce legal risks and save a lot
of time. This solution shows how artificial intelligence can be used to support
legal and business professionals by taking care of routine work and helping
them focus on more important decisions. It's a step toward making legal
processes smarter, faster, and more reliable.

</details>


### [213] [Toward Goal-Oriented Communication in Multi-Agent Systems: An overview](https://arxiv.org/abs/2508.07720)
*Themistoklis Charalambous,Nikolaos Pappas,Nikolaos Nomikos,Risto Wichman*

Main category: cs.MA

TL;DR: A survey on goal-oriented communication in multi-agent systems, highlighting its importance over traditional methods and exploring its applications and challenges.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of traditional communication paradigms in MAS by focusing on task-relevant information for shared objectives.

Method: Comprehensive review bridging information theory, communication theory, and machine learning, with emphasis on learning-based approaches and emergent protocols.

Result: Identifies key applications in swarm robotics, federated learning, and edge computing, and outlines coordination strategies under constraints.

Conclusion: Highlights open challenges and future research directions integrating communication theory, machine learning, and multi-agent decision making.

Abstract: As multi-agent systems (MAS) become increasingly prevalent in autonomous
systems, distributed control, and edge intelligence, efficient communication
under resource constraints has emerged as a critical challenge. Traditional
communication paradigms often emphasize message fidelity or bandwidth
optimization, overlooking the task relevance of the exchanged information. In
contrast, goal-oriented communication prioritizes the importance of information
with respect to the agents' shared objectives. This review provides a
comprehensive survey of goal-oriented communication in MAS, bridging
perspectives from information theory, communication theory, and machine
learning. We examine foundational concepts alongside learning-based approaches
and emergent protocols. Special attention is given to coordination under
communication constraints, as well as applications in domains such as swarm
robotics, federated learning, and edge computing. The paper concludes with a
discussion of open challenges and future research directions at the
intersection of communication theory, machine learning, and multi-agent
decision making.

</details>


### [214] [Multi-agent systems for chemical engineering: A review and perspective](https://arxiv.org/abs/2508.07880)
*Sophia Rupprecht,Qinghe Gao,Tanuj Karia,Artur M. Schweidtmann*

Main category: cs.MA

TL;DR: LLM-based multi-agent systems (MASs) are emerging as a transformative tool in chemical engineering, enabling collaborative workflows but facing challenges like architecture design and safety.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of MASs in transforming chemical engineering by decomposing complex workflows into specialized, collaborative agents.

Method: A review of the state-of-the-art MAS applications in chemical engineering, highlighting current capabilities and limitations.

Result: Early studies show promise, but challenges remain in architecture, data integration, domain-specific models, and safety.

Conclusion: MASs offer exciting opportunities to innovate chemical engineering workflows, though further research is needed to address existing challenges.

Abstract: Large language model (LLM)-based multi-agent systems (MASs) are a recent but
rapidly evolving technology with the potential to transform chemical
engineering by decomposing complex workflows into teams of collaborative agents
with specialized knowledge and tools. This review surveys the state-of-the-art
of MAS within chemical engineering. While early studies demonstrate promising
results, scientific challenges remain, including the design of tailored
architectures, integration of heterogeneous data modalities, development of
foundation models with domain-specific modalities, and strategies for ensuring
transparency, safety, and environmental impact. As a young but fast-moving
field, MASs offer exciting opportunities to rethink chemical engineering
workflows.

</details>
