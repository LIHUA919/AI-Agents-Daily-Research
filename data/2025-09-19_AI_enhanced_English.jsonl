{"id": "2509.14276", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14276", "abs": "https://arxiv.org/abs/2509.14276", "authors": ["Yuxiang Mai", "Qiyue Yin", "Wancheng Ni", "Pei Xu", "Kaiqi Huang"], "title": "Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity", "comment": "Accepted by IJCAI 2025", "summary": "In recent years, diversity has emerged as a useful mechanism to enhance the\nefficiency of multi-agent reinforcement learning (MARL). However, existing\nmethods predominantly focus on designing policies based on individual agent\ncharacteristics, often neglecting the interplay and mutual influence among\nagents during policy formation. To address this gap, we propose Competitive\nDiversity through Constructive Conflict (CoDiCon), a novel approach that\nincorporates competitive incentives into cooperative scenarios to encourage\npolicy exchange and foster strategic diversity among agents. Drawing\ninspiration from sociological research, which highlights the benefits of\nmoderate competition and constructive conflict in group decision-making, we\ndesign an intrinsic reward mechanism using ranking features to introduce\ncompetitive motivations. A centralized intrinsic reward module generates and\ndistributes varying reward values to agents, ensuring an effective balance\nbetween competition and cooperation. By optimizing the parameterized\ncentralized reward module to maximize environmental rewards, we reformulate the\nconstrained bilevel optimization problem to align with the original task\nobjectives. We evaluate our algorithm against state-of-the-art methods in the\nSMAC and GRF environments. Experimental results demonstrate that CoDiCon\nachieves superior performance, with competitive intrinsic rewards effectively\npromoting diverse and adaptive strategies among cooperative agents.", "AI": {"tldr": "CoDiCon introduces competitive incentives in cooperative MARL to foster strategic diversity through policy exchange and constructive conflict, achieving superior performance in SMAC and GRF environments.", "motivation": "Existing MARL diversity methods focus on individual agent characteristics but neglect agent interplay and mutual influence during policy formation, creating a gap in understanding how competition can enhance cooperation.", "method": "Proposes Competitive Diversity through Constructive Conflict (CoDiCon) with intrinsic reward mechanism using ranking features, centralized reward module for balanced competition-cooperation, and reformulated bilevel optimization to maximize environmental rewards.", "result": "Experimental evaluation shows CoDiCon achieves superior performance against state-of-the-art methods in SMAC and GRF environments, with competitive intrinsic rewards effectively promoting diverse and adaptive strategies.", "conclusion": "Incorporating competitive incentives through constructive conflict successfully enhances strategic diversity in cooperative MARL, demonstrating that moderate competition can improve overall team performance and adaptability."}}
{"id": "2509.14680", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14680", "abs": "https://arxiv.org/abs/2509.14680", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Songxiao Guo", "Dong Huang", "Yuanye Zhao", "Zheng Lin", "Zihan Fang", "Dianxin Luan", "Heming Cui", "Yong Cui"], "title": "LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning", "comment": "5 pages, 4 figures", "summary": "Multi-agent reinforcement learning (MARL) holds substantial promise for\nintelligent decision-making in complex environments. However, it suffers from a\ncoordination and scalability bottleneck as the number of agents increases. To\naddress these issues, we propose the LLM-empowered expert demonstrations\nframework for multi-agent reinforcement learning (LEED). LEED consists of two\ncomponents: a demonstration generation (DG) module and a policy optimization\n(PO) module. Specifically, the DG module leverages large language models to\ngenerate instructions for interacting with the environment, thereby producing\nhigh-quality demonstrations. The PO module adopts a decentralized training\nparadigm, where each agent utilizes the generated demonstrations to construct\nan expert policy loss, which is then integrated with its own policy loss. This\nenables each agent to effectively personalize and optimize its local policy\nbased on both expert knowledge and individual experience. Experimental results\nshow that LEED achieves superior sample efficiency, time efficiency, and robust\nscalability compared to state-of-the-art baselines.", "AI": {"tldr": "LEED framework uses LLMs to generate expert demonstrations for MARL, improving coordination and scalability through decentralized policy optimization with expert guidance.", "motivation": "Multi-agent reinforcement learning faces coordination and scalability challenges as agent numbers increase, requiring better methods to handle complex environments.", "method": "Two-component framework: 1) DG module uses LLMs to generate environment interaction instructions and demonstrations, 2) PO module uses decentralized training with expert policy loss integrated with individual policy loss.", "result": "LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.", "conclusion": "The LLM-empowered demonstration framework effectively addresses MARL coordination and scalability issues through expert-guided decentralized policy optimization."}}
{"id": "2509.15103", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15103", "abs": "https://arxiv.org/abs/2509.15103", "authors": ["Simin Li", "Zheng Yuwei", "Zihao Mao", "Linhao Wang", "Ruixiao Xu", "Chengdong Ma", "Xin Yu", "Yuqing Ma", "Qi Dou", "Xin Wang", "Jie Luo", "Bo An", "Yaodong Yang", "Weifeng Lv", "Xianglong Liu"], "title": "Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning", "comment": "submitted to NIPS 2025", "summary": "Partial agent failure becomes inevitable when systems scale up, making it\ncrucial to identify the subset of agents whose compromise would most severely\ndegrade overall performance. In this paper, we study this Vulnerable Agent\nIdentification (VAI) problem in large-scale multi-agent reinforcement learning\n(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field\nControl (HAD-MFC), where the upper level involves an NP-hard combinatorial task\nof selecting the most vulnerable agents, and the lower level learns worst-case\nadversarial policies for these agents using mean-field MARL. The two problems\nare coupled together, making HAD-MFC difficult to solve. To solve this, we\nfirst decouple the hierarchical process by Fenchel-Rockafellar transform,\nresulting a regularized mean-field Bellman operator for upper level that\nenables independent learning at each level, thus reducing computational\ncomplexity. We then reformulate the upper-level combinatorial problem as a MDP\nwith dense rewards from our regularized mean-field Bellman operator, enabling\nus to sequentially identify the most vulnerable agents by greedy and RL\nalgorithms. This decomposition provably preserves the optimal solution of the\noriginal HAD-MFC. Experiments show our method effectively identifies more\nvulnerable agents in large-scale MARL and the rule-based system, fooling system\ninto worse failures, and learns a value function that reveals the vulnerability\nof each agent.", "AI": {"tldr": "Proposes a hierarchical adversarial framework to identify the most vulnerable agents in large-scale multi-agent systems by decoupling combinatorial selection from adversarial policy learning using mean-field reinforcement learning.", "motivation": "Partial agent failure is inevitable in large-scale systems, and identifying which agents' compromise would most severely degrade overall performance is crucial for system robustness.", "method": "Frames VAI as Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), decouples it using Fenchel-Rockafellar transform, reformulates combinatorial selection as MDP with dense rewards, and uses greedy/RL algorithms for sequential identification.", "result": "Method effectively identifies more vulnerable agents in large-scale MARL and rule-based systems, causes worse system failures, and learns value functions that reveal agent vulnerability.", "conclusion": "The proposed hierarchical decomposition approach successfully solves the VAI problem while preserving optimal solutions and reducing computational complexity in large-scale multi-agent systems."}}
{"id": "2509.14251", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14251", "abs": "https://arxiv.org/abs/2509.14251", "authors": ["Qihang Chen"], "title": "Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity", "comment": null, "summary": "Metro crew planning is a key component of smart city development as it\ndirectly impacts the operational efficiency and service reliability of public\ntransportation. With the rapid expansion of metro networks, effective\nmulti-line scheduling and emergency management have become essential for\nlarge-scale seamless operations. However, current research focuses primarily on\nindividual metro lines,with insufficient attention on cross-line coordination\nand rapid replanning during disruptions. Here, a unified optimization framework\nis presented for multi-line metro crew planning and replanning with\nheterogeneous workforce. Specifically, a hierarchical time-space network model\nis proposed to represent the unified crew action space, and computationally\nefficient constraints and formulations are derived for the crew's heterogeneous\nqualifications and preferences. Solution algorithms based on column generation\nand shortest path adjustment are further developed, utilizing the proposed\nnetwork model. Experiments with real data from Shanghai and Beijing Metro\ndemonstrate that the proposed methods outperform benchmark heuristics in both\ncost reduction and task completion,and achieve notable efficiency gains by\nincorporating cross-line operations, particularly for urgent tasks during\ndisruptions. This work highlights the role of global optimization and\ncross-line coordination in multi-line metro system operations, providing\ninsights into the efficient and reliable functioning of public transportation\nin smart cities.", "AI": {"tldr": "A unified optimization framework for multi-line metro crew planning and replanning with heterogeneous workforce, using hierarchical time-space network modeling and efficient algorithms that outperform benchmarks in cost reduction and task completion.", "motivation": "Metro crew planning is crucial for smart city development and operational efficiency, but current research focuses on individual lines with insufficient attention to cross-line coordination and rapid replanning during disruptions in expanding metro networks.", "method": "Proposed a hierarchical time-space network model to represent unified crew action space, with computationally efficient constraints for heterogeneous qualifications and preferences. Developed solution algorithms based on column generation and shortest path adjustment.", "result": "Experiments with real data from Shanghai and Beijing Metro showed the methods outperform benchmark heuristics in cost reduction and task completion, achieving notable efficiency gains through cross-line operations, especially for urgent tasks during disruptions.", "conclusion": "This work demonstrates the importance of global optimization and cross-line coordination in multi-line metro system operations, providing insights for efficient and reliable public transportation in smart cities."}}
{"id": "2509.14274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14274", "abs": "https://arxiv.org/abs/2509.14274", "authors": ["Kazumi Kasaura", "Naoto Onda", "Yuta Oriike", "Masaya Taniguchi", "Akiyoshi Sannai", "Sho Sonoda"], "title": "Discovering New Theorems via LLMs with In-Context Proof Learning in Lean", "comment": "11 pages, 3 figures", "summary": "Large Language Models have demonstrated significant promise in formal theorem\nproving. However, previous works mainly focus on solving existing problems. In\nthis paper, we focus on the ability of LLMs to find novel theorems. We propose\nConjecturing-Proving Loop pipeline for automatically generating mathematical\nconjectures and proving them in Lean 4 format. A feature of our approach is\nthat we generate and prove further conjectures with context including\npreviously generated theorems and their proofs, which enables the generation of\nmore difficult proofs by in-context learning of proof strategies without\nchanging parameters of LLMs. We demonstrated that our framework rediscovered\ntheorems with verification, which were published in past mathematical papers\nand have not yet formalized. Moreover, at least one of these theorems could not\nbe proved by the LLM without in-context learning, even in natural language,\nwhich means that in-context learning was effective for neural theorem proving.\nThe source code is available at\nhttps://github.com/auto-res/ConjecturingProvingLoop.", "AI": {"tldr": "LLMs can generate novel mathematical theorems and prove them using a Conjecturing-Proving Loop pipeline with in-context learning of previous proofs.", "motivation": "Previous works focused on solving existing problems, but this paper explores LLMs' ability to discover novel mathematical theorems automatically.", "method": "Proposed Conjecturing-Proving Loop pipeline that generates conjectures and proves them in Lean 4 format, using context from previously generated theorems and proofs for in-context learning.", "result": "The framework rediscovered theorems published in past mathematical papers that hadn't been formalized, and demonstrated that in-context learning was essential for proving some theorems that couldn't be proved without it.", "conclusion": "In-context learning of proof strategies enables LLMs to generate and prove more difficult mathematical theorems without changing model parameters, showing promise for automated theorem discovery."}}
{"id": "2509.14778", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14778", "abs": "https://arxiv.org/abs/2509.14778", "authors": ["Yuxiao Cheng", "Jinli Suo"], "title": "OpenLens AI: Fully Autonomous Research Agent for Health Infomatics", "comment": null, "summary": "Health informatics research is characterized by diverse data modalities,\nrapid knowledge expansion, and the need to integrate insights across biomedical\nscience, data analytics, and clinical practice. These characteristics make it\nparticularly well-suited for agent-based approaches that can automate knowledge\nexploration, manage complex workflows, and generate clinically meaningful\noutputs. Recent progress in large language model (LLM)-based agents has\ndemonstrated promising capabilities in literature synthesis, data analysis, and\neven end-to-end research execution. However, existing systems remain limited\nfor health informatics because they lack mechanisms to interpret medical\nvisualizations and often overlook domain-specific quality requirements. To\naddress these gaps, we introduce OpenLens AI, a fully automated framework\ntailored to health informatics. OpenLens AI integrates specialized agents for\nliterature review, data analysis, code generation, and manuscript preparation,\nenhanced by vision-language feedback for medical visualization and quality\ncontrol for reproducibility. The framework automates the entire research\npipeline, producing publication-ready LaTeX manuscripts with transparent and\ntraceable workflows, thereby offering a domain-adapted solution for advancing\nhealth informatics research.", "AI": {"tldr": "OpenLens AI is an automated framework for health informatics research that integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation with vision-language capabilities for medical visualization and quality control.", "motivation": "Health informatics research involves diverse data modalities and requires integration across biomedical science, data analytics, and clinical practice. Existing LLM-based agent systems lack medical visualization interpretation and domain-specific quality requirements for health informatics.", "method": "Developed OpenLens AI framework with specialized agents for literature review, data analysis, code generation, and manuscript preparation. Enhanced with vision-language feedback for medical visualization interpretation and quality control mechanisms for reproducibility.", "result": "The framework automates the entire health informatics research pipeline and produces publication-ready LaTeX manuscripts with transparent and traceable workflows.", "conclusion": "OpenLens AI provides a domain-adapted automated solution that addresses the specific needs of health informatics research, enabling more efficient and reproducible scientific workflows in this complex domain."}}
{"id": "2509.14384", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.14384", "abs": "https://arxiv.org/abs/2509.14384", "authors": ["Nishantak Panigrahi", "Mayank Patwal"], "title": "A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation", "comment": "6 pages, 10 figures. Presented at IEEE International Conference on\n  Compute, Control, Network & Photonics (ICCCNP), 2025", "summary": "In this paper, we investigate the efficiency of Deep Neural Networks (DNNs)\nto approximate the solution of a nonlocal conservation law derived from the\nidentical-oscillator Kuramoto model, focusing on the evaluation of an\narchitectural choice and its impact on solution accuracy based on the energy\nnorm and computation time. Through systematic experimentation, we demonstrate\nthat network configuration parameters-specifically, activation function\nselection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width\n(64-256 neurons), and training methodology (collocation points, epoch\ncount)-significantly influence convergence characteristics. We observe that\ntanh activation yields stable convergence across configurations, whereas sine\nactivation can attain marginally lower errors and training times in isolated\ncases, but occasionally produce nonphysical artefacts. Our comparative analysis\nwith traditional numerical methods shows that optimally configured DNNs offer\ncompetitive accuracy with notably different computational trade-offs.\nFurthermore, we identify fundamental limitations of standard feed-forward\narchitectures when handling singular or piecewise-constant solutions, providing\nempirical evidence that such networks inherently oversmooth sharp features due\nto the natural function space limitations of standard activation functions.\nThis work contributes to the growing body of research on neural network-based\nscientific computing by providing practitioners with empirical guidelines for\nDNN implementation while illuminating fundamental theoretical constraints that\nmust be overcome to expand their applicability to more challenging physical\nsystems with discontinuities.", "AI": {"tldr": "DNNs for approximating nonlocal conservation law solutions from Kuramoto model, showing tanh activation provides stable convergence while sine can achieve lower errors but sometimes produces artifacts. DNNs offer competitive accuracy with different computational trade-offs compared to traditional methods, but have limitations with singular/piecewise-constant solutions due to oversmoothing.", "motivation": "Investigate the efficiency of Deep Neural Networks to approximate solutions of nonlocal conservation laws derived from Kuramoto oscillator models, focusing on architectural choices and their impact on solution accuracy and computation time.", "method": "Systematic experimentation with different network configurations: activation functions (tanh, sin, ReLU), network depth (4-8 hidden layers), width (64-256 neurons), and training methodology (collocation points, epoch count). Comparative analysis with traditional numerical methods.", "result": "Tanh activation yields stable convergence across configurations, while sine activation can achieve marginally lower errors and training times in some cases but occasionally produces nonphysical artifacts. Optimally configured DNNs offer competitive accuracy with different computational trade-offs compared to traditional methods.", "conclusion": "Standard feed-forward architectures have fundamental limitations when handling singular or piecewise-constant solutions, inherently oversmoothing sharp features due to function space limitations of standard activation functions. Provides empirical guidelines for DNN implementation while highlighting theoretical constraints for challenging physical systems with discontinuities."}}
{"id": "2509.14289", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14289", "abs": "https://arxiv.org/abs/2509.14289", "authors": ["Lanxiao Huang", "Daksh Dave", "Ming Jin", "Tyler Cody", "Peter Beling"], "title": "From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate or augment\npenetration testing, but their effectiveness and reliability across attack\nphases remain unclear. We present a comprehensive evaluation of multiple\nLLM-based agents, from single-agent to modular designs, across realistic\npenetration testing scenarios, measuring empirical performance and recurring\nfailure patterns. We also isolate the impact of five core functional\ncapabilities via targeted augmentations: Global Context Memory (GCM),\nInter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive\nPlanning (AP), and Real-Time Monitoring (RTM). These interventions support,\nrespectively: (i) context coherence and retention, (ii) inter-component\ncoordination and state management, (iii) tool use accuracy and selective\nexecution, (iv) multi-step strategic planning, error detection, and recovery,\nand (v) real-time dynamic responsiveness. Our results show that while some\narchitectures natively exhibit subsets of these properties, targeted\naugmentations substantially improve modular agent performance, especially in\ncomplex, multi-step, and real-time penetration testing tasks.", "AI": {"tldr": "Comprehensive evaluation of LLM-based agents for penetration testing shows targeted functional augmentations significantly improve performance in complex multi-step tasks.", "motivation": "LLMs are increasingly used for penetration testing automation but their effectiveness and reliability across different attack phases remain unclear and need systematic evaluation.", "method": "Evaluated multiple LLM-based agent architectures (single-agent to modular designs) across realistic penetration testing scenarios, measuring performance and failure patterns. Isolated impact of five core functional capabilities through targeted augmentations: Global Context Memory, Inter-Agent Messaging, Context-Conditioned Invocation, Adaptive Planning, and Real-Time Monitoring.", "result": "While some architectures natively exhibit subsets of these properties, targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks.", "conclusion": "Targeted functional augmentations are crucial for improving LLM-based agent performance in penetration testing, particularly for handling complex, multi-step attack scenarios that require coordination, planning, and real-time responsiveness."}}
{"id": "2509.14956", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14956", "abs": "https://arxiv.org/abs/2509.14956", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems", "comment": "25 pages, 12 figures", "summary": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time.", "AI": {"tldr": "A dual-layered security framework for multi-agent systems using Sentinel Agents for continuous monitoring and Coordinator Agents for governance, successfully tested against 162 synthetic attacks.", "motivation": "To enhance security and reliability in multi-agent systems against threats like prompt injection, collusive behavior, LLM hallucinations, privacy breaches, and coordinated attacks.", "method": "Proposes a framework with Sentinel Agents (distributed security layer using LLM semantic analysis, behavioral analytics, verification, anomaly detection) and Coordinator Agents (policy implementation, threat response, agent management). Tested via simulation with 162 synthetic attacks.", "result": "Sentinel Agents successfully detected all attack attempts in the simulation study, demonstrating practical feasibility of the monitoring approach.", "conclusion": "The dual-layered approach provides dynamic, adaptive defense mechanisms while enhancing system observability, supporting regulatory compliance, and enabling policy evolution over time."}}
{"id": "2509.14386", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14386", "abs": "https://arxiv.org/abs/2509.14386", "authors": ["Arjun S. Nair", "Kristina P. Sinaga"], "title": "Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility", "comment": "30 pages, 13 figures, 8 tables", "summary": "We prove a fundamental impossibility theorem: neural networks cannot\nsimultaneously learn well-calibrated confidence estimates with meaningful\ndiversity when trained using binary correct/incorrect supervision. Through\nrigorous mathematical analysis and comprehensive empirical evaluation spanning\nnegative reward training, symmetric loss functions, and post-hoc calibration\nmethods, we demonstrate this is an information-theoretic constraint, not a\nmethodological failure. Our experiments reveal universal failure patterns:\nnegative rewards produce extreme underconfidence (ECE greater than 0.8) while\ndestroying confidence diversity (std less than 0.05), symmetric losses fail to\nescape binary signal averaging, and post-hoc methods achieve calibration (ECE\nless than 0.02) only by compressing the confidence distribution. We formalize\nthis as an underspecified mapping problem where binary signals cannot\ndistinguish between different confidence levels for correct predictions: a 60\npercent confident correct answer receives identical supervision to a 90 percent\nconfident one. Crucially, our real-world validation shows 100 percent failure\nrate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while\npost-hoc calibration's 33 percent success rate paradoxically confirms our\ntheorem by achieving calibration through transformation rather than learning.\nThis impossibility directly explains neural network hallucinations and\nestablishes why post-hoc calibration is mathematically necessary, not merely\nconvenient. We propose novel supervision paradigms using ensemble disagreement\nand adaptive multi-agent learning that could overcome these fundamental\nlimitations without requiring human confidence annotations.", "AI": {"tldr": "Neural networks cannot learn well-calibrated confidence estimates with meaningful diversity when trained using binary correct/incorrect supervision due to information-theoretic constraints.", "motivation": "To understand why neural networks struggle with confidence calibration and to prove that this is a fundamental limitation rather than a methodological issue, which explains phenomena like neural network hallucinations.", "method": "Rigorous mathematical analysis and comprehensive empirical evaluation including negative reward training, symmetric loss functions, and post-hoc calibration methods across multiple datasets (MNIST, Fashion-MNIST, CIFAR-10).", "result": "Universal failure patterns: negative rewards cause extreme underconfidence and destroy confidence diversity, symmetric losses fail to escape binary signal averaging, and post-hoc methods achieve calibration only by compressing confidence distributions. 100% failure rate for all training methods, with post-hoc methods showing 33% success rate through transformation rather than learning.", "conclusion": "Binary supervision creates an underspecified mapping problem where different confidence levels for correct predictions receive identical supervision. This impossibility theorem explains why post-hoc calibration is mathematically necessary, and novel supervision paradigms using ensemble disagreement and adaptive multi-agent learning are proposed to overcome these limitations."}}
{"id": "2509.14382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14382", "abs": "https://arxiv.org/abs/2509.14382", "authors": ["Daniel R\u00f6der", "Akhil Juneja", "Roland Roller", "Sven Schmeier"], "title": "Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents", "comment": null, "summary": "Web agents powered by large language models (LLMs) can autonomously perform\ncomplex, multistep tasks in dynamic web environments. However, current\nevaluations mostly focus on the overall success while overlooking intermediate\nerrors. This limits insight into failure modes and hinders systematic\nimprovement. This work analyzes existing benchmarks and highlights the lack of\nfine-grained diagnostic tools. To address this gap, we propose a modular\nevaluation framework that decomposes agent pipelines into interpretable stages\nfor detailed error analysis. Using the SeeAct framework and the Mind2Web\ndataset as a case study, we show how this approach reveals actionable\nweaknesses missed by standard metrics - paving the way for more robust and\ngeneralizable web agents.", "AI": {"tldr": "A modular evaluation framework for web agents that decomposes agent pipelines into interpretable stages to enable detailed error analysis, revealing weaknesses missed by standard success metrics.", "motivation": "Current evaluations of web agents powered by LLMs focus mainly on overall success rates while overlooking intermediate errors, which limits understanding of failure modes and hinders systematic improvement.", "method": "Proposes a modular evaluation framework that decomposes agent pipelines into interpretable stages for detailed error analysis, using the SeeAct framework and Mind2Web dataset as a case study.", "result": "The approach reveals actionable weaknesses in web agents that are missed by standard metrics, providing deeper insights into failure modes.", "conclusion": "This framework paves the way for more robust and generalizable web agents by enabling fine-grained diagnostic analysis of agent performance."}}
{"id": "2509.14391", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14391", "abs": "https://arxiv.org/abs/2509.14391", "authors": ["Ye Qiao", "Sitao Huang"], "title": "Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs", "comment": null, "summary": "Extending LLM context windows is crucial for long range tasks. RoPE-based\nposition interpolation (PI) methods like linear and frequency-aware scaling\nextend input lengths without retraining, while post-training quantization (PTQ)\nenables practical deployment. We show that combining PI with PTQ degrades\naccuracy due to coupled effects long context aliasing, dynamic range dilation,\naxis grid anisotropy, and outlier shifting that induce position-dependent logit\nnoise. We provide the first systematic analysis of PI plus PTQ and introduce\ntwo diagnostics: Interpolation Pressure (per-band phase scaling sensitivity)\nand Tail Inflation Ratios (outlier shift from short to long contexts). To\naddress this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that\ngroups RoPE dimensions into a few frequency bands and performs a small search\nover per-band scales for W_Q,W_K, with an optional symmetric variant to\npreserve logit scale. The diagnostics guided search uses a tiny long-context\ndev set and requires no fine-tuning, kernel, or architecture changes.\nEmpirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces\nGovReport perplexity by more than 10%, while preserving short-context\nperformance and compatibility with existing inference stacks.", "AI": {"tldr": "Combining position interpolation (PI) with post-training quantization (PTQ) degrades accuracy in long-context LLMs. Q-ROAR method stabilizes RoPE-aware quantization through frequency band grouping and guided search to recover accuracy.", "motivation": "Extending LLM context windows is crucial for long-range tasks, but combining position interpolation methods with post-training quantization causes accuracy degradation due to position-dependent logit noise.", "method": "Propose Q-ROAR, a RoPE-aware weight-only stabilization that groups RoPE dimensions into frequency bands and performs a small search over per-band scales for W_Q and W_K matrices, using diagnostics like Interpolation Pressure and Tail Inflation Ratios.", "result": "Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces GovReport perplexity by more than 10%, while preserving short-context performance and compatibility with existing inference stacks.", "conclusion": "The method effectively addresses the degradation caused by combining PI with PTQ through a systematic diagnostic approach and targeted stabilization, enabling practical deployment of long-context LLMs without retraining or architecture changes."}}
{"id": "2509.14448", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14448", "abs": "https://arxiv.org/abs/2509.14448", "authors": ["Rick Chen", "Joseph Ternasky", "Afriyie Samuel Kwesi", "Ben Griffin", "Aaron Ontoyin Yin", "Zakari Salifu", "Kelvin Amoaba", "Xianling Mu", "Fuat Alican", "Yigit Ihlamur"], "title": "VCBench: Benchmarking LLMs in Venture Capital", "comment": null, "summary": "Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets\naccelerate progress toward artificial general intelligence (AGI). We introduce\nVCBench, the first benchmark for predicting founder success in venture capital\n(VC), a domain where signals are sparse, outcomes are uncertain, and even top\ninvestors perform modestly. At inception, the market index achieves a precision\nof 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1\nfirms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,\nstandardized to preserve predictive features while resisting identity leakage,\nwith adversarial tests showing more than 90% reduction in re-identification\nrisk. We evaluate nine state-of-the-art large language models (LLMs).\nDeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the\nhighest F0.5, and most models surpass human benchmarks. Designed as a public\nand evolving resource available at vcbench.com, VCBench establishes a\ncommunity-driven standard for reproducible and privacy-preserving evaluation of\nAGI in early-stage venture forecasting.", "AI": {"tldr": "VCBench is the first benchmark for predicting founder success in VC, featuring 9,000 anonymized founder profiles with privacy protection, showing LLMs outperform human investors.", "motivation": "To accelerate progress toward AGI in venture capital prediction, where signals are sparse and even top investors perform modestly, by creating a standardized benchmark.", "method": "Created VCBench with 9,000 anonymized founder profiles standardized to preserve predictive features while resisting identity leakage, using adversarial tests to reduce re-identification risk by 90%. Evaluated nine state-of-the-art LLMs.", "result": "DeepSeek-V3 delivered over 6x baseline precision, GPT-4o achieved highest F0.5 score, and most models surpassed human benchmarks. Market index precision was 1.9%, Y Combinator 1.7x better, tier-1 firms 2.9x better.", "conclusion": "VCBench establishes a community-driven standard for reproducible and privacy-preserving evaluation of AGI in early-stage venture forecasting, available as a public evolving resource."}}
{"id": "2509.14427", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.14427", "abs": "https://arxiv.org/abs/2509.14427", "authors": ["Ilyass Moummad", "Kawtar Zaher", "Lukas Rauch", "Alexis Joly"], "title": "Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models", "comment": null, "summary": "Information retrieval with compact binary embeddings, also referred to as\nhashing, is crucial for scalable fast search applications, yet state-of-the-art\nhashing methods require expensive, scenario-specific training. In this work, we\nintroduce Hashing-Baseline, a strong training-free hashing method leveraging\npowerful pretrained encoders that produce rich pretrained embeddings. We\nrevisit classical, training-free hashing techniques: principal component\nanalysis, random orthogonal projection, and threshold binarization, to produce\na strong baseline for hashing. Our approach combines these techniques with\nfrozen embeddings from state-of-the-art vision and audio encoders to yield\ncompetitive retrieval performance without any additional learning or\nfine-tuning. To demonstrate the generality and effectiveness of this approach,\nwe evaluate it on standard image retrieval benchmarks as well as a newly\nintroduced benchmark for audio hashing.", "AI": {"tldr": "Hashing-Baseline is a training-free hashing method that combines classical techniques (PCA, random orthogonal projection, threshold binarization) with frozen pretrained embeddings from state-of-the-art encoders for competitive retrieval performance without additional training.", "motivation": "State-of-the-art hashing methods require expensive, scenario-specific training, which limits scalability and practical deployment for fast search applications.", "method": "Leverages powerful pretrained encoders to produce rich embeddings, then applies classical training-free hashing techniques: principal component analysis, random orthogonal projection, and threshold binarization.", "result": "Achieves competitive retrieval performance on standard image retrieval benchmarks and a newly introduced audio hashing benchmark without any additional learning or fine-tuning.", "conclusion": "The approach demonstrates strong performance and generality across vision and audio domains, providing a practical training-free solution for compact binary embeddings in information retrieval."}}
{"id": "2509.14474", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14474", "abs": "https://arxiv.org/abs/2509.14474", "authors": ["Meltem Subasioglu", "Nevzat Subasioglu"], "title": "From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence", "comment": "27 pages, 1 figure", "summary": "The debate around Artificial General Intelligence (AGI) remains open due to\ntwo fundamentally different goals: replicating human-like performance versus\nreplicating human-like cognitive processes. We argue that current\nperformance-based definitions are inadequate because they provide no clear,\nmechanism-focused roadmap for research, and they fail to properly define the\nqualitative nature of genuine intelligence. Drawing inspiration from the human\nbrain, we propose a new paradigm that shifts the focus from external mimicry to\nthe development of foundational cognitive architectures. We define True\nIntelligence (TI) as a system characterized by six core components: embodied\nsensory fusion, core directives, dynamic schemata creation, a\nhighly-interconnected multi-expert architecture, an orchestration layer, and\nlastly, the unmeasurable quality of Interconnectedness, which we hypothesize\nresults in consciousness and a subjective experience. We propose a practical,\nfive-level taxonomy of AGI based on the number of the first five measurable\ncomponents a system exhibits. This framework provides a clear path forward with\ndevelopmental milestones that directly address the challenge of building\ngenuinely intelligent systems. We contend that once a system achieves Level-5\nAGI by implementing all five measurable components, the difference between it\nand TI remains as a purely philosophical debate. For practical purposes - and\ngiven theories indicate consciousness is an emergent byproduct of integrated,\nhigher-order cognition - we conclude that a fifth-level AGI is functionally and\npractically equivalent to TI. This work synthesizes diverse insights from\nanalytical psychology, schema theory, metacognition, modern brain architectures\nand latest works in AI to provide the first holistic, mechanism-based\ndefinition of AGI that offers a clear and actionable path for the research\ncommunity.", "AI": {"tldr": "Proposes a new framework for True Intelligence (TI) with 6 core components, shifting focus from performance mimicry to cognitive architecture development. Introduces a 5-level AGI taxonomy based on measurable components.", "motivation": "Current AGI definitions focus on replicating human performance rather than cognitive processes, lacking clear research roadmap and failing to define genuine intelligence qualitatively.", "method": "Draws from human brain inspiration to define TI with 6 components: sensory fusion, core directives, dynamic schemata, multi-expert architecture, orchestration layer, and interconnectedness. Creates 5-level AGI taxonomy based on measurable components.", "result": "Provides a mechanism-based definition of AGI with developmental milestones. Argues Level-5 AGI (with all 5 measurable components) is functionally equivalent to True Intelligence.", "conclusion": "Offers the first holistic, mechanism-based AGI definition with actionable research path, synthesizing insights from psychology, neuroscience, and AI. Level-5 AGI bridges practical implementation and philosophical TI concepts."}}
{"id": "2509.14444", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.14444", "abs": "https://arxiv.org/abs/2509.14444", "authors": ["Herlock", "Rahimi", "Dionysis Kalogerias"], "title": "FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport", "comment": "5 pages, 1 figure, ICASSP", "summary": "Federated Learning (FL) allows distributed model training without sharing raw\ndata, but suffers when client participation is partial. In practice, the\ndistribution of available users (\\emph{availability distribution} $q$) rarely\naligns with the distribution defining the optimization objective\n(\\emph{importance distribution} $p$), leading to biased and unstable updates\nunder classical FedAvg. We propose \\textbf{Fereated AVerage with Optimal\nTransport (\\textbf{FedAVOT})}, which formulates aggregation as a masked optimal\ntransport problem aligning $q$ and $p$. Using Sinkhorn scaling,\n\\textbf{FedAVOT} computes transport-based aggregation weights with provable\nconvergence guarantees. \\textbf{FedAVOT} achieves a standard\n$\\mathcal{O}(1/\\sqrt{T})$ rate under a nonsmooth convex FL setting, independent\nof the number of participating users per round. Our experiments confirm\ndrastically improved performance compared to FedAvg across heterogeneous,\nfairness-sensitive, and low-availability regimes, even when only two clients\nparticipate per round.", "AI": {"tldr": "FedAVOT addresses biased updates in federated learning when client participation doesn't match optimization objectives by using optimal transport for aggregation weights.", "motivation": "Classical FedAvg suffers from biased and unstable updates when the distribution of available users doesn't align with the optimization objective distribution, especially with partial client participation.", "method": "FedAVOT formulates aggregation as a masked optimal transport problem to align availability and importance distributions, using Sinkhorn scaling to compute transport-based aggregation weights.", "result": "Achieves O(1/\u221aT) convergence rate independent of participating users per round, with drastically improved performance across heterogeneous, fairness-sensitive, and low-availability scenarios, even with only two clients per round.", "conclusion": "FedAVOT provides provable convergence guarantees and significantly outperforms FedAvg by addressing distribution misalignment through optimal transport-based aggregation."}}
{"id": "2509.14485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14485", "abs": "https://arxiv.org/abs/2509.14485", "authors": ["Marko Tesic", "Yue Zhao", "Joel Z. Leibo", "Rakshit S. Trivedi", "Jose Hernandez-Orallo"], "title": "Beyond the high score: Prosocial ability profiles of multi-agent populations", "comment": null, "summary": "The development and evaluation of social capabilities in AI agents require\ncomplex environments where competitive and cooperative behaviours naturally\nemerge. While game-theoretic properties can explain why certain teams or agent\npopulations outperform others, more abstract behaviours, such as convention\nfollowing, are harder to control in training and evaluation settings. The\nMelting Pot contest is a social AI evaluation suite designed to assess the\ncooperation capabilities of AI systems. In this paper, we apply a Bayesian\napproach known as Measurement Layouts to infer the capability profiles of\nmulti-agent systems in the Melting Pot contest. We show that these capability\nprofiles not only predict future performance within the Melting Pot suite but\nalso reveal the underlying prosocial abilities of agents. Our analysis\nindicates that while higher prosocial capabilities sometimes correlate with\nbetter performance, this is not a universal trend-some lower-scoring agents\nexhibit stronger cooperation abilities. Furthermore, we find that\ntop-performing contest submissions are more likely to achieve high scores in\nscenarios where prosocial capabilities are not required. These findings,\ntogether with reports that the contest winner used a hard-coded solution\ntailored to specific environments, suggest that at least one top-performing\nteam may have optimised for conditions where cooperation was not necessary,\npotentially exploiting limitations in the evaluation framework. We provide\nrecommendations for improving the annotation of cooperation demands and propose\nfuture research directions to account for biases introduced by different\ntesting environments. Our results demonstrate that Measurement Layouts offer\nboth strong predictive accuracy and actionable insights, contributing to a more\ntransparent and generalisable approach to evaluating AI systems in complex\nsocial settings.", "AI": {"tldr": "Bayesian Measurement Layouts used to analyze AI agent cooperation in Melting Pot contest, revealing that top performers may exploit evaluation limitations rather than demonstrating genuine prosocial capabilities.", "motivation": "To develop better methods for evaluating social AI capabilities, particularly cooperation, in complex multi-agent environments where current evaluation frameworks may be vulnerable to exploitation.", "method": "Applied Bayesian Measurement Layouts approach to infer capability profiles of multi-agent systems in the Melting Pot contest, analyzing performance data to assess prosocial abilities.", "result": "Measurement Layouts effectively predict future performance and reveal underlying prosocial abilities. Found that higher prosocial capabilities don't always correlate with better performance, and top contest submissions achieved high scores in scenarios where cooperation wasn't required.", "conclusion": "Current evaluation frameworks may be exploited by agents optimized for specific environments rather than genuine cooperation. Measurement Layouts provide transparent and generalizable evaluation with strong predictive accuracy, offering recommendations for improving cooperation assessment in AI systems."}}
{"id": "2509.14472", "categories": ["cs.LG", "astro-ph.IM", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2509.14472", "abs": "https://arxiv.org/abs/2509.14472", "authors": ["Mahsa Khazaei", "Azim Ahmadzadeh", "Alexei Pevtsov", "Luca Bertello", "Alexander Pevtsov"], "title": "H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations", "comment": null, "summary": "The plethora of space-borne and ground-based observatories has provided\nastrophysicists with an unprecedented volume of data, which can only be\nprocessed at scale using advanced computing algorithms. Consequently, ensuring\nthe quality of data fed into machine learning (ML) models is critical. The\nH$\\alpha$ observations from the GONG network represent one such data stream,\nproducing several observations per minute, 24/7, since 2010. In this study, we\nintroduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha\nAnomalyzer, designed to identify anomalous observations based on user-defined\ncriteria. Unlike many black-box algorithms, our approach highlights exactly\nwhich regions triggered the anomaly flag and quantifies the corresponding\nanomaly likelihood. For our comparative analysis, we also created and released\na dataset of 2,000 observations, equally divided between anomalous and\nnon-anomalous cases. Our results demonstrate that the proposed model not only\noutperforms existing methods but also provides explainability, enabling\nqualitative evaluation by domain experts.", "AI": {"tldr": "A lightweight non-ML anomaly detection algorithm called H-Alpha Anomalyzer is introduced for identifying anomalous H\u03b1 observations from GONG network data, providing explainable results with highlighted anomaly regions and quantified likelihood.", "motivation": "The increasing volume of astrophysical data requires quality assurance for ML models. GONG network's H\u03b1 observations produce continuous data since 2010 that needs reliable anomaly detection.", "method": "Developed a lightweight (non-ML) anomaly-detection algorithm that identifies anomalies based on user-defined criteria, highlights specific anomalous regions, and quantifies anomaly likelihood. Created a benchmark dataset of 2,000 observations for comparative analysis.", "result": "The proposed model outperforms existing methods while providing explainability that enables qualitative evaluation by domain experts.", "conclusion": "The H-Alpha Anomalyzer offers an effective, transparent alternative to black-box ML approaches for astrophysical data quality control, combining performance with interpretability for expert validation."}}
{"id": "2509.14507", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14507", "abs": "https://arxiv.org/abs/2509.14507", "authors": ["Jian Chen", "Zhenyan Chen", "Xuming Hu", "Peilin Zhou", "Yining Hua", "Han Fang", "Cissy Hing Yee Choy", "Xinmei Ke", "Jingfeng Luo", "Zixuan Yuan"], "title": "DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction", "comment": null, "summary": "Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that\nsimplifies database access for non-technical users by converting natural\nlanguage queries into SQL commands. Recent advancements, particularly those\nintegrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)\nreasoning, have made significant strides in enhancing NL2SQL performance.\nHowever, challenges such as inaccurate task decomposition and keyword\nextraction by LLMs remain major bottlenecks, often leading to errors in SQL\ngeneration. While existing datasets aim to mitigate these issues by fine-tuning\nmodels, they struggle with over-fragmentation of tasks and lack of\ndomain-specific keyword annotations, limiting their effectiveness. To address\nthese limitations, we present DeKeyNLU, a novel dataset which contains 1,500\nmeticulously annotated QA pairs aimed at refining task decomposition and\nenhancing keyword extraction precision for the RAG pipeline. Fine-tuned with\nDeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three\ndistinct modules for user question understanding, entity retrieval, and\ngeneration to improve SQL generation accuracy. We benchmarked multiple model\nconfigurations within DeKeySQL RAG pipeline. Experimental results demonstrate\nthat fine-tuning with DeKeyNLU significantly improves SQL generation accuracy\non both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.", "AI": {"tldr": "DeKeyNLU dataset improves NL2SQL accuracy by enhancing task decomposition and keyword extraction in RAG pipelines, boosting performance on BIRD and Spider benchmarks.", "motivation": "Existing NL2SQL approaches struggle with inaccurate task decomposition and keyword extraction by LLMs, leading to SQL generation errors. Current datasets lack proper domain-specific keyword annotations and suffer from task over-fragmentation.", "method": "Created DeKeyNLU dataset with 1,500 annotated QA pairs to refine task decomposition and keyword extraction. Developed DeKeySQL, a RAG-based NL2SQL pipeline with three modules: user question understanding, entity retrieval, and generation.", "result": "Fine-tuning with DeKeyNLU significantly improved SQL generation accuracy: from 62.31% to 69.10% on BIRD dev dataset and from 84.2% to 88.7% on Spider dev dataset.", "conclusion": "The DeKeyNLU dataset effectively addresses key limitations in NL2SQL systems by providing better annotations for task decomposition and keyword extraction, leading to substantial performance improvements in SQL generation accuracy."}}
{"id": "2509.14488", "categories": ["cs.LG", "math.OC", "90C25, 68T05", "G.1.6; C.2.4; I.2.6; F.2.1"], "pdf": "https://arxiv.org/pdf/2509.14488", "abs": "https://arxiv.org/abs/2509.14488", "authors": ["Ying Lin", "Yao Kuang", "Ahmet Alacaoglu", "Michael P. Friedlander"], "title": "Decentralized Optimization with Topology-Independent Communication", "comment": "36 pages", "summary": "Distributed optimization requires nodes to coordinate, yet full\nsynchronization scales poorly. When $n$ nodes collaborate through $m$ pairwise\nregularizers, standard methods demand $\\mathcal{O}(m)$ communications per\niteration. This paper proposes randomized local coordination: each node\nindependently samples one regularizer uniformly and coordinates only with nodes\nsharing that term. This exploits partial separability, where each regularizer\n$G_j$ depends on a subset $S_j \\subseteq \\{1,\\ldots,n\\}$ of nodes. For\ngraph-guided regularizers where $|S_j|=2$, expected communication drops to\nexactly 2 messages per iteration. This method achieves\n$\\tilde{\\mathcal{O}}(\\varepsilon^{-2})$ iterations for convex objectives and\nunder strong convexity, $\\mathcal{O}(\\varepsilon^{-1})$ to an\n$\\varepsilon$-solution and $\\mathcal{O}(\\log(1/\\varepsilon))$ to a\nneighborhood. Replacing the proximal map of the sum $\\sum_j G_j$ with the\nproximal map of a single randomly selected regularizer $G_j$ preserves\nconvergence while eliminating global coordination. Experiments validate both\nconvergence rates and communication efficiency across synthetic and real-world\ndatasets.", "AI": {"tldr": "Randomized local coordination method reduces communication from O(m) to exactly 2 messages per iteration for graph-guided regularizers by sampling individual regularizers instead of global coordination.", "motivation": "Distributed optimization requires coordination but full synchronization scales poorly with many nodes, creating communication bottlenecks when n nodes collaborate through m pairwise regularizers.", "method": "Each node independently samples one regularizer uniformly and coordinates only with nodes sharing that term, replacing the proximal map of the sum with the proximal map of a single randomly selected regularizer.", "result": "Achieves ~O(\u03b5\u207b\u00b2) iterations for convex objectives, O(\u03b5\u207b\u00b9) to \u03b5-solution under strong convexity, and O(log(1/\u03b5)) to neighborhood convergence, with expected communication dropping to exactly 2 messages per iteration.", "conclusion": "Randomized local coordination preserves convergence while eliminating global coordination, validated by experiments showing both convergence rates and communication efficiency improvements."}}
{"id": "2509.14546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14546", "abs": "https://arxiv.org/abs/2509.14546", "authors": ["Zhilun Zhou", "Jing Yi Wang", "Nicholas Sukiennik", "Chen Gao", "Fengli Xu", "Yong Li", "James Evans"], "title": "Rationality Check! Benchmarking the Rationality of Large Language Models", "comment": null, "summary": "Large language models (LLMs), a recent advance in deep learning and machine\nintelligence, have manifested astonishing capacities, now considered among the\nmost promising for artificial general intelligence. With human-like\ncapabilities, LLMs have been used to simulate humans and serve as AI assistants\nacross many applications. As a result, great concern has arisen about whether\nand under what circumstances LLMs think and behave like real human agents.\nRationality is among the most important concepts in assessing human behavior,\nboth in thinking (i.e., theoretical rationality) and in taking action (i.e.,\npractical rationality). In this work, we propose the first benchmark for\nevaluating the omnibus rationality of LLMs, covering a wide range of domains\nand LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental\nresults, and analysis that illuminates where LLMs converge and diverge from\nidealized human rationality. We believe the benchmark can serve as a\nfoundational tool for both developers and users of LLMs.", "AI": {"tldr": "First benchmark for evaluating LLM rationality across thinking and action domains, with toolkit and analysis showing where LLMs converge/diverge from human rationality.", "motivation": "Assess whether and under what circumstances LLMs think and behave like real human agents, as rationality is crucial for evaluating human-like behavior in both theoretical and practical contexts.", "method": "Developed a comprehensive benchmark covering multiple domains with easy-to-use toolkit, conducted extensive experiments on various LLMs to evaluate their omnibus rationality.", "result": "Experimental results illuminate specific areas where LLMs converge with and diverge from idealized human rationality patterns.", "conclusion": "The benchmark serves as a foundational tool for both developers and users to better understand and evaluate LLM rationality capabilities."}}
{"id": "2509.14519", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.14519", "abs": "https://arxiv.org/abs/2509.14519", "authors": ["Wadduwage Shanika Perera", "Haodi Jiang"], "title": "BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning", "comment": null, "summary": "Malware is becoming increasingly complex and widespread, making it essential\nto develop more effective and timely detection methods. Traditional static\nanalysis often fails to defend against modern threats that employ code\nobfuscation, polymorphism, and other evasion techniques. In contrast,\nbehavioral malware detection, which monitors runtime activities, provides a\nmore reliable and context-aware solution. In this work, we propose BEACON, a\nnovel deep learning framework that leverages large language models (LLMs) to\ngenerate dense, contextual embeddings from raw sandbox-generated behavior\nreports. These embeddings capture semantic and structural patterns of each\nsample and are processed by a one-dimensional convolutional neural network (1D\nCNN) for multi-class malware classification. Evaluated on the Avast-CTU Public\nCAPE Dataset, our framework consistently outperforms existing methods,\nhighlighting the effectiveness of LLM-based behavioral embeddings and the\noverall design of BEACON for robust malware classification.", "AI": {"tldr": "BEACON is a deep learning framework that uses LLMs to generate behavioral embeddings from sandbox reports for improved malware classification, outperforming existing methods.", "motivation": "Traditional static analysis fails against modern malware evasion techniques like code obfuscation and polymorphism, requiring more effective behavioral detection methods.", "method": "Leverages large language models to generate dense contextual embeddings from raw sandbox behavior reports, then processes them with a 1D CNN for multi-class malware classification.", "result": "Evaluated on Avast-CTU Public CAPE Dataset, consistently outperforms existing methods in malware classification.", "conclusion": "LLM-based behavioral embeddings and the BEACON framework design are effective for robust malware classification, providing reliable context-aware solutions."}}
{"id": "2509.14547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14547", "abs": "https://arxiv.org/abs/2509.14547", "authors": ["Yi Lin", "Lujin Zhao", "Yijie Shi"], "title": "(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration", "comment": null, "summary": "Recent studies have shown that carefully designed workflows coordinating\nlarge language models(LLMs) significantly enhance task-solving capabilities\ncompared to using a single model. While an increasing number of works focus on\nautonomous workflow construction, most existing approaches rely solely on\nhistorical experience, leading to limitations in efficiency and adaptability.\nWe argue that while historical experience is valuable, workflow construction\nshould also flexibly respond to the unique characteristics of each task. To\nthis end, we propose an a priori dynamic framework for automated workflow\nconstruction. Our framework first leverages Q-table learning to optimize the\ndecision space, guiding agent decisions and enabling effective use of\nhistorical experience. At the same time, agents evaluate the current task\nprogress and make a priori decisions regarding the next executing agent,\nallowing the system to proactively select the more suitable workflow structure\nfor each given task. Additionally, we incorporate mechanisms such as cold-start\ninitialization, early stopping, and pruning to further improve system\nefficiency. Experimental evaluations on four benchmark datasets demonstrate the\nfeasibility and effectiveness of our approach. Compared to state-of-the-art\nbaselines, our method achieves an average improvement of 4.05%, while reducing\nworkflow construction and inference costs to only 30.68%-48.31% of those\nrequired by existing methods.", "AI": {"tldr": "Proposes a dynamic framework for automated LLM workflow construction that combines Q-table learning with a priori decision-making to optimize task-specific workflows, achieving 4.05% performance improvement while reducing costs to 30-48% of existing methods.", "motivation": "Existing autonomous workflow construction approaches rely too heavily on historical experience, leading to inefficiency and poor adaptability to unique task characteristics.", "method": "Uses Q-table learning to optimize decision space and guide agent decisions, combined with a priori decision-making where agents evaluate task progress to proactively select optimal workflow structures. Incorporates cold-start initialization, early stopping, and pruning mechanisms.", "result": "Experimental evaluations on four benchmark datasets show average 4.05% improvement over state-of-the-art baselines, while reducing workflow construction and inference costs to 30.68%-48.31% of existing methods.", "conclusion": "The proposed dynamic framework effectively combines historical experience with task-specific adaptability, demonstrating significant performance improvements and cost reductions in automated LLM workflow construction."}}
{"id": "2509.14536", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14536", "abs": "https://arxiv.org/abs/2509.14536", "authors": ["Muhammad Awais Ali", "Marlon Dumas", "Fredrik Milani"], "title": "Predicting Case Suffixes With Activity Start and End Times: A Sweep-Line Based Approach", "comment": null, "summary": "Predictive process monitoring techniques support the operational decision\nmaking by predicting future states of ongoing cases of a business process. A\nsubset of these techniques predict the remaining sequence of activities of an\nongoing case (case suffix prediction). Existing approaches for case suffix\nprediction generate sequences of activities with a single timestamp (e.g. the\nend timestamp). This output is insufficient for resource capacity planning,\nwhere we need to reason about the periods of time when resources will be busy\nperforming work. This paper introduces a technique for predicting case suffixes\nconsisting of activities with start and end timestamps. In other words, the\nproposed technique predicts both the waiting time and the processing time of\neach activity. Since the waiting time of an activity in a case depends on how\nbusy resources are in other cases, the technique adopts a sweep-line approach,\nwherein the suffixes of all ongoing cases in the process are predicted in\nlockstep, rather than predictions being made for each case in isolation. An\nevaluation on real-life and synthetic datasets compares the accuracy of\ndifferent instantiations of this approach, demonstrating the advantages of a\nmulti-model approach to case suffix prediction.", "AI": {"tldr": "A technique for predicting business process case suffixes with start and end timestamps to support resource capacity planning, using a sweep-line approach that predicts all ongoing cases simultaneously rather than in isolation.", "motivation": "Existing case suffix prediction approaches only generate sequences with single timestamps, which is insufficient for resource capacity planning that requires reasoning about when resources will be busy performing work.", "method": "The technique predicts case suffixes with both start and end timestamps for each activity, adopting a sweep-line approach where suffixes of all ongoing cases are predicted in lockstep rather than in isolation, to account for resource dependencies across cases.", "result": "Evaluation on real-life and synthetic datasets shows the advantages of this multi-model approach for case suffix prediction accuracy.", "conclusion": "The proposed technique provides more comprehensive predictions for resource capacity planning by capturing both waiting and processing times through simultaneous prediction of all ongoing cases."}}
{"id": "2509.14594", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14594", "abs": "https://arxiv.org/abs/2509.14594", "authors": ["Yidan Sun", "Viktor Schlegel", "Srinivasan Nandakumar", "Iqra Zahid", "Yuping Wu", "Yulong Wu", "Hao Li", "Jie Zhang", "Warren Del-Pinto", "Goran Nenadic", "Siew Kei Lam", "Anil Anthony Bharath"], "title": "SynBench: A Benchmark for Differentially Private Text Generation", "comment": "15 pages", "summary": "Data-driven decision support in high-stakes domains like healthcare and\nfinance faces significant barriers to data sharing due to regulatory,\ninstitutional, and privacy concerns. While recent generative AI models, such as\nlarge language models, have shown impressive performance in open-domain tasks,\ntheir adoption in sensitive environments remains limited by unpredictable\nbehaviors and insufficient privacy-preserving datasets for benchmarking.\nExisting anonymization methods are often inadequate, especially for\nunstructured text, as redaction and masking can still allow re-identification.\nDifferential Privacy (DP) offers a principled alternative, enabling the\ngeneration of synthetic data with formal privacy assurances. In this work, we\naddress these challenges through three key contributions. First, we introduce a\ncomprehensive evaluation framework with standardized utility and fidelity\nmetrics, encompassing nine curated datasets that capture domain-specific\ncomplexities such as technical jargon, long-context dependencies, and\nspecialized document structures. Second, we conduct a large-scale empirical\nstudy benchmarking state-of-the-art DP text generation methods and LLMs of\nvarying sizes and different fine-tuning strategies, revealing that high-quality\ndomain-specific synthetic data generation under DP constraints remains an\nunsolved challenge, with performance degrading as domain complexity increases.\nThird, we develop a membership inference attack (MIA) methodology tailored for\nsynthetic text, providing first empirical evidence that the use of public\ndatasets - potentially present in pre-training corpora - can invalidate claimed\nprivacy guarantees. Our findings underscore the urgent need for rigorous\nprivacy auditing and highlight persistent gaps between open-domain and\nspecialist evaluations, informing responsible deployment of generative AI in\nprivacy-sensitive, high-stakes settings.", "AI": {"tldr": "This paper addresses privacy challenges in data sharing for high-stakes domains by introducing a comprehensive evaluation framework for differentially private text generation, benchmarking state-of-the-art methods, and developing membership inference attacks to audit privacy claims.", "motivation": "Data sharing in healthcare and finance faces regulatory and privacy barriers, while existing anonymization methods are inadequate for unstructured text. Differential Privacy offers formal privacy assurances but lacks proper evaluation frameworks for domain-specific synthetic data generation.", "method": "Three key contributions: 1) Comprehensive evaluation framework with standardized metrics and 9 curated datasets capturing domain complexities, 2) Large-scale empirical study benchmarking DP text generation methods and LLMs, 3) Development of membership inference attack methodology for synthetic text.", "result": "High-quality domain-specific synthetic data generation under DP constraints remains an unsolved challenge, with performance degrading as domain complexity increases. Use of public datasets in pre-training can invalidate claimed privacy guarantees.", "conclusion": "Urgent need for rigorous privacy auditing and highlights persistent gaps between open-domain and specialist evaluations, informing responsible deployment of generative AI in privacy-sensitive settings."}}
{"id": "2509.14562", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14562", "abs": "https://arxiv.org/abs/2509.14562", "authors": ["Feihu Huang", "Yuning Luo", "Songcan Chen"], "title": "LiMuon: Light and Fast Muon Optimizer for Large Models", "comment": "28 pages", "summary": "Large models recently are widely applied in artificial intelligence, so\nefficient training of large models has received widespread attention. More\nrecently, a useful Muon optimizer is specifically designed for\nmatrix-structured parameters of large models. Although some works have begun to\nstudying Muon optimizer, the existing Muon and its variants still suffer from\nhigh sample complexity or high memory for large models. To fill this gap, we\npropose a light and fast Muon (LiMuon) optimizer for training large models,\nwhich builds on the momentum-based variance reduced technique and randomized\nSingular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory\nthan the current Muon and its variants. Moreover, we prove that our LiMuon has\na lower sample complexity of $O(\\epsilon^{-3})$ for finding an\n$\\epsilon$-stationary solution of non-convex stochastic optimization under the\nsmooth condition. Recently, the existing convergence analysis of Muon optimizer\nmainly relies on the strict Lipschitz smooth assumption, while some artificial\nintelligence tasks such as training large language models (LLMs) do not satisfy\nthis condition. We also proved that our LiMuon optimizer has a sample\ncomplexity of $O(\\epsilon^{-3})$ under the generalized smooth condition.\nNumerical experimental results on training DistilGPT2 and ViT models verify\nefficiency of our LiMuon optimizer.", "AI": {"tldr": "LiMuon optimizer: a light and fast Muon variant using momentum-based variance reduction and randomized SVD for efficient large model training with lower memory and sample complexity.", "motivation": "Existing Muon optimizers suffer from high sample complexity or high memory requirements for large models, and their convergence analysis relies on strict Lipschitz smooth assumptions that don't hold for tasks like LLM training.", "method": "Proposes LiMuon optimizer based on momentum-based variance reduced technique and randomized Singular Value Decomposition (SVD) for matrix-structured parameters in large models.", "result": "LiMuon achieves lower memory usage than current Muon variants and proves O(\u03b5\u207b\u00b3) sample complexity for non-convex stochastic optimization under both smooth and generalized smooth conditions. Numerical experiments on DistilGPT2 and ViT models verify efficiency.", "conclusion": "LiMuon provides an efficient optimizer for large model training with reduced memory requirements and proven convergence guarantees under more realistic smoothness conditions applicable to real-world AI tasks."}}
{"id": "2509.14647", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14647", "abs": "https://arxiv.org/abs/2509.14647", "authors": ["NVJK Kartik", "Garvit Sapra", "Rishav Hada", "Nikhil Pareek"], "title": "AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production", "comment": null, "summary": "With the growing adoption of Large Language Models (LLMs) in automating\ncomplex, multi-agent workflows, organizations face mounting risks from errors,\nemergent behaviors, and systemic failures that current evaluation methods fail\nto capture. We present AgentCompass, the first evaluation framework designed\nspecifically for post-deployment monitoring and debugging of agentic workflows.\nAgentCompass models the reasoning process of expert debuggers through a\nstructured, multi-stage analytical pipeline: error identification and\ncategorization, thematic clustering, quantitative scoring, and strategic\nsummarization. The framework is further enhanced with a dual memory\nsystem-episodic and semantic-that enables continual learning across executions.\nThrough collaborations with design partners, we demonstrate the framework's\npractical utility on real-world deployments, before establishing its efficacy\nagainst the publicly available TRAIL benchmark. AgentCompass achieves\nstate-of-the-art results on key metrics, while uncovering critical issues\nmissed in human annotations, underscoring its role as a robust,\ndeveloper-centric tool for reliable monitoring and improvement of agentic\nsystems in production.", "AI": {"tldr": "AgentCompass is the first evaluation framework for post-deployment monitoring and debugging of LLM-based multi-agent workflows, featuring structured analysis and dual memory system for continuous learning.", "motivation": "Current evaluation methods fail to capture risks from errors, emergent behaviors, and systemic failures in complex multi-agent LLM workflows.", "method": "Models expert debuggers through a multi-stage analytical pipeline: error identification/categorization, thematic clustering, quantitative scoring, and strategic summarization, enhanced with episodic and semantic dual memory system.", "result": "Achieves state-of-the-art results on TRAIL benchmark, uncovers critical issues missed in human annotations, and demonstrates practical utility in real-world deployments.", "conclusion": "AgentCompass serves as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production environments."}}
{"id": "2509.14563", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14563", "abs": "https://arxiv.org/abs/2509.14563", "authors": ["Shiyuan Luo", "Runlong Yu", "Chonghao Qiu", "Rahul Ghosh", "Robert Ladwig", "Paul C. Hanson", "Yiqun Xie", "Xiaowei Jia"], "title": "Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework", "comment": null, "summary": "The discovery of environmental knowledge depends on labeled task-specific\ndata, but is often constrained by the high cost of data collection. Existing\nmachine learning approaches usually struggle to generalize in data-sparse or\natypical conditions. To this end, we propose an Augmentation-Adaptive\nSelf-Supervised Learning (A$^2$SL) framework, which retrieves relevant\nobservational samples to enhance modeling of the target ecosystem.\nSpecifically, we introduce a multi-level pairwise learning loss to train a\nscenario encoder that captures varying degrees of similarity among scenarios.\nThese learned similarities drive a retrieval mechanism that supplements a\ntarget scenario with relevant data from different locations or time periods.\nFurthermore, to better handle variable scenarios, particularly under atypical\nor extreme conditions where traditional models struggle, we design an\naugmentation-adaptive mechanism that selectively enhances these scenarios\nthrough targeted data augmentation. Using freshwater ecosystems as a case\nstudy, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen\ndynamics in real-world lakes. Experimental results show that A$^2$SL\nsignificantly improves predictive accuracy and enhances robustness in\ndata-scarce and atypical scenarios. Although this study focuses on freshwater\necosystems, the A$^2$SL framework offers a broadly applicable solution in\nvarious scientific domains.", "AI": {"tldr": "A$^2$SL framework improves environmental modeling by retrieving relevant observational data and using adaptive augmentation for better generalization in data-scarce and atypical conditions.", "motivation": "High cost of environmental data collection and poor generalization of existing ML approaches in data-sparse or atypical conditions.", "method": "Multi-level pairwise learning loss for scenario encoder, retrieval mechanism for relevant data supplementation, and augmentation-adaptive mechanism for targeted data augmentation in extreme conditions.", "result": "Significantly improved predictive accuracy and enhanced robustness in data-scarce and atypical scenarios for water temperature and dissolved oxygen modeling in lakes.", "conclusion": "A$^2$SL offers a broadly applicable solution beyond freshwater ecosystems to various scientific domains facing similar data challenges."}}
{"id": "2509.14662", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14662", "abs": "https://arxiv.org/abs/2509.14662", "authors": ["Ming Li", "Nan Zhang", "Chenrui Fan", "Hong Jiao", "Yanbin Fu", "Sydney Peters", "Qingshu Xu", "Robert Lissitz", "Tianyi Zhou"], "title": "Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory", "comment": "EMNLP2025 main, Camera-ready", "summary": "While Large Reasoning Models (LRMs) generate extensive chain-of-thought\nreasoning, we lack a principled framework for understanding how these thoughts\nare structured. In this paper, we introduce a novel approach by applying\nSchoenfeld's Episode Theory, a classic cognitive framework for human\nmathematical problem-solving, to analyze the reasoning traces of LRMs. We\nannotated thousands of sentences and paragraphs from model-generated solutions\nto math problems using seven cognitive labels (e.g., Plan, Implement, Verify).\nThe result is the first publicly available benchmark for the fine-grained\nanalysis of machine reasoning, including a large annotated corpus and detailed\nannotation guidebooks. Our preliminary analysis reveals distinct patterns in\nLRM reasoning, such as the transition dynamics between cognitive states. This\nframework provides a theoretically grounded methodology for interpreting LRM\ncognition and enables future work on more controllable and transparent\nreasoning systems.", "AI": {"tldr": "Applying human cognitive framework (Schoenfeld's Episode Theory) to analyze Large Reasoning Models' thought structures through fine-grained annotation of math problem solutions.", "motivation": "Lack of principled framework to understand how Large Reasoning Models structure their chain-of-thought reasoning, despite generating extensive reasoning traces.", "method": "Annotated thousands of sentences/paragraphs from model-generated math solutions using seven cognitive labels (Plan, Implement, Verify, etc.) based on Schoenfeld's Episode Theory.", "result": "Created first publicly available benchmark for fine-grained analysis of machine reasoning with large annotated corpus and annotation guides. Revealed distinct patterns in LRM reasoning including cognitive state transition dynamics.", "conclusion": "Provides theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems."}}
{"id": "2509.14568", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.14568", "abs": "https://arxiv.org/abs/2509.14568", "authors": ["Hai Siong Tan", "Kuancheng Wang", "Rafe McBeth"], "title": "Evidential Physics-Informed Neural Networks for Scientific Discovery", "comment": "15 pages, 4 figures", "summary": "We present the fundamental theory and implementation guidelines underlying\nEvidential Physics-Informed Neural Network (E-PINN) -- a novel class of\nuncertainty-aware PINN. It leverages the marginal distribution loss function of\nevidential deep learning for estimating uncertainty of outputs, and infers\nunknown parameters of the PDE via a learned posterior distribution. Validating\nour model on two illustrative case studies -- the 1D Poisson equation with a\nGaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated\nempirical coverage probabilities that were calibrated significantly better than\nBayesian PINN and Deep Ensemble methods. To demonstrate real-world\napplicability, we also present a brief case study on applying E-PINN to analyze\nclinical glucose-insulin datasets that have featured in medical research on\ndiabetes pathophysiology.", "AI": {"tldr": "E-PINN is a novel uncertainty-aware Physics-Informed Neural Network that uses evidential deep learning for uncertainty estimation and parameter inference, outperforming Bayesian PINN and Deep Ensemble methods in calibration.", "motivation": "To develop a more reliable uncertainty-aware PINN framework that can better estimate uncertainty in PDE solutions and infer unknown parameters through learned posterior distributions.", "method": "Leverages marginal distribution loss function from evidential deep learning for uncertainty estimation, infers unknown PDE parameters via learned posterior distribution, validated on 1D Poisson equation and 2D Fisher-KPP equation.", "result": "E-PINN generated significantly better calibrated empirical coverage probabilities compared to Bayesian PINN and Deep Ensemble methods, and demonstrated real-world applicability in clinical glucose-insulin dataset analysis.", "conclusion": "E-PINN provides a superior uncertainty-aware framework for solving PDEs with better calibration than existing methods, with practical applications in medical research and other real-world domains."}}
{"id": "2509.14693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14693", "abs": "https://arxiv.org/abs/2509.14693", "authors": ["Song Xu", "Yilun Liu", "Minggui He", "Mingchen Dai", "Ziang Chen", "Chunguang Zhao", "Jingzhou Du", "Shimin Tao", "Weibin Meng", "Shenglin Zhang", "Yongqian Sun", "Boxing Chen", "Daimeng Wei"], "title": "RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning", "comment": "5 pages, 3 figures", "summary": "Logs constitute a form of evidence signaling the operational status of\nsoftware systems. Automated log anomaly detection is crucial for ensuring the\nreliability of modern software systems. However, existing approaches face\nsignificant limitations: traditional deep learning models lack interpretability\nand generalization, while methods leveraging Large Language Models are often\nhindered by unreliability and factual inaccuracies. To address these issues, we\npropose RationAnomaly, a novel framework that enhances log anomaly detection by\nsynergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our\napproach first instills expert-like reasoning patterns using CoT-guided\nsupervised fine-tuning, grounded in a high-quality dataset corrected through a\nrigorous expert-driven process. Subsequently, a reinforcement learning phase\nwith a multi-faceted reward function optimizes for accuracy and logical\nconsistency, effectively mitigating hallucinations. Experimentally,\nRationAnomaly outperforms state-of-the-art baselines, achieving superior\nF1-scores on key benchmarks while providing transparent, step-by-step\nanalytical outputs. We have released the corresponding resources, including\ncode and datasets.", "AI": {"tldr": "RationAnomaly is a novel framework that combines Chain-of-Thought fine-tuning with reinforcement learning to improve log anomaly detection, addressing interpretability and reliability issues in existing methods.", "motivation": "Existing log anomaly detection approaches face limitations: traditional deep learning models lack interpretability and generalization, while LLM-based methods suffer from unreliability and factual inaccuracies.", "method": "The framework uses CoT-guided supervised fine-tuning with expert-corrected dataset to instill reasoning patterns, followed by reinforcement learning with multi-faceted reward function to optimize accuracy and logical consistency while mitigating hallucinations.", "result": "RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs.", "conclusion": "The proposed framework successfully addresses the limitations of existing methods by synergizing CoT fine-tuning with reinforcement learning, resulting in more reliable and interpretable log anomaly detection with released code and datasets."}}
{"id": "2509.14577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14577", "abs": "https://arxiv.org/abs/2509.14577", "authors": ["Yang Xu", "Junpeng Li", "Changchun Hua", "Yana Yang"], "title": "Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition", "comment": null, "summary": "The Large Margin Distribution Machine (LMDM) is a recent advancement in\nclassifier design that optimizes not just the minimum margin (as in SVM) but\nthe entire margin distribution, thereby improving generalization. However,\nexisting LMDM formulations are limited to vectorized inputs and struggle with\nhigh-dimensional tensor data due to the need for flattening, which destroys the\ndata's inherent multi-mode structure and increases computational burden. In\nthis paper, we propose a Structure-Preserving Margin Distribution Learning for\nHigh-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates\ndirectly on tensor representations without vectorization. The SPMD-LRT\npreserves multi-dimensional spatial structure by incorporating first-order and\nsecond-order tensor statistics (margin mean and variance) into the objective,\nand it leverages low-rank tensor decomposition techniques including rank-1(CP),\nhigher-rank CP, and Tucker decomposition to parameterize the weight tensor. An\nalternating optimization (double-gradient descent) algorithm is developed to\nefficiently solve the SPMD-LRT, iteratively updating factor matrices and core\ntensor. This approach enables SPMD-LRT to maintain the structural information\nof high-order data while optimizing margin distribution for improved\nclassification. Extensive experiments on diverse datasets (including MNIST,\nimages and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior\nclassification accuracy compared to conventional SVM, vector-based LMDM, and\nprior tensor-based SVM extensions (Support Tensor Machines and Support Tucker\nMachines). Notably, SPMD-LRT with Tucker decomposition attains the highest\naccuracy, highlighting the benefit of structure preservation. These results\nconfirm the effectiveness and robustness of SPMD-LRT in handling\nhigh-dimensional tensor data for classification.", "AI": {"tldr": "SPMD-LRT is a tensor-based classifier that preserves multi-dimensional structure while optimizing margin distribution, outperforming traditional methods on high-dimensional tensor data.", "motivation": "Existing Large Margin Distribution Machines (LMDM) require vectorization of tensor data, which destroys structural information and increases computational burden for high-dimensional data.", "method": "Proposes Structure-Preserving Margin Distribution Learning with Low-Rank Decomposition (SPMD-LRT) that operates directly on tensor representations, incorporates first-order and second-order margin statistics, and uses low-rank tensor decomposition techniques (CP and Tucker) with alternating optimization.", "result": "Extensive experiments on MNIST, images, and fMRI data show SPMD-LRT achieves superior classification accuracy compared to SVM, vector-based LMDM, and prior tensor-based SVM extensions, with Tucker decomposition performing best.", "conclusion": "SPMD-LRT effectively handles high-dimensional tensor data while preserving structural information and optimizing margin distribution, demonstrating robustness and superior performance in classification tasks."}}
{"id": "2509.14704", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14704", "abs": "https://arxiv.org/abs/2509.14704", "authors": ["Masaharu Mizumoto", "Dat Nguyen", "Zhiheng Han", "Jiyuan Fang", "Heyuan Guan", "Xingfu Li", "Naoya Shiraishi", "Xuyang Tian", "Yo Nakawake", "Le Minh Nguyen"], "title": "The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs", "comment": null, "summary": "Benchmark saturation and contamination undermine confidence in LLM\nevaluation. We present Nazonazo, a cost-effective and extensible benchmark\nbuilt from Japanese children's riddles to test insight-based reasoning. Items\nare short (mostly one sentence), require no specialized domain knowledge, and\ncan be generated at scale, enabling rapid refresh of blind sets when leakage is\nsuspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No\nmodel except for GPT-5 is comparable to human performance, which achieves a\n52.9% mean accuracy. Model comparison on extended 201 items shows that\nreasoning models significantly outperform non-reasoning peers, while model size\nshows no reliable association with accuracy. Beyond aggregate accuracy, an\ninformal candidate-tracking analysis of thought logs reveals many cases of\nverification failure: models often produce the correct solution among\nintermediate candidates yet fail to select it as the final answer, which we\nillustrate with representative examples observed in multiple models. Nazonazo\nthus offers a cost-effective, scalable, and easily renewable benchmark format\nthat addresses the current evaluation crisis while also suggesting a recurrent\nmeta-cognitive weakness, providing clear targets for future control and\ncalibration methods.", "AI": {"tldr": "Nazonazo is a Japanese riddle-based benchmark for testing insight reasoning in LLMs, showing most models underperform humans except GPT-5, with verification failure being a key weakness.", "motivation": "Address benchmark saturation and contamination issues in LLM evaluation by creating a cost-effective, extensible benchmark for insight-based reasoning.", "method": "Built benchmark from Japanese children's riddles (120 items, mostly one-sentence), evaluated 38 frontier models and 126 humans, with extended analysis on 201 items and thought log examination.", "result": "No model except GPT-5 comparable to human performance (52.9% mean accuracy). Reasoning models outperform non-reasoning peers, model size shows no reliable association with accuracy. Verification failure identified as common weakness.", "conclusion": "Nazonazo provides a scalable, renewable benchmark format that addresses evaluation crisis and reveals meta-cognitive weaknesses, offering targets for future control and calibration methods."}}
{"id": "2509.14585", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14585", "abs": "https://arxiv.org/abs/2509.14585", "authors": ["Minh Vu", "Konstantinos Slavakis"], "title": "Online reinforcement learning via sparse Gaussian mixture model Q-functions", "comment": null, "summary": "This paper introduces a structured and interpretable online policy-iteration\nframework for reinforcement learning (RL), built around the novel class of\nsparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work\nthat trained GMM-QFs offline, the proposed framework develops an online scheme\nthat leverages streaming data to encourage exploration. Model complexity is\nregulated through sparsification by Hadamard overparametrization, which\nmitigates overfitting while preserving expressiveness. The parameter space of\nS-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing\nfor principled parameter updates via online gradient descent on a smooth\nobjective. Numerical tests show that S-GMM-QFs match the performance of dense\ndeep RL (DeepRL) methods on standard benchmarks while using significantly fewer\nparameters, and maintain strong performance even in low-parameter-count regimes\nwhere sparsified DeepRL methods fail to generalize.", "AI": {"tldr": "Online policy-iteration framework using sparse Gaussian mixture model Q-functions that achieves comparable performance to dense deep RL methods with fewer parameters and better generalization in low-parameter regimes.", "motivation": "To develop an interpretable and structured online reinforcement learning framework that can leverage streaming data for exploration while maintaining model simplicity and preventing overfitting.", "method": "Uses sparse Gaussian mixture model Q-functions with Hadamard overparametrization for sparsification, online gradient descent on Riemannian manifold structure, and online policy iteration with streaming data.", "result": "S-GMM-QFs match performance of dense deep RL methods on standard benchmarks while using significantly fewer parameters, and maintain strong performance in low-parameter regimes where sparsified DeepRL methods fail.", "conclusion": "The proposed framework provides an effective and interpretable online RL approach that balances expressiveness with model complexity through principled sparsification techniques."}}
{"id": "2509.14750", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14750", "abs": "https://arxiv.org/abs/2509.14750", "authors": ["Letian Zhang", "Guanghao Meng", "Xudong Ren", "Yiming Wang", "Shu-Tao Xia"], "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration", "comment": null, "summary": "Retrieval-augmented Generation (RAG) is a prevalent approach for\ndomain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a\nphenomenon where fine-tuned models fail to recognize and act upon poor-quality\nretrieved documents, thus undermining performance. To address this, we propose\nthe Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two\nheterogeneous agents: a generalist Detector that identifies knowledge gaps, and\na domain-specialized Resolver that provides precise solutions. Guided by a\nmoderator, these agents engage in an adversarial collaboration, where the\nDetector's persistent questioning challenges the Resolver's expertise. This\ndynamic process allows for iterative problem dissection and refined knowledge\nretrieval. Extensive experiments show that AC-RAG significantly improves\nretrieval accuracy and outperforms state-of-the-art RAG methods across various\nvertical domains.", "AI": {"tldr": "AC-RAG framework uses adversarial collaboration between a generalist Detector and domain-specialized Resolver to address retrieval hallucinations in RAG systems, significantly improving performance.", "motivation": "Retrieval-augmented Generation (RAG) suffers from 'Retrieval Hallucinations' where models fail to recognize poor-quality retrieved documents, undermining performance in domain-specific applications.", "method": "Proposes AC-RAG framework with two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides solutions. They engage in adversarial collaboration guided by a moderator, enabling iterative problem dissection and refined knowledge retrieval.", "result": "Extensive experiments show AC-RAG significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.", "conclusion": "The adversarial collaboration framework effectively addresses retrieval hallucinations in RAG systems, demonstrating superior performance through iterative problem-solving between specialized agents."}}
{"id": "2509.14600", "categories": ["cs.LG", "physics.bio-ph", "I.2.1"], "pdf": "https://arxiv.org/pdf/2509.14600", "abs": "https://arxiv.org/abs/2509.14600", "authors": ["Alexander Aghili", "Andy Bruce", "Daniel Sabo", "Razvan Marinescu"], "title": "TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics", "comment": "Proceedings of the ICML 2025 Workshop on Multi-modal Foundation\n  Models and Large Language Models for Life Sciences, Vancouver, Canada. 2025.\n  Copyright 2025 by the author(s). 4 Pages 5 Figures", "summary": "Molecular dynamics (MD) simulations provide atomistic insight into\nbiomolecular systems but are often limited by high computational costs required\nto access long timescales. Coarse-grained machine learning models offer a\npromising avenue for accelerating sampling, yet conventional force matching\napproaches often fail to capture the full thermodynamic landscape as fitting a\nmodel on the gradient may not fit the absolute differences between low-energy\nconformational states. In this work, we incorporate a complementary energy\nmatching term into the loss function. We evaluate our framework on the\nChignolin protein using the CGSchNet model, systematically varying the weight\nof the energy loss term. While energy matching did not yield statistically\nsignificant improvements in accuracy, it revealed distinct tendencies in how\nmodels generalize the free energy surface. Our results suggest future\nopportunities to enhance coarse-grained modeling through improved energy\nestimation techniques and multi-modal loss formulations.", "AI": {"tldr": "Adding energy matching to coarse-grained machine learning models for molecular dynamics doesn't significantly improve accuracy but reveals different generalization patterns in free energy surfaces.", "motivation": "Molecular dynamics simulations are computationally expensive for long timescales, and conventional force matching approaches often fail to capture the full thermodynamic landscape of biomolecular systems.", "method": "Incorporated a complementary energy matching term into the loss function and evaluated on Chignolin protein using CGSchNet model, systematically varying the weight of the energy loss term.", "result": "Energy matching did not yield statistically significant improvements in accuracy but revealed distinct tendencies in how models generalize the free energy surface.", "conclusion": "Future opportunities exist to enhance coarse-grained modeling through improved energy estimation techniques and multi-modal loss formulations."}}
{"id": "2509.14603", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14603", "abs": "https://arxiv.org/abs/2509.14603", "authors": ["Xingchen Wang", "Feijie Wu", "Chenglin Miao", "Tianchun Li", "Haoyu Hu", "Qiming Cao", "Jing Gao", "Lu Su"], "title": "Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking", "comment": null, "summary": "Split Federated Learning (SFL) has emerged as an efficient alternative to\ntraditional Federated Learning (FL) by reducing client-side computation through\nmodel partitioning. However, exchanging of intermediate activations and model\nupdates introduces significant privacy risks, especially from data\nreconstruction attacks that recover original inputs from intermediate\nrepresentations. Existing defenses using noise injection often degrade model\nperformance. To overcome these challenges, we present PM-SFL, a scalable and\nprivacy-preserving SFL framework that incorporates Probabilistic Mask training\nto add structured randomness without relying on explicit noise. This mitigates\ndata reconstruction risks while maintaining model utility. To address data\nheterogeneity, PM-SFL employs personalized mask learning that tailors submodel\nstructures to each client's local data. For system heterogeneity, we introduce\na layer-wise knowledge compensation mechanism, enabling clients with varying\nresources to participate effectively under adaptive model splitting.\nTheoretical analysis confirms its privacy protection, and experiments on image\nand wireless sensing tasks demonstrate that PM-SFL consistently improves\naccuracy, communication efficiency, and robustness to privacy attacks, with\nparticularly strong performance under data and system heterogeneity.", "AI": {"tldr": "PM-SFL is a privacy-preserving split federated learning framework that uses probabilistic mask training instead of noise injection to protect against data reconstruction attacks while maintaining model performance under data and system heterogeneity.", "motivation": "Split Federated Learning reduces client computation but introduces privacy risks from intermediate data exchange. Existing noise-based defenses degrade model performance, creating a need for better privacy-preserving methods.", "method": "Uses probabilistic mask training for structured randomness without explicit noise, personalized mask learning for data heterogeneity, and layer-wise knowledge compensation for system heterogeneity with adaptive model splitting.", "result": "Theoretical privacy protection confirmed. Experiments show improved accuracy, communication efficiency, and robustness to privacy attacks, especially under heterogeneous conditions.", "conclusion": "PM-SFL provides an effective privacy-preserving solution for split federated learning that maintains utility while addressing both data and system heterogeneity challenges."}}
{"id": "2509.14942", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14942", "abs": "https://arxiv.org/abs/2509.14942", "authors": ["Minh-Khoi Pham", "Tai Tan Mai", "Martin Crane", "Rob Brennan", "Marie E. Ward", "Una Geary", "Declan Byrne", "Brian O Connell", "Colm Bergin", "Donncha Creagh", "Nick McDonald", "Marija Bezbradica"], "title": "Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers", "comment": "Accepted to BMC Medical Informatics and Decision Making on September\n  18th 2025", "summary": "Carbapenemase-Producing Enterobacteriace poses a critical concern for\ninfection prevention and control in hospitals. However, predictive modeling of\npreviously highlighted CPE-associated risks such as readmission, mortality, and\nextended length of stay (LOS) remains underexplored, particularly with modern\ndeep learning approaches. This study introduces an eXplainable AI modeling\nframework to investigate CPE impact on patient outcomes from Electronic Medical\nRecords data of an Irish hospital. We analyzed an inpatient dataset from an\nIrish acute hospital, incorporating diagnostic codes, ward transitions, patient\ndemographics, infection-related variables and contact network features. Several\nTransformer-based architectures were benchmarked alongside traditional machine\nlearning models. Clinical outcomes were predicted, and XAI techniques were\napplied to interpret model decisions. Our framework successfully demonstrated\nthe utility of Transformer-based models, with TabTransformer consistently\noutperforming baselines across multiple clinical prediction tasks, especially\nfor CPE acquisition (AUROC and sensitivity). We found infection-related\nfeatures, including historical hospital exposure, admission context, and\nnetwork centrality measures, to be highly influential in predicting patient\noutcomes and CPE acquisition risk. Explainability analyses revealed that\nfeatures like \"Area of Residence\", \"Admission Ward\" and prior admissions are\nkey risk factors. Network variables like \"Ward PageRank\" also ranked highly,\nreflecting the potential value of structural exposure information. This study\npresents a robust and explainable AI framework for analyzing complex EMR data\nto identify key risk factors and predict CPE-related outcomes. Our findings\nunderscore the superior performance of the Transformer models and highlight the\nimportance of diverse clinical and network features.", "AI": {"tldr": "This study develops an explainable AI framework using Transformer models to predict CPE-related patient outcomes from EMR data, showing superior performance over traditional methods and identifying key risk factors including infection history, admission context, and network centrality measures.", "motivation": "Carbapenemase-Producing Enterobacteriace (CPE) poses critical infection control challenges in hospitals, but predictive modeling of CPE-associated risks like readmission, mortality, and extended length of stay remains underexplored with modern deep learning approaches.", "method": "Analyzed inpatient EMR data from an Irish hospital including diagnostic codes, ward transitions, demographics, infection variables, and contact network features. Benchmarking Transformer-based architectures against traditional ML models, with XAI techniques for model interpretation.", "result": "TabTransformer consistently outperformed baselines across multiple clinical prediction tasks, especially for CPE acquisition (high AUROC and sensitivity). Infection-related features, historical hospital exposure, admission context, and network centrality measures were highly influential predictors.", "conclusion": "The study presents a robust explainable AI framework for analyzing complex EMR data to identify key CPE risk factors. Transformer models showed superior performance, and diverse clinical/network features proved crucial for predicting patient outcomes and CPE acquisition risk."}}
{"id": "2509.14617", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14617", "abs": "https://arxiv.org/abs/2509.14617", "authors": ["Jianglan Wei", "Zhenyu Zhang", "Pengcheng Wang", "Mingjie Zeng", "Zhigang Zeng"], "title": "HD3C: Efficient Medical Data Classification for Embedded Devices", "comment": null, "summary": "Energy-efficient medical data classification is essential for modern disease\nscreening, particularly in home and field healthcare where embedded devices are\nprevalent. While deep learning models achieve state-of-the-art accuracy, their\nsubstantial energy consumption and reliance on GPUs limit deployment on such\nplatforms. We present Hyperdimensional Computing with Class-Wise Clustering\n(HD3C), a lightweight classification framework designed for low-power\nenvironments. HD3C encodes data into high-dimensional hypervectors, aggregates\nthem into multiple cluster-specific prototypes, and performs classification\nthrough similarity search in hyperspace. We evaluate HD3C across three medical\nclassification tasks; on heart sound classification, HD3C is $350\\times$ more\nenergy-efficient than Bayesian ResNet with less than 1% accuracy difference.\nMoreover, HD3C demonstrates exceptional robustness to noise, limited training\ndata, and hardware error, supported by both theoretical analysis and empirical\nresults, highlighting its potential for reliable deployment in real-world\nsettings. Code is available at https://github.com/jianglanwei/HD3C.", "AI": {"tldr": "HD3C is an energy-efficient hyperdimensional computing framework for medical classification that achieves 350x better energy efficiency than deep learning with minimal accuracy loss, while being robust to noise and limited data.", "motivation": "Deep learning models have high energy consumption and GPU dependency, making them unsuitable for embedded medical devices in home and field healthcare settings where energy efficiency is critical.", "method": "HD3C encodes data into high-dimensional hypervectors, aggregates them into multiple cluster-specific prototypes, and performs classification through similarity search in hyperspace.", "result": "On heart sound classification, HD3C achieves 350x better energy efficiency than Bayesian ResNet with less than 1% accuracy difference, while demonstrating exceptional robustness to noise, limited training data, and hardware errors.", "conclusion": "HD3C provides a lightweight, energy-efficient alternative to deep learning for medical classification tasks, making it suitable for reliable deployment in low-power real-world healthcare settings."}}
{"id": "2509.14633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14633", "abs": "https://arxiv.org/abs/2509.14633", "authors": ["Jiaxing Miao", "Liang Hu", "Qi Zhang", "Lai Zhong Yuan", "Usman Naseem"], "title": "CUFG: Curriculum Unlearning Guided by the Forgetting Gradient", "comment": "under review (early)", "summary": "As privacy and security take center stage in AI, machine unlearning, the\nability to erase specific knowledge from models, has garnered increasing\nattention. However, existing methods overly prioritize efficiency and\naggressive forgetting, which introduces notable limitations. In particular,\nradical interventions like gradient ascent, influence functions, and random\nlabel noise can destabilize model weights, leading to collapse and reduced\nreliability. To address this, we propose CUFG (Curriculum Unlearning via\nForgetting Gradients), a novel framework that enhances the stability of\napproximate unlearning through innovations in both forgetting mechanisms and\ndata scheduling strategies. Specifically, CUFG integrates a new gradient\ncorrector guided by forgetting gradients for fine-tuning-based unlearning and a\ncurriculum unlearning paradigm that progressively forgets from easy to hard.\nThese innovations narrow the gap with the gold-standard Retrain method by\nenabling more stable and progressive unlearning, thereby improving both\neffectiveness and reliability. Furthermore, we believe that the concept of\ncurriculum unlearning has substantial research potential and offers\nforward-looking insights for the development of the MU field. Extensive\nexperiments across various forgetting scenarios validate the rationale and\neffectiveness of our approach and CUFG. Codes are available at\nhttps://anonymous.4open.science/r/CUFG-6375.", "AI": {"tldr": "CUFG is a curriculum-based unlearning framework that uses forgetting gradients and progressive data scheduling to enable stable and effective model forgetting without causing model collapse.", "motivation": "Existing machine unlearning methods prioritize efficiency and aggressive forgetting too much, leading to model instability, weight collapse, and reduced reliability through radical interventions like gradient ascent and random label noise.", "method": "Proposes CUFG framework with two innovations: 1) gradient corrector guided by forgetting gradients for fine-tuning-based unlearning, and 2) curriculum unlearning paradigm that progressively forgets from easy to hard data samples.", "result": "Extensive experiments across various forgetting scenarios validate the approach, showing CUFG narrows the gap with gold-standard Retrain method while improving stability and reliability.", "conclusion": "CUFG enables more stable and progressive unlearning, improving both effectiveness and reliability. The curriculum unlearning concept shows substantial research potential for the machine unlearning field."}}
{"id": "2509.14963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14963", "abs": "https://arxiv.org/abs/2509.14963", "authors": ["Filip Naudot", "Andreas Br\u00e4nnstr\u00f6m", "Vicen\u00e7 Torra", "Timotheus Kampik"], "title": "Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles", "comment": null, "summary": "We present functions that quantify the contribution of a set of arguments in\nquantitative bipolar argumentation graphs to (the final strength of) an\nargument of interest, a so-called topic. Our set contribution functions are\ngeneralizations of existing functions that quantify the contribution of a\nsingle contributing argument to a topic. Accordingly, we generalize existing\ncontribution function principles for set contribution functions and provide a\ncorresponding principle-based analysis. We introduce new principles specific to\nset-based functions that focus on properties pertaining to the interaction of\narguments within a set. Finally, we sketch how the principles play out across\ndifferent set contribution functions given a recommendation system application\nscenario.", "AI": {"tldr": "Generalization of single-argument contribution functions to set-based functions in quantitative bipolar argumentation graphs, with new principles for set interactions and application to recommendation systems.", "motivation": "To extend existing quantitative bipolar argumentation frameworks by developing functions that measure the collective contribution of argument sets (rather than individual arguments) to a topic's final strength.", "method": "Generalize existing single-argument contribution functions to set-based functions, establish new principles specific to set interactions, and conduct principle-based analysis across different set contribution functions.", "result": "Development of set contribution functions that quantify how groups of arguments collectively influence argument strength, with new principles addressing set-specific properties and interactions.", "conclusion": "The proposed set contribution functions successfully generalize single-argument approaches and provide a principled framework for analyzing collective argument contributions, with practical applications in recommendation systems."}}
{"id": "2509.14640", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14640", "abs": "https://arxiv.org/abs/2509.14640", "authors": ["Habib Irani", "Vangelis Metsis"], "title": "DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers", "comment": null, "summary": "Existing positional encoding methods in transformers are fundamentally\nsignal-agnostic, deriving positional information solely from sequence indices\nwhile ignoring the underlying signal characteristics. This limitation is\nparticularly problematic for time series analysis, where signals exhibit\ncomplex, non-stationary dynamics across multiple temporal scales. We introduce\nDynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework\nthat generates positional embeddings directly from input time series using the\nDiscrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time\nseries datasets demonstrate that DyWPE consistently outperforms eight existing\nstate-of-the-art positional encoding methods, achieving average relative\nimprovements of 9.1\\% compared to baseline sinusoidal absolute position\nencoding in biomedical signals, while maintaining competitive computational\nefficiency.", "AI": {"tldr": "DyWPE is a signal-aware positional encoding method using wavelet transforms that outperforms existing position encoding methods in time series analysis with 9.1% average improvement.", "motivation": "Existing positional encoding methods are signal-agnostic, only using sequence indices and ignoring signal characteristics, which is problematic for time series with complex non-stationary dynamics.", "method": "Dynamic Wavelet Positional Encoding (DyWPE) generates positional embeddings directly from input time series using Discrete Wavelet Transform (DWT).", "result": "DyWPE consistently outperforms 8 state-of-the-art positional encoding methods across 10 diverse time series datasets, achieving 9.1% average improvement over baseline sinusoidal encoding in biomedical signals.", "conclusion": "Signal-aware positional encoding using wavelet transforms is more effective than signal-agnostic methods for time series analysis, providing better performance while maintaining computational efficiency."}}
{"id": "2509.14998", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14998", "abs": "https://arxiv.org/abs/2509.14998", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Yanyuan Qiao", "Imran Razzak", "Yutong Xie"], "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making", "comment": "The paper has been accepted to the EMNLP 2025 Main Conference", "summary": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC.", "AI": {"tldr": "KAMAC is a dynamic multi-agent framework that enables LLM agents to form and expand expert teams based on evolving diagnostic needs, outperforming existing methods in complex medical scenarios.", "motivation": "Current multi-agent collaboration frameworks use static, pre-assigned roles that limit adaptability and dynamic knowledge integration in medical decision-making.", "method": "Knowledge-driven Adaptive Multi-Agent Collaboration (KAMAC) framework that starts with expert agents and dynamically recruits additional specialists to fill knowledge gaps through knowledge-driven discussions.", "result": "KAMAC significantly outperforms both single-agent and advanced multi-agent methods on two real-world medical benchmarks, especially in complex clinical scenarios like cancer prognosis.", "conclusion": "The proposed dynamic team formation approach enables flexible, scalable collaboration and better handles complex clinical scenarios requiring cross-specialty expertise."}}
{"id": "2509.14642", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14642", "abs": "https://arxiv.org/abs/2509.14642", "authors": ["Yuemin Wu", "Zhongze Wu", "Xiu Su", "Feng Yang", "Hongyan Xu", "Xi Lin", "Wenti Huang", "Shan You", "Chang Xu"], "title": "DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training", "comment": null, "summary": "Modeling dynamic temporal dependencies is a critical challenge in time series\npre-training, which evolve due to distribution shifts and multi-scale patterns.\nThis temporal variability severely impairs the generalization of pre-trained\nmodels to downstream tasks. Existing frameworks fail to capture the complex\ninteractions of short- and long-term dependencies, making them susceptible to\nspurious correlations that degrade generalization. To address these\nlimitations, we propose DeCoP, a Dependency Controlled Pre-training framework\nthat explicitly models dynamic, multi-scale dependencies by simulating evolving\ninter-patch dependencies. At the input level, DeCoP introduces Instance-wise\nPatch Normalization (IPN) to mitigate distributional shifts while preserving\nthe unique characteristics of each patch, creating a robust foundation for\nrepresentation learning. At the latent level, a hierarchical Dependency\nControlled Learning (DCL) strategy explicitly models inter-patch dependencies\nacross multiple temporal scales, with an Instance-level Contrastive Module\n(ICM) enhances global generalization by learning instance-discriminative\nrepresentations from time-invariant positive pairs. DeCoP achieves\nstate-of-the-art results on ten datasets with lower computing resources,\nimproving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.", "AI": {"tldr": "DeCoP is a novel time series pre-training framework that addresses dynamic temporal dependencies and distribution shifts through instance-wise patch normalization and hierarchical dependency controlled learning, achieving state-of-the-art results with improved efficiency.", "motivation": "Existing time series pre-training models struggle with dynamic temporal dependencies caused by distribution shifts and multi-scale patterns, leading to poor generalization and susceptibility to spurious correlations in downstream tasks.", "method": "DeCoP introduces Instance-wise Patch Normalization (IPN) to handle distributional shifts while preserving patch characteristics, and a hierarchical Dependency Controlled Learning (DCL) strategy with Instance-level Contrastive Module (ICM) to model multi-scale inter-patch dependencies and learn instance-discriminative representations.", "result": "DeCoP achieves state-of-the-art performance on ten datasets, improving MSE by 3% on ETTh1 compared to PatchTST while using only 37% of the FLOPs, demonstrating superior efficiency and effectiveness.", "conclusion": "The proposed DeCoP framework successfully addresses the challenges of dynamic temporal dependencies in time series pre-training through explicit modeling of multi-scale dependencies and robust representation learning, offering a more efficient and generalizable solution for time series analysis."}}
{"id": "2509.15035", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15035", "abs": "https://arxiv.org/abs/2509.15035", "authors": ["Gabriela C. Zapata", "Bill Cope", "Mary Kalantzis", "Duane Searsmith"], "title": "Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews", "comment": "39 pages, 3 tables", "summary": "This study investigates the use of generative AI to support formative\nassessment through machine generated reviews of peer reviews in graduate online\ncourses in a public university in the United States. Drawing on Systemic\nFunctional Linguistics and Appraisal Theory, we analyzed 120 metareviews to\nexplore how generative AI feedback constructs meaning across ideational,\ninterpersonal, and textual dimensions. The findings suggest that generative AI\ncan approximate key rhetorical and relational features of effective human\nfeedback, offering directive clarity while also maintaining a supportive\nstance. The reviews analyzed demonstrated a balance of praise and constructive\ncritique, alignment with rubric expectations, and structured staging that\nforegrounded student agency. By modeling these qualities, AI metafeedback has\nthe potential to scaffold feedback literacy and enhance leaner engagement with\npeer review.", "AI": {"tldr": "AI-generated meta-reviews in online graduate courses effectively approximate human feedback qualities, balancing praise and critique while maintaining supportive stance and rubric alignment.", "motivation": "To investigate how generative AI can support formative assessment by providing machine-generated reviews of peer reviews in online education.", "method": "Analyzed 120 AI-generated metareviews using Systemic Functional Linguistics and Appraisal Theory to examine meaning construction across ideational, interpersonal, and textual dimensions.", "result": "Generative AI feedback demonstrated ability to approximate key rhetorical and relational features of effective human feedback, offering directive clarity with supportive stance, balanced praise/critique, rubric alignment, and structured staging that foregrounded student agency.", "conclusion": "AI metafeedback has potential to scaffold feedback literacy and enhance learner engagement with peer review by modeling effective feedback qualities."}}
{"id": "2509.14678", "categories": ["cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2509.14678", "abs": "https://arxiv.org/abs/2509.14678", "authors": ["Hyungjoon Soh", "Junghyo Jo"], "title": "Stochastic Clock Attention for Aligning Continuous and Ordered Sequences", "comment": "8 pages, 3 figures", "summary": "We formulate an attention mechanism for continuous and ordered sequences that\nexplicitly functions as an alignment model, which serves as the core of many\nsequence-to-sequence tasks. Standard scaled dot-product attention relies on\npositional encodings and masks but does not enforce continuity or monotonicity,\nwhich are crucial for frame-synchronous targets. We propose learned nonnegative\n\\emph{clocks} to source and target and model attention as the meeting\nprobability of these clocks; a path-integral derivation yields a closed-form,\nGaussian-like scoring rule with an intrinsic bias toward causal, smooth,\nnear-diagonal alignments, without external positional regularizers. The\nframework supports two complementary regimes: normalized clocks for parallel\ndecoding when a global length is available, and unnormalized clocks for\nautoregressive decoding -- both nearly-parameter-free, drop-in replacements. In\na Transformer text-to-speech testbed, this construction produces more stable\nalignments and improved robustness to global time-scaling while matching or\nimproving accuracy over scaled dot-product baselines. We hypothesize\napplicability to other continuous targets, including video and temporal signal\nmodeling.", "AI": {"tldr": "A novel attention mechanism using learned nonnegative clocks for continuous sequences that enforces alignment continuity and monotonicity without external positional regularizers, improving stability and robustness in sequence-to-sequence tasks.", "motivation": "Standard scaled dot-product attention lacks enforcement of continuity and monotonicity, which are crucial for frame-synchronous targets in sequence-to-sequence tasks like text-to-speech.", "method": "Proposed learned nonnegative clocks for source and target sequences, modeling attention as meeting probability of these clocks. Uses path-integral derivation to create Gaussian-like scoring with intrinsic bias toward causal, smooth, near-diagonal alignments. Supports both normalized clocks for parallel decoding and unnormalized clocks for autoregressive decoding.", "result": "In Transformer text-to-speech testbed, produces more stable alignments and improved robustness to global time-scaling while matching or improving accuracy over scaled dot-product baselines.", "conclusion": "The clock-based attention framework provides effective alignment modeling for continuous sequences with potential applications to video and temporal signal modeling tasks."}}
{"id": "2509.15084", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15084", "abs": "https://arxiv.org/abs/2509.15084", "authors": ["Doreen Jirak", "Pieter Maes", "Armeen Saroukanoff", "Dirk van Rooy"], "title": "From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support", "comment": "Paper accepted at Human Learning and Decision-Making Workshop\n  @ECML-PKDD Conference 2025, Porto, Portugal", "summary": "As autonomous technologies increasingly shape maritime operations,\nunderstanding why an AI system makes a decision becomes as crucial as what it\ndecides. In complex and dynamic maritime environments, trust in AI depends not\nonly on performance but also on transparency and interpretability. This paper\nhighlights the importance of Explainable AI (XAI) as a foundation for effective\nhuman-machine teaming in the maritime domain, where informed oversight and\nshared understanding are essential. To support the user-centered integration of\nXAI, we propose a domain-specific survey designed to capture maritime\nprofessionals' perceptions of trust, usability, and explainability. Our aim is\nto foster awareness and guide the development of user-centric XAI systems\ntailored to the needs of seafarers and maritime teams.", "AI": {"tldr": "Survey on maritime professionals' perceptions of trust and explainability in AI systems for maritime operations", "motivation": "As AI becomes crucial in maritime operations, trust depends on transparency and interpretability, not just performance. Effective human-machine teaming requires explainable AI (XAI) for informed oversight in complex maritime environments.", "method": "Propose a domain-specific survey designed to capture maritime professionals' perceptions of trust, usability, and explainability in AI systems", "result": "The paper aims to foster awareness and guide development of user-centric XAI systems tailored to seafarers' needs", "conclusion": "XAI is essential foundation for maritime human-machine teaming, requiring user-centered approaches that address specific domain needs through professional feedback"}}
{"id": "2509.14718", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14718", "abs": "https://arxiv.org/abs/2509.14718", "authors": ["Zihao Feng", "Xiaoxue Wang", "Bowen Wu", "Hailong Cao", "Tiejun Zhao", "Qun Yu", "Baoxun Wang"], "title": "ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning", "comment": null, "summary": "While reinforcement learning (RL) is increasingly used for LLM-based tool\nlearning, its efficiency is often hampered by an overabundance of simple\nsamples that provide diminishing learning value as training progresses.\nExisting dynamic sampling techniques are ill-suited for the multi-task\nstructure and fine-grained reward mechanisms inherent to tool learning. This\npaper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework\nspecifically designed to address this challenge by targeting the unique\ncharacteristics of tool learning: its multiple interdependent sub-tasks and\nmulti-valued reward functions. DSCL features two core components: Reward-Based\nDynamic Sampling, which uses multi-dimensional reward statistics (mean and\nvariance) to prioritize valuable data, and Task-Based Dynamic Curriculum\nLearning, which adaptively focuses training on less-mastered sub-tasks. Through\nextensive experiments, we demonstrate that DSCL significantly improves training\nefficiency and model performance over strong baselines, achieving a 3.29\\%\nimprovement on the BFCLv3 benchmark. Our method provides a tailored solution\nthat effectively leverages the complex reward signals and sub-task dynamics\nwithin tool learning to achieve superior results.", "AI": {"tldr": "DSCL framework improves RL efficiency for LLM tool learning by dynamically sampling valuable data and focusing on less-mastered sub-tasks using reward statistics and curriculum learning.", "motivation": "Existing dynamic sampling techniques are ill-suited for multi-task tool learning with fine-grained rewards, leading to inefficient training with diminishing returns from simple samples.", "method": "DSCL combines Reward-Based Dynamic Sampling (using multi-dimensional reward mean/variance) and Task-Based Dynamic Curriculum Learning to prioritize valuable data and focus on challenging sub-tasks.", "result": "Achieves 3.29% improvement on BFCLv3 benchmark, significantly enhancing training efficiency and model performance over strong baselines.", "conclusion": "DSCL provides a tailored solution that effectively leverages complex reward signals and sub-task dynamics in tool learning for superior results."}}
{"id": "2509.15172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15172", "abs": "https://arxiv.org/abs/2509.15172", "authors": ["Ankur Samanta", "Akshayaa Magesh", "Youliang Yu", "Runzhe Wu", "Ayush Jain", "Daniel Jiang", "Boris Vidolov", "Paul Sajda", "Yonathan Efroni", "Kaveh Hassani"], "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment", "comment": null, "summary": "Language Models (LMs) are inconsistent reasoners, often generating\ncontradictory responses to identical prompts. While inference-time methods can\nmitigate these inconsistencies, they fail to address the core problem: LMs\nstruggle to reliably select reasoning pathways leading to consistent outcomes\nunder exploratory sampling. To address this, we formalize self-consistency as\nan intrinsic property of well-aligned reasoning models and introduce\nMulti-Agent Consensus Alignment (MACA), a reinforcement learning framework that\npost-trains models to favor reasoning trajectories aligned with their internal\nconsensus using majority/minority outcomes from multi-agent debate. These\ntrajectories emerge from deliberative exchanges where agents ground reasoning\nin peer arguments, not just aggregation of independent attempts, creating\nricher consensus signals than single-round majority voting. MACA enables agents\nto teach themselves to be more decisive and concise, and better leverage peer\ninsights in multi-agent settings without external supervision, driving\nsubstantial improvements across self-consistency (+27.6% on GSM8K),\nsingle-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%\nPass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).\nThese findings, coupled with strong generalization to unseen benchmarks (+16.3%\non GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more\nreliably unlocks latent reasoning potential of language models.", "AI": {"tldr": "MACA is a reinforcement learning framework that uses multi-agent debate to train language models to be more self-consistent reasoners by aligning with internal consensus rather than just majority voting.", "motivation": "Language models are inconsistent reasoners that generate contradictory responses to identical prompts, and existing inference-time methods don't address the core problem of unreliable reasoning pathway selection.", "method": "Multi-Agent Consensus Alignment (MACA) framework that post-trains models using reinforcement learning to favor reasoning trajectories aligned with internal consensus from multi-agent debate, where agents ground reasoning in peer arguments.", "result": "Substantial improvements across multiple benchmarks: +27.6% on GSM8K, +23.7% on MATH, +22.4% Pass@20 on MATH, +42.7% on MathQA, with strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA).", "conclusion": "MACA enables robust self-alignment that more reliably unlocks the latent reasoning potential of language models through deliberative exchanges and consensus-based learning without external supervision."}}
{"id": "2509.14722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14722", "abs": "https://arxiv.org/abs/2509.14722", "authors": ["Yeyu Yan", "Shuai Zheng", "Wenjun Hui", "Xiangkai Zhu", "Dong Chen", "Zhenfeng Zhu", "Yao Zhao", "Kunlun He"], "title": "Towards Pre-trained Graph Condensation via Optimal Transport", "comment": null, "summary": "Graph condensation (GC) aims to distill the original graph into a small-scale\ngraph, mitigating redundancy and accelerating GNN training. However,\nconventional GC approaches heavily rely on rigid GNNs and task-specific\nsupervision. Such a dependency severely restricts their reusability and\ngeneralization across various tasks and architectures. In this work, we revisit\nthe goal of ideal GC from the perspective of GNN optimization consistency, and\nthen a generalized GC optimization objective is derived, by which those\ntraditional GC methods can be viewed nicely as special cases of this\noptimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)\nvia optimal transport is proposed to transcend the limitations of task- and\narchitecture-dependent GC methods. Specifically, a hybrid-interval graph\ndiffusion augmentation is presented to suppress the weak generalization ability\nof the condensed graph on particular architectures by enhancing the uncertainty\nof node states. Meanwhile, the matching between optimal graph transport plan\nand representation transport plan is tactfully established to maintain semantic\nconsistencies across source graph and condensed graph spaces, thereby freeing\ngraph condensation from task dependencies. To further facilitate the adaptation\nof condensed graphs to various downstream tasks, a traceable semantic\nharmonizer from source nodes to condensed nodes is proposed to bridge semantic\nassociations through the optimized representation transport plan in\npre-training. Extensive experiments verify the superiority and versatility of\nPreGC, demonstrating its task-independent nature and seamless compatibility\nwith arbitrary GNNs.", "AI": {"tldr": "PreGC is a task- and architecture-independent graph condensation method that uses optimal transport and hybrid-interval graph diffusion to create condensed graphs that work with any GNN architecture across various tasks.", "motivation": "Traditional graph condensation methods are limited by their dependency on specific GNN architectures and task-specific supervision, which restricts their reusability and generalization across different tasks and models.", "method": "Proposes Pre-trained Graph Condensation (PreGC) using optimal transport with hybrid-interval graph diffusion augmentation to enhance generalization, and establishes matching between graph transport and representation transport plans to maintain semantic consistency.", "result": "Extensive experiments show PreGC achieves superior performance and versatility, demonstrating task-independent nature and seamless compatibility with arbitrary GNN architectures.", "conclusion": "PreGC successfully overcomes the limitations of traditional graph condensation methods by providing a generalized approach that works across various tasks and GNN architectures through optimal transport and semantic consistency preservation."}}
{"id": "2509.15217", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15217", "abs": "https://arxiv.org/abs/2509.15217", "authors": ["Yue Xin", "Wenyuan Wang", "Rui Pan", "Ruida Wang", "Howard Meng", "Renjie Pi", "Shizhe Diao", "Tong Zhang"], "title": "Generalizable Geometric Image Caption Synthesis", "comment": null, "summary": "Multimodal large language models have various practical applications that\ndemand strong reasoning abilities. Despite recent advancements, these models\nstill struggle to solve complex geometric problems. A key challenge stems from\nthe lack of high-quality image-text pair datasets for understanding geometric\nimages. Furthermore, most template-based data synthesis pipelines typically\nfail to generalize to questions beyond their predefined templates. In this\npaper, we bridge this gap by introducing a complementary process of\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\npipeline. By adopting RLVR to refine captions for geometric images synthesized\nfrom 50 basic geometric relations and using reward signals derived from\nmathematical problem-solving tasks, our pipeline successfully captures the key\nfeatures of geometry problem-solving. This enables better task generalization\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\nscenarios, the generated dataset enhances the general reasoning capabilities of\nmultimodal large language models, yielding accuracy improvements of\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\nwith non-geometric input images of MathVista and MathVerse, along with\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\nin MMMU.", "AI": {"tldr": "The paper introduces RLVR (Reinforcement Learning with Verifiable Rewards) to generate high-quality geometric image-text datasets, improving multimodal LLMs' geometric reasoning and general problem-solving capabilities across various mathematical domains.", "motivation": "Multimodal LLMs struggle with complex geometric problems due to lack of high-quality geometric image-text datasets and limited generalization of template-based data synthesis methods.", "method": "Proposes RLVR pipeline that refines captions for geometric images using reward signals from mathematical problem-solving tasks, starting from 50 basic geometric relations.", "result": "Achieves 2.8%-4.8% accuracy improvements in non-geometric math tasks (statistics, arithmetic, algebra) and 2.4%-3.9% improvements in Art, Design, Tech, and Engineering tasks on benchmark datasets.", "conclusion": "RLVR effectively captures key geometric problem-solving features, enabling better generalization and enhancing multimodal LLMs' reasoning capabilities even in out-of-distribution scenarios."}}
{"id": "2509.14723", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14723", "abs": "https://arxiv.org/abs/2509.14723", "authors": ["Sosuke Hosokawa", "Toshiharu Kawakami", "Satoshi Kodera", "Masamichi Ito", "Norihiko Takeda"], "title": "Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models", "comment": null, "summary": "Single-cell foundation models (scFMs) have demonstrated state-of-the-art\nperformance on various tasks, such as cell-type annotation and perturbation\nresponse prediction, by learning gene regulatory networks from large-scale\ntranscriptome data. However, a significant challenge remains: the\ndecision-making processes of these models are less interpretable compared to\ntraditional methods like differential gene expression analysis. Recently,\ntranscoders have emerged as a promising approach for extracting interpretable\ndecision circuits from large language models (LLMs). In this work, we train a\ntranscoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By\nleveraging the trained transcoder, we extract internal decision-making circuits\nfrom the C2S model. We demonstrate that the discovered circuits correspond to\nreal-world biological mechanisms, confirming the potential of transcoders to\nuncover biologically plausible pathways within complex single-cell models.", "AI": {"tldr": "Training a transcoder on cell2sentence model to extract interpretable decision circuits from single-cell foundation models, revealing biologically meaningful pathways.", "motivation": "Single-cell foundation models lack interpretability compared to traditional methods, making it difficult to understand their decision-making processes despite superior performance.", "method": "Train a transcoder on the cell2sentence (C2S) model to extract internal decision-making circuits from this state-of-the-art single-cell foundation model.", "result": "The discovered circuits correspond to real-world biological mechanisms, demonstrating that transcoders can uncover biologically plausible pathways within complex single-cell models.", "conclusion": "Transcoders show significant potential for improving interpretability of single-cell foundation models by revealing meaningful biological decision circuits."}}
{"id": "2509.14724", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14724", "abs": "https://arxiv.org/abs/2509.14724", "authors": ["Zhiyuan Xue", "Ben Yang", "Xuetao Zhang", "Fei Wang", "Zhiping Lin"], "title": "One-step Multi-view Clustering With Adaptive Low-rank Anchor-graph Learning", "comment": "13 pages, 7 figures, journal article. Accepted by IEEE Transactions\n  on Multimedia, not yet published online", "summary": "In light of their capability to capture structural information while reducing\ncomputing complexity, anchor graph-based multi-view clustering (AGMC) methods\nhave attracted considerable attention in large-scale clustering problems.\nNevertheless, existing AGMC methods still face the following two issues: 1)\nThey directly embedded diverse anchor graphs into a consensus anchor graph\n(CAG), and hence ignore redundant information and numerous noises contained in\nthese anchor graphs, leading to a decrease in clustering effectiveness; 2) They\ndrop effectiveness and efficiency due to independent post-processing to acquire\nclustering indicators. To overcome the aforementioned issues, we deliver a\nnovel one-step multi-view clustering method with adaptive low-rank anchor-graph\nlearning (OMCAL). To construct a high-quality CAG, OMCAL provides a nuclear\nnorm-based adaptive CAG learning model against information redundancy and noise\ninterference. Then, to boost clustering effectiveness and efficiency\nsubstantially, we incorporate category indicator acquisition and CAG learning\ninto a unified framework. Numerous studies conducted on ordinary and\nlarge-scale datasets indicate that OMCAL outperforms existing state-of-the-art\nmethods in terms of clustering effectiveness and efficiency.", "AI": {"tldr": "OMCAL is a novel one-step multi-view clustering method that addresses limitations in existing anchor graph-based approaches by integrating adaptive low-rank anchor-graph learning and category indicator acquisition into a unified framework.", "motivation": "Existing anchor graph-based multi-view clustering methods suffer from two main issues: 1) they ignore redundant information and noise when embedding diverse anchor graphs into a consensus anchor graph, reducing clustering effectiveness, and 2) they require independent post-processing to obtain clustering indicators, compromising both effectiveness and efficiency.", "method": "The proposed OMCAL method uses a nuclear norm-based adaptive consensus anchor graph learning model to handle information redundancy and noise interference. It integrates category indicator acquisition and consensus anchor graph learning into a single unified framework to improve both clustering effectiveness and efficiency.", "result": "Numerous experiments on both ordinary and large-scale datasets demonstrate that OMCAL outperforms existing state-of-the-art methods in terms of both clustering effectiveness and computational efficiency.", "conclusion": "OMCAL successfully addresses the limitations of previous anchor graph-based multi-view clustering methods by providing a unified one-step approach that effectively handles noise and redundancy while improving both clustering performance and computational efficiency."}}
{"id": "2509.14775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14775", "abs": "https://arxiv.org/abs/2509.14775", "authors": ["Shuangshuang He", "Yuanting Zhang", "Hongli Liang", "Qingye Meng", "Xingyuan Yuan"], "title": "FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration", "comment": null, "summary": "Accurate hourly weather forecasting is critical for numerous applications.\nRecent deep learning models have demonstrated strong capability on 6-hour\nintervals, yet achieving accurate and stable hourly predictions remains a\ncritical challenge. This is primarily due to the rapid accumulation of errors\nin autoregressive rollouts and temporal discontinuities within the ERA5 data's\n12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a\nframework that models atmospheric state evolution as a continuous flow.\nFlowCast-ODE learns the conditional flow path directly from the previous state,\nan approach that aligns more naturally with physical dynamic systems and\nenables efficient computation. A coarse-to-fine strategy is introduced to train\nthe model on 6-hour data using dynamic flow matching and then refined on hourly\ndata that incorporates an Ordinary Differential Equation (ODE) solver to\nachieve temporally coherent forecasts. In addition, a lightweight low-rank\nAdaLN-Zero modulation mechanism is proposed and reduces model size by 15%\nwithout compromising accuracy. Experiments demonstrate that FlowCast-ODE\noutperforms strong baselines, yielding lower root mean square error (RMSE) and\nbetter energy conservation, which reduces blurring and preserves more\nfine-scale spatial details. It also shows comparable performance to the\nstate-of-the-art model in forecasting extreme events like typhoons.\nFurthermore, the model alleviates temporal discontinuities associated with\nassimilation cycle transitions.", "AI": {"tldr": "FlowCast-ODE is a deep learning framework for hourly weather forecasting that models atmospheric evolution as continuous flow using ODE solvers, achieving better accuracy and temporal coherence while reducing model size.", "motivation": "Accurate hourly weather forecasting is critical but challenging due to error accumulation in autoregressive models and temporal discontinuities in ERA5 data's 12-hour assimilation cycle.", "method": "Proposes FlowCast-ODE framework that models atmospheric state evolution as continuous flow using dynamic flow matching and ODE solvers, with coarse-to-fine training strategy and lightweight low-rank AdaLN-Zero modulation to reduce model size.", "result": "Outperforms baselines with lower RMSE, better energy conservation, reduced blurring, preserved spatial details, comparable performance on extreme events like typhoons, and alleviates temporal discontinuities.", "conclusion": "FlowCast-ODE provides an effective solution for stable and accurate hourly weather forecasting by modeling atmospheric dynamics as continuous flow, addressing key challenges in temporal coherence and error accumulation."}}
{"id": "2509.14786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14786", "abs": "https://arxiv.org/abs/2509.14786", "authors": ["Konwoo Kim", "Suhas Kotha", "Percy Liang", "Tatsunori Hashimoto"], "title": "Pre-training under infinite compute", "comment": null, "summary": "Since compute grows much faster than web text available for language model\npre-training, we ask how one should approach pre-training under fixed data and\nno compute constraints. We first show that existing data-constrained approaches\nof increasing epoch count and parameter count eventually overfit, and we\nsignificantly improve upon such recipes by properly tuning regularization,\nfinding that the optimal weight decay is $30\\times$ larger than standard\npractice. Since our regularized recipe monotonically decreases loss following a\nsimple power law in parameter count, we estimate its best possible performance\nvia the asymptote of its scaling law rather than the performance at a fixed\ncompute budget. We then identify that ensembling independently trained models\nachieves a significantly lower loss asymptote than the regularized recipe. Our\nbest intervention combining epoching, regularization, parameter scaling, and\nensemble scaling achieves an asymptote at 200M tokens using $5.17\\times$ less\ndata than our baseline, and our data scaling laws predict that this improvement\npersists at higher token budgets. We find that our data efficiency gains can be\nrealized at much smaller parameter counts as we can distill an ensemble into a\nstudent model that is 8$\\times$ smaller and retains $83\\%$ of the ensembling\nbenefit. Finally, our interventions designed for validation loss generalize to\ndownstream benchmarks, achieving a $9\\%$ improvement for pre-training evals and\na $17.5\\times$ data efficiency improvement over continued pre-training on math\nmid-training data. Our results show that simple algorithmic improvements can\nenable significantly more data-efficient pre-training in a compute-rich future.", "AI": {"tldr": "This paper presents algorithmic improvements for data-efficient language model pre-training when compute is abundant but data is limited, achieving significant data efficiency gains through optimized regularization, parameter scaling, and ensembling techniques.", "motivation": "As compute grows faster than available web text for language model pre-training, researchers need methods to optimize training under fixed data constraints with unlimited compute resources.", "method": "The authors developed a regularized training recipe with 30x larger weight decay than standard practice, combined parameter scaling, ensembling of independently trained models, and knowledge distillation to transfer ensemble benefits to smaller student models.", "result": "The approach achieved 5.17x less data usage at 200M tokens, with ensemble distillation retaining 83% of benefits in models 8x smaller. Downstream benchmarks showed 9% improvement on pre-training evals and 17.5x data efficiency improvement on math tasks.", "conclusion": "Simple algorithmic improvements can enable significantly more data-efficient pre-training in compute-rich scenarios, with the proposed techniques providing persistent benefits across different token budgets and model sizes."}}
{"id": "2509.14788", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.14788", "abs": "https://arxiv.org/abs/2509.14788", "authors": ["Jing Lan", "Hexiao Ding", "Hongzhao Chen", "Yufeng Jiang", "Nga-Chun Ng", "Gwing Kei Yip", "Gerald W. Y. Cheng", "Yunlin Mao", "Jing Cai", "Liang-ting Lin", "Jung Sun Yoo"], "title": "Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery", "comment": null, "summary": "Accurate identification of drug-target interactions (DTI) remains a central\nchallenge in computational pharmacology, where sequence-based methods offer\nscalability. This work introduces a sequence-based drug-target interaction\nframework that integrates structural priors into protein representations while\nmaintaining high-throughput screening capability. Evaluated across multiple\nbenchmarks, the model achieves state-of-the-art performance on Human and\nBioSNAP datasets and remains competitive on BindingDB. In virtual screening\ntasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in\nAUROC and BEDROC. Ablation studies confirm the critical role of learned\naggregation, bilinear attention, and contrastive alignment in enhancing\npredictive robustness. Embedding visualizations reveal improved spatial\ncorrespondence with known binding pockets and highlight interpretable attention\npatterns over ligand-residue contacts. These results validate the framework's\nutility for scalable and structure-aware DTI prediction.", "AI": {"tldr": "A sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability, achieving state-of-the-art performance.", "motivation": "Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability but often lack structural awareness.", "method": "Sequence-based DTI framework that integrates structural priors into protein representations, featuring learned aggregation, bilinear attention, and contrastive alignment mechanisms.", "result": "Achieves state-of-the-art performance on Human and BioSNAP datasets, remains competitive on BindingDB, and surpasses prior methods in virtual screening tasks on LIT-PCBA with substantial gains in AUROC and BEDROC metrics.", "conclusion": "The framework validates utility for scalable and structure-aware DTI prediction, with embedding visualizations showing improved spatial correspondence with known binding pockets and interpretable attention patterns."}}
{"id": "2509.14801", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14801", "abs": "https://arxiv.org/abs/2509.14801", "authors": ["Julian F. Schumann", "Anna M\u00e9sz\u00e1ros", "Jens Kober", "Arkady Zgonnikov"], "title": "STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models", "comment": null, "summary": "While trajectory prediction plays a critical role in enabling safe and\neffective path-planning in automated vehicles, standardized practices for\nevaluating such models remain underdeveloped. Recent efforts have aimed to\nunify dataset formats and model interfaces for easier comparisons, yet existing\nframeworks often fall short in supporting heterogeneous traffic scenarios,\njoint prediction models, or user documentation. In this work, we introduce STEP\n-- a new benchmarking framework that addresses these limitations by providing a\nunified interface for multiple datasets, enforcing consistent training and\nevaluation conditions, and supporting a wide range of prediction models. We\ndemonstrate the capabilities of STEP in a number of experiments which reveal 1)\nthe limitations of widely-used testing procedures, 2) the importance of joint\nmodeling of agents for better predictions of interactions, and 3) the\nvulnerability of current state-of-the-art models against both distribution\nshifts and targeted attacks by adversarial agents. With STEP, we aim to shift\nthe focus from the ``leaderboard'' approach to deeper insights about model\nbehavior and generalization in complex multi-agent settings.", "AI": {"tldr": "STEP is a new benchmarking framework for trajectory prediction models that addresses limitations in existing evaluation methods, supports heterogeneous traffic scenarios and joint prediction models, and provides deeper insights into model behavior and generalization.", "motivation": "Standardized practices for evaluating trajectory prediction models in automated vehicles are underdeveloped, with existing frameworks lacking support for heterogeneous traffic scenarios, joint prediction models, and proper documentation.", "method": "The authors introduce STEP - a benchmarking framework that provides a unified interface for multiple datasets, enforces consistent training/evaluation conditions, and supports various prediction models through comprehensive experiments.", "result": "Experiments reveal limitations of current testing procedures, demonstrate the importance of joint agent modeling for interaction predictions, and show vulnerability of state-of-the-art models to distribution shifts and adversarial attacks.", "conclusion": "STEP shifts focus from leaderboard comparisons to providing deeper insights about model behavior and generalization in complex multi-agent settings, addressing critical gaps in trajectory prediction evaluation."}}
{"id": "2509.14821", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14821", "abs": "https://arxiv.org/abs/2509.14821", "authors": ["Andrea Cavallo", "Samuel Rey", "Antonio G. Marques", "Elvin Isufi"], "title": "Precision Neural Networks: Joint Graph And Relational Learning", "comment": null, "summary": "CoVariance Neural Networks (VNNs) perform convolutions on the graph\ndetermined by the covariance matrix of the data, which enables expressive and\nstable covariance-based learning. However, covariance matrices are typically\ndense, fail to encode conditional independence, and are often precomputed in a\ntask-agnostic way, which may hinder performance. To overcome these limitations,\nwe study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix\n-- the inverse covariance. The precision matrix naturally encodes statistical\nindependence, often exhibits sparsity, and preserves the covariance spectral\nstructure. To make precision estimation task-aware, we formulate an\noptimization problem that jointly learns the network parameters and the\nprecision matrix, and solve it via alternating optimization, by sequentially\nupdating the network weights and the precision estimate. We theoretically bound\nthe distance between the estimated and true precision matrices at each\niteration, and demonstrate the effectiveness of joint estimation compared to\ntwo-step approaches on synthetic and real-world data.", "AI": {"tldr": "Precision Neural Networks (PNNs) extend VNNs by using precision matrices instead of covariance matrices, enabling task-aware joint learning of network parameters and precision estimation with theoretical guarantees.", "motivation": "Covariance matrices are dense, fail to encode conditional independence, and are often precomputed in a task-agnostic way, which limits performance of VNNs.", "method": "Formulate joint optimization problem to learn network parameters and precision matrix simultaneously via alternating optimization, with theoretical bounds on precision estimation accuracy.", "result": "PNNs effectively learn sparse precision matrices that encode statistical independence, outperform two-step approaches on synthetic and real-world data.", "conclusion": "Joint estimation of precision matrices with network parameters enables more expressive and stable covariance-based learning while preserving spectral structure."}}
{"id": "2509.14832", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.14832", "abs": "https://arxiv.org/abs/2509.14832", "authors": ["Stelios Zarifis", "Ioannis Kordonis", "Petros Maragos"], "title": "Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization", "comment": "5 pages, 2 figures, 2 tables, and 1 algorithm. This version is\n  submitted to the 51st IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2026), to be held in Barcelona, Spain, on May 4-8,\n  2026", "summary": "Stochastic forecasting is critical for efficient decision-making in uncertain\nsystems, such as energy markets and finance, where estimating the full\ndistribution of future scenarios is essential. We propose Diffusion Scenario\nTree (DST), a general framework for constructing scenario trees for\nmultivariate prediction tasks using diffusion-based probabilistic forecasting\nmodels. DST recursively samples future trajectories and organizes them into a\ntree via clustering, ensuring non-anticipativity (decisions depending only on\nobserved history) at each stage. We evaluate the framework on the optimization\ntask of energy arbitrage in New York State's day-ahead electricity market.\nExperimental results show that our approach consistently outperforms the same\noptimization algorithms that use scenario trees from more conventional models\nand Model-Free Reinforcement Learning baselines. Furthermore, using DST for\nstochastic optimization yields more efficient decision policies, achieving\nhigher performance by better handling uncertainty than deterministic and\nstochastic MPC variants using the same diffusion-based forecaster.", "AI": {"tldr": "DST is a diffusion-based framework for building scenario trees that enables better stochastic optimization in energy markets by handling uncertainty more effectively than conventional methods.", "motivation": "Stochastic forecasting is essential for decision-making in uncertain systems like energy markets, where estimating full probability distributions of future scenarios is crucial for optimization tasks.", "method": "Diffusion Scenario Tree (DST) recursively samples future trajectories using diffusion-based probabilistic forecasting models and organizes them into trees via clustering while ensuring non-anticipativity at each stage.", "result": "DST consistently outperforms conventional scenario tree models and Model-Free Reinforcement Learning baselines in energy arbitrage optimization, achieving higher performance by better handling uncertainty.", "conclusion": "The diffusion-based scenario tree framework enables more efficient decision policies in stochastic optimization problems, particularly in energy market applications, demonstrating superior performance over both deterministic and stochastic MPC variants."}}
{"id": "2509.14863", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14863", "abs": "https://arxiv.org/abs/2509.14863", "authors": ["Zhengwei Wang", "Gang Wu"], "title": "Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study", "comment": null, "summary": "Graph Transformers (GTs) show considerable potential in graph representation\nlearning. The architecture of GTs typically integrates Graph Neural Networks\n(GNNs) with global attention mechanisms either in parallel or as a precursor to\nattention mechanisms, yielding a local-and-global or local-to-global attention\nscheme. However, as the global attention mechanism primarily captures\nlong-range dependencies between nodes, these integration schemes may suffer\nfrom information loss, where the local neighborhood information learned by GNN\ncould be diluted by the attention mechanism. Therefore, we propose G2LFormer,\nfeaturing a novel global-to-local attention scheme where the shallow network\nlayers use attention mechanisms to capture global information, while the deeper\nlayers employ GNN modules to learn local structural information, thereby\npreventing nodes from ignoring their immediate neighbors. An effective\ncross-layer information fusion strategy is introduced to allow local layers to\nretain beneficial information from global layers and alleviate information\nloss, with acceptable trade-offs in scalability. To validate the feasibility of\nthe global-to-local attention scheme, we compare G2LFormer with\nstate-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The\nresults indicate that G2LFormer exhibits excellent performance while keeping\nlinear complexity.", "AI": {"tldr": "G2LFormer is a novel Graph Transformer that uses a global-to-local attention scheme where shallow layers capture global information through attention mechanisms and deeper layers use GNNs to learn local structural information, preventing information loss while maintaining linear complexity.", "motivation": "Existing Graph Transformers integrate GNNs with global attention in parallel or sequential schemes, which may suffer from information loss where local neighborhood information learned by GNNs gets diluted by attention mechanisms.", "method": "Proposes G2LFormer with global-to-local attention: shallow layers use attention to capture global dependencies, deeper layers employ GNN modules for local structure learning, plus cross-layer information fusion strategy to retain beneficial information from global layers.", "result": "G2LFormer shows excellent performance on both node-level and graph-level tasks compared to state-of-the-art linear GTs and GNNs, while maintaining linear complexity.", "conclusion": "The global-to-local attention scheme is feasible and effective, providing better information retention and performance than traditional integration approaches in Graph Transformers."}}
{"id": "2509.14848", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14848", "abs": "https://arxiv.org/abs/2509.14848", "authors": ["Houssem Sifaou", "Osvaldo Simeone"], "title": "Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization", "comment": null, "summary": "Optimizing a reinforcement learning (RL) policy typically requires extensive\ninteractions with a high-fidelity simulator of the environment, which are often\ncostly or impractical. Offline RL addresses this problem by allowing training\nfrom pre-collected data, but its effectiveness is strongly constrained by the\nsize and quality of the dataset. Hybrid offline-online RL leverages both\noffline data and interactions with a single simulator of the environment. In\nmany real-world scenarios, however, multiple simulators with varying levels of\nfidelity and computational cost are available. In this work, we study\nmulti-fidelity hybrid RL for policy optimization under a fixed cost budget. We\nintroduce multi-fidelity hybrid RL via information gain maximization\n(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity\nselection based on information gain maximization through a bootstrapping\napproach. Theoretical analysis establishes the no-regret property of\nMF-HRL-IGM, while empirical evaluations demonstrate its superior performance\ncompared to existing benchmarks.", "AI": {"tldr": "Multi-fidelity hybrid RL algorithm that optimizes policy training by selecting simulation fidelity levels based on information gain maximization under fixed budget constraints.", "motivation": "Traditional RL requires costly high-fidelity simulator interactions, while offline RL is limited by dataset quality. Real-world scenarios often have multiple simulators with varying fidelity levels and costs, but existing methods don't optimally leverage this multi-fidelity setup.", "method": "Proposes MF-HRL-IGM algorithm that uses information gain maximization through bootstrapping to select optimal fidelity levels for hybrid offline-online RL training under fixed cost budgets.", "result": "Theoretical analysis shows no-regret property, and empirical evaluations demonstrate superior performance compared to existing benchmarks in multi-fidelity environments.", "conclusion": "MF-HRL-IGM effectively leverages multiple simulators with varying fidelity levels to optimize RL policy training under budget constraints, outperforming traditional single-fidelity approaches."}}
{"id": "2509.14868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14868", "abs": "https://arxiv.org/abs/2509.14868", "authors": ["Qianyang Li", "Xingjun Zhang", "Shaoxun Wang", "Jia Wei"], "title": "DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting", "comment": null, "summary": "We conducted rigorous ablation studies to validate DPANet's key components\n(Table \\ref{tab:ablation-study}). The full model consistently outperforms all\nvariants. To test our dual-domain hypothesis, we designed two specialized\nversions: a Temporal-Only model (fusing two identical temporal pyramids) and a\nFrequency-Only model (fusing two spectral pyramids). Both variants\nunderperformed significantly, confirming that the fusion of heterogeneous\ntemporal and frequency information is critical. Furthermore, replacing the\ncross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the\nmost severe performance degradation. This result underscores that our\ninteractive fusion block is the most essential component.", "AI": {"tldr": "Ablation studies confirm DPANet's dual-domain architecture and cross-attention fusion mechanism are essential for optimal performance", "motivation": "To validate the importance of DPANet's key components and test the hypothesis that fusion of heterogeneous temporal and frequency information is critical for performance", "method": "Conducted ablation studies comparing the full DPANet model against specialized variants: Temporal-Only model (fusing two identical temporal pyramids), Frequency-Only model (fusing two spectral pyramids), and a version without cross-attention fusion mechanism", "result": "Full model consistently outperformed all variants. Both specialized versions (Temporal-Only and Frequency-Only) underperformed significantly, confirming the importance of dual-domain fusion. Removing cross-attention caused the most severe performance degradation", "conclusion": "The fusion of temporal and frequency information is critical, and the cross-attention interactive fusion block is the most essential component of DPANet"}}
{"id": "2509.15024", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15024", "abs": "https://arxiv.org/abs/2509.15024", "authors": ["Xuanting Xie", "Bingheng Li", "Erlin Pan", "Rui Hou", "Wenyu Chen", "Zhao Kang"], "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering", "comment": "9 pages, 5 figures", "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.", "AI": {"tldr": "AGCN is a novel graph clustering architecture that embeds attention directly into graph structure, addressing limitations of both GNNs (over-localization) and Transformers (over-globalization) through KV cache efficiency and contrastive loss.", "motivation": "Current attention mechanisms underperform on graph data compared to GNNs. GNNs overemphasize neighborhood aggregation causing homogenization, while Transformers over-globalize and miss local patterns. The paper questions if attention is redundant for unsupervised graph learning.", "method": "Proposes Attentive Graph Clustering Network (AGCN) that directly embeds attention into graph structure. Includes KV cache mechanism for computational efficiency and pairwise margin contrastive loss to enhance discriminative capacity in attention space.", "result": "Extensive experiments show AGCN outperforms state-of-the-art methods in graph clustering tasks.", "conclusion": "Attention is not redundant for graph learning when properly integrated. AGCN successfully bridges the gap between local topological sensitivity and global information extraction, demonstrating superior performance over existing approaches."}}
{"id": "2509.15032", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15032", "abs": "https://arxiv.org/abs/2509.15032", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Songxiao Guo", "Yuanye Zhao", "Zheng Lin", "Zihan Fang", "Yi Liu", "Dianxin Luan", "Dong Huang", "Heming Cui", "Yong Cui"], "title": "Sample Efficient Experience Replay in Non-stationary Environments", "comment": "5 pages, 3 figures", "summary": "Reinforcement learning (RL) in non-stationary environments is challenging, as\nchanging dynamics and rewards quickly make past experiences outdated.\nTraditional experience replay (ER) methods, especially those using TD-error\nprioritization, struggle to distinguish between changes caused by the agent's\npolicy and those from the environment, resulting in inefficient learning under\ndynamic conditions. To address this challenge, we propose the Discrepancy of\nEnvironment Dynamics (DoE), a metric that isolates the effects of environment\nshifts on value functions. Building on this, we introduce Discrepancy of\nEnvironment Prioritized Experience Replay (DEER), an adaptive ER framework that\nprioritizes transitions based on both policy updates and environmental changes.\nDEER uses a binary classifier to detect environment changes and applies\ndistinct prioritization strategies before and after each shift, enabling more\nsample-efficient learning. Experiments on four non-stationary benchmarks\ndemonstrate that DEER further improves the performance of off-policy algorithms\nby 11.54 percent compared to the best-performing state-of-the-art ER methods.", "AI": {"tldr": "Proposes DEER, an adaptive experience replay method that prioritizes transitions based on both policy updates and environmental changes in non-stationary RL environments, improving performance by 11.54% over state-of-the-art methods.", "motivation": "Traditional experience replay methods struggle in non-stationary environments because they cannot distinguish between changes caused by the agent's policy versus environmental shifts, leading to inefficient learning.", "method": "Introduces Discrepancy of Environment Dynamics (DoE) metric to isolate environment shift effects, and DEER framework that uses a binary classifier to detect environment changes and applies distinct prioritization strategies before/after each shift.", "result": "Experiments on four non-stationary benchmarks show DEER improves performance of off-policy algorithms by 11.54% compared to the best state-of-the-art ER methods.", "conclusion": "DEER enables more sample-efficient learning in dynamic environments by effectively distinguishing and prioritizing transitions based on both policy and environmental changes."}}
{"id": "2509.14887", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14887", "abs": "https://arxiv.org/abs/2509.14887", "authors": ["Hoang-Son Nguyen", "Hoi-To Wai"], "title": "Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis", "comment": "7 pages, 3 figures", "summary": "Learning the graph underlying a networked system from nodal signals is\ncrucial to downstream tasks in graph signal processing and machine learning.\nThe presence of hidden nodes whose signals are not observable might corrupt the\nestimated graph. While existing works proposed various robustifications of\nvanilla graph learning objectives by explicitly accounting for the presence of\nthese hidden nodes, a robustness analysis of \"naive\", hidden-node agnostic\napproaches is still underexplored. This work demonstrates that vanilla graph\ntopology learning methods are implicitly robust to partial observations of\nlow-pass filtered graph signals. We achieve this theoretical result through\nextending the restricted isometry property (RIP) to the Dirichlet energy\nfunction used in graph learning objectives. We show that smoothness-based graph\nlearning formulation (e.g., the GL-SigRep method) on partial observations can\nrecover the ground truth graph topology corresponding to the observed nodes.\nSynthetic and real data experiments corroborate our findings.", "AI": {"tldr": "Vanilla graph topology learning methods are implicitly robust to partial observations of low-pass filtered graph signals, even when hidden nodes are present.", "motivation": "Existing graph learning methods often assume full nodal observations, but hidden nodes whose signals are unobservable can corrupt graph estimates. While robust methods exist, the inherent robustness of naive approaches to partial observations is underexplored.", "method": "Extend the restricted isometry property (RIP) to the Dirichlet energy function used in graph learning objectives. Analyze smoothness-based graph learning formulations (like GL-SigRep) on partial observations.", "result": "Theoretical analysis shows that vanilla graph learning methods can recover the ground truth graph topology corresponding to observed nodes from partial observations of low-pass filtered signals. Synthetic and real data experiments support these findings.", "conclusion": "Standard graph topology learning approaches possess inherent robustness to hidden nodes when dealing with low-pass filtered graph signals, providing theoretical justification for their effectiveness in partial observation scenarios."}}
{"id": "2509.15040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15040", "abs": "https://arxiv.org/abs/2509.15040", "authors": ["Juwon Kim", "Hyunwook Lee", "Hyotaek Jeon", "Seungmin Jin", "Sungahn Ko"], "title": "From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets", "comment": "10 pages, 7 figures, accepted at ACM CIKM 2025 conference", "summary": "Directional forecasting in financial markets requires both accuracy and\ninterpretability. Before the advent of deep learning, interpretable approaches\nbased on human-defined patterns were prevalent, but their structural vagueness\nand scale ambiguity hindered generalization. In contrast, deep learning models\ncan effectively capture complex dynamics, yet often offer limited transparency.\nTo bridge this gap, we propose a two-stage framework that integrates\nunsupervised pattern extracion with interpretable forecasting. (i) SIMPC\nsegments and clusters multivariate time series, extracting recurrent patterns\nthat are invariant to amplitude scaling and temporal distortion, even under\nvarying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses\nthe initial part of extracted patterns as input and forecasts subsequent\npartial sequences for short-term directional movement. Experiments on Bitcoin\nand three S&P 500 equities demonstrate that our method ranks first or second in\n11 out of 12 metric--dataset combinations, consistently outperforming\nbaselines. Unlike conventional deep learning models that output buy-or-sell\nsignals without interpretable justification, our approach enables transparent\ndecision-making by revealing the underlying pattern structures that drive\npredictive outcomes.", "AI": {"tldr": "A two-stage framework combining unsupervised pattern extraction (SIMPC) and interpretable forecasting (JISC-Net) for financial directional forecasting that achieves top performance while maintaining transparency.", "motivation": "Bridge the gap between interpretable but limited traditional pattern-based methods and accurate but opaque deep learning models in financial forecasting.", "method": "Two-stage approach: (1) SIMPC segments and clusters multivariate time series to extract amplitude- and time-invariant patterns, (2) JISC-Net uses shapelet-based classification on pattern beginnings to forecast short-term directional movements.", "result": "Ranked first or second in 11 out of 12 metric-dataset combinations on Bitcoin and S&P 500 equities, consistently outperforming baselines.", "conclusion": "The framework provides both high accuracy and interpretability by revealing underlying pattern structures that drive predictions, enabling transparent decision-making in financial markets."}}
{"id": "2509.14894", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2509.14894", "abs": "https://arxiv.org/abs/2509.14894", "authors": ["Guillermo Hijano Mendizabal", "Davide Lancierini", "Alex Marshall", "Andrea Mauri", "Patrick Haworth Owen", "Mitesh Patel", "Konstantinos Petridis", "Shah Rukh Qasim", "Nicola Serra", "William Sutcliffe", "Hanae Tilquin"], "title": "Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics", "comment": "32 pages, 12 figures", "summary": "Experimental studies of beauty hadron decays face significant challenges due\nto a wide range of backgrounds arising from the numerous possible decay\nchannels with similar final states. For a particular signal decay, the process\nfor ascertaining the most relevant background processes necessitates a detailed\nanalysis of final state particles, potential misidentifications, and kinematic\noverlaps, which, due to computational limitations, is restricted to the\nsimulation of only the most relevant backgrounds. Moreover, this process\ntypically relies on the physicist's intuition and expertise, as no systematic\nmethod exists.\n  This paper has two primary goals. First, from a particle physics perspective,\nwe present a novel approach that utilises Reinforcement Learning (RL) to\novercome the aforementioned challenges by systematically determining the\ncritical backgrounds affecting beauty hadron decay measurements. While beauty\nhadron physics serves as the case study in this work, the proposed strategy is\nbroadly adaptable to other types of particle physics measurements. Second, from\na Machine Learning perspective, we introduce a novel algorithm which exploits\nthe synergy between RL and Genetic Algorithms (GAs) for environments with\nhighly sparse rewards and a large trajectory space. This strategy leverages GAs\nto efficiently explore the trajectory space and identify successful\ntrajectories, which are used to guide the RL agent's training. Our method also\nincorporates a transformer architecture for the RL agent to handle token\nsequences representing decays.", "AI": {"tldr": "A novel RL-GA hybrid method to systematically identify critical background processes in beauty hadron decay measurements, addressing sparse rewards and large search spaces.", "motivation": "Overcoming challenges in beauty hadron decay analysis where background identification relies on physicist intuition due to computational limitations and lack of systematic methods.", "method": "Combines Reinforcement Learning with Genetic Algorithms, using GAs to explore trajectory space and identify successful paths, plus transformer architecture to handle decay token sequences.", "result": "Developed a systematic approach for determining critical backgrounds in particle physics measurements, particularly for beauty hadron decays.", "conclusion": "The RL-GA hybrid strategy provides an effective solution for sparse reward environments and is broadly applicable beyond beauty hadron physics to other particle physics measurements."}}
{"id": "2509.15042", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15042", "abs": "https://arxiv.org/abs/2509.15042", "authors": ["Thomas Ackermann", "Moritz Spang", "Hamza A. A. Gardi"], "title": "Reinforcement Learning Agent for a 2D Shooter Game", "comment": null, "summary": "Reinforcement learning agents in complex game environments often suffer from\nsparse rewards, training instability, and poor sample efficiency. This paper\npresents a hybrid training approach that combines offline imitation learning\nwith online reinforcement learning for a 2D shooter game agent. We implement a\nmulti-head neural network with separate outputs for behavioral cloning and\nQ-learning, unified by shared feature extraction layers with attention\nmechanisms. Initial experiments using pure deep Q-Networks exhibited\nsignificant instability, with agents frequently reverting to poor policies\ndespite occasional good performance. To address this, we developed a hybrid\nmethodology that begins with behavioral cloning on demonstration data from\nrule-based agents, then transitions to reinforcement learning. Our hybrid\napproach achieves consistently above 70% win rate against rule-based opponents,\nsubstantially outperforming pure reinforcement learning methods which showed\nhigh variance and frequent performance degradation. The multi-head architecture\nenables effective knowledge transfer between learning modes while maintaining\ntraining stability. Results demonstrate that combining demonstration-based\ninitialization with reinforcement learning optimization provides a robust\nsolution for developing game AI agents in complex multi-agent environments\nwhere pure exploration proves insufficient.", "AI": {"tldr": "Hybrid training combining offline imitation learning (behavioral cloning) with online reinforcement learning (Q-learning) using multi-head neural network with attention mechanisms, achieving stable 70%+ win rates in 2D shooter games.", "motivation": "Address sparse rewards, training instability, and poor sample efficiency in complex game environments where pure reinforcement learning methods show high variance and frequent performance degradation.", "method": "Multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Starts with behavioral cloning on demonstration data from rule-based agents, then transitions to reinforcement learning.", "result": "Consistently achieves above 70% win rate against rule-based opponents, substantially outperforming pure reinforcement learning methods which showed high variance and frequent performance degradation.", "conclusion": "Combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments where pure exploration proves insufficient."}}
{"id": "2509.14904", "categories": ["cs.LG", "cs.CG"], "pdf": "https://arxiv.org/pdf/2509.14904", "abs": "https://arxiv.org/abs/2509.14904", "authors": ["Keanu Sisouk", "Eloi Tanguy", "Julie Delon", "Julien Tierny"], "title": "Robust Barycenters of Persistence Diagrams", "comment": null, "summary": "This short paper presents a general approach for computing robust Wasserstein\nbarycenters of persistence diagrams. The classical method consists in computing\nassignment arithmetic means after finding the optimal transport plans between\nthe barycenter and the persistence diagrams. However, this procedure only works\nfor the transportation cost related to the $q$-Wasserstein distance $W_q$ when\n$q=2$. We adapt an alternative fixed-point method to compute a barycenter\ndiagram for generic transportation costs ($q > 1$), in particular those robust\nto outliers, $q \\in (1,2)$. We show the utility of our work in two\napplications: \\emph{(i)} the clustering of persistence diagrams on their metric\nspace and \\emph{(ii)} the dictionary encoding of persistence diagrams. In both\nscenarios, we demonstrate the added robustness to outliers provided by our\ngeneralized framework. Our Python implementation is available at this address:\nhttps://github.com/Keanu-Sisouk/RobustBarycenter .", "AI": {"tldr": "General approach for computing robust Wasserstein barycenters of persistence diagrams using fixed-point method for q>1 transportation costs, particularly robust q\u2208(1,2) cases that handle outliers better than classical q=2 methods.", "motivation": "Classical methods for computing Wasserstein barycenters only work for q=2 Wasserstein distance, limiting robustness to outliers. There's a need for methods that work with generic transportation costs (q>1) to handle outliers more effectively in persistence diagram analysis.", "method": "Adapted fixed-point method to compute barycenter diagrams for generic transportation costs (q>1), particularly focusing on robust q\u2208(1,2) cases. The approach handles optimal transport plans between barycenter and persistence diagrams.", "result": "Successfully implemented robust barycenter computation for persistence diagrams with q>1 transportation costs. Demonstrated utility in clustering persistence diagrams and dictionary encoding, showing improved robustness to outliers compared to classical q=2 methods.", "conclusion": "The proposed fixed-point method provides a generalized framework for computing robust Wasserstein barycenters that works for q>1 transportation costs, offering significant improvements in outlier robustness for persistence diagram analysis applications."}}
{"id": "2509.15044", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15044", "abs": "https://arxiv.org/abs/2509.15044", "authors": ["Iva Popova", "Hamza A. A. Gardi"], "title": "Credit Card Fraud Detection", "comment": null, "summary": "Credit card fraud remains a significant challenge due to class imbalance and\nfraudsters mimicking legitimate behavior. This study evaluates five machine\nlearning models - Logistic Regression, Random Forest, XGBoost, K-Nearest\nNeighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using\nundersampling, SMOTE, and a hybrid approach. Our models are evaluated on the\noriginal imbalanced test set to better reflect real-world performance. Results\nshow that the hybrid method achieves the best balance between recall and\nprecision, especially improving MLP and KNN performance.", "AI": {"tldr": "Evaluation of 5 ML models for credit card fraud detection using undersampling, SMOTE, and hybrid approaches on imbalanced data, with hybrid method showing best performance.", "motivation": "Credit card fraud detection faces challenges from class imbalance and fraudsters mimicking legitimate behavior, requiring robust ML approaches.", "method": "Tested Logistic Regression, Random Forest, XGBoost, KNN, and MLP on real-world dataset using undersampling, SMOTE, and hybrid sampling techniques, evaluated on original imbalanced test set.", "result": "Hybrid sampling method achieved the best balance between recall and precision, particularly improving performance of MLP and KNN models.", "conclusion": "Hybrid sampling approach is most effective for credit card fraud detection, providing optimal performance metrics while handling class imbalance challenges."}}
{"id": "2509.14925", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.14925", "abs": "https://arxiv.org/abs/2509.14925", "authors": ["Konrad Nowosadko", "Franco Ruggeri", "Ahmad Terra"], "title": "Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation", "comment": null, "summary": "Reinforcement Learning (RL) methods that incorporate deep neural networks\n(DNN), though powerful, often lack transparency. Their black-box characteristic\nhinders interpretability and reduces trustworthiness, particularly in critical\ndomains. To address this challenge in RL tasks, we propose a solution based on\nSelf-Explaining Neural Networks (SENNs) along with explanation extraction\nmethods to enhance interpretability while maintaining predictive accuracy. Our\napproach targets low-dimensionality problems to generate robust local and\nglobal explanations of the model's behaviour. We evaluate the proposed method\non the resource allocation problem in mobile networks, demonstrating that SENNs\ncan constitute interpretable solutions with competitive performance. This work\nhighlights the potential of SENNs to improve transparency and trust in\nAI-driven decision-making for low-dimensional tasks. Our approach strong\nperformance on par with the existing state-of-the-art methods, while providing\nrobust explanations.", "AI": {"tldr": "Proposes using Self-Explaining Neural Networks (SENNs) to enhance interpretability in reinforcement learning while maintaining competitive performance on low-dimensional tasks like mobile network resource allocation.", "motivation": "Address the lack of transparency in deep reinforcement learning methods, which hinders interpretability and reduces trustworthiness in critical domains due to their black-box nature.", "method": "Uses Self-Explaining Neural Networks (SENNs) with explanation extraction methods to generate robust local and global explanations of model behavior, specifically targeting low-dimensionality problems.", "result": "Demonstrated competitive performance on par with state-of-the-art methods while providing robust explanations for the resource allocation problem in mobile networks.", "conclusion": "SENNs can constitute interpretable solutions that improve transparency and trust in AI-driven decision-making for low-dimensional tasks without sacrificing predictive accuracy."}}
{"id": "2509.15057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15057", "abs": "https://arxiv.org/abs/2509.15057", "authors": ["Quincy Hershey", "Randy Paffenroth"], "title": "Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning", "comment": null, "summary": "This paper develops alternative hyperparameters for specifying sparse\nRecurrent Neural Networks (RNNs). These hyperparameters allow for varying\nsparsity within the trainable weight matrices of the model while improving\noverall performance. This architecture enables the definition of a novel\nmetric, hidden proportion, which seeks to balance the distribution of unknowns\nwithin the model and provides significant explanatory power of model\nperformance. Together, the use of the varied sparsity RNN architecture combined\nwith the hidden proportion metric generates significant performance gains while\nimproving performance expectations on an a priori basis. This combined approach\nprovides a path forward towards generalized meta-learning applications and\nmodel optimization based on intrinsic characteristics of the data set,\nincluding input and output dimensions.", "AI": {"tldr": "Novel hyperparameters for sparse RNNs enabling variable sparsity with improved performance, introducing hidden proportion metric for better model balancing and explanatory power.", "motivation": "To develop alternative hyperparameters that allow varying sparsity in RNN weight matrices while enhancing performance, and to create a metric that balances unknown distributions within models.", "method": "Developed varied sparsity RNN architecture with alternative hyperparameters and introduced hidden proportion metric to balance unknown distributions and explain model performance.", "result": "Achieved significant performance gains and improved a priori performance expectations through the combined approach of varied sparsity architecture and hidden proportion metric.", "conclusion": "The approach provides a path toward generalized meta-learning applications and model optimization based on intrinsic dataset characteristics including input/output dimensions."}}
{"id": "2509.14933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14933", "abs": "https://arxiv.org/abs/2509.14933", "authors": ["Xiangfei Qiu", "Yuhan Zhu", "Zhengyu Li", "Hanyin Cheng", "Xingjian Wu", "Chenjuan Guo", "Bin Yang", "Jilin Hu"], "title": "DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables", "comment": null, "summary": "Time series forecasting is crucial in various fields such as economics,\ntraffic, and AIOps. However, in real-world applications, focusing solely on the\nendogenous variables (i.e., target variables), is often insufficient to ensure\naccurate predictions. Considering exogenous variables (i.e., covariates)\nprovides additional predictive information, thereby improving forecasting\naccuracy. However, existing methods for time series forecasting with exogenous\nvariables (TSF-X) have the following shortcomings: 1) they do not leverage\nfuture exogenous variables, 2) they fail to account for the causal\nrelationships between endogenous and exogenous variables. As a result, their\nperformance is suboptimal. In this study, to better leverage exogenous\nvariables, especially future exogenous variable, we propose a general framework\nDAG, which utilizes dual causal network along both the temporal and channel\ndimensions for time series forecasting with exogenous variables. Specifically,\nwe first introduce the Temporal Causal Module, which includes a causal\ndiscovery module to capture how historical exogenous variables affect future\nexogenous variables. Following this, we construct a causal injection module\nthat incorporates the discovered causal relationships into the process of\nforecasting future endogenous variables based on historical endogenous\nvariables. Next, we propose the Channel Causal Module, which follows a similar\ndesign principle. It features a causal discovery module models how historical\nexogenous variables influence historical endogenous variables, and a causal\ninjection module incorporates the discovered relationships to enhance the\nprediction of future endogenous variables based on future exogenous variables.", "AI": {"tldr": "Proposes DAG framework with dual causal networks for time series forecasting with exogenous variables, addressing limitations of existing methods by leveraging future exogenous variables and modeling causal relationships.", "motivation": "Existing time series forecasting methods with exogenous variables fail to utilize future exogenous variables and ignore causal relationships between endogenous and exogenous variables, leading to suboptimal performance.", "method": "Dual causal network framework with Temporal Causal Module (captures how historical exogenous variables affect future exogenous variables) and Channel Causal Module (models how historical exogenous variables influence historical endogenous variables), both with causal discovery and injection components.", "result": "The proposed DAG framework better leverages exogenous variables, especially future ones, by incorporating causal relationships along temporal and channel dimensions to improve forecasting accuracy.", "conclusion": "The DAG framework provides a comprehensive approach to time series forecasting with exogenous variables by addressing causal relationships, enabling more accurate predictions through better utilization of both historical and future exogenous information."}}
{"id": "2509.15058", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.15058", "abs": "https://arxiv.org/abs/2509.15058", "authors": ["Federico Alvetreti", "Jary Pomponi", "Paolo Di Lorenzo", "Simone Scardapane"], "title": "Communication Efficient Split Learning of ViTs with Attention-based Double Compression", "comment": null, "summary": "This paper proposes a novel communication-efficient Split Learning (SL)\nframework, named Attention-based Double Compression (ADC), which reduces the\ncommunication overhead required for transmitting intermediate Vision\nTransformers activations during the SL training process. ADC incorporates two\nparallel compression strategies. The first one merges samples' activations that\nare similar, based on the average attention score calculated in the last client\nlayer; this strategy is class-agnostic, meaning that it can also merge samples\nhaving different classes, without losing generalization ability nor decreasing\nfinal results. The second strategy follows the first and discards the least\nmeaningful tokens, further reducing the communication cost. Combining these\nstrategies not only allows for sending less during the forward pass, but also\nthe gradients are naturally compressed, allowing the whole model to be trained\nwithout additional tuning or approximations of the gradients. Simulation\nresults demonstrate that Attention-based Double Compression outperforms\nstate-of-the-art SL frameworks by significantly reducing communication\noverheads while maintaining high accuracy.", "AI": {"tldr": "ADC framework reduces communication overhead in Split Learning for Vision Transformers by using two parallel compression strategies: merging similar activations and discarding least meaningful tokens.", "motivation": "To address the high communication overhead required for transmitting intermediate Vision Transformer activations during Split Learning training process.", "method": "Attention-based Double Compression (ADC) with two strategies: 1) class-agnostic merging of similar activations based on average attention scores, 2) discarding least meaningful tokens to further reduce communication.", "result": "ADC significantly reduces communication overheads while maintaining high accuracy, outperforming state-of-the-art SL frameworks.", "conclusion": "The proposed ADC framework enables efficient Split Learning for Vision Transformers by naturally compressing both activations and gradients without additional tuning."}}
{"id": "2509.14936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14936", "abs": "https://arxiv.org/abs/2509.14936", "authors": ["Rohan Veit", "Michael Lones"], "title": "A Comparative Analysis of Transformer Models in Social Bot Detection", "comment": "To appear in proceedings of UKCI 2025", "summary": "Social media has become a key medium of communication in today's society.\nThis realisation has led to many parties employing artificial users (or bots)\nto mislead others into believing untruths or acting in a beneficial manner to\nsuch parties. Sophisticated text generation tools, such as large language\nmodels, have further exacerbated this issue. This paper aims to compare the\neffectiveness of bot detection models based on encoder and decoder\ntransformers. Pipelines are developed to evaluate the performance of these\nclassifiers, revealing that encoder-based classifiers demonstrate greater\naccuracy and robustness. However, decoder-based models showed greater\nadaptability through task-specific alignment, suggesting more potential for\ngeneralisation across different use cases in addition to superior observa.\nThese findings contribute to the ongoing effort to prevent digital environments\nbeing manipulated while protecting the integrity of online discussion.", "AI": {"tldr": "Comparison of encoder vs decoder transformer models for bot detection, showing encoders are more accurate but decoders have better generalization potential.", "motivation": "Social media manipulation through AI-generated bots using advanced text generation tools requires effective detection methods to maintain online discussion integrity.", "method": "Developed evaluation pipelines to test performance of encoder-based and decoder-based transformer classifiers for bot detection.", "result": "Encoder-based classifiers demonstrated greater accuracy and robustness, while decoder-based models showed greater adaptability through task-specific alignment with superior generalization potential.", "conclusion": "Both encoder and decoder approaches contribute to combating social media manipulation, with encoders offering better accuracy and decoders providing better generalization across different use cases."}}
{"id": "2509.14938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14938", "abs": "https://arxiv.org/abs/2509.14938", "authors": ["Zeyu Chen", "Wen Chen", "Jun Li", "Qingqing Wu", "Ming Ding", "Xuefeng Han", "Xiumei Deng", "Liwei Wang"], "title": "Hierarchical Federated Learning for Social Network with Mobility", "comment": null, "summary": "Federated Learning (FL) offers a decentralized solution that allows\ncollaborative local model training and global aggregation, thereby protecting\ndata privacy. In conventional FL frameworks, data privacy is typically\npreserved under the assumption that local data remains absolutely private,\nwhereas the mobility of clients is frequently neglected in explicit modeling.\nIn this paper, we propose a hierarchical federated learning framework based on\nthe social network with mobility namely HFL-SNM that considers both data\nsharing among clients and their mobility patterns. Under the constraints of\nlimited resources, we formulate a joint optimization problem of resource\nallocation and client scheduling, which objective is to minimize the energy\nconsumption of clients during the FL process. In social network, we introduce\nthe concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate.\nWe analyze the impact of effective data and redundant data on the model\nperformance through preliminary experiments. We decouple the optimization\nproblem into multiple sub-problems, analyze them based on preliminary\nexperimental results, and propose Dynamic Optimization in Social Network with\nMobility (DO-SNM) algorithm. Experimental results demonstrate that our\nalgorithm achieves superior model performance while significantly reducing\nenergy consumption, compared to traditional baseline algorithms.", "AI": {"tldr": "Proposes HFL-SNM framework that integrates social networks and client mobility into federated learning, optimizing resource allocation and client scheduling to minimize energy consumption while maintaining model performance.", "motivation": "Traditional FL frameworks assume static clients and absolute data privacy, but neglect client mobility patterns and potential data sharing opportunities in social networks that could improve efficiency.", "method": "Develops hierarchical FL framework with social network mobility (HFL-SNM), introduces Effective/Redundant Data Coverage Rate concepts, formulates joint optimization problem for resource allocation and client scheduling, and proposes DO-SNM algorithm to solve it.", "result": "Experimental results show the proposed algorithm achieves better model performance while significantly reducing energy consumption compared to traditional baseline algorithms.", "conclusion": "The HFL-SNM framework successfully addresses both data privacy and mobility challenges in FL, demonstrating that considering social network relationships and mobility patterns can optimize resource usage and improve overall system efficiency."}}
{"id": "2509.15207", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15207", "abs": "https://arxiv.org/abs/2509.15207", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Dinghuai Zhang", "Hengli Li", "Kaiyan Zhang", "Che Jiang", "Youbang Sun", "Ermo Hua", "Yuxin Zuo", "Xingtai Lv", "Qizheng Zhang", "Lin Chen", "Fanghao Shao", "Bo Xue", "Yunchong Song", "Zhenjie Yang", "Ganqu Cui", "Ning Ding", "Jianfeng Gao", "Xiaodong Liu", "Bowen Zhou", "Hongyuan Mei", "Zhouhan Lin"], "title": "FlowRL: Matching Reward Distributions for LLM Reasoning", "comment": null, "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.", "AI": {"tldr": "FlowRL is a new reinforcement learning method for LLMs that matches full reward distributions instead of maximizing rewards, achieving better diversity and performance on math and code reasoning tasks.", "motivation": "Traditional reward-maximizing methods like PPO and GRPO tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, reducing diversity in LLM reasoning.", "method": "Transform scalar rewards into a normalized target distribution using a learnable partition function, then minimize reverse KL divergence between policy and target distribution through flow-balanced optimization.", "result": "Achieves 10.0% average improvement over GRPO and 5.1% over PPO on math benchmarks, with consistent better performance on code reasoning tasks.", "conclusion": "Reward distribution-matching is a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning."}}
{"id": "2509.14945", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14945", "abs": "https://arxiv.org/abs/2509.14945", "authors": ["Amsalu Tessema", "Tizazu Bayih", "Kassahun Azezew", "Ayenew Kassie"], "title": "Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models", "comment": "9 pages, 5 figures, 2 Tables", "summary": "Malnutrition among pregnant women is a major public health challenge in\nEthiopia, increasing the risk of adverse maternal and neonatal outcomes.\nTraditional statistical approaches often fail to capture the complex and\nmultidimensional determinants of nutritional status. This study develops a\npredictive model using ensemble machine learning techniques, leveraging data\nfrom the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108\nrecords with 30 socio-demographic and health attributes. Data preprocessing\nincluded handling missing values, normalization, and balancing with SMOTE,\nfollowed by feature selection to identify key predictors. Several supervised\nensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost\nwere applied to classify nutritional status. Among them, the Random Forest\nmodel achieved the best performance, classifying women into four categories\n(normal, moderate malnutrition, severe malnutrition, and overnutrition) with\n97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86%\nROC AUC. These findings demonstrate the effectiveness of ensemble learning in\ncapturing hidden patterns from complex datasets and provide timely insights for\nearly detection of nutritional risks. The results offer practical implications\nfor healthcare providers, policymakers, and researchers, supporting data-driven\nstrategies to improve maternal nutrition and health outcomes in Ethiopia.", "AI": {"tldr": "Ensemble machine learning models, particularly Random Forest, achieved 97.87% accuracy in classifying pregnant women's nutritional status in Ethiopia using demographic and health survey data.", "motivation": "Malnutrition among pregnant women in Ethiopia is a major public health challenge with risk of adverse outcomes, and traditional statistical methods fail to capture complex multidimensional determinants of nutritional status.", "method": "Used ensemble machine learning techniques (XGBoost, Random Forest, CatBoost, AdaBoost) on Ethiopian Demographic and Health Survey data (2005-2020, 18,108 records, 30 attributes) with preprocessing including missing value handling, normalization, SMOTE balancing, and feature selection.", "result": "Random Forest model performed best with 97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86% ROC AUC in classifying four nutritional categories (normal, moderate malnutrition, severe malnutrition, overnutrition).", "conclusion": "Ensemble learning effectively captures hidden patterns from complex datasets for early nutritional risk detection, providing practical implications for healthcare providers and policymakers to improve maternal nutrition in Ethiopia."}}
{"id": "2509.14952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14952", "abs": "https://arxiv.org/abs/2509.14952", "authors": ["Zhuanghua Liu", "Luo Luo"], "title": "Stochastic Bilevel Optimization with Heavy-Tailed Noise", "comment": null, "summary": "This paper considers the smooth bilevel optimization in which the lower-level\nproblem is strongly convex and the upper-level problem is possibly nonconvex.\nWe focus on the stochastic setting that the algorithm can access the unbiased\nstochastic gradient evaluation with heavy-tailed noise, which is prevalent in\nmany machine learning applications such as training large language models and\nreinforcement learning. We propose a nested-loop normalized stochastic bilevel\napproximation (N$^2$SBA) for finding an $\\epsilon$-stationary point with the\nstochastic first-order oracle (SFO) complexity of\n$\\tilde{\\mathcal{O}}\\big(\\kappa^{\\frac{7p-3}{p-1}} \\sigma^{\\frac{p}{p-1}}\n\\epsilon^{-\\frac{4 p - 2}{p-1}}\\big)$, where $\\kappa$ is the condition number,\n$p\\in(1,2]$ is the order of central moment for the noise, and $\\sigma$ is the\nnoise level. Furthermore, we specialize our idea to solve the\nnonconvex-strongly-concave minimax optimization problem, achieving an\n$\\epsilon$-stationary point with the SFO complexity of $\\tilde{\\mathcal\nO}\\big(\\kappa^{\\frac{2p-1}{p-1}} \\sigma^{\\frac{p}{p-1}}\n\\epsilon^{-\\frac{3p-2}{p-1}}\\big)$. All above upper bounds match the best-known\nresults under the special case of the bounded variance setting, i.e., $p=2$.", "AI": {"tldr": "A stochastic bilevel optimization method for nonconvex-strongly convex problems with heavy-tailed noise, achieving improved complexity bounds.", "motivation": "Address stochastic bilevel optimization with heavy-tailed noise common in ML applications like LLM training and RL, where traditional bounded variance assumptions don't hold.", "method": "Propose nested-loop normalized stochastic bilevel approximation (N\u00b2SBA) algorithm that handles heavy-tailed noise using normalized gradients and nested-loop structure.", "result": "Achieves \u03f5-stationary point with SFO complexity \u01a9(\u03ba^{(7p-3)/(p-1)} \u03c3^{p/(p-1)} \u03f5^{-(4p-2)/(p-1)}) for bilevel optimization and \u01a9(\u03ba^{(2p-1)/(p-1)} \u03c3^{p/(p-1)} \u03f5^{-(3p-2)/(p-1)}) for minimax optimization.", "conclusion": "The method provides optimal complexity bounds that match best-known results for bounded variance case (p=2) while handling more general heavy-tailed noise distributions."}}
{"id": "2509.14968", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.14968", "abs": "https://arxiv.org/abs/2509.14968", "authors": ["Carlos Barroso-Fern\u00e1ndez", "Alejandro Calvillo-Fernandez", "Antonio de la Oliva", "Carlos J. Bernardos"], "title": "FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference", "comment": "7 pages, 6 figures and tables, less than 5500 words. Under revision\n  at IEEE Communication Magazine", "summary": "The upcoming generations of wireless technologies promise an era where\neverything is interconnected and intelligent. As the need for intelligence\ngrows, networks must learn to better understand the physical world. However,\ndeploying dedicated hardware to perceive the environment is not always\nfeasible, mainly due to costs and/or complexity. Integrated Sensing and\nCommunication (ISAC) has made a step forward in addressing this challenge.\nWithin ISAC, passive sensing emerges as a cost-effective solution that reuses\nwireless communications to sense the environment, without interfering with\nexisting communications. Nevertheless, the majority of current solutions are\nlimited to one technology (mostly Wi-Fi or 5G), constraining the maximum\naccuracy reachable. As different technologies work with different spectrums, we\nsee a necessity in integrating more than one technology to augment the coverage\narea. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a\nMultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.\nFAWN is based on the original transformers architecture, to fuse information\nfrom Wi-Fi and 5G, making the network capable of understanding the physical\nworld without interfering with the current communication. To test our solution,\nwe have built a prototype and integrated it in a real scenario. Results show\nerrors below 0.6 m around 84% of times.", "AI": {"tldr": "FAWN is a MultiEncoder Fusion-Attention Wave Network that fuses Wi-Fi and 5G signals for indoor scene inference using ISAC passive sensing, achieving sub-0.6m accuracy 84% of the time.", "motivation": "Current ISAC passive sensing solutions are limited to single technologies (Wi-Fi or 5G), constraining accuracy. Different wireless technologies operate in different spectrums, creating a need to integrate multiple technologies to enhance coverage and sensing capabilities.", "method": "FAWN uses a transformer-based architecture with multi-encoder fusion-attention to combine information from both Wi-Fi and 5G signals for passive environmental sensing without interfering with existing communications.", "result": "The prototype achieved errors below 0.6 meters approximately 84% of the time in real-world testing scenarios.", "conclusion": "Integrating multiple wireless technologies through FAWN's transformer-based fusion approach significantly improves indoor scene inference accuracy while maintaining the cost-effectiveness and non-interference benefits of passive ISAC sensing."}}
{"id": "2509.14969", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.14969", "abs": "https://arxiv.org/abs/2509.14969", "authors": ["Jean-Fran\u00e7ois Aujol", "J\u00e9r\u00e9mie Bigot", "Camille Castera"], "title": "Stochastic Adaptive Gradient Descent Without Descent", "comment": null, "summary": "We introduce a new adaptive step-size strategy for convex optimization with\nstochastic gradient that exploits the local geometry of the objective function\nonly by means of a first-order stochastic oracle and without any\nhyper-parameter tuning. The method comes from a theoretically-grounded\nadaptation of the Adaptive Gradient Descent Without Descent method to the\nstochastic setting. We prove the convergence of stochastic gradient descent\nwith our step-size under various assumptions, and we show that it empirically\ncompetes against tuned baselines.", "AI": {"tldr": "New adaptive step-size strategy for stochastic gradient descent that requires no hyperparameter tuning and uses only first-order stochastic oracle information.", "motivation": "To develop a theoretically-grounded adaptive step-size method for stochastic convex optimization that eliminates the need for manual hyperparameter tuning while maintaining competitive performance.", "method": "Adaptation of the Adaptive Gradient Descent Without Descent method to stochastic setting, exploiting local geometry of objective function using only first-order stochastic oracle information.", "result": "Proven convergence under various assumptions and empirical demonstration of competitive performance against tuned baseline methods.", "conclusion": "The proposed adaptive step-size strategy provides an effective, theoretically-sound approach for stochastic gradient descent that eliminates hyperparameter tuning while maintaining competitive empirical performance."}}
{"id": "2509.15033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15033", "abs": "https://arxiv.org/abs/2509.15033", "authors": ["Padmaksha Roy", "Almuatazbellah Boker", "Lamine Mili"], "title": "Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection", "comment": null, "summary": "In this paper, we aim to improve multivariate anomaly detection (AD) by\nmodeling the \\textit{time-varying non-linear spatio-temporal correlations}\nfound in multivariate time series data . In multivariate time series data, an\nanomaly may be indicated by the simultaneous deviation of interrelated time\nseries from their expected collective behavior, even when no individual time\nseries exhibits a clearly abnormal pattern on its own. In many existing\napproaches, time series variables are assumed to be (conditionally)\nindependent, which oversimplifies real-world interactions. Our approach\naddresses this by modeling joint dependencies in the latent space and\ndecoupling the modeling of \\textit{marginal distributions, temporal dynamics,\nand inter-variable dependencies}. We use a transformer encoder to capture\ntemporal patterns, and to model spatial (inter-variable) dependencies, we fit a\nmulti-variate likelihood and a copula. The temporal and the spatial components\nare trained jointly in a latent space using a self-supervised contrastive\nlearning objective to learn meaningful feature representations to separate\nnormal and anomaly samples.", "AI": {"tldr": "Proposes a novel multivariate anomaly detection method that models time-varying non-linear spatio-temporal correlations using transformer encoders and copula models in a joint learning framework.", "motivation": "Existing approaches oversimplify real-world interactions by assuming time series variables are independent, missing the collective behavior patterns that indicate anomalies when individual series appear normal.", "method": "Uses transformer encoder for temporal patterns, multivariate likelihood and copula for spatial dependencies, trained jointly in latent space with self-supervised contrastive learning to separate normal and anomaly samples.", "result": "The approach effectively captures complex spatio-temporal correlations and decouples marginal distributions, temporal dynamics, and inter-variable dependencies for improved anomaly detection.", "conclusion": "Modeling joint dependencies in latent space with transformer and copula components provides superior anomaly detection performance by capturing the collective behavior patterns in multivariate time series data."}}
{"id": "2509.15060", "categories": ["cs.LG", "cs.IT", "math.IT", "math.ST", "stat.CO", "stat.ML", "stat.TH", "94A20, 94A13, 94A12, 94A08, 94-08, 94-04, 68T07, 68P30", "G.3; E.4; I.2; I.2.6; I.5.5"], "pdf": "https://arxiv.org/pdf/2509.15060", "abs": "https://arxiv.org/abs/2509.15060", "authors": ["Lukas Silvester Barth", "Paulo von Petersenn"], "title": "Probabilistic and nonlinear compressive sensing", "comment": null, "summary": "We present a smooth probabilistic reformulation of $\\ell_0$ regularized\nregression that does not require Monte Carlo sampling and allows for the\ncomputation of exact gradients, facilitating rapid convergence to local optima\nof the best subset selection problem. The method drastically improves\nconvergence speed compared to similar Monte Carlo based approaches.\nFurthermore, we empirically demonstrate that it outperforms compressive sensing\nalgorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and\nsignal-to-noise ratios. The implementation runs efficiently on both CPUs and\nGPUs and is freely available at\nhttps://github.com/L0-and-behold/probabilistic-nonlinear-cs.\n  We also contribute to research on nonlinear generalizations of compressive\nsensing by investigating when parameter recovery of a nonlinear teacher network\nis possible through compression of a student network. Building upon theorems of\nFefferman and Markel, we show theoretically that the global optimum in the\ninfinite-data limit enforces recovery up to certain symmetries. For empirical\nvalidation, we implement a normal-form algorithm that selects a canonical\nrepresentative within each symmetry class. However, while compression can help\nto improve test loss, we find that exact parameter recovery is not even\npossible up to symmetries. In particular, we observe a surprising rebound\neffect where teacher and student configurations initially converge but\nsubsequently diverge despite continuous decrease in test loss. These findings\nindicate fundamental differences between linear and nonlinear compressive\nsensing.", "AI": {"tldr": "A smooth probabilistic reformulation of \u21130 regularized regression that enables exact gradient computation without Monte Carlo sampling, significantly improving convergence speed and outperforming compressive sensing algorithms like IHT and Lasso.", "motivation": "To address the computational challenges of \u21130 regularized regression by developing a method that avoids Monte Carlo sampling while enabling exact gradient computation for faster convergence to local optima in best subset selection problems.", "method": "Probabilistic reformulation of \u21130 regularization that computes exact gradients without Monte Carlo sampling. Also investigates nonlinear compressive sensing by studying parameter recovery of nonlinear teacher networks through student network compression, building on Fefferman and Markel theorems.", "result": "The method drastically improves convergence speed compared to Monte Carlo approaches and outperforms compressive sensing algorithms (IHT, Relaxed-Lasso) across various settings and SNR ratios. However, exact parameter recovery in nonlinear compressive sensing is not possible even up to symmetries, with observed rebound effect where configurations initially converge then diverge despite decreasing test loss.", "conclusion": "The approach provides efficient \u21130 regularization with exact gradients and fast convergence. For nonlinear compressive sensing, fundamental differences from linear case exist - compression can improve test loss but exact parameter recovery is impossible even with symmetries, indicating theoretical limitations in nonlinear settings."}}
{"id": "2509.15072", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15072", "abs": "https://arxiv.org/abs/2509.15072", "authors": ["Martha Cash", "Alexander Wyglinski"], "title": "Improving Internet Traffic Matrix Prediction via Time Series Clustering", "comment": "Accepted to ICMLA 2025", "summary": "We present a novel framework that leverages time series clustering to improve\ninternet traffic matrix (TM) prediction using deep learning (DL) models.\nTraffic flows within a TM often exhibit diverse temporal behaviors, which can\nhinder prediction accuracy when training a single model across all flows. To\naddress this, we propose two clustering strategies, source clustering and\nhistogram clustering, that group flows with similar temporal patterns prior to\nmodel training. Clustering creates more homogeneous data subsets, enabling\nmodels to capture underlying patterns more effectively and generalize better\nthan global prediction approaches that fit a single model to the entire TM.\nCompared to existing TM prediction methods, our method reduces RMSE by up to\n92\\% for Abilene and 75\\% for G\\'EANT. In routing scenarios, our clustered\npredictions also reduce maximum link utilization (MLU) bias by 18\\% and 21\\%,\nrespectively, demonstrating the practical benefits of clustering when TMs are\nused for network optimization.", "AI": {"tldr": "A framework using time series clustering to group internet traffic flows by similar temporal patterns before training deep learning models, significantly improving prediction accuracy and network optimization performance.", "motivation": "Traffic flows in internet traffic matrices exhibit diverse temporal behaviors, which reduces prediction accuracy when using a single model for all flows.", "method": "Proposes two clustering strategies (source clustering and histogram clustering) to group flows with similar temporal patterns, then trains deep learning models on these homogeneous clusters.", "result": "Reduces RMSE by up to 92% for Abilene and 75% for G\u00c9ANT networks. Also reduces maximum link utilization bias by 18% and 21% respectively in routing scenarios.", "conclusion": "Clustering traffic flows before model training significantly improves prediction accuracy and demonstrates practical benefits for network optimization tasks."}}
{"id": "2509.15073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15073", "abs": "https://arxiv.org/abs/2509.15073", "authors": ["Shaoang Li", "Jian Li"], "title": "Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits", "comment": null, "summary": "Non-stationary multi-armed bandits enable agents to adapt to changing\nenvironments by incorporating mechanisms to detect and respond to shifts in\nreward distributions, making them well-suited for dynamic settings. However,\nexisting approaches typically assume that reward feedback is available at every\nround - an assumption that overlooks many real-world scenarios where feedback\nis limited. In this paper, we take a significant step forward by introducing a\nnew model of constrained feedback in non-stationary multi-armed bandits, where\nthe availability of reward feedback is restricted. We propose the first\nprior-free algorithm - that is, one that does not require prior knowledge of\nthe degree of non-stationarity - that achieves near-optimal dynamic regret in\nthis setting. Specifically, our algorithm attains a dynamic regret of\n$\\tilde{\\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the\nnumber of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$\nis the variation budget capturing the degree of non-stationarity.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.15076", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15076", "abs": "https://arxiv.org/abs/2509.15076", "authors": ["Mohammad Saleh Vahdatpour", "Maryam Eyvazi", "Yanqing Zhang"], "title": "Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models", "comment": "Published at ICCVW 2025", "summary": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms.", "AI": {"tldr": "AI system predicts air pollution from sky images and generates visual pollution scenarios using generative modeling and vision-language models for public awareness.", "motivation": "Conventional air pollution monitoring has limited spatial coverage and accessibility, requiring more transparent and interpretable systems for public engagement and environmental decision-making.", "method": "Combines statistical texture analysis with supervised learning for pollution classification, and uses VLM-guided image generation to create realistic pollution visualizations from sky images.", "result": "Validated on urban sky image dataset, effective in pollution level estimation and semantically consistent visual synthesis, with human-centered design for accessibility.", "conclusion": "System provides foundation for user-facing interfaces that improve transparency in air quality forecasting, with plans for green CNN architecture and FPGA-based incremental learning for scalable edge deployment."}}
{"id": "2509.15087", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15087", "abs": "https://arxiv.org/abs/2509.15087", "authors": ["Lei Wang", "Jieming Bian", "Letian Zhang", "Jie Xu"], "title": "Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning", "comment": "Accepted to NeurIPS 2025", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but fine-tuning them for domain-specific applications often\nrequires substantial domain-specific data that may be distributed across\nmultiple organizations. Federated Learning (FL) offers a privacy-preserving\nsolution, but faces challenges with computational constraints when applied to\nLLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient\nfine-tuning approach, though a single LoRA module often struggles with\nheterogeneous data across diverse domains. This paper addresses two critical\nchallenges in federated LoRA fine-tuning: 1. determining the optimal number and\nallocation of LoRA experts across heterogeneous clients, and 2. enabling\nclients to selectively utilize these experts based on their specific data\ncharacteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation\nand SElection), a novel framework that adaptively clusters clients based on\nrepresentation similarity to allocate and train domain-specific LoRA experts.\nIt also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows\neach client to select the optimal number of utilized experts. Our extensive\nexperiments on diverse benchmark datasets demonstrate that FedLEASE\nsignificantly outperforms existing federated fine-tuning approaches in\nheterogeneous client settings while maintaining communication efficiency.", "AI": {"tldr": "FedLEASE is a federated learning framework that adaptively allocates and selects LoRA experts for efficient LLM fine-tuning across heterogeneous clients.", "motivation": "Fine-tuning LLMs for domain-specific applications requires substantial distributed data, but federated learning faces computational constraints and single LoRA modules struggle with heterogeneous data.", "method": "Proposes FedLEASE framework that clusters clients based on representation similarity to allocate domain-specific LoRA experts, and uses adaptive top-M Mixture-of-Experts mechanism for client-specific expert selection.", "result": "Extensive experiments on diverse benchmark datasets show FedLEASE significantly outperforms existing federated fine-tuning approaches in heterogeneous settings while maintaining communication efficiency.", "conclusion": "FedLEASE effectively addresses the challenges of optimal LoRA expert allocation and selection in federated learning environments with heterogeneous client data."}}
{"id": "2509.15090", "categories": ["cs.LG", "cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.15090", "abs": "https://arxiv.org/abs/2509.15090", "authors": ["Natalie Collina", "Surbhi Goel", "Aaron Roth", "Emily Ryu", "Mirah Shi"], "title": "Emergent Alignment via Competition", "comment": null, "summary": "Aligning AI systems with human values remains a fundamental challenge, but\ndoes our inability to create perfectly aligned models preclude obtaining the\nbenefits of alignment? We study a strategic setting where a human user\ninteracts with multiple differently misaligned AI agents, none of which are\nindividually well-aligned. Our key insight is that when the users utility lies\napproximately within the convex hull of the agents utilities, a condition that\nbecomes easier to satisfy as model diversity increases, strategic competition\ncan yield outcomes comparable to interacting with a perfectly aligned model. We\nmodel this as a multi-leader Stackelberg game, extending Bayesian persuasion to\nmulti-round conversations between differently informed parties, and prove three\nresults: (1) when perfect alignment would allow the user to learn her\nBayes-optimal action, she can also do so in all equilibria under the convex\nhull condition (2) under weaker assumptions requiring only approximate utility\nlearning, a non-strategic user employing quantal response achieves near-optimal\nutility in all equilibria and (3) when the user selects the best single AI\nafter an evaluation period, equilibrium guarantees remain near-optimal without\nfurther distributional assumptions. We complement the theory with two sets of\nexperiments.", "AI": {"tldr": "Strategic competition among multiple misaligned AI agents can achieve outcomes comparable to perfect alignment when user utility lies within the convex hull of agents' utilities, especially with increased model diversity.", "motivation": "Addressing the fundamental challenge of aligning AI systems with human values, and exploring whether imperfect alignment can still yield benefits through strategic interactions with multiple misaligned agents.", "method": "Modeled as a multi-leader Stackelberg game extending Bayesian persuasion to multi-round conversations between differently informed parties, with theoretical analysis and experimental validation.", "result": "Three key theoretical results: (1) user can learn Bayes-optimal action under convex hull condition, (2) non-strategic user achieves near-optimal utility with quantal response, (3) equilibrium guarantees remain near-optimal when selecting best single AI after evaluation.", "conclusion": "Strategic competition among diverse, misaligned AI agents can effectively substitute for perfect alignment, providing near-optimal outcomes without requiring perfectly aligned individual models."}}
{"id": "2509.15097", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15097", "abs": "https://arxiv.org/abs/2509.15097", "authors": ["Mohammad Saleh Vahdatpour", "Huaiyuan Chu", "Yanqing Zhang"], "title": "The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning", "comment": "Published at IJCNN 2025", "summary": "The rising computational and energy demands of deep learning, particularly in\nlarge-scale architectures such as foundation models and large language models\n(LLMs), pose significant challenges to sustainability. Traditional\ngradient-based training methods are inefficient, requiring numerous iterative\nupdates and high power consumption. To address these limitations, we propose a\nhybrid framework that combines hierarchical decomposition with FPGA-based\ndirect equation solving and incremental learning. Our method divides the neural\nnetwork into two functional tiers: lower layers are optimized via single-step\nequation solving on FPGAs for efficient and parallelizable feature extraction,\nwhile higher layers employ adaptive incremental learning to support continual\nupdates without full retraining. Building upon this foundation, we introduce\nthe Compound LLM framework, which explicitly deploys LLM modules across both\nhierarchy levels. The lower-level LLM handles reusable representation learning\nwith minimal energy overhead, while the upper-level LLM performs adaptive\ndecision-making through energy-aware updates. This integrated design enhances\nscalability, reduces redundant computation, and aligns with the principles of\nsustainable AI. Theoretical analysis and architectural insights demonstrate\nthat our method reduces computational costs significantly while preserving high\nmodel performance, making it well-suited for edge deployment and real-time\nadaptation in energy-constrained environments.", "AI": {"tldr": "Proposes a hybrid framework combining hierarchical decomposition with FPGA-based equation solving and incremental learning to reduce computational and energy demands of large language models while maintaining performance.", "motivation": "Address the rising computational and energy demands of deep learning, particularly in large-scale architectures like foundation models and LLMs, which pose sustainability challenges due to inefficient gradient-based training methods.", "method": "Divides neural network into two tiers: lower layers optimized via single-step equation solving on FPGAs for efficient feature extraction, and higher layers using adaptive incremental learning. Introduces Compound LLM framework with lower-level LLM handling representation learning and upper-level LLM performing energy-aware adaptive decision-making.", "result": "Theoretical analysis shows significant reduction in computational costs while preserving high model performance, making it suitable for edge deployment and real-time adaptation in energy-constrained environments.", "conclusion": "The integrated design enhances scalability, reduces redundant computation, and aligns with sustainable AI principles, providing an efficient solution for energy-constrained deep learning applications."}}
{"id": "2509.15105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15105", "abs": "https://arxiv.org/abs/2509.15105", "authors": ["Liran Nochumsohn", "Raz Marshanski", "Hedi Zisling", "Omri Azencot"], "title": "Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting", "comment": null, "summary": "Time series forecasting (TSF) is critical in domains like energy, finance,\nhealthcare, and logistics, requiring models that generalize across diverse\ndatasets. Large pre-trained models such as Chronos and Time-MoE show strong\nzero-shot (ZS) performance but suffer from high computational costs. In this\nwork, We introduce Super-Linear, a lightweight and scalable mixture-of-experts\n(MoE) model for general forecasting. It replaces deep architectures with simple\nfrequency-specialized linear experts, trained on resampled data across multiple\nfrequency regimes. A lightweight spectral gating mechanism dynamically selects\nrelevant experts, enabling efficient, accurate forecasting. Despite its\nsimplicity, Super-Linear matches state-of-the-art performance while offering\nsuperior efficiency, robustness to various sampling rates, and enhanced\ninterpretability. The implementation of Super-Linear is available at\n\\href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}", "AI": {"tldr": "Super-Linear is a lightweight MoE model that uses frequency-specialized linear experts and spectral gating for efficient time series forecasting, matching SOTA performance with better efficiency and interpretability.", "motivation": "Large pre-trained models like Chronos and Time-MoE show strong zero-shot forecasting performance but suffer from high computational costs, creating a need for more efficient alternatives.", "method": "Replaces deep architectures with simple frequency-specialized linear experts trained on resampled data across multiple frequency regimes, using a lightweight spectral gating mechanism to dynamically select relevant experts.", "result": "Matches state-of-the-art performance while offering superior efficiency, robustness to various sampling rates, and enhanced interpretability.", "conclusion": "Super-Linear demonstrates that lightweight linear experts with spectral gating can achieve competitive forecasting performance with significantly improved computational efficiency and practical benefits."}}
{"id": "2509.15107", "categories": ["cs.LG", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.15107", "abs": "https://arxiv.org/abs/2509.15107", "authors": ["Amy Rafferty", "Rishi Ramaesh", "Ajitha Rajan"], "title": "Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges", "comment": null, "summary": "Artificial intelligence has shown significant promise in chest radiography,\nwhere deep learning models can approach radiologist-level diagnostic\nperformance. Progress has been accelerated by large public datasets such as\nMIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of\nthousands of labelled images with pathology annotations. However, these\ndatasets also present important limitations. Automated label extraction from\nradiology reports introduces errors, particularly in handling uncertainty and\nnegation, and radiologist review frequently disagrees with assigned labels. In\naddition, domain shift and population bias restrict model generalisability,\nwhile evaluation practices often overlook clinically meaningful measures. We\nconduct a systematic analysis of these challenges, focusing on label quality,\ndataset bias, and domain shift. Our cross-dataset domain shift evaluation\nacross multiple model architectures revealed substantial external performance\ndegradation, with pronounced reductions in AUPRC and F1 scores relative to\ninternal testing. To assess dataset bias, we trained a source-classification\nmodel that distinguished datasets with near-perfect accuracy, and performed\nsubgroup analyses showing reduced performance for minority age and sex groups.\nFinally, expert review by two board-certified radiologists identified\nsignificant disagreement with public dataset labels. Our findings highlight\nimportant clinical weaknesses of current benchmarks and emphasise the need for\nclinician-validated datasets and fairer evaluation frameworks.", "AI": {"tldr": "Systematic analysis reveals significant limitations in current chest X-ray AI datasets including label errors, domain shift, and performance degradation across datasets and demographic groups.", "motivation": "Current public chest X-ray datasets have accelerated AI progress but suffer from label extraction errors, domain shift issues, population bias, and lack of clinically meaningful evaluation measures that limit their real-world applicability.", "method": "Conducted cross-dataset domain shift evaluation across multiple model architectures, trained source-classification models to detect dataset bias, performed subgroup analyses for demographic groups, and conducted expert radiologist review of dataset labels.", "result": "Found substantial external performance degradation (reduced AUPRC and F1 scores), near-perfect dataset distinguishability indicating bias, reduced performance for minority age/sex groups, and significant disagreement between radiologist review and automated labels.", "conclusion": "Current chest X-ray benchmarks have important clinical weaknesses, highlighting the need for clinician-validated datasets and fairer evaluation frameworks to ensure reliable AI deployment in medical settings."}}
{"id": "2509.15110", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15110", "abs": "https://arxiv.org/abs/2509.15110", "authors": ["Dan Zhang", "Min Cai", "Jonathan Li", "Ziniu Hu", "Yisong Yue", "Yuxiao Dong", "Jie Tang"], "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference", "comment": "9 figures, 7 tables", "summary": "Reward models are central to both reinforcement learning (RL) with language\nmodels and inference-time verification. However, existing reward models often\nlack temporal consistency, leading to ineffective policy updates and unstable\nRL training. We introduce TDRM, a method for learning smoother and more\nreliable reward models by minimizing temporal differences during training. This\ntemporal-difference (TD) regularization produces smooth rewards and improves\nalignment with long-term objectives. Incorporating TDRM into the actor-critic\nstyle online RL loop yields consistent empirical gains. It is worth noting that\nTDRM is a supplement to verifiable reward methods, and both can be used in\nseries. Experiments show that TD-trained process reward models (PRMs) improve\nperformance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)\nsettings. When combined with Reinforcement Learning with Verifiable Rewards\n(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable\nperformance with just 2.5k data to what baseline methods require 50.1k data to\nattain -- and yield higher-quality language model policies on 8 model variants\n(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,\nQwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release\nall code at https://github.com/THUDM/TDRM.", "AI": {"tldr": "TDRM introduces temporal-difference regularization to train smoother, more reliable reward models that improve RL training stability and alignment with long-term objectives.", "motivation": "Existing reward models lack temporal consistency, leading to ineffective policy updates and unstable reinforcement learning training.", "method": "TDRM minimizes temporal differences during training to produce smooth rewards, and is incorporated into actor-critic style online RL loops as a supplement to verifiable reward methods.", "result": "TD-trained process reward models improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with RLVR, they achieve comparable performance with just 2.5k data vs 50.1k data for baselines, and yield higher-quality policies on 8 model variants.", "conclusion": "TDRM provides an effective method for learning temporally consistent reward models that significantly improve RL efficiency and performance across multiple language model variants."}}
{"id": "2509.15113", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15113", "abs": "https://arxiv.org/abs/2509.15113", "authors": ["Andrei Chertkov", "Artem Basharin", "Mikhail Saygin", "Evgeny Frolov", "Stanislav Straupe", "Ivan Oseledets"], "title": "Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers", "comment": null, "summary": "The growing demand for energy-efficient, high-performance AI systems has led\nto increased attention on alternative computing platforms (e.g., photonic,\nneuromorphic) due to their potential to accelerate learning and inference.\nHowever, integrating such physical components into deep learning pipelines\nremains challenging, as physical devices often offer limited expressiveness,\nand their non-differentiable nature renders on-device backpropagation difficult\nor infeasible. This motivates the development of hybrid architectures that\ncombine digital neural networks with reconfigurable physical layers, which\neffectively behave as black boxes. In this work, we present a framework for the\nend-to-end training of such hybrid networks. This framework integrates\nstochastic zeroth-order optimization for updating the physical layer's internal\nparameters with a dynamic low-rank surrogate model that enables gradient\npropagation through the physical layer. A key component of our approach is the\nimplicit projector-splitting integrator algorithm, which updates the\nlightweight surrogate model after each forward pass with minimal hardware\nqueries, thereby avoiding costly full matrix reconstruction. We demonstrate our\nmethod across diverse deep learning tasks, including: computer vision, audio\nclassification, and language modeling. Notably, across all modalities, the\nproposed approach achieves near-digital baseline accuracy and consistently\nenables effective end-to-end training of hybrid models incorporating various\nnon-differentiable physical components (spatial light modulators, microring\nresonators, and Mach-Zehnder interferometers). This work bridges hardware-aware\ndeep learning and gradient-free optimization, thereby offering a practical\npathway for integrating non-differentiable physical components into scalable,\nend-to-end trainable AI systems.", "AI": {"tldr": "A framework for end-to-end training of hybrid digital-physical neural networks using stochastic zeroth-order optimization and dynamic low-rank surrogate models to handle non-differentiable physical components.", "motivation": "The need to integrate energy-efficient physical computing components (photonic, neuromorphic) into deep learning pipelines despite their non-differentiable nature and limited expressiveness.", "method": "Combines stochastic zeroth-order optimization for physical layer updates with dynamic low-rank surrogate models for gradient propagation, using an implicit projector-splitting integrator algorithm to minimize hardware queries.", "result": "Achieves near-digital baseline accuracy across computer vision, audio classification, and language modeling tasks with various non-differentiable physical components (spatial light modulators, microring resonators, Mach-Zehnder interferometers).", "conclusion": "Bridges hardware-aware deep learning and gradient-free optimization, providing a practical pathway for integrating non-differentiable physical components into scalable, end-to-end trainable AI systems."}}
{"id": "2509.15120", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15120", "abs": "https://arxiv.org/abs/2509.15120", "authors": ["Yahav Cohen", "Jacob Goldberger", "Tom Tirer"], "title": "Efficient Conformal Prediction for Regression Models under Label Noise", "comment": null, "summary": "In high-stakes scenarios, such as medical imaging applications, it is\ncritical to equip the predictions of a regression model with reliable\nconfidence intervals. Recently, Conformal Prediction (CP) has emerged as a\npowerful statistical framework that, based on a labeled calibration set,\ngenerates intervals that include the true labels with a pre-specified\nprobability. In this paper, we address the problem of applying CP for\nregression models when the calibration set contains noisy labels. We begin by\nestablishing a mathematically grounded procedure for estimating the noise-free\nCP threshold. Then, we turn it into a practical algorithm that overcomes the\nchallenges arising from the continuous nature of the regression problem. We\nevaluate the proposed method on two medical imaging regression datasets with\nGaussian label noise. Our method significantly outperforms the existing\nalternative, achieving performance close to the clean-label setting.", "AI": {"tldr": "A method to apply conformal prediction for regression models with noisy calibration labels, achieving near-clean-label performance in medical imaging applications.", "motivation": "In high-stakes medical imaging applications, reliable confidence intervals are critical, but existing conformal prediction methods struggle when calibration sets contain noisy labels.", "method": "Developed a mathematically grounded procedure to estimate noise-free conformal prediction thresholds, then created a practical algorithm to handle continuous regression problems with noisy calibration data.", "result": "Evaluated on two medical imaging regression datasets with Gaussian label noise, the method significantly outperformed existing alternatives and achieved performance close to clean-label settings.", "conclusion": "The proposed approach successfully enables reliable conformal prediction for regression models even when calibration data contains noisy labels, making it suitable for critical medical applications."}}
{"id": "2509.15145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15145", "abs": "https://arxiv.org/abs/2509.15145", "authors": ["Lorne Applebaum", "Travis Dick", "Claudio Gentile", "Haim Kaplan", "Tomer Koren"], "title": "Optimal Learning from Label Proportions with General Loss Functions", "comment": null, "summary": "Motivated by problems in online advertising, we address the task of Learning\nfrom Label Proportions (LLP). In this partially-supervised setting, training\ndata consists of groups of examples, termed bags, for which we only observe the\naverage label value. The main goal, however, remains the design of a predictor\nfor the labels of individual examples. We introduce a novel and versatile\nlow-variance de-biasing methodology to learn from aggregate label information,\nsignificantly advancing the state of the art in LLP. Our approach exhibits\nremarkable flexibility, seamlessly accommodating a broad spectrum of\npractically relevant loss functions across both binary and multi-class\nclassification settings. By carefully combining our estimators with standard\ntechniques, we substantially improve sample complexity guarantees for a large\nclass of losses of practical relevance. We also empirically validate the\nefficacy of our proposed approach across a diverse array of benchmark datasets,\ndemonstrating compelling empirical advantages over standard baselines.", "AI": {"tldr": "Novel low-variance de-biasing method for Learning from Label Proportions (LLP) that works with various loss functions in binary and multi-class classification, improving sample complexity and outperforming baselines.", "motivation": "Addressing problems in online advertising where training data consists of groups (bags) with only average label values available, requiring individual example label prediction.", "method": "Introduces a versatile low-variance de-biasing methodology that can handle a broad spectrum of loss functions and combines with standard techniques.", "result": "Substantially improves sample complexity guarantees for practical loss functions and demonstrates compelling empirical advantages across diverse benchmark datasets.", "conclusion": "The approach significantly advances the state of the art in LLP by providing flexible, low-variance estimation that works effectively across various classification settings and loss functions."}}
{"id": "2509.15147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15147", "abs": "https://arxiv.org/abs/2509.15147", "authors": ["Viktor Kovalchuk", "Nikita Kotelevskii", "Maxim Panov", "Samuel Horv\u00e1th", "Martin Tak\u00e1\u010d"], "title": "Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning", "comment": null, "summary": "Federated learning (FL) usually shares model weights or gradients, which is\ncostly for large models. Logit-based FL reduces this cost by sharing only\nlogits computed on a public proxy dataset. However, aggregating information\nfrom heterogeneous clients is still challenging. This paper studies this\nproblem, introduces and compares three logit aggregation methods: simple\naveraging, uncertainty-weighted averaging, and a learned meta-aggregator.\nEvaluated on MNIST and CIFAR-10, these methods reduce communication overhead,\nimprove robustness under non-IID data, and achieve accuracy competitive with\ncentralized training.", "AI": {"tldr": "Logit-based federated learning reduces communication costs by sharing only logits instead of full model weights/gradients, with three aggregation methods showing competitive accuracy to centralized training.", "motivation": "Traditional federated learning shares model weights or gradients which is costly for large models. Logit-based FL reduces communication overhead by sharing only logits computed on a public proxy dataset.", "method": "Introduces and compares three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator.", "result": "Evaluated on MNIST and CIFAR-10, the methods reduce communication overhead, improve robustness under non-IID data, and achieve accuracy competitive with centralized training.", "conclusion": "Logit-based FL with effective aggregation methods provides a communication-efficient alternative to traditional FL while maintaining competitive performance."}}
{"id": "2509.15155", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15155", "abs": "https://arxiv.org/abs/2509.15155", "authors": ["Seyed Kamyar Seyed Ghasemipour", "Ayzaan Wahid", "Jonathan Tompson", "Pannag Sanketi", "Igor Mordatch"], "title": "Self-Improving Embodied Foundation Models", "comment": "Appearing in the Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Foundation models trained on web-scale data have revolutionized robotics, but\ntheir application to low-level control remains largely limited to behavioral\ncloning. Drawing inspiration from the success of the reinforcement learning\nstage in fine-tuning large language models, we propose a two-stage\npost-training approach for robotics. The first stage, Supervised Fine-Tuning\n(SFT), fine-tunes pretrained foundation models using both: a) behavioral\ncloning, and b) steps-to-go prediction objectives. In the second stage,\nSelf-Improvement, steps-to-go prediction enables the extraction of a\nwell-shaped reward function and a robust success detector, enabling a fleet of\nrobots to autonomously practice downstream tasks with minimal human\nsupervision. Through extensive experiments on real-world and simulated robot\nembodiments, our novel post-training recipe unveils significant results on\nEmbodied Foundation Models. First, we demonstrate that the combination of SFT\nand Self-Improvement is significantly more sample-efficient than scaling\nimitation data collection for supervised learning, and that it leads to\npolicies with significantly higher success rates. Further ablations highlight\nthat the combination of web-scale pretraining and Self-Improvement is the key\nto this sample-efficiency. Next, we demonstrate that our proposed combination\nuniquely unlocks a capability that current methods cannot achieve: autonomously\npracticing and acquiring novel skills that generalize far beyond the behaviors\nobserved in the imitation learning datasets used during training. These\nfindings highlight the transformative potential of combining pretrained\nfoundation models with online Self-Improvement to enable autonomous skill\nacquisition in robotics. Our project website can be found at\nhttps://self-improving-efms.github.io .", "AI": {"tldr": "A two-stage post-training approach for robotics that combines supervised fine-tuning with self-improvement, enabling autonomous skill acquisition beyond imitation learning datasets.", "motivation": "Foundation models have transformed robotics but remain limited to behavioral cloning for low-level control. The success of reinforcement learning in fine-tuning LLMs inspired a similar approach for robotics to enable autonomous skill acquisition.", "method": "Two-stage approach: 1) Supervised Fine-Tuning (SFT) using behavioral cloning and steps-to-go prediction, 2) Self-Improvement stage where steps-to-go prediction enables reward function extraction and success detection for autonomous practice by robot fleets.", "result": "Significantly more sample-efficient than scaling imitation data collection, leads to higher success rates, and uniquely enables autonomous acquisition of novel skills that generalize beyond training datasets.", "conclusion": "Combining pretrained foundation models with online self-improvement has transformative potential for autonomous skill acquisition in robotics, enabling capabilities beyond current methods."}}
{"id": "2509.15157", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15157", "abs": "https://arxiv.org/abs/2509.15157", "authors": ["Shiwan Zhao", "Xuyang Zhao", "Jiaming Zhou", "Aobo Kong", "Qicheng Li", "Yong Qin"], "title": "Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) of large language models can be viewed as an\noff-policy learning problem, where expert demonstrations come from a fixed\nbehavior policy while training aims to optimize a target policy. Importance\nsampling is the standard tool for correcting this distribution mismatch, but\nlarge policy gaps lead to high variance and training instability. Existing\napproaches mitigate this issue using KL penalties or clipping, which passively\nconstrain updates rather than actively reducing the gap. We propose a simple\nyet effective data rewriting framework that proactively shrinks the policy gap\nby keeping correct solutions as on-policy data and rewriting incorrect ones\nwith guided re-solving, falling back to expert demonstrations only when needed.\nThis aligns the training distribution with the target policy before\noptimization, reducing importance sampling variance and stabilizing off-policy\nfine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate\nconsistent and significant gains over both vanilla SFT and the state-of-the-art\nDynamic Fine-Tuning (DFT) approach. The data and code will be released at\nhttps://github.com/NKU-HLT/Off-Policy-SFT.", "AI": {"tldr": "Proactive data rewriting framework for off-policy SFT that reduces policy gap by keeping correct solutions and rewriting incorrect ones, improving stability and performance over vanilla SFT and DFT.", "motivation": "Supervised fine-tuning suffers from distribution mismatch between expert demonstrations and target policy, leading to high variance and instability with importance sampling when policy gaps are large.", "method": "Data rewriting framework that proactively shrinks policy gap by: 1) keeping correct solutions as on-policy data, 2) rewriting incorrect solutions with guided re-solving, 3) using expert demonstrations only when needed.", "result": "Consistent and significant gains over vanilla SFT and state-of-the-art Dynamic Fine-Tuning on five mathematical reasoning benchmarks.", "conclusion": "Aligning training distribution with target policy before optimization reduces importance sampling variance and stabilizes off-policy fine-tuning, demonstrating effectiveness through improved performance."}}
{"id": "2509.15187", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.15187", "abs": "https://arxiv.org/abs/2509.15187", "authors": ["Giorgos Armeniakos", "Alexis Maras", "Sotirios Xydis", "Dimitrios Soudris"], "title": "MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration", "comment": "Accepted for publication by IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems, March 2025", "summary": "The evolution of quantization and mixed-precision techniques has unlocked new\npossibilities for enhancing the speed and energy efficiency of NNs. Several\nrecent studies indicate that adapting precision levels across different\nparameters can maintain accuracy comparable to full-precision models while\nsignificantly reducing computational demands. However, existing embedded\nmicroprocessors lack sufficient architectural support for efficiently executing\nmixed-precision NNs, both in terms of ISA extensions and hardware design,\nresulting in inefficiencies such as excessive data packing/unpacking and\nunderutilized arithmetic units. In this work, we propose novel ISA extensions\nand a micro-architecture implementation specifically designed to optimize\nmixed-precision execution, enabling energy-efficient deep learning inference on\nRISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software\nco-design framework that enhances power efficiency and performance through a\ncombination of hardware improvements, mixed-precision quantization, ISA-level\noptimizations, and cycle-accurate emulation. At the hardware level, we enhance\nthe ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for\nweights/activations and employ multi-pumping to reduce execution latency while\nimplementing soft SIMD for efficient 2-bit ops. At the software level, we\nintegrate a pruning-aware fine-tuning method to optimize model compression and\na greedy-based DSE approach to efficiently search for Pareto-optimal\nmixed-quantized models. Additionally, we incorporate voltage scaling to boost\nthe power efficiency of our system. Our experimental evaluation over widely\nused DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our\nframework can achieve, on average, 17.6x speedup for less than 1% accuracy loss\nand outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up\nto 1.8 TOPs/W.", "AI": {"tldr": "Proposes MaRVIn, a hardware-software co-design framework with novel ISA extensions and micro-architecture for efficient mixed-precision NN execution on RISC-V, achieving 17.6x speedup with <1% accuracy loss.", "motivation": "Existing embedded microprocessors lack architectural support for efficient mixed-precision NN execution, causing inefficiencies like excessive data packing/unpacking and underutilized arithmetic units.", "method": "Hardware-software co-design with enhanced ALU for configurable mixed-precision arithmetic (2,4,8 bits), multi-pumping, soft SIMD, pruning-aware fine-tuning, greedy-based DSE, and voltage scaling.", "result": "Achieves 17.6x average speedup with <1% accuracy loss on CIFAR10 and ImageNet, outperforms state-of-the-art RISC-V cores with up to 1.8 TOPs/W power efficiency.", "conclusion": "MaRVIn framework successfully enables energy-efficient mixed-precision deep learning inference on RISC-V through cross-layer optimizations and novel architectural support."}}
{"id": "2509.15194", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15194", "abs": "https://arxiv.org/abs/2509.15194", "authors": ["Yujun Zhou", "Zhenwen Liang", "Haolin Liu", "Wenhao Yu", "Kishan Panaganti", "Linfeng Song", "Dian Yu", "Xiangliang Zhang", "Haitao Mi", "Dong Yu"], "title": "Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation", "comment": null, "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.", "AI": {"tldr": "EVOL-RL is a label-free reinforcement learning method that prevents entropy collapse by combining majority-vote stability with novelty-aware variation, enabling LLMs to self-improve without sacrificing exploration capacity.", "motivation": "Existing label-free RL methods for LLMs cause entropy collapse - generations become shorter, less diverse, and brittle. There's a need for methods that enable general improvement without sacrificing exploration capacity and generalization ability.", "method": "EVOL-RL couples majority-voted answers as stable anchors with novelty-aware rewards that favor semantically different responses. Uses asymmetric clipping to preserve strong signals and entropy regularization to sustain search.", "result": "EVOL-RL significantly outperforms TTRL baseline, improving Qwen3-4B-Base AIME25 pass@1 from 4.6% to 16.4% and pass@16 from 18.5% to 37.9%. Prevents diversity collapse and improves generalization across domains like GPQA.", "conclusion": "EVOL-RL effectively prevents entropy collapse in label-free RL settings, maintains exploration capacity, and demonstrates strong generalization across domains while also working well in RLVR settings."}}
{"id": "2509.15198", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.15198", "abs": "https://arxiv.org/abs/2509.15198", "authors": ["Ahc\u00e8ne Boubekki", "Konstantinos Patlatzoglou", "Joseph Barker", "Fu Siong Ng", "Ant\u00f4nio H. Ribeiro"], "title": "Explaining deep learning for ECG using time-localized clusters", "comment": null, "summary": "Deep learning has significantly advanced electrocardiogram (ECG) analysis,\nenabling automatic annotation, disease screening, and prognosis beyond\ntraditional clinical capabilities. However, understanding these models remains\na challenge, limiting interpretation and gaining knowledge from these\ndevelopments. In this work, we propose a novel interpretability method for\nconvolutional neural networks applied to ECG analysis. Our approach extracts\ntime-localized clusters from the model's internal representations, segmenting\nthe ECG according to the learned characteristics while quantifying the\nuncertainty of these representations. This allows us to visualize how different\nwaveform regions contribute to the model's predictions and assess the certainty\nof its decisions. By providing a structured and interpretable view of deep\nlearning models for ECG, our method enhances trust in AI-driven diagnostics and\nfacilitates the discovery of clinically relevant electrophysiological patterns.", "AI": {"tldr": "Novel interpretability method for CNN-based ECG analysis that extracts time-localized clusters from model representations to visualize waveform contributions and quantify prediction uncertainty.", "motivation": "Deep learning models for ECG analysis lack interpretability, limiting clinical trust and knowledge extraction from these advanced diagnostic tools.", "method": "Extracts time-localized clusters from CNN's internal representations to segment ECG based on learned characteristics while quantifying uncertainty of these representations.", "result": "Enables visualization of how different ECG waveform regions contribute to predictions and assessment of decision certainty.", "conclusion": "Enhances trust in AI-driven ECG diagnostics and facilitates discovery of clinically relevant electrophysiological patterns through structured interpretability."}}
{"id": "2509.15199", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.15199", "abs": "https://arxiv.org/abs/2509.15199", "authors": ["Ying Zheng", "Yangfan Jiang", "Kian-Lee Tan"], "title": "CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness", "comment": null, "summary": "Causal fairness in databases is crucial to preventing biased and inaccurate\noutcomes in downstream tasks. While most prior work assumes a known causal\nmodel, recent efforts relax this assumption by enforcing additional\nconstraints. However, these approaches often fail to capture broader attribute\nrelationships that are critical to maintaining utility. This raises a\nfundamental question: Can we harness the benefits of causal reasoning to design\nefficient and effective fairness solutions without relying on strong\nassumptions about the underlying causal model? In this paper, we seek to answer\nthis question by introducing CausalPre, a scalable and effective\ncausality-guided data pre-processing framework that guarantees justifiable\nfairness, a strong causal notion of fairness. CausalPre extracts causally fair\nrelationships by reformulating the originally complex and computationally\ninfeasible extraction task into a tailored distribution estimation problem. To\nensure scalability, CausalPre adopts a carefully crafted variant of\nlow-dimensional marginal factorization to approximate the joint distribution,\ncomplemented by a heuristic algorithm that efficiently tackles the associated\ncomputational challenge. Extensive experiments on benchmark datasets\ndemonstrate that CausalPre is both effective and scalable, challenging the\nconventional belief that achieving causal fairness requires trading off\nrelationship coverage for relaxed model assumptions.", "AI": {"tldr": "CausalPre is a scalable causality-guided data pre-processing framework that achieves causal fairness without requiring strong causal model assumptions, using efficient distribution estimation and heuristic algorithms.", "motivation": "Existing causal fairness approaches either require known causal models or fail to capture broader attribute relationships, limiting their utility and effectiveness in real-world applications.", "method": "CausalPre reformulates causal fairness extraction into a distribution estimation problem, uses low-dimensional marginal factorization for approximation, and employs heuristic algorithms for computational efficiency.", "result": "Extensive experiments show CausalPre is both effective and scalable, achieving causal fairness without trading off relationship coverage for relaxed assumptions.", "conclusion": "CausalPre demonstrates that causal fairness can be achieved efficiently without strong causal model assumptions, challenging conventional beliefs about the trade-offs required for causal fairness solutions."}}
{"id": "2506.11445", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11445", "abs": "https://arxiv.org/abs/2506.11445", "authors": ["Xuan Duy Ta", "Bang Giang Le", "Thanh Ha Le", "Viet Cuong Ta"], "title": "Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention", "comment": null, "summary": "In mixed-traffic environments, autonomous vehicles must adapt to\nhuman-controlled vehicles and other unusual driving situations. This setting\ncan be framed as a multi-agent reinforcement learning (MARL) environment with\nfull cooperative reward among the autonomous vehicles. While methods such as\nMulti-agent Proximal Policy Optimization can be effective in training MARL\ntasks, they often fail to resolve local conflict between agents and are unable\nto generalize to stochastic events. In this paper, we propose a Local State\nAttention module to assist the input state representation. By relying on the\nself-attention operator, the module is expected to compress the essential\ninformation of nearby agents to resolve the conflict in traffic situations.\nUtilizing a simulated highway merging scenario with the priority vehicle as the\nunexpected event, our approach is able to prioritize other vehicles'\ninformation to manage the merging process. The results demonstrate significant\nimprovements in merging efficiency compared to popular baselines, especially in\nhigh-density traffic settings.", "AI": {"tldr": "Proposes Local State Attention module for autonomous vehicles in mixed traffic to resolve conflicts and handle unexpected events using self-attention mechanism.", "motivation": "Autonomous vehicles need to adapt to human-controlled vehicles and unusual driving situations in mixed-traffic environments, but current MARL methods fail to resolve local conflicts and generalize to stochastic events.", "method": "Uses Local State Attention module with self-attention operator to compress essential information of nearby agents, tested in simulated highway merging scenario with priority vehicle as unexpected event.", "result": "Significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.", "conclusion": "The proposed attention-based approach effectively prioritizes vehicle information to manage merging processes and resolve traffic conflicts."}}
