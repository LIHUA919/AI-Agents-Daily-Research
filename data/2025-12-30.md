<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 21]
- [cs.MA](#cs.MA) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: Bidirectional RAG enhances RAG systems by allowing safe expansion of the knowledge base through validated write-back of generated responses, nearly doubling coverage while reducing added documents.


<details>
  <summary>Details</summary>
Motivation: Conventional RAG architectures use static corpora that don't evolve from user interactions, limiting knowledge accumulation and system improvement over time.

Method: Introduces a multi-stage acceptance layer with grounding verification, including NLI-based entailment, attribution checking, and novelty detection to validate high-quality responses for write-back, preventing hallucination pollution.

Result: Across four datasets with multiple experiments, Bidirectional RAG achieves 40.58% average coverage (nearly double Standard RAG's 20.33%) while adding 72% fewer documents than naive write-back (140 vs 500).

Conclusion: Self-improving RAG is feasible and safe when governed by rigorous validation, offering a practical path for RAG systems to learn from deployment.

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [2] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: AI can persuade users unprompted, especially after fine-tuning, raising concerns about emergent risks.


<details>
  <summary>Details</summary>
Motivation: AI influence on human beliefs is growing, but prior studies focused on misuse (prompted persuasion). This paper investigates when models persuade unprompted, to assess real-world risks.

Method: Study unprompted persuasion under two scenarios: (1) steering models via internal activation towards persona traits, and (2) supervised fine-tuning (SFT) on persuasion-related datasets.

Result: Steering towards traits (persuasive or unrelated) does not reliably increase unprompted persuasion, but SFT does. SFT on benign topics can lead to higher persuasion on harmful topics, indicating emergent harmful behavior.

Conclusion: Emergent harmful persuasion can arise from SFT on general datasets, highlighting risks that warrant further study and mitigation strategies.

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [3] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench is a novel benchmark for evaluating spatial reasoning in MLLMs using origami-inspired folding tasks, revealing major limitations in current models' geometric understanding.


<details>
  <summary>Details</summary>
Motivation: Address the gap in existing benchmarks that focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of spatial reasoning, which is a key component of human intelligence.

Method: Developed GamiBench benchmark with 186 regular and 186 impossible 2D crease patterns paired with 3D folded shapes from six distinct viewpoints across three VQA tasks (3D fold prediction, viewpoint validation, impossible pattern detection), introducing new metrics for holistic evaluation.

Result: Leading MLLMs like GPT-5 and Gemini-2.5-Pro struggle significantly on single-step spatial understanding tasks, showing poor performance on spatial reasoning despite proficiency in perception and instruction-following.

Conclusion: GamiBench establishes a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs, revealing significant limitations even in leading models and providing new diagnostic metrics for assessing these capabilities.

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [4] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: AI fairness framework reduces bias in flood aid allocation in Bangladesh, improving equity while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Systematic biases in post-disaster aid often disadvantage vulnerable regions, perpetuating historical inequities in developing nations like Bangladesh, which faces frequent floods.

Method: Develop adversarial debiasing model using real 2022 flood data (affecting 7.2M people, $405.5M damages), applying fairness-aware representation learning with gradient reversal layer to remove biases against marginalized areas.

Result: Model applied to 87 upazilas across 11 districts, reducing statistical parity difference by 41.6%, decreasing regional fairness gaps by 43.2%, and maintaining predictive accuracy (R-squared=0.784 vs baseline 0.811).

Conclusion: Algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing tools for more equitable disaster recovery strategies and actionable aid priority rankings based on genuine need.

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [5] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: LLM deliberation improves forecasting accuracy only with diverse models and shared information, not with homogeneous models or extra context.


<details>
  <summary>Details</summary>
Motivation: To investigate if allowing LLMs to review each other's forecasts before updating can improve accuracy, akin to structured deliberation in human forecasters.

Method: Study used 202 resolved binary questions from Metaculus Q2 2025 AI Forecasting Tournament, assessing accuracy across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, (4) homogeneous models with shared information.

Result: The intervention significantly improved accuracy in scenario (2) (diverse models with shared information), reducing Log Loss by 0.020 or ~4% (p=0.017). No benefit seen in homogeneous models. Additional contextual information did not improve accuracy.

Conclusion: Deliberation can improve LLM forecasting accuracy when models are diverse but lack intrinsic benefit for homogeneous models and does not benefit from additional contextual information.

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [6] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: The paper introduces the Agentic Risk & Capability (ARC) Framework to help organizations manage risks from autonomous AI systems through technical governance.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems, which autonomously execute tasks like coding and internet interactions, pose novel and evolving risks that challenge effective organizational governance and risk mitigation.

Method: The framework develops a capability-centric perspective, identifies three primary risk sources (components, design, capabilities), links these to materialized risks and technical controls, and provides a structured implementation approach.

Result: The ARC Framework offers a robust and adaptable methodology for identifying, assessing, and mitigating risks in agentic AI systems, enabling safe and responsible deployment while supporting innovation.

Conclusion: This framework equips organizations to navigate agentic AI complexities, ensuring secure and responsible use through practical technical controls and open-sourced resources.

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [7] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: SPIRAL integrates three LLM agents into MCTS for better planning via guided self-correction, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex planning requiring exploration and self-correction; existing search algorithms like MCTS are ineffective with sparse rewards and don't leverage LLM semantic capabilities.

Method: SPIRAL framework with three specialized LLM agents (Planner, Simulator, Critic) embedded in MCTS loop for integrated planning pipeline.

Result: SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, improving over 16 percentage points against next-best search framework, with superior token efficiency.

Conclusion: Structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners.

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [8] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: Humans struggle to reliably distinguish AI-generated images from real photographs, with low accuracy only slightly above random chance.


<details>
  <summary>Details</summary>
Motivation: Test the assumption that people can easily tell apart AI-generated images from real photographs, as AI-generated content becomes more prevalent online.

Method: Conducted an interactive web experiment where 165 participants classified 20 images as real or AI-generated, using a dataset of 120 difficult cases including real images from CC12M and AI-generated counterparts from MidJourney.

Result: Average accuracy was 54% (slightly above random guessing), limited improvement across repeated attempts, average response time 7.3 seconds, and some images were more deceptive than others.

Conclusion: Human judgment alone is insufficient for distinguishing real from AI-generated images, highlighting the need for greater awareness and ethical guidelines as synthetic media improves.

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [9] [Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks](https://arxiv.org/abs/2512.22255)
*Abhranil Chandra,Ayush Agrawal,Arian Hosseini,Sebastian Fischmeister,Rishabh Agarwal,Navin Goyal,Aaron Courville*

Main category: cs.AI

TL;DR: Training language models on synthetic chain-of-thought data from more capable models, even with incorrect answers, improves their reasoning, surpassing human-annotated datasets, due to distribution alignment and partial reasoning validity.


<details>
  <summary>Details</summary>
Motivation: To explore if synthetic CoT data from more capable models can enhance language model reasoning, even when traces lead to wrong answers, and test hypotheses about distribution closeness and partial validity.

Method: Conducted experiments using MATH, GSM8K, Countdown, and MBPP datasets on Qwen, Llama, and Gemma models (1.5B-9B), testing with flawed CoT traces, paraphrasing human traces, and analyzing performance.

Result: Models trained on synthetic CoT data with incorrect answers outperformed those on human-annotated data; paraphrasing human traces improved performance, and models showed tolerance to partial flaws in reasoning.

Conclusion: Curating datasets closer to the model's distribution is critical, and correct answers alone don't ensure faithful reasoning, highlighting the value of synthetic, even flawed, CoT data.

Abstract: We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.

</details>


### [10] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Logic Sketch Prompting (LSP) framework improves LLM reliability for rule-based tasks with high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: LLMs are unreliable on tasks requiring strict rule adherence, determinism, and auditability.

Method: Logic Sketch Prompting (LSP) framework with typed variables, deterministic condition evaluators, and rule-based validator.

Result: LSP consistently achieves highest accuracy (0.83-0.89) and F1 score (0.83-0.89), outperforming zero-shot prompting (0.24-0.60), concise prompts (0.16-0.30), and chain-of-thought prompting (0.56-0.75). McNemar tests show statistically significant gains (p<0.01).

Conclusion: LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety-critical decision support systems.

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [11] [SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence](https://arxiv.org/abs/2512.22334)
*Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: SciEvalKit is a unified benchmarking toolkit designed to evaluate AI models across multiple scientific disciplines and specialized tasks, supporting six major domains with an extensible pipeline for reproducible results.


<details>
  <summary>Details</summary>
Motivation: To address the need for a standardized, domain-specific evaluation platform that focuses on core scientific competencies, bridging capability-based assessment and disciplinary diversity for AI models in science.

Method: By curating expert-grade scientific benchmarks from real-world, domain-specific datasets and developing a flexible, extensible evaluation pipeline that supports batch evaluation, custom model and dataset integration.

Result: The toolkit provides transparent, reproducible, and comparable benchmarking results, enabling assessment of models across scientific multimodal perception, reasoning, understanding, symbolic reasoning, code generation, hypothesis generation, and knowledge understanding.

Conclusion: SciEvalKit offers a standardized yet customizable infrastructure to benchmark next-generation scientific foundation models, fostering community-driven progress in AI4Science through its open-source and actively maintained design.

Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.

</details>


### [12] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World: A tool-augmented multi-agent framework for generating symbolic world models using multi-agent feedback for planning, improving performance via fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Training LLMs to generate symbolic world models lacks large-scale verifiable supervision; current static validation methods miss behavior-level errors from interactive execution.

Method: Three-stage pipeline: Deep Researcher agent synthesizes knowledge via web searching; Model Developer agent implements executable world models; Testing Team conducts adaptive unit testing and simulation-based validation.

Result: Superior inference-time performance across three benchmarks for PDDL and executable code, with consistent state-of-the-art results. Fine-tuning on generated trajectories yields average 30.95% relative gain.

Conclusion: Agent2World effectively generates world models and serves as a data engine for fine-tuning, addressing supervision gaps with interactive multi-agent feedback, enhancing planning capabilities.

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [13] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: A compilation method transforms numeric planning with control parameters into simpler tasks, enabling the use of standard heuristics for problems with infinite possible actions.


<details>
  <summary>Details</summary>
Motivation: Standard numeric heuristics fail in numeric planning with control parameters due to infinite applicable actions, necessitating a tractable solution to extend heuristic applicability.

Method: Identify controllable, simple numeric problems and use an optimistic compilation that abstracts control-dependent expressions into bounded constant effects and relaxed preconditions.

Result: The compilation effectively allows subgoaling heuristics to estimate goal distance, making traditional numeric heuristics computationally feasible in this setting.

Conclusion: This approach is an effective and feasible way to apply numeric heuristics to planning with control parameters, advancing the state of the art.

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [14] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: The paper introduces HalluMatData, a benchmark for evaluating hallucination detection in AI-generated materials science content, and HalluMatDetector, a multi-stage framework to detect and reduce LLM hallucinations by 30% with metrics like PHCS.


<details>
  <summary>Details</summary>
Motivation: AI, especially LLMs, accelerates scientific discovery but suffers from hallucination, generating factually incorrect content that threatens research integrity in fields like materials science.

Method: Developed HalluMatData as a benchmark dataset and proposed HalluMatDetector, a framework using intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment.

Result: Hallucination levels vary across materials science subdomains, with high-entropy queries showing more inconsistencies. HalluMatDetector reduces hallucination rates by 30% compared to standard LLM outputs.

Conclusion: The work provides tools and metrics, like PHCS, to enhance hallucination detection and improve LLM reliability in scientific applications, addressing a critical challenge in AI-driven research.

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [15] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: A lightweight framework called GatedBias adapts frozen knowledge graph embeddings for personalized link prediction at inference time, requiring only ~300 trainable parameters and preserving global accuracy.


<details>
  <summary>Details</summary>
Motivation: Foundation models for knowledge graphs achieve good cohort-level link prediction but fail to capture individual user preferences, creating a gap between general relational reasoning and personalized ranking.

Method: GatedBias uses a structure-gated adaptation approach: it combines profile-specific features with graph-derived binary gates to produce interpretable, per-entity biases, without retraining the foundational KG embeddings.

Result: Evaluation on Amazon-Book and Last-FM datasets shows statistically significant improvements in alignment metrics while maintaining cohort performance. Counterfactual perturbations reveal 6–30x greater rank improvements for entities benefiting from specific preference signals.

Conclusion: Personalized adaptation of foundation models can be parameter-efficient and causally verifiable, effectively bridging general knowledge representations with individual user needs.

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [16] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: Monadic Context Engineering (MCE) uses algebraic structures (Functors, Applicatives, Monads) to provide a formal foundation for building robust, composable, and efficient autonomous AI agents, addressing issues like state management and error handling.


<details>
  <summary>Details</summary>
Motivation: Current agent architectures are often built with imperative, ad-hoc patterns, leading to brittle systems with difficulties in state management, error handling, and concurrency.

Method: Introduces the MCE paradigm that treats agent workflows as computational contexts managed by algebraic properties, utilizing Monads for sequential composition, Applicatives for parallel execution, and Monad Transformers for capability composition. Extends to Meta-Agents for generative orchestration via metaprogramming.

Result: MCE enables the construction of complex, resilient, and efficient AI agents from simple, independently verifiable components, with demonstrated applications in robust sequential and parallel execution.

Conclusion: MCE offers a principled architectural approach to agent design, systematically addressing cross-cutting concerns through algebraic abstractions, leading to more maintainable and scalable autonomous systems.

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [17] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: A benchmark dataset and framework for nuanced assessment of manipulative content in LLM outputs across seven harm types.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack nuance for manipulative behaviors in LLM outputs, necessitating a fine-grained diagnostic framework.

Method: The framework implements a four-layer analytical pipeline with MGD, MSIAN, THP, and DCRA, leveraging 401 expert-annotated examples to assess GPT-4, Claude 3.5, and LLaMA-3-70B.

Result: Significant variability (65.2%–89.7%) in detection among state-of-the-art models, with consistent weaknesses in detecting autonomy harm.

Conclusion: DarkPatterns-LLM establishes the first standardized benchmark for detecting manipulative content in LLMs across seven harm categories, offering actionable diagnostics to foster more trustworthy AI systems.

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [18] [Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation](https://arxiv.org/abs/2512.22529)
*Yiming Lu,Tingyu Lu,Di Zhang,Lili Ye,Hao Li*

Main category: cs.AI

TL;DR: A 'human-in-the-loop' AI framework develops a machine learning potential for accurate, large-scale simulations of aluminum nanoparticle oxidation, revealing temperature-dependent mechanisms and resolving mass transfer debates.


<details>
  <summary>Details</summary>
Motivation: Understanding atomic mechanisms of aluminum nanoparticle combustion is limited by computational trade-offs: ab initio methods are accurate but small-scale, while empirical potentials lack reactive fidelity for combustion.

Method: A self-auditing AI Agent framework validates machine learning potential evolution through human-in-the-loop visualization to ensure quantum accuracy, scaling to million-atom systems and nanosecond timescales.

Result: Simulations show a temperature-regulated dual-mode oxidation: moderate temperatures feature a dynamic oxide shell with 'breathing mode' channels; higher temperatures trigger 'rupture mode' explosive combustion. Aluminum cation outward diffusion dominates mass transfer across temperatures, exceeding oxygen diffusion by 2-3 orders.

Conclusion: The framework provides an atomic-scale design tool for energetic nanomaterials, enabling precise engineering of ignition sensitivity and energy release via computational design.

Abstract: Aluminum nanoparticles (ANPs) are among the most energy-dense solid fuels, yet the atomic mechanisms governing their transition from passivated particles to explosive reactants remain elusive. This stems from a fundamental computational bottleneck: ab initio methods offer quantum accuracy but are restricted to small spatiotemporal scales (< 500 atoms, picoseconds), while empirical force fields lack the reactive fidelity required for complex combustion environments. Herein, we bridge this gap by employing a "human-in-the-loop" closed-loop framework where self-auditing AI Agents validate the evolution of a machine learning potential (MLP). By acting as scientific sentinels that visualize hidden model artifacts for human decision-making, this collaborative cycle ensures quantum mechanical accuracy while exhibiting near-linear scalability to million-atom systems and accessing nanosecond timescales (energy RMSE: 1.2 meV/atom, force RMSE: 0.126 eV/Angstrom). Strikingly, our simulations reveal a temperature-regulated dual-mode oxidation mechanism: at moderate temperatures, the oxide shell acts as a dynamic "gatekeeper," regulating oxidation through a "breathing mode" of transient nanochannels; above a critical threshold, a "rupture mode" unleashes catastrophic shell failure and explosive combustion. Importantly, we resolve a decades-old controversy by demonstrating that aluminum cation outward diffusion, rather than oxygen transport, dominates mass transfer across all temperature regimes, with diffusion coefficients consistently exceeding those of oxygen by 2-3 orders of magnitude. These discoveries establish a unified atomic-scale framework for energetic nanomaterial design, enabling the precision engineering of ignition sensitivity and energy release rates through intelligent computational design.

</details>


### [19] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: This paper argues that current foundation models (e.g., LLMs) are limited by missing components like action integration, hierarchical structure, and episodic memory, inspired by predictive coding models in neuroscience. It proposes integrating these to improve AI safety, interpretability, efficiency, and human-likeness, comparing to trends like CoT and RAG.


<details>
  <summary>Details</summary>
Motivation: Current foundation models, based on next-token prediction optimization, ignore key components from state-of-the-art predictive coding models in neuroscience, such as action integration, hierarchical composition, and episodic memory. This limits their safety, interpretability, energy efficiency, and human-like capabilities.

Method: The paper reviews evidence from neuroscience and cognitive science on the importance of action integration, hierarchical compositional structure, and episodic memory. It describes how incorporating these components into foundation models could address deficiencies like hallucinations, lack of grounding, missing agency, safety threats, and energy inefficiency. The proposal is compared to current trends like chain-of-thought reasoning and retrieval-augmented generation.

Result: By integrating actions at multiple scales, a compositional generative architecture, and episodic memory, foundation models could potentially achieve safer, more interpretable, energy-efficient, and human-like AI. This approach addresses current deficiencies and enhances capabilities beyond simple next-token prediction.

Conclusion: A resurgence of collaboration between neuroscience and AI is essential for advancing towards safe and interpretable human-centered AI. Incorporating brain-inspired components into foundation models can pave the way for more robust and effective artificial intelligence systems.

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [20] [SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G](https://arxiv.org/abs/2512.22579)
*Yong Xiao,Xubo Li,Haoran Zhou,Yingyu Li,Yayu Gao,Guangming Shi,Ping Zhang,Marwan Krunz*

Main category: cs.AI

TL;DR: This paper proposes SANet, a semantic-aware AgentNet architecture for wireless networks, addressing multi-agent optimization with conflicting objectives, introducing evaluation metrics, a model partitioning framework, and achieving efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Agentic AI networking (AgentNet) enables autonomous network management, but as a decentralized framework, it faces challenges where collaborating agents have different or conflicting objectives, necessitating optimal solutions.

Method: Formulates SANet as a multi-agent multi-objective optimization problem, proposes three novel evaluation metrics, develops a model partition and sharing (MoPS) framework for efficient deployment, and introduces two decentralized optimization algorithms with theoretical analysis.

Result: Experimental results using a hardware prototype show performance gains up to 14.61% while requiring only 44.37% of FLOPs compared to state-of-the-art algorithms.

Conclusion: SANet effectively optimizes wireless networks in AgentNet by addressing conflicting objectives through semantic-aware design and efficient model partitioning, demonstrating practical benefits in real-world implementations.

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.

</details>


### [21] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee is a unified, modular toolkit for intelligent physiological healthcare that addresses data and reproducibility issues, outperforming baselines on 13 datasets.


<details>
  <summary>Details</summary>
Motivation: Deep learning in physiological signal analysis is hindered by heterogeneous data formats, inconsistent preprocessing, fragmented models, and non-reproducible setups.

Method: Tyee introduces a unified data interface for 12 signal modalities, a modular architecture for flexible integration, and end-to-end workflow configuration.

Result: Tyee showed practical effectiveness, outperforming or matching baselines on all tasks, with state-of-the-art results on 12 of 13 datasets.

Conclusion: The Tyee toolkit, available on GitHub, provides a fully-integrated solution for reproducible and scalable physiological healthcare research.

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [22] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: The paper introduces M³ob, a multi-modal method for human mobility prediction using large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG) to address generalization issues and improve location recommendation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human mobility prediction have limited generalization: unimodal methods face data sparsity and biases, while multi-modal methods fail to capture mobility dynamics due to semantic gaps between static multi-modal representations and spatial-temporal dynamics.

Method: First, construct a unified spatial-temporal relational graph (STRG) for multi-modal representation using LLMs-enhanced spatial-temporal knowledge graph (STKG). Second, design a gating mechanism to fuse spatial-temporal graph representations and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into static image modality.

Result: Extensive experiments on six public datasets show consistent improvements in normal scenarios and significant generalization ability in abnormal scenarios.

Conclusion: The proposed M³ob effectively leverages multi-modal spatial-temporal knowledge to enhance human mobility prediction, overcoming generalization limitations and benefiting applications like location recommendation.

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses](https://arxiv.org/abs/2512.22128)
*Yongyu Wang*

Main category: cs.LG

TL;DR: A pruning framework improves GNN robustness by removing fragile graph edges using adversarial robustness scores, enhancing defense against perturbations.


<details>
  <summary>Details</summary>
Motivation: GNNs are vulnerable to adversarial attacks due to noise amplification in graph structure and features, compromising model reliability in high-perturbation scenarios.

Method: A pruning framework that uses adversarial robustness evaluation to identify and selectively remove detrimental edges, applied to three GNN architectures with extensive benchmark experiments.

Result: The approach significantly enhances GNN defense capability in the high-perturbation regime, yielding cleaner and more resilient graph representations.

Conclusion: Explicit pruning guided by robustness scores effectively mitigates vulnerabilities in GNNs, improving their stability and reliability against adversarial threats.

Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning on graph-structured data, thanks to their ability to jointly exploit node features and relational information encoded in the graph topology. This joint modeling, however, also introduces a critical weakness: perturbations or noise in either the structure or the features can be amplified through message passing, making GNNs highly vulnerable to adversarial attacks and spurious connections. In this work, we introduce a pruning framework that leverages adversarial robustness evaluation to explicitly identify and remove fragile or detrimental components of the graph. By using robustness scores as guidance, our method selectively prunes edges that are most likely to degrade model reliability, thereby yielding cleaner and more resilient graph representations. We instantiate this framework on three representative GNN architectures and conduct extensive experiments on benchmarks. The experimental results show that our approach can significantly enhance the defense capability of GNNs in the high-perturbation regime.

</details>


### [24] [Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders](https://arxiv.org/abs/2512.22150)
*Hans Jarett J. Ong,Brian Godwin S. Lim,Dominic Dayta,Renzo Roel P. Tan,Kazushi Ikeda*

Main category: cs.LG

TL;DR: LANCA uses a deterministic autoencoder with an additive noise model layer to perform unsupervised causal discovery, improving identifiability and robustness over existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard unsupervised representation learning methods fail to capture causal dependencies due to identifiability issues; the need for strong inductive biases to disentangle causal variables from observational data without supervision.

Method: Proposes Latent Additive Noise Model Causal Autoencoder (LANCA), a deterministic Wasserstein Auto-Encoder coupled with a differentiable ANM Layer, transforming residual independence into an explicit optimization objective.

Result: LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow) and photorealistic environments (CANDLE), showing superior robustness to spurious correlations.

Conclusion: The additive noise model acts as an effective inductive bias for unsupervised causal discovery, with LANCA demonstrating practical improvements in identifiability and performance on complex datasets.

Abstract: Unsupervised representation learning seeks to recover latent generative factors, yet standard methods relying on statistical independence often fail to capture causal dependencies. A central challenge is identifiability: as established in disentangled representation learning and nonlinear ICA literature, disentangling causal variables from observational data is impossible without supervision, auxiliary signals, or strong inductive biases. In this work, we propose the Latent Additive Noise Model Causal Autoencoder (LANCA) to operationalize the Additive Noise Model (ANM) as a strong inductive bias for unsupervised discovery. Theoretically, we prove that while the ANM constraint does not guarantee unique identifiability in the general mixing case, it resolves component-wise indeterminacy by restricting the admissible transformations from arbitrary diffeomorphisms to the affine class. Methodologically, arguing that the stochastic encoding inherent to VAEs obscures the structural residuals required for latent causal discovery, LANCA employs a deterministic Wasserstein Auto-Encoder (WAE) coupled with a differentiable ANM Layer. This architecture transforms residual independence from a passive assumption into an explicit optimization objective. Empirically, LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow), and on photorealistic environments (CANDLE), where it demonstrates superior robustness to spurious correlations arising from complex background scenes.

</details>


### [25] [SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models](https://arxiv.org/abs/2512.22170)
*Jiesong Lian,Ruizhe Zhong,Zixiang Zhou,Xiaoyue Mi,Yixue Hao,Yuan Zhou,Qinglin Lu,Long Hu,Junchi Yan*

Main category: cs.LG

TL;DR: SoliReward is a framework for training video reward models using better data collection, hierarchical attention, and loss modification to improve alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Current video reward models face data noise from pairwise annotations, limited architectural exploration, and reward hacking, requiring enhanced training methods for human preference alignment.

Method: Collects binary-annotated data and uses cross-prompt pairing; employs Hierarchical Progressive Query Attention for feature aggregation; modifies BT loss for win-tie scenarios.

Result: Shows improvements in benchmarks for physical plausibility, subject deformity, and semantic alignment, enhancing reward model metrics and video generation model post-training efficacy.

Conclusion: SoliReward addresses key limitations of video reward models through systematic data, architecture, and training innovations, advancing the reliability of post-training alignment with human preferences.

Abstract: Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM's score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark will be publicly available.

</details>


### [26] [Wireless Traffic Prediction with Large Language Model](https://arxiv.org/abs/2512.22178)
*Chuanting Zhang,Haixia Zhang,Jingping Qiao,Zongzhang Li,Mohamed-Slim Alouini*

Main category: cs.LG

TL;DR: TIDES is an LLM-based framework for urban wireless traffic prediction that integrates spatial-temporal correlations through clustering, prompt engineering, and a DeepSeek module, outperforming baselines with efficient fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Intelligent resource management in next-generation wireless networks requires accurate traffic prediction, but existing deep learning and LLM methods neglect spatial dependencies in city-scale traffic dynamics.

Method: Propose TIDES: cluster regions to identify heterogeneous patterns and train personalized models; use prompt engineering to embed statistical features as structured inputs; design a DeepSeek module for spatial alignment via cross-domain attention; fine-tune lightweight components while freezing core LLM layers.

Result: Extensive experiments on real-world cellular traffic datasets show TIDES significantly surpasses state-of-the-art baselines in prediction accuracy and robustness.

Conclusion: Integrating spatial awareness into LLM-based predictors is crucial for scalable and intelligent network management in future 6G systems.

Abstract: The growing demand for intelligent, adaptive resource management in next-generation wireless networks has underscored the importance of accurate and scalable wireless traffic prediction. While recent advancements in deep learning and foundation models such as large language models (LLMs) have demonstrated promising forecasting capabilities, they largely overlook the spatial dependencies inherent in city-scale traffic dynamics. In this paper, we propose TIDES (Traffic Intelligence with DeepSeek-Enhanced Spatial-temporal prediction), a novel LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. TIDES first identifies heterogeneous traffic patterns across regions through a clustering mechanism and trains personalized models for each region to balance generalization and specialization. To bridge the domain gap between numerical traffic data and language-based models, we introduce a prompt engineering scheme that embeds statistical traffic features as structured inputs. Furthermore, we design a DeepSeek module that enables spatial alignment via cross-domain attention, allowing the LLM to leverage information from spatially related regions. By fine-tuning only lightweight components while freezing core LLM layers, TIDES achieves efficient adaptation to domain-specific patterns without incurring excessive training overhead. Extensive experiments on real-world cellular traffic datasets demonstrate that TIDES significantly outperforms state-of-the-art baselines in both prediction accuracy and robustness. Our results indicate that integrating spatial awareness into LLM-based predictors is the key to unlocking scalable and intelligent network management in future 6G systems.

</details>


### [27] [Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection](https://arxiv.org/abs/2512.22179)
*Rajeeb Thapa Chhetri,Zhixiong Chen,Saurab Thapa*

Main category: cs.LG

TL;DR: Latent Sculpting framework addresses generalization collapse in deep learning by sculpting compact latent manifolds using dual-centroid compression, achieving robust zero-shot anomaly detection with 0.87 F1-Score on OOD data where supervised methods fail.


<details>
  <summary>Details</summary>
Motivation: Addressing "Generalization Collapse" in supervised deep learning for high-dimensional tabular domains, where models fail catastrophically on Out-of-Distribution data due to lack of topological constraints in latent space, leading to diffuse manifolds where anomalies remain indistinguishable from benign data.

Method: Latent Sculpting, a hierarchical two-stage representation learning framework. Stage 1 uses a hybrid 1D-CNN and Transformer Encoder with Dual-Centroid Compactness Loss to sculpt benign traffic into a low-entropy hyperspherical cluster. Stage 2 conditions a Masked Autoregressive Flow on this pre-structured manifold for exact density estimation.

Result: Achieved F1-Score of 0.87 on zero-shot anomalies, compared to supervised baselines (F1 approx 0.30) and strongest unsupervised baseline (F1 0.76). Notably achieved 88.89% detection rate on complex "Infiltration" scenarios where state-of-the-art supervised models had 0.00% accuracy.

Conclusion: The findings suggest that decoupling structure learning from density estimation provides a scalable path toward generalized anomaly detection, with explicit manifold sculpting being a prerequisite for robust zero-shot generalization.

Abstract: A fundamental limitation of supervised deep learning in high-dimensional tabular domains is "Generalization Collapse": models learn precise decision boundaries for known distributions but fail catastrophically when facing Out-of-Distribution (OOD) data. We hypothesize that this failure stems from the lack of topological constraints in the latent space, resulting in diffuse manifolds where novel anomalies remain statistically indistinguishable from benign data. To address this, we propose Latent Sculpting, a hierarchical two-stage representation learning framework. Stage 1 utilizes a hybrid 1D-CNN and Transformer Encoder trained with a novel Dual-Centroid Compactness Loss (DCCL) to actively "sculpt" benign traffic into a low-entropy, hyperspherical cluster. Unlike standard contrastive losses that rely on triplet mining, DCCL optimizes global cluster centroids to enforce absolute manifold density. Stage 2 conditions a Masked Autoregressive Flow (MAF) on this pre-structured manifold to learn an exact density estimate. We evaluate this methodology on the rigorous CIC-IDS-2017 benchmark, treating it as a proxy for complex, non-stationary data streams. Empirical results demonstrate that explicit manifold sculpting is a prerequisite for robust zero-shot generalization. While supervised baselines suffered catastrophic performance collapse on unseen distribution shifts (F1 approx 0.30) and the strongest unsupervised baseline achieved only 0.76, our framework achieved an F1-Score of 0.87 on strictly zero-shot anomalies. Notably, we report an 88.89% detection rate on "Infiltration" scenarios--a complex distributional shift where state-of-the-art supervised models achieved 0.00% accuracy. These findings suggest that decoupling structure learning from density estimation provides a scalable path toward generalized anomaly detection.

</details>


### [28] [Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks](https://arxiv.org/abs/2512.22186)
*Vishnu Mohan*

Main category: cs.LG

TL;DR: A Dueling Double Deep Q-Network with curriculum learning successfully optimizes tennis strategy, achieving near-perfect win rates but revealing a defensive bias in learned policies.


<details>
  <summary>Details</summary>
Motivation: Tennis strategy optimization presents challenges including hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and opponent adaptation. The research aims to develop an effective RL approach to address these complex sequential decision-making problems in tennis.

Method: The study employs a reinforcement learning framework integrating a custom tennis simulation environment with a Dueling Double Deep Q-Network (DDQN) trained using curriculum learning. The environment models complete tennis scoring, tactical decisions across ten discrete actions, fatigue dynamics, and opponent skill. Curriculum learning progressively increases opponent difficulty.

Result: The trained agent achieves win rates of 98-100% against balanced opponents and maintains strong performance against challenging opponents. Serve efficiency ranges from 63.0-67.5% and return efficiency from 52.8-57.1%. Ablation studies confirm the necessity of dueling architecture and curriculum learning, while standard DQN fails to learn effective policies.

Conclusion: The paper highlights that despite achieving high win rates, the optimized policy exhibits a defensive bias, prioritizing error avoidance over aggressive play. This reveals a limitation of win-rate optimization in simplified sports simulations and emphasizes the importance of reward design for realistic sports reinforcement learning.

Abstract: Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.

</details>


### [29] [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://arxiv.org/abs/2512.23236)
*Gang Liao,Hongsen Qin,Ying Wang,Alicia Golden,Michael Kuchnik,Yavuz Yetim,Jia Jiunn Ang,Chunli Fu,Yihan He,Samuel Hsia,Zewei Jiang,Dianshi Li,Uladzimir Pashkevich,Varna Puvvada,Feng Shi,Matt Steiner,Ruichao Xiao,Nathan Yan,Xiayu Yu,Zhou Fang,Abdul Zainul-Abedin,Ketan Singh,Hongtao Yu,Wenyuan Chi,Barney Huang,Sean Zhang,Noah Weller,Zach Marine,Wyatt Cook,Carole-Jean Wu,Gaoxiang Liu*

Main category: cs.LG

TL;DR: KernelEvolve is an agentic kernel coding framework that automates kernel generation and optimization for deep learning recommendation models (DLRM) across heterogeneous hardware, reducing development time and improving performance.


<details>
  <summary>Details</summary>
Motivation: Deep learning recommendation model (DLRM) training and inference require speed and efficiency, but face challenges from model architecture diversity, kernel primitive diversity, and hardware heterogeneity.

Method: KernelEvolve takes kernel specifications as input, automating kernel generation and optimization across hardware by using multiple programming abstractions (e.g., Triton, CuTe DSL) and a graph-based search process with selection policy, universal operator, fitness function, and termination rule, enhanced by retrieval-augmented prompt synthesis for dynamic adaptation.

Result: KernelEvolve validated on KernelBench with 100% pass rate on 250 problems and 100% correctness on 160 PyTorch ATen operators across three hardware platforms. It reduces development time from weeks to hours and outperforms PyTorch baselines in performance for production use cases and heterogeneous AI systems.

Conclusion: KernelEvolve effectively addresses hardware heterogeneity challenges in DLRM, enhancing performance and programmability, and enables automated kernel generation for new AI hardware, mitigating barriers to adoption.

Abstract: Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.

</details>


### [30] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification](https://arxiv.org/abs/2512.22189)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: This second paper in a series integrates physics and uncertainty into machine learning for electrical transformer health assessment, covering Physics-Informed Neural Networks (PINNs) for spatiotemporal thermal modeling and insulation ageing, Bayesian PINNs for uncertainty quantification and robust predictions under sparse data, and emerging research directions for physics-aware trustworthy AI in power assets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance the monitoring, diagnostics, and prognostics of electrical transformers by integrating physics-based knowledge with machine learning models, specifically addressing the need for robust predictions and uncertainty quantification under sparse data conditions.

Method: The method involves applying Physics-Informed Neural Networks (PINNs) to spatiotemporal thermal modeling and solid insulation ageing problems, and extending this with Bayesian PINNs to quantify epistemic uncertainty and ensure prediction robustness when data is limited.

Result: The approach enables more accurate modeling of transformer thermal behavior and insulation ageing by incorporating physical laws, while Bayesian methods provide principled uncertainty estimates, improving reliability in health assessment tasks under data-scarce scenarios.

Conclusion: The paper concludes that physics-aware and trustworthy machine learning, particularly through PINNs and Bayesian extensions, holds significant potential for advancing the health assessment of critical power assets like transformers, and outlines emerging research directions to further this field.

Abstract: The integration of physics-based knowledge with machine learning models is increasingly shaping the monitoring, diagnostics, and prognostics of electrical transformers. In this two-part series, the first paper introduced the foundations of Neural Networks (NNs) and their variants for health assessment tasks. This second paper focuses on integrating physics and uncertainty into the learning process. We begin with the fundamentals of Physics-Informed Neural Networks (PINNs), applied to spatiotemporal thermal modeling and solid insulation ageing. Building on this, we present Bayesian PINNs as a principled framework to quantify epistemic uncertainty and deliver robust predictions under sparse data. Finally, we outline emerging research directions that highlight the potential of physics-aware and trustworthy machine learning for critical power assets.

</details>


### [31] [The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models](https://arxiv.org/abs/2512.23340)
*Dakuan Lu,Jiaqi Zhang,Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: The paper proposes the Law of Multi-model Collaboration as a new scaling law that predicts the performance of ensembles of large language models (LLMs) based on total parameter count. It introduces a method-agnostic formulation with an idealized integration oracle.


<details>
  <summary>Details</summary>
Motivation: While single LLMs follow scaling laws for improvements, their capabilities are inherently bounded. Multi-model collaboration through techniques like routing or ensembling can surpass individual models, but a theoretical framework for its performance scaling is lacking.

Method: The authors adopt a method-agnostic formulation assuming an idealized integration oracle where the total cross-entropy loss per sample is determined by the minimum loss of any model in the model pool. Experimental analysis examines scaling with respect to total parameter count.

Result: Multi-model systems follow a power-law scaling with total parameters, showing more significant improvement and lower theoretical loss floor than single models. Ensembles of heterogeneous model families achieve better scaling than homogeneous ones, highlighting model diversity as key to collaboration gains.

Conclusion: Model collaboration is a critical axis for extending the intelligence frontier of LLMs, with the proposed law providing a theoretical basis for understanding and leveraging collaborative gains.

Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.

</details>


### [32] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants](https://arxiv.org/abs/2512.22190)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: This paper reviews the application of neural networks in power transformer condition monitoring, covering NNs, CNNs, and RL for diagnostics, prognostics, and control in power systems.


<details>
  <summary>Details</summary>
Motivation: Traditional condition monitoring methods for power transformers, which are critical to grid reliability, face challenges like uncertainty, limited data, and operational complexity, prompting the need for ML-based approaches to improve accuracy.

Method: Introduces basic neural network concepts, explores convolutional neural networks for condition monitoring across multiple data types, and integrates neural networks with reinforcement learning for decision-making and control.

Result: The paper lays the groundwork for using neural networks and their extensions to enhance transformer monitoring, but specific experimental outcomes or validation results are not detailed in the abstract.

Conclusion: Neural networks and their extensions offer powerful tools for advancing transformer condition monitoring, with emerging research directions highlighted for future work.

Abstract: Power transformers are critical assets in power networks, whose reliability directly impacts grid resilience and stability. Traditional condition monitoring approaches, often rule-based or purely physics-based, struggle with uncertainty, limited data availability, and the complexity of modern operating conditions. Recent advances in machine learning (ML) provide powerful tools to complement and extend these methods, enabling more accurate diagnostics, prognostics, and control. In this two-part series, we examine the role of Neural Networks (NNs) and their extensions in transformer condition monitoring and health management tasks. This first paper introduces the basic concepts of NNs, explores Convolutional Neural Networks (CNNs) for condition monitoring using diverse data modalities, and discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control. Finally, perspectives on emerging research directions are also provided.

</details>


### [33] [Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks](https://arxiv.org/abs/2512.22192)
*Jiahao Lu*

Main category: cs.LG

TL;DR: The paper introduces a spectral bias framework for CNNs showing L2 regularization suppresses high-frequency features, revealing an accuracy-robustness trade-off.


<details>
  <summary>Details</summary>
Motivation: To understand the physical mechanisms of L2 regularization and Dropout in deep neural networks, focusing on their feature frequency selection properties.

Method: Developed a Visual Diagnostic Framework to track weight frequencies during training, proposed Spectral Suppression Ratio (SSR), and used discrete radial profiling to address aliasing in small CNN kernels; tested on ResNet-18 with CIFAR-10.

Result: L2 regularization reduces high-frequency energy by over 3x compared to unregularized baselines; it shows sensitivity to Gaussian noise but superior robustness (>6% improvement) against high-frequency information loss like blurring.

Conclusion: Regularization imposes a spectral inductive bias towards low-frequency structures, providing a signal-processing perspective on generalization and highlighting a trade-off between accuracy and robustness.

Abstract: Regularization techniques such as L2 regularization (Weight Decay) and Dropout are fundamental to training deep neural networks, yet their underlying physical mechanisms regarding feature frequency selection remain poorly understood. In this work, we investigate the Spectral Bias of modern Convolutional Neural Networks (CNNs). We introduce a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during training and propose a novel metric, the Spectral Suppression Ratio (SSR), to quantify the "low-pass filtering" intensity of different regularizers. By addressing the aliasing issue in small kernels (e.g., 3x3) through discrete radial profiling, our empirical results on ResNet-18 and CIFAR-10 demonstrate that L2 regularization suppresses high-frequency energy accumulation by over 3x compared to unregularized baselines. Furthermore, we reveal a critical Accuracy-Robustness Trade-off: while L2 models are sensitive to broadband Gaussian noise due to over-specialization in low frequencies, they exhibit superior robustness against high-frequency information loss (e.g., low resolution), outperforming baselines by >6% in blurred scenarios. This work provides a signal-processing perspective on generalization, confirming that regularization enforces a strong spectral inductive bias towards low-frequency structures.

</details>


### [34] [Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents](https://arxiv.org/abs/2512.22200)
*Dhruv Tiwari*

Main category: cs.LG

TL;DR: The paper proposes Emotion-Inspired Learning Signals (EILS), a bio-inspired framework that uses continuous internal states like curiosity and stress to dynamically modulate agent learning, aiming to improve robustness and efficiency in open-ended environments over traditional extrinsic reward methods.


<details>
  <summary>Details</summary>
Motivation: Traditional AI methods based on static extrinsic rewards lead to fragile agents in open-ended, non-stationary environments, lacking internal autonomy and adaptability. The unaddressed need is a high-level control mechanism analogous to biological emotion.

Method: Introduce Emotion-Inspired Learning Signals (EILS), modeling emotions as continuous, homeostatic appraisal signals (e.g., Curiosity, Stress, Confidence) derived from interaction history. These signals dynamically modulate the optimization landscape in real time, such as regulating entropy, plasticity, and trust regions.

Result: Hypothesize that EILS agents will outperform standard baselines in sample efficiency and adaptation to non-stationary environments due to closed-loop homeostatic regulation, though empirical results are not provided in the abstract.

Conclusion: EILS replaces scattered heuristics with a unified bio-inspired internal feedback engine, offering a promising direction for robust autonomy in AI by mimicking emotional homeostasis, potentially addressing limitations of extrinsic reward systems.

Abstract: The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this "extrinsic maximization" approach has rendered superhuman performance in closed, stationary fields, it produces agents that are fragile in open-ended, real-world environments. Standard agents lack internal autonomy: they struggle to explore without dense feedback, fail to adapt to distribution shifts (non-stationarity), and require extensive manual tuning of static hyperparameters. This paper proposes that the unaddressed factor in robust autonomy is a functional analog to biological emotion, serving as a high-level homeostatic control mechanism. We introduce Emotion-Inspired Learning Signals (EILS), a unified framework that replaces scattered optimization heuristics with a coherent, bio-inspired internal feedback engine. Unlike traditional methods that treat emotions as semantic labels, EILS models them as continuous, homeostatic appraisal signals such as Curiosity, Stress, and Confidence. We formalize these signals as vector-valued internal states derived from interaction history. These states dynamically modulate the agent's optimization landscape in real time: curiosity regulates entropy to prevent mode collapse, stress modulates plasticity to overcome inactivity, and confidence adapts trust regions to stabilize convergence. We hypothesize that this closed-loop homeostatic regulation can enable EILS agents to outperform standard baselines in terms of sample efficiency and non-stationary adaptation.

</details>


### [35] [Transformer Reconstructed with Dynamic Value Attention](https://arxiv.org/abs/2512.22212)
*Xiaowei Wang*

Main category: cs.LG

TL;DR: This paper proposes Dynamic Value Attention (DVA), a single-head attention mechanism for transformers that dynamically assigns values per query, reducing redundancy and training time while improving learning capability.


<details>
  <summary>Details</summary>
Motivation: Transformers have a static value for each query in attention heads, leading to redundancy despite multi-head designs, which are limited by complexity. A need exists for a more efficient dynamic approach.

Method: The method introduces DVA, which assigns a value dynamically for each query, allowing the elimination of redundant heads and the entire feed-forward network, as revised embeddings fetch sufficient useful values beyond context.

Result: Experiments show DVA reduces training time by 37.6% compared to the original transformer while enhancing learning capability.

Conclusion: A single-head DVA is sufficient for transformers, offering a streamlined and effective alternative to traditional multi-head attention structures.

Abstract: Since transformer was firstly published in 2017, several works have been proposed to optimize it. However, the major structure of transformer remains unchanged, ignoring one of its main intrinsic limitations, which is the same static value is used for every query in a head. Transformer itself tries to solve this problem by implementing multi-head attentions, yet the number of heads is limited by complexity. I propose a method to decide a value for each query dynamically, which could cut down all the redundant heads, keeping only one. Consequently, the following feed forward network could be cut down entirely, as each revised embedding has already fetched enough useful values far beyond the context. As a result, a single-head Dynamic Value Attention (DVA) is all you need in a transformer. According to the experiment, DVA may save 37.6% training time than the original transformer meanwhile increasing the learning capability.

</details>


### [36] [On the Existence and Behaviour of Secondary Attention Sinks](https://arxiv.org/abs/2512.22213)
*Jeffrey T. H. Wong,Cheng Zhang,Louis Mahon,Wayne Luk,Anton Isopoussu,Yiren Zhao*

Main category: cs.LG

TL;DR: This paper introduces 'secondary sinks,' a new class of attention sinks distinct from primary sinks (like BOS tokens), which form in middle layers via specific MLP modules, persist variably, affect attention dynamics, and exhibit more deterministic patterns in larger models.


<details>
  <summary>Details</summary>
Motivation: Prior work identified attention sinks like the BOS token as primary sinks with consistent properties, but the paper reveals the existence of secondary sinks that differ fundamentally in formation, location, and impact, motivating a deeper analysis to understand their role in attention mechanisms.

Method: The authors conduct extensive experiments across 11 model families to analyze secondary sinks, examining their appearance, properties (e.g., sink score, persistence), formation through middle-layer MLP modules that align tokens with primary sink directions, and impact on attention.

Result: Findings show that secondary sinks are formed by specific MLP modules in middle layers, with their sink score influenced by the L2-norm of resulting vectors, and they emerge as primary sinks weaken. Larger models like QwQ-32B and Qwen3-14B exhibit three and six sink levels, respectively, indicating more deterministic patterns.

Conclusion: Secondary sinks represent a distinct and important phenomenon in attention mechanisms, with formation and behavior that elucidate how attention mass is distributed, especially in larger models, contributing to a broader understanding of transformer dynamics.

Abstract: Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.

</details>


### [37] [Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning](https://arxiv.org/abs/2512.22221)
*Soroush Vahidi*

Main category: cs.LG

TL;DR: A framework for semi-supervised node classification using explicit combinatorial inference instead of deep message passing, adaptable to both homophilic and heterophilic graphs with an optional hybrid neural refinement for improved performance.


<details>
  <summary>Details</summary>
Motivation: Graph neural networks (GNNs) perform well on homophilic graphs but struggle under heterophily, where adjacent nodes often belong to different classes, highlighting the need for more adaptable and interpretable methods.

Method: The method uses a confidence-ordered greedy procedure driven by an additive scoring function that integrates class priors, neighborhood statistics, feature similarity, and training-derived label-label compatibility, controlled by transparent hyperparameters. It includes a validation-gated hybrid strategy where combinatorial predictions are optionally injected as priors into a lightweight neural model.

Result: Experiments on heterophilic and transitional benchmarks show competitive performance with modern GNNs while offering advantages in interpretability, tunability, and computational efficiency.

Conclusion: The framework provides an interpretable and adaptive solution for node classification that smoothly transitions between homophilic and heterophilic regimes, with flexibility in using neural refinement when beneficial.

Abstract: Graph neural networks (GNNs) achieve strong performance on homophilic graphs but often struggle under heterophily, where adjacent nodes frequently belong to different classes. We propose an interpretable and adaptive framework for semi-supervised node classification based on explicit combinatorial inference rather than deep message passing. Our method assigns labels using a confidence-ordered greedy procedure driven by an additive scoring function that integrates class priors, neighborhood statistics, feature similarity, and training-derived label-label compatibility. A small set of transparent hyperparameters controls the relative influence of these components, enabling smooth adaptation between homophilic and heterophilic regimes.
  We further introduce a validation-gated hybrid strategy in which combinatorial predictions are optionally injected as priors into a lightweight neural model. Hybrid refinement is applied only when it improves validation performance, preserving interpretability when neuralization is unnecessary. All adaptation signals are computed strictly from training data, ensuring a leakage-free evaluation protocol. Experiments on heterophilic and transitional benchmarks demonstrate competitive performance with modern GNNs while offering advantages in interpretability, tunability, and computational efficiency.

</details>


### [38] [Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases](https://arxiv.org/abs/2512.22222)
*Gnankan Landry Regis N'guessan*

Main category: cs.LG

TL;DR: Müntz-Szász Networks (MSN) replace fixed smooth activations with learnable fractional power bases for approximating singular or fractional power functions, achieving better performance than standard MLPs with fewer parameters and parameters.


<details>
  <summary>Details</summary>
Motivation: Standard neural network activations like ReLU, tanh, and sigmoid are poorly suited for approximating functions with singular or fractional power behavior common in physics, such as boundary layers and corner singularities.

Method: Introduce MSN architecture where each edge computes $φ(x) = \sum_k a_k |x|^{μ_k} + \sum_k b_k \mathrm{sign}(x)|x|^{λ_k}$, learning exponents $\{μ_k, λ_k\}$ alongside coefficients, based on Müntz-Szász theorem for universal approximation.

Result: MSN achieves 5-8x lower error than MLPs with 10x fewer parameters on singular target functions, and 3-6x improvement on PINN benchmarks including singular ODE and boundary-layer problems, with interpretable exponents matching known solution structure.

Conclusion: Theory-guided architectural design using MSN yields dramatic improvements for scientifically-motivated function classes, demonstrating enhanced approximation efficiency and interpretability.

Abstract: Standard neural network architectures employ fixed activation functions (ReLU, tanh, sigmoid) that are poorly suited for approximating functions with singular or fractional power behavior, a structure that arises ubiquitously in physics, including boundary layers, fracture mechanics, and corner singularities. We introduce Müntz-Szász Networks (MSN), a novel architecture that replaces fixed smooth activations with learnable fractional power bases grounded in classical approximation theory. Each MSN edge computes $φ(x) = \sum_k a_k |x|^{μ_k} + \sum_k b_k \mathrm{sign}(x)|x|^{λ_k}$, where the exponents $\{μ_k, λ_k\}$ are learned alongside the coefficients. We prove that MSN inherits universal approximation from the Müntz-Szász theorem and establish novel approximation rates: for functions of the form $|x|^α$, MSN achieves error $\mathcal{O}(|μ- α|^2)$ with a single learned exponent, whereas standard MLPs require $\mathcal{O}(ε^{-1/α})$ neurons for comparable accuracy. On supervised regression with singular target functions, MSN achieves 5-8x lower error than MLPs with 10x fewer parameters. Physics-informed neural networks (PINNs) represent a particularly demanding application for singular function approximation; on PINN benchmarks including a singular ODE and stiff boundary-layer problems, MSN achieves 3-6x improvement while learning interpretable exponents that match the known solution structure. Our results demonstrate that theory-guided architectural design can yield dramatic improvements for scientifically-motivated function classes.

</details>


### [39] [ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis](https://arxiv.org/abs/2512.22223)
*Shaghayegh Shajarian,Kennedy Marsh,James Benson,Sajad Khorsandroo,Mahmoud Abdelsalam*

Main category: cs.LG

TL;DR: ReGAIN is a multi-stage framework combining traffic summarization, retrieval-augmented generation (RAG), and LLMs for transparent network traffic analysis with high accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Traditional systems face high false positives and lack interpretability, limiting trust.

Method: Creates natural-language summaries, embeds into vector database, uses hierarchical retrieval with filtering, MMR sampling, reranking, and abstention.

Result: Achieved 95.95% to 98.82% accuracy on ICMP ping and TCP SYN flood attacks, validated by ground truth and expert assessments, outperforming baselines.

Conclusion: ReGAIN provides accurate, trustable network traffic analysis with evidence-backed explanations.

Abstract: Modern networks generate vast, heterogeneous traffic that must be continuously analyzed for security and performance. Traditional network traffic analysis systems, whether rule-based or machine learning-driven, often suffer from high false positives and lack interpretability, limiting analyst trust. In this paper, we present ReGAIN, a multi-stage framework that combines traffic summarization, retrieval-augmented generation (RAG), and Large Language Model (LLM) reasoning for transparent and accurate network traffic analysis. ReGAIN creates natural-language summaries from network traffic, embeds them into a multi-collection vector database, and utilizes a hierarchical retrieval pipeline to ground LLM responses with evidence citations. The pipeline features metadata-based filtering, MMR sampling, a two-stage cross-encoder reranking mechanism, and an abstention mechanism to reduce hallucinations and ensure grounded reasoning. Evaluated on ICMP ping flood and TCP SYN flood traces from the real-world traffic dataset, it demonstrates robust performance, achieving accuracy between 95.95% and 98.82% across different attack types and evaluation benchmarks. These results are validated against two complementary sources: dataset ground truth and human expert assessments. ReGAIN also outperforms rule-based, classical ML, and deep learning baselines while providing unique explainability through trustworthy, verifiable responses.

</details>


### [40] [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)
*Ying Zhu,Jiaxin Wan,Xiaoran Liu,Siyanag He,Qiqi Wang,Xu Guo,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.LG

TL;DR: We propose DiRL & DiPO: an efficient post-training framework for Diffusion LLMs, including the first GRPO implementation for dLLMs, achieving state-of-the-art math performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs hold promise but lack efficient post-training methods that address computational inefficiency and objective mismatches, limiting performance on complex tasks like math.

Method: Introduce DiRL framework integrating FlexAttention-accelerated blockwise training with LMDeploy-optimized inference for efficient two-stage post-training, and propose DiPO as the first unbiased GRPO implementation for dLLMs.

Result: DiRL-8B-Instruct trained on math data achieves state-of-the-art math performance among dLLMs and beats comparable Qwen2.5 series models on benchmarks.

Conclusion: DiRL addresses key limitations in dLLM post-training, enhancing efficiency and performance, paving the way for broader application of dLLMs.

Abstract: Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.

</details>


### [41] [Masking Teacher and Reinforcing Student for Distilling Vision-Language Models](https://arxiv.org/abs/2512.22238)
*Byung-Kwan Lee,Yu-Chiang Frank Wang,Ryo Hachiuma*

Main category: cs.LG

TL;DR: Masters is a mask-progressive reinforcement learning framework for distilling large vision-language models into compact ones, improving stability and performance by gradually masking and restoring teacher weights and using offline RL with dual rewards.


<details>
  <summary>Details</summary>
Motivation: Large-scale vision-language models are effective but too large for mobile or edge devices, and distilling them to smaller students is challenging due to size gaps causing unstable learning and degraded performance.

Method: Propose Masters: a mask-progressive reinforcement learning distillation framework that masks non-dominant teacher weights to simplify learning, then progressively increases teacher capacity, and uses offline RL with accuracy reward and distillation reward based on pre-generated responses from masked teachers.

Result: The framework enables students to learn richer representations smoothly and stably from teachers through incremental complexity and efficient offline guidance, avoiding computationally expensive processes.

Conclusion: Masters addresses the distillation challenge by reducing complexity and enhancing knowledge transfer, making compact vision-language models more practical for deployment while maintaining strong performance.

Abstract: Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.

</details>


### [42] [EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs](https://arxiv.org/abs/2512.22240)
*Chama Bensmail*

Main category: cs.LG

TL;DR: EvoXplain is a framework that measures the stability of model explanations across repeated training, revealing that high-accuracy models can rely on different internal mechanisms, showing explanatory instability even in assumed stable models.


<details>
  <summary>Details</summary>
Motivation: The assumption that high predictive accuracy implies correct and trustworthy explanations is questioned, as models might achieve the same outcome via different internal logics. The need to test whether explanations are stable or multimodal across training runs is highlighted.

Method: EvoXplain treats explanations as samples from stochastic optimization processes without aggregating predictions or using ensembles. It analyzes explanations from repeated training on datasets like Breast Cancer and COMPAS, using models like Logistic Regression and Random Forests, to detect multimodality or distinct explanatory modes.

Result: High-accuracy models often exhibit clear multimodality in explanations, even with stable models like Logistic Regression. Differences are not due to hyperparameter variation or performance trade-offs, making explanatory instability quantifiable.

Conclusion: EvoXplain reframes interpretability as a property of model classes under repeated instantiation, revealing when single or averaged explanations obscure multiple underlying mechanisms, emphasizing the importance of explanatory stability over just predictive performance.

Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. However, this assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different -- and potentially competing -- mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing a single trained model, EvoXplain treats explanations as samples drawn from the stochastic optimisation process itself -- without aggregating predictions or constructing ensembles -- and examines whether these samples form a single coherent explanation or separate into multiple, distinct explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using two widely deployed model classes: Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can produce multiple well-separated explanatory basins under repeated training on the same data split. These differences are not explained by hyperparameter variation or simple performance trade-offs. EvoXplain does not attempt to select a 'correct' explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.

</details>


### [43] [Enhanced geometry prediction in laser directed energy deposition using meta-learning](https://arxiv.org/abs/2512.22241)
*Abdul Malik Al Mardhouf Al Saadi,Amrita Basak*

Main category: cs.LG

TL;DR: A cross-dataset knowledge transfer model based on meta-learning is proposed for predicting deposited track geometry in laser-directed energy deposition, enabling accurate predictions with limited data.


<details>
  <summary>Details</summary>
Motivation: Accurate bead geometry prediction in L-DED is hindered by scarce and heterogeneous experimental datasets under different materials, machine configurations, and process parameters.

Method: Two gradient-based meta-learning algorithms, Model-Agnostic Meta-Learning (MAML) and Reptile, are investigated for rapid adaptation to new conditions. The framework is evaluated using multiple datasets from literature and experiments across powder-fed, wire-fed, and hybrid wire-powder L-DED processes.

Result: MAML and Reptile achieve accurate bead height predictions on unseen tasks with as few as 3-9 training examples, outperforming conventional feedforward neural networks, with R-squared values up to ~0.9 and mean absolute errors of 0.03-0.08 mm.

Conclusion: The meta-learning models show strong generalization and effective knowledge transfer across heterogeneous L-DED settings, enabling reliable predictions with minimal data.

Abstract: Accurate bead geometry prediction in laser-directed energy deposition (L-DED) is often hindered by the scarcity and heterogeneity of experimental datasets collected under different materials, machine configurations, and process parameters. To address this challenge, a cross-dataset knowledge transfer model based on meta-learning for predicting deposited track geometry in L-DED is proposed. Specifically, two gradient-based meta-learning algorithms, i.e., Model-Agnostic Meta-Learning (MAML) and Reptile, are investigated to enable rapid adaptation to new deposition conditions with limited data. The proposed framework is performed using multiple experimental datasets compiled from peer-reviewed literature and in-house experiments and evaluated across powder-fed, wire-fed, and hybrid wire-powder L-DED processes. Results show that both MAML and Reptile achieve accurate bead height predictions on unseen target tasks using as few as three to nine training examples, consistently outperforming conventional feedforward neural networks trained under comparable data constraints. Across multiple target tasks representing different printing conditions, the meta-learning models achieve strong generalization performance, with R-squared values reaching up to approximately 0.9 and mean absolute errors between 0.03-0.08 mm, demonstrating effective knowledge transfer across heterogeneous L-DED settings.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [44] [ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling](https://arxiv.org/abs/2512.22129)
*Conor Wallace,Umer Siddique,Yongcan Cao*

Main category: cs.MA

TL;DR: LLM-based framework for ad-hoc teamwork using behavior classification and RAG for improved adaptation in cooperative environments.


<details>
  <summary>Details</summary>
Motivation: Ad-hoc teamwork requires adaptive inference of unseen teammates; conventional models are brittle under partial observability, motivating the use of flexible LLMs as behavioral world models.

Method: Introduce Collab: classifies teammate types via behavior rubric from trajectory features; extend to ReCollab: incorporates RAG with exemplar trajectories to stabilize inference.

Result: Collab effectively distinguishes teammate types; ReCollab improves adaptation across layouts, achieving Pareto-optimal trade-offs between classification accuracy and episodic return in Overcooked.

Conclusion: LLMs show potential as behavioral world models for AHT, with retrieval grounding crucial in challenging coordination settings.

Abstract: Ad-hoc teamwork (AHT) requires agents to infer the behavior of previously unseen teammates and adapt their policy accordingly. Conventional approaches often rely on fixed probabilistic models or classifiers, which can be brittle under partial observability and limited interaction. Large language models (LLMs) offer a flexible alternative: by mapping short behavioral traces into high-level hypotheses, they can serve as world models over teammate behavior. We introduce \Collab, a language-based framework that classifies partner types using a behavior rubric derived from trajectory features, and extend it to \ReCollab, which incorporates retrieval-augmented generation (RAG) to stabilize inference with exemplar trajectories. In the cooperative Overcooked environment, \Collab effectively distinguishes teammate types, while \ReCollab consistently improves adaptation across layouts, achieving Pareto-optimal trade-offs between classification accuracy and episodic return. These findings demonstrate the potential of LLMs as behavioral world models for AHT and highlight the importance of retrieval grounding in challenging coordination settings.

</details>


### [45] [Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time](https://arxiv.org/abs/2512.22171)
*Stefan Edelkamp*

Main category: cs.MA

TL;DR: Novel mission planning for multi-agent fleets in graphs with dynamic goal assignment achieves polynomial-time conflict-free routing, unlike NP-hard traditional vehicle routing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address multi-agent mission planning in graphs where goal assignment is not predefined, unlike typical multi-agent path-finding. The aim is to achieve near-optimal solutions in continuous Euclidean spaces and polynomial-time solutions for discrete cases despite NP-hardness of traditional vehicle routing.

Method: The method involves solving mission planning in graphs by dynamically assigning goals to agents. It uses global assignment strategies to reduce conflicts, then resolves remaining conflicts via techniques like ants-on-the-stick concept, local assignment problems, path interleaving, and destination kicking.

Result: The planner finds conflict-free optimized routes for agents. Global assignment strategies significantly reduce conflicts, which are further resolved by local assignment, path interleaving, and kicking agents out of destinations.

Conclusion: The paper concludes that the novel approach for mission planning in graphs with goal assignment yields polynomial-time solutions for conflict-free routes, unlike traditional NP-hard vehicle routing. It successfully integrates global assignment strategies and conflict resolution techniques.

Abstract: In this paper, we plan missions for a fleet of agents in undirected graphs, such as grids, with multiple goals. In contrast to regular multi-agent path-finding, the solver finds and updates the assignment of goals to the agents on its own. In the continuous case for a point agent with motions in the Euclidean plane, the problem can be solved arbitrarily close to optimal. For discrete variants that incur node and edge conflicts, we show that it can be solved in polynomial time, which is unexpected, since traditional vehicle routing on general graphs is NP-hard. We implement a corresponding planner that finds conflict-free optimized routes for the agents. Global assignment strategies greatly reduce the number of conflicts, with the remaining ones resolved by elaborating on the concept of ants-on-the-stick, by solving local assignment problems, by interleaving agent paths, and by kicking agents that have already arrived out of their destinations

</details>


### [46] [Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring](https://arxiv.org/abs/2512.22496)
*Saisab Sadhu,Ashim Dhor*

Main category: cs.MA

TL;DR: HPO framework uses structured adversarial debates to improve LLM pedagogical reasoning, outperforming GPT-4o with 20x fewer parameters on math tutoring tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail at pedagogical reasoning, frequently validating incorrect student solutions (sycophancy) or providing overly direct answers that hinder learning, and there is a need for reliable automated tutors to address educator shortages.

Method: Hierarchical Pedagogical Oversight (HPO), a framework that adapts structured adversarial synthesis to educational assessment with specialist agents that distill dialogue context and a moderated five-act debate between opposing pedagogical critics.

Result: The 8B-parameter model achieves a Macro F1 of 0.845 on the MRBench dataset of 1,214 middle-school mathematics dialogues, outperforming GPT-4o (0.812) by 3.3% while using 20 times fewer parameters.

Conclusion: The results establish adversarial reasoning as a critical mechanism for deploying reliable, low-compute pedagogical oversight in resource-constrained environments.

Abstract: Large Language Models (LLMs) are increasingly deployed as automated tutors to address educator shortages; however, they often fail at pedagogical reasoning, frequently validating incorrect student solutions (sycophancy) or providing overly direct answers that hinder learning. We introduce Hierarchical Pedagogical Oversight (HPO), a framework that adapts structured adversarial synthesis to educational assessment. Unlike cooperative multi-agent systems that often drift toward superficial consensus, HPO enforces a dialectical separation of concerns: specialist agents first distill dialogue context, which then grounds a moderated, five-act debate between opposing pedagogical critics. We evaluate this framework on the MRBench dataset of 1,214 middle-school mathematics dialogues. Our 8B-parameter model achieves a Macro F1 of 0.845, outperforming GPT-4o (0.812) by 3.3% while using 20 times fewer parameters. These results establish adversarial reasoning as a critical mechanism for deploying reliable, low-compute pedagogical oversight in resource-constrained environments.

</details>


### [47] [MARPO: A Reflective Policy Optimization for Multi Agent Reinforcement Learning](https://arxiv.org/abs/2512.22832)
*Cuiling Wu,Yaozhong Gan,Junliang Xing,Ying Fu*

Main category: cs.MA

TL;DR: MARPO improves sample efficiency and stability in multi-agent reinforcement learning using reflection and asymmetric clipping.


<details>
  <summary>Details</summary>
Motivation: To address the issue of sample inefficiency in multi-agent reinforcement learning, as conventional methods often require large amounts of data, limiting practical applications.

Method: Introduces a reflection mechanism that uses subsequent trajectories to enhance sample efficiency and an asymmetric clipping mechanism derived from KL divergence to dynamically adjust clipping range for better training stability.

Result: MARPO consistently outperforms other methods in classic multi-agent environments, demonstrating improved performance.

Conclusion: MARPO effectively alleviates sample inefficiency and enhances training stability in multi-agent reinforcement learning, showing promise for practical use.

Abstract: We propose Multi Agent Reflective Policy Optimization (MARPO) to alleviate the issue of sample inefficiency in multi agent reinforcement learning. MARPO consists of two key components: a reflection mechanism that leverages subsequent trajectories to enhance sample efficiency, and an asymmetric clipping mechanism that is derived from the KL divergence and dynamically adjusts the clipping range to improve training stability. We evaluate MARPO in classic multi agent environments, where it consistently outperforms other methods.

</details>


### [48] [Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks](https://arxiv.org/abs/2512.22876)
*Maksim Kryzhanovskiy,Svetlana Glazyrina,Roman Ischenko,Konstantin Vorontsov*

Main category: cs.MA

TL;DR: This paper introduces Reinforcement Networks, a framework for Multi-Agent Reinforcement Learning (MARL) that organizes agents as vertices in a directed acyclic graph (DAG), enabling flexible training and coordination without restrictive assumptions.


<details>
  <summary>Details</summary>
Motivation: Modern AI systems often have multiple learnable components organized as graphs, but end-to-end training is challenging without restrictive architectural or training assumptions, creating a need for a more flexible MARL approach.

Method: The authors propose Reinforcement Networks, formalizing training and inference methods within this framework and connecting it to LevelEnv for reproducible construction, training, and evaluation. They develop several models based on this framework.

Result: The approach is demonstrated on collaborative MARL setups, showing improved performance over standard MARL baselines. It unifies hierarchical, modular, and graph-structured views of MARL.

Conclusion: Reinforcement Networks offers a principled path for designing complex multi-agent systems, with potential extensions to richer graph morphologies, compositional curricula, and graph-aware exploration, positioning it as a foundation for scalable, structured MARL research.

Abstract: Modern AI systems often comprise multiple learnable components that can be naturally organized as graphs. A central challenge is the end-to-end training of such systems without restrictive architectural or training assumptions. Such tasks fit the theory and approaches of the collaborative Multi-Agent Reinforcement Learning (MARL) field. We introduce Reinforcement Networks, a general framework for MARL that organizes agents as vertices in a directed acyclic graph (DAG). This structure extends hierarchical RL to arbitrary DAGs, enabling flexible credit assignment and scalable coordination while avoiding strict topologies, fully centralized training, and other limitations of current approaches. We formalize training and inference methods for the Reinforcement Networks framework and connect it to the LevelEnv concept to support reproducible construction, training, and evaluation. We demonstrate the effectiveness of our approach on several collaborative MARL setups by developing several Reinforcement Networks models that achieve improved performance over standard MARL baselines. Beyond empirical gains, Reinforcement Networks unify hierarchical, modular, and graph-structured views of MARL, opening a principled path toward designing and training complex multi-agent systems. We conclude with theoretical and practical directions - richer graph morphologies, compositional curricula, and graph-aware exploration. That positions Reinforcement Networks as a foundation for a new line of research in scalable, structured MARL.

</details>


### [49] [Heterogeneity in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.22941)
*Tianyi Hu,Zhiqiang Pu,Yuan Wang,Tenghai Qiu,Min Chen,Xin Yu*

Main category: cs.MA

TL;DR: This paper provides a systematic framework for defining, quantifying, and utilizing heterogeneity in multi-agent reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Heterogeneity is a fundamental property in MARL but lacks a rigorous definition and deeper understanding in the field, which limits its application.

Method: The paper categorizes heterogeneity into five types with mathematical definitions, defines heterogeneity distance with a practical quantification method, and designs a heterogeneity-based multi-agent dynamic parameter sharing algorithm.

Result: Case studies show the method effectively identifies and quantifies agent heterogeneity; experiments demonstrate the algorithm has better interpretability and adaptability compared to baselines.

Conclusion: The methodology helps the MARL community gain a comprehensive understanding of heterogeneity and promotes practical algorithm development.

Abstract: Heterogeneity is a fundamental property in multi-agent reinforcement learning (MARL), which is closely related not only to the functional differences of agents, but also to policy diversity and environmental interactions. However, the MARL field currently lacks a rigorous definition and deeper understanding of heterogeneity. This paper systematically discusses heterogeneity in MARL from the perspectives of definition, quantification, and utilization. First, based on an agent-level modeling of MARL, we categorize heterogeneity into five types and provide mathematical definitions. Second, we define the concept of heterogeneity distance and propose a practical quantification method. Third, we design a heterogeneity-based multi-agent dynamic parameter sharing algorithm as an example of the application of our methodology. Case studies demonstrate that our method can effectively identify and quantify various types of agent heterogeneity. Experimental results show that the proposed algorithm, compared to other parameter sharing baselines, has better interpretability and stronger adaptability. The proposed methodology will help the MARL community gain a more comprehensive and profound understanding of heterogeneity, and further promote the development of practical algorithms.

</details>


### [50] [Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing](https://arxiv.org/abs/2512.23445)
*Manuel Franco-Vivo*

Main category: cs.MA

TL;DR: This paper analyzes behavior coverage in multi-agent simulations for autonomous vehicle testing, proposing an MPC-based pedestrian agent to improve testing realism and identify system robustness.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety and reliability of autonomous vehicles requires comprehensive testing in diverse real-world scenarios, necessitating methods to evaluate behavior coverage in simulations.

Method: Defines driving scenarios and agent interactions to measure behavior coverage, uses coverage metrics analysis, and proposes a Model Predictive Control (MPC) pedestrian agent with an objective function for interesting tests and realistic behavior.

Result: Highlights the importance of behavior coverage for validation, identifies improvement areas through coverage-based testing, and shows the MPC agent promotes more realistic behavior than previous pedestrian agents.

Conclusion: Contributes to autonomous vehicle testing by enabling comprehensive evaluation of system behavior in simulations, offering insights to enhance safety, reliability, and performance through rigorous methodologies.

Abstract: As autonomous vehicle technology advances, ensuring the safety and reliability of these systems becomes paramount. Consequently, comprehensive testing methodologies are essential to evaluate the performance of autonomous vehicles in diverse and complex real-world scenarios. This study focuses on the behaviour coverage analysis of a multi-agent system simulation designed for autonomous vehicle testing, and provides a systematic approach to measure and assess behaviour coverage within the simulation environment. By defining a set of driving scenarios, and agent interactions, we evaluate the extent to which the simulation encompasses a broad range of behaviours relevant to autonomous driving.
  Our findings highlight the importance of behaviour coverage in validating the effectiveness and robustness of autonomous vehicle systems. Through the analysis of behaviour coverage metrics and coverage-based testing, we identify key areas for improvement and optimization in the simulation framework. Thus, a Model Predictive Control (MPC) pedestrian agent is proposed, where its objective function is formulated to encourage \textit{interesting} tests while promoting a more realistic behaviour than other previously studied pedestrian agents. This research contributes to advancing the field of autonomous vehicle testing by providing insights into the comprehensive evaluation of system behaviour in simulated environments. The results offer valuable implications for enhancing the safety, reliability, and performance of autonomous vehicles through rigorous testing methodologies.

</details>
