<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 73]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: Prover Agent combines LLMs and Lean for theorem proving, achieving 86.1% success on MiniF2F with fewer samples than prior methods.


<details>
  <summary>Details</summary>
Motivation: To improve automated theorem proving by integrating informal reasoning (LLMs) with formal proof assistants (Lean) and generating auxiliary lemmas.

Method: Coordinates an LLM, a formal prover model, and Lean feedback while generating lemmas to aid proof discovery.

Result: 86.1% success rate on MiniF2F, outperforming SLM-based methods with lower sample budgets.

Conclusion: Prover Agent sets a new benchmark for automated theorem proving by leveraging LLMs and Lean effectively.

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [2] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: A novel framework for context attribution in generative QA systems using combinatorial multi-armed bandits (CMAB) and Combinatorial Thompson Sampling (CTS) to improve query efficiency and attribution fidelity.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and trustworthiness in generative QA systems by identifying which parts of retrieved context contribute to the model's answers.

Method: Formulates context attribution as a CMAB problem, treating each context segment as a bandit arm and using CTS to explore context subsets efficiently. A reward function based on normalized token likelihoods evaluates segment relevance.

Result: Achieves competitive attribution quality with fewer model queries compared to traditional perturbation-based methods like SHAP.

Conclusion: The proposed method offers a query-efficient and high-fidelity solution for context attribution in generative QA systems.

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [3] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: The paper benchmarks LLMs for quantum code generation using QHackBench, a dataset from Quantum Hackathon, evaluating performance with standard prompting and RAG. Results show RAG-enhanced models perform comparably, especially in complex tasks, and a multi-agent pipeline improves execution success. The dataset and framework will be publicly released.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of LLMs in quantum computing code generation, using real-world challenges from QHack.

Method: Introduces QHackBench, evaluates LLMs with vanilla prompting and RAG, and assesses functional correctness, syntactic validity, and execution success. Uses a multi-agent pipeline for refining solutions.

Result: RAG-enhanced models perform similarly to standard prompting, particularly in complex quantum algorithms. The multi-agent pipeline improves execution success rates.

Conclusion: The study demonstrates the viability of LLMs in quantum code generation and commits to releasing QHackBench and tools to advance AI-assisted quantum programming.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [4] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: Custom RAG models outperform commercial LLMs in medical tasks with higher accuracy and lower energy consumption.


<details>
  <summary>Details</summary>
Motivation: Address environmental and ethical concerns of AI in healthcare, focusing on resource use and patient privacy.

Method: Developed a customizable RAG framework for medical tasks, monitoring energy and CO2 emissions, and compared performance with commercial models.

Result: RAG models, especially llama3.1:8B, achieved higher accuracy (58.5%) and lower energy use (0.52 Performance per kWh, 473g CO2) than commercial models.

Conclusion: Local LLMs can create sustainable, high-performing RAGs for medical tasks, aligning with UN sustainability goals.

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [5] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: The paper explores real-time AI-driven decision support systems, focusing on low-latency models, Edge-IoT integration, and human-AI collaboration. It highlights advancements like DeLLMa and model compression, addressing resource constraints and adaptable frameworks.


<details>
  <summary>Details</summary>
Motivation: To enhance real-time decision-making with AI, especially in resource-limited scenarios, by integrating Edge-IoT and improving human-AI teamwork.

Method: Reviews holistic AI tools, Edge-IoT integration, and techniques like DeLLMa and model compression for edge devices.

Result: Provides insights into efficient AI-supported systems, development strategies, and application areas.

Conclusion: Sets the stage for future AI breakthroughs in real-time decision support, emphasizing flexibility and efficiency.

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [6] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: Persona-assigned LLMs exhibit human-like motivated reasoning, reducing veracity discernment and showing identity-congruent biases, which are hard to mitigate with debiasing prompts.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs exhibit motivated reasoning akin to humans, particularly when assigned personas, and assess its societal implications.

Method: Tested 8 LLMs with 8 personas across political and socio-demographic attributes on veracity discernment and scientific evidence evaluation tasks.

Result: Persona-assigned LLMs showed reduced veracity discernment (up to 9%) and higher likelihood (up to 90%) of identity-congruent reasoning, with debiasing prompts proving ineffective.

Conclusion: LLMs display human-like motivated reasoning, raising concerns about exacerbating biases in both AI and human decision-making.

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [7] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM integrates EHR data into medical dialogues for clinical test recommendation, result interpretation, and diagnosis prediction, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing medical LLMs overlook EHR data and focus narrowly on diagnosis, limiting clinical applicability.

Method: DiaLLM uses a Clinical Test Reference (CTR) strategy to map EHR codes to descriptions and classify results. It employs reinforcement learning with reject sampling and custom rewards for accurate diagnosis.

Result: DiaLLM outperforms baselines in clinical test recommendation and diagnosis prediction.

Conclusion: DiaLLM enhances clinical applicability by integrating EHR data and improving diagnostic accuracy.

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [8] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: OpenPub is an AI-powered platform with a Reproducibility Copilot that reduces reproduction time from 30+ hours to ~1 hour by generating structured Jupyter Notebooks and identifying reproducibility barriers.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ensuring independent reproducibility of research findings in open science.

Method: Introduces OpenPub's Reproducibility Copilot, which analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations. Feasibility tests were conducted using papers with known reproducibility benchmarks.

Result: OpenPub reduces reproduction time significantly (30+ hours to ~1 hour) and achieves high coverage of reproducible figures, tables, and results. It detects barriers like missing hyperparameters and incomplete datasets.

Conclusion: AI-driven tools like OpenPub can ease reproducibility efforts and enhance transparent scientific communication, with potential for broader open science applications.

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [9] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: Genesys, a multi-agent LLM system, discovers novel LM architectures by simulating research stages, using genetic programming for efficiency, and outperforms known architectures on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can autonomously discover novel LM architectures by mimicking real research processes.

Method: Multi-agent LLM approach with stages like ideation, code generation, pre-training, and evaluation, using genetic programming and scaling laws.

Result: 1,162 new designs discovered (1,062 verified), with top designs outperforming GPT2 and Mamba2 on benchmarks.

Conclusion: Genesys demonstrates effective autonomous discovery of competitive LM architectures, offering insights for future systems.

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [10] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: A 14-task framework based on Bloom's Taxonomy is proposed to evaluate LLMs for enterprise tasks, addressing noisy data and costly annotation with a scalable pipeline. Open-source models like DeepSeek R1 compete with proprietary ones in reasoning but lag in judgment tasks. The benchmark highlights enterprise performance gaps and offers optimization insights.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like MMLU fail to assess enterprise-specific task complexities, necessitating a tailored evaluation framework for LLMs in enterprise contexts.

Method: A scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG) is developed to curate a robust 9,700-sample benchmark.

Result: Open-source models (e.g., DeepSeek R1) rival proprietary models in reasoning tasks but underperform in judgment-based scenarios, likely due to overthinking.

Conclusion: The benchmark identifies enterprise performance gaps and provides actionable insights for optimizing LLMs, advancing practical deployment in enterprise settings.

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [11] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: Mobile-R1 introduces multi-turn reinforcement learning with task-level rewards to enhance mobile agents' exploration and error correction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods limit agents' dynamic interaction with environments, leading to local optima and weak exploration.

Method: Three-stage training: initial finetuning, single-step online training with action-level rewards, and multi-turn online training with task-level rewards.

Result: Improved performance in exploration and error correction, supported by a new dataset and benchmark.

Conclusion: Mobile-R1 advances mobile agent capabilities and provides open-source resources for further research.

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [12] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: REFeat guides LLMs to generate diverse and informative features for tabular data using structured reasoning, outperforming existing methods in accuracy and feature quality.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based feature engineering methods produce overly simple or repetitive features due to biases and lack of reasoning guidance.

Method: REFeat leverages multiple reasoning types to steer LLM-driven feature generation, ensuring diversity and informativeness.

Result: Experiments on 59 datasets show higher predictive accuracy and more meaningful features compared to existing approaches.

Conclusion: Incorporating rich reasoning and adaptive strategies enhances LLM-driven feature discovery for tabular data.

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [13] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: The paper introduces Paladin-mini, a compact classifier model, and a grounding-benchmark dataset to evaluate grounding claims in documents.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of grounding claims in documents by providing supportive evidence.

Method: Developed Paladin-mini (3.8B parameters) for labeling grounded/ungrounded claims and created the grounding-benchmark dataset.

Result: Demonstrated Paladin-mini's performance against State-of-the-art benchmarks with reproducible results.

Conclusion: The contributions (Paladin-mini and grounding-benchmark) effectively address grounding claims in real-world scenarios.

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [14] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: The paper introduces the Electric Vehicle Orienteering Problem with V2G (EVOP-V2G), focusing on profit maximization for EV drivers by optimizing charging/discharging, route planning, and dynamic pricing. Two metaheuristic algorithms (EA and LNS) outperform baselines, doubling profits and scaling well.


<details>
  <summary>Details</summary>
Motivation: The integration of EVs into ride-hailing and delivery services, coupled with V2G technology, creates new challenges and opportunities for optimizing profits while managing charging/discharging and dynamic electricity prices.

Method: The problem is formulated as a Mixed Integer Programming (MIP) model, and two metaheuristic algorithms (evolutionary and large neighborhood search) are proposed for near-optimal solutions.

Result: Experiments on real-world data show the algorithms double driver profits compared to baselines, with near-optimal performance on small instances and excellent scalability on larger ones.

Conclusion: The work demonstrates a promising approach for smarter, more profitable EV-based mobility systems that support the energy grid.

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [15] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: GymPN is a software library using Deep Reinforcement Learning for optimal decision-making in business processes, addressing partial observability and multiple decisions.


<details>
  <summary>Details</summary>
Motivation: To support optimal task allocation decisions in organizations by overcoming limitations of prior work, such as partial process observability and single-decision modeling.

Method: Develops GymPN, a library leveraging Deep Reinforcement Learning, to model and solve business process decisions, including partial observability and multiple decisions.

Result: Evaluated on eight business process patterns, GymPN effectively models problems and learns optimal policies.

Conclusion: GymPN advances process management by enabling realistic decision-making scenarios and optimal policy learning.

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [16] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: MNCA combines mixture models with Neural Cellular Automata to better model stochastic biological systems, showing improved robustness and accuracy in simulations.


<details>
  <summary>Details</summary>
Motivation: The deterministic nature of NCAs limits their ability to capture real-world stochasticity, prompting the need for a probabilistic approach like MNCA.

Method: MNCA integrates probabilistic rule assignments and intrinsic noise into NCAs, enabling diverse local behaviors and stochastic dynamics.

Result: MNCA outperforms in tissue growth simulations, image morphogenesis robustness, and microscopy segmentation, offering interpretable rule segmentation.

Conclusion: MNCA is a promising tool for modeling stochastic systems and studying self-growth processes due to its robustness and biological accuracy.

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [17] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: A functional and computational definition of sentience for AI, emphasizing assertoric and qualitative sensory signals, with implementation insights.


<details>
  <summary>Details</summary>
Motivation: To provide a clear, implementable definition of sentience for AI, ensuring it captures subjective experience beyond mere perceptual encoding.

Method: Proposes that sentience requires sensory signals to be both assertoric (persistent) and qualitative, with potential implementation methods outlined.

Result: A framework for designing sentient AI, aiding in intentional creation and timely recognition of sentient artificial agents.

Conclusion: Defining sentience functionally helps guide AI development and prevents inadvertent creation of sentient machines.

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [18] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: The paper introduces a CBR-LLM framework combining case-based reasoning with LLMs for evasive maneuver decisions in driving, improving accuracy and human alignment.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous driving decision-making by addressing LLMs' limitations in domain adaptation, contextual grounding, and experiential knowledge.

Method: Integrates semantic scene understanding from dashcam videos with retrieval of past driving cases to guide LLM-generated recommendations.

Result: Improves decision accuracy, justification quality, and alignment with human experts; case retrieval outperforms random sampling.

Conclusion: The CBR-LLM framework is robust and adaptive, showing promise as a trustworthy decision-support tool for intelligent driving.

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [19] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: A multi-agent AI framework using GPT-based LLMs for sustainable protein research, focusing on microbial sources, improved information extraction with fine-tuning and prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Addressing the global demand for sustainable protein by enhancing knowledge synthesis in microbial protein production.

Method: Two GPT-based agents: one for literature search, another for information extraction, optimized via fine-tuning and prompt engineering.

Result: Fine-tuning improved mean cosine similarity scores to ≥0.94, while prompt engineering achieved ≥0.89 with lower uncertainty.

Conclusion: The framework effectively supports sustainable protein research, with fine-tuning showing superior performance.

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [20] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: CogGen is an AI architecture that converts programming videos into interactive, adaptive learning experiences using student modeling and generative AI tutoring based on Cognitive Apprenticeship.


<details>
  <summary>Details</summary>
Motivation: To enhance video-based programming education by making it interactive and adaptive, bridging structured student modeling with AI-driven tutoring.

Method: Three components: (1) video segmentation by learning goals, (2) conversational tutoring engine using Cognitive Apprenticeship, (3) student model with Bayesian Knowledge Tracing for adaptation.

Result: Effective video segmentation accuracy and strong pedagogical alignment across layers. Ablation studies confirm each component's necessity.

Conclusion: CogGen advances AI-powered tutoring by integrating structured modeling with interactive AI, offering scalable improvements for programming education.

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [21] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: The paper discusses using LLMs to organize and utilize PETSc's fragmented knowledge base, employing tools like RAG and chatbots to improve accessibility and documentation.


<details>
  <summary>Details</summary>
Motivation: PETSc's extensive but informal knowledge base is underutilized, hindering users and developers. The goal is to leverage LLMs to make this knowledge more accessible and actionable.

Method: The team built an LLM-powered system with RAG, reranking algorithms, and chatbots, evaluated using various LLMs and embedding models.

Result: Initial experiences show promise in enhancing PETSc's documentation and workflows, particularly for scalable Krylov solvers.

Conclusion: The work aims to create an extensible AI framework for scientific software, with plans to expand it into a robust platform for accelerating research.

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [22] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: MLE-Live is a framework for evaluating ML agents' ability to collaborate in a simulated research community. CoMind, a novel agent, excels in this context, outperforming most human competitors in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: Existing ML agents operate in isolation, missing the collaborative benefits of human research communities. The goal is to enhance agents' ability to communicate and leverage collective knowledge.

Method: Introduces MLE-Live, a live evaluation framework, and CoMind, an agent designed to exchange insights and develop solutions within a simulated Kaggle community.

Result: CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% of human competitors in four Kaggle competitions.

Conclusion: CoMind demonstrates the potential of collaborative ML agents, bridging the gap between isolated research and community-driven innovation.

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [23] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: The paper introduces Decrypto, a game-based benchmark for evaluating multi-agent reasoning and theory of mind (ToM) in LLMs, addressing gaps in existing benchmarks. It finds current LLMs lag behind humans and older models in ToM tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for ToM and multi-agent reasoning in LLMs are limited by narrow scope, data leakage, saturation, and lack of interactivity. Decrypto aims to fill this gap.

Method: Decrypto is designed as an interactive game-based benchmark inspired by cognitive science, computational pragmatics, and multi-agent reinforcement learning. It includes empirical evaluations of LLMs, robustness studies, and human-AI cross-play experiments.

Result: LLMs perform worse than humans and simple baselines in ToM tasks. Surprisingly, state-of-the-art reasoning models underperform older counterparts in key ToM abilities.

Conclusion: Decrypto successfully addresses a critical gap in ToM and reasoning evaluations, paving the way for improved artificial agents.

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: The paper proposes a 'Refutations and Critiques' (R&C) Track at ML conferences to systematically correct errors and flawed research, enhancing the field's self-correcting capabilities.


<details>
  <summary>Details</summary>
Motivation: Rapid advancements in ML research have led to flawed or misleading studies being accepted due to peer review fallibility, necessitating a mechanism for correction.

Method: The paper suggests designing an R&C Track with specific review principles, addressing potential pitfalls, and provides an example submission.

Result: The proposed R&C Track would offer a reputable platform to critique and correct prior research, fostering a self-correcting ecosystem.

Conclusion: ML conferences should implement official mechanisms like the R&C Track to improve research integrity and self-correction.

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [25] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: The paper introduces STIMULUS, a novel multi-objective optimization algorithm, and its enhanced versions (STIMULUS-M, STIMULUS+, STIMULUS-M+), offering improved convergence rates and sample complexity compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing MOO methods often suffer from slow convergence and high sample complexity, prompting the need for a more efficient algorithm.

Method: STIMULUS uses a recursive framework for stochastic gradient updates, while STIMULUS-M adds momentum. Adaptive batching is introduced in STIMULUS+/STIMULUS-M+ to reduce full gradient evaluations.

Result: The algorithms achieve $O(1/T)$ (non-convex) and $O(\exp{-\mu T})$ (strongly convex) convergence rates, with state-of-the-art sample complexities.

Conclusion: STIMULUS and its variants provide robust, efficient solutions for MOO problems, outperforming traditional methods in both theory and practice.

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [26] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: FlightKooba is a new framework combining HIPPO, Koopman theory, and state space equations to improve flight trajectory prediction by reducing training time and enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: Current Koopman-based models for flight trajectory prediction are ineffective, lack interpretability, and are computationally intensive.

Method: FlightKooba constructs Koopman operators directly from data using structural state space equations, reducing trainable parameters and training time.

Result: FlightKooba achieves faster training (comparable to Mamba without CUDA), reduces memory usage by over 50%, and cuts parameters by tenfold.

Conclusion: FlightKooba offers an efficient and interpretable solution for Koopman operator computation, advancing time series forecasting and control.

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [27] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: The paper proposes an intelligent framework combining adaptive keyframe extraction and causal-aware RL to optimize QoE in multi-user VR, outperforming benchmarks in latency, fairness, and QoE.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook causal relationships between bandwidth, CPU frequency, and user perception, limiting QoE improvements in VR interactions.

Method: The framework integrates adaptive keyframe extraction with causal-aware RL (PS-CDDPG), using a novel QoE metric and MIP modeling for optimization.

Result: Experiments show reduced latency, improved QoE, and maintained fairness, outperforming benchmarks.

Conclusion: The proposed framework effectively balances latency, fidelity, and fairness, enhancing QoE in multi-user VR.

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [28] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: A novel class-aware soft pruning framework using orthogonal convolutional kernel regularization achieves rapid and precise machine unlearning with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between unlearning speed and predictive accuracy in machine unlearning, ensuring compliance with privacy regulations like GDPR.

Method: Leverages orthogonal convolutional kernel regularization and activation difference analysis to decorrelate filters and identify class-specific channels.

Result: Achieves millisecond-level unlearning, complete forgetting of targeted classes, and minimal accuracy loss on retained data, outperforming baselines.

Conclusion: Provides an efficient, practical solution for real-time machine unlearning in MLaaS scenarios.

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [29] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: DeKA-g, a distillation-based knowledge alignment algorithm, improves GSC systems by enhancing edge-cloud knowledge alignment and adapting to diverse wireless conditions, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: The surge in AI-generated content (AIGC) strains networks when provisioning to edges and mobile users. GSC offers a solution but faces challenges in knowledge alignment between cloud-GAI and edges/users, and between transmission knowledge and actual channels.

Method: DeKA-g distills cloud-GAI knowledge into low-rank matrices for edge use and adapts transmission knowledge to channel conditions. It includes MAKD for efficient distillation and VGSA for adapting to compression rates and SNR ranges.

Result: DeKA-g improves edge-cloud image alignment by 44%, adapts to compression rates 116% more efficiently, and enhances low-SNR performance by 28%.

Conclusion: DeKA-g effectively addresses knowledge alignment challenges in GSC, offering a scalable and efficient solution for provisioning AIGC to edges and mobile users.

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [30] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: The paper uses deep neural networks (DNN) and explainable AI (XAI) methods like SHAP and Gradient to forecast and analyze electricity market prices, enhancing understanding of market dynamics.


<details>
  <summary>Details</summary>
Motivation: Electricity markets are complex, and traditional econometric methods lack the power of DNNs. The goal is to improve understanding of price drivers using XAI.

Method: The study employs DNNs for price forecasting and XAI techniques (SHAP, Gradient, visual heatmaps) to analyze feature contributions across five markets. Introduces SSHAP values and lines for better representation.

Result: The approach provides insights into price dynamics and feature contributions in electricity markets.

Conclusion: Combining DNNs with XAI enhances understanding of electricity market behavior, with SSHAP concepts improving interpretability.

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [31] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: A novel post-hoc framework measures neural network uncertainty using retrieved training cases with similar activation vectors, introducing Decision Change and Layer Uncertainty metrics to improve uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Neural networks can produce wrong solutions in high-risk domains, necessitating better uncertainty measurement to detect and mitigate errors.

Method: The framework retrieves training cases with similar activation vectors per layer and introduces two metrics: Decision Change and Layer Uncertainty.

Result: Evaluation on CIFAR-10 and MNIST shows the metrics enhance uncertainty estimation, outperforming softmax-based confidence.

Conclusion: The proposed framework and metrics improve uncertainty estimation in neural networks, particularly for challenging tasks.

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [32] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: The study investigates reinforcement learning (RL), specifically Deep Q-Networks (DQNs), for bearing fault classification, showing comparable performance to traditional methods but better adaptability.


<details>
  <summary>Details</summary>
Motivation: Bearing faults cause operational disruptions and high costs; current methods lack adaptability in dynamic environments.

Method: Uses Deep Q-Networks (DQNs) for bearing fault classification in machine condition monitoring.

Result: RL models match traditional methods in performance but excel in adaptability with optimized rewards, though they are computationally demanding.

Conclusion: RL has potential to complement traditional methods, enabling adaptive diagnostic frameworks.

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [33] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: The paper compares autoregressive (AR) and masked diffusion models (MDMs) in a decoder-only framework, highlighting trade-offs and refining the AO-AR objective.


<details>
  <summary>Details</summary>
Motivation: To fairly compare AR and MDM paradigms by decoupling architectural influences from core modeling differences.

Method: Evaluates MDMs in a decoder-only setup, comparing them as Any-Order AR (AO-AR) and standard AR, and investigates architectural impacts.

Result: Decoder-only MDMs achieve significant speedups (~25x) and comparable perplexity, despite modeling a larger space.

Conclusion: The study clarifies paradigm differences and architectural influences, aiding future model design.

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [34] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: The paper introduces a method to evaluate the importance of feature groups in GAMs, addressing the oversight of joint signals in interpretable ML. It demonstrates the method's efficiency and applicability in high-dimensional settings through synthetic and real-world case studies.


<details>
  <summary>Details</summary>
Motivation: Traditional feature importance methods often neglect the combined effect of feature groups, which can be critical for accurate insights, especially in datasets with natural groupings like multimodal data.

Method: A novel approach to determine feature group importance in GAMs without retraining, allowing posthoc group definition and overlapping groups. It parallels explained variation in statistics.

Result: The method is validated through synthetic experiments and applied to real-world datasets, showing that group importance provides a more accurate and holistic view than single-feature analysis.

Conclusion: Analyzing feature groups enhances interpretability and accuracy in machine learning, particularly for datasets with inherent groupings, as demonstrated in medical case studies.

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [35] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: HERCULES is a hierarchical k-means clustering algorithm integrating LLMs for interpretable cluster summaries, supporting diverse data types and interactive visualization.


<details>
  <summary>Details</summary>
Motivation: The need for advanced tools to group complex datasets and provide human-understandable insights drives the development of HERCULES.

Method: Recursive k-means clustering with LLM-generated titles/descriptions, using direct or description-based embeddings. Supports topic-guided summaries.

Result: Enhanced interpretability of hierarchical clusters through semantically rich summaries and interactive visualization.

Conclusion: HERCULES effectively extracts hierarchical knowledge from complex datasets, offering both clustering and interpretability.

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [36] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: TRACED improves zero-shot generalization in UED by combining transition prediction error and co-learnability for adaptive curriculum design, requiring fewer interactions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing deep reinforcement learning agents to unseen environments by refining regret approximation in UED.

Method: Introduces transition prediction error and co-learnability to approximate regret, forming TRACED for adaptive task generation.

Result: TRACED improves generalization and reduces required environment interactions by up to 2x compared to baselines.

Conclusion: Refined regret approximation and explicit task relationship modeling enhance sample-efficient curriculum design in UED.

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [37] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: RF-SNN architecture uses resonate-and-fire neurons for efficient spectral feature extraction in wireless sensing, reducing energy use while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional LIF neurons fail to capture rich spectral features in streaming signals, necessitating costly pre-processing. RF neurons offer a more efficient solution.

Method: Proposes a wireless split computing architecture with RF neurons to process time-domain signals directly, avoiding spectral pre-processing. Evaluated on audio and modulation classification tasks.

Result: RF-SNN matches LIF-SNN and ANN accuracy but with lower spike rates and energy consumption during inference and communication.

Conclusion: RF-SNN is a viable, energy-efficient alternative for real-time time-series processing in edge applications.

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [38] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: A novel deep unfolding-based approach for Quantum Federated Learning (QFL) autonomously optimizes hyperparameters to address client heterogeneity, achieving ~90% accuracy, outperforming traditional methods (~55%).


<details>
  <summary>Details</summary>
Motivation: Client heterogeneity in QFL leads to performance challenges, requiring dynamic adaptation to mitigate overfitting and ensure robust optimization.

Method: Leverages deep unfolding for autonomous hyperparameter optimization (e.g., learning rates, regularization) based on client-specific training behavior.

Result: Achieves ~90% accuracy on IBM quantum hardware and Qiskit Aer simulators, significantly surpassing traditional methods (~55%).

Conclusion: The framework enhances QFL applicability in critical domains like healthcare and genomics by addressing heterogeneity and improving optimization.

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [39] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM is a preprocessing framework for training robust time series imputation models, outperforming traditional methods by handling real-world missing data patterns efficiently.


<details>
  <summary>Details</summary>
Motivation: Real-world datasets often have complex, heterogeneous missing data patterns, unlike artificially masked training data, necessitating a more robust solution.

Method: DIM-SUM combines pattern clustering and adaptive masking strategies with theoretical learning guarantees to address diverse missing patterns.

Result: DIM-SUM outperforms traditional methods, achieving similar accuracy with less training data and lower processing time, and 2x higher accuracy than pre-trained models with faster inference.

Conclusion: DIM-SUM effectively bridges the gap between artificial and real-world missing data patterns, offering a scalable and efficient solution for time series imputation.

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [40] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: ERDM unifies rolling forecasts with Elucidated Diffusion Models, improving probabilistic forecasting in chaotic systems by addressing temporal dependencies and uncertainty growth.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models struggle with temporal dependencies and uncertainty growth in high-dimensional chaotic systems, motivating the need for a unified framework.

Method: ERDM adapts EDM components (noise schedule, network preconditioning, Heun sampler) for rolling forecasts, introducing a novel loss weighting scheme, efficient initialization, and hybrid sequence architecture.

Result: ERDM outperforms baselines on 2D Navier-Stokes simulations and ERA5 weather forecasting, demonstrating superior performance in modeling escalating uncertainty.

Conclusion: ERDM provides a flexible, powerful framework for diffusion-based sequence generation, particularly where uncertainty modeling is critical.

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [41] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: Loss weighting remains effective in last layer retraining (LLR) for models between underparameterized and overparameterized extremes, but weights must account for model overparameterization.


<details>
  <summary>Details</summary>
Motivation: Addressing biases in machine learning models, especially in the intermediate regime of LLR where data is inseparable and model size is proportionate.

Method: Theoretical and practical exploration of loss weighting in LLR, considering model overparameterization.

Result: Loss weighting is effective in LLR, but requires adjusting for relative overparameterization.

Conclusion: Properly weighted loss functions can mitigate biases in intermediate model regimes, bridging gaps between underparameterized and overparameterized extremes.

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [42] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: The paper introduces a framework for generating diverse courses of action (COAs) for multi-agent operations, using a graph-based approach and genetic algorithm to optimize task allocation and sequencing.


<details>
  <summary>Details</summary>
Motivation: Automated planning for multi-agent missions is needed due to environmental changes and varying agent capabilities, requiring diverse COAs for adaptability.

Method: A graph abstraction of tasks and COAs is used, with a genetic algorithm for task allocation and a graph neural network for task sequencing.

Result: The framework outperforms random baselines, shows small optimality gaps, and plans 20 COAs for 5 agents/100 tasks in ~50 minutes.

Conclusion: The approach effectively generates diverse and adaptable COAs, suitable for dynamic multi-agent operations.

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [43] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: A framework using zk-SNARKs verifies data unlearning on edge-device models without privacy loss, ensuring minimal overhead and preserved personalization.


<details>
  <summary>Details</summary>
Motivation: Addressing issues like copyright, biases, or regulations requiring verifiable data removal across edge devices.

Method: Leverages zero-knowledge proofs (zk-SNARKs) for verification, with algorithms designed for efficient unlearning and minimal overhead.

Result: Practical and effective verifiable unlearning with minimal performance degradation post-unlearning.

Conclusion: Ensures privacy-preserving, verifiable, and effective machine unlearning on edge devices.

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [44] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: The paper introduces CLVQVAE, a framework using vector quantization to map and collapse redundant features in transformer layers into interpretable concept vectors, addressing challenges in analyzing cross-layer superposition.


<details>
  <summary>Details</summary>
Motivation: Current methods analyze neural representations at single layers, missing cross-layer superposition and redundancy, and rely on limited predefined concepts.

Method: Proposes CLVQVAE with top-k temperature-based sampling, EMA codebook updates, and scaled-spherical k-means++ for initialization to cluster by directional similarity.

Result: The framework collapses duplicated residual-stream features into compact, interpretable concept vectors.

Conclusion: CLVQVAE provides a controlled exploration of latent space while maintaining codebook diversity, better aligning with semantic structure.

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [45] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: The paper introduces LSH-DynED, a novel method combining Locality Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) and Dynamic Ensemble Diversification (DynED) to address multi-class imbalanced data streams, outperforming 15 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of dynamic imbalance ratios in multi-class imbalanced data streams, with limited existing solutions, motivates the development of a robust and resilient approach.

Method: LSH-RHP is integrated into DynED for undersampling majority classes, creating balanced training sets and enhancing ensemble prediction.

Result: LSH-DynED outperforms 15 methods on 23 real-world and 10 semi-synthetic datasets, excelling in Kappa and mG-Mean metrics, especially in large-scale, high-dimensional scenarios.

Conclusion: LSH-DynED is effective for multi-class imbalanced non-stationary data streams, offering adaptability and robustness, with potential for future research.

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [46] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Main category: cs.LG

TL;DR: The paper proposes a knowledge distillation-based method to efficiently quantify predictive uncertainty in GNNs for healthcare, outperforming traditional Bayesian and ensemble methods in computational cost and precision.


<details>
  <summary>Details</summary>
Motivation: Quantifying predictive uncertainty in GNNs is crucial for trustworthiness in clinical settings, but existing methods like Bayesian and ensemble approaches are computationally expensive and lack diversity capture.

Method: The authors use self-distillation (same network as teacher and student) and develop a weighted uncertainty metric to capture model diversity, avoiding the need for multiple independent networks.

Result: Experiments on MIMIC-IV and Enzymes datasets show the method effectively captures uncertainty, matches MC Dropout and ensemble performance, and distinguishes out-of-distribution data.

Conclusion: The proposed method offers a computationally efficient and precise way to quantify GNN uncertainty, enhancing trustworthiness in healthcare applications.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [47] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: The paper explores using randomly generated data for model pre-training, supported by theoretical insights from algorithmic complexity and empirical validation of zero-shot learning and scalability.


<details>
  <summary>Details</summary>
Motivation: To justify and validate the use of synthetic data for pre-training models, leveraging theoretical foundations and extending practical applications to real-world data.

Method: Theoretical analysis of algorithmic complexity, empirical pre-training with synthetic data, and finetuning on real-world datasets.

Result: Models pre-trained on synthetic data exhibit zero-shot learning, scalability, and improved convergence and generalization after finetuning.

Conclusion: Synthetic data pre-training is theoretically sound and practically effective, enhancing model performance and adaptability.

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [48] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: Proposes using LLMs to relabel unsuccessful trajectories in reinforcement learning, improving sample efficiency and policy performance without heavy human annotation.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in instruction-following RL, such as reliance on human-labeled data and sparse rewards, by leveraging LLMs for automated instruction generation.

Method: Uses LLMs to retrospectively generate open-ended instructions from agent trajectories, relabeling unsuccessful ones to identify meaningful subtasks and enrich training data.

Result: Demonstrates improved sample efficiency, instruction coverage, and policy performance in the Craftax environment compared to baselines.

Conclusion: LLM-guided open-ended instruction relabeling effectively enhances instruction-following RL, reducing dependency on human annotations.

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [49] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: The paper introduces a supervised coupled matrix-tensor factorization (SCMTF) method to integrate patient-reported outcomes (PROs) and lab data for predicting medication persistence in ulcerative colitis, achieving high AUC scores and interpretable phenotypes.


<details>
  <summary>Details</summary>
Motivation: Patient-reported symptoms in ulcerative colitis are often noisy and sparse, leading to their exclusion in phenotyping. This paper aims to leverage PROs using a novel tensor-based method to improve prediction and understanding of disease progression.

Method: The proposed SCMTF method integrates temporal PROs and lab data with static features in a deep learning framework, handling missing data effectively.

Result: The model predicts medication changes 8 and 20 months ahead with AUCs of 0.853 and 0.803, respectively, and identifies interpretable phenotypes.

Conclusion: The study demonstrates the successful application of tensor-based phenotyping to UC and PROs, revealing their relevance in predicting medication persistence.

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [50] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Main category: cs.LG

TL;DR: A review comparing regression- and classification-based predictive maintenance (PdM) methods, highlighting their roles in prognostics, challenges, and future trends.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comparative studies between regression and classification in PdM, aiming to guide researchers and practitioners in method selection.

Method: Analyzes literature on PdM, focusing on regression (RUL estimation) and classification (failure probability) methods, including challenges like data imbalance.

Result: Identifies advancements, challenges, and trends (e.g., hybrid approaches, AI-enabled systems) in PdM methodologies.

Conclusion: Provides insights into method strengths and compromises, suggesting future work on practical tools and datasets to advance PdM research.

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [51] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Main category: cs.LG

TL;DR: Proposes Multi-Level Ensemble Learning (MEL) for resilient edge inference, balancing accuracy and fault tolerance with lightweight models.


<details>
  <summary>Details</summary>
Motivation: Edge environments are resource-constrained and prone to failures; existing resilience methods compromise latency or accuracy.

Method: MEL trains multiple lightweight backup models collaboratively, formulated as a multi-objective optimization problem to ensure diversity and standalone performance.

Result: MEL achieves performance comparable to original models, with 95.6% accuracy retention during failures, using 40% of the original model size.

Conclusion: MEL offers a robust solution for edge inference, combining fault tolerance, accuracy, and deployment flexibility.

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [52] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Main category: cs.LG

TL;DR: The paper proposes using a pretrained multimodal earth-observation model to create large-scale LFMC maps, improving accuracy by 20% over previous methods, with applications in wildfire monitoring.


<details>
  <summary>Details</summary>
Motivation: Wildfires are intensifying, and ground-based LFMC sampling is costly and sparse. AI and satellite data offer a scalable solution for real-time monitoring.

Method: A pretrained, multimodal earth-observation model is used to generate spatially complete LFMC maps, outperforming randomly initialized models.

Result: Achieves a 20% reduction in RMSE compared to previous methods, demonstrated in wildfire-impacted regions (Eaton and Palisades).

Conclusion: The automated pipeline enables rapid, accurate LFMC mapping, aiding wildfire research and response.

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [53] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Main category: cs.LG

TL;DR: The paper proposes a method called Partition of Variables (PoV) for causal discovery in LTI-DAE systems, improving upon a 2022 method by Kathari and Tangirala by handling systems with algebraic relations.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DIPCA fail for dynamical systems with algebraic relations or feedback control, necessitating a more robust approach.

Method: PoV uses DIPCA to identify algebraic and dynamical relations, then partitions variables via constraint matrix analysis to find causal drivers.

Result: PoV effectively identifies causal drivers in LTI-DAE systems, outperforming the 2022 method.

Conclusion: The PoV method is superior for causal discovery in mixed causal systems, validated through case studies.

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [54] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: A framework for discovering causal structure in PDEs using physics-informed neural networks and counterfactual perturbations, outperforming traditional methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address limitations of classical methods like residual minimization or sparse regression in identifying causal structure in PDEs by introducing a principled, interpretable approach.

Method: Uses physics-informed neural networks and counterfactual perturbations, introducing causal sensitivity indices and structural deviation metrics to assess operator influence.

Result: Theoretically proves exact recovery of causal operator support under certain conditions and empirically validates the framework on synthetic and real-world datasets, showing superior performance.

Conclusion: The framework makes causal PDE discovery tractable and interpretable, grounded in structural causal models and variational residual analysis.

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [55] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Main category: cs.LG

TL;DR: DuoGPT combines unstructured weight pruning and activation sparsity to reduce LLM deployment costs, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: High memory and compute costs of LLMs hinder deployment; activation sparsity is often overlooked in pruning methods.

Method: DuoGPT integrates activation-aware calibration and output residuals from dense models, optimizing for GPU execution.

Result: DuoGPT achieves up to 9.17% higher accuracy than structured pruning methods at a 1.39x speedup.

Conclusion: DuoGPT effectively balances accuracy and efficiency, scaling well for billion-parameter LLMs.

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [56] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Main category: cs.LG

TL;DR: The paper introduces Anubis, a zero-shot tool for attributing code generated by LLMs using hypothesis testing and density estimates, achieving high accuracy with minimal samples.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of attributing code generated by LLMs due to the curse of dimensionality, leveraging available samples and density estimates.

Method: Proposes Anubis, framing attribution as a distribution testing problem using hypothesis testing and LLM density estimates.

Result: Anubis achieves AUROC scores ≥0.9 in distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and Stable-Code with ≈2000 samples.

Conclusion: Anubis is an effective tool for zero-shot attribution of LLM-generated code, demonstrating high accuracy with limited data.

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [57] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Main category: cs.LG

TL;DR: The paper introduces the Affective Priming Score (APS) to detect priming effects in physiological data, reducing misclassification in affective computing models.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored impact of priming on physiological data, which can lead to model misclassifications.

Method: Proposes APS to score data points for priming effects, validated on SEED and SEED-VII datasets by comparing original and priming-free data.

Result: Misclassification rates drop significantly when using priming-free sequences, proving APS effectiveness.

Conclusion: APS mitigates priming effects at the data level, improving model robustness and dataset design in affective computing.

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [58] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Main category: cs.LG

TL;DR: A novel GNN framework combining feature embedding and community information improves directed link prediction, outperforming state-of-the-art methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Link prediction in directed graphs is a key problem, and existing deep learning methods rely on node similarities and graph convolutions. This work aims to enhance performance by integrating community information.

Method: Proposes a GNN framework fusing feature embedding with community information and transforms input graphs into directed line graphs for richer information aggregation.

Result: Outperforms state-of-the-art methods on benchmark datasets with varying training data proportions (30%-60%).

Conclusion: The hybrid feature approach and directed line graph transformation significantly improve directed link prediction performance.

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [59] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: FedBKD is a novel data-free distillation framework for federated learning, addressing non-IID data challenges by using GANs for synthetic data and bidirectional knowledge distillation to improve both global and local models.


<details>
  <summary>Details</summary>
Motivation: To solve the issues of non-IID data in federated learning without relying on public datasets, which risk data leakage, while improving both global and local model performances.

Method: Proposes FedBKD, using GANs to generate synthetic data (with local models as frozen discriminators) and bidirectional distillation between global and local models for knowledge transfer.

Result: FedBKD achieves state-of-the-art performance on 4 benchmarks under various non-IID settings.

Conclusion: FedBKD effectively addresses non-IID challenges in FL, enhancing both global and local models without compromising data privacy.

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [60] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.LG

TL;DR: The paper evaluates safety risks in quantized LLMs and introduces Q-resafe, a framework to restore safety without compromising utility.


<details>
  <summary>Details</summary>
Motivation: Quantization of LLMs for resource-constrained environments may reduce safety capabilities, necessitating systematic evaluation and mitigation.

Method: Comprehensive safety evaluations across quantization techniques and calibration datasets, followed by the Q-resafe framework for safety restoration.

Result: Q-resafe effectively re-aligns safety in quantized LLMs with pre-quantization levels, even in challenging scenarios.

Conclusion: The study highlights the importance of safety in quantized LLMs and presents Q-resafe as a viable solution.

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [61] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Main category: cs.LG

TL;DR: The paper evaluates data-driven methods for generating synthetic long-term energy consumption data, comparing WGAN, DDPM, HMM, and MABF for accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Long-term forecasting of individual power consumption is understudied, and high-fidelity synthetic data is needed for energy system applications.

Method: Comparative evaluation of WGAN, DDPM, HMM, and MABF to replicate temporal dynamics and dependencies in energy consumption profiles.

Result: The study highlights strengths and limitations of each method, aiding in selecting the best approach for energy-related tasks.

Conclusion: The framework enhances synthetic data accuracy and privacy, using open-source German household data for practical applications.

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [62] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Main category: cs.LG

TL;DR: The paper addresses the challenge of providing robust counterfactual explanations (CEs) under model multiplicity (MM) in machine learning, proposing recourse-aware ensembling (RAE) and an argumentative ensembling method to ensure CE robustness.


<details>
  <summary>Details</summary>
Motivation: Model multiplicity complicates recourse recommendations as CEs may not be valid across all models, necessitating a robust solution.

Method: Introduces RAE and a novel argumentative ensembling method using computational argumentation to resolve conflicts between models and CEs, with customizable preferences.

Result: Theoretical analysis and empirical evaluations show the method's effectiveness in satisfying six desirable properties for RAE.

Conclusion: Argumentative ensembling provides a robust and customizable solution for recourse under MM, validated by theoretical and empirical results.

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [63] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Main category: cs.LG

TL;DR: The paper introduces a novel Federated Learning (FL) framework that distills a universal expert model from clustered FL to capture shared knowledge across non-IID data, improving performance and flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing Clustered Federated Learning (CFL) methods overlook shared information across clusters, limiting generalizable knowledge.

Method: The framework involves three iterative steps: local model training, cluster-specific aggregation, and universal expert distillation.

Result: The proposed method outperforms traditional gradient-based aggregation, handling model heterogeneity better and reducing conflicts.

Conclusion: The framework advances CFL by effectively balancing personalized and shared knowledge, demonstrated by superior experimental results.

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [64] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: This paper explores learning functions of medium input-sensitivity, using Transformers for QR code decoding beyond theoretical limits, generalizing across languages and ignoring error-correction bits.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between input-sensitivity and learning hardness, focusing on medium-sensitivity tasks like QR code decoding.

Method: Employ Transformers to decode QR codes, testing generalization across languages and random strings, and analyzing attention to data vs. error-correction bits.

Result: Transformers successfully decode QR codes beyond error-correction limits, generalize to unseen languages/strings, and focus on data bits over error-correction.

Conclusion: Transformers can learn medium-sensitivity tasks like QR decoding uniquely, suggesting potential for broader applications in structured data tasks.

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [65] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Main category: cs.LG

TL;DR: ILDE is a novel imitation learning algorithm that uses double exploration (optimistic policy optimization and curiosity-driven exploration) to improve sample efficiency and achieve beyond-expert performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in accurately learning expert policies from limited demonstrations and the need for exploration to surpass expert performance.

Method: ILDE combines optimistic policy optimization (rewarding uncertain state-action pairs) and curiosity-driven exploration (exploring states outside demonstrations).

Result: ILDE outperforms state-of-the-art methods in sample efficiency and achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations.

Conclusion: ILDE is theoretically justified as an uncertainty-regularized policy optimization method with sublinear regret growth.

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [66] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: AI-driven crop disease detection system using deep learning models (EfficientNet, ResNet101, MobileNetV2, custom CNN) achieves 95.76% validation accuracy, aiding rural farmers.


<details>
  <summary>Details</summary>
Motivation: To assist farmers in rural areas with limited resources by improving crop disease detection and management.

Method: Comparative analysis of deep learning models (EfficientNet, ResNet101, MobileNetV2, custom CNN) for transfer learning efficacy.

Result: Custom CNN achieved 95.76% validation accuracy, demonstrating effective disease classification.

Conclusion: Transfer learning can reshape agricultural practices, enhance crop health management, and support sustainable farming in rural areas.

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [67] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: The paper introduces Permutation Equivariant Neural Graph CDEs, a more efficient variant of Graph Neural CDEs, reducing parameters while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of dynamic graphs by improving the efficiency and generalisation of Graph Neural CDEs.

Method: Projects Graph Neural CDEs onto permutation equivariant function spaces, reducing parameters without losing representational power.

Result: Improved performance in interpolation and extrapolation tasks on simulated and real-world datasets.

Conclusion: The proposed method enhances efficiency and generalisation of Graph Neural CDEs, validated by empirical results.

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [68] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: The paper addresses fairness in sequential bundle recommendations, focusing on producer-fairness to ensure balanced exposure for item groups while maintaining bundle quality. It proposes exact and heuristic solutions, including adaptive balancing, and validates them on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios require fair exposure for different item groups in sequential bundle recommendations, ensuring both fairness and high-quality bundles.

Method: The study formalizes producer-fairness, proposes an exact solution for small instances, and examines two heuristics (quality-first and fairness-first) with an adaptive variant for dynamic balance.

Result: Experiments on three datasets show the effectiveness of the solutions in achieving fair recommendations without sacrificing bundle quality.

Conclusion: The proposed methods successfully balance fairness and quality in sequential bundle recommendations, with adaptive variants offering flexible trade-offs.

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [69] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Main category: cs.LG

TL;DR: The paper proposes a new approach to Granger Causality (GC) using deep learning, focusing on prediction and model uncertainty to uncover causal structures without explicit variable selection.


<details>
  <summary>Details</summary>
Motivation: Traditional linear VAR models for GC are limited in capturing complex associations, and existing DNN-based methods treat GC as variable selection, which may not fully leverage deep learning's potential.

Method: The authors suggest modeling time series jointly with a well-regularized deep learning model and comparing residuals or model uncertainty when specific components are dropped. Input layer dropout effects are also analyzed.

Result: A well-regularized deep learning model can learn the true GC structure from data without explicit variable selection terms in the loss function.

Conclusion: The proposed paradigm shifts GC estimation from variable selection to prediction-based learning, demonstrating the potential of deep learning for uncovering causal relationships.

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [70] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Main category: cs.LG

TL;DR: DipSVD enhances SVD-based LLM compression by protecting critical matrix components locally and globally, outperforming existing methods at high compression ratios.


<details>
  <summary>Details</summary>
Motivation: Existing SVD-based compression methods neglect critical matrix components, leading to inferior performance.

Method: Proposes dual-level importance protection: local (channel-weighted whitening) and global (heuristic/optimization-based layer compression).

Result: DipSVD outperforms existing SVD methods across benchmarks, especially at high compression ratios.

Conclusion: DipSVD effectively preserves critical components, improving compressed model performance.

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [71] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: The paper introduces MVPA, a self-attention mechanism for heterogeneous time-series data, and MVPFormer, a generative model for iEEG, achieving expert-level seizure detection and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning from multi-variate time-series with varying channel configurations, especially in clinical iEEG data.

Method: Proposes MVPA, disentangling content, temporal, and spatial attention, and builds MVPFormer, a generative model trained on the SWEC iEEG dataset.

Result: MVPFormer generalizes well across subjects, excels in seizure detection, and outperforms state-of-the-art baselines. MVPA also performs competitively on standard tasks.

Conclusion: MVPA is a versatile attention mechanism, and MVPFormer is a pioneering open-source iEEG model with top-tier clinical performance.

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [72] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Main category: cs.LG

TL;DR: The paper introduces a taxonomy-based feature selection method for trajectory analysis to address high-dimensionality issues, improving efficiency, interpretability, and predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: High-dimensionality in trajectory data reduces model efficiency and interpretability, necessitating effective feature selection methods.

Method: A taxonomy-based approach categorizes features into geometric (curvature, indentation) and kinematic (speed, acceleration) groups, reducing combinatorial space.

Result: The method achieved comparable or superior predictive performance, reduced feature selection time, and provided insights into dataset sensitivity.

Conclusion: Taxonomy-based feature selection enhances interpretability, reduces complexity, and supports high-level decision-making in trajectory analysis and explainable AI.

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [73] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: LaplaceGNN is a self-supervised graph learning framework that avoids negative sampling by using spectral bootstrapping and Laplacian-based signals, achieving efficient and scalable performance.


<details>
  <summary>Details</summary>
Motivation: To simplify and improve self-supervised graph learning by eliminating the need for negative sampling and handcrafted augmentations.

Method: Integrates Laplacian-based signals and spectral bootstrapping, using max-min centrality-guided optimization for spectral augmentations and adversarial bootstrapped training.

Result: Outperforms state-of-the-art self-supervised graph methods on benchmark datasets.

Conclusion: LaplaceGNN provides an efficient, scalable, and robust alternative for learning expressive graph representations.

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [74] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Main category: cs.LG

TL;DR: TESSERA is a novel Remote Sensing Foundation Model (RSFM) using Self-Supervised Learning (SSL) to create global representations from satellite data, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: To enable diverse Earth observation applications like climate modeling and conservation by generating robust, high-resolution representations from satellite data.

Method: Uses two Transformer-based encoders for Sentinel-1 SAR and Sentinel-2 MSI data, fused via MLP to create global representations (2017-2024).

Result: Sets a new state-of-the-art benchmark, outperforming task-specific and other foundation models in five diverse tasks.

Conclusion: TESSERA democratizes access to high-performance RS representations and excels in downstream applications.

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [75] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Main category: cs.LG

TL;DR: P4 is a decentralized method for personalized learning in IoT, ensuring privacy and robustness against attacks, outperforming existing approaches in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The need for efficient, private, and robust personalized learning in decentralized IoT ecosystems drives the development of P4.

Method: P4 uses a lightweight decentralized algorithm for client similarity detection and collaborative group formation, employing differentially private knowledge distillation for co-training.

Result: P4 achieves 5%-30% higher accuracy than peers, remains robust with up to 30% malicious clients, and adds minimal overhead (~7 seconds) in deployment.

Conclusion: P4 effectively addresses challenges in decentralized personalized learning, offering privacy, robustness, and practicality for IoT devices.

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [76] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Main category: cs.LG

TL;DR: The paper introduces a novel estimator (OPFV) for future off-policy evaluation and learning in non-stationary environments, leveraging time-series structures to reduce bias and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of estimating and optimizing policy values in non-stationary environments (e.g., e-commerce recommendations) where future data is absent in historical logs.

Method: Proposes OPFV, an estimator using time-series structures (e.g., seasonal effects) via a new importance weighting technique, and extends it to a policy-gradient method for proactive learning.

Result: OPFV outperforms existing methods in estimating and optimizing future policy values under non-stationarity, validated by empirical results.

Conclusion: The OPFV estimator effectively addresses non-stationarity by leveraging time-related structures, offering a robust solution for future policy evaluation and learning.

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [77] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: Proposes KDIA, a federated learning method using knowledge distillation and inequitable aggregation to address client heterogeneity and partial participation challenges, improving accuracy with fewer training rounds.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning methods struggle with client label skew, data quantity skew, and partial participation in large-scale settings, degrading model performance.

Method: KDIA combines knowledge distillation with teacher-student inequitable aggregation, leveraging participation intervals, counts, and data volume for weighted aggregation. Self-knowledge distillation and server-generated IID data aid local training.

Result: KDIA outperforms on CIFAR-10/100/CINIC-10 datasets, achieving higher accuracy with fewer rounds, especially under severe heterogeneity.

Conclusion: KDIA effectively addresses federated learning challenges in heterogeneous and partial participation settings, offering improved performance and efficiency.

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [78] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Main category: cs.LG

TL;DR: An algorithm for selecting the optimal number of demonstrations in ICL for tabular data, integrating data distribution, prompt template, and LLM, validated against random selection methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining the ideal number of demonstrations in ICL for tabular data classification.

Method: Uses Spectral Graph Theory to quantify demonstration similarities, constructs a similarity graph, and analyzes Laplacian eigenvalues to derive the minimum required demonstrations.

Result: Validated through experiments showing improved performance over random selection on diverse datasets and LLMs.

Conclusion: The proposed method effectively automates demonstration selection, enhancing ICL for tabular data.

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [79] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Main category: cs.LG

TL;DR: A new quadrature method for selecting collocation points in PINNs, leveraging the Hessian of the function to improve training.


<details>
  <summary>Details</summary>
Motivation: Improve the efficiency of PINNs by refining the selection of collocation points during training.

Method: Proposes a quadrature method using the Hessian of the function to guide collocation point selection.

Result: Enhanced training process for PINNs by adaptive sampling based on function curvature.

Conclusion: The method improves PINN training by optimizing collocation point selection through Hessian-based quadrature.

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [80] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: The paper explores memorization in machine learning models beyond self-influence, emphasizing the role of near-duplicates and the full influence distribution of training samples.


<details>
  <summary>Details</summary>
Motivation: To address privacy and generalization concerns by understanding how memorization is influenced by interactions among training samples, not just self-influence.

Method: Analyzes the full influence distribution of training samples on each other, using a small language model and extending to image classification (CIFAR-10).

Result: Self-influence alone underestimates memorization risks; near-duplicates reduce self-influence but remain extractable. Influence distributions reveal near-duplicates in datasets.

Conclusion: Memorization is complex and better understood through the full influence distribution, not just self-influence, highlighting the need for broader analysis.

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [81] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Main category: cs.LG

TL;DR: SIDED dataset and AMDA method improve NILM for industrial energy disaggregation, reducing error from 0.451 to 0.093.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and privacy issues in industrial NILM with synthetic datasets and better augmentation.

Method: Created SIDED dataset using Digital Twin simulations and proposed AMDA for data augmentation.

Result: AMDA-augmented models achieved a Normalized Disaggregation Error of 0.093, outperforming others.

Conclusion: SIDED and AMDA enhance NILM model generalization for industrial energy disaggregation.

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [82] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: Multi-modal learning combines data from various sources (images, text, audio) to enhance AI understanding, using techniques like representation learning, alignment, and fusion. Challenges include data diversity and adversarial attacks, with ongoing research focusing on unsupervised learning and better benchmarks. Future applications span vision, NLP, and healthcare.


<details>
  <summary>Details</summary>
Motivation: To improve AI systems' ability to interpret and reason by leveraging multiple data modalities, addressing real-world complexity.

Method: Uses representation learning, alignment methods, and fusion strategies via deep learning models. Explores unsupervised/semi-supervised learning and AutoML.

Result: Enhanced AI performance in interpretation, reasoning, and decision-making, though challenges like data diversity persist.

Conclusion: Multi-modal learning holds promise for advancing AI in diverse fields, aiming for human-like understanding and adaptability.

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [83] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: The paper proposes a method to optimize local batch sizes in Federated Learning using a greedy randomized search, improving convergence speed without compromising performance.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in local training processes due to improper hardware configurations in Federated Learning, where participants share hardware but lack information exchange.

Method: A greedy randomized search is employed to optimize local batch sizes, leveraging the parallel processing nature of Federated Learning.

Result: The method improves convergence speed compared to default settings, performing nearly as well as individually optimized local parameters.

Conclusion: Optimizing local batch sizes via greedy randomized search enhances Federated Learning efficiency without requiring extensive parameter tuning.

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [84] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: A novel framework for Federated Learning (FL) introduces client-specific tokens and decentralized finance (DeFi) to improve reward distribution and enable third-party investment.


<details>
  <summary>Details</summary>
Motivation: Existing FL incentive schemes lack flexible and scalable reward distribution methods, and third-party investment mechanisms are understudied.

Method: Proposes a framework using client-specific tokens, DeFi platforms, and automated market makers (AMMs) for reward distribution and investment.

Result: The framework aims to enhance flexibility, scalability, and inclusivity in FL reward systems.

Conclusion: The proposed solution addresses gaps in FL incentive schemes by integrating DeFi and AMMs for better reward distribution and investment opportunities.

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [85] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: The paper analyzes an off-policy REINFORCE algorithm for aligning large language models, showing that adjusting the baseline V impacts performance, with theoretical and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between off-policy RL and supervised fine-tuning for aligning LLMs, addressing suboptimal performance of off-policy methods.

Method: Study a simple off-policy REINFORCE algorithm with a tunable baseline V, analyzing its policy improvement guarantees and experimental validation.

Result: Theoretical analysis shows policy improvement when V lower-bounds expected reward; experiments confirm focusing on positive rewards benefits off-policy updates.

Conclusion: Off-policy REINFORCE with a tunable baseline V is effective for aligning LLMs, especially when emphasizing positive rewards.

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [86] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Main category: cs.LG

TL;DR: The paper introduces FEA-PINN, a hybrid framework combining Physics-Informed Neural Networks (PINN) with corrective FEA simulations to efficiently predict thermal fields in LPBF, reducing computational costs while maintaining FEA-level accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods like FEA are computationally expensive for simulating LPBF processes, necessitating a more efficient solution.

Method: The FEA-PINN framework integrates PINN with dynamic material updating and corrective FEA simulations to address residual accumulation and ensure physical consistency.

Result: FEA-PINN matches FEA accuracy with significantly lower computational costs, validated using benchmark data and single-track LPBF scanning.

Conclusion: FEA-PINN offers an efficient and accurate alternative to traditional FEA for LPBF thermal field prediction, with potential for broader applications.

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [87] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Main category: cs.LG

TL;DR: The paper explores reinforcement learning for optimal routing in skill-based queueing systems, demonstrating its adaptability and superiority over static policies, with added heuristics for delay reduction and multi-objective optimization.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient and adaptive control in complex queueing systems like data centers and cloud networks, leveraging reinforcement learning for practical implementation.

Method: Uses a real-world case study to test a reinforcement learning algorithm, introduces a heuristic for delay reduction, and incorporates multi-objective optimization with parameter tuning.

Result: The algorithm outperforms static policies, adapts to dynamic environments, and successfully balances multiple objectives like payoff maximization, server fairness, and reduced waiting times.

Conclusion: The study highlights the algorithm's real-world applicability, robustness to estimation errors, and potential for live deployment in complex queueing systems.

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [88] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Main category: cs.LG

TL;DR: The paper explores transformer-based approaches, particularly the iTransformer, for anomaly detection in multivariate time series, analyzing parameters, label extraction, training impacts, and model comparisons.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in multivariate time series is crucial but challenging due to unknown anomalies and complex interdependencies.

Method: Investigates iTransformer for anomaly detection, focusing on parameters, label extraction, training impacts, and loss functions.

Result: Comprehensive comparison of transformer models on diverse datasets, assessing performance and robustness.

Conclusion: Transformer-based methods, especially iTransformer, show promise for effective anomaly detection in multivariate time series.

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [89] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Main category: cs.LG

TL;DR: The paper explores the effectiveness of graph-transformers (GTs) and hybrid GT-MPNN backbones for out-of-distribution (OOD) generalization in graph neural networks, showing they outperform traditional MPNNs. It also introduces a novel post-training analysis method for evaluating generalization.


<details>
  <summary>Details</summary>
Motivation: Current graph neural network methods assume training and testing data share the same distribution, which is unrealistic. The study aims to evaluate GT and hybrid backbones under OOD settings to address this gap.

Method: The authors systematically evaluate GT and hybrid backbones in OOD settings, adapt domain generalization algorithms for GTs, and propose a post-training analysis method to examine domain alignment and class separation.

Result: GT and hybrid GT-MPNN backbones consistently outperform MPNNs in OOD generalization, even without specialized domain generalization algorithms. The post-training analysis provides deeper insights into generalization abilities.

Conclusion: The findings highlight the potential of graph-transformers for robust real-world graph learning and suggest a new direction for OOD generalization research.

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [90] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: The paper introduces the Support Vector Graph (SVG), a machine learning-based graph index for vector search in metric and non-metric spaces, with formal navigability guarantees. It generalizes popular indices like HNSW and DiskANN and proposes SVG-L0 for bounded out-degree.


<details>
  <summary>Details</summary>
Motivation: Existing graph indices for vector search rely on Euclidean space assumptions. The work aims to extend these principles to metric and non-metric spaces using machine learning.

Method: SVG leverages kernel methods for graph connectivity, with SVG-L0 adding an ℓ0 sparsity constraint for bounded out-degree, avoiding heuristic truncation.

Result: SVG provides formal navigability guarantees in diverse spaces, and SVG-L0 offers self-tuning and computational efficiency.

Conclusion: The work generalizes and improves graph indices for vector search, enabling broader applicability and principled solutions to practical constraints.

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [91] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Main category: cs.LG

TL;DR: H-FEX, a symbolic learning method, accurately learns Hamiltonian functions from data, preserving energy conservation in complex systems.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven methods struggle to capture complex Hamiltonian functions while conserving energy.

Method: Proposed H-FEX uses novel interaction nodes in symbolic learning to effectively model intricate Hamiltonian interactions.

Result: H-FEX successfully recovers Hamiltonian functions, accurately capturing dynamics and conserving energy in stiff systems.

Conclusion: H-FEX is a promising framework for discovering closed-form expressions of complex dynamical systems.

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [92] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Main category: cs.LG

TL;DR: Closed-loop learning in exponential family models converges to biased absorbing states, but mitigation strategies like data pollution, MAP estimation, or regularization can prevent this.


<details>
  <summary>Details</summary>
Motivation: The rise of neural networks training on self-generated data raises concerns about amplifying biases, motivating study of closed-loop learning dynamics.

Method: Analyze dynamics of exponential family models under maximum likelihood estimation, exploring parameter convergence and bias amplification.

Result: Closed-loop learning converges to biased absorbing states, but strategies like data pollution, MAP, or regularization can prevent this.

Conclusion: Mitigation techniques are essential to avoid bias amplification in closed-loop learning, and dynamics are not reparametrisation invariant.

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [93] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: PLoP (Precise LoRA Placement) is a lightweight method for automatically identifying optimal LoRA adapter placements in pretrained models, outperforming manual strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA adapter placement strategies are inconsistent (attention vs. MLP modules), lacking a systematic approach. PLoP addresses this gap.

Method: PLoP uses intuitive theoretical analysis to determine optimal adapter placement for given pretrained models and tasks.

Result: PLoP consistently outperforms or matches manual placement strategies in supervised finetuning and reinforcement learning tasks.

Conclusion: PLoP provides an efficient, automated solution for LoRA adapter placement, enhancing model adaptation performance.

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [94] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: FedEDS is a federated learning scheme for edge devices that uses encrypted data sharing to improve convergence speed and address data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Current federated learning approaches neglect network topology, physical distance, and data heterogeneity, causing latency and performance issues.

Method: FedEDS trains a data encryptor using client models and stochastic layers, enabling encrypted data sharing among clients for local model training.

Result: FedEDS accelerates convergence and mitigates data heterogeneity, enhancing model performance.

Conclusion: FedEDS is effective for edge devices needing rapid convergence and improved privacy protection.

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [95] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: The paper introduces novel surrogate loss functions and algorithms for learning to defer with multiple experts, addressing consistency challenges in single-stage and two-stage scenarios.


<details>
  <summary>Details</summary>
Motivation: The challenge is to optimally assign input instances to experts, balancing accuracy and computational cost, which is critical in fields like natural language generation, image processing, and medical diagnostics.

Method: The paper proposes new surrogate loss functions and efficient algorithms, with theoretical guarantees for realizable $H$-consistency, $H$-consistency bounds, and Bayes-consistency in single-stage and two-stage deferral learning.

Result: Theoretical guarantees are provided, and experiments show the proposed surrogate losses outperform existing baselines.

Conclusion: The work advances the field by addressing open questions in deferral learning, offering strong theoretical and empirical results.

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [96] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: The paper analyzes malicious gradient leakage attacks in federated learning, highlighting their practical limitations and proposing a lightweight client-side detection mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the risk of sensitive data exposure in federated learning due to malicious server manipulation of gradient updates.

Method: Comprehensive analysis of attack techniques, trade-offs in effectiveness and stealth, and development of a client-side detection mechanism.

Result: Attacks are limited in practice and detectable; a simple detection method is proposed.

Conclusion: Malicious gradient leakage attacks are less practical than feared, and defenses are feasible with minimal overhead.

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [97] [Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.20039)
*Koorosh Moslemi,Chi-Guhn Lee*

Main category: cs.MA

TL;DR: A framework for learning two-sided team formation in dynamic multi-agent systems is introduced, addressing gaps in existing MARL studies.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on unilateral groupings or fixed settings, leaving bilateral team formation in dynamic populations underexplored.

Method: A framework for learning two-sided team formation in dynamic multi-agent systems is proposed.

Result: The approach shows competitive performance and improved generalization in widely adopted multi-agent scenarios.

Conclusion: The study provides insights into algorithmic properties influencing policy performance and generalization in bilateral team formation.

Abstract: Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.

</details>


### [98] [A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem](https://arxiv.org/abs/2506.20400)
*Kristoffer Christensen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.MA

TL;DR: A Python-based dashboard framework for analyzing multi-agent simulations of EV home charging, enabling root-cause analysis of system-level anomalies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting and explaining unexpected events in complex EV charging simulations.

Method: Developed a modular dashboard using Dash by Plotly, featuring three coordinated views for multi-level exploration.

Result: Demonstrated effectiveness in identifying anomalies like transformer overloads in a Danish residential network case study.

Conclusion: The framework provides actionable insights and is adaptable to other energy systems.

Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging
ecosystems generate large, complex, and stochastic time-series datasets that
capture interactions between households, grid infrastructure, and energy
markets. These interactions can lead to unexpected system-level events, such as
transformer overloads or consumer dissatisfaction, that are difficult to detect
and explain through static post-processing. This paper presents a modular,
Python-based dashboard framework, built using Dash by Plotly, that enables
efficient, multi-level exploration and root-cause analysis of emergent behavior
in MABS outputs. The system features three coordinated views (System Overview,
System Analysis, and Consumer Analysis), each offering high-resolution
visualizations such as time-series plots, spatial heatmaps, and agent-specific
drill-down tools. A case study simulating full EV adoption with smart charging
in a Danish residential network demonstrates how the dashboard supports rapid
identification and contextual explanation of anomalies, including clustered
transformer overloads and time-dependent charging failures. The framework
facilitates actionable insight generation for researchers and distribution
system operators, and its architecture is adaptable to other distributed energy
resources and complex energy systems.

</details>
