<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 87]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study](https://arxiv.org/abs/2506.15207)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: The paper explores Reinforcement Learning (RL) and Multi-Agent RL (MARL) for autonomous Earth Observation (EO) mission planning in multi-satellite systems, addressing challenges like energy limits and coordination.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LEO satellites requires efficient autonomous coordination for EO missions, which traditional methods fail to handle in real-time.

Method: The study models single-satellite operations and extends to multi-satellite constellations using MARL, evaluating algorithms like PPO, IPPO, MAPPO, and HAPPO in a realistic simulation.

Result: MARL effectively balances imaging and resource management, handling non-stationarity and reward interdependency in multi-satellite coordination.

Conclusion: The findings support autonomous satellite operations, providing guidelines for improving policy learning in decentralised EO missions.

Abstract: The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised
Earth Observation (EO) missions, addressing challenges in climate monitoring,
disaster management, and more. However, autonomous coordination in
multi-satellite systems remains a fundamental challenge. Traditional
optimisation approaches struggle to handle the real-time decision-making
demands of dynamic EO missions, necessitating the use of Reinforcement Learning
(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we
investigate RL-based autonomous EO mission planning by modelling
single-satellite operations and extending to multi-satellite constellations
using MARL frameworks. We address key challenges, including energy and data
storage limitations, uncertainties in satellite observations, and the
complexities of decentralised coordination under partial observability. By
leveraging a near-realistic satellite simulation environment, we evaluate the
training stability and performance of state-of-the-art MARL algorithms,
including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can
effectively balance imaging and resource management while addressing
non-stationarity and reward interdependency in multi-satellite coordination.
The insights gained from this study provide a foundation for autonomous
satellite operations, offering practical guidelines for improving policy
learning in decentralised EO missions.

</details>


### [2] [SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence](https://arxiv.org/abs/2506.15672)
*Yao Zhang,Chenyang Lin,Shijie Tang,Haokun Chen,Shijie Zhou,Yunpu Ma,Volker Tresp*

Main category: cs.AI

TL;DR: SwarmAgentic is a fully automated framework for generating and optimizing agentic systems, outperforming baselines by 261.8% on the TravelPlanner benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing agentic systems lack full autonomy, limiting adaptability and scalability. SwarmAgentic addresses this by enabling from-scratch agent generation and self-optimization.

Method: SwarmAgentic uses language-driven exploration and feedback-guided updates inspired by Particle Swarm Optimization (PSO) to evolve candidate systems.

Result: Outperforms baselines, achieving a +261.8% improvement on the TravelPlanner benchmark.

Conclusion: SwarmAgentic advances scalable and autonomous agentic system design by integrating swarm intelligence with automated multi-agent generation.

Abstract: The rapid progress of Large Language Models has advanced agentic systems in
decision-making, coordination, and task execution. Yet, existing agentic system
generation frameworks lack full autonomy, missing from-scratch agent
generation, self-optimizing agent functionality, and collaboration, limiting
adaptability and scalability. We propose SwarmAgentic, a framework for fully
automated agentic system generation that constructs agentic systems from
scratch and jointly optimizes agent functionality and collaboration as
interdependent components through language-driven exploration. To enable
efficient search over system-level structures, SwarmAgentic maintains a
population of candidate systems and evolves them via feedback-guided updates,
drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our
method on six real-world, open-ended, and exploratory tasks involving
high-level planning, system-level coordination, and creative reasoning. Given
only a task description and an objective function, SwarmAgentic outperforms all
baselines, achieving a +261.8% relative improvement over ADAS on the
TravelPlanner benchmark, highlighting the effectiveness of full automation in
structurally unconstrained tasks. This framework marks a significant step
toward scalable and autonomous agentic system design, bridging swarm
intelligence with fully automated system multi-agent generation. Our code is
publicly released at https://yaoz720.github.io/SwarmAgentic/.

</details>


### [3] [CALM: Contextual Analog Logic with Multimodality](https://arxiv.org/abs/2506.14936)
*Maxwell J. Jacobson,Corey J. Maley,Yexiang Xue*

Main category: cs.AI

TL;DR: CALM integrates symbolic reasoning with neural generation for context-sensitive decisions in multi-modal data, outperforming classical logic and LLMs.


<details>
  <summary>Details</summary>
Motivation: Classic logic systems lack nuance in human decision-making and require rigid grounding, while neural networks lack interpretable reasoning structures. CALM bridges this gap.

Method: CALM uses domain trees for predicates, refining analog truth values with neural networks and symbolic reasoning to ensure constraints.

Result: CALM achieved 92.2% accuracy in object placement tasks, surpassing classical logic (86.3%) and LLMs (59.4%), and aligned with human preferences.

Conclusion: CALM combines logic precision with neural multi-modal processing, paving the way for next-gen AI systems.

Abstract: In this work, we introduce Contextual Analog Logic with Multimodality (CALM).
CALM unites symbolic reasoning with neural generation, enabling systems to make
context-sensitive decisions grounded in real-world multi-modal data.
  Background: Classic bivalent logic systems cannot capture the nuance of human
decision-making. They also require human grounding in multi-modal environments,
which can be ad-hoc, rigid, and brittle. Neural networks are good at extracting
rich contextual information from multi-modal data, but lack interpretable
structures for reasoning.
  Objectives: CALM aims to bridge the gap between logic and neural perception,
creating an analog logic that can reason over multi-modal inputs. Without this
integration, AI systems remain either brittle or unstructured, unable to
generalize robustly to real-world tasks. In CALM, symbolic predicates evaluate
to analog truth values computed by neural networks and constrained search.
  Methods: CALM represents each predicate using a domain tree, which
iteratively refines its analog truth value when the contextual groundings of
its entities are determined. The iterative refinement is predicted by neural
networks capable of capturing multi-modal information and is filtered through a
symbolic reasoning module to ensure constraint satisfaction.
  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%
accuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It
also demonstrated spatial heatmap generation aligned with logical constraints
and delicate human preferences, as shown by a human study.
  Conclusions: CALM demonstrates the potential to reason with logic structure
while aligning with preferences in multi-modal environments. It lays the
foundation for next-gen AI systems that require the precision and
interpretation of logic and the multimodal information processing of neural
networks.

</details>


### [4] [MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.14990)
*Tristan Tomilin,Luka van den Boogaard,Samuel Garcin,Bram Grooten,Meng Fang,Mykola Pechenizkiy*

Main category: cs.AI

TL;DR: MEAL is the first benchmark for continual multi-agent reinforcement learning (CMARL), addressing the gap in CL for cooperative multi-agent settings by leveraging GPU acceleration for scalability.


<details>
  <summary>Details</summary>
Motivation: The lack of benchmarks for continual learning in cooperative multi-agent settings limits research progress in CMARL.

Method: MEAL uses JAX for GPU acceleration, enabling efficient learning across 100 tasks on a desktop PC.

Result: Naive combinations of CL and MARL methods perform well in simple environments but fail in complex settings requiring coordination and adaptation.

Conclusion: MEAL identifies key architectural and algorithmic features critical for effective CMARL, providing a scalable benchmark for future research.

Abstract: Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms, with environment availability strongly
impacting research. One particularly underexplored intersection is continual
learning (CL) in cooperative multi-agent settings. To remedy this, we introduce
MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark
tailored for continual multi-agent reinforcement learning (CMARL). Existing CL
benchmarks run environments on the CPU, leading to computational bottlenecks
and limiting the length of task sequences. MEAL leverages JAX for GPU
acceleration, enabling continual learning across sequences of 100 tasks on a
standard desktop PC in a few hours. We show that naively combining popular CL
and MARL methods yields strong performance on simple environments, but fails to
scale to more complex settings requiring sustained coordination and adaptation.
Our ablation study identifies architectural and algorithmic features critical
for CMARL on MEAL.

</details>


### [5] [Truncated Proximal Policy Optimization](https://arxiv.org/abs/2506.15050)
*Tiantian Fan,Lingjun Liu,Yu Yue,Jiaze Chen,Chengyi Wang,Qiying Yu,Chi Zhang,Zhiqi Lin,Ruofei Zhu,Yufeng Yuan,Xiaochen Zuo,Bole Ma,Mofan Zhang,Gaohong Liu,Ru Zhang,Haotian Zhou,Cong Xie,Ruidong Zhu,Zhi Zhang,Xin Liu,Mingxuan Wang,Lin Yan,Yonghui Wu*

Main category: cs.AI

TL;DR: T-PPO, a novel extension of PPO, enhances training efficiency for reasoning LLMs by optimizing policy updates and response generation, achieving up to 2.5x faster training without performance loss.


<details>
  <summary>Details</summary>
Motivation: PPO's inefficiency in training reasoning LLMs due to long response times and low hardware utilization motivates the development of T-PPO.

Method: T-PPO introduces Extended Generalized Advantage Estimation (EGAE) and a mechanism for independent optimization of policy and value models, reducing redundant computations.

Result: T-PPO improves training efficiency by up to 2.5x and outperforms competitors on the AIME 2024 benchmark with a 32B model.

Conclusion: T-PPO effectively addresses PPO's inefficiencies, offering a scalable solution for training reasoning LLMs.

Abstract: Recently, test-time scaling Large Language Models (LLMs) have demonstrated
exceptional reasoning capabilities across scientific and professional tasks by
generating long chains-of-thought (CoT). As a crucial component for developing
these reasoning models, reinforcement learning (RL), exemplified by Proximal
Policy Optimization (PPO) and its variants, allows models to learn through
trial and error. However, PPO can be time-consuming due to its inherent
on-policy nature, which is further exacerbated by increasing response lengths.
In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a
novel extension to PPO that improves training efficiency by streamlining policy
update and length-restricted response generation. T-PPO mitigates the issue of
low hardware utilization, an inherent drawback of fully synchronized
long-generation procedures, where resources often sit idle during the waiting
periods for complete rollouts. Our contributions are two-folds. First, we
propose Extended Generalized Advantage Estimation (EGAE) for advantage
estimation derived from incomplete responses while maintaining the integrity of
policy learning. Second, we devise a computationally optimized mechanism that
allows for the independent optimization of the policy and value models. By
selectively filtering prompt and truncated tokens, this mechanism reduces
redundant computations and accelerates the training process without sacrificing
convergence performance. We demonstrate the effectiveness and efficacy of T-PPO
on AIME 2024 with a 32B base model. The experimental results show that T-PPO
improves the training efficiency of reasoning LLMs by up to 2.5x and
outperforms its existing competitors.

</details>


### [6] [HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges](https://arxiv.org/abs/2506.15196)
*Xianliang Yang,Ling Zhang,Haolong Qian,Lei Song,Jiang Bian*

Main category: cs.AI

TL;DR: HeurAgenix is a two-stage hyper-heuristic framework using LLMs to evolve and select heuristics for combinatorial optimization, outperforming existing methods and matching specialized solvers.


<details>
  <summary>Details</summary>
Motivation: Traditional heuristic designs for combinatorial optimization rely on manual expertise and lack generalization. HeurAgenix aims to automate and improve heuristic evolution and selection.

Method: HeurAgenix uses LLMs to evolve heuristics by comparing seed and high-quality solutions, then dynamically selects the best heuristic for each problem state. It includes a dual-reward mechanism for fine-tuning.

Result: HeurAgenix outperforms existing LLM-based hyper-heuristics and matches or exceeds specialized solvers in benchmarks.

Conclusion: HeurAgenix demonstrates the potential of LLMs in automating heuristic design for combinatorial optimization, offering flexibility and robustness.

Abstract: Heuristic algorithms play a vital role in solving combinatorial optimization
(CO) problems, yet traditional designs depend heavily on manual expertise and
struggle to generalize across diverse instances. We introduce
\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large
language models (LLMs) that first evolves heuristics and then selects among
them automatically. In the heuristic evolution phase, HeurAgenix leverages an
LLM to compare seed heuristic solutions with higher-quality solutions and
extract reusable evolution strategies. During problem solving, it dynamically
picks the most promising heuristic for each problem state, guided by the LLM's
perception ability. For flexibility, this selector can be either a
state-of-the-art LLM or a fine-tuned lightweight model with lower inference
cost. To mitigate the scarcity of reliable supervision caused by CO complexity,
we fine-tune the lightweight heuristic selector with a dual-reward mechanism
that jointly exploits singals from selection preferences and state perception,
enabling robust selection under noisy annotations. Extensive experiments on
canonical benchmarks show that HeurAgenix not only outperforms existing
LLM-based hyper-heuristics but also matches or exceeds specialized solvers.
Code is available at https://github.com/microsoft/HeurAgenix.

</details>


### [7] [Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels](https://arxiv.org/abs/2506.15225)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.AI

TL;DR: The paper proposes a cooperative MEC framework using UAVs and vessels for efficient computation offloading and resource allocation in MIoT, addressing uncertain tasks with Lyapunov optimization and a heterogeneous-agent soft actor-critic method.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of MIoT demands efficient computation offloading and resource allocation, but uncertain maritime tasks pose challenges.

Method: A cooperative MEC framework is introduced, leveraging Lyapunov optimization for uncertain tasks and reformulating the problem into a Markov game solved by a heterogeneous-agent soft actor-critic.

Result: Simulations confirm the framework's effectiveness in minimizing execution time and handling uncertain MIoT tasks.

Conclusion: The proposed approach successfully addresses computational offloading and resource allocation in uncertain maritime environments.

Abstract: The computation demands from the maritime Internet of Things (MIoT) increase
rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels
based multi-access edge computing (MEC) can fulfill these MIoT requirements.
However, the uncertain maritime tasks present significant challenges of
inefficient computation offloading and resource allocation. In this paper, we
focus on the maritime computation offloading and resource allocation through
the cooperation of UAVs and vessels, with consideration of uncertain tasks.
Specifically, we propose a cooperative MEC framework for computation offloading
and resource allocation, including MIoT devices, UAVs and vessels. Then, we
formulate the optimization problem to minimize the total execution time. As for
the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the
unpredictable task arrivals and varying computational resource availability. By
converting the long-term constraints into short-term constraints, we obtain a
set of small-scale optimization problems. Further, considering the
heterogeneity of actions and resources of UAVs and vessels, we reformulate the
small-scale optimization problem into a Markov game (MG). Moreover, a
heterogeneous-agent soft actor-critic is proposed to sequentially update
various neural networks and effectively solve the MG problem. Finally,
simulations are conducted to verify the effectiveness in addressing
computational offloading and resource allocation.

</details>


### [8] [Efficient and Generalizable Environmental Understanding for Visual Navigation](https://arxiv.org/abs/2506.15377)
*Ruoyu Wang,Xinshu Li,Chen Wang,Lina Yao*

Main category: cs.AI

TL;DR: The paper introduces Causality-Aware Navigation (CAN), a method that improves visual navigation by modeling causal relationships in sequential data, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing navigation methods process historical observations simultaneously, ignoring internal associations, which limits performance. The paper addresses this by analyzing navigation tasks causally.

Method: Proposes CAN with a Causal Understanding Module to enhance environmental understanding, leveraging causality to improve sequential data modeling.

Result: CAN consistently outperforms baselines in various tasks and environments, with gains attributed to the Causal Understanding Module.

Conclusion: The Causal Understanding Module generalizes well in Reinforcement and Supervised Learning settings without added computational cost, demonstrating the value of causal modeling in navigation.

Abstract: Visual Navigation is a core task in Embodied AI, enabling agents to navigate
complex environments toward given objectives. Across diverse settings within
Navigation tasks, many necessitate the modelling of sequential data accumulated
from preceding time steps. While existing methods perform well, they typically
process all historical observations simultaneously, overlooking the internal
association structure within the data, which may limit the potential for
further improvements in task performance. We address this by examining the
unique characteristics of Navigation tasks through the lens of causality,
introducing a causal framework to highlight the limitations of conventional
sequential methods. Leveraging this insight, we propose Causality-Aware
Navigation (CAN), which incorporates a Causal Understanding Module to enhance
the agent's environmental understanding capability. Empirical evaluations show
that our approach consistently outperforms baselines across various tasks and
simulation environments. Extensive ablations studies attribute these gains to
the Causal Understanding Module, which generalizes effectively in both
Reinforcement and Supervised Learning settings without computational overhead.

</details>


### [9] [Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents](https://arxiv.org/abs/2506.15567)
*Aline Dobrovsky,Konstantin Schekotihin,Christian Burmer*

Main category: cs.AI

TL;DR: The paper proposes an LLM-based Planning Agent (LPA) to streamline Failure Analysis (FA) tasks by automating workflows and integrating AI components.


<details>
  <summary>Details</summary>
Motivation: FA is complex and knowledge-intensive; AI integration can automate tasks but requires efficient orchestration.

Method: Design and implement an LPA combining LLMs with planning and external tools for autonomous query processing and data retrieval.

Result: The LPA proves effective and reliable in supporting FA tasks.

Conclusion: The LPA successfully enhances FA workflows by integrating AI components seamlessly.

Abstract: Failure Analysis (FA) is a highly intricate and knowledge-intensive process.
The integration of AI components within the computational infrastructure of FA
labs has the potential to automate a variety of tasks, including the detection
of non-conformities in images, the retrieval of analogous cases from diverse
data sources, and the generation of reports from annotated images. However, as
the number of deployed AI models increases, the challenge lies in orchestrating
these components into cohesive and efficient workflows that seamlessly
integrate with the FA process.
  This paper investigates the design and implementation of a Large Language
Model (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their
analysis cases. The LPA integrates LLMs with advanced planning capabilities and
external tool utilization, enabling autonomous processing of complex queries,
retrieval of relevant data from external systems, and generation of
human-readable responses. Evaluation results demonstrate the agent's
operational effectiveness and reliability in supporting FA tasks.

</details>


### [10] [The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games](https://arxiv.org/abs/2506.15624)
*Lyle Goodyear,Rachel Guo,Ramesh Johari*

Main category: cs.AI

TL;DR: A framework for constructing natural language state representations for LLM agents in repeated games, improving comparability and behavior alignment with game theory.


<details>
  <summary>Details</summary>
Motivation: Address the ad hoc and inconsistent approaches to encoding game history in LLM agents, which hinder comparability and obscure the impact of state representation on behavior.

Method: Propose a framework categorizing state representations along three axes: action informativeness, reward informativeness, and prompting style. Tested in a dynamic selfish routing game.

Result: Summarized history, regret-based rewards, and limited action info lead to behavior closer to game theory equilibrium and stable play. Other representations cause deviations or instability.

Conclusion: State representation significantly impacts LLM agent behavior; structured frameworks enhance comparability and alignment with theoretical predictions.

Abstract: Large Language Models (LLMs) have shown promise as decision-makers in dynamic
settings, but their stateless nature necessitates creating a natural language
representation of history. We present a unifying framework for systematically
constructing natural language "state" representations for prompting LLM agents
in repeated multi-agent games. Previous work on games with LLM agents has taken
an ad hoc approach to encoding game history, which not only obscures the impact
of state representation on agents' behavior, but also limits comparability
between studies. Our framework addresses these gaps by characterizing methods
of state representation along three axes: action informativeness (i.e., the
extent to which the state representation captures actions played); reward
informativeness (i.e., the extent to which the state representation describes
rewards obtained); and prompting style (or natural language compression, i.e.,
the extent to which the full text history is summarized).
  We apply this framework to a dynamic selfish routing game, chosen because it
admits a simple equilibrium both in theory and in human subject experiments
\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find
that there are key dependencies of LLM agent behavior on the natural language
state representation. In particular, we observe that representations which
provide agents with (1) summarized, rather than complete, natural language
representations of past history; (2) information about regrets, rather than raw
payoffs; and (3) limited information about others' actions lead to behavior
that more closely matches game theoretic equilibrium predictions, and with more
stable game play by the agents. By contrast, other representations can exhibit
either large deviations from equilibrium, higher variation in dynamic game play
over time, or both.

</details>


### [11] [The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy](https://arxiv.org/abs/2506.15639)
*James Weichert,Daniel Dunlap,Mohammed Farghally,Hoda Eldardiry*

Main category: cs.AI

TL;DR: The paper introduces an AI Policy Module for CS students to address gaps in AI ethics and policy education, showing improved student awareness and confidence post-module.


<details>
  <summary>Details</summary>
Motivation: Current CS curricula lack preparation for AI practitioners to integrate ethical principles and policy into AI development.

Method: Developed and piloted an AI Policy Module, including a technical assignment on AI regulation, and evaluated student attitudes via surveys.

Result: Students showed increased concern about AI ethics and greater confidence in discussing AI regulation after the module.

Conclusion: The AI Policy Module effectively bridges the gap in AI ethics and policy education, with the AI Regulation Assignment being a key tool.

Abstract: As artificial intelligence (AI) further embeds itself into many settings
across personal and professional contexts, increasing attention must be paid
not only to AI ethics, but also to the governance and regulation of AI
technologies through AI policy. However, the prevailing post-secondary
computing curriculum is currently ill-equipped to prepare future AI
practitioners to confront increasing demands to implement abstract ethical
principles and normative policy preferences into the design and development of
AI systems. We believe that familiarity with the 'AI policy landscape' and the
ability to translate ethical principles to practices will in the future
constitute an important responsibility for even the most technically-focused AI
engineers.
  Toward preparing current computer science (CS) students for these new
expectations, we developed an AI Policy Module to introduce discussions of AI
policy into the CS curriculum. Building on a successful pilot in fall 2024, in
this innovative practice full paper we present an updated and expanded version
of the module, including a technical assignment on "AI regulation". We present
the findings from our pilot of the AI Policy Module 2.0, evaluating student
attitudes towards AI ethics and policy through pre- and post-module surveys.
Following the module, students reported increased concern about the ethical
impacts of AI technologies while also expressing greater confidence in their
abilities to engage in discussions about AI regulation. Finally, we highlight
the AI Regulation Assignment as an effective and engaging tool for exploring
the limits of AI alignment and emphasizing the role of 'policy' in addressing
ethical challenges.

</details>


### [12] [Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement](https://arxiv.org/abs/2506.15647)
*Weixiang Zhao,Jiahe Guo,Yang Deng,Xingyu Sui,Yulin Hu,Yanyan Zhao,Wanxiang Che,Bing Qin,Tat-Seng Chua,Ting Liu*

Main category: cs.AI

TL;DR: The paper addresses the inefficiency of large reasoning models (LRMs) due to overthinking and proposes two methods to enhance efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs often generate verbose and redundant content, increasing inference costs and reducing efficiency. The study aims to uncover and exploit the models' inherent capacity for concise reasoning.

Method: Two approaches are introduced: Efficiency Steering (a training-free activation technique) and Self-Rewarded Efficiency RL (a reinforcement learning framework). Both methods aim to balance brevity and accuracy.

Result: Experiments on seven LRM backbones show significant reduction in reasoning length while maintaining or improving task performance.

Conclusion: The study demonstrates that LRMs' efficiency can be enhanced by leveraging their intrinsic capabilities through self-guided methods.

Abstract: Recent advancements in large reasoning models (LRMs) have significantly
enhanced language models' capabilities in complex problem-solving by emulating
human-like deliberative thinking. However, these models often exhibit
overthinking (i.e., the generation of unnecessarily verbose and redundant
content), which hinders efficiency and inflates inference cost. In this work,
we explore the representational and behavioral origins of this inefficiency,
revealing that LRMs inherently possess the capacity for more concise reasoning.
Empirical analyses show that correct reasoning paths vary significantly in
length, and the shortest correct responses often suffice, indicating untapped
efficiency potential. Exploiting these findings, we propose two lightweight
methods to enhance LRM efficiency. First, we introduce Efficiency Steering, a
training-free activation steering technique that modulates reasoning behavior
via a single direction in the model's representation space. Second, we develop
Self-Rewarded Efficiency RL, a reinforcement learning framework that
dynamically balances task accuracy and brevity by rewarding concise correct
solutions. Extensive experiments on seven LRM backbones across multiple
mathematical reasoning benchmarks demonstrate that our methods significantly
reduce reasoning length while preserving or improving task performance. Our
results highlight that reasoning efficiency can be improved by leveraging and
guiding the intrinsic capabilities of existing models in a self-guided manner.

</details>


### [13] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong,Rui Sun,Bingxuan Li,Xingcheng Yao,Maxine Wu,Alexander Chien,Da Yin,Ying Nian Wu,Zhecan James Wang,Kai-Wei Chang*

Main category: cs.AI

TL;DR: The paper introduces Embodied Web Agents, a paradigm integrating physical and digital intelligence for tasks like cooking and navigation, along with a benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Current AI agents are siloed, limiting their ability to perform tasks requiring integrated physical and digital intelligence.

Method: Developed a unified simulation platform (Embodied Web Agents task environments) and benchmark for tasks like cooking and navigation.

Result: Revealed performance gaps between state-of-the-art AI systems and humans in cross-domain intelligence tasks.

Conclusion: The work highlights challenges and opportunities in merging embodied cognition with web-scale knowledge, with all resources publicly available.

Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [MedSyn: Enhancing Diagnostics with Human-AI Collaboration](https://arxiv.org/abs/2506.14774)
*Burcu Sayin,Ipek Baris Schlicht,Ngoc Vo Hong,Sara Allievi,Jacopo Staiano,Pasquale Minervini,Andrea Passerini*

Main category: cs.LG

TL;DR: MedSyn is a hybrid human-AI framework for clinical decision-making, enabling interactive dialogues between physicians and LLMs to refine diagnoses and treatments.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making is complex and prone to biases; current LLM usage lacks interaction depth.

Method: Proposes MedSyn, a multi-step interactive dialogue system between physicians and open-source LLMs.

Result: Open-source LLMs show promise as physician assistants in simulated interactions.

Conclusion: Future work will validate MedSyn's impact on diagnostic accuracy and patient outcomes in real-world settings.

Abstract: Clinical decision-making is inherently complex, often influenced by cognitive
biases, incomplete information, and case ambiguity. Large Language Models
(LLMs) have shown promise as tools for supporting clinical decision-making, yet
their typical one-shot or limited-interaction usage may overlook the
complexities of real-world medical practice. In this work, we propose a hybrid
human-AI framework, MedSyn, where physicians and LLMs engage in multi-step,
interactive dialogues to refine diagnoses and treatment decisions. Unlike
static decision-support tools, MedSyn enables dynamic exchanges, allowing
physicians to challenge LLM suggestions while the LLM highlights alternative
perspectives. Through simulated physician-LLM interactions, we assess the
potential of open-source LLMs as physician assistants. Results show open-source
LLMs are promising as physician assistants in the real world. Future work will
involve real physician interactions to further validate MedSyn's usefulness in
diagnostic accuracy and patient outcomes.

</details>


### [15] [Two-dimensional Parallel Tempering for Constrained Optimization](https://arxiv.org/abs/2506.14781)
*Corentin Delacour,M Mahmudul Hasan Sajeeb,Joao P. Hespanha,Kerem Y. Camsari*

Main category: cs.LG

TL;DR: The paper introduces a 2D parallel tempering algorithm (2D-PT) to improve sampling in constrained Ising problems, eliminating the need to tune penalty strengths and achieving better mixing.


<details>
  <summary>Details</summary>
Motivation: Sampling Boltzmann distributions is crucial in machine learning and optimization, but practical Ising machine implementations face challenges with soft constraints.

Method: Extends parallel tempering (PT) by adding a second dimension of replicas to interpolate penalty strengths, ensuring constraint satisfaction.

Result: 2D-PT achieves near-ideal mixing (KL divergence decays as O(1/t)) and significant speedup over conventional PT in graph sparsification and Wishart instances.

Conclusion: 2D-PT is broadly applicable to constrained Ising problems and can be deployed on existing Ising machines.

Abstract: Sampling Boltzmann probability distributions plays a key role in machine
learning and optimization, motivating the design of hardware accelerators such
as Ising machines. While the Ising model can in principle encode arbitrary
optimization problems, practical implementations are often hindered by soft
constraints that either slow down mixing when too strong, or fail to enforce
feasibility when too weak. We introduce a two-dimensional extension of the
powerful parallel tempering algorithm (PT) that addresses this challenge by
adding a second dimension of replicas interpolating the penalty strengths. This
scheme ensures constraint satisfaction in the final replicas, analogous to
low-energy states at low temperature. The resulting two-dimensional parallel
tempering algorithm (2D-PT) improves mixing in heavily constrained replicas and
eliminates the need to explicitly tune the penalty strength. In a
representative example of graph sparsification with copy constraints, 2D-PT
achieves near-ideal mixing, with Kullback-Leibler divergence decaying as
O(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of
magnitude speedup over conventional PT with the same number of replicas. The
method applies broadly to constrained Ising problems and can be deployed on
existing Ising machines.

</details>


### [16] [Integrating Dynamical Systems Learning with Foundational Models: A Meta-Evolutionary AI Framework for Clinical Trials](https://arxiv.org/abs/2506.14782)
*Joseph Geraci,Bessi Qorri,Christian Cumbaa,Mike Tsay,Paul Leonczyk,Luca Pani*

Main category: cs.LG

TL;DR: The paper compares two AI approaches: DeepSeek-V3, a large-scale LLM, and NetraAI, a dynamical system-based framework for clinical datasets. NetraAI combines contraction mappings, information geometry, and evolutionary algorithms to identify predictive patient subgroups, achieving high accuracy with few features.


<details>
  <summary>Details</summary>
Motivation: The study aims to bridge the gap between large-scale AI models and interpretable, stable frameworks for clinical data, focusing on uncovering high-effect-size subpopulations in medical research.

Method: NetraAI uses contraction mappings, information geometry, and evolutionary algorithms to embed features in a metric space, iteratively contracting them toward stable attractors. An LLM Strategist acts as a meta-evolutionary layer to guide discovery.

Result: NetraAI transformed weak baseline models into near-perfect classifiers in case studies (schizophrenia, depression, pancreatic cancer) using only a few features.

Conclusion: NetraAI represents a new generation of adaptive, explainable AI for clinical discovery, aligning with concept-level reasoning paradigms like JEPA.

Abstract: Artificial intelligence (AI) has evolved into an ecosystem of specialized
"species," each with unique strengths. We analyze two: DeepSeek-V3, a
671-billion-parameter Mixture of Experts large language model (LLM)
exemplifying scale-driven generality, and NetraAI, a dynamical system-based
framework engineered for stability and interpretability on small clinical trial
datasets. We formalize NetraAI's foundations, combining contraction mappings,
information geometry, and evolutionary algorithms to identify predictive
patient cohorts. Features are embedded in a metric space and iteratively
contracted toward stable attractors that define latent subgroups. A
pseudo-temporal embedding and long-range memory enable exploration of
higher-order feature interactions, while an internal evolutionary loop selects
compact, explainable 2-4-variable bundles ("Personas").
  To guide discovery, we introduce an LLM Strategist as a meta-evolutionary
layer that observes Persona outputs, prioritizes promising variables, injects
domain knowledge, and assesses robustness. This two-tier architecture mirrors
the human scientific process: NetraAI as experimentalist, the LLM as theorist,
forming a self-improving loop.
  In case studies (schizophrenia, depression, pancreatic cancer), NetraAI
uncovered small, high-effect-size subpopulations that transformed weak baseline
models (AUC ~0.50-0.68) into near-perfect classifiers using only a few
features. We position NetraAI at the intersection of dynamical systems,
information geometry, and evolutionary learning, aligned with emerging
concept-level reasoning paradigms such as LeCun's Joint Embedding Predictive
Architecture (JEPA). By prioritizing reliable, explainable knowledge, NetraAI
offers a new generation of adaptive, self-reflective AI to accelerate clinical
discovery.

</details>


### [17] [ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification](https://arxiv.org/abs/2506.14783)
*Mohamed Masry,Mohamed Amen,Mohamed Elzyat,Mohamed Hamed,Norhan Magdy,Maram Khaled*

Main category: cs.LG

TL;DR: The paper proposes ETS, a framework integrating EEG and eye-tracking for open-vocabulary text generation and sentiment classification, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of decoding natural language from EEG in open-vocabulary scenarios, where noise and variability hinder performance.

Method: ETS integrates EEG with synchronized eye-tracking data for text generation and sentiment classification.

Result: Superior performance in BLEU/Rouge scores for EEG-to-text and 10% F1 improvement in sentiment classification.

Conclusion: ETS shows potential for high-performance open-vocabulary EEG-to-text systems across diverse subjects and data sources.

Abstract: Decoding natural language from brain activity using non-invasive
electroencephalography (EEG) remains a significant challenge in neuroscience
and machine learning, particularly for open-vocabulary scenarios where
traditional methods struggle with noise and variability. Previous studies have
achieved high accuracy on small-closed vocabularies, but it still struggles on
open vocabularies. In this study, we propose ETS, a framework that integrates
EEG with synchronized eye-tracking data to address two critical tasks: (1)
open-vocabulary text generation and (2) sentiment classification of perceived
language. Our model achieves a superior performance on BLEU and Rouge score for
EEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment
classification, which significantly outperforms supervised baselines.
Furthermore, we show that our proposed model can handle data from various
subjects and sources, showing great potential for high performance open
vocabulary eeg-to-text system.

</details>


### [18] [Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation](https://arxiv.org/abs/2506.14784)
*Emre Yilmaz,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: A transfer learning method using convolutional neural networks (ConvNet) is proposed to predict onflow parameters like angle of attack and speed, addressing challenges of sensor faults and data distribution changes.


<details>
  <summary>Details</summary>
Motivation: Traditional direct measurements of onflow parameters face challenges like sensor faults. A data-driven approach is needed for real-time learning in applications like wind tunnel monitoring.

Method: The method involves training a ConvNet offline, freezing most weights, and retraining selected layers for transfer learning. It's tested with CFD data for domain and task adaptation.

Result: The approach successfully adapts to changing data distributions, domain extensions, and task updates but is less effective with noisy data.

Conclusion: Transfer learning with ConvNet shows promise for real-time prediction of onflow parameters, though noisy data remains a challenge.

Abstract: Determining onflow parameters is crucial from the perspectives of wind tunnel
testing and regular flight and wind turbine operations. These parameters have
traditionally been predicted via direct measurements which might lead to
challenges in case of sensor faults. Alternatively, a data-driven prediction
model based on surface pressure data can be used to determine these parameters.
It is essential that such predictors achieve close to real-time learning as
dictated by practical applications such as monitoring wind tunnel operations or
learning the variations in aerodynamic performance of aerospace and wind energy
systems. To overcome the challenges caused by changes in the data distribution
as well as in adapting to a new prediction task, we propose a transfer learning
methodology to predict the onflow parameters, specifically angle of attack and
onflow speed. It requires first training a convolutional neural network
(ConvNet) model offline for the core prediction task, then freezing the weights
of this model except the selected layers preceding the output node, and finally
executing transfer learning by retraining these layers. A demonstration of this
approach is provided using steady CFD analysis data for an airfoil for i)
domain adaptation where transfer learning is performed with data from a target
domain having different data distribution than the source domain and ii) task
adaptation where the prediction task is changed. Further exploration on the
influence of noisy data, performance on an extended domain, and trade studies
varying sampling sizes and architectures are provided. Results successfully
demonstrate the potential of the approach for adaptation to changing data
distribution, domain extension, and task update while the application for noisy
data is concluded to be not as effective.

</details>


### [19] [PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series](https://arxiv.org/abs/2506.14786)
*Haobo Li,Eunseo Jung,Zixin Chen,Zhaowei Wang,Yueya Wang,Huamin Qu,Alexis Kai Hon Lau*

Main category: cs.LG

TL;DR: PIPE, a physics-informed positional encoding method, enhances multimodal time series forecasting by embedding physical data into vision language models, improving accuracy by 12% in typhoon intensity forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches neglect visual data in time series, missing critical physical information like geospatial context in satellite imagery.

Method: PIPE introduces physics-informed positional indexing and variant-frequency positional encoding to embed physical and sequential order information.

Result: PIPE achieves state-of-the-art performance, with a 12% improvement in typhoon intensity forecasting over prior methods.

Conclusion: PIPE effectively bridges the gap in utilizing visual data for forecasting, demonstrating superior accuracy and alignment in multimodal tasks.

Abstract: Multimodal time series forecasting is foundational in various fields, such as
utilizing satellite imagery and numerical data for predicting typhoons in
climate science. However, existing multimodal approaches primarily focus on
utilizing text data to help time series forecasting, leaving the visual data in
existing time series datasets untouched. Furthermore, it is challenging for
models to effectively capture the physical information embedded in visual data,
such as satellite imagery's temporal and geospatial context, which extends
beyond images themselves. To address this gap, we propose physics-informed
positional encoding (PIPE), a lightweight method that embeds physical
information into vision language models (VLMs). PIPE introduces two key
innovations: (1) a physics-informed positional indexing scheme for mapping
physics to positional IDs, and (2) a variant-frequency positional encoding
mechanism for encoding frequency information of physical variables and
sequential order of tokens within the embedding space. By preserving both the
physical information and sequential order information, PIPE significantly
improves multimodal alignment and forecasting accuracy. Through the experiments
on the most representative and the largest open-sourced satellite image
dataset, PIPE achieves state-of-the-art performance in both deep learning
forecasting and climate domain methods, demonstrating superiority across
benchmarks, including a 12% improvement in typhoon intensity forecasting over
prior works. Our code is provided in the supplementary material.

</details>


### [20] [Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems](https://arxiv.org/abs/2506.14787)
*Funing Li,Yuan Tian,Ruben Noortwyck,Jifeng Zhou,Liming Kuang,Robert Schulz*

Main category: cs.LG

TL;DR: A deep reinforcement learning framework using GNN and Transformer is proposed to optimize retrieval in multi-deep storage systems with heterogeneous items, minimizing tardiness.


<details>
  <summary>Details</summary>
Motivation: The demand for high-density, efficient storage systems in logistics is unmet due to lane blockage issues in multi-deep AVS/RS, especially with heterogeneous items.

Method: A graph-based state representation and a neural network combining GNN and Transformer are designed to prioritize retrievals and generalize across layouts.

Result: The framework outperforms heuristic methods in minimizing tardiness, validated by extensive experiments.

Conclusion: The proposed approach effectively addresses retrieval challenges in multi-deep systems, enhancing flexibility and performance.

Abstract: In modern industrial and logistics environments, the rapid expansion of fast
delivery services has heightened the demand for storage systems that combine
high efficiency with increased density. Multi-deep autonomous vehicle storage
and retrieval systems (AVS/RS) present a viable solution for achieving greater
storage density. However, these systems encounter significant challenges during
retrieval operations due to lane blockages. A conventional approach to mitigate
this issue involves storing items with homogeneous characteristics in a single
lane, but this strategy restricts the flexibility and adaptability of
multi-deep storage systems.
  In this study, we propose a deep reinforcement learning-based framework to
address the retrieval problem in multi-deep storage systems with heterogeneous
item configurations. Each item is associated with a specific due date, and the
objective is to minimize total tardiness. To effectively capture the system's
topology, we introduce a graph-based state representation that integrates both
item attributes and the local topological structure of the multi-deep
warehouse. To process this representation, we design a novel neural network
architecture that combines a Graph Neural Network (GNN) with a Transformer
model. The GNN encodes topological and item-specific information into
embeddings for all directly accessible items, while the Transformer maps these
embeddings into global priority assignments. The Transformer's strong
generalization capability further allows our approach to be applied to storage
systems with diverse layouts. Extensive numerical experiments, including
comparisons with heuristic methods, demonstrate the superiority of the proposed
neural network architecture and the effectiveness of the trained agent in
optimizing retrieval tardiness.

</details>


### [21] [AZT1D: A Real-World Dataset for Type 1 Diabetes](https://arxiv.org/abs/2506.14789)
*Saman Khamesian,Asiful Arefeen,Bithika M. Thompson,Maria Adela Grando,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: AZT1D is a high-quality dataset for T1D management, addressing the lack of detailed public datasets by providing comprehensive patient data from 25 individuals on AID systems.


<details>
  <summary>Details</summary>
Motivation: The scarcity of detailed, publicly available datasets for T1D management limits progress in personalized therapy and AI applications.

Method: AZT1D includes CGM, insulin pump, carbohydrate intake, and device mode data from 25 T1D patients on AID systems over 6-8 weeks, with granular bolus insulin details.

Result: The dataset offers rich, naturalistic data, enabling diverse AI/ML applications for improving T1D care.

Conclusion: AZT1D fills a critical gap, supporting advancements in personalized T1D management and clinical decision-making.

Abstract: High quality real world datasets are essential for advancing data driven
approaches in type 1 diabetes (T1D) management, including personalized therapy
design, digital twin systems, and glucose prediction models. However, progress
in this area has been limited by the scarcity of publicly available datasets
that offer detailed and comprehensive patient data. To address this gap, we
present AZT1D, a dataset containing data collected from 25 individuals with T1D
on automated insulin delivery (AID) systems. AZT1D includes continuous glucose
monitoring (CGM) data, insulin pump and insulin administration data,
carbohydrate intake, and device mode (regular, sleep, and exercise) obtained
over 6 to 8 weeks for each patient. Notably, the dataset provides granular
details on bolus insulin delivery (i.e., total dose, bolus type, correction
specific amounts) features that are rarely found in existing datasets. By
offering rich, naturalistic data, AZT1D supports a wide range of artificial
intelligence and machine learning applications aimed at improving clinical
decision making and individualized care in T1D.

</details>


### [22] [Continuous Evolution Pool: Taming Recurring Concept Drift in Online Time Series Forecasting](https://arxiv.org/abs/2506.14790)
*Tianxiang Zhan,Ming Jin,Yuanpeng He,Yuxuan Liang,Yong Deng,Shirui Pan*

Main category: cs.LG

TL;DR: The paper proposes the Continuous Evolution Pool (CEP) to address recurring concept drift in time series by storing and retrieving forecasters for different concepts, improving prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Recurring concept drift causes forgetting of previously learned patterns, reducing prediction accuracy. Existing methods update parameters but may lose knowledge and lack retention mechanisms.

Method: CEP stores forecasters for different concepts, retrieves the nearest forecaster to test samples, and evolves new models for emerging concepts while clearing outdated knowledge.

Result: Experiments show CEP effectively retains conceptual knowledge and significantly improves prediction results in recurring concept drift scenarios.

Conclusion: CEP enhances online forecasting by retaining and utilizing knowledge of recurring concepts, outperforming existing methods.

Abstract: Recurring concept drift, a type of concept drift in which previously observed
data patterns reappear after some time, is one of the most prevalent types of
concept drift in time series. As time progresses, concept drift occurs and
previously encountered concepts are forgotten, thereby leading to a decline in
the accuracy of online predictions. Existing solutions employ parameter
updating techniques to delay forgetting; however, this may result in the loss
of some previously learned knowledge while neglecting the exploration of
knowledge retention mechanisms. To retain all conceptual knowledge and fully
utilize it when the concepts recur, we propose the Continuous Evolution Pool
(CEP), a pooling mechanism that stores different instances of forecasters for
different concepts. Our method first selects the forecaster nearest to the test
sample and then learns the features from its neighboring samples - a process we
refer to as the retrieval. If there are insufficient neighboring samples, it
indicates that a new concept has emerged, and a new model will evolve from the
current nearest sample to the pool to store the knowledge of the concept.
Simultaneously, the elimination mechanism will enable outdated knowledge to be
cleared to ensure the prediction effect of the forecasters. Experiments on
different architectural models and eight real datasets demonstrate that CEP
effectively retains the knowledge of different concepts. In the scenario of
online forecasting with recurring concepts, CEP significantly enhances the
prediction results.

</details>


### [23] [Protein Language Model Zero-Shot Fitness Predictions are Improved by Inference-only Dropout](https://arxiv.org/abs/2506.14793)
*Aditya Ravuri,Neil D. Lawrence*

Main category: cs.LG

TL;DR: Adding dropout at inference time improves zero-shot performance of Protein Language Models (PLMs) like ESM2 on ProteinGym tasks, without retraining.


<details>
  <summary>Details</summary>
Motivation: To enhance zero-shot prediction of protein properties (fitness) using PLMs without additional training.

Method: Inject a dropout layer (0.1 rate) between the featurizer and transformer layers during inference, averaging outputs like Monte-Carlo dropout.

Result: Improved zero-shot performance on ProteinGym dataset, even without prior dropout training.

Conclusion: Inference-time dropout is a simple, effective method to boost PLM performance without retraining.

Abstract: Protein Language Models (PLMs) such as ESM2 have been shown to be capable of
zero-shot prediction of critical scalar properties of proteins (fitness). In
this work, we show that injecting a dropout layer at inference time between a
PLM's featurizer/embedding layer and its transformer, and averaging its output
akin to Monte-Carlo dropout increases zero-shot performance on a subset of the
ProteinGym dataset. This is the case even when the model was not trained with
dropouts to begin with, and does not require retraining or finetuning of the
PLM. A dropout of 0.1 seems performant across all models.

</details>


### [24] [Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors](https://arxiv.org/abs/2506.14794)
*Henrik Klagges,Robert Dahlke,Fabian Klemm,Benjamin Merkel,Daniel Klingmann,David A. Reiss,Dan Zecha*

Main category: cs.LG

TL;DR: The paper introduces 'Assembly-of-Experts' (AoE), a method to efficiently create child models from Mixture-of-Experts parents by interpolating weights, reducing computational costs while maintaining functionality.


<details>
  <summary>Details</summary>
Motivation: The high computational cost (10^13-10^15 FLOPs) for pretraining LLMs motivates the need for a more efficient method to leverage existing models.

Method: AoE interpolates individual weight tensors from parent models to create child variants, enabling gradual or sharp trait transitions.

Result: The DeepSeek R1T 'Chimera' model, created without fine-tuning, achieves R1-level intelligence with 40% fewer output tokens and V3-like speed.

Conclusion: AoE is a promising method for efficiently generating functional child models from existing ones, with potential for compact and orderly reasoning.

Abstract: Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM
during pretraining is extremely expensive and seems inefficient. To better
leverage the huge investments made into pretrained models, we develop the new
"Assembly-of-Experts" (AoE) construction method to create capable child
variants of existing Mixture-of-Experts parent models in linear time. Model
weight tensors get interpolated individually, allowing to enhance or suppress
semantic features of the parents.
  Varying the proportion of weights taken from the parent models, we observe
some properties of the AoE child model changing gradually, while other
behavioral traits emerge with a sharp transition. Surprisingly, nearly every
generated model is functional and capable, which makes searching the model
space straightforward.
  We construct the DeepSeek R1T "Chimera", a 671B open-weights hybrid model
combining DeepSeek's V3-0324 and R1 model variants. The child inherits only the
routed expert tensors of R1, but still achieves about R1-level intelligence. At
the same time, it uses about 40\% fewer output tokens, close to V3 speed.
Constructed without any fine-tuning or distillation, the Chimera exhibits
surprisingly compact, orderly reasoning compared to its parent models.

</details>


### [25] [Bound by semanticity: universal laws governing the generalization-identification tradeoff](https://arxiv.org/abs/2506.14797)
*Marco Nurisso,Jesseba Fernando,Raj Deshpande,Alan Perotti,Raja Marjieh,Steven M. Frankland,Richard L. Lewis,Taylor W. Webb,Declan Campbell,Francesco Vaccarino,Jonathan D. Cohen,Giovanni Petri*

Main category: cs.LG

TL;DR: The paper explores the tradeoff between structured and selective internal representations in intelligent systems, deriving universal limits on generalization and identification. It validates these limits in neural networks and complex models.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental constraints on how intelligent systems balance generalization and input identity preservation in their internal representations.

Method: Derives closed-form expressions for generalization and identification probabilities, extends analysis to noisy and heterogeneous spaces, and validates findings in neural networks and complex models.

Result: Reveals a universal Pareto front for the tradeoff, predicts a sharp collapse in multi-input capacity, and confirms these limits persist in complex models.

Conclusion: Finite-resolution similarity is a fundamental constraint, shaping the representational capacity of deep networks and brains.

Abstract: Intelligent systems must deploy internal representations that are
simultaneously structured -- to support broad generalization -- and selective
-- to preserve input identity. We expose a fundamental limit on this tradeoff.
For any model whose representational similarity between inputs decays with
finite semantic resolution $\varepsilon$, we derive closed-form expressions
that pin its probability of correct generalization $p_S$ and identification
$p_I$ to a universal Pareto front independent of input space geometry.
Extending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs
predicts a sharp $1/n$ collapse of multi-input processing capacity and a
non-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end
reproduces these laws: during learning a resolution boundary self-organizes and
empirical $(p_S,p_I)$ trajectories closely follow theoretical curves for
linearly decaying similarity. Finally, we demonstrate that the same limits
persist in two markedly more complex settings -- a convolutional neural network
and state-of-the-art vision-language models -- confirming that
finite-resolution similarity is a fundamental emergent informational
constraint, not merely a toy-model artifact. Together, these results provide an
exact theory of the generalization-identification trade-off and clarify how
semantic resolution shapes the representational capacity of deep networks and
brains alike.

</details>


### [26] [ss-Mamba: Semantic-Spline Selective State-Space Model](https://arxiv.org/abs/2506.14802)
*Zuochen Ye*

Main category: cs.LG

TL;DR: ss-Mamba is a new foundation model for time series forecasting, combining semantic-aware embeddings and adaptive spline-based temporal encoding with selective state-space modeling for efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve time series forecasting by addressing the computational inefficiency of Transformer models while maintaining performance and adding interpretability.

Method: Integrates semantic-aware embeddings (from pretrained language models) and spline-based Kolmogorov-Arnold Networks (KAN) within a selective state-space framework (Mamba) for linear-time complexity.

Result: Outperforms traditional Transformer models in accuracy, robustness, and interpretability while reducing computational complexity.

Conclusion: ss-Mamba is a versatile and efficient alternative to Transformer-based models for time-series forecasting.

Abstract: We propose ss-Mamba, a novel foundation model that enhances time series
forecasting by integrating semantic-aware embeddings and adaptive spline-based
temporal encoding within a selective state-space modeling framework. Building
upon the recent success of Transformer architectures, ss-Mamba adopts the Mamba
selective state space model as an efficient alternative that achieves
comparable performance while significantly reducing computational complexity
from quadratic to linear time. Semantic index embeddings, initialized from
pretrained language models, allow effective generalization to previously unseen
series through meaningful semantic priors. Additionally, spline-based
Kolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex
seasonalities and non-stationary temporal effects, providing a powerful
enhancement over conventional temporal feature encodings. Extensive
experimental evaluations confirm that ss-Mamba delivers superior accuracy,
robustness, and interpretability, demonstrating its capability as a versatile
and computationally efficient alternative to traditional Transformer-based
models in time-series forecasting.

</details>


### [27] [Heavy-Ball Momentum Method in Continuous Time and Discretization Error Analysis](https://arxiv.org/abs/2506.14806)
*Bochen Lyu,Xiaojing Zhang,Fangyi Zheng,He Wang,Zheng Wang,Zhanxing Zhu*

Main category: cs.LG

TL;DR: The paper introduces a continuous-time approximation for the Heavy-Ball momentum method, addressing discretization error explicitly to bridge the gap between discrete and continuous dynamics.


<details>
  <summary>Details</summary>
Motivation: To study the Heavy-Ball momentum method in continuous time while focusing on discretization error, providing theoretical tools for optimization methods.

Method: Designs a first-order piece-wise continuous differential equation with counter terms to account for discretization error explicitly.

Result: A continuous-time model for the HB method allowing control of discretization error to arbitrary order, with applications in deep learning.

Conclusion: Theoretical findings and numerical experiments support the model's effectiveness, demonstrating its utility in analyzing implicit bias and regularization.

Abstract: This paper establishes a continuous time approximation, a piece-wise
continuous differential equation, for the discrete Heavy-Ball (HB) momentum
method with explicit discretization error. Investigating continuous
differential equations has been a promising approach for studying the discrete
optimization methods. Despite the crucial role of momentum in gradient-based
optimization methods, the gap between the original discrete dynamics and the
continuous time approximations due to the discretization error has not been
comprehensively bridged yet. In this work, we study the HB momentum method in
continuous time while putting more focus on the discretization error to provide
additional theoretical tools to this area. In particular, we design a
first-order piece-wise continuous differential equation, where we add a number
of counter terms to account for the discretization error explicitly. As a
result, we provide a continuous time model for the HB momentum method that
allows the control of discretization error to arbitrary order of the step size.
As an application, we leverage it to find a new implicit regularization of the
directional smoothness and investigate the implicit bias of HB for diagonal
linear networks, indicating how our results can be used in deep learning. Our
theoretical findings are further supported by numerical experiments.

</details>


### [28] [PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models](https://arxiv.org/abs/2506.14808)
*Jenny Schmalfuss,Nadine Chang,Vibashan VS,Maying Shen,Andres Bruhn,Jose M. Alvarez*

Main category: cs.LG

TL;DR: PARC is a framework to analyze prompt sensitivity in Vision Language Models (VLMs), revealing VLMs inherit LLM prompt instability, with InternVL2 models being the most robust.


<details>
  <summary>Details</summary>
Motivation: VLMs integrate visual and language modalities, but their prompt sensitivity, inherited from LLMs, is understudied. PARC aims to analyze this sensitivity.

Method: PARC uses plausible prompt variations, a novel reliability score, and calibration to evaluate VLM prompt sensitivity across 22 models.

Result: VLMs show language prompt sensitivity in the vision domain, with InternVL2 models being the most robust. Prompt sensitivity may link to training data.

Conclusion: PARC provides a systematic way to analyze VLM prompt sensitivity, highlighting robust models and the impact of training data.

Abstract: Vision language models (VLMs) respond to user-crafted text prompts and visual
inputs, and are applied to numerous real-world problems. VLMs integrate visual
modalities with large language models (LLMs), which are well known to be
prompt-sensitive. Hence, it is crucial to determine whether VLMs inherit this
instability to varying prompts. We therefore investigate which prompt
variations VLMs are most sensitive to and which VLMs are most agnostic to
prompt variations. To this end, we introduce PARC (Prompt Analysis via
Reliability and Calibration), a VLM prompt sensitivity analysis framework built
on three pillars: (1) plausible prompt variations in both the language and
vision domain, (2) a novel model reliability score with built-in guarantees,
and (3) a calibration step that enables dataset- and prompt-spanning prompt
variation analysis. Regarding prompt variations, PARC's evaluation shows that
VLMs mirror LLM language prompt sensitivity in the vision domain, and most
destructive variations change the expected answer. Regarding models,
outstandingly robust VLMs among 22 evaluated models come from the InternVL2
family. We further find indications that prompt sensitivity is linked to
training data. The code will be at https://github.com/NVlabs/PARC.

</details>


### [29] [Intelligent Routing for Sparse Demand Forecasting: A Comparative Evaluation of Selection Strategies](https://arxiv.org/abs/2506.14810)
*Qiwen Zhang*

Main category: cs.LG

TL;DR: A Model-Router framework dynamically selects the best forecasting model for sparse and intermittent demand, improving accuracy and efficiency in supply chains.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle with frequent zero-demand periods, impacting inventory management.

Method: Proposes a Model-Router framework using rule-based, LightGBM, and InceptionTime routers to assign forecasting strategies based on demand patterns.

Result: InceptionTime router improves accuracy by 11.8% (NWRMSLE) and speeds up inference by 4.67x on the Favorita dataset.

Conclusion: The framework enhances forecasting precision, reducing stockouts and excess inventory, highlighting adaptive AI's role in supply chain optimization.

Abstract: Sparse and intermittent demand forecasting in supply chains presents a
critical challenge, as frequent zero-demand periods hinder traditional model
accuracy and impact inventory management. We propose and evaluate a
Model-Router framework that dynamically selects the most suitable forecasting
model-spanning classical, ML, and DL methods for each product based on its
unique demand pattern. By comparing rule-based, LightGBM, and InceptionTime
routers, our approach learns to assign appropriate forecasting strategies,
effectively differentiating between smooth, lumpy, or intermittent demand
regimes to optimize predictions. Experiments on the large-scale Favorita
dataset show our deep learning (Inception Time) router improves forecasting
accuracy by up to 11.8% (NWRMSLE) over strong, single-model benchmarks with
4.67x faster inference time. Ultimately, these gains in forecasting precision
will drive substantial reductions in both stockouts and wasteful excess
inventory, underscoring the critical role of intelligent, adaptive Al in
optimizing contemporary supply chain operations.

</details>


### [30] [Self-Composing Policies for Scalable Continual Reinforcement Learning](https://arxiv.org/abs/2506.14811)
*Mikel Malagón,Josu Ceberio,Jose A. Lozano*

Main category: cs.LG

TL;DR: A growable, modular neural network for continual reinforcement learning avoids catastrophic forgetting, scales linearly with tasks, and outperforms alternatives.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting and interference in continual reinforcement learning while maintaining scalability and plasticity.

Method: Proposes a modular architecture allowing selective combination of previous policies with internal policies, ensuring linear parameter growth with tasks.

Result: Achieves superior knowledge transfer and performance in benchmark continuous control and visual tasks.

Conclusion: The approach effectively balances scalability, plasticity, and performance in continual learning.

Abstract: This work introduces a growable and modular neural network architecture that
naturally avoids catastrophic forgetting and interference in continual
reinforcement learning. The structure of each module allows the selective
combination of previous policies along with its internal policy, accelerating
the learning process on the current task. Unlike previous growing neural
network approaches, we show that the number of parameters of the proposed
approach grows linearly with respect to the number of tasks, and does not
sacrifice plasticity to scale. Experiments conducted in benchmark continuous
control and visual problems reveal that the proposed approach achieves greater
knowledge transfer and performance than alternative methods.

</details>


### [31] [Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks](https://arxiv.org/abs/2506.14813)
*Yuxuan Jiang,Ziming Zhou,Boyu Xu,Beijie Liu,Runhui Xu,Peng Huang*

Main category: cs.LG

TL;DR: TRAINCHECK is a framework for detecting silent errors in DL training by inferring invariants and providing debugging support.


<details>
  <summary>Details</summary>
Motivation: Silent errors in DL training are hard to detect and diagnose, complicating the training process.

Method: TRAINCHECK infers invariants tailored for DL training and uses them to proactively detect errors.

Result: It detected 18 out of 20 reproduced errors in one iteration and found 6 unknown bugs in training libraries.

Conclusion: TRAINCHECK effectively addresses silent training errors by leveraging invariants for proactive detection and debugging.

Abstract: Training deep learning (DL) models is a complex process, making it prone to
silent errors that are challenging to detect and diagnose. This paper presents
TRAINCHECK, a framework that takes a proactive checking approach to address
silent training errors. TRAINCHECK automatically infers invariants tailored for
DL training. It uses these invariants to proactively detect silent errors
during the training process while providing debugging help. To evaluate
TRAINCHECK, we reproduce 20 real-world silent training errors with diverse root
causes. TRAINCHECK successfully detects 18 errors within a single training
iteration. It also uncovers 6 unknown bugs in popular training libraries that
lead to silent errors.

</details>


### [32] [Predicting Anthropometric Body Composition Variables Using 3D Optical Imaging and Machine Learning](https://arxiv.org/abs/2506.14815)
*Gyaneshwar Agrahari,Kiran Bist,Monika Pandey,Jacob Kapita,Zachary James,Jackson Knox,Steven Heymsfield,Sophia Ramirez,Peter Wolenski,Nadejda Drenska*

Main category: cs.LG

TL;DR: The paper proposes a semi-supervised $p$-Laplacian regression model to predict body composition variables (ALM, BFP, BMD) from 3D optical images, addressing data limitations in healthcare. It compares performance with supervised methods like SVR.


<details>
  <summary>Details</summary>
Motivation: DXA scans for body composition measurement are costly and time-consuming. The study aims to provide a cheaper, efficient alternative using 3D optical images and machine learning.

Method: Uses a semi-supervised $p$-Laplacian regression model on biomarkers from 3D images (847 patients). Compares with supervised methods (SVR, Least Squares SVR) under varying training data sizes.

Result: $p$-Laplacian model achieved errors of ~13% (ALM), ~10% (BMD), ~20% (BFP) with 10% training data. SVR performed best for ALM/BMD (~8% error), Least Squares SVR for BFP (~11% error) with 80% data.

Conclusion: The $p$-Laplacian model is promising for data-constrained healthcare applications, offering a viable alternative to DXA scans.

Abstract: Accurate prediction of anthropometric body composition variables, such as
Appendicular Lean Mass (ALM), Body Fat Percentage (BFP), and Bone Mineral
Density (BMD), is essential for early diagnosis of several chronic diseases.
Currently, researchers rely on Dual-Energy X-ray Absorptiometry (DXA) scans to
measure these metrics; however, DXA scans are costly and time-consuming. This
work proposes an alternative to DXA scans by applying statistical and machine
learning models on biomarkers (height, volume, left calf circumference, etc)
obtained from 3D optical images. The dataset consists of 847 patients and was
sourced from Pennington Biomedical Research Center. Extracting patients' data
in healthcare faces many technical challenges and legal restrictions. However,
most supervised machine learning algorithms are inherently data-intensive,
requiring a large amount of training data. To overcome these limitations, we
implemented a semi-supervised model, the $p$-Laplacian regression model. This
paper is the first to demonstrate the application of a $p$-Laplacian model for
regression. Our $p$-Laplacian model yielded errors of $\sim13\%$ for ALM,
$\sim10\%$ for BMD, and $\sim20\%$ for BFP when the training data accounted for
10 percent of all data. Among the supervised algorithms we implemented, Support
Vector Regression (SVR) performed the best for ALM and BMD, yielding errors of
$\sim 8\%$ for both, while Least Squares SVR performed the best for BFP with
$\sim 11\%$ error when trained on 80 percent of the data. Our findings position
the $p$-Laplacian model as a promising tool for healthcare applications,
particularly in a data-constrained environment.

</details>


### [33] [Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints](https://arxiv.org/abs/2506.14821)
*Sunil Kumar,Bowen Zhao,Leo Dirac,Paulina Varshavskaya*

Main category: cs.LG

TL;DR: Smaller VLMs trained with GRPO and external tools (e.g., zoom) outperform baselines in VQA tasks by leveraging detailed visual info.


<details>
  <summary>Details</summary>
Motivation: Addressing VLMs' struggles with detailed visual reasoning under limited compute.

Method: Train smaller models using GRPO, simple rewards, tool-calling interfaces, extra tokens for tool results, and data favoring hard visual examples.

Result: Improved performance on VQA tasks compared to similarly-sized baselines.

Conclusion: Combining GRPO, tools, and strategic training enhances VLM visual reasoning.

Abstract: Despite tremendous recent advances in large model reasoning ability,
vision-language models (VLMs) still struggle with detailed visual reasoning,
especially when compute resources are limited. To address this challenge, we
draw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale
models with Group Relative Policy Optimization (GRPO) to use external tools
such as zoom. The greatest benefit is obtained with a combination of GRPO
learning, a simple reward structure, a simplified tool-calling interface,
allocating additional tokens to the result of the tool call, and a training
data mix that over-represents visually difficult examples. Compared to
similarly-sized baseline models, our method achieves better performance on some
visual question-answering (VQA) tasks, thanks to the detailed visual
information gathered from the external tool.

</details>


### [34] [FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models](https://arxiv.org/abs/2506.14824)
*Yao Zhang,Hewei Gao,Haokun Chen,Weiguo Li,Yunpu Ma,Volker Tresp*

Main category: cs.LG

TL;DR: FedNano is an FL framework for MLLMs that centralizes the LLM on the server and uses lightweight client modules (NanoEdge) to reduce storage and communication costs while handling data heterogeneity and privacy.


<details>
  <summary>Details</summary>
Motivation: MLLMs face deployment challenges in real-world scenarios due to distributed data and privacy constraints. FL is a solution, but existing methods are impractical for large-scale MLLMs.

Method: FedNano centralizes the LLM on the server and introduces NanoEdge, a lightweight module with modality-specific encoders and low-rank adapters, reducing client-side storage and communication overhead.

Result: FedNano reduces client storage by 95% and communication costs to 0.01% of model parameters, outperforming prior FL baselines.

Conclusion: FedNano bridges the gap between MLLM scale and FL feasibility, enabling scalable, decentralized multimodal AI systems.

Abstract: Multimodal Large Language Models (MLLMs) excel in tasks like multimodal
reasoning and cross-modal retrieval but face deployment challenges in
real-world scenarios due to distributed multimodal data and strict privacy
requirements. Federated Learning (FL) offers a solution by enabling
collaborative model training without centralizing data. However, realizing FL
for MLLMs presents significant challenges, including high computational
demands, limited client capacity, substantial communication costs, and
heterogeneous client data. Existing FL methods assume client-side deployment of
full models, an assumption that breaks down for large-scale MLLMs due to their
massive size and communication demands. To address these limitations, we
propose FedNano, the first FL framework that centralizes the LLM on the server
while introducing NanoEdge, a lightweight module for client-specific
adaptation. NanoEdge employs modality-specific encoders, connectors, and
trainable NanoAdapters with low-rank adaptation. This design eliminates the
need to deploy LLM on clients, reducing client-side storage by 95%, and
limiting communication overhead to only 0.01% of the model parameters. By
transmitting only compact NanoAdapter updates, FedNano handles heterogeneous
client data and resource constraints while preserving privacy. Experiments
demonstrate that FedNano outperforms prior FL baselines, bridging the gap
between MLLM scale and FL feasibility, and enabling scalable, decentralized
multimodal AI systems.

</details>


### [35] [Accurate and Uncertainty-Aware Multi-Task Prediction of HEA Properties Using Prior-Guided Deep Gaussian Processes](https://arxiv.org/abs/2506.14828)
*Sk Md Ahnaf Akif Alvi,Mrinalini Mulukutla,Nicolas Flores,Danial Khatamsaz,Jan Janssen,Danny Perez,Douglas Allaire,Vahid Attari,Raymundo Arroyave*

Main category: cs.LG

TL;DR: The study evaluates four surrogate models for predicting material properties in high-entropy alloys, finding Deep Gaussian Processes (DGP) with ML-based priors most effective.


<details>
  <summary>Details</summary>
Motivation: Accelerate HEA discovery by integrating computational and experimental data using surrogate models.

Method: Systematic comparison of cGP, DGP, encoder-decoder neural networks, and XGBoost on hybrid datasets.

Result: DGP with ML-based priors outperforms others in capturing correlations and uncertainty.

Conclusion: Advanced surrogate models like DGP enhance predictive accuracy for efficient materials design.

Abstract: Surrogate modeling techniques have become indispensable in accelerating the
discovery and optimization of high-entropy alloys(HEAs), especially when
integrating computational predictions with sparse experimental observations.
This study systematically evaluates the fitting performance of four prominent
surrogate models conventional Gaussian Processes(cGP), Deep Gaussian
Processes(DGP), encoder-decoder neural networks for multi-output regression and
XGBoost applied to a hybrid dataset of experimental and computational
properties in the AlCoCrCuFeMnNiV HEA system. We specifically assess their
capabilities in predicting correlated material properties, including yield
strength, hardness, modulus, ultimate tensile strength, elongation, and average
hardness under dynamic and quasi-static conditions, alongside auxiliary
computational properties. The comparison highlights the strengths of
hierarchical and deep modeling approaches in handling heteroscedastic,
heterotopic, and incomplete data commonly encountered in materials informatics.
Our findings illustrate that DGP infused with machine learning-based prior
outperform other surrogates by effectively capturing inter-property
correlations and input-dependent uncertainty. This enhanced predictive accuracy
positions advanced surrogate models as powerful tools for robust and
data-efficient materials design.

</details>


### [36] [Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model](https://arxiv.org/abs/2506.14830)
*Zhizhao Wen,Ruoxin Zhang,Chao Wang*

Main category: cs.LG

TL;DR: A hybrid BiGRU-MHA model is proposed for SSD health state prediction, combining temporal feature extraction and multi-head attention for improved accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: To enhance data reliability by accurately predicting SSD health, addressing the limitations of traditional models.

Method: Uses a BiGRU network for bidirectional timing modeling and multi-head attention for dynamic feature weighting.

Result: Achieves 92.70% training and 92.44% test accuracy, with 0.94 AUC, demonstrating strong generalization.

Conclusion: The model offers a practical solution for SSD health prediction, reducing data loss risks and optimizing maintenance costs.

Abstract: Aiming at the critical role of SSD health state prediction in data
reliability assurance, this study proposes a hybrid BiGRU-MHA model that
incorporates a multi-head attention mechanism to enhance the accuracy and
stability of storage device health classification. The model innovatively
integrates temporal feature extraction and key information focusing
capabilities. Specifically, it leverages the bidirectional timing modeling
advantages of the BiGRU network to capture both forward and backward
dependencies of SSD degradation features. Simultaneously, the multi-head
attention mechanism dynamically assigns feature weights, improving the model's
sensitivity to critical health indicators. Experimental results show that the
proposed model achieves classification accuracies of 92.70% on the training set
and 92.44% on the test set, with a minimal performance gap of only 0.26%,
demonstrating excellent generalization ability. Further analysis using the
receiver operating characteristic (ROC) curve shows an area under the curve
(AUC) of 0.94 on the test set, confirming the model's robust binary
classification performance. This work not only presents a new technical
approach for SSD health prediction but also addresses the generalization
bottleneck of traditional models, offering a verifiable method with practical
value for preventive maintenance of industrial-grade storage systems. The
results show the model can significantly reduce data loss risks by providing
early failure warnings and help optimize maintenance costs, supporting
intelligent decision-making in building reliable storage systems for cloud
computing data centers and edge storage environments.

</details>


### [37] [Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders](https://arxiv.org/abs/2506.14937)
*Luan Gonçalves Miranda,Pedro Ivo da Cruz,Murilo Bellezoni Loiola*

Main category: cs.LG

TL;DR: The paper proposes using machine learning algorithms to automatically define the threshold for anomaly detection in Autoencoders, addressing performance issues caused by non-standardized thresholds.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection systems using Autoencoders face challenges due to non-trivial and non-standardized separation thresholds, impacting performance.

Method: Three machine learning algorithms (K-Nearest Neighbors, K-Means, and Support Vector Machine) were evaluated for automatic threshold definition.

Result: The evaluation of these algorithms aims to improve the performance of anomaly detection by optimizing the threshold.

Conclusion: Automating threshold definition with machine learning can enhance the effectiveness of anomaly detection systems using Autoencoders.

Abstract: Currently, digital security mechanisms like Anomaly Detection Systems using
Autoencoders (AE) show great potential for bypassing problems intrinsic to the
data, such as data imbalance. Because AE use a non-trivial and nonstandardized
separation threshold to classify the extracted reconstruction error, the
definition of this threshold directly impacts the performance of the detection
process. Thus, this work proposes the automatic definition of this threshold
using some machine learning algorithms. For this, three algorithms were
evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.

</details>


### [38] [CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration](https://arxiv.org/abs/2506.14843)
*Luca Gherardini,Imre Lengyel,Tunde Peto,Caroline C. W. Klaverd,Magda A. Meester-Smoord,Johanna Maria Colijnd,EYE-RISK Consortium,E3 Consortium,Jose Sousa*

Main category: cs.LG

TL;DR: CACTUS improves AMD classification by integrating diverse factors, offering explainability and outperforming standard ML models.


<details>
  <summary>Details</summary>
Motivation: Healthcare data is often incomplete, and ML models lack transparency, hindering trust and performance, especially in AMD diagnosis.

Method: Introduced CACTUS, a tool for AMD stage classification that incorporates genetic, dietary, clinical, and demographic factors, ensuring explainability and flexibility.

Result: CACTUS outperforms standard ML models, identifies key diagnostic factors, and provides confidence in results, aligning with medical knowledge.

Conclusion: CACTUS enhances decision-making in AMD diagnosis by addressing data limitations and biases, offering a transparent and reliable tool for clinicians.

Abstract: Machine Learning (ML) is used to tackle various tasks, such as disease
classification and prediction. The effectiveness of ML models relies heavily on
having large amounts of complete data. However, healthcare data is often
limited or incomplete, which can hinder model performance. Additionally, issues
like the trustworthiness of solutions vary with the datasets used. The lack of
transparency in some ML models further complicates their understanding and use.
In healthcare, particularly in the case of Age-related Macular Degeneration
(AMD), which affects millions of older adults, early diagnosis is crucial due
to the absence of effective treatments for reversing progression. Diagnosing
AMD involves assessing retinal images along with patients' symptom reports.
There is a need for classification approaches that consider genetic, dietary,
clinical, and demographic factors. Recently, we introduced the -Comprehensive
Abstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed
at improving AMD stage classification. CACTUS offers explainability and
flexibility, outperforming standard ML models. It enhances decision-making by
identifying key factors and providing confidence in its results. The important
features identified by CACTUS allow us to compare with existing medical
knowledge. By eliminating less relevant or biased data, we created a clinical
scenario for clinicians to offer feedback and address biases.

</details>


### [39] [Flat Channels to Infinity in Neural Loss Landscapes](https://arxiv.org/abs/2506.14951)
*Flavio Martinelli,Alexander Van Meegen,Berfin Şimşek,Wulfram Gerstner,Johanni Brea*

Main category: cs.LG

TL;DR: The paper identifies and analyzes special channels in neural network loss landscapes where loss decreases slowly, weights diverge, and neurons form gated linear units.


<details>
  <summary>Details</summary>
Motivation: To understand the structure and behavior of quasi-flat regions in neural network loss landscapes and their impact on optimization.

Method: Characterizes these channels through gradient dynamics, geometry, and functional interpretation, using gradient flow solvers and optimization methods like SGD or ADAM.

Result: Gradient-based methods often reach these channels, which resemble flat minima but lead to gated linear units.

Conclusion: The study reveals unexpected computational capabilities of fully connected layers through the emergence of gated linear units in these channels.

Abstract: The loss landscapes of neural networks contain minima and saddle points that
may be connected in flat regions or appear in isolation. We identify and
characterize a special structure in the loss landscape: channels along which
the loss decreases extremely slowly, while the output weights of at least two
neurons, $a_i$ and $a_j$, diverge to $\pm$infinity, and their input weight
vectors, $\mathbf{w_i}$ and $\mathbf{w_j}$, become equal to each other. At
convergence, the two neurons implement a gated linear unit:
$a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot
\mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot
\mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$. Geometrically, these
channels to infinity are asymptotically parallel to symmetry-induced lines of
critical points. Gradient flow solvers, and related optimization methods like
SGD or ADAM, reach the channels with high probability in diverse regression
settings, but without careful inspection they look like flat local minima with
finite parameter values. Our characterization provides a comprehensive picture
of these quasi-flat regions in terms of gradient dynamics, geometry, and
functional interpretation. The emergence of gated linear units at the end of
the channels highlights a surprising aspect of the computational capabilities
of fully connected layers.

</details>


### [40] [Generalized Reference Kernel With Negative Samples For Support Vector One-class Classification](https://arxiv.org/abs/2506.14895)
*Jenni Raitoharju*

Main category: cs.LG

TL;DR: The paper proposes GRKneg, an improved kernel for OC-SVM using negative samples without labels, outperforming standard OC-SVM and binary SVM in low negative sample scenarios.


<details>
  <summary>Details</summary>
Motivation: To enhance one-class classification by leveraging available negative samples without requiring labels for model optimization.

Method: Proposes GRKneg, a kernel improvement for OC-SVM using negative samples, comparing it with standard OC-SVM and binary SVM.

Result: GRKneg outperforms standard OC-SVM and binary SVM when negative samples are scarce.

Conclusion: GRKneg is effective for small-scale one-class classification with limited negative samples.

Abstract: This paper focuses on small-scale one-class classification with some negative
samples available. We propose Generalized Reference Kernel with Negative
Samples (GRKneg) for One-class Support Vector Machine (OC-SVM). We study
different ways to select/generate the reference vectors and recommend an
approach for the problem at hand. It is worth noting that the proposed method
does not use any labels in the model optimization but uses the original OC-SVM
implementation. Only the kernel used in the process is improved using the
negative data. We compare our method with the standard OC-SVM and with the
binary Support Vector Machine (SVM) using different amounts of negative
samples. Our approach consistently outperforms the standard OC-SVM using Radial
Basis Function kernel. When there are plenty of negative samples, the binary
SVM outperforms the one-class approaches as expected, but we show that for the
lowest numbers of negative samples the proposed approach clearly outperforms
the binary SVM.

</details>


### [41] [Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective](https://arxiv.org/abs/2506.14965)
*Zhoujun Cheng,Shibo Hao,Tianyang Liu,Fan Zhou,Yutao Xie,Feng Yao,Yuexin Bian,Yonghao Zhuang,Nilabjo Dey,Yuheng Zha,Yi Gu,Kun Zhou,Yuqi Wang,Yuan Li,Richard Fan,Jianshu She,Chengqian Gao,Abulhair Saparov,Haonan Li,Taylor W. Killian,Mikhail Yurochkin,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.LG

TL;DR: The paper introduces Guru, a curated RL reasoning corpus of 92K examples across six domains, addressing the challenge of scalable RL rewards for diverse reasoning tasks. It revisits RL findings for LLMs, showing varied domain performance and introduces state-of-the-art models, Guru-7B and Guru-32B.


<details>
  <summary>Details</summary>
Motivation: To broaden the understanding of RL's applicability beyond math and code in LLM reasoning by addressing the lack of reliable, scalable RL reward signals across diverse domains.

Method: Developed Guru, a corpus of 92K verifiable examples across six reasoning domains, using domain-specific reward design, deduplication, and filtering. Evaluated RL training effects across domains and introduced Guru-7B and Guru-32B models.

Result: Guru-7B and Guru-32B achieved state-of-the-art performance, outperforming baselines by 7.9% and 6.7% on a 17-task evaluation suite. RL training showed nuanced benefits, with cross-domain gains for familiar domains and in-domain training required for less exposed domains.

Conclusion: RL can facilitate genuine skill acquisition in LLMs, especially for domains with limited pretraining exposure. The release of data, models, and code aims to advance general-purpose reasoning.

Abstract: Reinforcement learning (RL) has emerged as a promising approach to improve
large language model (LLM) reasoning, yet most open efforts focus narrowly on
math and code, limiting our understanding of its broader applicability to
general reasoning. A key challenge lies in the lack of reliable, scalable RL
reward signals across diverse reasoning domains. We introduce Guru, a curated
RL reasoning corpus of 92K verifiable examples spanning six reasoning
domains--Math, Code, Science, Logic, Simulation, and Tabular--each built
through domain-specific reward design, deduplication, and filtering to ensure
reliability and effectiveness for RL training. Based on Guru, we systematically
revisit established findings in RL for LLM reasoning and observe significant
variation across domains. For example, while prior work suggests that RL
primarily elicits existing knowledge from pretrained models, our results reveal
a more nuanced pattern: domains frequently seen during pretraining (Math, Code,
Science) easily benefit from cross-domain RL training, while domains with
limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain
training to achieve meaningful performance gains, suggesting that RL is likely
to facilitate genuine skill acquisition. Finally, we present Guru-7B and
Guru-32B, two models that achieve state-of-the-art performance among open
models RL-trained with publicly available data, outperforming best baselines by
7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We
also show that our models effectively improve the Pass@k performance of their
base models, particularly on complex tasks less likely to appear in pretraining
data. We release data, models, training and evaluation code to facilitate
general-purpose reasoning at: https://github.com/LLM360/Reasoning360

</details>


### [42] [Event-Driven Online Vertical Federated Learning](https://arxiv.org/abs/2506.14911)
*Ganyu Wang,Boyu Wang,Bin Gu,Charles Ling*

Main category: cs.LG

TL;DR: Proposes an event-driven online Vertical Federated Learning (VFL) framework to address asynchronous data streaming challenges, introducing dynamic local regret (DLR) for stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Online learning in VFL faces challenges due to asynchronous data streaming and non-intersecting feature sets among clients, overlooked by prior research.

Method: Developed an event-driven online VFL framework where only a subset of clients activates per event, incorporating DLR for non-convex, non-stationary environments.

Result: The framework outperformed existing methods in stability, communication, and computation efficiency under non-stationary conditions.

Conclusion: The proposed event-driven VFL framework effectively addresses online learning challenges in VFL, offering practical advantages in real-world scenarios.

Abstract: Online learning is more adaptable to real-world scenarios in Vertical
Federated Learning (VFL) compared to offline learning. However, integrating
online learning into VFL presents challenges due to the unique nature of VFL,
where clients possess non-intersecting feature sets for the same sample. In
real-world scenarios, the clients may not receive data streaming for the
disjoint features for the same entity synchronously. Instead, the data are
typically generated by an \emph{event} relevant to only a subset of clients. We
are the first to identify these challenges in online VFL, which have been
overlooked by previous research. To address these challenges, we proposed an
event-driven online VFL framework. In this framework, only a subset of clients
were activated during each event, while the remaining clients passively
collaborated in the learning process. Furthermore, we incorporated
\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by
online learning problems with non-convex models within a non-stationary
environment. We conducted a comprehensive regret analysis of our proposed
framework, specifically examining the DLR under non-convex conditions with
event-driven online VFL. Extensive experiments demonstrated that our proposed
framework was more stable than the existing online VFL framework under
non-stationary data conditions while also significantly reducing communication
and computation costs.

</details>


### [43] [Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits](https://arxiv.org/abs/2506.14988)
*Tianyi Xu,Jiaxin Liu,Zizhan Zheng*

Main category: cs.LG

TL;DR: A multi-agent multi-armed bandit (MA-MAB) framework ensures fairness and performance by using a probing strategy for decision-making under limited information.


<details>
  <summary>Details</summary>
Motivation: To balance fairness and performance in multi-agent systems with limited reward information.

Method: Introduces a probing framework for strategic information gathering, with greedy (offline) and sublinear regret (online) algorithms.

Result: Outperforms baselines in fairness and efficiency on synthetic and real-world datasets.

Conclusion: The proposed MA-MAB framework effectively balances fairness and performance in both offline and online settings.

Abstract: We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at
ensuring fair outcomes across agents while maximizing overall system
performance. A key challenge in this setting is decision-making under limited
information about arm rewards. To address this, we introduce a novel probing
framework that strategically gathers information about selected arms before
allocation. In the offline setting, where reward distributions are known, we
leverage submodular properties to design a greedy probing algorithm with a
provable performance bound. For the more complex online setting, we develop an
algorithm that achieves sublinear regret while maintaining fairness. Extensive
experiments on synthetic and real-world datasets show that our approach
outperforms baseline methods, achieving better fairness and efficiency.

</details>


### [44] [FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning](https://arxiv.org/abs/2506.14929)
*Ganyu Wang,Jinjie Fang,Maxwell J. Ying,Bin Gu,Xi Chen,Boyu Wang,Charles Ling*

Main category: cs.LG

TL;DR: FedOne optimizes query efficiency in federated black-box prompt learning by activating one client per round, reducing costs for cloud-based LLMs.


<details>
  <summary>Details</summary>
Motivation: Previous federated black-box prompt tuning ignored the high query costs of cloud-based LLMs, prompting a need for efficient methods.

Method: Proposed FedOne framework, activating one client per round (FedOne) for optimal query efficiency, and validated through numerical experiments.

Result: FedOne significantly improves query efficiency, aligning with theoretical findings.

Conclusion: FedOne offers a cost-effective solution for federated black-box prompt learning with cloud-based LLMs.

Abstract: Black-Box Discrete Prompt Learning is a prompt-tuning method that optimizes
discrete prompts without accessing model parameters or gradients, making the
prompt tuning on a cloud-based Large Language Model (LLM) feasible. Adapting
federated learning to BDPL could further enhance prompt tuning performance by
leveraging data from diverse sources. However, all previous research on
federated black-box prompt tuning had neglected the substantial query cost
associated with the cloud-based LLM service. To address this gap, we conducted
a theoretical analysis of query efficiency within the context of federated
black-box prompt tuning. Our findings revealed that degrading FedAvg to
activate only one client per round, a strategy we called \textit{FedOne},
enabled optimal query efficiency in federated black-box prompt learning.
Building on this insight, we proposed the FedOne framework, a federated
black-box discrete prompt learning method designed to maximize query efficiency
when interacting with cloud-based LLMs. We conducted numerical experiments on
various aspects of our framework, demonstrating a significant improvement in
query efficiency, which aligns with our theoretical results.

</details>


### [45] [Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment](https://arxiv.org/abs/2506.15019)
*Yue Gao*

Main category: cs.LG

TL;DR: Stable CDE-based state representations improve RL for sepsis treatment by addressing training instability and enforcing acuity-aware features, leading to superior policy performance.


<details>
  <summary>Details</summary>
Motivation: The need for stable and clinically meaningful state representations from irregular ICU time series to enhance RL for sepsis treatment, as previous works overlooked training instability.

Method: Use Controlled Differential Equations (CDE) with early stopping or stabilization methods and correlation regularization with clinical scores (SOFA, SAPS-II, OASIS).

Result: Stable CDE autoencoder produces representations correlated with acuity scores, enabling high-performing RL policies (WIS return > 0.9), while unstable training fails (WIS return ~ 0).

Conclusion: Training stability is crucial for sequential representation learning in clinical RL, with CDEs offering practical guidelines for encoding irregular medical time series.

Abstract: Effective reinforcement learning (RL) for sepsis treatment depends on
learning stable, clinically meaningful state representations from irregular ICU
time series. While previous works have explored representation learning for
this task, the critical challenge of training instability in sequential
representations and its detrimental impact on policy performance has been
overlooked. This work demonstrates that Controlled Differential Equations (CDE)
state representation can achieve strong RL policies when two key factors are
met: (1) ensuring training stability through early stopping or stabilization
methods, and (2) enforcing acuity-aware representations by correlation
regularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the
MIMIC-III sepsis cohort reveal that stable CDE autoencoder produces
representations strongly correlated with acuity scores and enables RL policies
with superior performance (WIS return $> 0.9$). In contrast, unstable CDE
representation leads to degraded representations and policy failure (WIS return
$\sim$ 0). Visualizations of the latent space show that stable CDEs not only
separate survivor and non-survivor trajectories but also reveal clear acuity
score gradients, whereas unstable training fails to capture either pattern.
These findings highlight practical guidelines for using CDEs to encode
irregular medical time series in clinical RL, emphasizing the need for training
stability in sequential representation learning.

</details>


### [46] [SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models](https://arxiv.org/abs/2506.15021)
*Gyuhak Kim,Sumiran Singh Thakur,Su Min Park,Wei Wei,Yujia Bao*

Main category: cs.LG

TL;DR: SFT-GO introduces a novel supervised fine-tuning method for LLMs by grouping tokens based on importance, optimizing with a weighted loss, and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing SFT methods treat all tokens uniformly, ignoring critical task-specific information in subsets of tokens.

Method: SFT-GO groups tokens by importance, uses a weighted combination of worst-group loss and cross-entropy loss, and adaptively emphasizes challenging groups.

Result: SFT-GO consistently outperforms baselines across benchmarks, datasets, and base models, with theoretical convergence analysis.

Conclusion: SFT-GO is robust and effective, improving LLM fine-tuning by focusing on important token groups.

Abstract: Supervised fine-tuning (SFT) has become an essential step in tailoring large
language models (LLMs) to align with human expectations and specific downstream
tasks. However, existing SFT methods typically treat each training instance as
a uniform sequence, giving equal importance to all tokens regardless of their
relevance. This overlooks the fact that only a subset of tokens often contains
critical, task-specific information. To address this limitation, we introduce
Supervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that
treats groups of tokens differently based on their importance.SFT-GO groups
tokens in each sample based on their importance values and optimizes the LLM
using a weighted combination of the worst-group loss and the standard
cross-entropy loss. This mechanism adaptively emphasizes the most challenging
token groups and guides the model to better handle different group
distributions, thereby improving overall learning dynamics. We provide a
theoretical analysis of SFT-GO's convergence rate, demonstrating its
efficiency. Empirically, we apply SFT-GO with three different token grouping
strategies and show that models trained with SFT-GO consistently outperform
baseline approaches across popular LLM benchmarks. These improvements hold
across various datasets and base models, demonstrating the robustness and the
effectiveness of our method.

</details>


### [47] [Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size](https://arxiv.org/abs/2506.15025)
*Soufiane Hayou,Liyuan Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pretraining large language models is a costly process. To make this process
more efficient, several methods have been proposed to optimize model
architecture/parametrization and hardware use. On the parametrization side,
$\mu P$ (Maximal Update Parametrization) parametrizes model weights and
learning rate (LR) in a way that makes hyperparameters (HPs) transferable with
width (embedding dimension): HPs can be tuned for a small model and used for
larger models without additional tuning. While $\mu$P showed impressive results
in practice, recent empirical studies have reported conflicting observations
when applied to LLMs. One limitation of the theory behind $\mu$P is the fact
that input dimension (vocabulary size in LLMs) is considered fixed when taking
the width to infinity. This is unrealistic since vocabulary size is generally
much larger than width in practice. In this work, we provide a theoretical
analysis of the effect of vocabulary size on training dynamics, and
subsequently show that as vocabulary size increases, the training dynamics
\emph{interpolate between the $\mu$P regime and another regime that we call
Large Vocab (LV) Regime}, where optimal scaling rules are different from those
predicted by $\mu$P. Our analysis reveals that in the LV regime, the optimal
embedding LR to hidden LR ratio should roughly scale as $\Theta(\sqrt{width})$,
surprisingly close to the empirical findings previously reported in the
literature, and different from the $\Theta(width)$ ratio predicted by $\mu$P.
We conduct several experiments to validate our theory, and pretrain a 1B model
from scratch to show the benefit of our suggested scaling rule for the
embedding LR.

</details>


### [48] [Sequential Policy Gradient for Adaptive Hyperparameter Optimization](https://arxiv.org/abs/2506.15051)
*Zheng Li,Jerry Cheng,Huanying Helen Gu*

Main category: cs.LG

TL;DR: SPG introduces a lightweight online hyperparameter optimization method using sequential policy gradient modeling, reducing computational costs while improving model performance.


<details>
  <summary>Details</summary>
Motivation: Conventional reinforcement learning for hyperparameter optimization is computationally expensive, limiting widespread use.

Method: SPG extends base models with temporary modules to generate state-action trajectories in a single forward pass, enabling efficient hyperparameter optimization.

Result: SPG improves model performance by +0.2∼7% across diverse datasets (ImageNet, COCO, GLUE, SQuAD, SUPERB) with low computational costs.

Conclusion: SPG is a scalable and efficient alternative to traditional policy gradient methods, demonstrating consistent performance gains.

Abstract: Reinforcement learning is essential for neural architecture search and
hyperparameter optimization, but the conventional approaches impede widespread
use due to prohibitive time and computational costs. Inspired by DeepSeek-V3
multi-token prediction architecture, we propose Sequential Policy Gradient
modeling (SPG), a novel trajectory generation paradigm for lightweight online
hyperparameter optimization. In contrast to conventional policy gradient
methods, SPG extends the base model with temporary modules, enabling it to
generate state-action (padded) trajectories in a single forward pass. Our
experiments demonstrate that models gain performance when retrained with SPG on
their original datasets and also outperform standard transfer fine-tuning. We
evaluate on five datasets spanning computer vision (ImageNet, COCO), natural
language processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial
applicability of SPG. The proposed method demonstrates consistent improvements
across widely adopted models, achieving performance gains of $+0.2\sim7\%$,
with significantly low computational costs. Fully reproducible code and
pre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.

</details>


### [49] [ODD: Overlap-aware Estimation of Model Performance under Distribution Shift](https://arxiv.org/abs/2506.14978)
*Aayush Mishra,Anqi Liu*

Main category: cs.LG

TL;DR: The paper introduces Overlap-aware Disagreement Discrepancy (ODD) to improve error estimation of ML models in unseen test domains by focusing on non-overlapping regions, outperforming prior DIS² methods.


<details>
  <summary>Details</summary>
Motivation: Accurate error estimation in unseen domains is critical for safe ML systems. Prior DIS² methods compete in overlapping regions, limiting reliability.

Method: Proposes ODD, which maximizes disagreement only in non-overlapping target domains using domain-classifiers to estimate overlap.

Result: ODD-based bounds outperform DIS² in predicting target performance across benchmarks, improving estimation error while remaining reliable.

Conclusion: ODD provides a more accurate and reliable method for error estimation in unseen domains by avoiding competition in overlapping regions.

Abstract: Reliable and accurate estimation of the error of an ML model in unseen test
domains is an important problem for safe intelligent systems. Prior work uses
disagreement discrepancy (DIS^2) to derive practical error bounds under
distribution shifts. It optimizes for a maximally disagreeing classifier on the
target domain to bound the error of a given source classifier. Although this
approach offers a reliable and competitively accurate estimate of the target
error, we identify a problem in this approach which causes the disagreement
discrepancy objective to compete in the overlapping region between source and
target domains. With an intuitive assumption that the target disagreement
should be no more than the source disagreement in the overlapping region due to
high enough support, we devise Overlap-aware Disagreement Discrepancy (ODD).
Maximizing ODD only requires disagreement in the non-overlapping target domain,
removing the competition. Our ODD-based bound uses domain-classifiers to
estimate domain-overlap and better predicts target performance than DIS^2. We
conduct experiments on a wide array of benchmarks to show that our method
improves the overall performance-estimation error while remaining valid and
reliable. Our code and results are available on GitHub.

</details>


### [50] [Singular Value Decomposition on Kronecker Adaptation for Large Language Model](https://arxiv.org/abs/2506.15251)
*Yee Hin Chong,Peng Qu*

Main category: cs.LG

TL;DR: SoKA is a parameter-efficient fine-tuning method combining Kronecker-product factorization and SVD-driven initialization, reducing trainable parameters by 25% while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Full fine-tuning of large Transformer models is costly; existing PEFT methods have drawbacks like latency, suboptimal convergence, or rigid rank choices.

Method: SoKA uses Kronecker-product SVD (KPSVD) for compact factorization and dynamic rank selection based on energy-threshold and elbow-point criteria.

Result: On LLaMA2-7B, SoKA achieves comparable or better performance with 0.99M parameters, 25% fewer than LoRA/PiSSA, and shows faster convergence.

Conclusion: SoKA is a robust and efficient PEFT method for large-scale model adaptation, balancing parameter efficiency and performance.

Abstract: Large pre-trained Transformer models achieve state-of-the-art results across
diverse language and reasoning tasks, but full fine-tuning incurs substantial
storage, memory, and computational overhead. Parameter-efficient fine-tuning
(PEFT) methods mitigate these costs by learning only a small subset of
task-specific parameters, yet existing approaches either introduce
inference-time latency (adapter modules), suffer from suboptimal convergence
(randomly initialized low-rank updates), or rely on fixed rank choices that may
not match task complexity (Kronecker-based decompositions).
  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that
combines Kronecker-product tensor factorization with SVD-driven initialization
and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)
procedure extracts principal components of the full weight update into compact
Kronecker factors, while an adaptive rank selection algorithm uses
energy-threshold and elbow-point criteria to prune negligible components.
  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal
mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires
only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or
exceeding baseline performance. Moreover, SoKA exhibits faster convergence and
more stable gradients, highlighting its robustness and efficiency for
large-scale model adaptation.

</details>


### [51] [Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks](https://arxiv.org/abs/2506.14986)
*Maxime Usdin,Lito Kriara,Licinio Craveiro*

Main category: cs.LG

TL;DR: The paper predicts MS disability progression using sparse clinical data and digital Floodlight data, showing improved accuracy with multimodal transformer models.


<details>
  <summary>Details</summary>
Motivation: Early prediction of MS disability progression is difficult due to disease heterogeneity, prompting the use of advanced models to integrate clinical and digital data.

Method: Employed tabular and time-series foundation models, a custom multimodal attention-based transformer, and machine learning methods to analyze baseline clinical data and 12 weeks of digital Floodlight data.

Result: Integration of digital data improved prediction performance (AUROC 0.63). The multimodal transformer outperformed unimodal models, highlighting the benefit of combining clinical and digital data.

Conclusion: Foundation models and multimodal approaches show promise for extracting predictive signals from diverse clinical and digital data, enhancing prognostics for MS and other complex diseases.

Abstract: Early multiple sclerosis (MS) disability progression prediction is
challenging due to disease heterogeneity. This work predicts 48- and 72-week
disability using sparse baseline clinical data and 12 weeks of daily digital
Floodlight data from the CONSONANCE clinical trial. We employed
state-of-the-art tabular and time-series foundation models (FMs), a custom
multimodal attention-based transformer, and machine learning methods. Despite
the difficulty of early prediction (AUROC 0.63), integrating digital data via
advanced models improved performance over clinical data alone. A transformer
model using unimodal embeddings from the Moment FM yielded the best result, but
our multimodal transformer consistently outperformed its unimodal counterpart,
confirming the advantages of combining clinical with digital data. Our findings
demonstrate the promise of FMs and multimodal approaches to extract predictive
signals from complex and diverse clinical and digital life sciences data (e.g.,
imaging, omics), enabling more accurate prognostics for MS and potentially
other complex diseases.

</details>


### [52] [Unlocking Post-hoc Dataset Inference with Synthetic Data](https://arxiv.org/abs/2506.15271)
*Bihe Zhao,Pratyush Maini,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: The paper proposes a method to generate synthetic held-out data for Dataset Inference (DI), enabling detection of unauthorized dataset use in LLM training without requiring in-distribution private data.


<details>
  <summary>Details</summary>
Motivation: Existing DI methods need in-distribution held-out data, which is rarely available, limiting their practicality. The paper aims to overcome this by synthetically generating such data.

Method: The approach involves (1) training a data generator on a suffix-based completion task to create diverse, high-quality synthetic data, and (2) post-hoc calibration to bridge likelihood gaps between real and synthetic data.

Result: Experiments show the synthetic data enables DI to detect original training sets with high confidence and low false positives, aiding copyright claims.

Conclusion: The method reliably supports real-world litigations by verifying dataset usage without requiring inaccessible held-out data.

Abstract: The remarkable capabilities of Large Language Models (LLMs) can be mainly
attributed to their massive training datasets, which are often scraped from the
internet without respecting data owners' intellectual property rights. Dataset
Inference (DI) offers a potential remedy by identifying whether a suspect
dataset was used in training, thereby enabling data owners to verify
unauthorized use. However, existing DI methods require a private set-known to
be absent from training-that closely matches the compromised dataset's
distribution. Such in-distribution, held-out data is rarely available in
practice, severely limiting the applicability of DI. In this work, we address
this challenge by synthetically generating the required held-out set. Our
approach tackles two key obstacles: (1) creating high-quality, diverse
synthetic data that accurately reflects the original distribution, which we
achieve via a data generator trained on a carefully designed suffix-based
completion task, and (2) bridging likelihood gaps between real and synthetic
data, which is realized through post-hoc calibration. Extensive experiments on
diverse text datasets show that using our generated data as a held-out set
enables DI to detect the original training sets with high confidence, while
maintaining a low false positive rate. This result empowers copyright owners to
make legitimate claims on data usage and demonstrates our method's reliability
for real-world litigations. Our code is available at
https://github.com/sprintml/PostHocDatasetInference.

</details>


### [53] [Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation](https://arxiv.org/abs/2506.15309)
*Júlia Vilalta-Mor,Alexis Molina,Laura Ortega Varga,Isaac Filella-Merce,Victor Guallar*

Main category: cs.LG

TL;DR: A structured active learning method integrates Seq2Seq VAE to optimize multi-target drug molecules, balancing diversity and affinity, demonstrated with coronavirus protease inhibitors.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of optimizing molecules for multiple therapeutic targets due to sparse rewards and conflicting constraints.

Method: Combines Seq2Seq VAE with iterative loops to expand chemical diversity and apply multi-target docking thresholds.

Result: Generated diverse pan-inhibitor candidates for coronavirus proteases, improving exploration of beneficial chemical space.

Conclusion: The framework offers a generalizable approach for navigating complex polypharmacological landscapes efficiently.

Abstract: Simultaneously optimizing molecules against multiple therapeutic targets
remains a profound challenge in drug discovery, particularly due to sparse
rewards and conflicting design constraints. We propose a structured active
learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational
autoencoder (VAE) into iterative loops designed to balance chemical diversity,
molecular quality, and multi-target affinity. Our method alternates between
expanding chemically feasible regions of latent space and progressively
constraining molecules based on increasingly stringent multi-target docking
thresholds. In a proof-of-concept study targeting three related coronavirus
main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently
generated a structurally diverse set of pan-inhibitor candidates. We
demonstrate that careful timing and strategic placement of chemical filters
within this active learning pipeline markedly enhance exploration of beneficial
chemical space, transforming the sparse-reward, multi-objective drug design
problem into an accessible computational task. Our framework thus provides a
generalizable roadmap for efficiently navigating complex polypharmacological
landscapes.

</details>


### [54] [When and How Unlabeled Data Provably Improve In-Context Learning](https://arxiv.org/abs/2506.15329)
*Yingcong Li,Xiangyu Chang,Muti Kara,Xiaofeng Liu,Amit Roy-Chowdhury,Samet Oymak*

Main category: cs.LG

TL;DR: The paper explores how in-context learning (ICL) works with missing or incorrect labels, showing that multilayer transformers outperform single-layer models by leveraging unlabeled data effectively.


<details>
  <summary>Details</summary>
Motivation: To understand why ICL works even with imperfect labels and to improve semi-supervised learning in tabular data.

Method: Theoretical analysis of linear attention models and transformers under a binary Gaussian mixture model (GMM) with missing labels.

Result: Multilayer transformers implicitly construct optimal estimators, outperforming single-layer models, and looping enhances semi-supervised performance.

Conclusion: Looping transformers improves semi-supervised learning, with practical benefits demonstrated on real-world datasets.

Abstract: Recent research shows that in-context learning (ICL) can be effective even
when demonstrations have missing or incorrect labels. To shed light on this
capability, we examine a canonical setting where the demonstrations are drawn
according to a binary Gaussian mixture model (GMM) and a certain fraction of
the demonstrations have missing labels. We provide a comprehensive theoretical
study to show that: (1) The loss landscape of one-layer linear attention models
recover the optimal fully-supervised estimator but completely fail to exploit
unlabeled data; (2) In contrast, multilayer or looped transformers can
effectively leverage unlabeled data by implicitly constructing estimators of
the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting
features and partially-observed labels (with missing entries set to zero). We
characterize the class of polynomials that can be expressed as a function of
depth and draw connections to Expectation Maximization, an iterative
pseudo-labeling algorithm commonly used in semi-supervised learning.
Importantly, the leading polynomial power is exponential in depth, so mild
amount of depth/looping suffices. As an application of theory, we propose
looping off-the-shelf tabular foundation models to enhance their
semi-supervision capabilities. Extensive evaluations on real-world datasets
show that our method significantly improves the semisupervised tabular learning
performance over the standard single pass inference.

</details>


### [55] [Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI](https://arxiv.org/abs/2506.15408)
*David Dembinsky,Adriano Lucieri,Stanislav Frolov,Hiba Najjar,Ko Watanabe,Andreas Dengel*

Main category: cs.LG

TL;DR: The paper addresses the lack of standardized evaluation protocols for Explainable AI (XAI) by introducing a unified framework (VXAI) through a systematic literature review.


<details>
  <summary>Details</summary>
Motivation: The opacity of black-box AI models like Deep Neural Networks undermines trustworthiness, necessitating rigorous evaluation of XAI explanations.

Method: A systematic literature review following PRISMA guidelines, analyzing 362 publications and categorizing 41 metric groups. A three-dimensional scheme (explanation type, contextuality, quality) is proposed.

Result: The VXAI framework offers a comprehensive, structured overview of XAI evaluation metrics, enabling systematic selection and comparability.

Conclusion: VXAI provides a flexible foundation for future XAI evaluation, promoting trust and standardization in the field.

Abstract: Modern AI systems frequently rely on opaque black-box models, most notably
Deep Neural Networks, whose performance stems from complex architectures with
millions of learned parameters. While powerful, their complexity poses a major
challenge to trustworthiness, particularly due to a lack of transparency.
Explainable AI (XAI) addresses this issue by providing human-understandable
explanations of model behavior. However, to ensure their usefulness and
trustworthiness, such explanations must be rigorously evaluated. Despite the
growing number of XAI methods, the field lacks standardized evaluation
protocols and consensus on appropriate metrics. To address this gap, we conduct
a systematic literature review following the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a
unified framework for the eValuation of XAI (VXAI). We identify 362 relevant
publications and aggregate their contributions into 41 functionally similar
metric groups. In addition, we propose a three-dimensional categorization
scheme spanning explanation type, evaluation contextuality, and explanation
quality desiderata. Our framework provides the most comprehensive and
structured overview of VXAI to date. It supports systematic metric selection,
promotes comparability across methods, and offers a flexible foundation for
future extensions.

</details>


### [56] [Reward Models in Deep Reinforcement Learning: A Survey](https://arxiv.org/abs/2506.15421)
*Rui Yu,Shenghua Wan,Yucen Wang,Chen-Xiao Gao,Le Gan,Zongzhang Zhang,De-Chuan Zhan*

Main category: cs.LG

TL;DR: A survey on reward modeling techniques in deep reinforcement learning, covering background, recent approaches, applications, evaluation methods, and future directions.


<details>
  <summary>Details</summary>
Motivation: To systematically review and categorize reward modeling techniques in RL, addressing the lack of a comprehensive survey in current literature.

Method: Categorizes reward modeling approaches by source, mechanism, and learning paradigm, and reviews applications and evaluation methods.

Result: Provides a structured overview of both established and emerging reward modeling techniques.

Conclusion: Highlights promising research directions and fills the gap for a systematic review of reward models in RL.

Abstract: In reinforcement learning (RL), agents continually interact with the
environment and use the feedback to refine their behavior. To guide policy
optimization, reward models are introduced as proxies of the desired
objectives, such that when the agent maximizes the accumulated reward, it also
fulfills the task designer's intentions. Recently, significant attention from
both academic and industrial researchers has focused on developing reward
models that not only align closely with the true objectives but also facilitate
policy optimization. In this survey, we provide a comprehensive review of
reward modeling techniques within the deep RL literature. We begin by outlining
the background and preliminaries in reward modeling. Next, we present an
overview of recent reward modeling approaches, categorizing them based on the
source, the mechanism, and the learning paradigm. Building on this
understanding, we discuss various applications of these reward modeling
techniques and review methods for evaluating reward models. Finally, we
conclude by highlighting promising research directions in reward modeling.
Altogether, this survey includes both established and emerging methods, filling
the vacancy of a systematic review of reward models in current literature.

</details>


### [57] [Zero-Shot Reinforcement Learning Under Partial Observability](https://arxiv.org/abs/2506.15446)
*Scott Jeen,Tom Bewley,Jonathan M. Cullen*

Main category: cs.LG

TL;DR: Memory-based zero-shot RL methods improve performance in partially observable environments compared to memory-free baselines.


<details>
  <summary>Details</summary>
Motivation: To address the degradation of zero-shot RL methods in partially observable settings, common in real-world applications.

Method: Use memory-based architectures to handle partial observability in zero-shot RL, tested in environments with partially observed states, rewards, and dynamics changes.

Result: Memory-based methods outperform memory-free baselines in partially observable scenarios.

Conclusion: Memory-based architectures are effective for zero-shot RL in partially observable environments, with open-sourced code available.

Abstract: Recent work has shown that, under certain assumptions, zero-shot
reinforcement learning (RL) methods can generalise to any unseen task in an
environment after reward-free pre-training. Access to Markov states is one such
assumption, yet, in many real-world applications, the Markov state is only
partially observable. Here, we explore how the performance of standard
zero-shot RL methods degrades when subjected to partially observability, and
show that, as in single-task RL, memory-based architectures are an effective
remedy. We evaluate our memory-based zero-shot RL methods in domains where the
states, rewards and a change in dynamics are partially observed, and show
improved performance over memory-free baselines. Our code is open-sourced via:
https://enjeeneer.io/projects/bfms-with-memory/.

</details>


### [58] [Muon Optimizes Under Spectral Norm Constraints](https://arxiv.org/abs/2506.15054)
*Lizhang Chen,Jonathan Li,Qiang Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The pursuit of faster optimization algorithms remains an active and important
research direction in deep learning. Recently, the Muon optimizer [JJB+24] has
demonstrated promising empirical performance, but its theoretical foundation
remains less understood. In this paper, we bridge this gap and provide a
theoretical analysis of Muon by placing it within the Lion-$\mathcal{K}$ family
of optimizers [CLLL24]. Specifically, we show that Muon corresponds to
Lion-$\mathcal{K}$ when equipped with the nuclear norm, and we leverage the
theoretical results of Lion-$\mathcal{K}$ to establish that Muon (with
decoupled weight decay) implicitly solves an optimization problem that enforces
a constraint on the spectral norm of weight matrices. This perspective not only
demystifies the implicit regularization effects of Muon but also leads to
natural generalizations through varying the choice of convex map $\mathcal{K}$,
allowing for the exploration of a broader class of implicitly regularized and
constrained optimization algorithms.

</details>


### [59] [Warping and Matching Subsequences Between Time Series](https://arxiv.org/abs/2506.15452)
*Simiao Lin,Wannes Meert,Pieter Robberechts,Hendrik Blockeel*

Main category: cs.LG

TL;DR: A novel technique simplifies warping paths to highlight key transformations (shift, compression, amplitude differences) in time series, improving interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack qualitative comparison and fail to show broader structural relationships between subsequences in time series.

Method: Proposes a technique to simplify warping paths, quantifying and visualizing key transformations like shift, compression, and amplitude differences.

Result: The method provides a clearer representation of subsequence matches, enhancing interpretability in time series comparison.

Conclusion: The proposed technique improves qualitative understanding of time series comparisons by highlighting key transformations.

Abstract: Comparing time series is essential in various tasks such as clustering and
classification. While elastic distance measures that allow warping provide a
robust quantitative comparison, a qualitative comparison on top of them is
missing. Traditional visualizations focus on point-to-point alignment and do
not convey the broader structural relationships at the level of subsequences.
This limitation makes it difficult to understand how and where one time series
shifts, speeds up or slows down with respect to another. To address this, we
propose a novel technique that simplifies the warping path to highlight,
quantify and visualize key transformations (shift, compression, difference in
amplitude). By offering a clearer representation of how subsequences match
between time series, our method enhances interpretability in time series
comparison.

</details>


### [60] [HiPreNets: High-Precision Neural Networks through Progressive Training](https://arxiv.org/abs/2506.15064)
*Ethan Mulle,Wei Kang,Qi Gong*

Main category: cs.LG

TL;DR: A progressive framework (HiPreNets) is introduced to train high-precision neural networks by refining residuals, improving accuracy beyond traditional MSE-focused methods.


<details>
  <summary>Details</summary>
Motivation: Training accurate neural networks for complex problems is challenging due to non-convex optimization and overlooked L∞ error.

Method: Uses staged training to refine residuals with additional networks, guided by residual structure for loss function, parameters, and adaptive sampling.

Result: Validated effectiveness through benchmark problems, showing improved accuracy.

Conclusion: HiPreNets offers a structured approach to enhance neural network precision by focusing on residuals and adaptive techniques.

Abstract: Deep neural networks are powerful tools for solving nonlinear problems in
science and engineering, but training highly accurate models becomes
challenging as problem complexity increases. Non-convex optimization and
numerous hyperparameters to tune make performance improvement difficult, and
traditional approaches often prioritize minimizing mean squared error (MSE)
while overlooking $L^{\infty}$ error, which is the critical focus in many
applications. To address these challenges, we present a progressive framework
for training and tuning high-precision neural networks (HiPreNets). Our
approach refines a previously explored staged training technique for neural
networks that improves an existing fully connected neural network by
sequentially learning its prediction residuals using additional networks,
leading to improved overall accuracy. We discuss how to take advantage of the
structure of the residuals to guide the choice of loss function, number of
parameters to use, and ways to introduce adaptive data sampling techniques. We
validate our framework's effectiveness through several benchmark problems.

</details>


### [61] [Pixel-level Certified Explanations via Randomized Smoothing](https://arxiv.org/abs/2506.15499)
*Alaa Anani,Tobias Lorenz,Mario Fritz,Bernt Schiele*

Main category: cs.LG

TL;DR: The paper introduces a certification framework to ensure pixel-level robustness in post-hoc attribution methods for deep learning models, addressing their vulnerability to input perturbations.


<details>
  <summary>Details</summary>
Motivation: Attribution methods are non-robust to small input perturbations, undermining trust in their explanations. The paper aims to provide rigorous robustness guarantees for these methods.

Method: The framework uses randomized smoothing to sparsify and smooth attribution maps, reformulating the task as a segmentation problem to certify pixel importance against perturbations.

Result: The proposed method demonstrates robust, interpretable, and faithful attributions across 12 attribution methods and 5 ImageNet models.

Conclusion: The certification framework enables reliable use of attribution methods in downstream tasks by ensuring robustness and interpretability.

Abstract: Post-hoc attribution methods aim to explain deep learning predictions by
highlighting influential input pixels. However, these explanations are highly
non-robust: small, imperceptible input perturbations can drastically alter the
attribution map while maintaining the same prediction. This vulnerability
undermines their trustworthiness and calls for rigorous robustness guarantees
of pixel-level attribution scores. We introduce the first certification
framework that guarantees pixel-level robustness for any black-box attribution
method using randomized smoothing. By sparsifying and smoothing attribution
maps, we reformulate the task as a segmentation problem and certify each
pixel's importance against $\ell_2$-bounded perturbations. We further propose
three evaluation metrics to assess certified robustness, localization, and
faithfulness. An extensive evaluation of 12 attribution methods across 5
ImageNet models shows that our certified attributions are robust,
interpretable, and faithful, enabling reliable use in downstream tasks. Our
code is at https://github.com/AlaaAnani/certified-attributions.

</details>


### [62] [HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models](https://arxiv.org/abs/2506.15065)
*Trishna Chakraborty,Udita Ghosh,Xiaopan Zhang,Fahim Faisal Niloy,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.LG

TL;DR: The paper studies hallucinations in LLM-based embodied agents, focusing on their occurrence, triggers, and model responses under scene-task inconsistencies. It introduces a probing set to evaluate 12 models, revealing limitations in handling infeasible tasks and offering guidance for improvement.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate hallucinations in LLM-based embodied agents, understanding their causes and impacts on long-horizon tasks, and to improve model robustness.

Method: Constructed a hallucination probing set from an existing benchmark, evaluated 12 models across two simulation environments, and analyzed their responses to scene-task inconsistencies.

Result: Models showed reasoning but failed to resolve inconsistencies, with hallucination rates up to 40x higher than base prompts.

Conclusion: The study highlights fundamental limitations in handling infeasible tasks and provides actionable insights for developing more reliable planning strategies.

Abstract: Large language models (LLMs) are increasingly being adopted as the cognitive
core of embodied agents. However, inherited hallucinations, which stem from
failures to ground user instructions in the observed physical environment, can
lead to navigation errors, such as searching for a refrigerator that does not
exist. In this paper, we present the first systematic study of hallucinations
in LLM-based embodied agents performing long-horizon tasks under scene-task
inconsistencies. Our goal is to understand to what extent hallucinations occur,
what types of inconsistencies trigger them, and how current models respond. To
achieve these goals, we construct a hallucination probing set by building on an
existing benchmark, capable of inducing hallucination rates up to 40x higher
than base prompts. Evaluating 12 models across two simulation environments, we
find that while models exhibit reasoning, they fail to resolve scene-task
inconsistencies-highlighting fundamental limitations in handling infeasible
tasks. We also provide actionable insights on ideal model behavior for each
scenario, offering guidance for developing more robust and reliable planning
strategies.

</details>


### [63] [Over-squashing in Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2506.15507)
*Ivan Marisca,Jacob Bamberger,Cesare Alippi,Michael M. Bronstein*

Main category: cs.LG

TL;DR: The paper formalizes spatiotemporal over-squashing in STGNNs, showing its unique challenges compared to static GNNs, and reveals counterintuitive propagation biases favoring distant temporal points.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored issue of over-squashing in STGNNs, which is exacerbated by the temporal dimension, and to understand its distinct characteristics.

Method: Theoretical analysis of spatiotemporal over-squashing, comparing it to static cases, and empirical validation on synthetic and real-world datasets.

Result: Convolutional STGNNs favor information from temporally distant points, and both time-and-space and time-then-space paradigms are equally affected.

Conclusion: The study provides insights into STGNN dynamics and offers principled guidance for more effective designs.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various
domains. However, recent theoretical advances have identified fundamental
limitations in their information propagation capabilities, such as
over-squashing, where distant nodes fail to effectively exchange information.
While extensively studied in static contexts, this issue remains unexplored in
Spatiotemporal GNNs (STGNNs), which process sequences associated with graph
nodes. Nonetheless, the temporal dimension amplifies this challenge by
increasing the information that must be propagated. In this work, we formalize
the spatiotemporal over-squashing problem and demonstrate its distinct
characteristics compared to the static case. Our analysis reveals that
counterintuitively, convolutional STGNNs favor information propagation from
points temporally distant rather than close in time. Moreover, we prove that
architectures that follow either time-and-space or time-then-space processing
paradigms are equally affected by this phenomenon, providing theoretical
justification for computationally efficient implementations. We validate our
findings on synthetic and real-world datasets, providing deeper insights into
their operational dynamics and principled guidance for more effective designs.

</details>


### [64] [Neural Canonical Polyadic Factorization for Traffic Analysis](https://arxiv.org/abs/2506.15079)
*Yikai Hou,Peng Tang*

Main category: cs.LG

TL;DR: The paper proposes a Neural Canonical Polyadic Factorization (NCPF) model for robust traffic data imputation by combining low-rank tensor algebra with deep learning.


<details>
  <summary>Details</summary>
Motivation: Accurate spatiotemporal traffic analysis is crucial for urban mobility, but missing data from sensor failures and sensing gaps hinders reliable modeling.

Method: NCPF integrates CP decomposition into neural architecture using learnable embeddings and hierarchical feature fusion with Hadamard products and MLPs.

Result: NCPF outperforms six state-of-the-art baselines on six urban traffic datasets.

Conclusion: NCPF unifies interpretable factor analysis with neural networks' nonlinear power, offering a flexible solution for traffic data imputation and supporting advanced transportation systems.

Abstract: Modern intelligent transportation systems rely on accurate spatiotemporal
traffic analysis to optimize urban mobility and infrastructure resilience.
However, pervasive missing data caused by sensor failures and heterogeneous
sensing gaps fundamentally hinders reliable traffic modeling. This paper
proposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes
low-rank tensor algebra with deep representation learning for robust traffic
data imputation. The model innovatively embeds CP decomposition into neural
architecture through learnable embedding projections, where sparse traffic
tensors are encoded into dense latent factors across road segments, time
intervals, and mobility metrics. A hierarchical feature fusion mechanism
employs Hadamard products to explicitly model multilinear interactions, while
stacked multilayer perceptron layers nonlinearly refine these representations
to capture complex spatiotemporal couplings. Extensive evaluations on six urban
traffic datasets demonstrate NCPF's superiority over six state-of-the-art
baselines. By unifying CP decomposition's interpretable factor analysis with
neural network's nonlinear expressive power, NCPF provides a principled yet
flexible approaches for high-dimensional traffic data imputation, offering
critical support for next-generation transportation digital twins and adaptive
traffic control systems.

</details>


### [65] [RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation](https://arxiv.org/abs/2506.15513)
*Le Vu Anh,Nguyen Viet Anh,Mehmet Dik,Luong Van Nghia*

Main category: cs.LG

TL;DR: RePCS is a model-agnostic method to detect if retrieval-augmented generation (RAG) systems rely on memorized data instead of retrieved context, using KL divergence between two inference paths.


<details>
  <summary>Details</summary>
Motivation: To address the issue of LLMs bypassing retrieved evidence and producing contaminated outputs in RAG systems.

Method: RePCS compares the output distributions of a parametric path (query-only) and a retrieval-augmented path (query + context) using KL divergence.

Result: Achieves a ROC-AUC of 0.918 on Prompt-WNQA, outperforming prior methods by 6.5%, with minimal latency overhead.

Conclusion: RePCS provides a lightweight, black-box safeguard for verifying meaningful retrieval use in RAG systems, crucial for safety-critical applications.

Abstract: Retrieval-augmented generation (RAG) has become a common strategy for
updating large language model (LLM) responses with current, external
information. However, models may still rely on memorized training data, bypass
the retrieved evidence, and produce contaminated outputs. We introduce
Retrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects
such behavior without requiring model access or retraining. RePCS compares two
inference paths: (i) a parametric path using only the query, and (ii) a
retrieval-augmented path using both the query and retrieved context by
computing the Kullback-Leibler (KL) divergence between their output
distributions. A low divergence suggests that the retrieved context had minimal
impact, indicating potential memorization. This procedure is model-agnostic,
requires no gradient or internal state access, and adds only a single
additional forward pass. We further derive PAC-style guarantees that link the
KL threshold to user-defined false positive and false negative rates. On the
Prompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result
outperforms the strongest prior method by 6.5 percentage points while keeping
latency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,
black-box safeguard to verify whether a RAG system meaningfully leverages
retrieval, making it especially valuable in safety-critical applications.

</details>


### [66] [Towards Reliable Forgetting: A Survey on Machine Unlearning Verification, Challenges, and Future Directions](https://arxiv.org/abs/2506.15115)
*Lulu Xue,Shengshan Hu,Wei Lu,Yan Shen,Dongxu Li,Peijin Guo,Ziqi Zhou,Minghui Li,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.LG

TL;DR: This paper surveys machine unlearning verification methods, proposing a taxonomy and analyzing their strengths, limitations, and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The need for privacy, security, and compliance (e.g., GDPR) drives the importance of verifying machine unlearning operations, which lacks a unified framework.

Method: The paper categorizes verification techniques into behavioral and parametric verification, analyzing representative methods in each category.

Result: A taxonomy is proposed, and existing methods are evaluated, revealing gaps and vulnerabilities in current approaches.

Conclusion: The paper highlights open problems in verification research to guide future development of robust and efficient unlearning verification mechanisms.

Abstract: With growing demands for privacy protection, security, and legal compliance
(e.g., GDPR), machine unlearning has emerged as a critical technique for
ensuring the controllability and regulatory alignment of machine learning
models. However, a fundamental challenge in this field lies in effectively
verifying whether unlearning operations have been successfully and thoroughly
executed. Despite a growing body of work on unlearning techniques, verification
methodologies remain comparatively underexplored and often fragmented. Existing
approaches lack a unified taxonomy and a systematic framework for evaluation.
To bridge this gap, this paper presents the first structured survey of machine
unlearning verification methods. We propose a taxonomy that organizes current
techniques into two principal categories -- behavioral verification and
parametric verification -- based on the type of evidence used to assess
unlearning fidelity. We examine representative methods within each category,
analyze their underlying assumptions, strengths, and limitations, and identify
potential vulnerabilities in practical deployment. In closing, we articulate a
set of open problems in current verification research, aiming to provide a
foundation for developing more robust, efficient, and theoretically grounded
unlearning verification mechanisms.

</details>


### [67] [Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework](https://arxiv.org/abs/2506.15538)
*Laura Kopf,Nils Feldhus,Kirill Bykov,Philine Lou Bommer,Anna Hedström,Marina M. -C. Höhne,Oliver Eberle*

Main category: cs.LG

TL;DR: PRISM is a new framework for identifying and scoring polysemantic features in neural networks, improving interpretability by capturing multiple concepts per feature.


<details>
  <summary>Details</summary>
Motivation: Current feature description methods are limited by assumptions of monosemanticity and lack robustness, restricting their ability to fully explain model behavior.

Method: PRISM introduces a framework to identify and score polysemantic features, providing nuanced descriptions for both polysemantic and monosemantic features.

Result: PRISM outperforms existing methods in description quality and polysemanticity capture, as shown in benchmarks with language models.

Conclusion: PRISM enhances interpretability by addressing polysemanticity, offering more accurate and faithful feature descriptions.

Abstract: Automated interpretability research aims to identify concepts encoded in
neural network features to enhance human understanding of model behavior.
Current feature description methods face two critical challenges: limited
robustness and the flawed assumption that each neuron encodes only a single
concept (monosemanticity), despite growing evidence that neurons are often
polysemantic. This assumption restricts the expressiveness of feature
descriptions and limits their ability to capture the full range of behaviors
encoded in model internals. To address this, we introduce Polysemantic FeatuRe
Identification and Scoring Method (PRISM), a novel framework that captures the
inherent complexity of neural network features. Unlike prior approaches that
assign a single description per feature, PRISM provides more nuanced
descriptions for both polysemantic and monosemantic features. We apply PRISM to
language models and, through extensive benchmarking against existing methods,
demonstrate that our approach produces more accurate and faithful feature
descriptions, improving both overall description quality (via a description
score) and the ability to capture distinct concepts when polysemanticity is
present (via a polysemanticity score).

</details>


### [68] [ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed Machine Learning](https://arxiv.org/abs/2506.15181)
*Bing Liu,Chengcheng Zhao,Li Chai,Peng Cheng,Yaonan Wang*

Main category: cs.LG

TL;DR: Proposes ImprovDML, a decentralized DML framework that ensures privacy and Byzantine resilience without significant accuracy loss. Uses resilient vector consensus and Gaussian noise for gradients, offering tighter error bounds and better privacy-accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between Byzantine resilience, privacy preservation, and model accuracy in distributed machine learning.

Method: Leverages resilient vector consensus for aggregation and adds multivariate Gaussian noise to gradients for privacy.

Result: Achieves high model accuracy with tighter error bounds and improved privacy-accuracy trade-offs compared to existing methods.

Conclusion: ImprovDML effectively balances privacy, resilience, and accuracy, validated by theoretical guarantees and simulations.

Abstract: Jointly addressing Byzantine attacks and privacy leakage in distributed
machine learning (DML) has become an important issue. A common strategy
involves integrating Byzantine-resilient aggregation rules with differential
privacy mechanisms. However, the incorporation of these techniques often
results in a significant degradation in model accuracy. To address this issue,
we propose a decentralized DML framework, named ImprovDML, that achieves high
model accuracy while simultaneously ensuring privacy preservation and
resilience to Byzantine attacks. The framework leverages a kind of resilient
vector consensus algorithms that can compute a point within the normal
(non-Byzantine) agents' convex hull for resilient aggregation at each
iteration. Then, multivariate Gaussian noises are introduced to the gradients
for privacy preservation. We provide convergence guarantees and derive
asymptotic learning error bounds under non-convex settings, which are tighter
than those reported in existing works. For the privacy analysis, we adopt the
notion of concentrated geo-privacy, which quantifies privacy preservation based
on the Euclidean distance between inputs. We demonstrate that it enables an
improved trade-off between privacy preservation and model accuracy compared to
differential privacy. Finally, numerical simulations validate our theoretical
results.

</details>


### [69] [Learning Algorithms in the Limit](https://arxiv.org/abs/2506.15543)
*Hristo Papazov,Nicolas Flammarion*

Main category: cs.LG

TL;DR: The paper extends Gold's inductive inference framework to include computational observations and restricted input sources, enabling the learning of general recursive functions under realistic constraints.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional input-output observations in learning general recursive functions by incorporating computational complexity constraints and additional observation types.

Method: Introduces Time-Bound Observations and Policy-Trajectory Observations, and builds a formal framework for learning from computational agents' observations.

Result: Shows that learning computable functions from policy trajectories reduces to learning rational functions, but computable or polynomial-mass characteristic sets are impossible for linear-time computable functions.

Conclusion: The study reveals connections to finite-state transducer inference and highlights the challenges in learning under computational constraints.

Abstract: This paper studies the problem of learning computable functions in the limit
by extending Gold's inductive inference framework to incorporate
\textit{computational observations} and \textit{restricted input sources}.
Complimentary to the traditional Input-Output Observations, we introduce
Time-Bound Observations, and Policy-Trajectory Observations to study the
learnability of general recursive functions under more realistic constraints.
While input-output observations do not suffice for learning the class of
general recursive functions in the limit, we overcome this learning barrier by
imposing computational complexity constraints or supplementing with approximate
time-bound observations. Further, we build a formal framework around
observations of \textit{computational agents} and show that learning computable
functions from policy trajectories reduces to learning rational functions from
input and output, thereby revealing interesting connections to finite-state
transducer inference. On the negative side, we show that computable or
polynomial-mass characteristic sets cannot exist for the class of linear-time
computable functions even for policy-trajectory observations.

</details>


### [70] [Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors](https://arxiv.org/abs/2506.15190)
*Jiyi Wang,Jingyang Ke,Bo Dai,Anqi Wu*

Main category: cs.LG

TL;DR: SKIL introduces a reinforcement learning-based imitation framework to infer interpretable skill sets and parameterize policies as dynamic mixtures of these skills, improving behavior understanding beyond traditional discrete models.


<details>
  <summary>Details</summary>
Motivation: Existing behavior-segmentation methods oversimplify animal behavior by imposing discrete syllables, failing to reflect the dynamic recombination of motor primitives.

Method: SKIL uses representation learning on transition probabilities to infer latent skill sets and parameterizes policies as dynamic mixtures of these skills.

Result: The method identifies reusable skill components, learns continuously evolving policies, and generates realistic trajectories, outperforming traditional discrete models.

Conclusion: SKIL provides a principled account of how complex animal behaviors emerge from dynamic combinations of fundamental motor primitives.

Abstract: Animals flexibly recombine a finite set of core motor primitives to meet
diverse task demands, but existing behavior-segmentation methods oversimplify
this process by imposing discrete syllables under restrictive generative
assumptions. To reflect the animal behavior generation procedure, we introduce
skill-based imitation learning (SKIL) for behavior understanding, a
reinforcement learning-based imitation framework that (1) infers interpretable
skill sets, i.e., latent basis functions of behavior, by leveraging
representation learning on transition probabilities, and (2) parameterizes
policies as dynamic mixtures of these skills. We validate our approach on a
simple grid world, a discrete labyrinth, and unconstrained videos of freely
moving animals. Across tasks, it identifies reusable skill components, learns
continuously evolving compositional policies, and generates realistic
trajectories beyond the capabilities of traditional discrete models. By
exploiting generative behavior modeling with compositional representations, our
method offers a concise, principled account of how complex animal behaviors
emerge from dynamic combinations of fundamental motor primitives.

</details>


### [71] [DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones](https://arxiv.org/abs/2506.15554)
*Akhil Singampalli,Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: DAILOC is a domain-incremental learning framework for Wi-Fi fingerprinting-based indoor localization, addressing device and temporal domain shifts with disentanglement and memory-guided alignment, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in Wi-Fi fingerprinting due to device heterogeneity and temporal variations, leading to poor generalization and catastrophic forgetting.

Method: Proposes DAILOC, using a multi-level variational autoencoder for disentanglement and memory-guided class latent alignment to mitigate domain shifts and forgetting.

Result: Achieves up to 2.74x lower average error and 4.6x lower worst-case error compared to existing methods.

Conclusion: DAILOC effectively addresses domain shifts and catastrophic forgetting, significantly improving indoor localization performance.

Abstract: Wi-Fi fingerprinting-based indoor localization faces significant challenges
in real-world deployments due to domain shifts arising from device
heterogeneity and temporal variations within indoor environments. Existing
approaches often address these issues independently, resulting in poor
generalization and susceptibility to catastrophic forgetting over time. In this
work, we propose DAILOC, a novel domain-incremental learning framework that
jointly addresses both temporal and device-induced domain shifts. DAILOC
introduces a novel disentanglement strategy that separates domain shifts from
location-relevant features using a multi-level variational autoencoder.
Additionally, we introduce a novel memory-guided class latent alignment
mechanism to address the effects of catastrophic forgetting over time.
Experiments across multiple smartphones, buildings, and time instances
demonstrate that DAILOC significantly outperforms state-of-the-art methods,
achieving up to 2.74x lower average error and 4.6x lower worst-case error.

</details>


### [72] [Interpretability and Generalization Bounds for Learning Spatial Physics](https://arxiv.org/abs/2506.15199)
*Alejandro Francisco Queiruga,Theo Gutman-Solo,Shuai Jiang*

Main category: cs.LG

TL;DR: The paper evaluates ML techniques for solving the 1D Poisson equation, emphasizing quantitative accuracy and generalization. It identifies data function space as critical, derives generalization bounds, and shows empirical validation. It also introduces a new interpretability lens and cross-validation technique.


<details>
  <summary>Details</summary>
Motivation: To rigorously assess ML methods for scientific problems, focusing on accuracy and generalization in solving differential equations like the 1D Poisson equation.

Method: Applies numerical analysis rigor to ML, quantifying accuracy of techniques for the 1D Poisson equation. Analyzes training dynamics, derives generalization bounds, and tests models like linear models, neural networks, and physics-specific operators.

Result: Generalization to the true equation isn't guaranteed; different models show opposing behaviors. Green's function representations can be extracted from black-box models.

Conclusion: Proposes a new cross-validation technique for generalization in physical systems, suggesting the Poisson equation as a benchmark for future methods.

Abstract: While there are many applications of ML to scientific problems that look
promising, visuals can be deceiving. For scientific applications, actual
quantitative accuracy is crucial. This work applies the rigor of numerical
analysis for differential equations to machine learning by specifically
quantifying the accuracy of applying different ML techniques to the elementary
1D Poisson differential equation. Beyond the quantity and discretization of
data, we identify that the function space of the data is critical to the
generalization of the model. We prove generalization bounds and convergence
rates under finite data discretizations and restricted training data subspaces
by analyzing the training dynamics and deriving optimal parameters for both a
white-box differential equation discovery method and a black-box linear model.
The analytically derived generalization bounds are replicated empirically.
Similar lack of generalization is empirically demonstrated for deep linear
models, shallow neural networks, and physics-specific DeepONets and Neural
Operators. We theoretically and empirically demonstrate that generalization to
the true physical equation is not guaranteed in each explored case.
Surprisingly, we find that different classes of models can exhibit opposing
generalization behaviors. Based on our theoretical analysis, we also
demonstrate a new mechanistic interpretability lens on scientific models
whereby Green's function representations can be extracted from the weights of
black-box models. Our results inform a new cross-validation technique for
measuring generalization in physical systems. We propose applying it to the
Poisson equation as an evaluation benchmark of future methods.

</details>


### [73] [Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates](https://arxiv.org/abs/2506.15559)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: LogNet, a logic gate-based framework, enhances interpretability and performance in DL-based indoor localization by identifying influential APs and diagnosing noise impact.


<details>
  <summary>Details</summary>
Motivation: Existing DL models for indoor localization lack interpretability, hindering understanding of temporal variations and long-term reliability.

Method: LogNet uses logic gates to transparently reason about AP influence and noise disruption in DL models.

Result: LogNet reduces localization error (1.1x-2.8x), model size (3.4x-43.3x), and latency (1.5x-3.6x) compared to prior models.

Conclusion: LogNet improves interpretability and performance, making DL-based indoor localization more reliable for long-term use.

Abstract: Indoor localization using deep learning (DL) has demonstrated strong accuracy
in mapping Wi-Fi RSS fingerprints to physical locations; however, most existing
DL frameworks function as black-box models, offering limited insight into how
predictions are made or how models respond to real-world noise over time. This
lack of interpretability hampers our ability to understand the impact of
temporal variations - caused by environmental dynamics - and to adapt models
for long-term reliability. To address this, we introduce LogNet, a novel logic
gate-based framework designed to interpret and enhance DL-based indoor
localization. LogNet enables transparent reasoning by identifying which access
points (APs) are most influential for each reference point (RP) and reveals how
environmental noise disrupts DL-driven localization decisions. This
interpretability allows us to trace and diagnose model failures and adapt DL
systems for more stable long-term deployments. Evaluations across multiple
real-world building floorplans and over two years of temporal variation show
that LogNet not only interprets the internal behavior of DL models but also
improves performance-achieving up to 1.1x to 2.8x lower localization error,
3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to
prior DL-based models.

</details>


### [74] [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606)
*Gabrel J. Perin,Runjin Chen,Xuxi Chen,Nina S. T. Hirata,Zhangyang Wang,Junyuan Hong*

Main category: cs.LG

TL;DR: LoX enhances LLM safety robustness by extrapolating low-rank subspaces, reducing attack success rates by 11-54%.


<details>
  <summary>Details</summary>
Motivation: Addressing vulnerabilities in aligned LLMs where safety protections can be undermined by fine-tuning, even with benign data.

Method: Proposes Low-Rank Extrapolation (LoX), a training-free method to extrapolate safety-critical subspaces in LLM parameters.

Result: LoX significantly improves robustness against fine-tuning attacks, reducing attack success rates by 11-54%.

Conclusion: LoX effectively moves LLM parameters to a flatter zone, making them less sensitive to perturbations while preserving adaptability.

Abstract: Large Language Models (LLMs) have become indispensable in real-world
applications. However, their widespread adoption raises significant safety
concerns, particularly in responding to socially harmful questions. Despite
substantial efforts to improve model safety through alignment, aligned models
can still have their safety protections undermined by subsequent fine-tuning -
even when the additional training data appears benign. In this paper, we
empirically demonstrate that this vulnerability stems from the sensitivity of
safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building
on this insight, we propose a novel training-free method, termed Low-Rank
Extrapolation (LoX), to enhance safety robustness by extrapolating the safety
subspace of an aligned LLM. Our experimental results confirm the effectiveness
of LoX, demonstrating significant improvements in robustness against both
benign and malicious fine-tuning attacks while preserving the model's
adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute
reductions in attack success rates (ASR) facing benign or malicious fine-tuning
attacks. By investigating the ASR landscape of parameters, we attribute the
success of LoX to that the extrapolation moves LLM parameters to a flatter
zone, thereby less sensitive to perturbations. The code is available at
github.com/VITA-Group/LoX.

</details>


### [75] [Centroid Approximation for Byzantine-Tolerant Federated Learning](https://arxiv.org/abs/2506.15264)
*Mélanie Cambus,Darya Melnyk,Tijana Milentijević,Stefan Schmid*

Main category: cs.LG

TL;DR: The paper explores the robustness of federated learning against Byzantine behavior, revealing a tradeoff between averaging and validity conditions. It provides theoretical bounds on centroid approximation and introduces a new algorithm for better performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether federated learning is resilient to Byzantine faults and to understand the tradeoff between averaging and validity conditions in distributed computing.

Method: The study uses theoretical analysis to derive lower and upper bounds on centroid approximation under validity conditions. It also introduces a new algorithm for convex validity and validates results empirically in federated learning settings.

Result: The paper establishes a lower bound of min{(n-t)/t, √d} and an upper bound of 2min{n, √d} for centroid approximation. A new algorithm achieves a √2d-approximation under convex validity, proving tightness of existing bounds.

Conclusion: Federated learning's robustness to Byzantine behavior is limited by tradeoffs between averaging and validity. The derived bounds and new algorithm provide insights and improvements for practical implementations.

Abstract: Federated learning allows each client to keep its data locally when training
machine learning models in a distributed setting. Significant recent research
established the requirements that the input must satisfy in order to guarantee
convergence of the training loop. This line of work uses averaging as the
aggregation rule for the training models. In particular, we are interested in
whether federated learning is robust to Byzantine behavior, and observe and
investigate a tradeoff between the average/centroid and the validity conditions
from distributed computing. We show that the various validity conditions alone
do not guarantee a good approximation of the average. Furthermore, we show that
reaching good approximation does not give good results in experimental settings
due to possible Byzantine outliers. Our main contribution is the first lower
bound of $\min\{\frac{n-t}{t},\sqrt{d}\}$ on the centroid approximation under
box validity that is often considered in the literature, where $n$ is the
number of clients, $t$ the upper bound on the number of Byzantine faults, and
$d$ is the dimension of the machine learning model. We complement this lower
bound by an upper bound of $2\min\{n,\sqrt{d}\}$, by providing a new analysis
for the case $n<d$. In addition, we present a new algorithm that achieves a
$\sqrt{2d}$-approximation under convex validity, which also proves that the
existing lower bound in the literature is tight. We show that all presented
bounds can also be achieved in the distributed peer-to-peer setting. We
complement our analytical results with empirical evaluations in federated
stochastic gradient descent and federated averaging settings.

</details>


### [76] [GFLC: Graph-based Fairness-aware Label Correction for Fair Classification](https://arxiv.org/abs/2506.15620)
*Modar Sulaiman,Kallol Roy*

Main category: cs.LG

TL;DR: The paper introduces GFLC, a method to correct biased and noisy labels in ML datasets while ensuring fairness, improving both performance and fairness metrics.


<details>
  <summary>Details</summary>
Motivation: Fairness in ML is crucial for trustworthy AI systems, but biased/noisy labels in training data undermine fairness and performance.

Method: GFLC combines prediction confidence, graph-based regularization (Ricci-flow-optimized graph Laplacians), and demographic parity incentives.

Result: Experiments show GFLC improves the trade-off between performance and fairness compared to baselines.

Conclusion: GFLC effectively addresses label bias and enhances fairness in ML models.

Abstract: Fairness in machine learning (ML) has a critical importance for building
trustworthy machine learning system as artificial intelligence (AI) systems
increasingly impact various aspects of society, including healthcare decisions
and legal judgments. Moreover, numerous studies demonstrate evidence of unfair
outcomes in ML and the need for more robust fairness-aware methods. However,
the data we use to train and develop debiasing techniques often contains biased
and noisy labels. As a result, the label bias in the training data affects
model performance and misrepresents the fairness of classifiers during testing.
To tackle this problem, our paper presents Graph-based Fairness-aware Label
Correction (GFLC), an efficient method for correcting label noise while
preserving demographic parity in datasets. In particular, our approach combines
three key components: prediction confidence measure, graph-based regularization
through Ricci-flow-optimized graph Laplacians, and explicit demographic parity
incentives. Our experimental findings show the effectiveness of our proposed
approach and show significant improvements in the trade-off between performance
and fairness metrics compared to the baseline.

</details>


### [77] [Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction](https://arxiv.org/abs/2506.15626)
*Vincent Roca,Marc Tommasi,Paul Andrey,Aurélien Bellet,Markus D. Schirmer,Hilde Henon,Laurent Puy,Julien Ramon,Grégory Kuchcinski,Martin Bretzner,Renaud Lopes*

Main category: cs.LG

TL;DR: Federated learning (FL) is effective for BrainAGE estimation in stroke patients, outperforming single-site models and showing strong associations with vascular risk factors and functional outcomes.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in training robust BrainAGE models by evaluating FL's performance and its clinical relevance in stroke patients.

Method: Used FLAIR images from 1674 patients across 16 centers, comparing centralized learning, FL, and single-site learning. Examined associations with vascular risk factors and functional outcomes.

Result: FL outperformed single-site models, with BrainAGE linked to diabetes mellitus and post-stroke recovery.

Conclusion: FL enables accurate BrainAGE predictions without data centralization, proving valuable for stroke care prognosis.

Abstract: $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a
neuroimaging biomarker reflecting brain health. However, training robust
BrainAGE models requires large datasets, often restricted by privacy concerns.
This study evaluates the performance of federated learning (FL) for BrainAGE
estimation in ischemic stroke patients treated with mechanical thrombectomy,
and investigates its association with clinical phenotypes and functional
outcomes.
  $\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients
across 16 hospital centers. We implemented standard machine learning and deep
learning models for BrainAGE estimates under three data management strategies:
centralized learning (pooled data), FL (local training at each site), and
single-site learning. We reported prediction errors and examined associations
between BrainAGE and vascular risk factors (e.g., diabetes mellitus,
hypertension, smoking), as well as functional outcomes at three months
post-stroke. Logistic regression evaluated BrainAGE's predictive value for
these outcomes, adjusting for age, sex, vascular risk factors, stroke severity,
time between MRI and arterial puncture, prior intravenous thrombolysis, and
recanalisation outcome.
  $\textbf{Results:}$ While centralized learning yielded the most accurate
predictions, FL consistently outperformed single-site models. BrainAGE was
significantly higher in patients with diabetes mellitus across all models.
Comparisons between patients with good and poor functional outcomes, and
multivariate predictions of these outcomes showed the significance of the
association between BrainAGE and post-stroke recovery.
  $\textbf{Conclusion:}$ FL enables accurate age predictions without data
centralization. The strong association between BrainAGE, vascular risk factors,
and post-stroke recovery highlights its potential for prognostic modeling in
stroke care.

</details>


### [78] [DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for Optimizing Large-Scale EV Charging Infrastructure](https://arxiv.org/abs/2506.15289)
*Chuan Li,Shunyu Zhao,Vincent Gauthier,Hassine Moungla*

Main category: cs.LG

TL;DR: DOVA-PATBM is a geo-computational framework for optimizing EV charging infrastructure, balancing urban, suburban, and rural needs. It improves coverage, reduces travel distance for low-income residents, and ensures power constraints, scalable for national use.


<details>
  <summary>Details</summary>
Motivation: The need for scalable infrastructure planning tools to address the conflicting requirements of urban, suburban, and rural areas for EV charging networks.

Method: Uses a hierarchical H3 grid, graph neural networks for intersection importance, Voronoi tessellation for coverage, and a greedy heuristic for site selection.

Result: In Georgia, USA, it increased 30 km coverage by 12%, halved travel distance for low-income residents, and met power constraints.

Conclusion: DOVA-PATBM bridges academic optimization and deployable policy, proving effective for large-scale EV infrastructure planning.

Abstract: The accelerating uptake of battery-electric vehicles demands infrastructure
planning tools that are both data-rich and geographically scalable. Whereas
most prior studies optimise charging locations for single cities, state-wide
and national networks must reconcile the conflicting requirements of dense
metropolitan cores, car-dependent exurbs, and power-constrained rural
corridors.
  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,
Adaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework
that unifies these contexts in a single pipeline. The method rasterises
heterogeneous data (roads, population, night lights, POIs, and feeder lines)
onto a hierarchical H3 grid, infers intersection importance with a
zone-normalised graph neural network centrality model, and overlays a Voronoi
tessellation that guarantees at least one five-port DC fast charger within
every 30 km radius. Hourly arrival profiles, learned from loop-detector and
floating-car traces, feed a finite M/M/c queue to size ports under
feeder-capacity and outage-risk constraints. A greedy maximal-coverage
heuristic with income-weighted penalties then selects the minimum number of
sites that satisfy coverage and equity targets.
  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile
coverage by 12 percentage points, (ii) halves the mean distance that low-income
residents travel to the nearest charger, and (iii) meets sub-transmission
headroom everywhere -- all while remaining computationally tractable for
national-scale roll-outs. These results demonstrate that a tightly integrated,
GNN-driven, multi-resolution approach can bridge the gap between academic
optimisation and deployable infrastructure policy.

</details>


### [79] [AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning](https://arxiv.org/abs/2506.15651)
*Tevin Wang,Chenyan Xiong*

Main category: cs.LG

TL;DR: AutoRule automates rule extraction from human feedback for RLHF, improving model performance and reducing reward hacking.


<details>
  <summary>Details</summary>
Motivation: Current RLHF methods rely on manual rule engineering, which is inefficient. AutoRule aims to automate this process.

Method: AutoRule extracts rules in three stages: interpreting preferences, identifying candidate rules, and synthesizing them. It uses rule-based rewards alongside learned rewards.

Result: AutoRule improves win rates by 28.6% on AlpacaEval2.0 and gains 6.1% on MT-Bench, with reduced reward hacking.

Conclusion: AutoRule effectively automates rule extraction, enhancing RLHF performance and aligning rules with dataset preferences.

Abstract: Rule-based rewards offer a promising strategy for improving reinforcement
learning from human feedback (RLHF), but current approaches often rely on
manual rule engineering. We present AutoRule, a fully automated method for
extracting rules from preference feedback and formulating them into rule-based
rewards. AutoRule extraction operates in three stages: it leverages a reasoning
model to interpret user preferences, identifies candidate rules from the
reasoning chain of these interpretations, and synthesizes them into a unified
rule set. Leveraging the finalized rule set, we employ language-model verifiers
to compute the fraction of rules satisfied by each output, using this metric as
an auxiliary reward alongside the learned reward model during policy
optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\%
relative improvement in length-controlled win rate on AlpacaEval2.0, and a
6.1\% relative gain in second-turn performance on a held-out MT-Bench subset,
compared to a GRPO baseline trained with the same learned reward model but
without the rule-based auxiliary reward. Our analysis confirms that the
extracted rules exhibit good agreement with dataset preference. We find that
AutoRule demonstrates reduced reward hacking compared to a learned reward model
when run over two episodes. Finally, our case study suggests that the extracted
rules capture unique qualities valued in different datasets. The extracted
rules are provided in the appendix, and the code is open-sourced at
https://github.com/cxcscmu/AutoRule.

</details>


### [80] [Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance](https://arxiv.org/abs/2506.15305)
*Qingkai Zhang,L. Jeff Hong,Houmin Yan*

Main category: cs.LG

TL;DR: The paper proposes a credit risk management framework for 3PL-led SCF in CBEC, using generative AI (QRGMM and DeepFM) to assess risk and determine loan sizes, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Financing challenges for SMEs in CBEC due to limited credit histories, addressed by leveraging 3PL-led SCF and in-transit inventory as collateral.

Method: Combines QRGMM for sales distribution modeling and DeepFM for covariate interactions, creating a unified framework for risk estimation and loan determination.

Result: Validated efficacy through synthetic and real-world data, improving credit risk assessment and SME access to capital.

Conclusion: Pioneers generative AI in CBEC SCF, enhancing credit practices and supporting SMEs.

Abstract: The rapid expansion of cross-border e-commerce (CBEC) has created significant
opportunities for small and medium-sized enterprises (SMEs), yet financing
remains a critical challenge due to SMEs' limited credit histories. Third-party
logistics (3PL)-led supply chain finance (SCF) has emerged as a promising
solution, leveraging in-transit inventory as collateral. We propose an advanced
credit risk management framework tailored for 3PL-led SCF, addressing the dual
challenges of credit risk assessment and loan size determination. Specifically,
we leverage conditional generative modeling of sales distributions through
Quantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for
risk estimation. We propose a unified framework that enables flexible
estimation of multiple risk measures while introducing a functional risk
measure formulation that systematically captures the relationship between these
risk measures and varying loan levels, supported by theoretical guarantees. To
capture complex covariate interactions in e-commerce sales data, we integrate
QRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on
synthetic and real-world data validate the efficacy of our model for credit
risk assessment and loan size determination. This study represents a pioneering
application of generative AI in CBEC SCF risk management, offering a solid
foundation for enhanced credit practices and improved SME access to capital.

</details>


### [81] [Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679)
*Xiaoqing Sun,Alessandro Stolfo,Joshua Engels,Ben Wu,Senthooran Rajamanoharan,Mrinmaya Sachan,Max Tegmark*

Main category: cs.LG

TL;DR: SAEs often produce dense latents, which are meaningful and functional, not just training artifacts. They form antipodal pairs, serve roles like position tracking, and evolve semantically across layers.


<details>
  <summary>Details</summary>
Motivation: To investigate whether dense latents in SAEs are undesirable artifacts or meaningful representations in language models.

Method: Systematic analysis of dense latents' geometry, function, and origin, including ablation studies and taxonomy development.

Result: Dense latents are persistent, form antipodal pairs, and serve functional roles (e.g., position tracking, semantic shifts).

Conclusion: Dense latents are intrinsic to the residual space and play functional roles in language models, not mere noise.

Abstract: Sparse autoencoders (SAEs) are designed to extract interpretable features
from language models by enforcing a sparsity constraint. Ideally, training an
SAE would yield latents that are both sparse and semantically meaningful.
However, many SAE latents activate frequently (i.e., are \emph{dense}), raising
concerns that they may be undesirable artifacts of the training procedure. In
this work, we systematically investigate the geometry, function, and origin of
dense latents and show that they are not only persistent but often reflect
meaningful model representations. We first demonstrate that dense latents tend
to form antipodal pairs that reconstruct specific directions in the residual
stream, and that ablating their subspace suppresses the emergence of new dense
features in retrained SAEs -- suggesting that high density features are an
intrinsic property of the residual space. We then introduce a taxonomy of dense
latents, identifying classes tied to position tracking, context binding,
entropy regulation, letter-specific output signals, part-of-speech, and
principal component reconstruction. Finally, we analyze how these features
evolve across layers, revealing a shift from structural features in early
layers, to semantic features in mid layers, and finally to output-oriented
signals in the last layers of the model. Our findings indicate that dense
latents serve functional roles in language model computation and should not be
dismissed as training noise.

</details>


### [82] [SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language Models Using Forward-Only Passes](https://arxiv.org/abs/2506.15307)
*Jinglong Luo,Zhuo Zhang,Yehong Zhang,Shiyu Liu,Ye Dong,Xun Zhou,Hui Wang,Yue Yu,Zenglin Xu*

Main category: cs.LG

TL;DR: SecFwT is a framework for efficient, privacy-preserving fine-tuning of large language models (LLMs) using secure multi-party computation (MPC), addressing computational challenges and inefficiencies in MPC settings.


<details>
  <summary>Details</summary>
Motivation: The scarcity of accessible training data in privacy-sensitive domains like healthcare and finance limits LLM adaptation. MPC-based privacy-preserving machine learning is powerful but faces computational challenges in fine-tuning LLMs.

Method: SecFwT introduces a forward-only tuning paradigm to eliminate backward and optimizer computations and uses MPC-friendly Random Feature Attention to approximate softmax attention, reducing non-linear operations.

Result: Experimental results show SecFwT significantly improves efficiency and privacy preservation, enabling scalable and secure LLM fine-tuning.

Conclusion: SecFwT successfully addresses the computational and efficiency challenges of MPC-based privacy-preserving LLM fine-tuning, making it viable for privacy-critical applications.

Abstract: Large language models (LLMs) have transformed numerous fields, yet their
adaptation to specialized tasks in privacy-sensitive domains, such as
healthcare and finance, is constrained by the scarcity of accessible training
data due to stringent privacy requirements. Secure multi-party computation
(MPC)-based privacy-preserving machine learning offers a powerful approach to
protect both model parameters and user data, but its application to LLMs has
been largely limited to inference, as fine-tuning introduces significant
computational challenges, particularly in privacy-preserving backward
propagation and optimizer operations. This paper identifies two primary
obstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the
substantial computational overhead of backward and optimizer processes, and (2)
the inefficiency of softmax-based attention mechanisms in MPC settings. To
address these challenges, we propose SecFwT, the first MPC-based framework
designed for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a
forward-only tuning paradigm to eliminate backward and optimizer computations
and employs MPC-friendly Random Feature Attention to approximate softmax
attention, significantly reducing costly non-linear operations and
computational complexity. Experimental results demonstrate that SecFwT delivers
substantial improvements in efficiency and privacy preservation, enabling
scalable and secure fine-tuning of LLMs for privacy-critical applications.

</details>


### [83] [Universal Laboratory Model: prognosis of abnormal clinical outcomes based on routine tests](https://arxiv.org/abs/2506.15330)
*Pavel Karpov,Ilya Petrenkov,Ruslan Raiman*

Main category: cs.LG

TL;DR: The paper proposes a method to predict abnormal values of unprescribed clinical tests using performed test results, focusing on CBC and routine biochemical panels, achieving up to 8% AUC improvement.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis is crucial, and predicting abnormal test values from existing data can make it more accessible. CBC tests are widely used, but missing data is common.

Method: Formulates the problem as a set translation task, using GPT-like embeddings for labels and values, handling missing data without estimation.

Result: Achieves up to 8% AUC improvement in predicting high uric acid, glucose, cholesterol, and low ferritin levels.

Conclusion: The method effectively bridges LLMs with tabular data, improving clinical predictions without needing to estimate missing values.

Abstract: Clinical laboratory results are ubiquitous in any diagnosis making.
Predicting abnormal values of not prescribed tests based on the results of
performed tests looks intriguing, as it would be possible to make early
diagnosis available to everyone. The special place is taken by the Common Blood
Count (CBC) test, as it is the most widely used clinical procedure. Combining
routine biochemical panels with CBC presents a set of test-value pairs that
varies from patient to patient, or, in common settings, a table with missing
values. Here we formulate a tabular modeling problem as a set translation
problem where the source set comprises pairs of GPT-like label column embedding
and its corresponding value while the target set consists of the same type
embeddings only. The proposed approach can effectively deal with missing values
without implicitly estimating them and bridges the world of LLM with the
tabular domain. Applying this method to clinical laboratory data, we achieve an
improvement up to 8% AUC for joint predictions of high uric acid, glucose,
cholesterol, and low ferritin levels.

</details>


### [84] [Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations](https://arxiv.org/abs/2506.15337)
*Naoki Matsumura,Yuta Yoshimoto,Yuto Iwasaki,Meguru Yamazaki,Yasufumi Sakai*

Main category: cs.LG

TL;DR: A novel knowledge distillation framework for neural network potentials (NNPs) avoids fine-tuning the teacher model, enabling better exploration of high-energy structures and reducing DFT calculations by 10x.


<details>
  <summary>Details</summary>
Motivation: Traditional KD methods fine-tune NNPs, increasing energy barriers and limiting high-energy structure data, which is crucial for stable MD simulations.

Method: Proposes a two-stage training: student NNP trained with data from an off-the-shelf teacher, then fine-tuned with a small DFT dataset.

Result: Achieves comparable or superior accuracy in reproducing physical properties for organic and inorganic materials while reducing DFT calculations.

Conclusion: The framework efficiently generates accurate NNPs with fewer DFT calculations, benefiting material property evaluations.

Abstract: Neural network potentials (NNPs) offer a powerful alternative to traditional
force fields for molecular dynamics (MD) simulations. Accurate and stable MD
simulations, crucial for evaluating material properties, require training data
encompassing both low-energy stable structures and high-energy structures.
Conventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as
a teacher model to generate training data for a student model. However, in
material-specific models, this fine-tuning process increases energy barriers,
making it difficult to create training data containing high-energy structures.
To address this, we propose a novel KD framework that leverages a
non-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy
landscape facilitates the exploration of a wider range of structures, including
the high-energy structures crucial for stable MD simulations. Our framework
employs a two-stage training process: first, the student NNP is trained with a
dataset generated by the off-the-shelf teacher; then, it is fine-tuned with a
smaller, high-accuracy density functional theory (DFT) dataset. We demonstrate
the effectiveness of our framework by applying it to both organic (polyethylene
glycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving
comparable or superior accuracy in reproducing physical properties compared to
existing methods. Importantly, our method reduces the number of expensive DFT
calculations by 10x compared to existing NNP generation methods, without
sacrificing accuracy.

</details>


### [85] [Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges](https://arxiv.org/abs/2506.15346)
*A. S. Stankevich,I. B. Petrov*

Main category: cs.LG

TL;DR: The paper introduces a conditional Image-to-Image Schrödinger Bridge (cI²SB) framework to improve deep learning-based Full Waveform Inversion (FWI) by addressing limitations of diffusion models, such as iterative sampling and heuristic output control.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models in FWI lack efficient ways to incorporate approximate velocity models and suffer from iterative, stochastic sampling. The paper aims to overcome these limitations.

Method: The authors propose cI²SB, extending the Image-to-Image Schrödinger Bridge (I²SB) to conditional sampling, enabling nonlinear drift learning between ground truth and smoothed velocity models.

Result: Experiments show cI²SB outperforms prior conditional diffusion models, achieving higher sample fidelity with fewer neural function evaluations (NFEs).

Conclusion: The cI²SB framework offers a more efficient and effective solution for FWI, validated by superior performance and reduced computational cost.

Abstract: Recent developments in application of deep learning models to acoustic Full
Waveform Inversion (FWI) are marked by the use of diffusion models as prior
distributions for Bayesian-like inference procedures. The advantage of these
methods is the ability to generate high-resolution samples, which are otherwise
unattainable with classical inversion methods or other deep learning-based
solutions. However, the iterative and stochastic nature of sampling from
diffusion models along with heuristic nature of output control remain limiting
factors for their applicability. For instance, an optimal way to include the
approximate velocity model into diffusion-based inversion scheme remains
unclear, even though it is considered an essential part of FWI pipeline. We
address the issue by employing a Schr\"odinger Bridge that interpolates between
the distributions of ground truth and smoothed velocity models. To facilitate
the learning of nonlinear drifts that transfer samples between distributions we
extend the concept of Image-to-Image Schr\"odinger Bridge
($\text{I}^2\text{SB}$) to conditional sampling, resulting in a conditional
Image-to-Image Schr\"odinger Bridge (c$\text{I}^2\text{SB}$) framework. To
validate our method, we assess its effectiveness in reconstructing the
reference velocity model from its smoothed approximation, coupled with the
observed seismic signal of fixed shape. Our experiments demonstrate that the
proposed solution outperforms our reimplementation of conditional diffusion
model suggested in earlier works, while requiring only a few neural function
evaluations (NFEs) to achieve sample fidelity superior to that attained with
supervised learning-based approach. The supplementary code implementing the
algorithms described in this paper can be found in the repository
https://github.com/stankevich-mipt/seismic_inversion_via_I2SB.

</details>


### [86] [Enhancing One-run Privacy Auditing with Quantile Regression-Based Membership Inference](https://arxiv.org/abs/2506.15349)
*Terrance Liu,Matteo Boglioni,Yiwei Fu,Shengyuan Hu,Pratiksha Thaker,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: The paper proposes a method to improve one-run DP auditing in black-box settings by using stronger membership inference attacks (MIA) with quantile regression, achieving tighter bounds efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing one-run DP auditing methods in black-box settings have a gap between empirical lower bounds and theoretical upper bounds, prompting the need for improved techniques.

Method: The approach incorporates quantile regression for stronger MIA to enhance one-run auditing in black-box settings, tested on DP-SGD-trained CIFAR-10 models.

Result: The proposed method achieves tighter empirical lower bounds while maintaining computational efficiency.

Conclusion: Using quantile regression for MIA improves one-run DP auditing in black-box settings, bridging the gap between empirical and theoretical bounds.

Abstract: Differential privacy (DP) auditing aims to provide empirical lower bounds on
the privacy guarantees of DP mechanisms like DP-SGD. While some existing
techniques require many training runs that are prohibitively costly, recent
work introduces one-run auditing approaches that effectively audit DP-SGD in
white-box settings while still being computationally efficient. However, in the
more practical black-box setting where gradients cannot be manipulated during
training and only the last model iterate is observed, prior work shows that
there is still a large gap between the empirical lower bounds and theoretical
upper bounds. Consequently, in this work, we study how incorporating approaches
for stronger membership inference attacks (MIA) can improve one-run auditing in
the black-box setting. Evaluating on image classification models trained on
CIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes
quantile regression for MIA, achieves tighter bounds while crucially
maintaining the computational efficiency of one-run methods.

</details>


### [87] [Sampling 3D Molecular Conformers with Diffusion Transformers](https://arxiv.org/abs/2506.15378)
*J. Thorben Frank,Winfried Ripken,Gregor Lied,Klaus-Robert Müller,Oliver T. Unke,Stefan Chmiela*

Main category: cs.LG

TL;DR: DiTMC adapts Diffusion Transformers (DiTs) for molecular conformer generation, addressing challenges like integrating graph data with 3D geometry and handling symmetries. It achieves state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To apply DiTs to molecular conformer generation, overcoming challenges like discrete graph integration, Euclidean symmetries, and variable molecule sizes.

Method: DiTMC uses a modular architecture separating 3D coordinate processing from atomic connectivity conditioning, with graph-based strategies and flexible attention mechanisms (non-equivariant and SO(3)-equivariant).

Result: DiTMC achieves state-of-the-art precision and physical validity on benchmarks (GEOM-QM9, -DRUGS, -XL).

Conclusion: The framework highlights the impact of architectural choices and symmetry priors on sample quality, offering directions for large-scale molecular generative modeling.

Abstract: Diffusion Transformers (DiTs) have demonstrated strong performance in
generative modeling, particularly in image synthesis, making them a compelling
choice for molecular conformer generation. However, applying DiTs to molecules
introduces novel challenges, such as integrating discrete molecular graph
information with continuous 3D geometry, handling Euclidean symmetries, and
designing conditioning mechanisms that generalize across molecules of varying
sizes and structures. We propose DiTMC, a framework that adapts DiTs to address
these challenges through a modular architecture that separates the processing
of 3D coordinates from conditioning on atomic connectivity. To this end, we
introduce two complementary graph-based conditioning strategies that integrate
seamlessly with the DiT architecture. These are combined with different
attention mechanisms, including both standard non-equivariant and
SO(3)-equivariant formulations, enabling flexible control over the trade-off
between between accuracy and computational efficiency. Experiments on standard
conformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC
achieves state-of-the-art precision and physical validity. Our results
highlight how architectural choices and symmetry priors affect sample quality
and efficiency, suggesting promising directions for large-scale generative
modeling of molecular structures. Code available at
https://github.com/ML4MolSim/dit_mc.

</details>


### [88] [Global Ground Metric Learning with Applications to scRNA data](https://arxiv.org/abs/2506.15383)
*Damin Kühn,Michael T. Schaub*

Main category: cs.LG

TL;DR: The paper proposes a novel method for learning ground metrics in optimal transport, addressing limitations of predefined and supervised approaches, and demonstrates its effectiveness in various tasks using scRNA-seq data.


<details>
  <summary>Details</summary>
Motivation: Traditional ground metrics in optimal transport are either predefined (e.g., Euclidean) or learned supervisedly, which lack flexibility or generalizability. The paper aims to overcome these limitations.

Method: The approach learns a global ground metric for arbitrary distributions over a shared metric space, requiring only class labels on a distribution-level for training.

Result: The learned metric improves optimal transport distances, enhancing performance in embedding, clustering, and classification tasks.

Conclusion: The method is effective and interpretable, as shown by its application to scRNA-seq data across multiple diseases.

Abstract: Optimal transport provides a robust framework for comparing probability
distributions. Its effectiveness is significantly influenced by the choice of
the underlying ground metric. Traditionally, the ground metric has either been
(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a
supervised way, by utilizing labeled data to learn a suitable ground metric for
enhanced task-specific performance. Yet, predefined metrics typically cannot
account for the inherent structure and varying importance of different features
in the data, and existing supervised approaches to ground metric learning often
do not generalize across multiple classes or are restricted to distributions
with shared supports. To address these limitations, we propose a novel approach
for learning metrics for arbitrary distributions over a shared metric space.
Our method provides a distance between individual points like a global metric,
but requires only class labels on a distribution-level for training. The
learned global ground metric enables more accurate optimal transport distances,
leading to improved performance in embedding, clustering and classification
tasks. We demonstrate the effectiveness and interpretability of our approach
using patient-level scRNA-seq data spanning multiple diseases.

</details>


### [89] [Provable Maximum Entropy Manifold Exploration via Diffusion Models](https://arxiv.org/abs/2506.15385)
*Riccardo De Santi,Marin Vlastelica,Ya-Ping Hsieh,Zebang Shen,Niao He,Andreas Krause*

Main category: cs.LG

TL;DR: A novel framework for exploration using diffusion models without explicit uncertainty quantification, leveraging entropy maximization and density estimation.


<details>
  <summary>Details</summary>
Motivation: To enable exploration in decision-making problems like scientific discovery by generating novel designs without relying on existing data distributions.

Method: Introduces a framework based on entropy maximization over a diffusion model's data manifold, using density estimation and mirror descent for scalable exploration.

Result: The algorithm converges to an optimal exploratory diffusion model under realistic assumptions, validated on synthetic and high-dimensional tasks.

Conclusion: The approach shows promise for scalable exploration in generative models, particularly in text-to-image diffusion.

Abstract: Exploration is critical for solving real-world decision-making problems such
as scientific discovery, where the objective is to generate truly novel designs
rather than mimic existing data distributions. In this work, we address the
challenge of leveraging the representational power of generative models for
exploration without relying on explicit uncertainty quantification. We
introduce a novel framework that casts exploration as entropy maximization over
the approximate data manifold implicitly defined by a pre-trained diffusion
model. Then, we present a novel principle for exploration based on density
estimation, a problem well-known to be challenging in practice. To overcome
this issue and render this method truly scalable, we leverage a fundamental
connection between the entropy of the density induced by a diffusion model and
its score function. Building on this, we develop an algorithm based on mirror
descent that solves the exploration problem as sequential fine-tuning of a
pre-trained diffusion model. We prove its convergence to the optimal
exploratory diffusion model under realistic assumptions by leveraging recent
understanding of mirror flows. Finally, we empirically evaluate our approach on
both synthetic and high-dimensional text-to-image diffusion, demonstrating
promising results.

</details>


### [90] [Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control](https://arxiv.org/abs/2506.15397)
*Sepehr Elahi,Paula Mürmann,Patrick Thiran*

Main category: cs.LG

TL;DR: The paper addresses minimizing disease extinction time in an SIS model with an unknown graph, proposing a learning algorithm for graph recovery and optimal vaccination strategies.


<details>
  <summary>Details</summary>
Motivation: To tackle the practical challenge of unknown graphs in disease spread, focusing on effective vaccination strategies.

Method: Splits the problem into graph learning and vaccination strategy determination, introducing an inclusion-exclusion-based learning algorithm and optimal SRM algorithm.

Result: Establishes sample complexity for graph recovery and proves polynomial-time solutions for bounded treewidth graphs, with a greedy heuristic for general graphs.

Conclusion: Validated by experiments, the approach effectively combines learning and vaccination strategies for unknown graphs.

Abstract: The Susceptible-Infected-Susceptible (SIS) model is a widely used model for
the spread of information and infectious diseases, particularly non-immunizing
ones, on a graph. Given a highly contagious disease, a natural question is how
to best vaccinate individuals to minimize the disease's extinction time. While
previous works showed that the problem of optimal vaccination is closely linked
to the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that
the graph is known, which is often not the case in practice. In this work, we
consider the problem of minimizing the extinction time of an outbreak modeled
by an SIS model where the graph on which the disease spreads is unknown and
only the infection states of the vertices are observed. To this end, we split
the problem into two: learning the graph and determining effective vaccination
strategies. We propose a novel inclusion-exclusion-based learning algorithm
and, unlike previous approaches, establish its sample complexity for graph
recovery. We then detail an optimal algorithm for the SRM problem and prove
that its running time is polynomial in the number of vertices for graphs with
bounded treewidth. This is complemented by an efficient and effective
polynomial-time greedy heuristic for any graph. Finally, we present experiments
on synthetic and real-world data that numerically validate our learning and
vaccination algorithms.

</details>


### [91] [Semi-supervised Graph Anomaly Detection via Robust Homophily Learning](https://arxiv.org/abs/2506.15448)
*Guoguo Ai,Hezhe Qiao,Hui Yan,Guansong Pang*

Main category: cs.LG

TL;DR: RHO introduces adaptive homophily learning for semi-supervised graph anomaly detection, outperforming existing methods by capturing diverse homophily patterns in normal nodes.


<details>
  <summary>Details</summary>
Motivation: Current methods assume uniform homophily among normal nodes, which fails in real-world datasets where homophily varies. RHO addresses this limitation.

Method: RHO uses adaptive frequency response filters (AdaFreq) and graph normality alignment (GNA) to learn and align homophily patterns in node attributes.

Result: RHO outperforms state-of-the-art methods on eight real-world datasets by effectively capturing diverse homophily patterns.

Conclusion: RHO robustly learns varying homophily in normal nodes, improving anomaly detection in graphs with diverse homophily patterns.

Abstract: Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled
normal nodes to identify abnormal nodes from a large set of unlabeled nodes in
a graph. Current methods in this line posit that 1) normal nodes share a
similar level of homophily and 2) the labeled normal nodes can well represent
the homophily patterns in the normal class. However, this assumption often does
not hold well since normal nodes in a graph can exhibit diverse homophily in
real-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily
Learning, to adaptively learn such homophily patterns. RHO consists of two
novel modules, adaptive frequency response filters (AdaFreq) and graph
normality alignment (GNA). AdaFreq learns a set of adaptive spectral filters
that capture different frequency components of the labeled normal nodes with
varying homophily in the channel-wise and cross-channel views of node
attributes. GNA is introduced to enforce consistency between the channel-wise
and cross-channel homophily representations to robustify the normality learned
by the filters in the two views. Experiments on eight real-world GAD datasets
show that RHO can effectively learn varying, often under-represented, homophily
in the small normal node set and substantially outperforms state-of-the-art
competing methods. Code is available at https://github.com/mala-lab/RHO.

</details>


### [92] [Creating User-steerable Projections with Interactive Semantic Mapping](https://arxiv.org/abs/2506.15479)
*Artur André Oliveira,Mateus Espadoto,Roberto Hirata Jr.,Roberto M. Cesar Jr.,Alex C. Telea*

Main category: cs.LG

TL;DR: A user-guided projection framework for dimensionality reduction (DR) leverages MLLMs to create customizable, interpretable visualizations via natural-language prompts, enhancing interactivity and semantic exploration.


<details>
  <summary>Details</summary>
Motivation: Current DR techniques lack the ability to explore semantic structures not directly available in variables or labels, limiting user-driven analysis.

Method: The framework uses zero-shot classification with MLLMs, allowing users to dynamically steer projections via natural-language prompts to uncover high-level semantic relationships.

Result: Evaluations show improved cluster separation and transform DR into an interactive, user-driven process, bridging automated techniques and human-centered exploration.

Conclusion: The approach offers a flexible, adaptive way to tailor DR projections to specific analytical needs, enhancing interpretability and user control.

Abstract: Dimensionality reduction (DR) techniques map high-dimensional data into
lower-dimensional spaces. Yet, current DR techniques are not designed to
explore semantic structure that is not directly available in the form of
variables or class labels. We introduce a novel user-guided projection
framework for image and text data that enables customizable, interpretable,
data visualizations via zero-shot classification with Multimodal Large Language
Models (MLLMs). We enable users to steer projections dynamically via
natural-language guiding prompts, to specify high-level semantic relationships
of interest to the users which are not explicitly present in the data
dimensions. We evaluate our method across several datasets and show that it not
only enhances cluster separation, but also transforms DR into an interactive,
user-driven process. Our approach bridges the gap between fully automated DR
techniques and human-centered data exploration, offering a flexible and
adaptive way to tailor projections to specific analytical needs.

</details>


### [93] [LIT-LVM: Structured Regularization for Interaction Terms in Linear Predictors using Latent Variable Models](https://arxiv.org/abs/2506.15492)
*Mohammadreza Nemati,Zhipeng Huang,Kevin S. Xu*

Main category: cs.LG

TL;DR: LIT-LVM introduces a low-dimensional latent representation for interaction terms in linear predictors, improving accuracy and mitigating overfitting compared to elastic net and factorization machines.


<details>
  <summary>Details</summary>
Motivation: Linear predictors with interaction terms can model non-linear relationships but face challenges in accurately estimating coefficients, especially in high-dimensional settings.

Method: Proposes LIT-LVM, which represents features via latent vectors in a low-dimensional space, acting as structured regularization.

Result: LIT-LVM outperforms elastic net and factorization machines in prediction accuracy, particularly with many interaction terms and few samples.

Conclusion: LIT-LVM provides accurate coefficient estimation, reduces overfitting, and offers interpretable low-dimensional feature representations.

Abstract: Some of the simplest, yet most frequently used predictors in statistics and
machine learning use weighted linear combinations of features. Such linear
predictors can model non-linear relationships between features by adding
interaction terms corresponding to the products of all pairs of features. We
consider the problem of accurately estimating coefficients for interaction
terms in linear predictors. We hypothesize that the coefficients for different
interaction terms have an approximate low-dimensional structure and represent
each feature by a latent vector in a low-dimensional space. This
low-dimensional representation can be viewed as a structured regularization
approach that further mitigates overfitting in high-dimensional settings beyond
standard regularizers such as the lasso and elastic net. We demonstrate that
our approach, called LIT-LVM, achieves superior prediction accuracy compared to
elastic net and factorization machines on a wide variety of simulated and real
data, particularly when the number of interaction terms is high compared to the
number of samples. LIT-LVM also provides low-dimensional latent representations
for features that are useful for visualizing and analyzing their relationships.

</details>


### [94] [Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review](https://arxiv.org/abs/2506.15506)
*Salijona Dyrmishi,Mohamed Djilani,Thibault Simonetto,Salah Ghamizi,Maxime Cordy*

Main category: cs.LG

TL;DR: A systematic review of adversarial attacks on tabular machine learning models, highlighting trends, attack strategies, and practical challenges.


<details>
  <summary>Details</summary>
Motivation: Research on adversarial attacks in tabular data is scattered, unlike in computer vision and NLP, necessitating a structured overview.

Method: Conducts a systematic literature review to categorize attack strategies and analyze real-world applicability.

Result: Identifies key trends, attack methods, and practical considerations, along with current challenges and open questions.

Conclusion: The review provides a structured guide for future research on adversarial vulnerabilities in tabular ML.

Abstract: Adversarial attacks in machine learning have been extensively reviewed in
areas like computer vision and NLP, but research on tabular data remains
scattered. This paper provides the first systematic literature review focused
on adversarial attacks targeting tabular machine learning models. We highlight
key trends, categorize attack strategies and analyze how they address practical
considerations for real-world applicability. Additionally, we outline current
challenges and open research questions. By offering a clear and structured
overview, this review aims to guide future efforts in understanding and
addressing adversarial vulnerabilities in tabular machine learning.

</details>


### [95] [A Simplified Analysis of SGD for Linear Regression with Weight Averaging](https://arxiv.org/abs/2506.15535)
*Alexandru Meterez,Depen Morwani,Costin-Andrei Oncescu,Jingfeng Wu,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: Simplified analysis of SGD in linear regression using basic linear algebra, matching prior results without complex PSD matrix manipulations.


<details>
  <summary>Details</summary>
Motivation: To make SGD analysis more accessible and facilitate further improvements in optimization algorithms.

Method: Uses simple linear algebra tools to derive bias and variance bounds for SGD in linear regression.

Result: Recovers the same bounds as prior work but with a simpler approach.

Conclusion: The simplified analysis enhances accessibility and may aid in advancing optimization techniques for realistic models.

Abstract: Theoretically understanding stochastic gradient descent (SGD) in
overparameterized models has led to the development of several optimization
algorithms that are widely used in practice today. Recent work
by~\citet{zou2021benign} provides sharp rates for SGD optimization in linear
regression using constant learning rate, both with and without tail iterate
averaging, based on a bias-variance decomposition of the risk. In our work, we
provide a simplified analysis recovering the same bias and variance bounds
provided in~\citep{zou2021benign} based on simple linear algebra tools,
bypassing the requirement to manipulate operators on positive semi-definite
(PSD) matrices. We believe our work makes the analysis of SGD on linear
regression very accessible and will be helpful in further analyzing
mini-batching and learning rate scheduling, leading to improvements in the
training of realistic models.

</details>


### [96] [Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning](https://arxiv.org/abs/2506.15544)
*Roger Creus Castanyer,Johan Obando-Ceron,Lu Li,Pierre-Luc Bacon,Glen Berseth,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: The paper identifies gradient pathologies and non-stationarity as key challenges in scaling deep reinforcement learning, proposing simple interventions to stabilize gradient flow for robust performance.


<details>
  <summary>Details</summary>
Motivation: Understanding and addressing the root causes of performance degradation in scaled deep reinforcement learning networks.

Method: Empirical analyses to identify issues, followed by proposing and testing simple interventions to stabilize gradient flow.

Result: Effective stabilization of gradient flow, enabling robust performance across varying network scales.

Conclusion: Simple interventions can address scaling challenges, improving performance in deep reinforcement learning.

Abstract: Scaling deep reinforcement learning networks is challenging and often results
in degraded performance, yet the root causes of this failure mode remain poorly
understood. Several recent works have proposed mechanisms to address this, but
they are often complex and fail to highlight the causes underlying this
difficulty. In this work, we conduct a series of empirical analyses which
suggest that the combination of non-stationarity with gradient pathologies, due
to suboptimal architectural choices, underlie the challenges of scale. We
propose a series of direct interventions that stabilize gradient flow, enabling
robust performance across a range of network depths and widths. Our
interventions are simple to implement and compatible with well-established
algorithms, and result in an effective mechanism that enables strong
performance even at large scales. We validate our findings on a variety of
agents and suites of environments.

</details>


### [97] [Task-Agnostic Experts Composition for Continual Learning](https://arxiv.org/abs/2506.15566)
*Luigi Quarantiello,Andrea Cossu,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: A compositional approach using expert models improves accuracy and efficiency in neural networks.


<details>
  <summary>Details</summary>
Motivation: To enhance AI efficiency and sustainability by leveraging human-like compositional reasoning.

Method: Ensemble zero-shot expert models tested on a benchmark for compositionality.

Result: Higher accuracy and lower computational costs compared to baselines.

Conclusion: The Expert Composition method is effective for efficient and sustainable AI.

Abstract: Compositionality is one of the fundamental abilities of the human reasoning
process, that allows to decompose a complex problem into simpler elements. Such
property is crucial also for neural networks, especially when aiming for a more
efficient and sustainable AI framework. We propose a compositional approach by
ensembling zero-shot a set of expert models, assessing our methodology using a
challenging benchmark, designed to test compositionality capabilities. We show
that our Expert Composition method is able to achieve a much higher accuracy
than baseline algorithms while requiring less computational resources, hence
being more efficient.

</details>


### [98] [MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing](https://arxiv.org/abs/2506.15571)
*Le Vu Anh,Nguyen Viet Anh,Mehmet Dik,Tu Nguyen Thi Ngoc*

Main category: cs.LG

TL;DR: MicroRicci is a self-tuning, local Ricci-flow solver that uses coding theory and neural modules for real-time mesh smoothing, achieving faster convergence and better quality than existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of real-time mesh smoothing, which is hindered by costly global updates in classical methods and slow convergence or brittle tuning in greedy heuristics.

Method: MicroRicci combines a greedy syndrome-decoding step (O(E) time) with two tiny neural modules for adaptive vertex and step-size selection.

Result: Achieves 2.4x speedup, tighter curvature spread (0.19 to 0.185), and high UV-distortion-to-MOS correlation (r = -0.93), with only 0.25 ms added per iteration.

Conclusion: MicroRicci's efficiency and quality make it ideal for real-time, resource-limited applications in graphics and simulation.

Abstract: Real-time mesh smoothing at scale remains a formidable challenge: classical
Ricci-flow solvers demand costly global updates, while greedy heuristics suffer
from slow convergence or brittle tuning. We present MicroRicci, the first truly
self-tuning, local Ricci-flow solver that borrows ideas from coding theory and
packs them into just 1K + 200 parameters. Its primary core is a greedy
syndrome-decoding step that pinpoints and corrects the largest curvature error
in O(E) time, augmented by two tiny neural modules that adaptively choose
vertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,
MicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),
tightens curvature spread from 0.19 to 0.185, and achieves a remarkable
UV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per
iteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration
over state-of-the-art methods. MicroRicci's combination of linear-time updates,
automatic hyperparameter adaptation, and high-quality geometric and perceptual
results makes it well suited for real-time, resource-limited applications in
graphics, simulation, and related fields.

</details>


### [99] [Memory-Efficient Differentially Private Training with Gradient Random Projection](https://arxiv.org/abs/2506.15588)
*Alex Mulrooney,Devansh Gupta,James Flemings,Huanyu Zhang,Murali Annavaram,Meisam Razaviyayn,Xinwei Zhang*

Main category: cs.LG

TL;DR: DP-GRAPE reduces memory overhead in DP training by privatizing gradients after projection, using random Gaussian matrices, and applying projection during backpropagation, achieving comparable utility to DP-SGD.


<details>
  <summary>Details</summary>
Motivation: Standard DP methods like DP-Adam have high memory overhead due to per-sample gradient clipping, limiting scalability.

Method: DP-GRAPE introduces three modifications: privatizing gradients after projection, using random Gaussian matrices instead of SVD, and applying projection during backpropagation.

Result: DP-GRAPE reduces memory usage by over 63% for Vision Transformers and 70% for RoBERTa-Large, with similar performance to DP-Adam. It scales to models like OPT (6.7B parameters).

Conclusion: DP-GRAPE offers a memory-efficient DP training method without sacrificing utility, enabling scalability for large models.

Abstract: Differential privacy (DP) protects sensitive data during neural network
training, but standard methods like DP-Adam suffer from high memory overhead
due to per-sample gradient clipping, limiting scalability. We introduce
DP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly
reduces memory usage while maintaining utility on par with first-order DP
approaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces
three key modifications: (1) gradients are privatized after projection, (2)
random Gaussian matrices replace SVD-based subspaces, and (3) projection is
applied during backpropagation. These contributions eliminate the need for
costly SVD computations, enable substantial memory savings, and lead to
improved utility. Despite operating in lower-dimensional subspaces, our
theoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off
comparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE
can reduce the memory footprint of DP training without sacrificing accuracy or
training time. In particular, DP-GRAPE reduces memory usage by over 63% when
pre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as
compared to DP-Adam, while achieving similar performance. We further
demonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with
up to 6.7 billion parameters.

</details>


### [100] [CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization](https://arxiv.org/abs/2506.15654)
*Ranting Hu*

Main category: cs.LG

TL;DR: The paper addresses over-conservatism in offline RL due to poor explorations in suboptimal data, proposing CAWR with robust loss functions and prioritized replay to improve policy learning.


<details>
  <summary>Details</summary>
Motivation: Offline RL often suffers from distribution shift, leading to over-conservative policies. The paper investigates how poor explorations in suboptimal data exacerbate this issue.

Method: The study analyzes poor explorations' impact on KL divergence and policy approximation, then introduces CAWR with robust loss functions and advantage-based prioritized replay.

Result: CAWR outperforms on the D4RL benchmark, learning better policies from suboptimal data.

Conclusion: CAWR effectively mitigates over-conservatism by addressing poor explorations, enhancing offline RL performance.

Abstract: Offline reinforcement learning (offline RL) algorithms often require
additional constraints or penalty terms to address distribution shift issues,
such as adding implicit or explicit policy constraints during policy
optimization to reduce the estimation bias of functions. This paper focuses on
a limitation of the Advantage-Weighted Regression family (AWRs), i.e., the
potential for learning over-conservative policies due to data corruption,
specifically the poor explorations in suboptimal offline data. We study it from
two perspectives: (1) how poor explorations impact the theoretically optimal
policy based on KL divergence, and (2) how such poor explorations affect the
approximation of the theoretically optimal policy. We prove that such
over-conservatism is mainly caused by the sensitivity of the loss function for
policy optimization to poor explorations, and the proportion of poor
explorations in offline datasets. To address this concern, we propose
Corruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a
set of robust loss functions during policy optimization and an advantage-based
prioritized experience replay method to filter out poor explorations. Numerical
experiments on the D4RL benchmark show that our method can learn superior
policies from suboptimal offline data, significantly enhancing the performance
of policy optimization.

</details>
