<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 32]
- [cs.LG](#cs.LG) [Total: 69]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself is a conversational AI tool that makes LLM interpretability accessible through natural language queries and interactive visualizations.


<details>
  <summary>Details</summary>
Motivation: Existing LLM interpretability tools are fragmented and code-intensive, lacking user-friendly interfaces for natural interaction.

Method: It uses an orchestrator LLM to reformulate user queries, an agent router to direct them to specialized modules, and then contextualizes outputs into coherent explanations with interactive visualizations.

Result: The system consolidates interpretability capabilities into a chat-based interface where users can upload models, ask natural language questions, and receive guided explanations with visualizations.

Conclusion: KnowThyself successfully provides an accessible and extensible platform that lowers technical barriers for LLM interpretability through its chat-based approach.

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [2] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym is a framework that uses synthetic experiences generated through reasoning to enable scalable reinforcement learning for LLM agents, overcoming challenges like costly rollouts and limited task diversity.


<details>
  <summary>Details</summary>
Motivation: Practical adoption of RL for LLM agents is hindered by expensive real-environment rollouts, limited task variety, unreliable rewards, and infrastructure complexity, which impede scalable experience data collection.

Method: DreamGym distills environment dynamics into a reasoning-based experience model for consistent state transitions and feedback, uses an experience replay buffer initialized with offline data, and adaptively generates new tasks for curriculum learning.

Result: Experiments show DreamGym improves RL training significantly, outperforming baselines by over 30% on non-RL-ready tasks like WebArena, matching GRPO and PPO with synthetic data only, and enhancing sim-to-real transfer with fewer real interactions.

Conclusion: DreamGym provides a scalable, effective solution for RL training of autonomous agents, enabling robust performance gains and efficient warm-start strategies without heavy reliance on real-world interactions.

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [3] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: Empowerment-based AI assistance designed for single humans can disempower other humans in multi-human environments; joint empowerment helps but reduces user rewards.


<details>
  <summary>Details</summary>
Motivation: Prior work on empowerment-based assistance focused on single-human settings, but real-world scenarios like homes and hospitals involve multiple humans. The research aims to investigate if assistive agents optimizing for one human's empowerment can negatively impact other humans in multi-human environments.

Method: The authors introduced Disempower-Grid, an open source multi-human gridworld test suite, and used it to empirically test assistive RL agents. They characterized when disempowerment occurs and evaluated joint empowerment as a mitigation strategy.

Result: The research found that assistive RL agents optimizing for one human's empowerment can significantly reduce another human's environmental influence and rewards, a phenomenon termed as disempowerment. Joint empowerment was shown to mitigate disempowerment but at the cost of the user's reward.

Conclusion: The study reveals that goal-agnostic objectives like empowerment can become misaligned in multi-agent contexts, indicating a broader challenge for AI alignment when transitioning from single-agent to multi-agent settings.

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [4] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: This paper evaluates NLP tokenization models for assembly code analysis, examining how tokenizer choices affect vocabulary size, semantic coverage, and downstream task performance like function signature prediction.


<details>
  <summary>Details</summary>
Motivation: Tokenization is fundamental in assembly code analysis but remains underexplored, with significant impacts on vocabulary size, semantic coverage, and downstream task performance that need systematic investigation.

Method: Conducted thorough study of various tokenization models with systematic analysis of encoding efficiency and semantic capture. Used state-of-the-art pre-trained models (Llama 3.2, BERT, BART) and evaluated tokenizers across multiple performance metrics with preprocessing customization and pre-tokenization rules tailored to assembly code.

Result: Tokenizer choice significantly influences downstream performance, with intrinsic metrics providing partial but incomplete predictability of extrinsic evaluation outcomes. Complex trade-offs exist between intrinsic tokenizer properties and their utility in practical assembly code tasks.

Conclusion: The study provides valuable insights for optimizing tokenization models in low-level code analysis, contributing to the robustness and scalability of NLM-based binary analysis workflows by revealing the complex relationship between tokenizer properties and practical performance.

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [5] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: LLMs can replicate human decision-making patterns in game theory experiments, with Llama showing high fidelity to human cooperation patterns and Qwen aligning with Nash equilibrium. This enables systematic exploration of human social decision-making beyond traditional experimental constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to understand how closely LLMs mirror actual human decision-making, especially as LLMs are increasingly used in critical domains like health, education, and law, and for simulating human behavior. A gap in understanding LLM-human alignment could lead to harmful outcomes in practical applications or ineffective social simulations.

Method: The method involves developing a digital twin of game-theoretic experiments and introducing a systematic prompting and probing framework for machine-behavioral evaluation. The researchers tested three open-source LLMs (Llama, Mistral, and Qwen) on their ability to reproduce human cooperation patterns.

Result: The results show that Llama reproduces human cooperation patterns with high fidelity, capturing human deviations from rational choice theory, while Qwen aligns closely with Nash equilibrium predictions. Notably, population-level behavioral replication was achieved without persona-based prompting. The researchers also generated and preregistered testable hypotheses for novel game configurations outside the original parameter grid.

Conclusion: The study concludes that appropriately calibrated LLMs can replicate aggregate human behavioral patterns and enable systematic exploration of unexplored experimental spaces, offering a complementary approach to traditional research in the social and behavioral sciences that generates new empirical predictions about human social decision-making.

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [6] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: BehaviorLens framework compares text vs. image representations of user behavior data for MLLMs, finding image representations boost prediction accuracy by 87.5% without extra cost.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of textual vs. visual representations of user behavior data for MLLM performance is not well studied.

Method: BehaviorLens benchmarks six MLLMs using transaction data represented as text paragraphs, scatter plots, and flowcharts on a real-world purchase-sequence dataset.

Result: Image representations improved next-purchase prediction accuracy by 87.5% compared to text representations, with no additional computational overhead.

Conclusion: Visual representations of user behavior data significantly enhance MLLM reasoning performance, suggesting prioritizing image-based inputs for agentic systems.

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [7] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: DR.WELL is a decentralized neurosymbolic framework for multi-agent cooperative planning that uses a two-phase negotiation protocol and symbolic planning to avoid brittle trajectory-level coordination.


<details>
  <summary>Details</summary>
Motivation: Cooperative multi-agent planning often fails at trajectory level due to timing deviations and conflicts. Symbolic planning provides abstraction and synchronization capabilities to address this challenge.

Method: Two-phase negotiation: agents propose roles with reasoning, then commit to joint allocation. After commitment, agents independently generate symbolic plans for their roles using a shared world model that gets updated during execution.

Result: Experiments on cooperative block-push tasks show improved task completion rates and efficiency, with the dynamic world model capturing reusable patterns and enabling evolving collaboration strategies.

Conclusion: Symbolic planning enables higher-level operations that are reusable, synchronizable, and interpretable, trading time overhead for more efficient collaboration while avoiding brittle step-level alignment.

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [8] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: This paper challenges the common belief that DKT's effectiveness comes from modeling bidirectional relationships between knowledge components. Instead, it shows DKT actually excels by modeling prerequisite (causal) relationships as DAGs.


<details>
  <summary>Details</summary>
Motivation: Previous research attributed DKT's superior performance to its ability to model bidirectional KC relationships. However, the authors suspect this explanation is incomplete and propose that DKT's true strength lies in modeling causal prerequisite structures.

Method: The authors prune exercise relation graphs into Directed Acyclic Graphs (DAGs) and train DKT on causal subsets of the Assistments dataset. They also propose a new method to extract exercise relation DAGs using DKT's learned representations.

Result: Experimental results show DKT's predictive performance aligns strongly with causal structures rather than bidirectional relationships. The proposed DAG extraction method provides empirical support for the causal modeling hypothesis.

Conclusion: DKT's effectiveness is primarily driven by its ability to approximate causal dependencies between knowledge components, not just relational mappings. This changes the fundamental understanding of how deep knowledge tracing models work.

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [9] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: LLMs show cultural bias despite prompt variations, consistently favoring values from Netherlands, Germany, US, and Japan, with limited ability to represent global cultural diversity.


<details>
  <summary>Details</summary>
Motivation: To examine whether LLMs can represent cultural diversity given imbalances in training data and optimization objectives, and how prompt language and cultural framing influence model responses.

Method: Probed 10 LLMs with 63 items from Hofstede Values Survey Module and World Values Survey, translated into 11 languages, formulated as prompts with and without explicit cultural perspectives.

Result: Both prompt language and cultural perspective produce variation in outputs, but models show systematic bias toward values from Netherlands, Germany, US, and Japan. Cultural framing improves alignment more than targeted language, but combining both approaches is no more effective than cultural framing with English prompts.

Conclusion: LLMs occupy an uncomfortable middle ground - responsive enough to produce variation but too anchored to specific cultural defaults to adequately represent cultural diversity.

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [10] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot is a multi-agent system that uses proxy-based evaluation and adaptive search to efficiently automate ML engineering without relying heavily on expensive full training runs.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents for automated ML engineering require repeated full training runs, leading to high computational costs, limited scalability, and slow iteration cycles.

Method: ArchPilot integrates architecture generation, proxy-based evaluation, and adaptive search with three specialized agents: an orchestration agent using MCTS-inspired algorithm with restart, a generation agent for creating/improving architectures, and an evaluation agent for proxy training and scoring.

Result: Experiments on MLE-Bench show ArchPilot outperforms SOTA baselines like AIDE and ML-Master.

Conclusion: The multi-agent collaboration enables efficient ML engineering under limited budgets by prioritizing high-potential candidates with minimal full training reliance.

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [11] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: This paper introduces anomaly detection for multi-agent AI systems using LLMs, creates benchmark datasets with 4,275 and 894 trajectories, and shows supervised and semi-supervised methods achieve up to 98% and 96% accuracy respectively.


<details>
  <summary>Details</summary>
Motivation: Multi-Agentic AI systems with LLMs are non-deterministic and prone to silent failures like drift, cycles, and missing output details that are difficult to detect, creating a need for systematic anomaly detection.

Method: Developed a dataset curation pipeline capturing user behavior, agent non-determinism, and LLM variation to create two benchmark datasets, then benchmarked anomaly detection methods including supervised (XGBoost) and semi-supervised (SVDD) approaches.

Result: Supervised (XGBoost) and semi-supervised (SVDD) anomaly detection methods performed comparably, achieving accuracies up to 98% and 96% respectively on the curated datasets.

Conclusion: This work provides the first systematic study of anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks, and insights to guide future research in detecting silent failures in agentic trajectories.

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [12] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: LLMs show amplified numerical correlations and context-induced representation shifts, revealing decision-making vulnerabilities that differ by model size.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying representational mechanisms behind numerical reasoning errors in LLMs, particularly how they integrate multiple numerical attributes and how irrelevant context affects these representations.

Method: The researchers used linear probing with partial correlation analysis and prompt-based vulnerability tests across LLMs of varying sizes to investigate numerical attribute integration and context effects.

Result: LLMs encode real-world numerical correlations but systematically amplify them. Irrelevant context induces consistent shifts in magnitude representations, with downstream effects varying by model size.

Conclusion: The paper concludes that LLMs exhibit systematic vulnerabilities in numerical reasoning due to shared latent subspaces and amplification of real-world correlations, providing groundwork for developing fairer, representation-aware control mechanisms.

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [13] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: Agentmandering introduces a game-theoretic framework using LLM agents to model redistricting as strategic negotiation, significantly reducing partisan bias with more stable outcomes than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current computational redistricting methods generate legally valid maps but ignore strategic selection processes, allowing partisan manipulation despite technical compliance.

Method: Framework uses turn-based negotiation between opposing political agents inspired by Choose-and-Freeze protocol, where agents alternately select/freeze districts from candidate maps.

Result: Evaluation on 2020 Census data shows significant reduction in partisan bias/unfairness and 2-3 orders of magnitude lower variance than baselines, especially effective in swing states.

Conclusion: Agentmandering successfully embeds strategic interaction into redistricting, demonstrating improved fairness and stability while maintaining interpretability.

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [14] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui,Zequn Sun,Wei Hu,Zhangjie Fu*

Main category: cs.AI

TL;DR: Proposes LLM-KGFR framework combining LLMs with Knowledge Graph retriever for zero-shot reasoning on knowledge graphs using progressive propagation and iterative reasoning loops.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with knowledge-intensive questions due to limited context and parametric knowledge, while existing retrieval methods have scalability and generalization limitations.

Method: Collaborative framework where LLM works with KGFR retriever that encodes relations using LLM-generated descriptions and initializes entities based on question roles. Uses Asymmetric Progressive Propagation for efficient large graph handling.

Result: Framework achieves strong performance while maintaining scalability and generalization, providing practical KG-augmented reasoning solution.

Conclusion: LLM-KGFR offers effective zero-shot generalization to unseen knowledge graphs with controllable reasoning loops, addressing key limitations of current approaches.

Abstract: Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.

</details>


### [15] [Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms](https://arxiv.org/abs/2511.04133)
*Miguel E. Andres,Vadim Fedorov,Rida Sadek,Enric Spagnolo-Arrizabalaga,Nadescha Trudel*

Main category: cs.AI

TL;DR: A framework for evaluating voice AI testing quality using human benchmarking and statistical methods reveals performance gaps among commercial platforms.


<details>
  <summary>Details</summary>
Motivation: Lack of systematic methods to validate voice AI testing reliability as deployments scale to billions of interactions.

Method: Combines psychometric techniques (e.g., pairwise comparisons, Elo ratings, bootstrap confidence intervals) with empirical evaluation using human judgments and ground truth validation.

Result: Significant performance differences found; top platform (Evalion) achieved 0.92 evaluation quality (f1-score) versus 0.73 for others, and 0.61 simulation quality versus 0.43.

Conclusion: The framework provides empirical validation for testing capabilities, enabling confident large-scale voice AI deployment.

Abstract: Voice AI agents are rapidly transitioning to production deployments, yet
systematic methods for ensuring testing reliability remain underdeveloped.
Organizations cannot objectively assess whether their testing approaches
(internal tools or external platforms) actually work, creating a critical
measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing
quality through human-centered benchmarking. Our methodology addresses the
fundamental dual challenge of testing platforms: generating realistic test
conversations (simulation quality) and accurately evaluating agent responses
(evaluation quality). The framework combines established psychometric
techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence
intervals, and permutation tests) with rigorous statistical validation to
provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted
comprehensive empirical evaluation of three leading commercial platforms
focused on Voice AI Testing using 21,600 human judgments across 45 simulations
and ground truth validation on 60 conversations. Results reveal statistically
significant performance differences with the proposed framework, with the
top-performing platform, Evalion, achieving 0.92 evaluation quality measured as
f1-score versus 0.73 for others, and 0.61 simulation quality using a league
based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate
the testing capabilities of any platform, providing essential measurement
foundations for confident voice AI deployment at scale. Supporting materials
are made available to facilitate reproducibility and adoption.

</details>


### [16] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: The paper introduces the Opus Workflow Evaluation Framework, a probabilistic-normative model for quantifying workflow quality and efficiency through reward functions and normative penalties.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic mathematical framework for comparing, scoring, and optimizing workflows by integrating correctness, reliability, and cost metrics.

Method: Combines Opus Workflow Reward (probabilistic function for expected performance) with Opus Workflow Normative Penalties (measurable functions for structural and informational quality across Cohesion, Coupling, Observability, and Information Hygiene).

Result: Enables automated workflow assessment, ranking, and optimization within automation systems like Opus, and can be integrated into Reinforcement Learning for workflow discovery and refinement.

Conclusion: The framework provides a unified optimization formulation for identifying optimal workflows under joint reward-penalty trade-offs, supporting systematic workflow evaluation and improvement.

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [17] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: Multi-agent predictive coding framework that minimizes mutual uncertainty through information bottleneck, enabling efficient communication and social place cells for coordination under bandwidth constraints.


<details>
  <summary>Details</summary>
Motivation: Addressing catastrophic coordination failures in multi-agent systems due to partial observability and limited bandwidth by developing consistent spatial memory sharing.

Method: Multi-agent predictive coding with information bottleneck objective, grid-cell-like metric for spatial coding, hierarchical reinforcement learning policy, and emergent social place cells.

Result: Exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, outperforming baseline that collapses from 67.6% to 28.6%.

Conclusion: Establishes a theoretically principled and biologically plausible basis for emergent social representations from unified predictive drive, leading to social collective intelligence.

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [18] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLoop introduces iterative policy re-initialization to combat RL overfitting by preserving solution diversity during training, leading to significant generalization improvements over standard RL methods.


<details>
  <summary>Details</summary>
Motivation: RL for Verifiable Rewards (RLVR) faces a critical challenge of RL overfitting, where models gain training rewards but lose generalization due to policy over-specialization and catastrophic forgetting of diverse solutions generated during training. Standard optimization discards valuable inter-step policy diversity.

Method: RLoop is a self-improving framework based on iterative policy initialization. It involves: 1) Using RL to explore solution space from a given policy, 2) Filtering successful trajectories to create an expert dataset, 3) Applying Rejection-sampling Fine-Tuning (RFT) to refine the initial policy, creating a better starting point for the next iteration.

Result: RLoop substantially improves generalization, mitigating forgetting and boosting average accuracy by 9% and pass@32 by over 15% compared to vanilla RL.

Conclusion: RLoop effectively converts transient policy diversity into robust performance improvements by leveraging iterative re-initialization, addressing the critical challenge of RL overfitting in training dynamics.

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [19] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: GUI-360° is a large-scale dataset and benchmark for computer-using agents (CUAs) addressing gaps in real-world tasks, automated multimodal data collection, and unified evaluation of GUI grounding, screen parsing, and action prediction.


<details>
  <summary>Details</summary>
Motivation: Current CUA research faces three main gaps: scarcity of real-world tasks, lack of automated pipelines for multimodal trajectory collection, and absence of unified benchmarks evaluating GUI grounding, screen parsing, and action prediction together.

Method: Uses an LLM-augmented automated pipeline for query sourcing, environment construction, task instantiation, batched execution, and quality filtering. Contains over 1.2M action steps across Windows office applications with screenshots, metadata, goals, reasoning traces, and both successful/failed trajectories.

Result: Benchmarking shows state-of-the-art vision-language models have substantial shortcomings in grounding and action prediction. Supervised fine-tuning and reinforcement learning yield significant improvements but don't reach human-level reliability.

Conclusion: GUI-360° facilitates reproducible research and accelerates progress on robust desktop CUAs. The dataset and code are publicly available to address current limitations in CUA development.

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [20] [Probing the Probes: Methods and Metrics for Concept Alignment](https://arxiv.org/abs/2511.04312)
*Jacob Lysnæs-Larsen,Marte Eggen,Inga Strümke*

Main category: cs.AI

TL;DR: This paper challenges the reliability of using probe accuracy alone to evaluate Concept Activation Vectors (CAVs) in explainable AI, showing probes can capture spurious correlations. It introduces a new localization method using spatial linear attribution and proposes three alignment metrics to better assess concept representation.


<details>
  <summary>Details</summary>
Motivation: Current CAV evaluation relies heavily on probe accuracy, which may not faithfully represent concept alignment due to potential spurious correlations captured by probes.

Method: The authors introduce a novel concept localization method based on spatial linear attribution and compare it with existing feature visualization techniques. They also propose three quantitative metrics: hard accuracy, segmentation scores, and augmentation robustness.

Result: Demonstration that misaligned probes exploiting spurious correlations achieve similar accuracy to standard probes. Probes with translation invariance and spatial alignment show improved concept alignment.

Conclusion: Probe accuracy alone is insufficient for evaluating CAVs; alignment-based metrics tailored to model architecture and concept nature are necessary for reliable concept representation assessment.

Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained
by training linear classifier probes to detect human-understandable concepts as
directions in the activation space of deep neural networks. It is widely
assumed that a high probe accuracy indicates a CAV faithfully representing its
target concept. However, we show that the probe's classification accuracy alone
is an unreliable measure of concept alignment, i.e., the degree to which a CAV
captures the intended concept. In fact, we argue that probes are more likely to
capture spurious correlations than they are to represent only the intended
concept. As part of our analysis, we demonstrate that deliberately misaligned
probes constructed to exploit spurious correlations, achieve an accuracy close
to that of standard probes. To address this severe problem, we introduce a
novel concept localization method based on spatial linear attribution, and
provide a comprehensive comparison of it to existing feature visualization
techniques for detecting and mitigating concept misalignment. We further
propose three classes of metrics for quantitatively assessing concept
alignment: hard accuracy, segmentation scores, and augmentation robustness. Our
analysis shows that probes with translation invariance and spatial alignment
consistently increase concept alignment. These findings highlight the need for
alignment-based evaluation metrics rather than probe accuracy, and the
importance of tailoring probes to both the model architecture and the nature of
the target concept.

</details>


### [21] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: Introduces AdversariaLLM, a toolbox for LLM jailbreak robustness research that unifies fragmented implementations and enables reproducible, comparable safety evaluations.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of LLM safety research has created a fragmented, buggy ecosystem with implementations, datasets, and evaluation methods that make reproducibility and comparability across studies challenging.

Method: The framework implements twelve adversarial attack algorithms, integrates seven benchmark datasets, provides access to open-weight LLMs via Hugging Face, and includes advanced features like compute-resource tracking, deterministic results, and distributional evaluation techniques.

Result: The toolbox enables reproducible and comparable LLM jailbreak robustness research with structured attack implementations, standardized datasets, and integrated evaluation tools.

Conclusion: AdversariaLLM provides a robust foundation for transparent, comparable, and reproducible research in LLM safety by unifying fragmented implementations, datasets, and evaluation methods into a cohesive toolbox.

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [22] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: This paper introduces RxSafeBench, an LLM evaluation framework that assesses medication safety in simulated clinical consultations with a 2,443-scenario benchmark using embedded risks like contraindications and drug interactions.


<details>
  <summary>Details</summary>
Motivation: Limited research on LLM medication safety exists primarily because of gaps in real-world datasets constrained by privacy issues, and the lack of evaluation in realistic clinical consultation settings.

Method: The framework generates clinical inquiry dialogues with embedded drug risks, constructs a medication safety database (RxRisk DB), ensures clinical realism via a two-stage filtering strategy, and evaluates LLMs via structured multiple choice questions in simulated patient contexts.

Result: Current LLMs struggle integrating contraindication and interaction knowledge, especially with implied risks, showing key challenges in recommendation safety despite high-quality scenarios.

Conclusion: RxSafeBench is the first comprehensive benchmark for improving LLM-based clinical decision support, highlighting the need for better prompting and task-specific tuning to enhance medication safety reliability.

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [23] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: The paper proposes MGV, a Monitor-Generate-Verify framework that adds metacognitive monitoring to existing reasoning architectures to prevent early commitment to poor reasoning paths.


<details>
  <summary>Details</summary>
Motivation: Current test-time reasoning architectures lack monitoring processes, leading to the 'prefix dominance trap' where models commit early to suboptimal paths and lose about 20% accuracy.

Method: The authors formalize metacognitive theories into computational specifications, extending the Generate-Verify paradigm by adding explicit monitoring before generation and refining it through verification feedback.

Result: While no empirical validation is presented, the work provides the first systematic computational translation of metacognitive theories.

Conclusion: MGV offers a principled framework for understanding reasoning failures and suggests architectural improvements for future test-time reasoning systems.

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [24] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: Introduces Iterative RMFT, a method that fine-tunes LLMs using low-regret decision trajectories to improve their decision-making in interactive environments.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with basic online decision-making problems, failing to achieve low regret or effective exploration-exploitation tradeoffs despite being deployed as agents.

Method: Iteratively distills low-regret decision trajectories: rolls out multiple trajectories, selects k-lowest regret ones, and fine-tunes the model on them using natural-language reasoning.

Result: Improves decision-making performance across diverse models (Transformers, open-weight LLMs, GPT-4o mini), generalizes across tasks with varying horizons, action spaces, and contexts.

Conclusion: Iterative RMFT provides a principled, flexible post-training framework to enhance LLMs' decision-making capabilities, with theoretical support for no-regret learning in simplified settings.

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [25] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: CoRPO improves upon GRPO by using an adaptive baseline to properly handle ordinal rewards, preventing reinforcement of failures and enabling better convergence and generalization in RL training for LLMs.


<details>
  <summary>Details</summary>
Motivation: GRPO's simplicity makes it ill-specified for richer, non-binary feedback, as its group-average baseline often reinforces incorrect behavior by assigning positive advantage to failed trajectories when using ordinal rewards for partial credit.

Method: CoRPO uses an adaptive baseline with a minimum quality threshold to ensure failed solutions are never positively reinforced, then transitions to a relative preference mode to push for optimal solutions rather than just acceptable ones.

Result: Empirical validation on a code verification task shows CoRPO achieves more stable convergence and better out-of-domain generalization compared to GRPO.

Conclusion: CoRPO addresses a critical limitation in GRPO by introducing an adaptive baseline that prevents reinforcement of failed trajectories and promotes optimal solutions over merely acceptable ones, representing a significant advancement in enabling LLMs to learn from rich, multi-dimensional feedback through reinforcement learning.

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [26] [Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context](https://arxiv.org/abs/2511.04464)
*Carnot Braun,Rafael O. Jarczewski,Gabriel U. Talasso,Leandro A. Villas,Allan M. de Souza*

Main category: cs.AI

TL;DR: PAVe combines LLM-based semantic reasoning with multi-objective Dijkstra routing to create personalized vehicular routing that understands complex user contexts, achieving 88%+ accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional vehicle routing systems optimize singular metrics like time or distance but lack capability to interpret and integrate complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs.

Method: Employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm, evaluating options against user-provided tasks, preferences, and avoidance rules using a pre-processed geospatial cache of urban Points of Interest (POIs).

Result: In realistic urban scenarios, PAVe successfully converted complex user intent into appropriate route modifications, achieving over 88% accuracy in initial route selections with a local model.

Conclusion: Combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.

Abstract: Traditional vehicle routing systems efficiently optimize singular metrics
like time or distance, and when considering multiple metrics, they need more
processes to optimize . However, they lack the capability to interpret and
integrate the complex, semantic, and dynamic contexts of human drivers, such as
multi-step tasks, situational constraints, or urgent needs. This paper
introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a
hybrid agentic assistant designed to augment classical pathfinding algorithms
with contextual reasoning. Our approach employs a Large Language Model (LLM)
agent that operates on a candidate set of routes generated by a multi-objective
(time, CO2) Dijkstra algorithm. The agent evaluates these options against
user-provided tasks, preferences, and avoidance rules by leveraging a
pre-processed geospatial cache of urban Points of Interest (POIs). In a
benchmark of realistic urban scenarios, PAVe successfully used complex user
intent into appropriate route modifications, achieving over 88% accuracy in its
initial route selections with a local model. We conclude that combining
classical routing algorithms with an LLM-based semantic reasoning layer is a
robust and effective approach for creating personalized, adaptive, and scalable
solutions for urban mobility optimization.

</details>


### [27] [Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis](https://arxiv.org/abs/2511.04481)
*Lars Krupp,Daniel Geißler,Vishal Banwari,Paul Lukowicz,Jakob Karolus*

Main category: cs.AI

TL;DR: This paper explores the sustainability issues of web agents, finding that different design approaches significantly affect energy consumption without guaranteeing better performance, and calls for energy-focused evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the largely unexplored sustainability issues in web agent research, despite the thriving development of these systems, by highlighting the urgency of examining their energy and CO2 costs.

Method: The research uses both a theoretical approach through estimation and an empirical perspective via benchmarking to evaluate the energy and CO2 costs associated with web agents.

Result: The results show that different philosophies in web agent creation can severely impact energy expenditure, and that more energy consumed does not necessarily lead to better results.

Conclusion: The study calls for a change in how web agents are evaluated, advocating for dedicated metrics to measure energy consumption in benchmarks and highlighting the lack of transparency regarding model parameters and processes as a limiting factor in estimating energy costs.

Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful
agentic systems pushing the boundaries of Large Language Models (LLM). They can
autonomously interact with the internet at the user's behest, such as
navigating websites, filling search masks, and comparing price lists. Though
web agent research is thriving, induced sustainability issues remain largely
unexplored. To highlight the urgency of this issue, we provide an initial
exploration of the energy and $CO_2$ cost associated with web agents from both
a theoretical -via estimation- and an empirical perspective -by benchmarking.
Our results show how different philosophies in web agent creation can severely
impact the associated expended energy, and that more energy consumed does not
necessarily equate to better results. We highlight a lack of transparency
regarding disclosing model parameters and processes used for some web agents as
a limiting factor when estimating energy consumption. Our work contributes
towards a change in thinking of how we evaluate web agents, advocating for
dedicated metrics measuring energy consumption in benchmarks.

</details>


### [28] [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](https://arxiv.org/abs/2511.04556)
*Zihang Ding,Kun Zhang*

Main category: cs.AI

TL;DR: A data-driven sparse sensing framework optimizes sensor placement and reconstructs peak flowrates in stormwater systems using minimal sensors with high accuracy (NSE 0.92-0.95), enabling effective flood monitoring under resource constraints.


<details>
  <summary>Details</summary>
Motivation: Urban surface water flooding is increasingly frequent but high-resolution monitoring is constrained by time, budget, and technology limitations. There's a need for effective flood prediction and monitoring under limited resources.

Method: Integrated a data-driven sparse sensing (DSS) framework with EPA-SWMM model. Used singular value decomposition for dimensionality reduction and QR factorization for sensor allocation. Generated training dataset from SWMM simulations and validated sensor placement effectiveness.

Result: Three optimally placed sensors among 77 nodes achieved satisfactory reconstruction performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95. The model showed good robustness to measurement uncertainty, with location-dependent robustness to sensor failures that improves with more sensors.

Conclusion: The DSS framework successfully balances computational efficiency and physical interpretability, enabling high-accuracy flow reconstruction with minimal sensors. It provides a practical solution for flood monitoring under resource constraints and can be integrated with predictive models for early warning systems.

Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming
drainage systems, is increasingly frequent and widespread. While flood
prediction and monitoring in high spatial-temporal resolution are desired,
practical constraints in time, budget, and technology hinder its full
implementation. How to monitor urban drainage networks and predict flow
conditions under constrained resource is a major challenge. This study presents
a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to
optimize sensor placement and reconstruct peak flowrates in a stormwater
system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case
study. We utilized a SWMM model to generate a training dataset of peak flowrate
profiles across the stormwater network. Furthermore, we applied DSS -
leveraging singular value decomposition for dimensionality reduction and QR
factorization for sensor allocation - to identify the optimal monitoring nodes
based on the simulated training dataset. We then validated the
representativeness of these identified monitoring nodes by comparing the
DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three
optimally placed sensors among 77 nodes achieved satisfactory reconstruction
performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to
75th percentiles). In addition, the model showed good robustness to uncertainty
in measurements. Its robustness to sensor failures is location-dependent and
improves with the number of sensors deployed. The framework balances
computational efficiency and physical interpretability, enabling high-accuracy
flow reconstruction with minimal sensors. This DSS framework can be further
integrated with predictive models to realize flood early warning and real-time
control under limited sensing and monitoring resource.

</details>


### [29] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: Jr. AI Scientist is an advanced autonomous system that mimics student research workflow, producing higher-quality papers than existing automated systems, but reveals key limitations and risks in current AI Scientist capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand current capabilities and risks of AI Scientist systems for ensuring trustworthy AI-driven scientific progress while preserving academic integrity.

Method: Developed Jr. AI Scientist, an autonomous system that mimics novice researcher workflow: analyzes paper limitations, formulates hypotheses, validates through experiments, and writes papers. Uses modern coding agents for complex implementations and was evaluated through AI Reviewers, author evaluations, and submissions to Agents4Science venue.

Result: Jr. AI Scientist generates papers that receive higher review scores than existing fully automated systems, but important limitations were identified from both author evaluation and venue reviews.

Conclusion: The paper concludes that while Jr. AI Scientist outperforms existing fully automated systems, current AI Scientist systems still face important limitations and risks. The identified challenges provide guidance for future research in AI-driven scientific discovery.

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [30] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: Proposes reframing ambiguity in natural language queries for tabular data as cooperative interaction, develops a framework to classify queries by resolvability, analyzes 15 datasets revealing mixed query types unsuitable for current evaluations, and advocates for embracing cooperation in query resolution to improve design and evaluation.


<details>
  <summary>Details</summary>
Motivation: Ambiguity in natural language interfaces to tabular data is traditionally seen as a flaw; instead, treating it as a collaborative opportunity between user and system can lead to better query handling.

Method: Developed a principled framework to distinguish cooperative (resolvable) queries from uncooperative (unresolvable) ones, applied it to analyze queries in 15 tabular QA and analysis datasets.

Result: Found that current datasets mix query types inappropriately, making them inadequate for evaluating execution accuracy or interpretation capabilities of systems.

Conclusion: Shifting perspective from fixing ambiguity to embracing cooperation in query resolution enables more informed design and evaluation of natural language interfaces for tabular data, with outlined future directions.

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [31] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: This paper develops algorithms to audit question representativeness in deliberative forums using justified representation metrics, compares moderator-selected vs optimized vs AI-generated questions, and deploys the framework on a global deliberation platform.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of selecting a limited number of questions that best represent all participants' interests in deliberative processes like citizens' assemblies and deliberative polls, where only a small subset of proposed questions can be posed to expert panels due to time constraints.

Method: The paper introduces an auditing framework based on justified representation (JR) and presents efficient algorithms for auditing JR in the general utility setting, with the fastest algorithm running in O(mn log n) time. It compares different question selection methods including moderator-chosen questions, participant questions selected via integer linear programming, and summary questions generated by large language models.

Result: The results show that the auditing methods can be applied to historical deliberations to compare representativeness across different question selection approaches, and the methods have been integrated into an online deliberation platform used internationally.

Conclusion: The paper concludes that the auditing framework and algorithms successfully measure and improve representation in deliberative processes, highlighting the potential and current limitations of LLMs for this purpose.

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [32] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: VeriCoT is a neuro-symbolic method that extracts and verifies formal logical arguments from Chain-of-Thought reasoning using first-order logic and automated solvers, improving reasoning validity and accuracy through various fine-tuning approaches.


<details>
  <summary>Details</summary>
Motivation: LLMs using Chain-of-Thought reasoning cannot reliably verify their own logic, undermining trust in high-stakes scenarios, necessitating a method to identify flawed reasoning steps.

Method: Extracts formal logical arguments from CoT reasoning, formalizes each step into first-order logic, identifies premises grounded in context/commonsense/prior steps, uses automated solvers for verification.

Result: Experiments on ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning and serves as strong predictor of final answer correctness.

Conclusion: VeriCoT improves reasoning validity and accuracy through inference-time self-reflection, supervised fine-tuning, and preference fine-tuning with verification-based rewards.

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland](https://arxiv.org/abs/2511.03749)
*Oluwadurotimi Onibonoje,Vuong M. Ngo,Andrew McCarre,Elodie Ruelle,Bernadette O-Briend,Mark Roantree*

Main category: cs.LG

TL;DR: Deep learning models, especially a temporal convolutional network, effectively forecast grass growth using historical data, improving sustainability in dairy farming.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of current mechanistic models for grass growth forecasting in the Irish dairy sector, which faces profitability and sustainability challenges.

Method: Deep learning models, specifically a temporal convolutional network, were developed for univariate datasets using historical grass height data.

Result: The temporal convolutional network achieved high performance with RMSE of 2.74 and MAE of 3.46, validated on a 34-year dataset of 1,757 weeks.

Conclusion: The temporal convolutional network provides a reliable and cost-effective solution for grass growth forecasting, enhancing sustainable dairy farming practices.

Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink,
play a crucial role in biodiversity and the regulation of the carbon cycle.
Currently, the Irish dairy sector, a significant economic contributor, grapples
with challenges related to profitability and sustainability. Presently, grass
growth forecasting relies on impractical mechanistic models. In response, we
propose deep learning models tailored for univariate datasets, presenting
cost-effective alternatives. Notably, a temporal convolutional network designed
for forecasting Perennial Ryegrass growth in Cork exhibits high performance,
leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46.
Validation across a comprehensive dataset spanning 1,757 weeks over 34 years
provides insights into optimal model configurations. This study enhances our
understanding of model behavior, thereby improving reliability in grass growth
forecasting and contributing to the advancement of sustainable dairy farming
practices.

</details>


### [34] [Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices](https://arxiv.org/abs/2511.03753)
*Youssef Elmir,Yassine Himeur,Abbes Amira*

Main category: cs.LG

TL;DR: Federated learning framework using GAF image transformation for privacy-preserving ECG classification on IoT devices, achieving 95.18% accuracy with efficient resource utilization.


<details>
  <summary>Details</summary>
Motivation: To enable privacy-preserving ECG classification in IoT healthcare environments while keeping sensitive medical data local to devices and addressing heterogeneous IoT device constraints.

Method: Transform 1D ECG signals into 2D Gramian Angular Field (GAF) images for feature extraction using CNNs, deployed across server, laptop, and Raspberry Pi 4 in a federated learning setup.

Result: FL-GAF model achieves 95.18% classification accuracy in multi-client setup, outperforming single-client baseline in both accuracy and training time, with efficient resource utilization despite GAF complexity.

Conclusion: The framework demonstrates potential for lightweight, privacy-preserving AI in IoT healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.

Abstract: This study presents a federated learning (FL) framework for
privacy-preserving electrocardiogram (ECG) classification in Internet of Things
(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian
Angular Field (GAF) images, the proposed approach enables efficient feature
extraction through Convolutional Neural Networks (CNNs) while ensuring that
sensitive medical data remain local to each device. This work is among the
first to experimentally validate GAF-based federated ECG classification across
heterogeneous IoT devices, quantifying both performance and communication
efficiency. To evaluate feasibility in realistic IoT settings, we deployed the
framework across a server, a laptop, and a resource-constrained Raspberry Pi 4,
reflecting edge-cloud integration in IoT ecosystems. Experimental results
demonstrate that the FL-GAF model achieves a high classification accuracy of
95.18% in a multi-client setup, significantly outperforming a single-client
baseline in both accuracy and training time. Despite the added computational
complexity of GAF transformations, the framework maintains efficient resource
utilization and communication overhead. These findings highlight the potential
of lightweight, privacy-preserving AI for IoT-based healthcare monitoring,
supporting scalable and secure edge deployments in smart health systems.

</details>


### [35] [Laugh, Relate, Engage: Stylized Comment Generation for Short Videos](https://arxiv.org/abs/2511.03757)
*Xuan Ouyang,Senan Wang,Bouzhou Wang,Siyuan Xiahou,Jinrong Zhou,Yuekang Li*

Main category: cs.LG

TL;DR: LOLGORITHM is a modular multi-agent system that generates stylistically diverse and context-aware comments for short videos, supporting six comment styles and achieving over 87% human preference rates.


<details>
  <summary>Details</summary>
Motivation: Short-video platforms need comments that are both compliant with guidelines and stylistically diverse to enhance user engagement and creative interaction, but current methods struggle with this balance.

Method: A modular multi-agent system integrating video segmentation, contextual/affective analysis, and style-aware prompt construction, powered by a multimodal LLM that processes videos directly with explicit prompt markers and few-shot examples.

Result: Significantly outperforms baseline models with preference rates over 90% on Douyin and 87.55% on YouTube, as validated by large-scale human evaluation involving 40 videos and 105 participants.

Conclusion: LOLGORITHM presents a scalable and culturally adaptive framework for stylized comment generation that enhances user engagement and creative interaction on short-video platforms.

Abstract: Short-video platforms have become a central medium in the modern Internet
landscape, where efficient information delivery and strong interactivity are
reshaping user engagement and cultural dissemination. Among the various forms
of user interaction, comments play a vital role in fostering community
participation and enabling content re-creation. However, generating comments
that are both compliant with platform guidelines and capable of exhibiting
stylistic diversity and contextual awareness remains a significant challenge.
We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for
controllable short-video comment generation. The system integrates video
segmentation, contextual and affective analysis, and style-aware prompt
construction. It supports six distinct comment styles: puns (homophones),
rhyming, meme application, sarcasm (irony), plain humor, and content
extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM
directly processes video inputs and achieves fine-grained style control through
explicit prompt markers and few-shot examples. To support development and
evaluation, we construct a bilingual dataset using official APIs from Douyin
(Chinese) and YouTube (English), covering five popular video genres: comedy
skits, daily life jokes, funny animal clips, humorous commentary, and talk
shows. Evaluation combines automated metrics originality, relevance, and style
conformity with a large-scale human preference study involving 40 videos and
105 participants. Results show that LOLGORITHM significantly outperforms
baseline models, achieving preference rates of over 90% on Douyin and 87.55% on
YouTube. This work presents a scalable and culturally adaptive framework for
stylized comment generation on short-video platforms, offering a promising path
to enhance user engagement and creative interaction.

</details>


### [36] [Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems](https://arxiv.org/abs/2511.04594)
*Utkarsh U. Chavan,Prashant Trivedi,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: First regret lower bound analysis for decentralized multi-agent stochastic shortest path problems under linear function approximation, showing Ω(√K) regret.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems in applications like swarm robotics and traffic routing require decentralized coordination, but learning in decentralized multi-agent SSPs remains largely unexplored compared to single-agent settings.

Method: Study decentralized multi-agent SSPs under linear function approximation, using novel symmetry-based arguments to identify optimal policy structure and construct hard-to-learn instances.

Result: Established the first regret lower bound of Ω(√K) over K episodes for Dec-MASSPs, demonstrating inherent learning difficulty in decentralized control.

Conclusion: The analysis clarifies learning complexity in decentralized multi-agent systems and provides guidance for designing efficient learning algorithms.

Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics
and traffic routing, where agents must coordinate in a decentralized manner to
achieve a common objective. Stochastic Shortest Path (SSP) problems provide a
natural framework for modeling decentralized control in such settings. While
the problem of learning in SSP has been extensively studied in single-agent
settings, the decentralized multi-agent variant remains largely unexplored. In
this work, we take a step towards addressing that gap. We study decentralized
multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the
transition dynamics and costs are represented using linear models. Applying
novel symmetry-based arguments, we identify the structure of optimal policies.
Our main contribution is the first regret lower bound for this setting based on
the construction of hard-to-learn instances for any number of agents, $n$. Our
regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the
inherent learning difficulty in Dec-MASSPs. These insights clarify the learning
complexity of decentralized control and can further guide the design of
efficient learning algorithms in multi-agent systems.

</details>


### [37] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: A new benchmark called Common-O reveals that despite strong performance on perception tasks, multimodal language models struggle with reasoning across scenes and suffer from hallucinations, with even the best models achieving only 35% accuracy.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models show strong performance on perception benchmarks but still hallucinate when reasoning about real-world scenes, indicating a gap between perception and reasoning capabilities.

Method: Created Common-O benchmark with 10.5k+ examples using new images not in training data, designed to probe reasoning across scenes by asking "what's in common?" inspired by cognitive tests.

Result: Models perform well on single-image perception but struggle with cross-scene reasoning (best model: 35% on Common-O, 1% on Complex version). Models hallucinate more when similar objects are present, suggesting reliance on training co-occurrence patterns.

Conclusion: Multimodal models need improved reasoning capabilities beyond perception. Multi-image training shows promise, and the benchmark is released to spur research on reducing hallucinations in scene reasoning.

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [38] [Contamination Detection for VLMs using Multi-Modal Semantic Perturbation](https://arxiv.org/abs/2511.03774)
*Jaden Park,Mu Cai,Feng Yao,Jingbo Shang,Soochahn Lee,Yong Jae Lee*

Main category: cs.LG

TL;DR: Proposes a novel detection method for test-set leakage in Vision-Language Models using multi-modal semantic perturbation, showing contaminated models fail to generalize under controlled perturbations.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored problem of detecting test-set leakage in VLMs, as existing detection approaches fail or show inconsistent behavior when models are contaminated with benchmark data.

Method: Deliberately contaminate open-source VLMs on popular benchmarks, then develop a detection method based on multi-modal semantic perturbation that exploits contaminated models' inability to generalize under controlled perturbations.

Result: The proposed detection method effectively identifies contaminated models across multiple realistic contamination strategies, demonstrating robustness and effectiveness.

Conclusion: The work provides a practical solution for detecting test-set leakage in VLMs, with publicly released code and perturbed dataset to facilitate further research.

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved
state-of-the-art performance on numerous benchmark tasks. However, the use of
internet-scale, often proprietary, pretraining corpora raises a critical
concern for both practitioners and users: inflated performance due to test-set
leakage. While prior works have proposed mitigation strategies such as
decontamination of pretraining data and benchmark redesign for LLMs, the
complementary direction of developing detection methods for contaminated VLMs
remains underexplored. To address this gap, we deliberately contaminate
open-source VLMs on popular benchmarks and show that existing detection
approaches either fail outright or exhibit inconsistent behavior. We then
propose a novel simple yet effective detection method based on multi-modal
semantic perturbation, demonstrating that contaminated models fail to
generalize under controlled perturbations. Finally, we validate our approach
across multiple realistic contamination strategies, confirming its robustness
and effectiveness. The code and perturbed dataset will be released publicly.

</details>


### [39] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: FusionDP is a two-step framework that improves model utility under feature-level differential privacy by using foundation models to impute sensitive features and a modified DP-SGD algorithm that trains on both original and imputed features while preserving privacy of sensitive attributes.


<details>
  <summary>Details</summary>
Motivation: Traditional DP-SGD applies privacy protection to all features equally, causing excessive noise and utility degradation, but in practice only some features (like demographic data) need privacy protection while others (like lab results) are less sensitive.

Method: 1) Use large foundation models to impute sensitive features from non-sensitive features as external priors, 2) Apply modified DP-SGD algorithm that trains models on both original and imputed features while formally preserving privacy of original sensitive features.

Result: Evaluation on sepsis prediction (PhysioNet tabular data) and clinical note classification (MIMIC-III) shows FusionDP significantly improves model performance compared to privacy-preserving baselines while maintaining rigorous feature-level privacy.

Conclusion: FusionDP demonstrates the potential of foundation model-driven imputation to enhance the privacy-utility trade-off across different data modalities by selectively protecting only sensitive features.

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [40] [Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations](https://arxiv.org/abs/2511.03807)
*Shivogo John*

Main category: cs.LG

TL;DR: Paper introduces adaptive SHAP variants to make credit model explanations stable and fair despite concept drift, showing improved stability and fairness without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Traditional explainability methods like SHAP fail when data distributions change over time, leading to unstable/unfair explanations in credit scoring.

Method: Developed three adaptive SHAP methods: per-slice reweighting, drift-aware rebaselining, and online surrogate calibration, tested on multi-year credit data with XGBoost.

Result: Adaptive methods (especially rebaselining and surrogate) significantly improved explanation stability and fairness across demographic groups while maintaining predictive accuracy.

Conclusion: Adaptive explainability frameworks are crucial for maintaining transparent, accountable, and ethical AI systems in evolving environments like credit scoring.

Abstract: Evolving borrower behaviors, shifting economic conditions, and changing
regulatory landscapes continuously reshape the data distributions underlying
modern credit-scoring systems. Conventional explainability techniques, such as
SHAP, assume static data and fixed background distributions, making their
explanations unstable and potentially unfair when concept drift occurs. This
study addresses that challenge by developing adaptive explanation frameworks
that recalibrate interpretability and fairness in dynamically evolving credit
models. Using a multi-year credit dataset, we integrate predictive modeling via
XGBoost with three adaptive SHAP variants: (A) per-slice explanation
reweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP
rebaselining with sliding-window background samples, and (C) online surrogate
calibration using incremental Ridge regression. Each method is benchmarked
against static SHAP explanations using metrics of predictive performance (AUC,
F1), directional and rank stability (cosine, Kendall tau), and fairness
(demographic parity and recalibration). Results show that adaptive methods,
particularly rebaselined and surrogate-based explanations, substantially
improve temporal stability and reduce disparate impact across demographic
groups without degrading predictive accuracy. Robustness tests, including
counterfactual perturbations, background sensitivity analysis, and
proxy-variable detection, confirm the resilience of adaptive explanations under
real-world drift conditions. These findings establish adaptive explainability
as a practical mechanism for sustaining transparency, accountability, and
ethical reliability in data-driven credit systems, and more broadly, in any
domain where decision models evolve with population change.

</details>


### [41] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: A routing system assigns problems to the smallest capable reasoning model to reduce computational costs without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Reasoning language models are effective but expensive to deploy due to their size and long reasoning processes.

Method: Using intermediate representations from a large model (s1.1-32B), lightweight predictors estimate problem difficulty or model correctness to route problems efficiently across a model pool.

Result: On math benchmarks, routing improves efficiency over random assignment and matches the large model's performance with significantly less compute.

Conclusion: Difficulty-aware routing is an effective strategy for cost-efficient deployment of reasoning models.

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [42] [One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA](https://arxiv.org/abs/2511.03809)
*François Belias,Naser Ezzati-Jivan,Foutse Khomh*

Main category: cs.LG

TL;DR: DEBA introduces architecture-aware adaptive batch scheduling that monitors gradient and loss metrics. Evaluation shows adaptation benefits vary significantly across architectures, with lightweight networks achieving 45-62% speedup and accuracy improvements, while deeper networks show mixed results.


<details>
  <summary>Details</summary>
Motivation: Existing adaptive batch size methods assume one-size-fits-all solutions across architectures, ignoring how different network structures respond to adaptation strategies.

Method: DEBA dynamically adjusts batch sizes by monitoring gradient variance, gradient norm variation, and loss variation. Systematic evaluation across six architectures on CIFAR datasets with multiple random seeds.

Result: Lightweight architectures achieved 45-62% training speedup with 1-7% accuracy improvements. ResNet-18 showed consistent gains, while ResNet-50 had high variance. ViT-B16 showed minimal speedup. Gradient stability metrics predict adaptation benefits.

Conclusion: Adaptive batch methods must be architecture-aware rather than assuming generalization across networks. Batch size adaptation requires considering baseline optimization characteristics through gradient stability analysis.

Abstract: Adaptive batch size methods aim to accelerate neural network training, but
existing approaches apply identical adaptation strategies across all
architectures, assuming a one-size-fits-all solution. We introduce DEBA
(Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors
gradient variance, gradient norm variation and loss variation to guide batch
size adaptations. Through systematic evaluation across six architectures
(ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on
CIFAR-10 and CIFAR-100, with five random seeds per configuration, we
demonstrate that the architecture fundamentally determines adaptation efficacy.
Our findings reveal that: (1) lightweight and medium-depth architectures
(MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup
with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks
(ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in
speedup, while deep residual networks (ResNet-50) exhibit high variance and
occasional degradation; (3) already-stable architectures (ViT-B16) show minimal
speedup (6%) despite maintaining accuracy, indicating that adaptation benefits
vary with baseline optimization characteristics. We introduce a baseline
characterization framework using gradient stability metrics (stability score,
gradient norm variation) that predicts which architectures will benefit from
adaptive scheduling. Our ablation studies reveal critical design choices often
overlooked in prior work: sliding window statistics (vs. full history) and
sufficient cooldown periods (5+ epochs) between adaptations are essential for
success. This work challenges the prevailing assumption that adaptive methods
generalize across architectures and provides the first systematic evidence that
batch size adaptation requires an architecture-aware design.

</details>


### [43] [Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks](https://arxiv.org/abs/2511.03824)
*Ryien Hosseini,Filippo Simini,Venkatram Vishwanath,Rebecca Willett,Henry Hoffmann*

Main category: cs.LG

TL;DR: Injecting randomized global embeddings (Sketched Random Features) into GNNs improves long-range dependency capture and alleviates limitations like oversquashing and oversmoothing.


<details>
  <summary>Details</summary>
Motivation: GNNs face challenges: oversquashing of long-range information, oversmoothing of node representations, and limited expressive power due to local message passing.

Method: Use Sketched Random Features—randomized global embeddings of node features that are unique, distance-sensitive, and topology-agnostic—injected into standard GNNs.

Result: Experimental results show consistent performance improvements over baseline GNNs on real-world graph learning tasks.

Conclusion: This strategy effectively addresses GNN limitations, serving as both a standalone solution and complementary enhancement to techniques like graph positional encodings.

Abstract: Graph Neural Networks learn on graph-structured data by iteratively
aggregating local neighborhood information. While this local message passing
paradigm imparts a powerful inductive bias and exploits graph sparsity, it also
yields three key challenges: (i) oversquashing of long-range information, (ii)
oversmoothing of node representations, and (iii) limited expressive power. In
this work we inject randomized global embeddings of node features, which we
term \textit{Sketched Random Features}, into standard GNNs, enabling them to
efficiently capture long-range dependencies. The embeddings are unique,
distance-sensitive, and topology-agnostic -- properties which we analytically
and empirically show alleviate the aforementioned limitations when injected
into GNNs. Experimental results on real-world graph learning tasks confirm that
this strategy consistently improves performance over baseline GNNs, offering
both a standalone solution and a complementary enhancement to existing
techniques such as graph positional encodings. Our source code is available at
\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.

</details>


### [44] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: Innovative method called StratDiff uses diffusion models and energy guidance to smoothly transition from offline to online RL by stratifying samples based on KL divergence, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Distributional shifts between offline datasets and evolving online policies pose critical challenges, with limited methods addressing the offline data's distributional structure.

Method: Deploys diffusion model to learn from offline data, refines with energy functions, computes KL divergence to stratify samples into offline-like and online-like subsets for different training strategies.

Result: Significantly outperforms existing methods on D4RL benchmarks when integrated with Cal-QL and IQL, showing enhanced adaptability and stable performance.

Conclusion: StratDiff effectively addresses offline-to-online RL transition challenges through sample stratification, demonstrating superior performance across diverse settings.

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [45] [Higher-Order Causal Structure Learning with Additive Models](https://arxiv.org/abs/2511.03831)
*James Enouen,Yujia Zheng,Ignavier Ng,Yan Liu,Kun Zhang*

Main category: cs.LG

TL;DR: Extends causal additive models (CAM) to include higher-order interactions using hyper DAGs, providing identifiability results and a greedy algorithm for learning.


<details>
  <summary>Details</summary>
Motivation: Real-world processes often exhibit higher-order interactions, but causal discovery methods have not explicitly addressed these, leaving a gap in modeling complex dependencies.

Method: Extends CAM to additive models with higher-order interactions, represented by directed acyclic hypergraphs. Introduces definitions, theoretical tools, and identifiability results for hyper DAGs, along with a greedy algorithm extension for learning.

Result: Identifiability results for hyper DAGs are established, and the extended greedy CAM algorithm is demonstrated to be effective in synthetic experiments, showing better empirical performance with more restrictive assumptions.

Conclusion: Incorporating higher-order interactions via hyper DAGs enhances causal structure learning, with theoretical support and practical algorithm improvements, enabling better handling of complex real-world data.

Abstract: Causal structure learning has long been the central task of inferring causal
insights from data. Despite the abundance of real-world processes exhibiting
higher-order mechanisms, however, an explicit treatment of interactions in
causal discovery has received little attention. In this work, we focus on
extending the causal additive model (CAM) to additive models with higher-order
interactions. This second level of modularity we introduce to the structure
learning problem is most easily represented by a directed acyclic hypergraph
which extends the DAG. We introduce the necessary definitions and theoretical
tools to handle the novel structure we introduce and then provide
identifiability results for the hyper DAG, extending the typical Markov
equivalence classes. We next provide insights into why learning the more
complex hypergraph structure may actually lead to better empirical results. In
particular, more restrictive assumptions like CAM correspond to easier-to-learn
hyper DAGs and better finite sample complexity. We finally develop an extension
of the greedy CAM algorithm which can handle the more complex hyper DAG search
space and demonstrate its empirical usefulness in synthetic experiments.

</details>


### [46] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: Proposes SADQ, a DQN variant using successor-state distributions for more stable Q-learning with theoretical guarantees of unbiased estimates.


<details>
  <summary>Details</summary>
Motivation: DQN suffers from high variance due to using potentially suboptimal next states from replay buffer, causing poor policy alignment.

Method: Integrates stochastic transition model and successor-state distributions into Q-value estimation, with efficient action selection using transition structure.

Result: Outperforms DQN variants in stability and learning efficiency across RL benchmarks and real-world control tasks.

Conclusion: SADQ effectively addresses DQN's variance issues through policy-aligned value updates with theoretical and empirical validation.

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [47] [Benchmark Datasets for Lead-Lag Forecasting on Social Platforms](https://arxiv.org/abs/2511.03877)
*Kimia Kazemian,Zhenzhen Liu,Yangfanyu Yang,Katie Z Luo,Shuhan Gu,Audrey Du,Xinyu Yang,Jack Jansons,Kilian Q Weinberger,John Thickstun,Yian Yin,Sarah Dean*

Main category: cs.LG

TL;DR: Introduces Lead-Lag Forecasting (LLF) as a new time-series problem to predict delayed outcomes (e.g., citations) from early interactions (e.g., views) using standardized datasets like arXiv and GitHub, with benchmarks and validation.


<details>
  <summary>Details</summary>
Motivation: Social platforms show delayed correlations between early interactions (likes, views) and later outcomes (citations, sales), but LLF lacks unified treatment due to no standardized datasets.

Method: Created benchmark datasets (arXiv: 2.3M papers for accesses→citations; GitHub: 3M repos for pushes/stars→forks), validated lead-lag dynamics statistically, and tested parametric/non-parametric regression baselines.

Result: Established LLF as a forecasting paradigm, provided open data portal, and demonstrated datasets capture long-horizon dynamics without survivorship bias.

Conclusion: LLF is a novel framework for predictive analytics in social data, with benchmarks enabling systematic research.

Abstract: Social and collaborative platforms emit multivariate time-series traces in
which early interactions-such as views, likes, or downloads-are followed,
sometimes months or years later, by higher impact like citations, sales, or
reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an
early usage channel (the lead), predict a correlated but temporally shifted
outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not
been treated as a unified forecasting problem within the time-series community,
largely due to the absence of standardized datasets. To anchor research in LLF,
here we present two high-volume benchmark datasets-arXiv (accesses -> citations
of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and
outline additional domains with analogous lead-lag dynamics, including
Wikipedia (page views -> edits), Spotify (streams -> concert attendance),
e-commerce (click-throughs -> purchases), and LinkedIn profile (views ->
messages). Our datasets provide ideal testbeds for lead-lag forecasting, by
capturing long-horizon dynamics across years, spanning the full spectrum of
outcomes, and avoiding survivorship bias in sampling. We documented all
technical details of data curation and cleaning, verified the presence of
lead-lag dynamics through statistical and classification tests, and benchmarked
parametric and non-parametric baselines for regression. Our study establishes
LLF as a novel forecasting paradigm and lays an empirical foundation for its
systematic exploration in social and usage data. Our data portal with downloads
and documentation is available at https://lead-lag-forecasting.github.io/.

</details>


### [48] [DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets](https://arxiv.org/abs/2511.03911)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: DecoHD introduces a decomposition method for hyperdimensional computing that compresses learned class prototypes while maintaining accuracy and enabling efficient hardware acceleration.


<details>
  <summary>Details</summary>
Motivation: Traditional hyperdimensional computing decompositions use fixed atomic hypervectors unsuitable for compressing learned class prototypes, leading to reduced concentration and robustness when shrinking networks.

Method: DecoHD learns directly in a decomposed HDC parameterization using shared per-layer channels with multiplicative binding across layers and bundling at the end, with a lightweight bundling head for class-axis compression.

Result: Achieves extreme memory savings with minimal accuracy loss (0.1-0.15% average degradation), improved robustness to noise, 97% fewer trainable parameters, and significant energy/speed gains over CPU, GPU, and baseline HDC ASIC.

Conclusion: DecoHD enables efficient hyperdimensional computing decomposition that preserves performance while achieving substantial hardware benefits, making it suitable for constrained deployment environments.

Abstract: Decomposition is a proven way to shrink deep networks without changing I/O.
We bring this idea to hyperdimensional computing (HDC), where footprint cuts
usually shrink the feature axis and erode concentration and robustness. Prior
HDC decompositions decode via fixed atomic hypervectors, which are ill-suited
for compressing learned class prototypes. We introduce DecoHD, which learns
directly in a decomposed HDC parameterization: a small, shared set of per-layer
channels with multiplicative binding across layers and bundling at the end,
yielding a large representational space from compact factors. DecoHD compresses
along the class axis via a lightweight bundling head while preserving native
bind-bundle-score; training is end-to-end, and inference remains pure HDC,
aligning with in/near-memory accelerators. In evaluation, DecoHD attains
extreme memory savings with only minor accuracy degradation under tight
deployment budgets. On average it stays within about 0.1-0.15% of a strong
non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip
noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters,
and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU
(AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x
over a baseline HDC ASIC.

</details>


### [49] [On Predicting Sociodemographics from Mobility Signals](https://arxiv.org/abs/2511.03924)
*Ekin Uğurel,Cynthia Chen,Brian H. Y. Lee,Filipe Rodrigues*

Main category: cs.LG

TL;DR: Paper introduces methods to improve sociodemographic prediction from mobility data using interpretable graph features, uncertainty metrics, and multitask learning.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with weak mobility-sociodemographic relationships and poor generalization across contexts.

Method: Develops directed mobility graph features, diagnostic tools for model confidence, and a multitask learning framework.

Result: Approach outperforms baselines in predicting age, gender, income, household structure, especially with limited data or distribution shifts.

Conclusion: Framework enables more reliable and interpretable sociodemographic inference for transportation planning.

Abstract: Inferring sociodemographic attributes from mobility data could help
transportation planners better leverage passively collected datasets, but this
task remains difficult due to weak and inconsistent relationships between
mobility patterns and sociodemographic traits, as well as limited
generalization across contexts. We address these challenges from three angles.
First, to improve predictive accuracy while retaining interpretability, we
introduce a behaviorally grounded set of higher-order mobility descriptors
based on directed mobility graphs. These features capture structured patterns
in trip sequences, travel modes, and social co-travel, and significantly
improve prediction of age, gender, income, and household structure over
baselines features. Second, we introduce metrics and visual diagnostic tools
that encourage evenness between model confidence and accuracy, enabling
planners to quantify uncertainty. Third, to improve generalization and sample
efficiency, we develop a multitask learning framework that jointly predicts
multiple sociodemographic attributes from a shared representation. This
approach outperforms single-task models, particularly when training data are
limited or when applying models across different time periods (i.e., when the
test set distribution differs from the training set).

</details>


### [50] [SynQuE: Estimating Synthetic Dataset Quality Without Annotations](https://arxiv.org/abs/2511.03928)
*Arthur Chen,Victor Zhong*

Main category: cs.LG

TL;DR: This paper introduces SynQuE, a framework for ranking synthetic datasets by task performance using limited real data. It proposes proxy metrics including LENS (LLM-based), showing 8.1% accuracy improvement when selecting top synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical challenge of ranking synthetic datasets by their expected real-world task performance using only limited unannotated real data, which is important when data is scarce due to collection costs or privacy constraints.

Method: The authors introduce and formalize the SynQuE problem and propose three types of proxy metrics: distribution and diversity-based distance measures adapted via embedding models, and a novel method called LENS that leverages large language model reasoning for complex tasks. They also create comprehensive benchmarks for evaluating these proxy metrics.

Result: The results show that SynQuE proxies correlate with real task performance across diverse tasks including sentiment analysis, Text2SQL, web navigation, and image classification. LENS consistently outperforms other methods on complex tasks. On text-to-SQL parsing, using top-3 datasets selected via SynQuE proxies raised accuracy from 30.4% to 38.4% (+8.1%) compared to random selection.

Conclusion: This work establishes SynQuE as a practical framework for synthetic data selection under real-data scarcity and motivates future research on foundation model-based data characterization and fine-grained data selection.

Abstract: We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE)
problem: ranking synthetic datasets by their expected real-world task
performance using only limited unannotated real data. This addresses a critical
and open challenge where data is scarce due to collection costs or privacy
constraints. We establish the first comprehensive benchmarks for this problem
by introducing and evaluating proxy metrics that choose synthetic data for
training to maximize task performance on real data. We introduce the first
proxy metrics for SynQuE by adapting distribution and diversity-based distance
measures to our context via embedding models. To address the shortcomings of
these metrics on complex planning tasks, we propose LENS, a novel proxy that
leverages large language model reasoning. Our results show that SynQuE proxies
correlate with real task performance across diverse tasks, including sentiment
analysis, Text2SQL, web navigation, and image classification, with LENS
consistently outperforming others on complex tasks by capturing nuanced
characteristics. For instance, on text-to-SQL parsing, training on the top-3
synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to
38.4 (+8.1)% on average compared to selecting data indiscriminately. This work
establishes SynQuE as a practical framework for synthetic data selection under
real-data scarcity and motivates future research on foundation model-based data
characterization and fine-grained data selection.

</details>


### [51] [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)
*NVIDIA,:,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping,Boxin Wang,Zhuolin Yang,Nayeon Lee,Shaokun Zhang,Fuxiao Liu,Zhiqi Li,Di Zhang,Greg Heinrich,Hongxu,Yin,Song Han,Pavlo Molchanov,Parth Mannan,Yao Xu,Jane Polak Scowcroft,Tom Balough,Subhashree Radhakrishnan,Paris Zhang,Sean Cha,Ratnesh Kumar,Zaid Pervaiz Bhat,Jian Zhang,Darragh Hanley,Pritam Biswas,Jesse Oliver,Kevin Vasques,Roger Waleffe,Duncan Riach,Oluwatobi Olabiyi,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Pritam Gundecha,Khanh Nguyen,Alexandre Milesi,Eugene Khvedchenia,Ran Zilberstein,Ofri Masad,Natan Bagrov,Nave Assaf,Tomer Asida,Daniel Afrimi,Amit Zuker,Netanel Haber,Zhiyu Cheng,Jingyu,Xin,Di,Wu,Nik Spirin,Maryam Moosaei,Roman Ageev,Vanshil Atul Shah,Yuting Wu,Daniel Korzekwa,Unnikrishnan Kizhakkemadam Sreekumar,Wanli Jiang,Padmavathy Subramanian,Alejandra Rico,Sandip Bhaskar,Saeid Motiian,Kedi Wu,Annie Surla,Chia-Chih Chen,Hayden Wolff,Matthew Feinberg,Melissa Corpuz,Marek Wawrzos,Eileen Long,Aastha Jhunjhunwala,Paul Hendricks,Farzan Memarian,Benika Hall,Xin-Yu Wang,David Mosallanezhad,Soumye Singhal,Luis Vega,Katherine Cheung,Krzysztof Pawelec,Michael Evans,Katherine Luna,Jie Lou,Erick Galinkin,Akshay Hazare,Kaustubh Purandare,Ann Guan,Anna Warno,Chen Cui,Yoshi Suhara,Shibani Likhite,Seph Mard,Meredith Price,Laya Sleiman,Saori Kaji,Udi Karpas,Kari Briski,Joey Conway,Michael Lightstone,Jan Kautz,Mohammad Shoeybi,Mostofa Patwary,Jonathen Cohen,Oleksii Kuchaiev,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron Nano V2 VL is an advanced vision-language model excelling in document understanding, video comprehension, and reasoning, with improved performance and efficiency over its predecessor.


<details>
  <summary>Details</summary>
Motivation: To enhance real-world document understanding, long video comprehension, and reasoning capabilities beyond previous models.

Method: Builds on a hybrid Mamba-Transformer LLM with token reduction techniques for efficient inference, utilizing improved architecture, datasets, and training recipes.

Result: Significant improvements across vision and text domains with higher throughput in long documents and videos.

Conclusion: The model and resources are publicly released to advance vision-language applications.

Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.

</details>


### [52] [LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction](https://arxiv.org/abs/2511.03938)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Pietro Mercati,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TL;DR: LogHD reduces hyperdimensional computing memory from O(CD) to O(D log C) using logarithmic class-axis compression while preserving dimensionality and improving robustness.


<details>
  <summary>Details</summary>
Motivation: Standard HDC requires O(CD) memory which is inefficient for constrained systems, and prior compaction methods that reduce dimensionality weaken robustness.

Method: Replace C per-class prototypes with n≈⌈log_k C⌉ bundle hypervectors using capacity-aware codebook and profile-based decoding, composable with feature-axis sparsification.

Result: Achieves competitive accuracy with smaller models, higher resilience at matched memory, sustains accuracy at 2.5-3.0× higher bit-flip rates, and delivers 498× energy efficiency and 62.6× speedup over CPU.

Conclusion: LogHD provides logarithmic memory reduction while maintaining or improving robustness and computational efficiency compared to standard HDC and feature-axis compression methods.

Abstract: Hyperdimensional computing (HDC) suits memory, energy, and
reliability-constrained systems, yet the standard "one prototype per class"
design requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior
compaction reduces $D$ (feature axis), improving storage/compute but weakening
robustness. We introduce LogHD, a logarithmic class-axis reduction that
replaces the $C$ per-class prototypes with $n\!\approx\!\lceil\log_k C\rceil$
bundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional
activation space, cutting memory to $O(D\log_k C)$ while preserving $D$. LogHD
uses a capacity-aware codebook and profile-based decoding, and composes with
feature-axis sparsification. Across datasets and injected bit flips, LogHD
attains competitive accuracy with smaller models and higher resilience at
matched memory. Under equal memory, it sustains target accuracy at roughly
$2.5$-$3.0\times$ higher bit-flip rates than feature-axis compression; an ASIC
instantiation delivers $498\times$ energy efficiency and $62.6\times$ speedup
over an AMD Ryzen 9 9950X and $24.3\times$/$6.58\times$ over an NVIDIA RTX
4090, and is $4.06\times$ more energy-efficient and $2.19\times$ faster than a
feature-axis HDC ASIC baseline.

</details>


### [53] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: Survey paper summarizing recent advances beyond traditional text-based RLHF methods for LLM alignment, covering multi-modal alignment, cultural fairness, and low-latency optimization.


<details>
  <summary>Details</summary>
Motivation: Canonical text-based RLHF methods are insufficient for modern alignment needs. The paper addresses critical gaps in multi-modal alignment, cultural fairness, and optimization speed required for building equitable AI systems.

Method: Systematic review of foundational algorithms (PPO, DPO, GRPO) followed by detailed analysis of latest innovations across key alignment domains. Provides comparative synthesis of techniques.

Result: Comprehensive roadmap identifying key approaches and their comparative strengths for multi-modal alignment, cultural fairness, and low-latency optimization.

Conclusion: This survey serves as essential guidance for researchers developing more robust, efficient, and equitable AI alignment systems by synthesizing cutting-edge approaches beyond traditional RLHF.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [54] [Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels](https://arxiv.org/abs/2511.03953)
*Wuxia Chen,Taposh Banerjee,Vahid Tarokh*

Main category: cs.LG

TL;DR: Novel score-based CUSUM method for quickest change detection in high-dimensional Markov processes with unknown transition kernels, using conditional score learning instead of explicit likelihood evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional change detection methods struggle with high-dimensional Markov processes where transition kernels are unknown and likelihood evaluation is computationally challenging.

Method: Learn conditional score functions from sample pairs, develop score-based CUSUM procedure with conditional Hyvarinen scores, and propose truncated statistic for bounded increments.

Result: Proves exponential lower bounds on mean time to false alarm using Hoeffding's inequality, and asymptotic upper bounds on detection delay.

Conclusion: Provides both theoretical guarantees and practical feasibility for score-based detection in high-dimensional Markov models without explicit likelihood evaluation.

Abstract: We address the problem of quickest change detection in Markov processes with
unknown transition kernels. The key idea is to learn the conditional score
$\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs
$( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are
high-dimensional data generated by the same transition kernel. In this way, we
avoid explicit likelihood evaluation and provide a practical way to learn the
transition dynamics. Based on this estimation, we develop a score-based CUSUM
procedure that uses conditional Hyvarinen score differences to detect changes
in the kernel. To ensure bounded increments, we propose a truncated version of
the statistic. With Hoeffding's inequality for uniformly ergodic Markov
processes, we prove exponential lower bounds on the mean time to false alarm.
We also prove asymptotic upper bounds on detection delay. These results give
both theoretical guarantees and practical feasibility for score-based detection
in high-dimensional Markov models.

</details>


### [55] [PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis](https://arxiv.org/abs/2511.03966)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.LG

TL;DR: First systematic study of data unlearning for cognitive diagnosis models, proposing HIF algorithm that addresses unique challenges of removing student data while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Growing need to comply with 'right to be forgotten' requests while existing CD models lack proper data removal mechanisms; general unlearning algorithms perform poorly on CD models' heterogeneous structure.

Method: Hierarchical importance-guided forgetting (HIF) algorithm leveraging layer-wise parameter importance characteristics, using smoothing mechanism combining individual and layer-level importance for precise parameter identification.

Result: Experiments on three real-world datasets show HIF significantly outperforms baselines in balancing unlearning completeness, model utility, and efficiency.

Conclusion: HIF provides the first effective solution for CD models to handle data removal requests, enabling deployment of privacy-preserving AI systems in educational applications.

Abstract: The need to remove specific student data from cognitive diagnosis (CD) models
has become a pressing requirement, driven by users' growing assertion of their
"right to be forgotten". However, existing CD models are largely designed
without privacy considerations and lack effective data unlearning mechanisms.
Directly applying general purpose unlearning algorithms is suboptimal, as they
struggle to balance unlearning completeness, model utility, and efficiency when
confronted with the unique heterogeneous structure of CD models. To address
this, our paper presents the first systematic study of the data unlearning
problem for CD models, proposing a novel and efficient algorithm: hierarchical
importanceguided forgetting (HIF). Our key insight is that parameter importance
in CD models exhibits distinct layer wise characteristics. HIF leverages this
via an innovative smoothing mechanism that combines individual and layer, level
importance, enabling a more precise distinction of parameters associated with
the data to be unlearned. Experiments on three real world datasets show that
HIF significantly outperforms baselines on key metrics, offering the first
effective solution for CD models to respond to user data removal requests and
for deploying high-performance, privacy preserving AI systems

</details>


### [56] [Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models](https://arxiv.org/abs/2511.03972)
*Semih Cayci*

Main category: cs.LG

TL;DR: Analysis shows stochastic Gauss-Newton method in overparameterized networks achieves favorable generalization when Gauss-Newton matrix has large minimum eigenvalues, with explicit bounds on convergence and generalization.


<details>
  <summary>Details</summary>
Motivation: To understand how higher-order optimization methods affect generalization in deep learning, specifically analyzing the impact of curvature, batch size, and overparameterization

Method: Stochastic Gauss-Newton method with Levenberg-Marquardt damping and mini-batch sampling for training overparameterized deep neural networks with smooth activations in regression

Result: Established finite-time convergence bounds with explicit dependencies on batch size, network width and depth, and derived non-asymptotic generalization bounds using uniform stability in the overparameterized regime

Conclusion: SGN exhibits favorable generalization properties in the overparameterized regime, with the minimum eigenvalue of the Gauss-Newton matrix along the optimization path serving as a key factor for achieving tighter stability bounds.

Abstract: An important question in deep learning is how higher-order optimization
methods affect generalization. In this work, we analyze a stochastic
Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch
sampling for training overparameterized deep neural networks with smooth
activations in a regression setting. Our theoretical contributions are twofold.
First, we establish finite-time convergence bounds via a variable-metric
analysis in parameter space, with explicit dependencies on the batch size,
network width and depth. Second, we derive non-asymptotic generalization bounds
for SGN using uniform stability in the overparameterized regime, characterizing
the impact of curvature, batch size, and overparameterization on generalization
performance. Our theoretical results identify a favorable generalization regime
for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along
the optimization path yields tighter stability bounds.

</details>


### [57] [PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction](https://arxiv.org/abs/2511.03976)
*Xu Zou*

Main category: cs.LG

TL;DR: PETRA is a novel transformer model that uses evolutionary trajectories from phylogenetic trees to predict SARS-CoV-2 mutations, outperforming baselines and enabling real-time mutation forecasting.


<details>
  <summary>Details</summary>
Motivation: SARS-CoV-2's rapid evolutionary trajectory and immune-evasive variants pose challenges to public health and vaccine development, while existing GPT models are limited in handling noisy viral genomic sequences directly.

Method: PETRA uses a transformer-based approach that models evolutionary trajectories derived from phylogenetic trees instead of raw RNA sequences, with a weighted training framework to handle geographical and temporal data imbalances.

Result: PETRA achieved weighted recall@1 of 9.45% for nucleotide mutations and 17.10% for spike amino-acid mutations, significantly outperforming the best baselines (0.49% and 6.64% respectively), and successfully predicted mutations for major clades in real-time.

Conclusion: PETRA provides an effective methodology for predicting SARS-CoV-2 mutations using evolutionary trajectories from phylogenetic trees, demonstrating superior performance over existing baselines and practical utility in real-time mutation prediction.

Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable
evolutionary trajectory, characterized by the continual emergence of
immune-evasive variants. This poses persistent challenges to public health and
vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have
revolutionized the modeling of sequential data, their direct applications to
noisy viral genomic sequences are limited. In this paper, we introduce
PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based
on evolutionary trajectories derived from phylogenetic trees rather than raw
RNA sequences. This method effectively mitigates sequencing noise and captures
the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and
temporal imbalances in global sequence data, PETRA excels in predicting future
SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide
mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and
6.64% respectively for the best baseline. PETRA also demonstrates its ability
to aid in the real-time mutation prediction of major clades like 24F(XEC) and
25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra

</details>


### [58] [Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models](https://arxiv.org/abs/2511.03981)
*Yuxiao Wang,Di Wu,Feng Liu,Zhimin Qiu,Chenrui Hu*

Main category: cs.LG

TL;DR: A composable fine-tuning method using graph structural priors and modular adapters to improve multi-task adaptation efficiency and stability in large pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Large-scale pre-trained models face high computational costs and structural instability during multi-task adaptation due to path conflicts and redundant computations.

Method: Uses relation matrices to model task dependencies as graph structural priors, with modular adapters embedded via low-rank mapping and pluggable mechanisms for efficient cross-task composition.

Result: Significant enhancements in task accuracy, adapter weight allocation precision, and computational efficiency while maintaining model lightweight design.

Conclusion: The framework demonstrates that integrating graph priors with modular mechanisms provides synergistic advantages for composable fine-tuning in multilingual scenarios.

Abstract: This paper proposes a composable fine-tuning method that integrates graph
structural priors with modular adapters to address the high computational cost
and structural instability faced by large-scale pre-trained models in
multi-task adaptation. The method introduces a relation matrix to model
dependencies among tasks, explicitly encoding correlations between nodes and
paths into graph structural priors, which provide unified structural
constraints for adapter weight allocation and path selection. Modular adapters
are embedded into different layers through low-rank mapping and a pluggable
mechanism, enabling efficient cross-task composition and reuse under prior
guidance. This mechanism not only improves parameter efficiency and training
stability but also alleviates path conflicts and redundant computation in
multi-task scenarios. Furthermore, experiments on hyperparameter sensitivity,
environmental sensitivity, and data sensitivity are conducted to systematically
analyze key factors such as routing temperature, gating thresholds, and
relation matrix regularization strength, verifying the consistency and superior
performance of the method under structural constraints. The results demonstrate
that the proposed framework significantly enhances task prediction accuracy,
adapter weight allocation precision, and overall computational efficiency while
maintaining model lightweight design, highlighting the synergistic advantages
of graph priors and modular mechanisms in composable fine-tuning.

</details>


### [59] [TwIST: Rigging the Lottery in Transformers with Independent Subnetwork Training](https://arxiv.org/abs/2511.03983)
*Michael Menezes,Barbara Su,Xinze Feng,Yehya Farhat,Hamza Shili,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: TwIST is a distributed training framework that identifies high-quality sparse LLM subnetworks during training, enabling zero-cost deployment with better performance than post-training methods.


<details>
  <summary>Details</summary>
Motivation: To enable efficient large language model sparsification without requiring post-training procedures such as calibration or Hessian-based recovery, allowing for zero-cost pruning at deployment time.

Method: TwIST trains multiple subnetworks in parallel, periodically aggregates their parameters, and resamples new subnetworks during training to identify high-quality subnetworks ("golden tickets").

Result: TwIST achieves perplexity competitive with state-of-the-art post-training sparsification methods, with benefits most pronounced under aggressive sparsity (e.g., 50%+), reaching 23.14 PPL compared to 31.64 for the closest prior approach.

Conclusion: TwIST provides an efficient training-time path to deployable sparse LLMs without additional fine-tuning or recovery overhead, enabling zero-cost pruning at deployment with competitive perplexity.

Abstract: We introduce TwIST, a distributed training framework for efficient large
language model (LLM) sparsification. TwIST trains multiple subnetworks in
parallel, periodically aggregates their parameters, and resamples new
subnetworks during training. This process identifies high-quality subnetworks
("golden tickets") without requiring post-training procedures such as
calibration or Hessian-based recovery. As a result, TwIST enables zero-cost
pruning at deployment time while achieving perplexity competitive with
state-of-the-art post-training sparsification methods. The benefits are most
pronounced under aggressive sparsity (e.g., 50%+), where TwIST significantly
outperforms baseline methods; for example, reaching 23.14 PPL compared to 31.64
for the closest prior approach. Unlike unstructured pruning, TwIST produces
structured, dense matrices that offer practical inference speedups and memory
reductions on commodity hardware (e.g., CPUs) that do not support efficient
sparse computation. TwIST provides an efficient training-time path to
deployable sparse LLMs without additional fine-tuning or recovery overhead.

</details>


### [60] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: Non-invasive metabolic phenotyping using CGM and wearables can identify specific dysglycemia subtypes for precision diabetes prevention.


<details>
  <summary>Details</summary>
Motivation: Current diabetes classification by static glucose thresholds misses pathophysiological heterogeneity driven by insulin resistance, beta-cell dysfunction, and incretin deficiency.

Method: Machine learning models analyze high-resolution glucose data from at-home CGM-enabled oral glucose tolerance tests and integrate wearable data on diet, sleep, and activity patterns.

Result: Predicts gold-standard measures of muscle IR and beta-cell function; identifies unique PPGR patterns and associations with lifestyle factors; shows phenotype-dependent efficacy of dietary mitigators.

Conclusion: CGM-based approach deconstructs early dysglycemia into actionable subphenotypes, enabling targeted nutritional, behavioral, and pharmacological interventions tailored to individual metabolic defects.

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [61] [Multiscale Astrocyte Network Calcium Dynamics for Biologically Plausible Intelligence in Anomaly Detection](https://arxiv.org/abs/2511.03993)
*Berk Iskar,Michael Taynnan Barros*

Main category: cs.LG

TL;DR: A bio-inspired Ca²⁺-modulated learning framework improves network anomaly detection by simulating astrocytic calcium dynamics, achieving ~98% accuracy with minimal runtime overhead after precomputation.


<details>
  <summary>Details</summary>
Motivation: Traditional offline-trained network anomaly detectors struggle with concept drift and new threats like zero-day attacks, requiring more adaptive solutions.

Method: Couples a multicellular astrocyte dynamics simulator (modeling IP₃-mediated Ca²⁺ release, SERCA pump uptake, and gap junction diffusion) with a deep neural network.

Result: Outperforms baseline DNN on CTU-13 network traffic data with up to ~98% accuracy, reduced false positives/negatives across multiple train/test splits.

Conclusion: The framework provides a generic, biologically grounded solution for streaming detection tasks needing rapid adaptation to evolving data patterns, demonstrated effectively in cybersecurity.

Abstract: Network anomaly detection systems encounter several challenges with
traditional detectors trained offline. They become susceptible to concept drift
and new threats such as zero-day or polymorphic attacks. To address this
limitation, we propose a Ca$^{2+}$-modulated learning framework that draws
inspiration from astrocytic Ca$^{2+}$ signaling in the brain, where rapid,
context-sensitive adaptation enables robust information processing. Our
approach couples a multicellular astrocyte dynamics simulator with a deep
neural network (DNN). The simulator models astrocytic Ca$^{2+}$ dynamics
through three key mechanisms: IP$_3$-mediated Ca$^{2+}$ release, SERCA pump
uptake, and conductance-aware diffusion through gap junctions between cells.
Evaluation of our proposed network on CTU-13 (Neris) network traffic data
demonstrates the effectiveness of our biologically plausible approach. The
Ca$^{2+}$-gated model outperforms a matched baseline DNN, achieving up to
$\sim$98\% accuracy with reduced false positives and negatives across multiple
train/test splits. Importantly, this improved performance comes with negligible
runtime overhead once Ca$^{2+}$ trajectories are precomputed. While
demonstrated here for cybersecurity applications, this Ca$^{2+}$-modulated
learning framework offers a generic solution for streaming detection tasks that
require rapid, biologically grounded adaptation to evolving data patterns.

</details>


### [62] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: Efficient synthetic data generation enables scalable meta-learning of decision trees with comparable performance to real data or optimal trees, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Decision trees are crucial in high-stakes domains like finance and healthcare due to their interpretability, but meta-learning them efficiently requires scalable pre-training data generation.

Method: Generate synthetic pre-training data by sampling near-optimal decision trees, then use MetaTree transformer architecture for meta-learning.

Result: Achieves performance comparable to pre-training on real-world data or computationally expensive optimal decision trees.

Conclusion: This approach significantly reduces computational costs, enhances data generation flexibility, and enables scalable meta-learning of interpretable decision tree models.

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [63] [Accelerating scientific discovery with the common task framework](https://arxiv.org/abs/2511.04001)
*J. Nathan Kutz,Peter Battaglia,Michael Brenner,Kevin Carlberg,Aric Hagberg,Shirley Ho,Stephan Hoyer,Henning Lange,Hod Lipson,Michael W. Mahoney,Frank Noe,Max Welling,Laure Zanna,Francis Zhu,Steven L. Brunton*

Main category: cs.LG

TL;DR: Introduces a common task framework (CTF) with challenge datasets to objectively evaluate diverse ML/AI algorithms for scientific and engineering applications like forecasting, reconstruction, generalization, and control.


<details>
  <summary>Details</summary>
Motivation: ML/AI algorithms are advancing rapidly in science and engineering, but there are no standardized metrics to fairly compare their performance across diverse objectives like forecasting and control, especially with limited/noisy data.

Method: Proposes a CTF featuring growing collections of challenge datasets that provide common objectives and evaluation criteria for algorithms.

Result: The CTF enables objective comparison of ML/AI methods, similar to its success in domains like speech recognition and computer vision.

Conclusion: A CTF is critically needed to accelerate ML/AI progress in science and engineering by providing standardized benchmarks for algorithm evaluation.

Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are
transforming and empowering the characterization and control of dynamic systems
in the engineering, physical, and biological sciences. These emerging modeling
paradigms require comparative metrics to evaluate a diverse set of scientific
objectives, including forecasting, state reconstruction, generalization, and
control, while also considering limited data scenarios and noisy measurements.
We introduce a common task framework (CTF) for science and engineering, which
features a growing collection of challenge data sets with a diverse set of
practical and common objectives. The CTF is a critically enabling technology
that has contributed to the rapid advance of ML/AI algorithms in traditional
applications such as speech recognition, language processing, and computer
vision. There is a critical need for the objective metrics of a CTF to compare
the diverse algorithms being rapidly developed and deployed in practice today
across science and engineering.

</details>


### [64] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: First autoregressive-aware split computing framework for LLMs on edge devices that combines strategic model partitioning, intermediate compression, and unified optimization to reduce communication overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs cannot be deployed on resource-constrained IoT devices due to massive parameter footprints and memory-intensive autoregressive decoding, while existing split computing approaches fail to address autoregressive inference challenges.

Method: Three key contributions: 1) One-point split compression (OPSC) for mixed-precision quantization and strategic model partitioning, 2) Two-stage intermediate compression pipeline with threshold splitting and token-wise adaptive bit quantization, 3) Unified optimization framework for joint selection of split points, quantization settings, and sequence lengths.

Result: Achieves 1.49× inference speedup and significant communication overhead reduction while maintaining or improving model accuracy, outperforming state-of-the-art quantization methods like SmoothQuant, OmniQuant, and Atom.

Conclusion: The proposed framework successfully enables practical LLM deployment on edge devices by addressing the unique challenges of autoregressive inference through strategic partitioning and compression techniques.

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [65] [Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training](https://arxiv.org/abs/2511.04040)
*Xiaoling Luo,Peng Chen,Chengliang Liu,Xiaopeng Jin,Jie Wen,Yumeng Liu,Junsong Wang*

Main category: cs.LG

TL;DR: DSRPGO uses dynamic selection and reconstructive pre-training with BInM and DSM modules to improve multimodal protein function prediction, outperforming benchmarks on human data.


<details>
  <summary>Details</summary>
Motivation: Multimodal protein features contain diverse information (structural, sequence, attributes, interaction networks) but their complex interconnections make protein function prediction challenging.

Method: A multimodal protein function prediction method called DSRPGO that uses dynamic selection and reconstructive pre-training mechanisms, including reconstructive pre-training for fine-grained information, Bidirectional Interaction Module (BInM) for multimodal feature interaction, and Dynamic Selection Module (DSM) for hierarchical multi-label classification.

Result: The DSRPGO model achieves significant improvements in BPO, MFO, and CCO on human datasets compared to other benchmark models.

Conclusion: The proposed DSRPGO model significantly outperforms other benchmark models in protein function prediction across three categories (BPO, MFO, and CCO) on human datasets, demonstrating the effectiveness of the dynamic selection and reconstructive pre-training approach.

Abstract: Multimodal protein features play a crucial role in protein function
prediction. However, these features encompass a wide range of information,
ranging from structural data and sequence features to protein attributes and
interaction networks, making it challenging to decipher their complex
interconnections. In this work, we propose a multimodal protein function
prediction method (DSRPGO) by utilizing dynamic selection and reconstructive
pre-training mechanisms. To acquire complex protein information, we introduce
reconstructive pre-training to mine more fine-grained information with low
semantic levels. Moreover, we put forward the Bidirectional Interaction Module
(BInM) to facilitate interactive learning among multimodal features.
Additionally, to address the difficulty of hierarchical multi-label
classification in this task, a Dynamic Selection Module (DSM) is designed to
select the feature representation that is most conducive to current protein
function prediction. Our proposed DSRPGO model improves significantly in BPO,
MFO, and CCO on human datasets, thereby outperforming other benchmark models.

</details>


### [66] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: DartQuant is an efficient distribution-aware rotational calibration method that improves quantization performance with 47x acceleration and 10x memory savings, making large model quantization feasible on resource-limited hardware.


<details>
  <summary>Details</summary>
Motivation: End-to-end fine-tuning of rotational optimization algorithms for quantization is computationally expensive and prone to overfitting, creating a need for more efficient methods.

Method: Proposes distribution-aware rotational calibration that constrains activation distributions after rotation and introduces QR-Orth optimization to replace expensive alternating optimization.

Result: Achieves 47x acceleration and 10x memory savings for rotational optimization on 70B models, successfully completing calibration on a single 3090 GPU for the first time.

Conclusion: DartQuant makes large language model quantization practical in resource-constrained environments while maintaining superior performance compared to existing methods.

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [67] [Pediatric Appendicitis Detection from Ultrasound Images](https://arxiv.org/abs/2511.04069)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: cs.LG

TL;DR: Researchers developed a ResNet-based deep learning model for automated appendicitis detection from pediatric ultrasound images, achieving high accuracy (93.44%) despite imaging challenges.


<details>
  <summary>Details</summary>
Motivation: Pediatric appendicitis diagnosis is challenging due to overlapping symptoms and variable ultrasound quality, necessitating automated tools to assist clinicians.

Method: Fine-tuned pretrained ResNet architecture on the Regensburg Pediatric Appendicitis Dataset using normalized, resized, and augmented ultrasound images to classify appendicitis vs non-appendicitis.

Result: The model achieved 93.44% accuracy, 91.53% precision, and 89.8% recall, effectively handling low contrast, noise, and anatomical variability in pediatric ultrasound.

Conclusion: The deep learning approach shows strong potential for reliable automated appendicitis detection in children, improving diagnostic consistency in clinical settings.

Abstract: Pediatric appendicitis remains one of the most common causes of acute
abdominal pain in children, and its diagnosis continues to challenge clinicians
due to overlapping symptoms and variable imaging quality. This study aims to
develop and evaluate a deep learning model based on a pretrained ResNet
architecture for automated detection of appendicitis from ultrasound images. We
used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound
scans, laboratory data, and clinical scores from pediatric patients admitted
with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each
subject had 1 to 15 ultrasound views covering the right lower quadrant,
appendix, lymph nodes, and related structures. For the image based
classification task, ResNet was fine tuned to distinguish appendicitis from
non-appendicitis cases. Images were preprocessed by normalization, resizing,
and augmentation to enhance generalization. The proposed ResNet model achieved
an overall accuracy of 93.44, precision of 91.53, and recall of 89.8,
demonstrating strong performance in identifying appendicitis across
heterogeneous ultrasound views. The model effectively learned discriminative
spatial features, overcoming challenges posed by low contrast, speckle noise,
and anatomical variability in pediatric imaging.

</details>


### [68] [Left Atrial Segmentation with nnU-Net Using MRI](https://arxiv.org/abs/2511.04071)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: cs.LG

TL;DR: nnU-Net framework applied to left atrium MRI segmentation achieved 93.5% Dice score, outperforming traditional methods and showing robust generalization across anatomical variations.


<details>
  <summary>Details</summary>
Motivation: Manual LA segmentation from cardiac MRI is time-consuming, observer-dependent, and impractical for clinical workflows, necessitating automated deep learning solutions.

Method: Applied nnU-Net framework to Left Atrial Segmentation Challenge 2013 dataset (30 MRI scans with expert annotations), using automated preprocessing, network configuration, and training pipeline adaptation.

Result: Achieved mean Dice score of 93.5%, demonstrating high overlap with expert annotations and outperforming traditional segmentation approaches, with robust generalization across LA shape, contrast, and image quality variations.

Conclusion: nnU-Net provides an effective automated solution for accurate LA segmentation, suitable for clinical applications in AF ablation guidance and cardiac modeling.

Abstract: Accurate segmentation of the left atrium (LA) from cardiac MRI is critical
for guiding atrial fibrillation (AF) ablation and constructing biophysical
cardiac models. Manual delineation is time-consuming, observer-dependent, and
impractical for large-scale or time-sensitive clinical workflows. Deep learning
methods, particularly convolutional architectures, have recently demonstrated
superior performance in medical image segmentation tasks. In this study, we
applied the nnU-Net framework, an automated, self-configuring deep learning
segmentation architecture, to the Left Atrial Segmentation Challenge 2013
dataset. The dataset consists of thirty MRI scans with corresponding
expert-annotated masks. The nnU-Net model automatically adapted its
preprocessing, network configuration, and training pipeline to the
characteristics of the MRI data. Model performance was quantitatively evaluated
using the Dice similarity coefficient (DSC), and qualitative results were
compared against expert segmentations. The proposed nnUNet model achieved a
mean Dice score of 93.5, demonstrating high overlap with expert annotations and
outperforming several traditional segmentation approaches reported in previous
studies. The network exhibited robust generalization across variations in left
atrial shape, contrast, and image quality, accurately delineating both the
atrial body and proximal pulmonary veins.

</details>


### [69] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于学习的过滤近似最近邻搜索方法，通过学习数据驱动的权重来替代固定的惩罚机制，相比传统方法提高了5-10%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的过滤ANN搜索方法使用固定的、独立于数据的惩罚机制，这种方法无法很好地适应不同数据集中的不同标签和向量分布，缺乏泛化能力。

Method: 将过滤ANN搜索问题建模为约束线性优化问题，直接从数据中学习向量距离和过滤匹配之间的最优权衡权重，这些权重同时指导搜索过程和索引构建。

Result: 实验表明，基于学习的方法相比固定惩罚方法在准确率上有5-10%的提升，创建的图索引结构更能有效捕捉底层过滤分布和语义。

Conclusion: 提出的数据自适应距离函数为过滤ANN搜索问题提供了一个更灵活和可泛化的框架，通过学习最优权重显著改善了搜索性能。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [70] [DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection](https://arxiv.org/abs/2511.04086)
*Qingfeng Chen,Haojin Zeng,Jingyi Jie,Shichao Zhang,Debo Cheng*

Main category: cs.LG

TL;DR: DeNoise framework for robust unsupervised graph-level anomaly detection that handles contaminated training data through adversarial optimization and denoising mechanisms.


<details>
  <summary>Details</summary>
Motivation: Most GNN approaches assume clean training data, but real-world data often contains anomalous graphs that degrade performance.

Method: Joint optimization of graph encoder, attribute decoder, and structure decoder with adversarial objective; encoder anchor-alignment denoising; contrastive learning component.

Result: Outperforms state-of-the-art UGAD baselines on eight real-world datasets under varying noise intensities.

Conclusion: DeNoise effectively learns noise-resistant representations for graph-level anomaly detection in contaminated data scenarios.

Abstract: With the rapid growth of graph-structured data in critical domains,
unsupervised graph-level anomaly detection (UGAD) has become a pivotal task.
UGAD seeks to identify entire graphs that deviate from normal behavioral
patterns. However, most Graph Neural Network (GNN) approaches implicitly assume
that the training set is clean, containing only normal graphs, which is rarely
true in practice. Even modest contamination by anomalous graphs can distort
learned representations and sharply degrade performance. To address this
challenge, we propose DeNoise, a robust UGAD framework explicitly designed for
contaminated training data. It jointly optimizes a graph-level encoder, an
attribute decoder, and a structure decoder via an adversarial objective to
learn noise-resistant embeddings. Further, DeNoise introduces an encoder
anchor-alignment denoising mechanism that fuses high-information node
embeddings from normal graphs into all graph embeddings, improving
representation quality while suppressing anomaly interference. A contrastive
learning component then compacts normal graph embeddings and repels anomalous
ones in the latent space. Extensive experiments on eight real-world datasets
demonstrate that DeNoise consistently learns reliable graph-level
representations under varying noise intensities and significantly outperforms
state-of-the-art UGAD baselines.

</details>


### [71] [KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea](https://arxiv.org/abs/2511.04094)
*Hyungjong Na,Wonho Song,Seungyong Han,Donghyeon Jo,Sejin Myung,Hyungjoon Kim*

Main category: cs.LG

TL;DR: Korean Tax Avoidance Panel (KoTaP) is a comprehensive dataset of Korean non-financial firms (2011-2024) linking tax avoidance to financial performance, stability, growth, and governance metrics for research applications.


<details>
  <summary>Details</summary>
Motivation: To create a standardized panel dataset that treats corporate tax avoidance as a predictor variable and links it to multiple financial and governance domains, while reflecting unique Korean institutional features.

Method: Constructed a balanced panel dataset from 12,653 firm-year observations (1,754 firms) after excluding financial firms, non-December fiscal years, capital impairment, and negative pre-tax income. Tax avoidance measured using CETR, GETR, TSTA, and TSDA indicators.

Result: KoTaP provides a standardized dataset with international comparability while capturing Korean-specific features like concentrated ownership, high foreign shareholding, and elevated liquidity ratios. The dataset shows consistency with international literature on core indicator distributions and correlations.

Conclusion: KoTaP serves as a critical open resource enabling applications in econometric modeling, deep learning, policy evaluation, audit planning, and investment analysis for accounting, finance, and interdisciplinary research.

Abstract: This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term
panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011
and 2024. After excluding financial firms, firms with non-December fiscal year
ends, capital impairment, and negative pre-tax income, the final dataset
consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed
to treat corporate tax avoidance as a predictor variable and link it to
multiple domains, including earnings management (accrual- and activity-based),
profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,
INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance
itself is measured using complementary indicators cash effective tax rate
(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,
TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is
its balanced panel structure with standardized variables and its consistency
with international literature on the distribution and correlation of core
indicators. At the same time, it reflects distinctive institutional features of
Korean firms, such as concentrated ownership, high foreign shareholding, and
elevated liquidity ratios, providing both international comparability and
contextual uniqueness. KoTaP enables applications in benchmarking econometric
and deep learning models, external validity checks, and explainable AI
analyses. It further supports policy evaluation, audit planning, and investment
analysis, making it a critical open resource for accounting, finance, and
interdisciplinary research.

</details>


### [72] [Decomposable Neuro Symbolic Regression](https://arxiv.org/abs/2511.04124)
*Giorgio Morales,John W. Sheppard*

Main category: cs.LG

TL;DR: 提出了一种可分解的符号回归方法，结合transformer模型、遗传算法和遗传编程，从训练好的回归模型中提取可解释的数学表达式


<details>
  <summary>Details</summary>
Motivation: 现有的符号回归方法通常优先最小化预测误差而非识别控制方程，导致产生过于复杂或不准确的表达式，需要更可解释的方法

Method: 使用多集transformer生成单变量符号骨架，通过遗传算法评估和选择高质量候选，再用遗传编程的级联程序合并骨架并保持结构，最后通过遗传算法优化系数

Result: 在受控和不同噪声程度的问题上评估，相比两种基于遗传编程的方法、三种神经符号回归方法和一种混合方法，该方法表现出更低或相当的插值和外推误差

Conclusion: 该方法能够一致地学习与原始数学结构匹配的表达式，提供了比现有方法更好的可解释性和准确性

Abstract: Symbolic regression (SR) models complex systems by discovering mathematical
expressions that capture underlying relationships in observed data. However,
most SR methods prioritize minimizing prediction error over identifying the
governing equations, often producing overly complex or inaccurate expressions.
To address this, we present a decomposable SR method that generates
interpretable multivariate expressions leveraging transformer models, genetic
algorithms (GAs), and genetic programming (GP). In particular, our explainable
SR method distills a trained ``opaque'' regression model into mathematical
expressions that serve as explanations of its computed function. Our method
employs a Multi-Set Transformer to generate multiple univariate symbolic
skeletons that characterize how each variable influences the opaque model's
response. We then evaluate the generated skeletons' performance using a
GA-based approach to select a subset of high-quality candidates before
incrementally merging them via a GP-based cascade procedure that preserves
their original skeleton structure. The final multivariate skeletons undergo
coefficient optimization via a GA. We evaluated our method on problems with
controlled and varying degrees of noise, demonstrating lower or comparable
interpolation and extrapolation errors compared to two GP-based methods, three
neural SR methods, and a hybrid approach. Unlike them, our approach
consistently learned expressions that matched the original mathematical
structure.

</details>


### [73] [Exploring the Feasibility of End-to-End Large Language Model as a Compiler](https://arxiv.org/abs/2511.04132)
*Hongbin Zhang,Shihao Gao,Yang Liu,Mingjie Xing,Yanjun Wu,Chen Zhao*

Main category: cs.LG

TL;DR: The paper explores using Large Language Models as Compilers (LaaC). It introduces CompilerEval for evaluation, finds current LLMs have basic compiler capability but low success rates, and proposes optimization methods for future improvement.


<details>
  <summary>Details</summary>
Motivation: While LLMs assist in compiler development, their potential as end-to-end compilers remains unexplored. The paper addresses this gap by investigating LLMs' feasibility in directly compiling code.

Method: Designed CompilerEval dataset and framework to evaluate LLM capabilities in code comprehension and assembly generation. Analyzed errors, tested optimization methods like prompt engineering, model scaling, and reasoning techniques.

Result: LLMs show basic compiler capabilities but currently achieve low compilation success rates. Optimization methods significantly enhance assembly code quality. Cross-platform compilation was also evaluated.

Conclusion: The paper is optimistic about LaaC's potential. With targeted training, knowledge-rich prompts, and specialized infrastructure, LLMs could generate high-quality assembly and drive a compilation paradigm shift.

Abstract: In recent years, end-to-end Large Language Model (LLM) technology has shown
substantial advantages across various domains. As critical system software and
infrastructure, compilers are responsible for transforming source code into
target code. While LLMs have been leveraged to assist in compiler development
and maintenance, their potential as an end-to-end compiler remains largely
unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and
its future directions. We designed the CompilerEval dataset and framework
specifically to evaluate the capabilities of mainstream LLMs in source code
comprehension and assembly code generation. In the evaluation, we analyzed
various errors, explored multiple methods to improve LLM-generated code, and
evaluated cross-platform compilation capabilities. Experimental results
demonstrate that LLMs exhibit basic capabilities as compilers but currently
achieve low compilation success rates. By optimizing prompts, scaling up the
model, and incorporating reasoning methods, the quality of assembly code
generated by LLMs can be significantly enhanced. Based on these findings, we
maintain an optimistic outlook for LaaC and propose practical architectural
designs and future research directions. We believe that with targeted training,
knowledge-rich prompts, and specialized infrastructure, LaaC has the potential
to generate high-quality assembly code and drive a paradigm shift in the field
of compilation.

</details>


### [74] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: EPO is a novel safe RL algorithm that handles infinite constraints by iteratively adjusting active constraint sets, achieving optimal performance with bounded safety violations.


<details>
  <summary>Details</summary>
Motivation: Current safe RL methods struggle with semi-infinite constraints (SI-safe RL) where safety conditions must be enforced across entire continuous parameter spaces, such as resource distribution at every spatial location.

Method: EPO iteratively solves safe RL subproblems with finite constraint sets and adaptively adjusts the active constraint set through expansion (adding violated constraints) and deletion (removing constraints with zero Lagrange multipliers).

Result: Theoretical analysis shows that EPO-trained strategies achieve performance comparable to optimal solutions while keeping global constraint violations strictly within prescribed bounds.

Conclusion: EPO successfully provides a solution for SI-safe RL by achieving optimal policy performance with deterministic bounded safety violations, addressing the challenge of infinite constraints through adaptive constraint management.

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [75] [Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories](https://arxiv.org/abs/2511.04155)
*Olav Finne Praesteng Larsen,Massimiliano Ruocco,Michail Spitieris,Abdulmajid Murad,Martina Ragosta*

Main category: cs.LG

TL;DR: Transfer learning with diffusion models enables realistic trajectory generation at data-scarce airports using just 5-20% of local data, overcoming ATM data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Secondary and regional airports face severe data scarcity, limiting the applicability of machine learning methods and the ability to perform large-scale simulations or "what-if" analyses in ATM.

Method: Adapt diffusion- and flow-matching-based generative models pretrained on data-rich Zurich airport and fine-tuned on data-scarce Dublin airport with varying amounts of local data (0% to 100%).

Result: Diffusion-based models achieve competitive performance with 5% of Dublin data and reach baseline-level performance with 20% of data, consistently outperforming models trained from scratch.

Conclusion: Transfer learning, especially with diffusion models, can substantially reduce data requirements for trajectory generation in ATM, making it feasible to generate realistic synthetic data even at airports with limited historical records.

Abstract: Access to trajectory data is a key requirement for developing and validating
Air Traffic Management (ATM) solutions, yet many secondary and regional
airports face severe data scarcity. This limits the applicability of machine
learning methods and the ability to perform large-scale simulations or
"what-if" analyses. In this paper, we investigate whether generative models
trained on data-rich airports can be efficiently adapted to data-scarce
airports using transfer learning. We adapt state-of-the-art diffusion- and
flow-matching-based architectures to the aviation domain and evaluate their
transferability between Zurich (source) and Dublin (target) landing trajectory
datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying
amounts of local data, ranging from 0% to 100%. Results show that
diffusion-based models achieve competitive performance with as little as 5% of
the Dublin data and reach baseline-level performance around 20%, consistently
outperforming models trained from scratch across metrics and visual
inspections. Latent flow matching and latent diffusion models also benefit from
pretraining, though with more variable gains, while flow matching models show
weaker generalization. Despite challenges in capturing rare trajectory
patterns, these findings demonstrate the potential of transfer learning to
substantially reduce data requirements for trajectory generation in ATM,
enabling realistic synthetic data generation even in environments with limited
historical records.

</details>


### [76] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: Transformer-based method for clinical risk classification using EHR data, handling temporal irregularity and heterogeneity through attention mechanisms and adaptive pooling.


<details>
  <summary>Details</summary>
Motivation: Address challenges in clinical risk classification with heterogeneous EHR data, including irregular temporal patterns, large modality differences, and complex semantic structures.

Method: Feature embedding for unified representation, learnable temporal encoding, multi-head self-attention for dependency modeling, semantic-weighted pooling for adaptive importance assignment, and linear mapping for risk scores.

Result: Outperforms traditional machine learning and temporal deep learning models in accuracy, recall, precision, and F1-Score.

Conclusion: Provides stable and precise risk identification in multi-source heterogeneous EHR environments, offering an efficient and reliable framework for clinical intelligent decision-making.

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [77] [On Joint Regularization and Calibration in Deep Ensembles](https://arxiv.org/abs/2511.04160)
*Laurits Fredsgaard,Mikkel N. Schmidt*

Main category: cs.LG

TL;DR: Jointly tuning weight decay, temperature scaling, and early stopping in deep ensembles improves performance and uncertainty calibration compared to individual tuning.


<details>
  <summary>Details</summary>
Motivation: While ensembles typically tune models individually, joint tuning might yield better performance, but its impact on uncertainty quantification and practical implementation strategies needs investigation.

Method: Proposes a partially overlapping holdout strategy as a practical compromise between joint evaluation and maximizing data usage for training.

Result: Joint tuning generally matches or improves performance, with effect size varying across tasks and metrics.

Conclusion: The overlapping holdout strategy offers a practical solution for joint optimization, providing valuable guidance for practitioners optimizing deep ensembles.

Abstract: Deep ensembles are a powerful tool in machine learning, improving both model
performance and uncertainty calibration. While ensembles are typically formed
by training and tuning models individually, evidence suggests that jointly
tuning the ensemble can lead to better performance. This paper investigates the
impact of jointly tuning weight decay, temperature scaling, and early stopping
on both predictive performance and uncertainty quantification. Additionally, we
propose a partially overlapping holdout strategy as a practical compromise
between enabling joint evaluation and maximizing the use of data for training.
Our results demonstrate that jointly tuning the ensemble generally matches or
improves performance, with significant variation in effect size across
different tasks and metrics. We highlight the trade-offs between individual and
joint optimization in deep ensemble training, with the overlapping holdout
strategy offering an attractive practical solution. We believe our findings
provide valuable insights and guidance for practitioners looking to optimize
deep ensemble models. Code is available at:
https://github.com/lauritsf/ensemble-optimality-gap

</details>


### [78] [ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads](https://arxiv.org/abs/2511.04162)
*Xiaokai Wang,Shaoyuan Huang,Yuting Li,Xiaofei Wang*

Main category: cs.LG

TL;DR: ScaleDL is a novel runtime prediction framework that combines nonlinear layer-wise modeling with GNN-based cross-layer interactions to accurately predict DNN runtime while reducing data collection costs.


<details>
  <summary>Details</summary>
Motivation: As DNN models grow in size and complexity, traditional runtime prediction methods struggle with accuracy and generalizability, while graph-enhanced approaches are costly. There's a need for balanced solution.

Method: Proposes ScaleDL framework using nonlinear layer-wise modeling with GNN-based cross-layer interaction mechanism, plus D-optimal method for cost reduction.

Result: Experiments on five popular DNN models show ScaleDL achieves 6x lower MRE and 5x lower RMSE compared to baseline models.

Conclusion: ScaleDL effectively balances prediction accuracy, generalizability, and data collection costs for DNN runtime prediction across different network architectures.

Abstract: Deep neural networks (DNNs) form the cornerstone of modern AI services,
supporting a wide range of applications, including autonomous driving,
chatbots, and recommendation systems. As models increase in size and
complexity, DNN workloads like training and inference tasks impose
unprecedented demands on distributed computing resources, making the accurate
prediction of runtime essential for optimizing development and resource
allocation. Traditional methods rely on additive computational unit models,
limiting their accuracy and generalizability. In contrast, graph-enhanced
modeling improves performance but significantly increases data collection
costs. Therefore, there is a critical need for a method that strikes a balance
between accuracy, generalizability, and the costs of data collection. To
address these challenges, we propose ScaleDL, a novel runtime prediction
framework that combines nonlinear layer-wise modeling with graph neural network
(GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtime
prediction and hierarchical generalizability across different network
architectures. Additionally, we employ the D-optimal method to reduce data
collection costs. Experiments on the workloads of five popular DNN models prove
that ScaleDL enhances runtime prediction accuracy and generalizability,
achieving 6$\times$ lower MRE and 5$\times$ lower RMSE compared to baseline
models.

</details>


### [79] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: This paper benchmarks PTQ methods for MXFP4 format, finds rotation-based methods incompatible with MXFP4's PoT scaling, and proposes block rotation to fix this issue.


<details>
  <summary>Details</summary>
Motivation: LLMs' growing scale imposes high costs, and while PTQ offers efficient deployment, accurate W4A4 quantization remains challenging. The emergence of MXFP4 format with hardware support raises questions about current PTQ methods' applicability.

Method: Established comprehensive benchmark of PTQ methods under MXFP4 format, analyzed incompatibility issues, and proposed block rotation strategy to adapt rotation-based methods to MXFP4's PoT block scaling.

Result: GPTQ consistently performs well, but rotation-based methods suffer severe incompatibility with MXFP4. Block rotation strategy leads to substantial accuracy improvements across diverse LLMs.

Conclusion: The findings provide clear guidance for practitioners and set foundation for advancing PTQ research under emerging low-precision formats like MXFP4.

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [80] [The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms](https://arxiv.org/abs/2511.04217)
*Hikari Otsuka,Daiki Chijiwa,Yasuyuki Okoshi,Daichi Fujiki,Susumu Takeuchi,Masato Motomura*

Main category: cs.LG

TL;DR: This paper provides the first theoretical proof that strong lottery tickets exist in multi-head attention (MHA) mechanisms and extends this to transformers, showing they can approximate target networks with exponential error reduction as hidden dimensions increase.


<details>
  <summary>Details</summary>
Motivation: The strong lottery ticket hypothesis (SLTH) lacked theoretical support for transformer architectures, particularly for the multi-head attention mechanism, which is core to transformers.

Method: The authors conduct a theoretical analysis to prove that randomly initialized MHAs with sufficient hidden dimensions contain subnetworks (strong lottery tickets) that can approximate any target MHA. They extend this analysis to transformers without normalization layers.

Result: The paper proves that an SLT exists in an MHA with high probability when the hidden dimension is O(d log(Hd^{3/2})), and empirical results show exponential decrease in approximation error with increasing hidden dimensions.

Conclusion: This work establishes the SLTH for transformers by theoretically validating the existence of strong lottery tickets in MHAs, bridging a significant gap in understanding neural network pruning and initialization strategies.

Abstract: The strong lottery ticket hypothesis (SLTH) conjectures that high-performing
subnetworks, called strong lottery tickets (SLTs), are hidden in randomly
initialized neural networks. Although recent theoretical studies have
established the SLTH across various neural architectures, the SLTH for
transformer architectures still lacks theoretical understanding. In particular,
the current theory of the SLTH does not yet account for the multi-head
attention (MHA) mechanism, a core component of transformers. To address this
gap, we introduce a theoretical analysis of the existence of SLTs within MHAs.
We prove that, if a randomly initialized MHA of $H$ heads and input dimension
$d$ has the hidden dimension $O(d\log(Hd^{3/2}))$ for the key and value, it
contains an SLT that approximates an arbitrary MHA with the same input
dimension with high probability. Furthermore, by leveraging this theory for
MHAs, we extend the SLTH to transformers without normalization layers. We
empirically validate our theoretical findings, demonstrating that the
approximation error between the SLT within a source model (MHA and transformer)
and an approximate target counterpart decreases exponentially by increasing the
hidden dimension of the source model.

</details>


### [81] [seqme: a Python library for evaluating biological sequence design](https://arxiv.org/abs/2511.04239)
*Rasmus Møller-Larsen,Adam Izdebski,Jan Olszewski,Pankhil Gawade,Michal Kmicikiewicz,Wojciech Zarzecki,Ewa Szczurek*

Main category: cs.LG

TL;DR: seqme is a modular Python library for evaluating biological sequence design methods using sequence-based, embedding-based, and property-based metrics across various biological sequences.


<details>
  <summary>Details</summary>
Motivation: There was a lack of a single software library implementing metrics to evaluate computational methods for biological sequence design, despite recent advances in the field.

Method: Developed seqme as a modular and extendable open-source Python library with three groups of model-agnostic metrics: sequence-based, embedding-based, and property-based.

Result: Created a comprehensive library applicable to various biological sequences (small molecules, DNA, ncRNA, mRNA, peptides, proteins) with embedding models, property models, diagnostics, and visualization functions.

Conclusion: seqme provides a unified framework for evaluating both one-shot and iterative computational design methods for biological sequences, addressing the gap in available evaluation tools.

Abstract: Recent advances in computational methods for designing biological sequences
have sparked the development of metrics to evaluate these methods performance
in terms of the fidelity of the designed sequences to a target distribution and
their attainment of desired properties. However, a single software library
implementing these metrics was lacking. In this work we introduce seqme, a
modular and highly extendable open-source Python library, containing
model-agnostic metrics for evaluating computational methods for biological
sequence design. seqme considers three groups of metrics: sequence-based,
embedding-based, and property-based, and is applicable to a wide range of
biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins.
The library offers a number of embedding and property models for biological
sequences, as well as diagnostics and visualization functions to inspect the
results. seqme can be used to evaluate both one-shot and iterative
computational design methods.

</details>


### [82] [Guided by Stars: Interpretable Concept Learning Over Time Series via Temporal Logic Semantics](https://arxiv.org/abs/2511.04244)
*Irene Ferfoglia,Simone Silvetti,Gaia Saveri,Laura Nenzi,Luca Bortolussi*

Main category: cs.LG

TL;DR: STELLE is a neuro-symbolic framework that combines time series classification with interpretable explanations using temporal logic concepts.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for time series classification are black-box and lack human-understandable explanations, which is problematic for safety-critical applications.

Method: Embeds time series trajectories into a space of temporal logic concepts using a novel STL-inspired kernel and jointly optimizes for accuracy and interpretability.

Result: Achieves competitive classification accuracy while providing both local (individual prediction) and global (class-characterising) explanations in human-readable STL formulae.

Conclusion: STELLE successfully unifies classification and explanation, offering a transparent alternative to black-box methods for time series analysis.

Abstract: Time series classification is a task of paramount importance, as this kind of
data often arises in safety-critical applications. However, it is typically
tackled with black-box deep learning methods, making it hard for humans to
understand the rationale behind their output. To take on this challenge, we
propose a novel approach, STELLE (Signal Temporal logic Embedding for
Logically-grounded Learning and Explanation), a neuro-symbolic framework that
unifies classification and explanation through direct embedding of trajectories
into a space of temporal logic concepts. By introducing a novel STL-inspired
kernel that maps raw time series to their alignment with predefined STL
formulae, our model jointly optimises accuracy and interpretability, as each
prediction is accompanied by the most relevant logical concepts that
characterise it. This yields (i) local explanations as human-readable STL
conditions justifying individual predictions, and (ii) global explanations as
class-characterising formulae. Experiments demonstrate that STELLE achieves
competitive accuracy while providing logically faithful explanations, validated
on diverse real-world benchmarks.

</details>


### [83] [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)
*Matteo Cercola,Valeria Capretti,Simone Formentin*

Main category: cs.LG

TL;DR: A hybrid framework combining RLHF's scalability and PBO's query efficiency for active preference learning, validated in high-dimensional optimization and LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Human preference data is costly to collect, requiring more efficient learning methods than existing RLHF and PBO approaches alone.

Method: Integrate an acquisition-driven module into the RLHF pipeline to actively gather preferences, leveraging both scalability and sample efficiency.

Result: Experiments show consistent improvements in sample efficiency and overall performance on high-dimensional preference tasks and LLM fine-tuning.

Conclusion: The hybrid framework effectively unifies the strengths of RLHF and PBO, offering a practical solution for efficient preference learning.

Abstract: Learning from human preferences is a cornerstone of aligning machine learning
models with subjective human judgments. Yet, collecting such preference data is
often costly and time-consuming, motivating the need for more efficient
learning paradigms. Two established approaches offer complementary advantages:
RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,
while PBO achieves greater sample efficiency through active querying. We
propose a hybrid framework that unifies RLHF's scalability with PBO's query
efficiency by integrating an acquisition-driven module into the RLHF pipeline,
thereby enabling active and sample-efficient preference gathering. We validate
the proposed approach on two representative domains: (i) high-dimensional
preference optimization and (ii) LLM fine-tuning. Experimental results
demonstrate consistent improvements in both sample efficiency and overall
performance across these tasks.

</details>


### [84] [Differentially Private In-Context Learning with Nearest Neighbor Search](https://arxiv.org/abs/2511.04332)
*Antti Koskela,Tejas Kulkarni,Laith Zumot*

Main category: cs.LG

TL;DR: A differentially private framework for in-context learning that incorporates nearest neighbor search with privacy filtering, achieving superior privacy-utility trade-offs compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing differentially private in-context learning approaches overlook the similarity search component in modern LLM pipelines, creating privacy risks in context data retrieval.

Method: Uses nearest neighbor retrieval from context data database combined with a privacy filter that tracks cumulative privacy cost of selected samples to maintain central differential privacy budget.

Result: Outperforms existing baselines by substantial margin across all evaluated benchmarks on text classification and document question answering tasks.

Conclusion: The proposed DP framework successfully integrates privacy-aware nearest neighbor search into in-context learning, demonstrating clear advantages over existing approaches.

Abstract: Differentially private in-context learning (DP-ICL) has recently become an
active research topic due to the inherent privacy risks of in-context learning.
However, existing approaches overlook a critical component of modern large
language model (LLM) pipelines: the similarity search used to retrieve relevant
context data. In this work, we introduce a DP framework for in-context learning
that integrates nearest neighbor search of relevant examples in a privacy-aware
manner. Our method outperforms existing baselines by a substantial margin
across all evaluated benchmarks, achieving more favorable privacy-utility
trade-offs. To achieve this, we employ nearest neighbor retrieval from a
database of context data, combined with a privacy filter that tracks the
cumulative privacy cost of selected samples to ensure adherence to a central
differential privacy budget. Experimental results on text classification and
document question answering show a clear advantage of the proposed method over
existing baselines.

</details>


### [85] [LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care](https://arxiv.org/abs/2511.04333)
*Federico Pirola,Fabio Stella,Marco Grzegorczyk*

Main category: cs.LG

TL;DR: Proposes a novel Gibbs sampling method for learning Dynamic Bayesian Networks from incomplete clinical data, treating missing values as unknown parameters for principled imputation and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Existing approaches handling missing data in longitudinal clinical datasets fail to account for temporal dependencies properly, limiting uncertainty quantification over time—critical for trustworthy models in settings like intensive care.

Method: Gibbs sampling-based approach treats missing values as unknown parameters following Gaussian distributions, sampling unobserved values from full conditional distributions at each iteration.

Result: Superior reconstruction accuracy and convergence properties compared to standard techniques like MICE on both simulated and real-world ICU datasets.

Conclusion: The full Bayesian framework provides more reliable imputations and deeper insight into model behavior, supporting safer clinical decision-making in data-scarce settings.

Abstract: Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to
their ability to model complex temporal relationships in patient data while
maintaining interpretability, an essential feature for clinical
decision-making. However, existing approaches to handling missing data in
longitudinal clinical datasets are largely derived from static Bayesian
networks literature, failing to properly account for the temporal nature of the
data. This gap limits the ability to quantify uncertainty over time, which is
particularly critical in settings such as intensive care, where understanding
the temporal dynamics is fundamental for model trustworthiness and
applicability across diverse patient groups. Despite the potential of DBNs, a
full Bayesian framework that integrates missing data handling remains
underdeveloped. In this work, we propose a novel Gibbs sampling-based method
for learning DBNs from incomplete data. Our method treats each missing value as
an unknown parameter following a Gaussian distribution. At each iteration, the
unobserved values are sampled from their full conditional distributions,
allowing for principled imputation and uncertainty estimation. We evaluate our
method on both simulated datasets and real-world intensive care data from
critically ill patients. Compared to standard model-agnostic techniques such as
MICE, our Bayesian approach demonstrates superior reconstruction accuracy and
convergence properties. These results highlight the clinical relevance of
incorporating full Bayesian inference in temporal models, providing more
reliable imputations and offering deeper insight into model behavior. Our
approach supports safer and more informed clinical decision-making,
particularly in settings where missing data are frequent and potentially
impactful.

</details>


### [86] [Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness](https://arxiv.org/abs/2511.04401)
*Subeen Park,Joowang Kim,Hakyung Lee,Sunjae Yoo,Kyungwoo Song*

Main category: cs.LG

TL;DR: SCER proposes embedding regularization to reduce reliance on spurious correlations, improving worst-group robustness in deep learning models.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often fail on underrepresented groups due to spurious correlations, and existing methods lack a theoretical link between embeddings and worst-group error.

Method: SCER regularizes feature representations by suppressing spurious directions identified from group-wise mean embedding differences, encouraging focus on core features.

Result: SCER achieves state-of-the-art worst-group accuracy across multiple vision and language benchmarks.

Conclusion: Theoretical embedding-level constraints effectively enhance model robustness by reducing sensitivity to spurious patterns.

Abstract: Deep learning models achieve strong performance across various domains but
often rely on spurious correlations, making them vulnerable to distribution
shifts. This issue is particularly severe in subpopulation shift scenarios,
where models struggle in underrepresented groups. While existing methods have
made progress in mitigating this issue, their performance gains are still
constrained. They lack a rigorous theoretical framework connecting the
embedding space representations with worst-group error. To address this
limitation, we propose Spurious Correlation-Aware Embedding Regularization for
Worst-Group Robustness (SCER), a novel approach that directly regularizes
feature representations to suppress spurious cues. We show theoretically that
worst-group error is influenced by how strongly the classifier relies on
spurious versus core directions, identified from differences in group-wise mean
embeddings across domains and classes. By imposing theoretical constraints at
the embedding level, SCER encourages models to focus on core features while
reducing sensitivity to spurious patterns. Through systematic evaluation on
multiple vision and language, we show that SCER outperforms prior
state-of-the-art studies in worst-group accuracy. Our code is available at
\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.

</details>


### [87] [On the Equivalence of Regression and Classification](https://arxiv.org/abs/2511.04422)
*Jayadeva,Naman Dwivedi,Hari Krishnan,N. M. Anoop Krishnan*

Main category: cs.LG

TL;DR: 本文建立了回归与分类之间的形式化联系，证明了具有M个数据的回归问题与具有2M个数据的线性可分分类任务存在一一对应关系，并提出了一种回归难度度量方法和线性化映射训练方法。


<details>
  <summary>Details</summary>
Motivation: 回归与分类之间的形式化联系一直较为薄弱，尽管支持向量回归中使用了边缘最大化项，但最多只能被证明是正则化器。论文旨在建立回归与分类之间的严格等价关系。

Method: 通过证明在超平面上的M个样本的回归问题与具有2M个样本的线性可分分类任务存在一一对应等价关系，利用该等价关系推导出新的回归公式。

Result: 提出了基于等价关系的回归问题重构方法，开发了"可回归性"度量来评估数据集回归难度，并训练神经网络学习线性化映射。

Conclusion: 建立了回归与分类的严格等价关系，为回归问题提供了新的理解和解决框架，能够在不先学习模型的情况下评估回归难度，并通过线性化映射改进回归性能。

Abstract: A formal link between regression and classification has been tenuous. Even
though the margin maximization term $\|w\|$ is used in support vector
regression, it has at best been justified as a regularizer. We show that a
regression problem with $M$ samples lying on a hyperplane has a one-to-one
equivalence with a linearly separable classification task with $2M$ samples. We
show that margin maximization on the equivalent classification task leads to a
different regression formulation than traditionally used. Using the
equivalence, we demonstrate a ``regressability'' measure, that can be used to
estimate the difficulty of regressing a dataset, without needing to first learn
a model for it. We use the equivalence to train neural networks to learn a
linearizing map, that transforms input variables into a space where a linear
regressor is adequate.

</details>


### [88] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: Current LLM uncertainty quantification methods fail on ambiguous data, prompting creation of MAQA* and AmbigQA* datasets and revealing theoretical limitations.


<details>
  <summary>Details</summary>
Motivation: Real-world language ambiguity requires accurate uncertainty quantification in LLMs, but existing UQ benchmarks lack ambiguous scenarios.

Method: Introduced MAQA* and AmbigQA* ambiguous QA datasets with ground-truth distributions from factual co-occurrence; evaluated various UQ approaches including predictive distribution, internal representations, and model ensembles.

Result: Current uncertainty estimators degrade to near-random performance on ambiguous data, with deterioration consistent across all evaluation paradigms.

Conclusion: Study exposes fundamental limitation of current UQ methods under ambiguity, necessitating rethinking of modeling approaches for trustworthy LLM deployment.

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [89] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: SynthKGQA is a framework for generating synthetic KGQA datasets from any knowledge graph, enabling better benchmarking and training of retrieval models.


<details>
  <summary>Details</summary>
Motivation: Existing KG retrieval methods lack challenging QA datasets with ground-truth targets, making comparisons difficult.

Method: Proposing SynthKGQA to automatically generate high-quality synthetic datasets containing full ground-truth facts for reasoning.

Result: Created GTSQA from Wikidata to test zero-shot generalization of KG retrievers on unseen structures and relations.

Conclusion: SynthKGQA improves KG retriever benchmarking and model training, demonstrated through GTSQA's utility.

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [90] [Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training](https://arxiv.org/abs/2511.04485)
*Ipsita Ghosh,Ethan Nguyen,Christian Kümmerle*

Main category: cs.LG

TL;DR: Q3R is a new low-rank training method using quadratic regularization that maintains prescribed low ranks while achieving performance close to dense models with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing parameter-efficient training methods based on low-rank optimization fail at low-rank pre-training tasks where maintaining both low-rank structure and model performance is challenging.

Method: Proposes Quadratic Reweighted Rank Regularizer (Q3R), inspired by iteratively reweighted least squares framework, using quadratic regularizer that majorizes smoothed log determinant as rank surrogate.

Result: Achieves comparable predictive performance to dense models with small computational overhead; demonstrated 60% and 80% parameter reduction in ViT-Tiny with only 1.3% and 4% accuracy drop on CIFAR-10.

Conclusion: Q3R effectively enables low-rank training with prescribed target ranks, works across transformers for both image and language tasks including fine-tuning, and maintains full compatibility with existing architectures.

Abstract: Parameter-efficient training, based on low-rank optimization, has become a
highly successful tool for fine-tuning large deep-learning models. However,
these methods fail at low-rank pre-training tasks where maintaining the
low-rank structure and the objective remains a challenging task. We propose the
Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel
low-rank inducing training strategy inspired by the iteratively reweighted
least squares (IRLS) framework. Q3R is based on a quadratic regularizer term
which majorizes a smoothed log determinant serving as rank surrogate objective.
Unlike other low-rank training techniques, Q3R is able to train weight matrices
with prescribed, low target ranks of models that achieve comparable predictive
performance as dense models, with small computational overhead, while remaining
fully compatible with existing architectures. For example, we demonstrated one
experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of
a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance
respectively. The efficacy of Q3R is confirmed on Transformers across both
image and language tasks, including for low-rank fine-tuning.

</details>


### [91] [ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting](https://arxiv.org/abs/2511.04445)
*Syeda Sitara Wishal Fatima,Afshin Rahimi*

Main category: cs.LG

TL;DR: ForecastGAN: A decomposition-based adversarial framework using 3 modules (decomposition, model selection, adversarial training) that outperforms transformers in short-term forecasting while integrating numerical/categorical features.


<details>
  <summary>Details</summary>
Motivation: Existing transformer models underperform in short-term forecasting scenarios and typically ignore categorical features, creating limitations for multi-horizon predictions.

Method: Three integrated modules: Decomposition Module extracts seasonality/trend; Model Selection Module chooses optimal neural networks by horizon; Adversarial Training Module enhances robustness via Conditional GAN.

Result: Outperforms state-of-the-art transformers on 11 benchmark datasets for short-term forecasting while remaining competitive for long-term horizons.

Conclusion: Establishes a generalizable approach adaptable to specific contexts with strong performance across diverse data characteristics without extensive hyperparameter tuning.

Abstract: Time series forecasting is essential across domains from finance to supply
chain management. This paper introduces ForecastGAN, a novel decomposition
based adversarial framework addressing limitations in existing approaches for
multi-horizon predictions. Although transformer models excel in long-term
forecasting, they often underperform in short-term scenarios and typically
ignore categorical features. ForecastGAN operates through three integrated
modules: a Decomposition Module that extracts seasonality and trend components;
a Model Selection Module that identifies optimal neural network configurations
based on forecasting horizon; and an Adversarial Training Module that enhances
prediction robustness through Conditional Generative Adversarial Network
training. Unlike conventional approaches, ForecastGAN effectively integrates
both numerical and categorical features. We validate our framework on eleven
benchmark multivariate time series datasets that span various forecasting
horizons. The results show that ForecastGAN consistently outperforms
state-of-the-art transformer models for short-term forecasting while remaining
competitive for long-term horizons. This research establishes a more
generalizable approach to time series forecasting that adapts to specific
contexts while maintaining strong performance across diverse data
characteristics without extensive hyperparameter tuning.

</details>


### [92] [Alternative Fairness and Accuracy Optimization in Criminal Justice](https://arxiv.org/abs/2511.04505)
*Shaolong Wu,James Blume,Geshi Yeung*

Main category: cs.LG

TL;DR: The paper proposes a modified group fairness approach for criminal justice algorithms that replaces exact parity with weighted error minimization while controlling false negative rate differences. It addresses key fairness critiques and provides a practical deployment framework based on need-based decisions, transparency, and tailored solutions.


<details>
  <summary>Details</summary>
Motivation: Algorithmic fairness concepts remain unsettled in criminal justice applications, and standard group fairness approaches face practical limitations and ethical trade-offs that need addressing.

Method: The authors review different fairness concepts and develop a modified group fairness approach that minimizes weighted error loss while keeping false negative rate differences within tolerance. They situate this within three classes of critique and propose a three-pillar deployment framework.

Result: The proposed approach makes solutions easier to find, can improve predictive accuracy, and makes error cost choices explicit. It provides a way to navigate fairness conflicts in practical applications.

Conclusion: The framework links technical design to legitimacy and offers actionable guidance for agencies using risk assessment tools, addressing both conceptual fairness issues and practical deployment challenges in criminal justice systems.

Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts
remain unsettled, especially in criminal justice. We review group, individual,
and process fairness and map the conditions under which they conflict. We then
develop a simple modification to standard group fairness. Rather than exact
parity across protected groups, we minimize a weighted error loss while keeping
differences in false negative rates within a small tolerance. This makes
solutions easier to find, can raise predictive accuracy, and surfaces the
ethical choice of error costs. We situate this proposal within three classes of
critique: biased and incomplete data, latent affirmative action, and the
explosion of subgroup constraints. Finally, we offer a practical framework for
deployment in public decision systems built on three pillars: need-based
decisions, Transparency and accountability, and narrowly tailored definitions
and solutions. Together, these elements link technical design to legitimacy and
provide actionable guidance for agencies that use risk assessment and related
tools.

</details>


### [93] [Federated Stochastic Minimax Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2511.04456)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: First federated minimax optimization algorithms with theoretical guarantees for heavy-tailed noise, achieving O(1/(TNp)^((s-1)/(2s))) convergence.


<details>
  <summary>Details</summary>
Motivation: Heavy-tailed noise is a more realistic assumption than standard bounded variance in stochastic optimization, especially in federated learning settings where gradient noise distribution is important.

Method: Two algorithms are proposed: Fed-NSGDA-M using normalized gradients and FedMuon-DA using Muon optimizer for local updates, both designed to handle heavy-tailed noise under milder conditions.

Result: Both algorithms achieve a convergence rate of O(1/(TNp)^((s-1)/(2s))) and extensive experiments validate their effectiveness.

Conclusion: The paper successfully addresses the problem of heavy-tailed gradient noise in nonconvex-PL federated minimax optimization by proposing two novel algorithms with theoretical guarantees.

Abstract: Heavy-tailed noise has attracted growing attention in nonconvex stochastic
optimization, as numerous empirical studies suggest it offers a more realistic
assumption than standard bounded variance assumption. In this work, we
investigate nonconvex-PL minimax optimization under heavy-tailed gradient noise
in federated learning. We propose two novel algorithms: Fed-NSGDA-M, which
integrates normalized gradients, and FedMuon-DA, which leverages the Muon
optimizer for local updates. Both algorithms are designed to effectively
address heavy-tailed noise in federated minimax optimization, under a milder
condition. We theoretically establish that both algorithms achieve a
convergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of our
knowledge, these are the first federated minimax optimization algorithms with
rigorous theoretical guarantees under heavy-tailed noise. Extensive experiments
further validate their effectiveness.

</details>


### [94] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: Introduces RGP, a temporal relational graph transformer that captures long-range dependencies and supports multi-task learning across diverse domains.


<details>
  <summary>Details</summary>
Motivation: Existing graph models for relational data focus mainly on spatial structure, treat temporal information as a filtering constraint, and are typically single-task, limiting their utility in dynamic domains like healthcare and e-commerce.

Method: Proposes a temporal subgraph sampler to capture temporally relevant relationships beyond immediate neighborhoods and introduces the Relational Graph Perceiver (RGP), which uses a cross-attention latent bottleneck to integrate structural and temporal contexts and a flexible decoder for joint multi-task learning.

Result: Experiments on RelBench, SALT, and CTU benchmarks show that RGP achieves state-of-the-art performance.

Conclusion: RGP provides a general, scalable solution for relational deep learning that effectively handles temporal dynamics and supports diverse predictive tasks.

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [95] [Towards Causal Market Simulators](https://arxiv.org/abs/2511.04469)
*Dennis Thumm,Luis Ontaneda Mijares*

Main category: cs.LG

TL;DR: Proposes TNCM-VAE combining VAEs with structural causal models to generate counterfactual financial time series preserving temporal and causal dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing market generators using deep generative models lack causal reasoning capabilities essential for counterfactual analysis and risk assessment.

Method: Combines variational autoencoders with structural causal models, enforces causal constraints via DAGs in decoder, and uses causal Wasserstein distance for training.

Result: Validated on synthetic datasets, achieves superior counterfactual probability estimation with L1 distances of 0.03-0.10 compared to ground truth.

Conclusion: Enables financial stress testing, scenario analysis, and enhanced backtesting through causal-aware counterfactual market trajectory generation.

Abstract: Market generators using deep generative models have shown promise for
synthetic financial data generation, but existing approaches lack causal
reasoning capabilities essential for counterfactual analysis and risk
assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that
combines variational autoencoders with structural causal models to generate
counterfactual financial time series while preserving both temporal
dependencies and causal relationships. Our approach enforces causal constraints
through directed acyclic graphs in the decoder architecture and employs the
causal Wasserstein distance for training. We validate our method on synthetic
autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating
superior performance in counterfactual probability estimation with L1 distances
as low as 0.03-0.10 compared to ground truth. The model enables financial
stress testing, scenario analysis, and enhanced backtesting by generating
plausible counterfactual market trajectories that respect underlying causal
mechanisms.

</details>


### [96] [Addressing divergent representations from causal interventions on neural networks](https://arxiv.org/abs/2511.04638)
*Satchel Grant,Simon Jerome Han,Alexa Tartaglini,Christopher Potts*

Main category: cs.LG

TL;DR: Mechanistic interpretability interventions often create out-of-distribution representations, which can lead to unfaithful explanations. The paper identifies harmless vs. pernicious divergences and proposes a regularization method to mitigate harmful effects.


<details>
  <summary>Details</summary>
Motivation: To determine if causal interventions in mechanistic interpretability create divergent representations that undermine the faithfulness of explanations to the model's natural state.

Method: Empirical demonstration of distribution shifts from interventions, theoretical analysis of divergence types (harmless vs. pernicious), and modification of Counterfactual Latent loss for regularization.

Result: Common interventions cause distribution shifts; harmless divergences occur in weight null-spaces or within behavioral bounds, while pernicious ones activate hidden pathways. The modified CL loss reduces harmful divergences.

Conclusion: The study provides insights into ensuring reliable interpretability by addressing divergence issues, offering a path towards more faithful explanatory methods.

Abstract: A common approach to mechanistic interpretability is to causally manipulate
model representations via targeted interventions in order to understand what
those representations encode. Here we ask whether such interventions create
out-of-distribution (divergent) representations, and whether this raises
concerns about how faithful their resulting explanations are to the target
model in its natural state. First, we demonstrate empirically that common
causal intervention techniques often do shift internal representations away
from the natural distribution of the target model. Then, we provide a
theoretical analysis of two classes of such divergences: `harmless' divergences
that occur in the null-space of the weights and from covariance within
behavioral decision boundaries, and `pernicious' divergences that activate
hidden network pathways and cause dormant behavioral changes. Finally, in an
effort to mitigate the pernicious cases, we modify the Counterfactual Latent
(CL) loss from Grant (2025) that regularizes interventions to remain closer to
the natural distributions, reducing the likelihood of harmful divergences while
preserving the interpretive power of interventions. Together, these results
highlight a path towards more reliable interpretability methods.

</details>


### [97] [Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks](https://arxiv.org/abs/2511.04494)
*Alper Kalle,Theo Rudkiewicz,Mohamed-Oumar Ouerfelli,Mohamed Tamaazousti*

Main category: cs.LG

TL;DR: Data-informed neural network compression using tensor decompositions that minimizes function-space error instead of weight-space error, achieving competitive accuracy without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Neural networks require significant computing power, and compression methods typically use isotropic norms in weight-space that don't account for actual data distribution, leading to accuracy loss that requires fine-tuning.

Method: Propose tensor decomposition algorithms (Tucker-2 and CPD) that minimize the change in layer's output distribution using data-informed norms: ∥(W - W̃)Σ¹ᐟ²∥F, where Σ¹ᐟ² is the square root of input covariance matrix.

Result: Achieves competitive accuracy without post-compression fine-tuning, and the covariance-based norm can transfer between datasets with minor accuracy drop. Validated on ResNet-18/50, GoogLeNet with ImageNet, FGVC-Aircraft, Cifar10, and Cifar100.

Conclusion: Data-informed compression using function-space error minimization provides effective neural network compression without requiring fine-tuning, and the approach is transferable across datasets when original training data is unavailable.

Abstract: Neural networks are widely used for image-related tasks but typically demand
considerable computing power. Once a network has been trained, however, its
memory- and compute-footprint can be reduced by compression. In this work, we
focus on compression through tensorization and low-rank representations.
Whereas classical approaches search for a low-rank approximation by minimizing
an isotropic norm such as the Frobenius norm in weight-space, we use
data-informed norms that measure the error in function space. Concretely, we
minimize the change in the layer's output distribution, which can be expressed
as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is
the square root of the covariance matrix of the layer's input and $W$,
$\widetilde{W}$ are the original and compressed weights. We propose new
alternating least square algorithms for the two most common tensor
decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike
conventional compression pipelines, which almost always require
post-compression fine-tuning, our data-informed approach often achieves
competitive accuracy without any fine-tuning. We further show that the same
covariance-based norm can be transferred from one dataset to another with only
a minor accuracy drop, enabling compression even when the original training
dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50,
and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100)
confirm the advantages of the proposed method.

</details>


### [98] [Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers](https://arxiv.org/abs/2511.04514)
*C. Hepburn,T. Zielke,A. P. Raulf*

Main category: cs.LG

TL;DR: Study shows linear mode connectivity (LMC) in neural networks is affected by data shifts; small learning rates and large batch sizes mitigate this impact, influencing model convergence and ensemble diversity.


<details>
  <summary>Details</summary>
Motivation: To understand how data shifts affect LMC, linking it to training stability, minima smoothness, and model diversity in deep learning.

Method: Experimental analysis of LMC under data shifts, treating shifts as gradient noise and testing parameters like learning rate and batch size.

Result: LMC helps models converge to similar minima, reducing error diversity but improving training efficiency versus larger ensembles.

Conclusion: LMC balances efficiency and diversity under data shifts, with optimal hyperparameters crucial for robust convergence.

Abstract: The phenomenon of linear mode connectivity (LMC) links several aspects of
deep learning, including training stability under noisy stochastic gradients,
the smoothness and generalization of local minima (basins), the similarity and
functional diversity of sampled models, and architectural effects on data
processing. In this work, we experimentally study LMC under data shifts and
identify conditions that mitigate their impact. We interpret data shifts as an
additional source of stochastic gradient noise, which can be reduced through
small learning rates and large batch sizes. These parameters influence whether
models converge to the same local minimum or to regions of the loss landscape
with varying smoothness and generalization. Although models sampled via LMC
tend to make similar errors more frequently than those converging to different
basins, the benefit of LMC lies in balancing training efficiency against the
gains achieved from larger, more diverse ensembles. Code and supplementary
materials will be made publicly available at https://github.com/DLR-KI/LMC in
due course.

</details>


### [99] [Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity](https://arxiv.org/abs/2511.04518)
*Obed Amo,Samit Ghosh,Markus Lange-Hegermann,Bogdan Raiţă,Michael Pokojovy*

Main category: cs.LG

TL;DR: B-EPGP surrogate model achieves ~100x better accuracy than CN-FEM for 2D wave equation under matched computational cost.


<details>
  <summary>Details</summary>
Motivation: To fairly compare a novel boundary-constrained Gaussian Process method (B-EPGP) against classical finite element method (CN-FEM) for solving PDEs.

Method: Uses exponential-polynomial bases from characteristic variety to exactly satisfy PDE/boundary conditions, with penalized least squares coefficient estimation. Implements DoF-matching protocol for fair comparison.

Result: B-EPGP consistently shows lower L² errors (both space-time and maximum-in-time) by approximately two orders of magnitude under matched DoF.

Conclusion: B-EPGP offers superior accuracy over traditional CN-FEM for wave equation problems with boundary constraints.

Abstract: We present a new benchmarking study comparing a boundary-constrained
Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical
finite element method combined with Crank--Nicolson time stepping (CN-FEM) for
solving the two-dimensional wave equation with homogeneous Dirichlet boundary
conditions. The B-EPGP construction leverages exponential-polynomial bases
derived from the characteristic variety to enforce the PDE and boundary
conditions exactly and employs penalized least squares to estimate the
coefficients. To ensure fairness across paradigms, we introduce a
degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP
consistently attains lower space-time $L^2$-error and maximum-in-time
$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of
magnitude.

</details>


### [100] [End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit](https://arxiv.org/abs/2511.04522)
*Daniel Mayfrank,Kayra Dernek,Laura Lang,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: RL-trained Koopman models scale well to large industrial air separation units, achieving constraint-compliant economic performance with limited observability.


<details>
  <summary>Details</summary>
Motivation: To extend the previously proposed method from small-scale case studies to more demanding industrial applications, particularly for demand response scenarios involving large-scale air separation units with limited observable variables.

Method: Reinforcement learning-based training of Koopman surrogate models for optimal performance in specific economic nonlinear model predictive control applications.

Result: The method delivered similar economic performance to purely system identification-based Koopman eNMPC but successfully avoided constraint violations that frequently occurred with the baseline approach.

Conclusion: The method successfully scales to a more challenging industrial case study and provides significant improvements in constraint satisfaction compared to purely system identification-based approaches, while maintaining similar economic performance.

Abstract: With our recently proposed method based on reinforcement learning (Mayfrank
et al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trained
for optimal performance in specific (economic) nonlinear model predictive
control ((e)NMPC) applications. So far, our method has exclusively been
demonstrated on a small-scale case study. Herein, we show that our method
scales well to a more challenging demand response case study built on a
large-scale model of a single-product (nitrogen) air separation unit. Across
all numerical experiments, we assume observability of only a few realistically
measurable plant variables. Compared to a purely system identification-based
Koopman eNMPC, which generates small economic savings but frequently violates
constraints, our method delivers similar economic performance while avoiding
constraint violations.

</details>


### [101] [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](https://arxiv.org/abs/2511.04534)
*Jonas E. Katona,Emily K. de Jong,Nipun Gunawardena*

Main category: cs.LG

TL;DR: A model-agnostic framework for uncertainty quantification in reduced-order models using conformal prediction to generate prediction intervals for latent dynamics, reconstruction, and end-to-end predictions.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification methods for reduced-order models are often architecture- or training-specific, limiting flexibility and generalization.

Method: Post hoc framework using conformal prediction that requires no modification to the underlying ROM architecture or training procedure.

Result: Accurately predicts evolution of droplet-size distributions and quantifies uncertainty across the ROM pipeline in a cloud microphysics application.

Conclusion: The proposed framework provides robust, model-agnostic uncertainty quantification for latent space ROMs without requiring architectural changes.

Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional
physical systems, but lack robust uncertainty quantification methods. Existing
approaches are frequently architecture- or training-specific, which limits
flexibility and generalization. We introduce a post hoc, model-agnostic
framework for predictive uncertainty quantification in latent space ROMs that
requires no modification to the underlying architecture or training procedure.
Using conformal prediction, our approach estimates statistical prediction
intervals for multiple components of the ROM pipeline: latent dynamics,
reconstruction, and end-to-end predictions. We demonstrate the method on a
latent space dynamical model for cloud microphysics, where it accurately
predicts the evolution of droplet-size distributions and quantifies uncertainty
across the ROM pipeline.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [102] [OptiMA: A Transaction-Based Framework with Throughput Optimization for Very Complex Multi-Agent Systems](https://arxiv.org/abs/2511.03761)
*Umut Çalıkyılmaz,Nitin Nayak,Jinghua Groppe,Sven Groppe*

Main category: cs.MA

TL;DR: Proposed OptiMA, a transaction-based framework with scheduling for very complex multi-agent systems, handling 100+ agents with 16% performance improvement and providing theoretical analysis tools.


<details>
  <summary>Details</summary>
Motivation: As multi-agent systems become larger and more complex to handle sophisticated tasks, they face two critical pitfalls: increased susceptibilities to faults and performance bottlenecks that need to be addressed.

Method: Proposed and implemented a transaction-based framework (OptiMA) that includes transaction scheduling for designing very complex multi-agent systems (VCMAS). The framework was developed to handle systems with over 100 agents.

Result: The OptiMA framework demonstrated successful execution of VCMAS with over 100 agents and showed transaction scheduling improvements of up to 16%. A theoretical analysis of transaction scheduling and practical tools for future research were also provided.

Conclusion: The OptiMA framework successfully addresses the two main pitfalls of complex multi-agent systems (susceptibilities to faults and performance bottlenecks) through transaction-based design and scheduling, enabling robust VCMAS with over 100 agents and providing tools for future research.

Abstract: In recent years, the research of multi-agent systems has taken a direction to
explore larger and more complex models to fulfill sophisticated tasks. We point
out two possible pitfalls that might be caused by increasing complexity;
susceptibilities to faults, and performance bottlenecks. To prevent the former
threat, we propose a transaction-based framework to design very complex
multi-agent systems (VCMAS). To address the second threat, we offer to
integrate transaction scheduling into the proposed framework. We implemented
both of these ideas to develop the OptiMA framework and show that it is able to
facilitate the execution of VCMAS with more than a hundred agents. We also
demonstrate the effect of transaction scheduling on such a system by showing
improvements up to more than 16\%. Furthermore, we also performed a theoretical
analysis on the transaction scheduling problem and provided practical tools
that can be used for future research on it.

</details>


### [103] [ASAP: an Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training](https://arxiv.org/abs/2511.03844)
*Yuran Ding,Xinwei Chen,Xiaofan Zhang,Zongwei Zhou*

Main category: cs.MA

TL;DR: ASAP is a multi-agent system that automates performance optimization for distributed LLM training, achieving up to 28% speedup and 2.58x throughput improvement.


<details>
  <summary>Details</summary>
Motivation: Existing manual or black-box optimization methods are too slow and inefficient for rapidly evolving LLM training needs, leading to resource underutilization.

Method: Uses Coordinator, Analyzer, and Proposal agents with LLM reasoning, performance profiling, roofline analysis, and knowledge base integration to diagnose bottlenecks and recommend optimized sharding configurations.

Result: Achieves up to 28% training step time reduction, 1.43x throughput improvement alone, and 2.58x throughput when combined with human expert optimizations.

Conclusion: ASAP provides a scalable, explainable AI-assisted methodology for performance engineering in large-scale LLM training.

Abstract: Optimizing large-language model (LLM) training on distributed domain-specific
accelerator systems presents significant challenges due to its complex
optimization space. Existing optimization methods, however, rely on
time-consuming manual tuning or resource-intensive black-box searches, which
struggle to keep pace with the rapidly evolving LLM domain, leading to slow
development and underutilized resources. To address this, we introduce ASAP, an
Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training. It
is a multi-agent system, featuring Coordinator, Analyzer, and Proposal agents,
which integrates LLM reasoning with insights from performance profiling tools,
roofline analysis, and a knowledge base of best practices and successful past
optimizations from human experts. Our proposed design can automate the
diagnosis of performance bottlenecks and recommend optimized sharding
configurations with reasoning, thus effectively improving the efficiency of
distributed LLM training. Experiments have shown that the ASAP-generated
sharding configurations can contribute up to 28% training step time reduction
and 1.43 times throughput improvement. When combined with additional
optimization from human experts, throughput can be further increased to 2.58
times. The proposed ASAP promises to provide a scalable and explainable
methodology for AI-assisted performance engineering in large-scale LLM
training.

</details>


### [104] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: A collaborative multi-agent framework improves math question generation by balancing complexity and cognitive demand through iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Automatic question generation for math education struggles with controlling problem complexity and cognitive demands using existing transformer models.

Method: Uses a multi-agent framework where agents collaboratively refine question-answer pairs during inference to better balance complexity and cognitive demands.

Result: Preliminary evaluations show improved question quality across five criteria: relevance, importance, clarity, difficulty matching, and answerability.

Conclusion: Collaborative multi-agent workflows can produce more controlled, pedagogically valuable content for automated educational content generation.

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>
