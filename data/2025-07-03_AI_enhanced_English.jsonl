{"id": "2507.01059", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01059", "abs": "https://arxiv.org/abs/2507.01059", "authors": ["Xiangbo Gao", "Keshu Wu", "Hao Zhang", "Kexin Tian", "Yang Zhou", "Zhengzhong Tu"], "title": "Automated Vehicles Should be Connected with Natural Language", "comment": null, "summary": "Multi-agent collaborative driving promises improvements in traffic safety and\nefficiency through collective perception and decision making. However, existing\ncommunication media -- including raw sensor data, neural network features, and\nperception results -- suffer limitations in bandwidth efficiency, information\ncompleteness, and agent interoperability. Moreover, traditional approaches have\nlargely ignored decision-level fusion, neglecting critical dimensions of\ncollaborative driving. In this paper we argue that addressing these challenges\nrequires a transition from purely perception-oriented data exchanges to\nexplicit intent and reasoning communication using natural language. Natural\nlanguage balances semantic density and communication bandwidth, adapts flexibly\nto real-time conditions, and bridges heterogeneous agent platforms. By enabling\nthe direct communication of intentions, rationales, and decisions, it\ntransforms collaborative driving from reactive perception-data sharing into\nproactive coordination, advancing safety, efficiency, and transparency in\nintelligent transportation systems.", "AI": {"tldr": "The paper proposes using natural language for intent and reasoning communication in multi-agent collaborative driving to overcome bandwidth, interoperability, and decision-level fusion limitations.", "motivation": "Existing communication methods (sensor data, neural features, perception results) lack bandwidth efficiency, completeness, and interoperability, and ignore decision-level fusion.", "method": "Transition from perception-oriented data exchanges to natural language for communicating intent and reasoning.", "result": "Natural language improves semantic density, bandwidth flexibility, and interoperability, enabling proactive coordination.", "conclusion": "Natural language enhances safety, efficiency, and transparency in collaborative driving by enabling direct communication of intentions and decisions."}}
{"id": "2507.01378", "categories": ["cs.MA", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01378", "abs": "https://arxiv.org/abs/2507.01378", "authors": ["Ziyao Wang", "Rongpeng Li", "Sizhao Li", "Yuming Xiang", "Haiping Wang", "Zhifeng Zhao", "Honggang Zhang"], "title": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms", "comment": null, "summary": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.", "AI": {"tldr": "The paper proposes RALLY, a Role-Adaptive LLM-Driven Yoked navigation algorithm, to enhance UAV swarm control by combining LLM semantic reasoning with MARL adaptability, improving task coverage and generalization.", "motivation": "Traditional MARL lacks semantic communication and role flexibility, while LLM-based frameworks struggle with exploration and adaptability. RALLY aims to bridge these gaps.", "method": "RALLY integrates an LLM-driven semantic decision framework, dynamic role-heterogeneity, and a Role-value Mixing Network (RMIX) to merge offline priors with online MARL policies.", "result": "Experiments show RALLY outperforms conventional methods in task coverage, convergence speed, and generalization.", "conclusion": "RALLY demonstrates strong potential for collaborative navigation in multi-UAV systems by combining semantic reasoning and adaptive learning."}}
{"id": "2507.01701", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01701", "abs": "https://arxiv.org/abs/2507.01701", "authors": ["Bochen Han", "Songmao Zhang"], "title": "Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture", "comment": null, "summary": "In this paper, we propose to incorporate the blackboard architecture into LLM\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\nthe information and others' messages during the whole problem-solving process,\n(2) agents that will take actions are selected based on the current content of\nthe blackboard, and (3) the selection and execution round is repeated until a\nconsensus is reached on the blackboard. We develop the first implementation of\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\nmathematical datasets. The results show that our system can be competitive with\nthe SOTA static and dynamic MASs by achieving the best average performance, and\nat the same time manage to spend less tokens. Our proposal has the potential to\nenable complex and dynamic problem-solving where well-defined structures or\nworkflows are unavailable.", "AI": {"tldr": "Incorporating blackboard architecture into LLM multi-agent systems improves information sharing, dynamic agent selection, and consensus-building, achieving competitive performance with fewer tokens.", "motivation": "To enhance multi-agent systems by enabling dynamic information sharing and agent selection, especially in scenarios lacking predefined workflows.", "method": "Proposes a blackboard architecture for LLM multi-agent systems, where agents share information via a blackboard, and actions are selected based on blackboard content iteratively until consensus.", "result": "Competitive with SOTA static and dynamic MASs, achieving best average performance with fewer tokens.", "conclusion": "The blackboard architecture enables complex, dynamic problem-solving in unstructured scenarios."}}
{"id": "2507.01769", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01769", "abs": "https://arxiv.org/abs/2507.01769", "authors": ["Yuta Takahashi", "Shin-ichiro Sakai"], "title": "Distance-based Relative Orbital Transition for Satellite Swarm Array Deployment Under J2 Perturbation", "comment": null, "summary": "This paper presents an autonomous guidance and control strategy for a\nsatellite swarm that enables scalable distributed space structures for\ninnovative science and business opportunities. The averaged $J_2$ orbital\nparameters that describe the drift and periodic orbital motion were derived\nalong with their target values to achieve a distributed space structure in a\ndecentralized manner. This enabled the design of a distance-based orbital\nstabilizer to ensure autonomous deployment into a monolithic formation of a\ncoplanar equidistant configuration on a user-defined orbital plane. Continuous\nformation control was assumed to be achieved through fuel-free actuation, such\nas satellite magnetic field interaction and differential aerodynamic forces,\nthereby maintaining long-term formation stability without thruster usage. A\nmajor challenge for such actuation systems is the potential loss of control\ncapability due to increasing inter-satellite distances resulting from unstable\norbital dynamics, particularly for autonomous satellite swarms. To mitigate\nthis risk, our decentralized deployment controller minimized drift distance\nduring unexpected communication outages. As a case study, we consider the\ndeployment of palm-sized satellites into a coplanar equidistant formation in a\n$J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented.", "AI": {"tldr": "Autonomous guidance for satellite swarms enables scalable distributed space structures using decentralized control and fuel-free actuation.", "motivation": "To enable innovative science and business opportunities by developing scalable, decentralized satellite swarm formations.", "method": "Derived averaged $J_2$ orbital parameters, designed a distance-based orbital stabilizer, and used fuel-free actuation (magnetic field interaction, differential aerodynamic forces).", "result": "Achieved autonomous deployment into coplanar equidistant formations and minimized drift during communication outages.", "conclusion": "Decentralized control and fuel-free actuation can maintain stable satellite swarm formations, addressing challenges like drift and communication loss."}}
{"id": "2507.01231", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["I\u00f1aki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here.", "AI": {"tldr": "The paper clarifies debates about LRMs' reasoning capabilities by refining benchmarks (Towers of Hanoi, River Crossing), showing failures stem from complexity and unsolvable setups, not just output constraints.", "motivation": "To address controversy over whether LRMs lack genuine reasoning, sparked by Apple's 'The Illusion of Thinking' and subsequent debates.", "method": "Replicated and refined contentious benchmarks, introducing incremental stepwise prompting and agentic collaborative dialogue.", "result": "LRMs struggle with moderate complexity (e.g., 8-disk Towers of Hanoi) but solve large solvable problems (e.g., 100+ agent pairs in River Crossing).", "conclusion": "LRMs are stochastic searchers in poorly understood state spaces; progress requires fine-grained analysis of their reasoning limits."}}
{"id": "2507.01026", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01026", "abs": "https://arxiv.org/abs/2507.01026", "authors": ["Md Shakil Ahamed Shohag", "Q. M. Jonathan Wu", "Farhad Pourpanah"], "title": "Few-Shot Inspired Generative Zero-Shot Learning", "comment": null, "summary": "Generative zero-shot learning (ZSL) methods typically synthesize visual\nfeatures for unseen classes using predefined semantic attributes, followed by\ntraining a fully supervised classification model. While effective, these\nmethods require substantial computational resources and extensive synthetic\ndata, thereby relaxing the original ZSL assumptions. In this paper, we propose\nFSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on\nlarge-scale feature synthesis. Our key insight is that class-level attributes\nexhibit instance-level variability, i.e., some attributes may be absent or\npartially visible, yet conventional ZSL methods treat them as uniformly\npresent. To address this, we introduce Model-Specific Attribute Scoring (MSAS),\nwhich dynamically re-scores class attributes based on model-specific\noptimization to approximate instance-level variability without access to unseen\ndata. We further estimate group-level prototypes as clusters of instances based\non MSAS-adjusted attribute scores, which serve as representative synthetic\nfeatures for each unseen class. To mitigate the resulting data imbalance, we\nintroduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training\na semantic-aware contrastive classifier (SCC) using these prototypes.\nExperiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves\ncompetitive performance using far fewer synthetic features.", "AI": {"tldr": "FSIGenZ is a few-shot-inspired generative ZSL framework that reduces reliance on large-scale feature synthesis by dynamically re-scoring class attributes and using group-level prototypes.", "motivation": "Traditional generative ZSL methods require extensive synthetic data and computational resources, relaxing ZSL assumptions. FSIGenZ addresses this by leveraging instance-level attribute variability.", "method": "Introduces Model-Specific Attribute Scoring (MSAS) to dynamically adjust class attributes and group-level prototypes. Uses Dual-Purpose Semantic Regularization (DPSR) and a semantic-aware contrastive classifier (SCC).", "result": "Achieves competitive performance on SUN, AwA2, and CUB benchmarks with fewer synthetic features.", "conclusion": "FSIGenZ effectively reduces computational demands while maintaining performance in ZSL tasks."}}
{"id": "2507.01489", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.", "AI": {"tldr": "The paper introduces Agent-as-tool, a hierarchical framework separating tool calling and reasoning processes in LLM-based agents, improving performance with minimal fine-tuning.", "motivation": "Existing methods struggle with simultaneous tool calling and reasoning, burdening models with redundant information.", "method": "Proposes Agent-as-tool, decoupling tool calling (handled by another agent) from reasoning, allowing focus on verbal reasoning.", "result": "Achieved comparable results with slight fine-tuning (180 samples) and outperformed Search-R1 in Bamboogle (63.2% exact match, 75.2% cover exact match).", "conclusion": "Agent-as-tool effectively addresses the challenge of integrating tool calling and reasoning, enhancing LLM-based agent performance."}}
{"id": "2507.01282", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice.", "AI": {"tldr": "The paper discusses the limitations of LLMs in clinical settings, particularly for dementia diagnosis, and suggests hybrid AI approaches combining statistical learning with expert knowledge for better interpretability and workflow integration.", "motivation": "To address the gap between high benchmark scores of LLMs and their lack of measurable clinical impact, especially in dementia diagnosis and care.", "method": "A scoping review highlighting limitations of data-driven AI (e.g., lack of transparency, hallucinations) and proposing hybrid approaches (e.g., neuro-symbolic AI) involving clinicians.", "result": "Current LLMs fail to improve diagnostic accuracy or speed in clinical settings. Hybrid methods like PEIRS and ATHENA-CDS show promise by integrating expert knowledge and improving interpretability.", "conclusion": "Future AI should prioritize explanatory coherence, clinician understanding, and workflow fit, moving beyond accuracy metrics to measure real-world impact on patient outcomes."}}
{"id": "2507.01027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01027", "abs": "https://arxiv.org/abs/2507.01027", "authors": ["Zijian Ye", "Wei Huang", "Yifei Yu", "Tianhe Ren", "Zhongrui Wang", "Xiaojuan Qi"], "title": "DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization", "comment": "19 pages; Appendix added", "summary": "Large language models (LLMs) demonstrate remarkable performance but face\nsubstantial computational and memory challenges that limit their practical\ndeployment. Quantization has emerged as a promising solution; however, its\neffectiveness is often limited by quantization errors arising from weight\ndistributions that are not quantization-friendly and the presence of activation\noutliers. To address these challenges, we introduce DBellQuant, an innovative\npost-training quantization (PTQ) framework that achieves nearly 1-bit weight\ncompression and 6-bit activation quantization with minimal performance\ndegradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)\nalgorithm, which transforms single-bell weight distributions into dual-bell\nforms to reduce binarization errors and applies inverse transformations to\nsmooth activations. DBellQuant sets a new state-of-the-art by preserving\nsuperior model performance under aggressive weight and activation quantization.\nFor example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of\n14.39 on LLaMA2-13B with 6-bit activation quantization, significantly\noutperforming BiLLM's 21.35 without activation quantization, underscoring its\npotential in compressing LLMs for real-world applications.", "AI": {"tldr": "DBellQuant is a PTQ framework for LLMs, achieving 1-bit weight and 6-bit activation quantization with minimal performance loss using LTDB.", "motivation": "Address computational and memory challenges in LLMs by improving quantization effectiveness, reducing errors from weight distributions and activation outliers.", "method": "Uses LTDB to transform single-bell weight distributions into dual-bell forms, reducing binarization errors and smoothing activations.", "result": "Achieves perplexity of 14.39 on LLaMA2-13B (Wikitext2), outperforming BiLLM's 21.35.", "conclusion": "DBellQuant sets a new SOTA for LLM compression, enabling practical deployment with minimal performance degradation."}}
{"id": "2507.01717", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.", "AI": {"tldr": "Agent Ideate, a framework using LLMs and autonomous agents, generates business ideas from patents, outperforming standalone LLMs in quality, relevance, and novelty.", "motivation": "Patents hold valuable technical knowledge, but accessing and interpreting them for innovative ideas is challenging.", "method": "Agent Ideate combines LLMs and agent-based architectures to mine and generate product concepts from patents, tested in Computer Science, NLP, and Material Chemistry.", "result": "The agentic approach consistently outperformed standalone LLMs in idea quality, relevance, and novelty.", "conclusion": "Combining LLMs with agentic workflows enhances innovation by unlocking patent data's potential for business idea generation."}}
{"id": "2507.01376", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.", "AI": {"tldr": "The paper reviews AI agents, LLM-Agents, MLLM-Agents, and Agentic AI, exploring their capabilities, applications in smart manufacturing, and challenges.", "motivation": "To clarify the definitions, boundaries, and practical uses of emerging AI paradigms in smart manufacturing.", "method": "Systematic review of AI and AI agent evolution, focusing on LLM-Agents, MLLM-Agents, and Agentic AI.", "result": "Identifies advancements in AI agents' capabilities and potential applications in manufacturing, alongside challenges.", "conclusion": "The study highlights the promise of AI agents in smart manufacturing but notes the need for clearer definitions and addressing integration challenges."}}
{"id": "2507.01028", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01028", "abs": "https://arxiv.org/abs/2507.01028", "authors": ["Jean Ponce", "Martial Hebert", "Basile Terver"], "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "comment": null, "summary": "The objective of non-contrastive approaches to self-supervised learning is to\ntrain on pairs of different views of the data an encoder and a predictor that\nminimize the mean discrepancy between the code predicted from the embedding of\nthe first view and the embedding of the second one. In this setting, the stop\ngradient and exponential moving average iterative procedures are commonly used\nto avoid representation collapse, with excellent performance in downstream\nsupervised applications. This presentation investigates these procedures from\nthe dual theoretical viewpoints of optimization and dynamical systems. We first\nshow that, in general, although they do not optimize the original objective, or\nfor that matter, any other smooth function, they do avoid collapse. Following\nTian et al. [2021], but without any of the extra assumptions used in their\nproofs, we then show using a dynamical system perspective that, in the linear\ncase, minimizing the original objective function without the use of a stop\ngradient or exponential moving average always leads to collapse. Conversely, we\nfinally show that the limit points of the dynamical systems associated with\nthese two procedures are, in general, asymptotically stable equilibria, with no\nrisk of degenerating to trivial solutions.", "AI": {"tldr": "The paper analyzes non-contrastive self-supervised learning methods, focusing on stop gradient and exponential moving average procedures to prevent representation collapse. It provides theoretical insights into their optimization and dynamical system behaviors.", "motivation": "To understand why stop gradient and exponential moving average procedures avoid representation collapse in non-contrastive self-supervised learning, despite not optimizing the original objective.", "method": "The study uses optimization and dynamical system perspectives to analyze these procedures, particularly in the linear case, without extra assumptions.", "result": "Stop gradient and exponential moving average avoid collapse, while minimizing the original objective without them leads to collapse. Their dynamical systems have stable equilibria without trivial solutions.", "conclusion": "The procedures are theoretically justified for preventing collapse, offering stable solutions in non-contrastive self-supervised learning."}}
{"id": "2507.01410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach.", "AI": {"tldr": "A formal method for Ethical Decision Making models using fuzzy rules and Petri nets, validated with a medical case study.", "motivation": "Addressing the challenge of evaluating moral machine performance due to ontological and epistemic complexities in ethics.", "method": "Develops Ethical Decision Making models via ethical risk assessment, represented as fuzzy rules, and validates them using fuzzy Petri nets.", "result": "Demonstrates the approach's feasibility through a medical case study.", "conclusion": "Proposes a viable method for verifying and validating ethical decision models in machines."}}
{"id": "2507.01029", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning.", "AI": {"tldr": "PathCoT is a zero-shot CoT prompting method integrating pathology expert knowledge and self-evaluation to improve MLLMs' performance in pathology visual reasoning.", "motivation": "Existing MLLMs underperform in pathology tasks due to lack of domain knowledge and errors introduced by additional CoT steps.", "method": "PathCoT integrates expert knowledge into MLLMs' reasoning and adds self-evaluation to mitigate errors.", "result": "Experiments on PathMMU show PathCoT enhances pathology visual understanding and reasoning.", "conclusion": "PathCoT effectively addresses domain-specific challenges in MLLMs for pathology tasks."}}
{"id": "2507.01431", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.", "AI": {"tldr": "Pensieve is an AI-assisted grading platform using LLMs to transcribe and evaluate handwritten STEM responses, reducing grading time by 65% with high accuracy.", "motivation": "Handwritten grading is a bottleneck in large STEM courses; Pensieve aims to streamline the process with AI assistance.", "method": "Pensieve leverages LLMs for transcription and evaluation, supporting the entire grading pipeline with a human-in-the-loop interface.", "result": "Deployed in 20+ institutions, graded 300,000+ responses, reducing grading time by 65% with 95.4% agreement for high-confidence predictions.", "conclusion": "Pensieve effectively reduces grading workload while maintaining high accuracy, proving viable for large-scale STEM courses."}}
{"id": "2507.01030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01030", "abs": "https://arxiv.org/abs/2507.01030", "authors": ["Reza Lotfi Navaei", "Mohammad Safarzadeh", "Seyed Mohammad Jafar Sobhani"], "title": "Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study", "comment": "It has been submitted to ASME Journal of Heat and Mass Transfer", "summary": "In chemistry tabulations and Flamelet combustion models, the Flamelet\nGenerated Manifold (FGM) is recognized for its precision and physical\nrepresentation. The practical implementation of FGM requires a significant\nallocation of memory resources. FGM libraries are developed specifically for a\nspecific fuel and subsequently utilized for all numerical problems using\nmachine learning techniques. This research aims to develop libraries of Laminar\nFGM utilizing machine learning algorithms for application in combustion\nsimulations of methane fuel. This study employs four Machine Learning\nalgorithms to regenerate Flamelet libraries, based on an understanding of data\nsources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.\nRandom Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries\nwere identified as appropriate for constructing a database for training machine\nlearning models, giving an error rate of 2.30%. The default architectures of\neach method were evaluated to determine the optimal approach, leading to the\nselection of the MLP method as the primary choice. The method was enhanced\nthrough hyperparameter tuning to improve accuracy. The quantity of hidden\nlayers and neurons significantly influences method performance. The optimal\nmodel, comprising four hidden layers with 10, 15, 20, and 25 neurons\nrespectively, achieved an accuracy of 99.81%.", "AI": {"tldr": "The paper explores using machine learning (ML) to regenerate Flamelet libraries for methane combustion, comparing four ML algorithms. MLP was selected as the best method, achieving 99.81% accuracy after hyperparameter tuning.", "motivation": "The Flamelet Generated Manifold (FGM) is precise but memory-intensive. This research aims to optimize FGM libraries for methane combustion using ML.", "method": "Four ML algorithms (MLP, Random Forest, Linear Regression, SVM) were tested to regenerate Flamelet libraries. Hyperparameter tuning was applied to MLP.", "result": "MLP outperformed others, achieving 99.81% accuracy with four hidden layers (10, 15, 20, 25 neurons). Error rate was 2.30%.", "conclusion": "MLP is optimal for regenerating FGM libraries, offering high accuracy for methane combustion simulations."}}
{"id": "2507.01446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.", "AI": {"tldr": "A multi-agent system using LLMs and fuzzy logic to improve customer service quality by reducing hallucination risks in SMS-based requests.", "motivation": "Enhancing customer service quality and response time is vital for loyalty and market share, but LLMs' hallucination risks pose challenges.", "method": "Proposes a multi-agent system combining LLMs with fuzzy logic to handle SMS customer requests.", "result": "The system aims to mitigate hallucination risks while improving service efficiency.", "conclusion": "Integrating fuzzy logic with LLMs can effectively address hallucination challenges in customer service."}}
{"id": "2507.01031", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.01031", "abs": "https://arxiv.org/abs/2507.01031", "authors": ["Fanchen Bu", "Kijung Shin"], "title": "PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs", "comment": "Conference paper: Accepted in Korea Computer Congress (KCC) 2025. The\n  library is available at https://github.com/bokveizen/gaudi-geometric-learning", "summary": "Geometric learning has emerged as a powerful paradigm for modeling\nnon-Euclidean data, especially graph-structured ones, with applications\nspanning social networks, molecular structures, knowledge graphs, and\nrecommender systems. While Nvidia's CUDA-enabled graphics processing units\n(GPUs) largely dominate the hardware landscape, emerging accelerators such as\nIntel's Gaudi Habana Processing Units (HPUs) offer competitive performance and\nenergy efficiency. However, the usage of such non-CUDA processing units\nrequires significant engineering effort and novel software adaptations. In this\nwork, we present our experiences porting PyTorch-based geometric learning\nframeworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that\nrestore essential operations (e.g., scatter, sparse indexing, k-nearest\nneighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and\neleven real-world examples with diagnostic analyses of encountered failures and\ndetailed workarounds. We collect all our experiences into a publicly accessible\nGitHub repository. Our contributions lower the barrier for researchers to\nexperiment with geometric-learning algorithms and models on non-CUDA hardware,\nproviding a foundation for further optimization and cross-platform portability.", "AI": {"tldr": "The paper discusses porting PyTorch-based geometric learning frameworks to Intel's Gaudi-v2 HPUs, addressing challenges and providing utilities, tutorials, and examples to ease adoption.", "motivation": "Geometric learning is powerful for non-Euclidean data, but non-CUDA hardware like Intel's Gaudi-v2 HPUs requires significant adaptation. The work aims to lower barriers for researchers using such hardware.", "method": "The authors port PyTorch-based frameworks to Gaudi-v2 HPUs, introducing core utilities for essential operations and providing tutorials and real-world examples with diagnostic analyses.", "result": "A collection of utilities, tutorials, and examples is created, consolidated into a GitHub repository, enabling easier experimentation with geometric learning on non-CUDA hardware.", "conclusion": "The work facilitates geometric learning on non-CUDA hardware, laying groundwork for further optimizations and cross-platform portability."}}
{"id": "2507.01032", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01032", "abs": "https://arxiv.org/abs/2507.01032", "authors": ["Nan Mu", "Hongbo Yang", "Chen Zhao"], "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks", "comment": null, "summary": "Background and Objective: High-throughput multi-omics technologies have\nproven invaluable for elucidating disease mechanisms and enabling early\ndiagnosis. However, the high cost of multi-omics profiling imposes a\nsignificant economic burden, with over reliance on full omics data potentially\nleading to unnecessary resource consumption. To address these issues, we\npropose an uncertainty-aware, multi-view dynamic decision framework for omics\ndata classification that aims to achieve high diagnostic accuracy while\nminimizing testing costs. Methodology: At the single-omics level, we refine the\nactivation functions of neural networks to generate Dirichlet distribution\nparameters, utilizing subjective logic to quantify both the belief masses and\nuncertainty mass of classification results. Belief mass reflects the support of\na specific omics modality for a disease class, while the uncertainty parameter\ncaptures limitations in data quality and model discriminability, providing a\nmore trustworthy basis for decision-making. At the multi omics level, we employ\na fusion strategy based on Dempster-Shafer theory to integrate heterogeneous\nmodalities, leveraging their complementarity to boost diagnostic accuracy and\nrobustness. A dynamic decision mechanism is then applied that omics data are\nincrementally introduced for each patient until either all data sources are\nutilized or the model confidence exceeds a predefined threshold, potentially\nbefore all data sources are utilized. Results and Conclusion: We evaluate our\napproach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.\nIn three datasets, over 50% of cases achieved accurate classification using a\nsingle omics modality, effectively reducing redundant testing. Meanwhile, our\nmethod maintains diagnostic performance comparable to full-omics models and\npreserves essential biological insights.", "AI": {"tldr": "Proposes an uncertainty-aware, multi-view dynamic decision framework for multi-omics classification to reduce costs while maintaining accuracy.", "motivation": "High costs and resource waste in multi-omics profiling necessitate a cost-effective, accurate diagnostic method.", "method": "Uses neural networks with refined activation functions for Dirichlet distribution parameters, subjective logic for belief/uncertainty quantification, and Dempster-Shafer theory for multi-omics fusion. A dynamic decision mechanism incrementally uses omics data.", "result": "Achieves accurate classification with fewer omics modalities in 50%+ cases, reducing redundant testing while matching full-omics model performance.", "conclusion": "The framework balances cost and accuracy, preserving biological insights and reducing unnecessary testing."}}
{"id": "2507.01597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "The paper proposes T3DM, a method for Temporal Knowledge Graph Reasoning (TKGR) that addresses distribution shift and improves negative sampling, outperforming existing baselines.", "motivation": "Existing TKGR methods struggle with event distribution shifts between training and test samples and rely on low-quality random negative sampling.", "method": "T3DM models distribution shifts and uses adversarial training for high-quality negative sampling.", "result": "T3DM achieves better and more robust performance than state-of-the-art baselines.", "conclusion": "T3DM effectively addresses key challenges in TKGR, enhancing model consistency and sampling quality."}}
{"id": "2507.01034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01034", "abs": "https://arxiv.org/abs/2507.01034", "authors": ["Asma Agaal", "Mansour Essgaer", "Hend M. Farkash", "Zulaiha Ali Othman"], "title": "Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya", "comment": "This article was published in International Journal of Intelligent\n  Systems and Applications (IJISA) (MECS Press), Vol. 17, No. 3, 8 Jun. 2025,\n  DOI: https://doi.org/10.5815/ijisa.2025.03.05", "summary": "Accurate electricity forecasting is crucial for grid stability and energy\nplanning, especially in Benghazi, Libya, where frequent load shedding,\ngeneration deficits, and infrastructure limitations persist. This study\nproposes a data-driven approach to forecast electricity load, generation, and\ndeficits for 2025 using historical data from 2019 (a year marked by\ninstability) and 2023 (a more stable year). Multiple time series models were\napplied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential\nsmoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural\nnetworks. The dataset was enhanced through missing value imputation, outlier\nsmoothing, and log transformation. Performance was assessed using mean squared\nerror, root mean squared error, mean absolute error, and mean absolute\npercentage error. LSTM outperformed all other models, showing strong\ncapabilities in modeling non-stationary and seasonal patterns. A key\ncontribution of this work is an optimized LSTM framework that integrates\nexogenous factors such as temperature and humidity, offering robust performance\nin forecasting multiple electricity indicators. These results provide practical\ninsights for policymakers and grid operators to enable proactive load\nmanagement and resource planning in data-scarce, volatile regions.", "AI": {"tldr": "The study proposes a data-driven approach using LSTM to forecast electricity load, generation, and deficits in Benghazi, Libya, outperforming other models and aiding grid stability.", "motivation": "Accurate electricity forecasting is vital for grid stability and planning in Benghazi, Libya, due to frequent load shedding and infrastructure challenges.", "method": "Multiple time series models (ARIMA, SARIMA, dynamic regression ARIMA, exponential smoothing, XGBoost, LSTM) were applied to historical data (2019 and 2023), enhanced with preprocessing techniques.", "result": "LSTM outperformed other models, effectively handling non-stationary and seasonal data, and integrating exogenous factors like temperature and humidity.", "conclusion": "The optimized LSTM framework provides actionable insights for policymakers and grid operators in volatile, data-scarce regions."}}
{"id": "2507.01035", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01035", "abs": "https://arxiv.org/abs/2507.01035", "authors": ["Yushang Zhao", "Haotian Lyu", "Yike Peng", "Aijia Sun", "Feng Jiang", "Xinyue Han"], "title": "Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems", "comment": null, "summary": "The incessant advent of online services demands high speed and efficient\nrecommender systems (ReS) that can maintain real-time performance along with\nprocessing very complex user-item interactions. The present study, therefore,\nconsiders computational bottlenecks involved in hybrid Graph Neural Network\n(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their\ninference latency and training efficiency. An extensive methodology was used:\nhybrid GNN-LLM integrated architecture-optimization strategies(quantization,\nLoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.\nExperimental improvements were significant, with the optimal Hybrid + FPGA +\nDeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms\nof latency, while LoRA brought down training time by 66% (3.8 hours) in\ncomparison to the non-optimized baseline. Irrespective of domain, such as\naccuracy or efficiency, it can be established that hardware-software co-design\nand parameter-efficient tuning permit hybrid models to outperform GNN or LLM\napproaches implemented independently. It recommends the use of FPGA as well as\nLoRA for real-time deployment. Future work should involve federated learning\nalong with advanced fusion architectures for better scalability and privacy\npreservation. Thus, this research marks the fundamental groundwork concerning\nnext-generation ReS balancing low-latency response with cutting-edge\npersonalization.", "AI": {"tldr": "The paper proposes a hybrid GNN-LLM recommender system optimized for speed and efficiency using quantization, LoRA, distillation, FPGA, and DeepSpeed, achieving higher accuracy and reduced training time.", "motivation": "Address computational bottlenecks in hybrid GNN-LLM recommender systems to improve inference latency and training efficiency.", "method": "Hybrid GNN-LLM architecture with optimization strategies (quantization, LoRA, distillation) and hardware acceleration (FPGA, DeepSpeed).", "result": "Optimal configuration improved accuracy by 13.6% (NDCG@10: 0.75) at 40-60ms latency, and LoRA reduced training time by 66% (3.8 hours).", "conclusion": "Hardware-software co-design and parameter-efficient tuning enable hybrid models to outperform standalone GNN or LLM approaches, recommending FPGA and LoRA for real-time deployment. Future work includes federated learning and advanced fusion architectures."}}
{"id": "2507.01749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01749", "abs": "https://arxiv.org/abs/2507.01749", "authors": ["Arash Dehghan", "Mucahit Cevik", "Merve Bodur", "Bissan Ghaddar"], "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers", "comment": null, "summary": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators.", "AI": {"tldr": "The paper proposes a crowd-shipping system using in-store customers as couriers, optimized via MDP, NeurADP, and DDQN for cost-efficient last-mile delivery.", "motivation": "Address the growing need for efficient last-mile delivery in urban areas by leveraging in-store shoppers as delivery couriers.", "method": "Uses a Markov Decision Process (MDP) model with NeurADP for adaptive order-to-shopper assignment and DDQN for dynamic pricing.", "result": "Achieves 6.7% cost savings over NeurADP with fixed pricing and 18% over baselines; flexible delays and multi-destination routing reduce costs further.", "conclusion": "Dynamic, forward-looking policies enhance crowd-shipping efficiency, offering practical benefits for urban logistics."}}
{"id": "2507.01037", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01037", "abs": "https://arxiv.org/abs/2507.01037", "authors": ["Wenbin Ouyang", "Sirui Li", "Yining Ma", "Cathy Wu"], "title": "Learning to Segment for Vehicle Routing Problems", "comment": null, "summary": "Iterative search heuristics are widely recognized as state-of-the-art for\nsolving Vehicle Routing Problems (VRPs). In this work, we identify and exploit\na critical observation: within these solvers, a large portion of the solution\nremains stable, i.e., unchanged across search iterations, causing redundant\ncomputations, especially for large-scale VRPs with long subtours. To address\nthis, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)\ndecomposition technique to accelerate iterative solvers. Specifically, FSTA\npreserves stable solution segments during the search, aggregates nodes within\neach segment into fixed hypernodes, and focuses the search only on unstable\nportions. Yet, a key challenge lies in identifying which segments should be\naggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),\na novel neural framework to intelligently differentiate potentially stable and\nunstable portions for FSTA decomposition. We present three L2Seg variants:\nnon-autoregressive (globally comprehensive but locally indiscriminate),\nautoregressive (locally refined but globally deficient), and their synergy,\nwith bespoke training and inference strategies. Empirical results on CVRP and\nVRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up\nto 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy\nachieves best performance by combining their complementary strengths. Notably,\nL2Seg is a flexible framework that is compatible with traditional,\nlearning-based, and hybrid solvers, while supporting a broad class of VRPs.", "AI": {"tldr": "The paper introduces the FSTA decomposition technique and L2Seg neural framework to accelerate iterative solvers for VRPs by focusing on unstable portions of solutions, achieving up to 7x speedup.", "motivation": "Redundant computations in iterative solvers due to stable solution segments in large-scale VRPs.", "method": "FSTA preserves stable segments and aggregates them into hypernodes, while L2Seg intelligently identifies stable/unstable portions using three variants (non-autoregressive, autoregressive, and their synergy).", "result": "Empirical results show up to 7x acceleration in state-of-the-art solvers for CVRP and VRPTW.", "conclusion": "L2Seg is a flexible framework compatible with various solvers and VRPs, with the NAR and AR synergy offering the best performance."}}
{"id": "2507.01833", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01833", "abs": "https://arxiv.org/abs/2507.01833", "authors": ["Yi-Dong Shen", "Thomas Eiter"], "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics", "comment": "76 pages. This article is a significantly extended version of a paper\n  presented by the authors at IJCAI-2022", "summary": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity.", "AI": {"tldr": "The paper questions mandatory conditions for answer set semantics in non-monotonic logic programming, proposes refined principles, and defines new semantics.", "motivation": "To evaluate if existing conditions (minimal model property, constraint monotonicity, foundedness) are too restrictive and to propose alternative principles for answer set semantics.", "method": "Illustrates limitations of current conditions, refines Gelfond's principles (well-supportedness, minimality), extends well-supportedness, defines new semantics, and analyzes computational complexity.", "result": "Existing conditions may exclude expected answer sets; refined principles (well-supportedness, minimality) offer a better baseline for semantics.", "conclusion": "Refined GAS principles provide a more flexible and intuitive framework for answer set semantics, validated through new definitions and complexity analysis."}}
{"id": "2507.01039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01039", "abs": "https://arxiv.org/abs/2507.01039", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Kelly Cohen"], "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization", "comment": "Submitted to NAFIPS 2025", "summary": "We propose a reinforcement learning (RL) approach for training neuro-fuzzy\ncontrollers using Proximal Policy Optimization (PPO). Building on prior work\nthat applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),\nour method replaces the off-policy value-based framework with a stable\non-policy actor-critic loop. We evaluate this approach in the CartPole-v1\nenvironment using multiple random seeds and compare its learning performance\nagainst ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained\nfuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000\nupdates, showcasing less variance than prior DQN-based methods during training\nand overall faster convergence. These findings suggest that PPO offers a\npromising pathway for training explainable neuro-fuzzy controllers in\nreinforcement learning tasks.", "AI": {"tldr": "A reinforcement learning approach using PPO for training neuro-fuzzy controllers shows improved stability and faster convergence compared to DQN-based methods.", "motivation": "To enhance the training of explainable neuro-fuzzy controllers by replacing off-policy DQN with stable on-policy PPO.", "method": "PPO is applied to train neuro-fuzzy controllers, evaluated in CartPole-v1 with multiple random seeds and compared to ANFIS-DQN baselines.", "result": "PPO-trained agents achieved a mean return of 500 +/- 0, with less variance and faster convergence than DQN methods.", "conclusion": "PPO is a promising method for training explainable neuro-fuzzy controllers in RL tasks."}}
{"id": "2507.01040", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01040", "abs": "https://arxiv.org/abs/2507.01040", "authors": ["Tianxiang Xia", "Max Neuwinger", "Lin Xiao"], "title": "Fast Clifford Neural Layers", "comment": "7 pages content-wise", "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers", "AI": {"tldr": "Clifford Neural Layers enhance PDE modeling using Clifford Algebra, optimizing 2/3D convolutional and multivector activation layers for CPU performance, achieving 30% speedup over PyTorch.", "motivation": "To improve PDE modeling efficiency by integrating Clifford Algebra into neural networks, focusing on CPU performance optimization.", "method": "Optimizing inference of 2/3D Clifford convolutional layers and multivector activation layers for single-core CPU performance.", "result": "30% faster than standard PyTorch implementation for large data and network sizes (>L2 cache).", "conclusion": "The optimized Clifford Neural Layers show significant performance gains, with open-sourced code available for further use."}}
{"id": "2507.01041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01041", "abs": "https://arxiv.org/abs/2507.01041", "authors": ["Zuguang Li", "Wen Wu", "Shaohua Wu", "Songge Zhang", "Ye Wang", "Xuemin", "Shen"], "title": "Fast AI Model Splitting over Edge Networks", "comment": "13 pages, 14 figures", "summary": "Split learning (SL) has emerged as a computationally efficient approach for\nartificial intelligence (AI) model training, which can alleviate device-side\ncomputational workloads. However, complex AI model architectures pose high\ncomputational complexity to obtain the optimal model splitting. In this paper,\nwe represent an arbitrary AI model as a directed acyclic graph (DAG), and then\nreformulate the optimal model splitting problem as a minimum s-t cut search\nproblem. To solve the problem, we propose a fast DAG-based model splitting\nalgorithm, which restructures the DAG to enable the optimal model splitting\nidentification via a maximum flow method. Theoretical analysis indicates that\nthe proposed algorithm is optimal. Furthermore, considering AI models with\nblock structures, we propose a block-wise model splitting algorithm to reduce\ncomputational complexity. The algorithm abstracts each block, i.e., a component\nconsisting of multiple layers, into a single vertex, thereby obtaining the\noptimal model splitting via a simplified DAG. Extensive experimental results\ndemonstrate that the proposed algorithms can determine the optimal model\nsplitting within milliseconds, as well as reduce training delay by\n24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art\nbenchmarks.", "AI": {"tldr": "The paper introduces a DAG-based algorithm for optimal model splitting in split learning, reducing computational complexity and training delay.", "motivation": "Complex AI model architectures increase computational complexity for optimal model splitting in split learning, necessitating efficient solutions.", "method": "Represent AI models as DAGs, reformulate splitting as a minimum s-t cut problem, and propose fast DAG-based and block-wise splitting algorithms.", "result": "The algorithms achieve optimal splitting in milliseconds and reduce training delay by 24.62%-38.95% compared to benchmarks.", "conclusion": "The proposed methods efficiently solve model splitting in split learning, offering significant performance improvements."}}
{"id": "2507.01043", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01043", "abs": "https://arxiv.org/abs/2507.01043", "authors": ["Szymon \u015awiderski", "Agnieszka Jastrz\u0119bska"], "title": "Data Classification with Dynamically Growing and Shrinking Neural Networks", "comment": "Paper submitted to Journal of Computational Science", "summary": "The issue of data-driven neural network model construction is one of the core\nproblems in the domain of Artificial Intelligence. A standard approach assumes\na fixed architecture with trainable weights. A conceptually more advanced\nassumption is that we not only train the weights, but also find out the optimal\nmodel architecture. We present a new method that realizes just that. This\narticle is an extended version of our conference paper titled \"Dynamic Growing\nand Shrinking of Neural Networks with Monte Carlo Tree Search [26]\". In the\npaper, we show in detail how to create a neural network with a procedure that\nallows dynamic shrinking and growing of the model while it is being trained.\nThe decision-making mechanism for the architectural design is governed by a\nMonte Carlo tree search procedure which simulates network behavior and allows\nto compare several candidate architecture changes to choose the best one. The\nproposed method was validated using both visual and time series datasets,\ndemonstrating its particular effectiveness in multivariate time series\nclassification. This is attributed to the architecture's ability to adapt\ndynamically, allowing independent modifications for each time series. The\napproach is supplemented by Python source code for reproducibility.\nExperimental evaluations in visual pattern and multivariate time series\nclassification tasks revealed highly promising performance, underscoring the\nmethod's robustness and adaptability.", "AI": {"tldr": "A method for dynamically adjusting neural network architecture during training using Monte Carlo tree search, validated on visual and time series datasets.", "motivation": "To optimize neural network performance by dynamically adjusting architecture during training, beyond just tuning weights.", "method": "Uses Monte Carlo tree search to simulate and compare candidate architecture changes, enabling dynamic growing and shrinking of the model.", "result": "Demonstrated effectiveness, especially in multivariate time series classification, due to dynamic adaptability.", "conclusion": "The method is robust and adaptable, with promising performance in diverse tasks, supported by reproducible Python code."}}
{"id": "2507.01045", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01045", "abs": "https://arxiv.org/abs/2507.01045", "authors": ["Xiao Gu", "Wei Tang", "Jinpei Han", "Veer Sangha", "Fenglin Liu", "Shreyank N Gowda", "Antonio H. Ribeiro", "Patrick Schwab", "Kim Branson", "Lei Clifton", "Antonio Luiz P. Ribeiro", "Zhangdaihong Liu", "David A. Clifton"], "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals", "comment": null, "summary": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms\n(PPG), are of paramount importance for the diagnosis, prevention, and\nmanagement of cardiovascular diseases, and have been extensively used in a\nvariety of clinical tasks. Conventional deep learning approaches for analyzing\nthese signals typically rely on homogeneous datasets and static bespoke models,\nlimiting their robustness and generalizability across diverse clinical settings\nand acquisition protocols. In this study, we present a cardiac sensing\nfoundation model (CSFM) that leverages advanced transformer architectures and a\ngenerative, masked pretraining strategy to learn unified representations from\nvast, heterogeneous health records. Our model is pretrained on an innovative\nmulti-modal integration of data from multiple large-scale datasets (including\nMIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the\ncorresponding clinical or machine-generated text reports from approximately 1.7\nmillion individuals. We demonstrate that the embeddings derived from our CSFM\nnot only serve as effective feature extractors across diverse cardiac sensing\nscenarios, but also enable seamless transfer learning across varying input\nconfigurations and sensor modalities. Extensive evaluations across diagnostic\ntasks, demographic information recognition, vital sign measurement, clinical\noutcome prediction, and ECG question answering reveal that CSFM consistently\noutperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits\nrobust performance across multiple ECG lead configurations from standard\n12-lead systems to single-lead setups, and in scenarios where only ECG, only\nPPG, or a combination thereof is available. These findings highlight the\npotential of CSFM as a versatile and scalable solution, for comprehensive\ncardiac monitoring.", "AI": {"tldr": "A cardiac sensing foundation model (CSFM) using transformers and masked pretraining achieves superior performance across diverse cardiac biosignal tasks, outperforming traditional methods.", "motivation": "To address limitations of conventional deep learning approaches in cardiac biosignal analysis, which lack robustness and generalizability due to reliance on homogeneous datasets and static models.", "method": "Developed CSFM with transformer architectures and generative masked pretraining, trained on multi-modal data (ECG, PPG, and text reports) from 1.7 million individuals.", "result": "CSFM embeddings excel in feature extraction and transfer learning, outperforming traditional methods in diagnostic tasks, demographic recognition, vital sign measurement, and more.", "conclusion": "CSFM is a versatile, scalable solution for comprehensive cardiac monitoring, robust across various sensor modalities and lead configurations."}}
{"id": "2507.01047", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01047", "abs": "https://arxiv.org/abs/2507.01047", "authors": ["Logan A. Burnett", "Umme Mahbuba Nabila", "Majdi I. Radaideh"], "title": "Variational Digital Twins", "comment": "33 pages, 14 figures, and 7 tables", "summary": "While digital twins (DT) hold promise for providing real-time insights into\ncomplex energy assets, much of the current literature either does not offer a\nclear framework for information exchange between the model and the asset, lacks\nkey features needed for real-time implementation, or gives limited attention to\nmodel uncertainty. Here, we aim to solve these gaps by proposing a variational\ndigital twin (VDT) framework that augments standard neural architectures with a\nsingle Bayesian output layer. This lightweight addition, along with a novel VDT\nupdating algorithm, lets a twin update in seconds on commodity GPUs while\nproducing calibrated uncertainty bounds that can inform experiment design,\ncontrol algorithms, and model reliability. The VDT is evaluated on four\nenergy-sector problems. For critical-heat-flux prediction, uncertainty-driven\nactive learning reaches R2 = 0.98 using 47 % fewer experiments and one-third\nthe training time of random sampling. A three-year renewable-generation twin\nmaintains R2 > 0.95 for solar output and curbs error growth for volatile wind\nforecasts via monthly updates that process only one month of data at a time. A\nnuclear reactor transient cooldown twin reconstructs thermocouple signals with\nR2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating\nrobustness to degraded instrumentation. Finally, a physics-informed Li-ion\nbattery twin, retrained after every ten discharges, lowers voltage mean-squared\nerror by an order of magnitude relative to the best static model while adapting\nits credible intervals as the cell approaches end-of-life. These results\ndemonstrate that combining modest Bayesian augmentation with efficient update\nschemes turns conventional surrogates into uncertainty-aware, data-efficient,\nand computationally tractable DTs, paving the way for dependable models across\nindustrial and scientific energy systems.", "AI": {"tldr": "The paper proposes a Variational Digital Twin (VDT) framework to address gaps in current digital twin literature, offering real-time updates, uncertainty calibration, and efficiency. It demonstrates success in energy-sector applications.", "motivation": "Current digital twin literature lacks clear frameworks for real-time implementation, information exchange, and model uncertainty. The VDT aims to solve these issues.", "method": "The VDT augments standard neural architectures with a Bayesian output layer and a novel updating algorithm, enabling fast updates and calibrated uncertainty bounds.", "result": "The VDT achieves high accuracy (R2 > 0.95) in energy-sector problems, reduces experiments by 47%, and adapts to sensor loss or end-of-life conditions.", "conclusion": "The VDT framework transforms conventional models into uncertainty-aware, efficient digital twins, suitable for industrial and scientific energy systems."}}
{"id": "2507.01048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01048", "abs": "https://arxiv.org/abs/2507.01048", "authors": ["Ricardo Emanuel Vaz Vargas", "Afr\u00e2nio Jos\u00e9 de Melo Junior", "Celso Jos\u00e9 Munaro", "Cl\u00e1udio Benevenuto de Campos Lima", "Eduardo Toledo de Lima Junior", "Felipe Muntzberg Barrocas", "Fl\u00e1vio Miguel Varej\u00e3o", "Guilherme Fidelis Peixer", "Igor de Melo Nery Oliveira", "Jader Riso Barbosa Jr.", "Jaime Andr\u00e9s Lozano Cadena", "Jean Carlos Dias de Ara\u00fajo", "Jo\u00e3o Neuenschwander Escosteguy Carneiro", "Lucas Gouveia Omena Lopes", "Lucas Pereira de Gouveia", "Mateus de Araujo Fernandes", "Matheus Lima Scramignon", "Patrick Marques Ciarelli", "Rodrigo Castello Branco", "Rog\u00e9rio Leite Alves Pinto"], "title": "3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells", "comment": "21 pages, 10 figures, and 7 tables", "summary": "In the oil industry, undesirable events in oil wells can cause economic\nlosses, environmental accidents, and human casualties. Solutions based on\nArtificial Intelligence and Machine Learning for Early Detection of such events\nhave proven valuable for diverse applications across industries. In 2019,\nrecognizing the importance and the lack of public datasets related to\nundesirable events in oil wells, Petrobras developed and publicly released the\nfirst version of the 3W Dataset, which is essentially a set of Multivariate\nTime Series labeled by experts. Since then, the 3W Dataset has been developed\ncollaboratively and has become a foundational reference for numerous works in\nthe field. This data article describes the current publicly available version\nof the 3W Dataset, which contains structural modifications and additional\nlabeled data. The detailed description provided encourages and supports the 3W\ncommunity and new 3W users to improve previous published results and to develop\nnew robust methodologies, digital products and services capable of detecting\nundesirable events in oil wells with enough anticipation to enable corrective\nor mitigating actions.", "AI": {"tldr": "The paper describes the updated 3W Dataset, a public resource for detecting undesirable events in oil wells using AI/ML, aimed at improving early detection and mitigation.", "motivation": "Undesirable events in oil wells cause economic, environmental, and human harm. Public datasets for AI/ML solutions were lacking, prompting the creation and collaborative development of the 3W Dataset.", "method": "The 3W Dataset is a labeled multivariate time series dataset, developed and refined collaboratively since 2019. The current version includes structural updates and additional labeled data.", "result": "The dataset serves as a foundational reference for research, enabling improved methodologies and digital tools for early event detection.", "conclusion": "The updated 3W Dataset supports the community in advancing detection capabilities, fostering corrective actions and new solutions."}}
{"id": "2507.01050", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/", "AI": {"tldr": "A two-stage training framework for detoxifying social media text achieves strong detoxification, semantic preservation, and generalization with reduced reliance on annotated data.", "motivation": "Addressing the limitations of existing detoxification methods, which struggle with performance, semantic preservation, and data efficiency.", "method": "A two-stage approach: supervised fine-tuning on filtered parallel data, followed by training with unlabeled toxic inputs and a custom reward model using Group Relative Policy Optimization.", "result": "State-of-the-art performance in detoxification, improved generalization, and reduced dependence on annotated data.", "conclusion": "The proposed framework effectively mitigates trade-offs in detoxification, offering a robust and efficient solution."}}
{"id": "2507.01052", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.01052", "abs": "https://arxiv.org/abs/2507.01052", "authors": ["Ahmed Farooq"], "title": "Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals", "comment": null, "summary": "In this study we introduce a novel energy functional for long-sequence\nmemory, building upon the framework of dense Hopfield networks which achieves\nexponential storage capacity through higher-order interactions. Building upon\nearlier work on long-sequence Hopfield memory models, we propose a temporal\nkernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient\nsequential retrieval of patterns over extended sequences. We demonstrate the\nsuccessful application of this technique for the storage and sequential\nretrieval of movies frames which are well suited for this because of the high\ndimensional vectors that make up each frame creating enough variation between\neven sequential frames in the high dimensional space. The technique has\napplications in modern transformer architectures, including efficient\nlong-sequence modeling, memory augmentation, improved attention with temporal\nbias, and enhanced handling of long-term dependencies in time-series data. Our\nmodel offers a promising approach to address the limitations of transformers in\nlong-context tasks, with potential implications for natural language\nprocessing, forecasting, and beyond.", "AI": {"tldr": "A novel energy functional for long-sequence memory is introduced, using dense Hopfield networks with higher-order interactions and a temporal kernel for efficient sequential retrieval. Demonstrated with movie frames, it has applications in transformers for long-context tasks.", "motivation": "To address limitations of transformers in long-context tasks by improving storage capacity and sequential retrieval in memory models.", "method": "Proposes a temporal kernel $K(m, k)$ within dense Hopfield networks to incorporate temporal dependencies for sequential pattern retrieval.", "result": "Successful storage and sequential retrieval of movie frames, showcasing the model's capability with high-dimensional data.", "conclusion": "The model enhances transformers for long-sequence tasks, with potential applications in NLP, forecasting, and time-series data."}}
{"id": "2507.01054", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01054", "abs": "https://arxiv.org/abs/2507.01054", "authors": ["Jithendaraa Subramanian", "Linda Hung", "Daniel Schweigert", "Santosh Suram", "Weike Ye"], "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science", "comment": "10 pages, 6 figures", "summary": "Recent advances in materials discovery have been driven by structure-based\nmodels, particularly those using crystal graphs. While effective for\ncomputational datasets, these models are impractical for real-world\napplications where atomic structures are often unknown or difficult to obtain.\nWe propose a scalable multimodal framework that learns directly from elemental\ncomposition and X-ray diffraction (XRD) -- two of the more available modalities\nin experimental workflows without requiring crystal structure input. Our\narchitecture integrates modality-specific encoders with a cross-attention\nfusion module and is trained on the 5-million-sample Alexandria dataset. We\npresent masked XRD modeling (MXM), and apply MXM and contrastive alignment as\nself-supervised pretraining strategies. Pretraining yields faster convergence\n(up to 4.2x speedup) and improves both accuracy and representation quality. We\nfurther demonstrate that multimodal performance scales more favorably with\ndataset size than unimodal baselines, with gains compounding at larger data\nregimes. Our results establish a path toward structure-free, experimentally\ngrounded foundation models for materials science.", "AI": {"tldr": "A scalable multimodal framework is proposed for materials discovery, using elemental composition and XRD without crystal structure input, achieving faster convergence and improved accuracy.", "motivation": "Current structure-based models are impractical for real-world applications where atomic structures are unknown or hard to obtain.", "method": "The framework integrates modality-specific encoders with a cross-attention fusion module, trained on the Alexandria dataset, and uses self-supervised pretraining (MXM and contrastive alignment).", "result": "Pretraining yields faster convergence (4.2x speedup), improved accuracy, and better representation quality. Multimodal performance scales better with dataset size than unimodal baselines.", "conclusion": "The work establishes a path toward structure-free, experimentally grounded foundation models for materials science."}}
{"id": "2507.01056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01056", "abs": "https://arxiv.org/abs/2507.01056", "authors": ["Lidan Peng", "Lu Gao", "Feng Hong", "Jingran Sun"], "title": "Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI", "comment": null, "summary": "Flooding can damage pavement infrastructure significantly, causing both\nimmediate and long-term structural and functional issues. This research\ninvestigates how flooding events affect pavement deterioration, specifically\nfocusing on measuring pavement roughness by the International Roughness Index\n(IRI). To quantify these effects, we utilized 20 years of pavement condition\ndata from TxDOT's PMIS database, which is integrated with flood event data,\nincluding duration and spatial extent. Statistical analyses were performed to\ncompare IRI values before and after flooding and to calculate the deterioration\nrates influenced by flood exposure. Moreover, we applied Explainable Artificial\nIntelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and\nLocal Interpretable Model-Agnostic Explanations (LIME), to assess the impact of\nflooding on pavement performance. The results demonstrate that flood-affected\npavements experience a more rapid increase in roughness compared to non-flooded\nsections. These findings emphasize the need for proactive flood mitigation\nstrategies, including improved drainage systems, flood-resistant materials, and\npreventative maintenance, to enhance pavement resilience in vulnerable regions.", "AI": {"tldr": "The study examines how flooding accelerates pavement roughness (measured by IRI) using 20 years of TxDOT data and XAI techniques like SHAP and LIME. Flooded pavements degrade faster, highlighting the need for mitigation strategies.", "motivation": "Flooding causes immediate and long-term damage to pavements, but its specific impact on roughness (IRI) is not well quantified.", "method": "Used 20 years of TxDOT PMIS data combined with flood event details. Applied statistical analysis and XAI (SHAP, LIME) to assess flood impact on IRI.", "result": "Flood-exposed pavements show faster roughness increase than non-flooded ones.", "conclusion": "Proactive flood mitigation (better drainage, resistant materials, maintenance) is crucial for pavement resilience in flood-prone areas."}}
{"id": "2507.01057", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2507.01057", "abs": "https://arxiv.org/abs/2507.01057", "authors": ["Lushun Fan", "Yuqin Xia", "Jun Li", "Karl Jenkins"], "title": "Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates", "comment": null, "summary": "In this study, an innovative intelligent optimization system for mesh quality\nis proposed, which is based on a deep convolutional neural network\narchitecture, to achieve mesh generation and optimization. The core of the\nstudy is the Loop2Net generator and loss function, it predicts the mesh based\non the given wing coordinates. And the model's performance is continuously\noptimised by two key loss functions during the training. Then discipline by\nadding penalties, the goal of mesh generation was finally reached.", "AI": {"tldr": "An intelligent optimization system using deep CNN for mesh quality, featuring Loop2Net generator and loss functions to predict and optimize mesh generation.", "motivation": "To improve mesh generation and optimization for given wing coordinates using deep learning techniques.", "method": "Utilizes a deep convolutional neural network (CNN) with Loop2Net generator and two key loss functions for training and optimization.", "result": "The system successfully predicts and optimizes mesh generation, achieving the goal through disciplined penalties.", "conclusion": "The proposed system effectively enhances mesh quality and generation for specified coordinates."}}
{"id": "2507.01067", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01067", "abs": "https://arxiv.org/abs/2507.01067", "authors": ["Keun Soo Yim"], "title": "Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services", "comment": null, "summary": "Time series forecasting models have diverse real world applications (e.g.,\nfrom electricity metrics to software workload). Latest foundational models\ntrained for time series forecasting show strengths (e.g., for long sequences\nand in zero-shot settings). However, foundational model was not yet used for\nforecasting rare, spiky events, i.e., a challenging target because those are a\ncorner case of extreme events. In this paper, we optimize a state-of-the-art\nfoundational model to forecast sporadic or spiky production outages of\nhigh-performance machine learning services powering billions of client devices.\nWe evaluate the forecasting errors of the foundational model compared with\nclassical stochastic forecasting models (e.g., moving average and\nautoregressive). The analysis helps us understand how each of the evaluated\nmodels performs for the sporadic or spiky events. For example, it identifies\nthe key patterns in the target data that are well tracked by the foundational\nmodel vs. each of the stochastic models. We use the models with optimal\nparameters to estimate a year-long outage statistics of a particular root cause\nwith less than 6% value errors.", "AI": {"tldr": "The paper explores optimizing a foundational model for forecasting rare, spiky events in high-performance ML services, comparing it with classical stochastic models and achieving <6% error in outage predictions.", "motivation": "To address the challenge of forecasting rare, spiky events (e.g., production outages) where foundational models haven't been applied, despite their strengths in other time series tasks.", "method": "Optimize a state-of-the-art foundational model for sporadic events and compare its forecasting errors with classical stochastic models (e.g., moving average, autoregressive).", "result": "Foundational model outperforms classical models for spiky events, with <6% error in year-long outage predictions for specific root causes.", "conclusion": "Foundational models can be effectively adapted for rare, spiky event forecasting, offering better accuracy than traditional methods."}}
{"id": "2507.01068", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01068", "abs": "https://arxiv.org/abs/2507.01068", "authors": ["Biplov Paneru"], "title": "Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors", "comment": null, "summary": "This study leverages an Inertial Measurement Unit (IMU) dataset to develop\nexplainable AI methods for the early detection and prediction of Freezing of\nGait (FOG), a common symptom in Parkinson's disease. Machine learning models,\nincluding CatBoost, XGBoost, and Extra Trees classifiers, are employed to\naccurately categorize FOG episodes based on relevant clinical features. A\nStacking Ensemble model achieves superior performance, surpassing a hybrid\nbidirectional GRU model and reaching nearly 99% classification accuracy. SHAP\ninterpretability analysis reveals that time (seconds) is the most influential\nfactor in distinguishing gait patterns. Additionally, the proposed FOG\nprediction framework incorporates federated learning, where models are trained\nlocally on individual devices and aggregated on a central server using a\nfederated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for\nenhanced predictive capability.", "AI": {"tldr": "The paper develops explainable AI methods using IMU data to predict Freezing of Gait (FOG) in Parkinson's disease, achieving 99% accuracy with a Stacking Ensemble model and incorporating federated learning for decentralized training.", "motivation": "To improve early detection and prediction of FOG in Parkinson's disease using explainable AI and federated learning.", "method": "Uses CatBoost, XGBoost, and Extra Trees classifiers, with a Stacking Ensemble model outperforming a hybrid bidirectional GRU. SHAP analysis identifies key features, and federated learning with a hybrid Conv1D + LSTM architecture is employed.", "result": "The Stacking Ensemble model achieves nearly 99% classification accuracy, with time (seconds) as the most influential feature.", "conclusion": "The proposed framework effectively predicts FOG with high accuracy and interpretability, while federated learning enhances scalability and privacy."}}
{"id": "2507.01073", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.01073", "abs": "https://arxiv.org/abs/2507.01073", "authors": ["Dian Jin"], "title": "Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs", "comment": null, "summary": "Graph neural networks (GNNs) have achieved remarkable success in molecular\nproperty prediction. However, traditional graph representations struggle to\neffectively encode the inherent 3D spatial structures of molecules, as\nmolecular orientations in 3D space introduce significant variability, severely\nlimiting model generalization and robustness. Existing approaches primarily\nfocus on rotation-invariant and rotation-equivariant methods. Invariant methods\noften rely heavily on prior knowledge and lack sufficient generalizability,\nwhile equivariant methods suffer from high computational costs. To address\nthese limitations, this paper proposes a novel plug-and-play 3D encoding module\nleveraging rotational sampling. By computing the expectation over the SO(3)\nrotational group, the method naturally achieves approximate rotational\ninvariance. Furthermore, by introducing a carefully designed post-alignment\nstrategy, strict invariance can be achieved without compromising performance.\nExperimental evaluations on the QM9 and C10 Datasets demonstrate superior\npredictive accuracy, robustness, and generalization performance compared to\nexisting methods. Moreover, the proposed approach maintains low computational\ncomplexity and enhanced interpretability, providing a promising direction for\nefficient and effective handling of 3D molecular information in drug discovery\nand material design.", "AI": {"tldr": "A novel 3D encoding module for GNNs using rotational sampling achieves rotational invariance and improves molecular property prediction.", "motivation": "Traditional GNNs struggle with 3D molecular structures due to variability in orientations, limiting generalization and robustness. Existing methods are either inflexible or computationally expensive.", "method": "Proposes a plug-and-play 3D encoding module using rotational sampling and SO(3) group expectation, with a post-alignment strategy for strict invariance.", "result": "Outperforms existing methods on QM9 and C10 datasets in accuracy, robustness, and generalization, with low computational cost.", "conclusion": "The method offers an efficient, interpretable solution for 3D molecular data in drug discovery and material design."}}
{"id": "2507.01075", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.01075", "abs": "https://arxiv.org/abs/2507.01075", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "Provenance Tracking in Large-Scale Machine Learning Systems", "comment": null, "summary": "As the demand for large scale AI models continues to grow, the optimization\nof their training to balance computational efficiency, execution time, accuracy\nand energy consumption represents a critical multidimensional challenge.\nAchieving this balance requires not only innovative algorithmic techniques and\nhardware architectures but also comprehensive tools for monitoring, analyzing,\nand understanding the underlying processes involved in model training and\ndeployment. Provenance data information about the origins, context, and\ntransformations of data and processes has become a key component in this\npursuit. By leveraging provenance, researchers and engineers can gain insights\ninto resource usage patterns, identify inefficiencies, and ensure\nreproducibility and accountability in AI development workflows. For this\nreason, the question of how distributed resources can be optimally utilized to\nscale large AI models in an energy efficient manner is a fundamental one. To\nsupport this effort, we introduce the yProv4ML library, a tool designed to\ncollect provenance data in JSON format, compliant with the W3C PROV and ProvML\nstandards. yProv4ML focuses on flexibility and extensibility, and enables users\nto integrate additional data collection tools via plugins. The library is fully\nintegrated with the yProv framework, allowing for higher level pairing in tasks\nrun also through workflow management systems.", "AI": {"tldr": "The paper introduces yProv4ML, a library for collecting provenance data in AI model training to optimize efficiency, accuracy, and energy use.", "motivation": "The growing demand for large-scale AI models necessitates tools to balance computational efficiency, execution time, accuracy, and energy consumption, requiring insights into training processes.", "method": "The yProv4ML library collects provenance data in JSON format, adhering to W3C PROV and ProvML standards, and supports extensibility via plugins.", "result": "The library enables monitoring and analysis of resource usage, inefficiencies, and ensures reproducibility in AI workflows.", "conclusion": "yProv4ML provides a flexible and extensible solution for optimizing distributed resource usage in large AI model training."}}
{"id": "2507.01196", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01196", "abs": "https://arxiv.org/abs/2507.01196", "authors": ["Na Lee", "Konstantinos Barmpas", "Yannis Panagakis", "Dimitrios Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "comment": null, "summary": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis.", "AI": {"tldr": "LBMs show marginal gains over traditional methods in BCI tasks but with higher computational costs. LoRA helps reduce parameters without losing performance, indicating inefficiencies in current LBM designs.", "motivation": "To evaluate the effectiveness of Large Brainwave Foundation Models (LBMs) in brainwave modeling and BCI tasks, given their unclear capabilities in this domain.", "method": "Systematic fine-tuning experiments on BCI benchmarks, including memory tasks and sleep stage classification, using full model fine-tuning and LoRA for parameter-efficient adaptation.", "result": "LBMs achieve only slight improvements (0.9%-1.2%) over traditional methods but require significantly more parameters. LoRA reduces parameters without performance loss.", "conclusion": "Current LBMs are inefficient for BCI tasks and may need redesign. Domain-specific strategies are crucial to unlock their potential in brainwave analysis."}}
{"id": "2507.01077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01077", "abs": "https://arxiv.org/abs/2507.01077", "authors": ["Bogdan Bogdan", "Arina Cazacu", "Laura Vasilie"], "title": "Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels", "comment": "6 pages, 7 figures, 4 tables, accepted to IEEE Intelligent Vehicles\n  Symposium (IV) 2025", "summary": "Anomaly detection often relies on supervised or clustering approaches, with\nlimited success in specialized domains like automotive communication systems\nwhere scalable solutions are essential. We propose a novel decoder-only Large\nLanguage Model (LLM) to detect anomalies in Electronic Control Unit (ECU)\ncommunication logs. Our approach addresses two key challenges: the lack of LLMs\ntailored for ECU communication and the complexity of inconsistent ground truth\ndata. By learning from UDP communication logs, we formulate anomaly detection\nsimply as identifying deviations in time from normal behavior. We introduce an\nentropy regularization technique that increases model's uncertainty in known\nanomalies while maintaining consistency in similar scenarios. Our solution\noffers three novelties: a decoder-only anomaly detection architecture, a way to\nhandle inconsistent labeling, and an adaptable LLM for different ECU\ncommunication use cases. By leveraging the generative capabilities of\ndecoder-only models, we present a new technique that addresses the high cost\nand error-prone nature of manual labeling through a more scalable system that\nis able to learn from a minimal set of examples, while improving detection\naccuracy in complex communication environments.", "AI": {"tldr": "A decoder-only LLM is proposed for anomaly detection in ECU communication logs, addressing challenges like lack of tailored LLMs and inconsistent ground truth data. It uses entropy regularization and generative capabilities for scalable, accurate detection.", "motivation": "Specialized domains like automotive communication systems lack scalable anomaly detection solutions, and existing methods struggle with inconsistent labeling and tailored models.", "method": "A decoder-only LLM learns from UDP communication logs, using entropy regularization to handle inconsistent labels and detect anomalies as deviations from normal behavior.", "result": "The approach introduces a novel architecture, handles inconsistent labeling, and adapts to various ECU use cases, improving accuracy and scalability.", "conclusion": "The proposed LLM offers a scalable, accurate solution for anomaly detection in ECU communication, reducing reliance on manual labeling and addressing domain-specific challenges."}}
{"id": "2507.01241", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01241", "abs": "https://arxiv.org/abs/2507.01241", "authors": ["Di Zhang", "Yihang Zhang"], "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW", "comment": null, "summary": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process.", "AI": {"tldr": "A novel stochastic conjugate subgradient method with adaptive sampling is proposed for training large language models (LLMs), outperforming traditional SGD in convergence and scalability.", "motivation": "Traditional SGD methods show performance limitations in large-scale LLM training, prompting the need for more efficient optimization techniques.", "method": "The proposed method combines stochastic conjugate subgradients, adaptive sampling, and AdamW-like step size adjustment to handle nonconvexity and non-smoothness in LLMs.", "result": "The method achieves faster convergence, improved scalability, and better optimization speed and accuracy compared to SGD.", "conclusion": "The new approach effectively addresses SGD's limitations, offering superior performance for LLM training."}}
{"id": "2507.01078", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.01078", "abs": "https://arxiv.org/abs/2507.01078", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "yProv4ML: Effortless Provenance Tracking for Machine Learning Systems", "comment": null, "summary": "The rapid growth of interest in large language models (LLMs) reflects their\npotential for flexibility and generalization, and attracted the attention of a\ndiverse range of researchers. However, the advent of these techniques has also\nbrought to light the lack of transparency and rigor with which development is\npursued. In particular, the inability to determine the number of epochs and\nother hyperparameters in advance presents challenges in identifying the best\nmodel. To address this challenge, machine learning frameworks such as MLFlow\ncan automate the collection of this type of information. However, these tools\ncapture data using proprietary formats and pose little attention to lineage.\nThis paper proposes yProv4ML, a framework to capture provenance information\ngenerated during machine learning processes in PROV-JSON format, with minimal\ncode modifications.", "AI": {"tldr": "The paper introduces yProv4ML, a framework for capturing provenance data in machine learning processes using PROV-JSON format to address transparency and rigor issues in LLM development.", "motivation": "The lack of transparency and rigor in large language model (LLM) development, especially in tracking hyperparameters and epochs, necessitates better provenance tracking tools.", "method": "The paper proposes yProv4ML, a framework that captures provenance information during ML processes in PROV-JSON format with minimal code changes.", "result": "yProv4ML provides a standardized way to track ML process data, addressing the limitations of proprietary formats and lineage neglect in existing tools like MLFlow.", "conclusion": "yProv4ML enhances transparency and rigor in ML development by enabling provenance tracking in a standardized format."}}
{"id": "2507.01271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01271", "abs": "https://arxiv.org/abs/2507.01271", "authors": ["Tatsuki Kawakami", "Kazuki Egashira", "Atsuyuki Miyai", "Go Irie", "Kiyoharu Aizawa"], "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning", "comment": null, "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.", "AI": {"tldr": "The paper introduces PULSE, a protocol for evaluating unlearning in large multimodal models (LMMs), addressing gaps in existing benchmarks by focusing on pre-trained knowledge unlearning and long-term sustainability.", "motivation": "Existing unlearning benchmarks for LMMs lack realism, focusing only on single-operation fine-tuned knowledge unlearning. This study aims to address this gap.", "method": "The PULSE protocol evaluates unlearning in LMMs by considering pre-trained knowledge unlearning and sequential unlearning requests.", "result": "Current unlearning methods struggle with pre-trained knowledge and sequential unlearning, showing performance degradation.", "conclusion": "The study highlights the need for more robust unlearning techniques in LMMs, particularly for pre-trained knowledge and sequential scenarios."}}
{"id": "2507.01080", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01080", "abs": "https://arxiv.org/abs/2507.01080", "authors": ["Edouard Lansiaux", "Ramy Azzouz", "Emmanuel Chazard", "Am\u00e9lie Vromant", "Eric Wiel"], "title": "Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept", "comment": "15 pages, 6 figures", "summary": "Triage errors, including undertriage and overtriage, are persistent\nchallenges in emergency departments (EDs). With increasing patient influx and\nstaff shortages, the integration of artificial intelligence (AI) into triage\nprotocols has gained attention. This study compares the performance of three AI\nmodels [Natural Language Processing (NLP), Large Language Models (LLM), and\nJoint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes\nagainst the FRENCH scale and clinical practice.We conducted a retrospective\nanalysis of a prospectively recruited cohort gathering adult patient triage\ndata over a 7-month period at the Roger Salengro Hospital ED (Lille, France).\nThree AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)\nURGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic\ndetails, verbatim chief complaints, vital signs, and triage outcomes based on\nthe FRENCH scale and GEMSA coding. The primary outcome was the concordance of\nAI-predicted triage level with the FRENCH gold-standard. It was assessed thanks\nto various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM\nmodel (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared\nto JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse\ntriage (-4.343). Secondary analyses highlighted the effectiveness of\nURGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness\nwith structured data versus raw transcripts (either for GEMSA prediction or for\nFRENCH prediction). LLM architecture, through abstraction of patient\nrepresentations, offers the most accurate triage predictions among tested\nmodels. Integrating AI into ED workflows could enhance patient safety and\noperational efficiency, though integration into clinical workflows requires\naddressing model limitations and ensuring ethical transparency.", "AI": {"tldr": "The study compares three AI models (NLP, LLM, JEPA) for ED triage, finding LLM (URGENTIAPARSE) most accurate, outperforming nurses and other models.", "motivation": "To address triage errors (undertriage/overtriage) in EDs by evaluating AI models' performance against traditional methods.", "method": "Retrospective analysis of patient data using three AI models (NLP, LLM, JEPA) trained on demographic, complaint, and vital sign data, validated against the FRENCH scale.", "result": "LLM (URGENTIAPARSE) achieved the highest accuracy (composite score: 2.514), outperforming JEPA (0.438), NLP (-3.511), and nurse triage (-4.343).", "conclusion": "LLMs show promise for improving ED triage accuracy, but integration requires addressing limitations and ethical concerns."}}
{"id": "2507.01313", "categories": ["cs.LG", "cs.AI", "math.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.01313", "abs": "https://arxiv.org/abs/2507.01313", "authors": ["Qian Qi"], "title": "Neural Hamiltonian Operator", "comment": null, "summary": "Stochastic control problems in high dimensions are notoriously difficult to\nsolve due to the curse of dimensionality. An alternative to traditional dynamic\nprogramming is Pontryagin's Maximum Principle (PMP), which recasts the problem\nas a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In\nthis paper, we introduce a formal framework for solving such problems with deep\nlearning by defining a \\textbf{Neural Hamiltonian Operator (NHO)}. This\noperator parameterizes the coupled FBSDE dynamics via neural networks that\nrepresent the feedback control and an ansatz for the value function's spatial\ngradient. We show how the optimal NHO can be found by training the underlying\nnetworks to enforce the consistency conditions dictated by the PMP. By adopting\nthis operator-theoretic view, we situate the deep FBSDE method within the\nrigorous language of statistical inference, framing it as a problem of learning\nan unknown operator from simulated data. This perspective allows us to prove\nthe universal approximation capabilities of NHOs under general martingale\ndrivers and provides a clear lens for analyzing the significant optimization\nchallenges inherent to this class of models.", "AI": {"tldr": "The paper introduces a Neural Hamiltonian Operator (NHO) to solve high-dimensional stochastic control problems using deep learning, framed as learning an operator from simulated data.", "motivation": "Addressing the curse of dimensionality in stochastic control problems by leveraging Pontryagin's Maximum Principle (PMP) and deep learning.", "method": "Defines NHO to parameterize FBSDE dynamics via neural networks, training them to enforce PMP consistency conditions.", "result": "Demonstrates universal approximation capabilities of NHOs under general martingale drivers.", "conclusion": "The NHO framework provides a rigorous, operator-theoretic approach to solving high-dimensional stochastic control problems with deep learning."}}
{"id": "2507.01098", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01098", "abs": "https://arxiv.org/abs/2507.01098", "authors": ["Liu Ziyin", "Isaac Chuang"], "title": "Proof of a perfect platonic representation hypothesis", "comment": null, "summary": "In this note, we elaborate on and explain in detail the proof given by Ziyin\net al. (2025) of the \"perfect\" Platonic Representation Hypothesis (PRH) for the\nembedded deep linear network model (EDLN). We show that if trained with SGD,\ntwo EDLNs with different widths and depths and trained on different data will\nbecome Perfectly Platonic, meaning that every possible pair of layers will\nlearn the same representation up to a rotation. Because most of the global\nminima of the loss function are not Platonic, that SGD only finds the perfectly\nPlatonic solution is rather extraordinary. The proof also suggests at least six\nways the PRH can be broken. We also show that in the EDLN model, the emergence\nof the Platonic representations is due to the same reason as the emergence of\nprogressive sharpening. This implies that these two seemingly unrelated\nphenomena in deep learning can, surprisingly, have a common cause. Overall, the\ntheory and proof highlight the importance of understanding emergent \"entropic\nforces\" due to the irreversibility of SGD training and their role in\nrepresentation learning. The goal of this note is to be instructive and avoid\nlengthy technical details.", "AI": {"tldr": "The note explains Ziyin et al.'s proof of the 'perfect' Platonic Representation Hypothesis (PRH) for embedded deep linear networks (EDLN), showing SGD leads to identical layer representations up to rotation, despite most global minima not being Platonic. It also links PRH to progressive sharpening and identifies six ways PRH can fail.", "motivation": "To clarify and detail Ziyin et al.'s proof of PRH in EDLNs, emphasizing the surprising role of SGD in achieving Platonic representations and exploring connections to other deep learning phenomena.", "method": "Analyzes the proof for PRH in EDLNs trained with SGD, demonstrating how layers converge to identical representations up to rotation, and identifies conditions under which PRH fails.", "result": "SGD uniquely finds perfectly Platonic solutions in EDLNs, despite most global minima not being Platonic. PRH's emergence is linked to progressive sharpening, suggesting a common cause. Six ways to break PRH are identified.", "conclusion": "The note underscores the significance of emergent 'entropic forces' in SGD-driven representation learning, revealing unexpected connections between seemingly unrelated deep learning phenomena."}}
{"id": "2507.01321", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01321", "abs": "https://arxiv.org/abs/2507.01321", "authors": ["Zhiyao Ren", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks", "comment": "ICML 2025", "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4).", "AI": {"tldr": "The paper introduces the dual-learning hypothesis to explain LLMs' vulnerability to backdoor attacks in ICL and proposes ICLShield, a defense mechanism that dynamically adjusts concept preference ratios to mitigate attacks.", "motivation": "ICL's adaptability in LLMs makes it vulnerable to backdoor attacks, where adversaries manipulate model behavior via poisoned demonstrations.", "method": "Proposes the dual-learning hypothesis and ICLShield, which adjusts concept preference ratios using confidence and similarity scores to select clean demonstrations.", "result": "ICLShield outperforms existing methods by 26.02% on average and shows strong adaptability, even with closed-source models like GPT-4.", "conclusion": "The dual-learning hypothesis and ICLShield provide a robust defense against ICL backdoor attacks, enhancing LLM security."}}
{"id": "2507.01117", "categories": ["cs.LG", "68T07, 35A99"], "pdf": "https://arxiv.org/pdf/2507.01117", "abs": "https://arxiv.org/abs/2507.01117", "authors": ["Nikita Sakovich", "Dmitry Aksenov", "Ekaterina Pleshakova", "Sergey Gataullin"], "title": "A Neural Operator based on Dynamic Mode Decomposition", "comment": "30 pages, 10 figures", "summary": "The scientific computation methods development in conjunction with artificial\nintelligence technologies remains a hot research topic. Finding a balance\nbetween lightweight and accurate computations is a solid foundation for this\ndirection. The study presents a neural operator based on the dynamic mode\ndecomposition algorithm (DMD), mapping functional spaces, which combines DMD\nand deep learning (DL) for spatiotemporal processes efficient modeling. Solving\nPDEs for various initial and boundary conditions requires significant\ncomputational resources. The method suggested automatically extracts key modes\nand system dynamics using them to construct predictions, reducing computational\ncosts compared to traditional numerical methods. The approach has demonstrated\nits efficiency through comparative analysis of performance with closest\nanalogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers\nequation solutions approximation, where it achieves high reconstruction\naccuracy.", "AI": {"tldr": "A neural operator combining DMD and DL for efficient spatiotemporal modeling, reducing computational costs while maintaining accuracy.", "motivation": "Balancing lightweight and accurate computations in scientific AI, especially for solving PDEs with high resource demands.", "method": "Uses dynamic mode decomposition (DMD) and deep learning to map functional spaces, extracting key modes and dynamics for predictions.", "result": "Outperforms DeepONet and FNO in accuracy for heat, Laplace, and Burgers equations with lower computational costs.", "conclusion": "The DMD-based neural operator is efficient for spatiotemporal modeling, offering a lightweight yet accurate alternative to traditional methods."}}
{"id": "2507.01327", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits.", "AI": {"tldr": "The paper introduces APARL, a framework for detecting abnormal events in customer service dialogues, enhancing adaptability and OOD generalization with a dual-loop dynamic curriculum learning approach.", "motivation": "Detecting abnormal events in customer service dialogues is complex due to dynamic interactions and diverse business scenarios, requiring strong OOD generalization for commercial value.", "method": "Proposes APARL, leveraging large language models with a dual-loop dynamic curriculum learning architecture to progressively tackle challenging samples.", "result": "Achieves significant improvements: 17.19% higher F1 score and 9.59% better OOD transfer performance in food delivery dialogue tasks.", "conclusion": "APARL offers a superior solution for industrial anomaly detection, boosting operational efficiency and commercial benefits."}}
{"id": "2507.01129", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01129", "abs": "https://arxiv.org/abs/2507.01129", "authors": ["Arun Ganesh", "Brendan McMahan", "Abhradeep Thakurta"], "title": "On Design Principles for Private Adaptive Optimizers", "comment": "PPML 2025", "summary": "The spherical noise added to gradients in differentially private (DP)\ntraining undermines the performance of adaptive optimizers like AdaGrad and\nAdam, and hence many recent works have proposed algorithms to address this\nchallenge. However, the empirical results in these works focus on simple tasks\nand models and the conclusions may not generalize to model training in\npractice. In this paper we survey several of these variants, and develop better\ntheoretical intuition for them as well as perform empirical studies comparing\nthem. We find that a common intuition of aiming for unbiased estimates of\nsecond moments of gradients in adaptive optimizers is misguided, and instead\nthat a simple technique called scale-then-privatize (which does not achieve\nunbiased second moments) has more desirable theoretical behaviors and\noutperforms all other variants we study on a small-scale language model\ntraining task. We additionally argue that scale-then-privatize causes the noise\naddition to better match the application of correlated noise mechanisms which\nare more desirable to use in practice.", "AI": {"tldr": "The paper critiques DP training methods, finding that unbiased second-moment estimates in adaptive optimizers are less effective than the scale-then-privatize technique, which performs better empirically.", "motivation": "Address the performance gap in DP training caused by spherical noise in adaptive optimizers like AdaGrad and Adam.", "method": "Survey and compare variants of DP training methods, focusing on theoretical intuition and empirical studies.", "result": "Scale-then-privatize outperforms other methods, despite not providing unbiased second-moment estimates, and aligns better with correlated noise mechanisms.", "conclusion": "The scale-then-privatize technique is more effective and theoretically sound for DP training, challenging common intuitions about unbiased estimates."}}
{"id": "2507.01131", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.01131", "abs": "https://arxiv.org/abs/2507.01131", "authors": ["Yuchao Lin", "Cong Fu", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations", "comment": null, "summary": "$\\rm{SO}(3)$-equivariant networks are the dominant models for machine\nlearning interatomic potentials (MLIPs). The key operation of such networks is\nthe Clebsch-Gordan (CG) tensor product, which is computationally expensive. To\naccelerate the computation, we develop tensor decomposition networks (TDNs) as\na class of approximately equivariant networks whose CG tensor products are\nreplaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)\ndecomposition. With the CP decomposition, we prove (i) a uniform bound on the\ninduced error of $\\rm{SO}(3)$-equivariance, and (ii) the universality of\napproximating any equivariant bilinear map. To further reduce the number of\nparameters, we propose path-weight sharing that ties all multiplicity-space\nweights across the $O(L^3)$ CG paths into a single path without compromising\nequivariance, where $L$ is the maximum angular degree. The resulting layer acts\nas a plug-and-play replacement for tensor products in existing networks, and\nthe computational complexity of tensor products is reduced from $O(L^6)$ to\n$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation\ndataset containing 105 million DFT-calculated snapshots. We also use existing\ndatasets, including OC20, and OC22. Results show that TDNs achieve competitive\nperformance with dramatic speedup in computations.", "AI": {"tldr": "TDNs replace CG tensor products with low-rank decompositions for faster, approximately equivariant MLIPs, achieving competitive performance with reduced computational cost.", "motivation": "To address the computational expense of CG tensor products in SO(3)-equivariant networks for MLIPs.", "method": "Develop TDNs using low-rank tensor decompositions (e.g., CP) and path-weight sharing to reduce complexity.", "result": "TDNs achieve competitive performance on PubChemQCR, OC20, and OC22 datasets with significant speedup.", "conclusion": "TDNs offer a practical, efficient alternative to traditional SO(3)-equivariant networks for MLIPs."}}
{"id": "2507.01381", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories.", "AI": {"tldr": "DSAC-D, a distributional reinforcement learning algorithm, addresses bias in value function estimation and achieves multimodal policy representations, outperforming existing methods in control tasks.", "motivation": "Traditional unimodal distributions in reinforcement learning cause bias in value function estimation, leading to poor performance.", "method": "DSAC-D introduces policy entropy and a value distribution function, using a diffusion model for reverse sampling to create a multimodal distributional policy iteration framework.", "result": "The algorithm achieves SOTA performance in 9 control tasks, reducing bias and improving returns by over 10%. Real-world tests confirm its ability to model multimodal driving styles.", "conclusion": "DSAC-D effectively addresses bias and enables multimodal policy learning, demonstrating superior performance in both simulated and real-world tasks."}}
{"id": "2507.01132", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2507.01132", "abs": "https://arxiv.org/abs/2507.01132", "authors": ["Brenda Nogueira", "Gabe Gomes", "Meng Jiang", "Nitesh V. Chawla", "Nuno Moniz"], "title": "Spectral Manifold Harmonization for Graph Imbalanced Regression", "comment": null, "summary": "Graph-structured data is ubiquitous in scientific domains, where models often\nface imbalanced learning settings. In imbalanced regression, domain preferences\nfocus on specific target value ranges representing the most scientifically\nvaluable cases; we observe a significant lack of research. In this paper, we\npresent Spectral Manifold Harmonization (SMH), a novel approach for addressing\nthis imbalanced regression challenge on graph-structured data by generating\nsynthetic graph samples that preserve topological properties while focusing on\noften underrepresented target distribution regions. Conventional methods fail\nin this context because they either ignore graph topology in case generation or\ndo not target specific domain ranges, resulting in models biased toward average\ntarget values. Experimental results demonstrate the potential of SMH on\nchemistry and drug discovery benchmark datasets, showing consistent\nimprovements in predictive performance for target domain ranges.", "AI": {"tldr": "SMH addresses imbalanced regression on graph-structured data by generating synthetic samples preserving topology and focusing on underrepresented target ranges, improving predictive performance.", "motivation": "Imbalanced regression in graph-structured data lacks research, especially for scientifically valuable target ranges. Conventional methods ignore topology or fail to target specific ranges, leading to biased models.", "method": "Spectral Manifold Harmonization (SMH) generates synthetic graph samples preserving topological properties and targeting underrepresented target distribution regions.", "result": "SMH shows consistent improvements in predictive performance for target domain ranges on chemistry and drug discovery benchmarks.", "conclusion": "SMH effectively addresses imbalanced regression in graph-structured data by harmonizing topology and target distribution focus, outperforming conventional methods."}}
{"id": "2507.01457", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.01457", "abs": "https://arxiv.org/abs/2507.01457", "authors": ["Federico Nicolas Peccia", "Frederik Haxel", "Oliver Bringmann"], "title": "Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs", "comment": "9 pages, 10 figures, 2 algorithms", "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions.", "AI": {"tldr": "The paper introduces a TVM compiler-based workflow to optimize AI workloads for RISC-V Vector Extension (RVV), outperforming autovectorization and hand-crafted libraries in efficiency and code size.", "motivation": "Efficiently deploying AI workloads on RISC-V RVV without expert knowledge is challenging due to lack of autotuning frameworks.", "method": "Integrated RVV into TVM's MetaSchedule framework, tested on FPGA-implemented RISC-V SoCs and a commercial SoC.", "result": "46% faster than GCC autovectorization, 29% faster than muRISCV-NN, smaller code footprint, and 35% faster than LLVM on a commercial SoC.", "conclusion": "The proposed workflow effectively optimizes AI workloads for RISC-V RVV, with open-sourced code for community expansion."}}
{"id": "2507.01154", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01154", "abs": "https://arxiv.org/abs/2507.01154", "authors": ["Liangyu Wang", "Junxiao Wang", "Jie Ren", "Zihang Xiang", "David E. Keyes", "Di Wang"], "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD", "comment": null, "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.", "AI": {"tldr": "FlashDP introduces a cache-friendly per-layer DP-SGD method to reduce memory and computational overhead in differentially private training of LLMs.", "motivation": "Addressing the inefficiencies and high memory demands of existing DP-SGD methods like Opacus and GhostClip.", "method": "FlashDP consolidates operations into a single task, calculating gradients once in a fused manner, reducing memory movement and redundant computations.", "result": "FlashDP reduces memory movement by 50% and redundant computations by 20%, achieving 90% throughput of Non-DP methods while maintaining accuracy.", "conclusion": "FlashDP is a significant advancement for efficient and privacy-preserving LLM training, with open-sourced code available."}}
{"id": "2507.01470", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives.", "AI": {"tldr": "The paper challenges the assumption that reward frequency measures task difficulty in reinforcement learning, identifying zero-incentive dynamics as a key issue where critical subgoals lack rewards.", "motivation": "To address the limitation of current methods that fail when essential subgoals don't yield direct rewards.", "method": "Analyzes zero-incentive dynamics and evaluates state-of-the-art deep subgoal-based algorithms.", "result": "Shows these algorithms struggle with unrewarded critical transitions and are sensitive to reward timing.", "conclusion": "Highlights the need for new mechanisms to infer latent task structure beyond immediate rewards."}}
{"id": "2507.01178", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01178", "abs": "https://arxiv.org/abs/2507.01178", "authors": ["Alec Helbling", "Duen Horng Chau"], "title": "Diffusion Explorer: Interactive Exploration of Diffusion Models", "comment": null, "summary": "Diffusion models have been central to the development of recent image, video,\nand even text generation systems. They posses striking geometric properties\nthat can be faithfully portrayed in low-dimensional settings. However, existing\nresources for explaining diffusion either require an advanced theoretical\nfoundation or focus on their neural network architectures rather than their\nrich geometric properties. We introduce Diffusion Explorer, an interactive tool\nto explain the geometric properties of diffusion models. Users can train 2D\ndiffusion models in the browser and observe the temporal dynamics of their\nsampling process. Diffusion Explorer leverages interactive animation, which has\nbeen shown to be a powerful tool for making engaging visualizations of dynamic\nsystems, making it well suited to explaining diffusion models which represent\nstochastic processes that evolve over time. Diffusion Explorer is open source\nand a live demo is available at alechelbling.com/Diffusion-Explorer.", "AI": {"tldr": "Diffusion Explorer is an interactive tool designed to explain the geometric properties of diffusion models through 2D training and real-time observation, making the concept accessible without advanced theory.", "motivation": "Existing explanations of diffusion models are either too theoretical or overly focused on neural architectures, neglecting their geometric properties.", "method": "Developed Diffusion Explorer, a browser-based interactive tool with animations to visualize the temporal dynamics of 2D diffusion models.", "result": "Users can train and observe diffusion models in real-time, enhancing understanding of their stochastic and geometric nature.", "conclusion": "Diffusion Explorer successfully bridges the gap in accessible explanations of diffusion models' geometric properties, with an open-source demo available."}}
{"id": "2507.01522", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations.", "AI": {"tldr": "Chargax is a JAX-based environment for simulating electric vehicle charging stations, offering 100x-1000x faster training for RL agents compared to existing tools.", "motivation": "Addressing slow reinforcement learning training and high sample complexity in sustainable energy applications, particularly grid congestion and operational efficiency.", "method": "Developed Chargax, a JAX-based environment for realistic EV charging station simulations, validated with real data and compared RL agents to baselines.", "result": "Chargax achieves 100x-1000x computational performance improvements and supports diverse real-world charging station configurations.", "conclusion": "Chargax provides a scalable, efficient solution for training RL agents in sustainable energy applications, bridging the gap between toy problems and real-world scenarios."}}
{"id": "2507.01551", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.", "AI": {"tldr": "SPRO is a novel framework for process-aware reinforcement learning in LLMs, eliminating the need for external reward models and improving efficiency and accuracy.", "motivation": "Addressing the computational overhead and lack of theoretical framework for process-level advantage estimation in PRL.", "method": "SPRO derives process rewards intrinsically from the policy model and introduces cumulative process rewards and Masked Step Advantage (MSA).", "result": "SPRO outperforms GRPO with 3.4x higher training efficiency, 17.5% test accuracy improvement, and reduced response length.", "conclusion": "SPRO offers a computationally efficient, stable, and effective solution for process-aware RL in LLMs."}}
{"id": "2507.01201", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "AI": {"tldr": "The paper proposes the Joint Autoencoder Modulator (JAM) framework to align disjoint vision and language representations, optimizing for mutual coherence while preserving modality-specific structures.", "motivation": "The Platonic Representation Hypothesis suggests that independently trained vision and language models may converge toward a shared statistical model of reality, prompting the need for explicit alignment optimization.", "method": "The JAM framework jointly trains modality-specific autoencoders on pre-trained single-modality models, using reconstruction and cross-modal objectives to encourage alignment.", "result": "The framework reliably induces alignment across frozen, independently trained representations, with evaluations on alignment objectives, layer depth, and foundation model scale.", "conclusion": "JAM offers a lightweight, Pareto-efficient solution for transforming unimodal foundations into specialist multimodal models, bridging disjoint representations."}}
{"id": "2507.01649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01649", "abs": "https://arxiv.org/abs/2507.01649", "authors": ["Yoav Gelberg", "Yam Eitan", "Aviv Navon", "Aviv Shamsian", "Theo", "Putterman", "Michael Bronstein", "Haggai Maron"], "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients", "comment": null, "summary": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature.", "AI": {"tldr": "The paper introduces GradMetaNet, a novel architecture for processing gradients in neural networks, guided by principles of equivariant design, multi-point gradient processing, and efficient representation. It outperforms previous methods on tasks like optimization and model editing.", "motivation": "Gradients in neural networks are valuable for tasks like optimization and model analysis, but existing architectures for gradient processing are limited. This work aims to design a principled architecture for efficient and effective gradient processing.", "method": "The authors propose GradMetaNet, built on equivariant blocks, to process gradients while preserving symmetries and capturing curvature. It uses rank-1 decomposition for efficient representation.", "result": "GradMetaNet achieves universality in gradient-based functions and outperforms prior methods on tasks like learned optimization and loss landscape curvature estimation.", "conclusion": "GradMetaNet provides a robust framework for gradient processing, demonstrating superior performance across diverse tasks, and offers theoretical guarantees of universality."}}
{"id": "2507.01208", "categories": ["cs.LG", "cs.CR", "C.2.0; I.2.0"], "pdf": "https://arxiv.org/pdf/2507.01208", "abs": "https://arxiv.org/abs/2507.01208", "authors": ["Pedro R. X. Carmo", "Igor de Moura", "Assis T. de Oliveira Filho", "Djamel Sadok", "Cleber Zanchettin"], "title": "Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform", "comment": null, "summary": "Modern vehicles are increasingly connected, and in this context, automotive\nEthernet is one of the technologies that promise to provide the necessary\ninfrastructure for intra-vehicle communication. However, these systems are\nsubject to attacks that can compromise safety, including flow injection\nattacks. Deep Learning-based Intrusion Detection Systems (IDS) are often\ndesigned to combat this problem, but they require expensive hardware to run in\nreal time. In this work, we propose to evaluate and apply fast neural network\ninference techniques like Distilling and Prunning for deploying IDS models on\nlow-cost platforms in real time. The results show that these techniques can\nachieve intrusion detection times of up to 727 {\\mu}s using a Raspberry Pi 4,\nwith AUCROC values of 0.9890.", "AI": {"tldr": "The paper proposes using fast neural network techniques (Distilling and Pruning) to deploy Intrusion Detection Systems (IDS) on low-cost hardware like Raspberry Pi 4, achieving real-time performance with high accuracy.", "motivation": "Modern vehicles rely on automotive Ethernet, which is vulnerable to attacks like flow injection. Existing deep learning-based IDS require expensive hardware for real-time operation, limiting practicality.", "method": "The authors evaluate and apply Distilling and Pruning techniques to optimize IDS models for deployment on low-cost platforms (e.g., Raspberry Pi 4).", "result": "The optimized IDS achieves intrusion detection times of 727 \u00b5s on Raspberry Pi 4 with an AUCROC of 0.9890.", "conclusion": "Fast neural network techniques enable efficient, real-time IDS deployment on affordable hardware, enhancing automotive security."}}
{"id": "2507.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01663", "abs": "https://arxiv.org/abs/2507.01663", "authors": ["Zhenyu Han", "Ansheng You", "Haibo Wang", "Kui Luo", "Guang Yang", "Wenqi Shi", "Menglong Chen", "Sicheng Zhang", "Zeshun Lan", "Chunshi Deng", "Huazhong Ji", "Wenjie Liu", "Yu Huang", "Yixiang Zhang", "Chenyi Pan", "Jing Wang", "Xin Huang", "Chunsheng Li", "Jianping Wu"], "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training", "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.", "AI": {"tldr": "AsyncFlow is an asynchronous streaming RL framework for efficient post-training of LLMs, addressing scalability and resource issues in traditional RL frameworks.", "motivation": "Traditional RL frameworks for LLMs face scalability bottlenecks, complex dataflows, and tight coupling with training/inference engines, limiting flexibility and efficiency.", "method": "Proposes AsyncFlow with distributed data storage, fine-grained scheduling, and a producer-consumer workflow to minimize idleness and enable dynamic load balancing.", "result": "Achieves a 1.59x throughput improvement over state-of-the-art baselines.", "conclusion": "AsyncFlow offers modular, customizable, and efficient RL post-training, providing insights for future RL system designs."}}
{"id": "2507.01216", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01216", "abs": "https://arxiv.org/abs/2507.01216", "authors": ["Xingke Yang", "Liang Li", "Zhiyi Wan", "Sicong Li", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Tomoaki Ohtsuki", "Xin Fu", "Miao Pan"], "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning", "comment": null, "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.", "AI": {"tldr": "PAE MobiLLM is a privacy-aware, efficient method for fine-tuning large language models on mobile devices using server-assisted side-tuning, reducing communication costs and protecting data privacy.", "motivation": "Address the gap between mobile device limitations and the demand for on-device LLM fine-tuning, while mitigating communication burdens and privacy risks of existing server-assisted methods.", "method": "Uses server-assisted additive side-tuning with activation caching, a one-token activation shortcut, and additive adapter side-network design to enhance efficiency and privacy.", "result": "Reduces communication costs, accelerates fine-tuning convergence, and ensures data, label, and model privacy.", "conclusion": "PAE MobiLLM effectively balances efficiency and privacy for on-device LLM fine-tuning, making it practical for mobile applications."}}
{"id": "2507.01679", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01679", "abs": "https://arxiv.org/abs/2507.01679", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.", "AI": {"tldr": "Prefix-RFT is a hybrid approach combining SFT and RFT, outperforming both and integrating easily into existing frameworks.", "motivation": "To address the trade-offs between SFT (good at mimicking but poor generalization) and RFT (performance-sensitive but prone to unexpected behaviors).", "method": "Proposes Prefix-RFT, a hybrid method synergizing demonstration (SFT) and exploration (RFT), tested on mathematical reasoning problems.", "result": "Prefix-RFT outperforms standalone SFT and RFT, and parallel mixed-policy RFT methods, with robustness to data variations.", "conclusion": "Prefix-RFT harmonizes SFT and RFT effectively, suggesting a unified paradigm for future LLM post-training research."}}
{"id": "2507.01235", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.01235", "abs": "https://arxiv.org/abs/2507.01235", "authors": ["Bara Rababa", "Bilal Farooq"], "title": "Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling", "comment": "Proceedings of IEEE Intelligent Transportation Systems Conference,\n  2025", "summary": "Quantum computing has opened new opportunities to tackle complex machine\nlearning tasks, for instance, high-dimensional data representations commonly\nrequired in intelligent transportation systems. We explore quantum machine\nlearning to model complex skin conductance response (SCR) events that reflect\npedestrian stress in a virtual reality road crossing experiment. For this\npurpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature\nmap and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and\nan eight-qubit ZZ feature map, were developed on Pennylane. The dataset\nconsists of SCR measurements along with features such as the response amplitude\nand elapsed time, which have been categorized into amplitude-based classes. The\nQSVM achieved good training accuracy, but had an overfitting problem, showing a\nlow test accuracy of 45% and therefore impacting the reliability of the\nclassification model. The QNN model reached a higher test accuracy of 55%,\nmaking it a better classification model than the QSVM and the classic versions.", "AI": {"tldr": "Quantum machine learning models (QSVM and QNN) were tested for classifying pedestrian stress using SCR data. QNN outperformed QSVM and classical methods with 55% test accuracy.", "motivation": "To leverage quantum computing for modeling complex SCR events in pedestrian stress scenarios, addressing limitations of classical methods.", "method": "Developed QSVM and QNN models using an eight-qubit ZZ feature map on Pennylane, tested on SCR data with amplitude-based classes.", "result": "QSVM showed overfitting (45% test accuracy), while QNN achieved better performance (55% test accuracy).", "conclusion": "QNN is more reliable than QSVM and classical models for classifying pedestrian stress, though further improvements are needed."}}
{"id": "2507.01693", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01693", "abs": "https://arxiv.org/abs/2507.01693", "authors": ["Adrians Skapars", "Edoardo Manino", "Youcheng Sun", "Lucas C. Cordeiro"], "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs", "comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.", "AI": {"tldr": "SODA, a gradient-based algorithm, reconstructs exact inputs from LLM outputs for forensic analysis, outperforming existing methods but struggling with longer sequences.", "motivation": "To enable post-incident analysis and detect fake outputs by reconstructing the exact input from LLM outputs.", "method": "Formalizes input reconstruction as a discrete optimization problem, introduces SODA with gradient-based search, periodic restarts, and parameter decay.", "result": "Recovers 79.5% of shorter out-of-distribution inputs with no false positives; struggles with longer sequences (15+ tokens).", "conclusion": "Standard deployment practices may currently protect against malicious use of SODA, but it excels in forensic analysis of shorter inputs."}}
{"id": "2507.01700", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01700", "abs": "https://arxiv.org/abs/2507.01700", "authors": ["Andrea Piras", "Matteo Negro", "Ragib Ahsan", "David Arbour", "Elena Zheleva"], "title": "Relational Causal Discovery with Latent Confounders", "comment": "30 pages, 19 figures. Accepted for publication at the 41st Conference\n  on Uncertainty in Artificial Intelligence (UAI 2025). Andrea Piras and Matteo\n  Negro contributed equally to this work", "summary": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders.", "AI": {"tldr": "RelFCI is a causal discovery algorithm for relational data with latent confounders, addressing gaps in existing methods by combining FCI and RCD approaches.", "motivation": "Existing causal discovery methods fail for relational data with latent confounders, either assuming i.i.d. data or causal sufficiency.", "method": "RelFCI builds on FCI and RCD, introducing new graphical models for relational d-separation with latent confounders.", "result": "RelFCI effectively identifies correct causal structures in relational models with latent confounders.", "conclusion": "RelFCI fills a critical gap in causal discovery for relational data, offering sound and complete guarantees."}}
{"id": "2507.01285", "categories": ["cs.LG", "cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01285", "abs": "https://arxiv.org/abs/2507.01285", "authors": ["Aymen Rayane Khouas", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Sunil Aryal"], "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation", "comment": "17 pages, 5 figures", "summary": "Graph federated recommendation systems offer a privacy-preserving alternative\nto traditional centralized recommendation architectures, which often raise\nconcerns about data security. While federated learning enables personalized\nrecommendations without exposing raw user data, existing aggregation methods\noverlook the unique properties of user embeddings in this setting. Indeed,\ntraditional aggregation methods fail to account for their complexity and the\ncritical role of user similarity in recommendation effectiveness. Moreover,\nevolving user interactions require adaptive aggregation while preserving the\ninfluence of high-relevance anchor users (the primary users before expansion in\ngraph-based frameworks). To address these limitations, we introduce\nDist-FedAvg, a novel distance-based aggregation method designed to enhance\npersonalization and aggregation efficiency in graph federated learning. Our\nmethod assigns higher aggregation weights to users with similar embeddings,\nwhile ensuring that anchor users retain significant influence in local updates.\nEmpirical evaluations on multiple datasets demonstrate that Dist-FedAvg\nconsistently outperforms baseline aggregation techniques, improving\nrecommendation accuracy while maintaining seamless integration into existing\nfederated learning frameworks.", "AI": {"tldr": "Dist-FedAvg is a distance-based aggregation method for graph federated recommendation systems, improving personalization and efficiency by weighting similar user embeddings higher and preserving anchor user influence.", "motivation": "Traditional aggregation methods in federated learning overlook the complexity of user embeddings and the role of user similarity, limiting recommendation effectiveness.", "method": "Introduces Dist-FedAvg, which assigns higher weights to users with similar embeddings and ensures anchor users retain influence in local updates.", "result": "Empirical evaluations show Dist-FedAvg outperforms baseline methods, enhancing recommendation accuracy while integrating seamlessly into federated frameworks.", "conclusion": "Dist-FedAvg addresses limitations of existing aggregation methods, offering improved personalization and efficiency in graph federated recommendation systems."}}
{"id": "2507.01752", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01752", "abs": "https://arxiv.org/abs/2507.01752", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.", "AI": {"tldr": "BBoxER is an evolutionary black-box method for LLM post-training, addressing privacy and security concerns of gradient-based optimization while overcoming scalability and computational challenges of black-box methods.", "motivation": "Gradient-based optimization in deep learning raises privacy and security concerns (e.g., data poisoning, overfitting). Black-box methods offer an alternative but face scalability and computational issues, especially in high-dimensional spaces like LLMs.", "method": "BBoxER introduces an evolutionary black-box approach for LLM post-training, using implicit data compression to create an information bottleneck. It provides theoretical guarantees on generalization, privacy, and robustness.", "result": "Experiments show BBoxER improves LLM performance and generalization on reasoning benchmarks with minimal iterations, offering a lightweight, modular enhancement.", "conclusion": "BBoxER is a promising add-on to gradient-based optimization, suitable for privacy-sensitive environments, with strong theoretical and empirical support."}}
{"id": "2507.01761", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01761", "abs": "https://arxiv.org/abs/2507.01761", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "comment": null, "summary": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models.", "AI": {"tldr": "The paper introduces Clipped Density and Clipped Coverage, two novel metrics for evaluating generative models, addressing issues of fidelity, coverage, and robustness in existing methods.", "motivation": "Current quality metrics for generative models lack reliability and interpretability due to calibration and robustness issues, hindering their use in critical applications.", "method": "The authors propose Clipped Density and Clipped Coverage, which clip individual sample contributions and nearest neighbor radii to prevent bias from outliers.", "result": "The metrics show linear score degradation with poor samples and outperform existing methods in robustness, sensitivity, and interpretability.", "conclusion": "Clipped Density and Clipped Coverage provide reliable, interpretable evaluation of generative models, advancing their practical applicability."}}
{"id": "2507.01781", "categories": ["cs.LG", "cs.AI", "68T07 (Primary) 62H30, 68T05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.01781", "abs": "https://arxiv.org/abs/2507.01781", "authors": ["Dalia Rodr\u00edguez-Salas", "Christian Riess"], "title": "BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification", "comment": "18 pages, 3 figures (with two images each)", "summary": "We introduce BranchNet, a neuro-symbolic learning framework that transforms\ndecision tree ensembles into sparse, partially connected neural networks. Each\nbranch, defined as a decision path from root to a parent of leaves, is mapped\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\noptimization. The resulting models are compact, interpretable, and require no\nmanual architecture tuning. Evaluated on a suite of structured multi-class\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\naccuracy, with statistically significant gains. We detail the architecture,\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\nsymbolic interpretability as well as its current limitations, particularly on\nbinary tasks where further adaptive calibration may be beneficial.", "AI": {"tldr": "BranchNet converts decision tree ensembles into sparse neural networks, combining symbolic structure with gradient-based optimization for improved accuracy and interpretability.", "motivation": "To bridge the gap between symbolic (decision trees) and neural network approaches, enabling interpretability and optimization benefits.", "method": "Maps decision tree branches to hidden neurons, creating sparse neural networks without manual tuning.", "result": "Outperforms XGBoost in accuracy on multi-class benchmarks, with significant gains.", "conclusion": "BranchNet offers interpretable, compact models but may need adaptive calibration for binary tasks."}}
{"id": "2507.01806", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.", "AI": {"tldr": "A CPU-friendly method for LoRA fine-tuning of LLMs using pre-trained adapters, offering a practical alternative to GPU-based training.", "motivation": "To enable parameter-efficient fine-tuning of LLMs for users with limited computational resources, such as standard laptop CPUs.", "method": "Learns a meta-operator to map input datasets to LoRA weights by combining pre-trained adapters, avoiding gradient-based updates.", "result": "Adapters outperform the base Mistral model but fall short of GPU-trained counterparts.", "conclusion": "Provides a viable, accessible alternative to GPU-based fine-tuning for resource-constrained users."}}
{"id": "2507.01354", "categories": ["cs.LG", "physics.ao-ph", "86A10 (Primary) 86A22, 68U10 (Secondary)", "J.2; I.4.4"], "pdf": "https://arxiv.org/pdf/2507.01354", "abs": "https://arxiv.org/abs/2507.01354", "authors": ["Chugang Yi", "Minghan Yu", "Weikang Qian", "Yixin Wen", "Haizhao Yang"], "title": "Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion", "comment": null, "summary": "Effective hydrological modeling and extreme weather analysis demand\nprecipitation data at a kilometer-scale resolution, which is significantly\nfiner than the 10 km scale offered by standard global products like IMERG. To\naddress this, we propose the Wavelet Diffusion Model (WDM), a generative\nframework that achieves 10x spatial super-resolution (downscaling to 1 km) and\ndelivers a 9x inference speedup over pixel-based diffusion models. WDM is a\nconditional diffusion model that learns the learns the complex structure of\nprecipitation from MRMS radar data directly in the wavelet domain. By focusing\non high-frequency wavelet coefficients, it generates exceptionally realistic\nand detailed 1-km precipitation fields. This wavelet-based approach produces\nvisually superior results with fewer artifacts than pixel-space models, and\ndelivers a significant gains in sampling efficiency. Our results demonstrate\nthat WDM provides a robust solution to the dual challenges of accuracy and\nspeed in geoscience super-resolution, paving the way for more reliable\nhydrological forecasts.", "AI": {"tldr": "The paper introduces the Wavelet Diffusion Model (WDM) for downscaling precipitation data from 10 km to 1 km resolution, achieving high accuracy and speed.", "motivation": "Standard global precipitation data (e.g., IMERG) lacks fine resolution (10 km), limiting hydrological modeling and extreme weather analysis.", "method": "WDM is a conditional diffusion model that learns precipitation structure from MRMS radar data in the wavelet domain, focusing on high-frequency coefficients.", "result": "WDM achieves 10x spatial super-resolution (1 km), 9x faster inference than pixel-based models, and produces realistic, artifact-free results.", "conclusion": "WDM addresses accuracy and speed challenges in geoscience super-resolution, enhancing hydrological forecasting reliability."}}
{"id": "2507.01825", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01825", "abs": "https://arxiv.org/abs/2507.01825", "authors": ["Franco Alberto Cardillo", "Hamza Khyari", "Umberto Straccia"], "title": "MILP-SAT-GNN: Yet Another Neural SAT Solver", "comment": null, "summary": "We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\nMILP problems, which are then encoded as weighted bipartite graphs and\nsubsequently fed into a GNN for training and testing. From a theoretical\nperspective: (i) we establish permutation and equivalence invariance results,\ndemonstrating that the method produces outputs that are stable under reordering\nof clauses and variables; (ii) we identify a theoretical limitation, showing\nthat for a class of formulae called foldable formulae, standard GNNs cannot\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\nuniversal approximation theorem, establishing that with Random Node\nInitialization (RNI), the method can approximate SAT solving to arbitrary\nprecision on finite datasets, that is, the GNN becomes approximately sound and\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\nthe same approximation guarantee can be achieved without the need for RNI.\nFinally, we conduct an experimental evaluation of our approach, which show\nthat, despite the simplicity of the neural architecture, the method achieves\npromising results.", "AI": {"tldr": "A novel method uses GNNs to solve SAT problems by mapping k-CNF formulae to MILP, encoding them as bipartite graphs, and training GNNs. Theoretical results include invariance properties, limitations for foldable formulae, and universal approximation with RNI. Experiments show promising results.", "motivation": "To leverage GNNs for solving SAT problems by bridging the gap between k-CNF formulae and MILP, enabling efficient and scalable SAT solving.", "method": "Map k-CNF formulae to MILP, encode as weighted bipartite graphs, and train GNNs. Theoretical analysis includes invariance, limitations, and approximation guarantees.", "result": "The method achieves permutation and equivalence invariance, identifies limitations for foldable formulae, and proves universal approximation with RNI. Experiments show promising performance.", "conclusion": "The proposed GNN-based method effectively solves SAT problems with theoretical guarantees and practical promise, though limitations exist for certain formula classes."}}
{"id": "2507.01829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01829", "abs": "https://arxiv.org/abs/2507.01829", "authors": ["Tristan Torchet", "Christian Metzner", "Laura Kriener", "Melika Payvand"], "title": "mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling", "comment": null, "summary": "Edge devices for temporal processing demand models that capture both short-\nand long- range dynamics under tight memory constraints. While Transformers\nexcel at sequence modeling, their quadratic memory scaling with sequence length\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\noffer constant memory but train sequentially, and Temporal Convolutional\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\nThis design allows the convolutional layer to realize a flexible delay\nembedding that captures rapid temporal variations, while the recurrent module\nefficiently maintains global context with minimal memory overhead. We validate\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\nseparates and preserves multi-scale temporal features. Furthermore, on\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\noutperforms both pure convolutional and pure recurrent counterparts using\napproximately 20% less memory footprint, highlighting its suitability for\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\npromise as an efficient solution for memory-constrained multi-scale temporal\nprocessing at the edge.", "AI": {"tldr": "mGRADE is a hybrid-memory system combining temporal convolution and minimal gated recurrence for efficient multi-scale temporal processing on edge devices.", "motivation": "Address the need for models that handle short- and long-range dynamics under tight memory constraints, where Transformers, RNNs, and TCNs fall short.", "method": "Integrates a 1D-convolution with learnable spacings and a minimal gated recurrent unit (minGRU) for flexible delay embedding and global context.", "result": "Outperforms pure convolutional and recurrent models on synthetic tasks and image classification benchmarks with 20% less memory.", "conclusion": "mGRADE is an efficient solution for memory-constrained multi-scale temporal processing at the edge."}}
{"id": "2507.01389", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.01389", "abs": "https://arxiv.org/abs/2507.01389", "authors": ["Anbang Wang", "Dunbo Cai", "Yu Zhang", "Yangqing Huang", "Xiangyang Feng", "Zhihong Zhang"], "title": "Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning", "comment": null, "summary": "Recently, a surrogate model was proposed that employs a factorization machine\nto approximate the underlying input-output mapping of the original system, with\nquantum annealing used to optimize the resulting surrogate function. Inspired\nby this approach, we propose an enhanced surrogate model that incorporates\nadditional slack variables into both the factorization machine and its\nassociated Ising representation thereby unifying what was by design a two-step\nprocess into a single, integrated step. During the training phase, the slack\nvariables are iteratively updated, enabling the model to account for\nhigher-order feature interactions. We apply the proposed method to the task of\npredicting drug combination effects. Experimental results indicate that the\nintroduction of slack variables leads to a notable improvement of performance.\nOur algorithm offers a promising approach for building efficient surrogate\nmodels that exploit potential quantum advantages.", "AI": {"tldr": "An enhanced surrogate model integrates slack variables into a factorization machine and Ising representation, improving performance in predicting drug combination effects.", "motivation": "To unify the two-step process of surrogate modeling into a single step and improve performance by accounting for higher-order feature interactions.", "method": "Incorporates slack variables into the factorization machine and Ising representation, iteratively updating them during training.", "result": "Notable performance improvement in predicting drug combination effects.", "conclusion": "The proposed algorithm is promising for efficient surrogate models leveraging quantum advantages."}}
{"id": "2507.01875", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01875", "abs": "https://arxiv.org/abs/2507.01875", "authors": ["Gast\u00f3n Garc\u00eda Gonz\u00e1lez", "Pedro Casas", "Emilio Mart\u00ednez", "Alicia Fern\u00e1ndez"], "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection", "comment": "Presented at ACM KDD 2024, MiLeTS 2024 Workshop, August 25, 2024,\n  Barcelona, Spain", "summary": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.", "AI": {"tldr": "FAE (Foundation Auto-Encoders) is a foundation generative-AI model for anomaly detection in time-series data, leveraging VAEs and DCNNs for zero-shot applications.", "motivation": "To create a pretrained model for time-series data that can detect anomalies accurately across unseen datasets.", "method": "Uses Variational Auto-Encoders (VAEs) and Dilated Convolutional Neural Networks (DCNNs) for univariate time-series modeling.", "result": "Preliminary results show effectiveness on multi-dimensional datasets, including a real mobile ISP dataset and KDD 2021.", "conclusion": "FAE demonstrates potential for zero-shot anomaly detection in diverse time-series domains."}}
{"id": "2507.01414", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01414", "abs": "https://arxiv.org/abs/2507.01414", "authors": ["Sultan Daniels", "Dylan Davis", "Dhruv Gautam", "Wentinn Liao", "Gireeja Ranade", "Anant Sahai"], "title": "Decomposing Prediction Mechanisms for In-Context Recall", "comment": "44 pages, 47 figures, 2 tables", "summary": "We introduce a new family of toy problems that combine features of\nlinear-regression-style continuous in-context learning (ICL) with discrete\nassociative recall. We pretrain transformer models on sample traces from this\ntoy, specifically symbolically-labeled interleaved state observations from\nrandomly drawn linear deterministic dynamical systems. We study if the\ntransformer models can recall the state of a sequence previously seen in its\ncontext when prompted to do so with the corresponding in-context label. Taking\na closer look at this task, it becomes clear that the model must perform two\nfunctions: (1) identify which system's state should be recalled and apply that\nsystem to its last seen state, and (2) continuing to apply the correct system\nto predict the subsequent states. Training dynamics reveal that the first\ncapability emerges well into a model's training. Surprisingly, the second\ncapability, of continuing the prediction of a resumed sequence, develops much\nearlier.\n  Via out-of-distribution experiments, and a mechanistic analysis on model\nweights via edge pruning, we find that next-token prediction for this toy\nproblem involves at least two separate mechanisms. One mechanism uses the\ndiscrete symbolic labels to do the associative recall required to predict the\nstart of a resumption of a previously seen sequence. The second mechanism,\nwhich is largely agnostic to the discrete symbolic labels, performs a\n\"Bayesian-style\" prediction based on the previous token and the context. These\ntwo mechanisms have different learning dynamics.\n  To confirm that this multi-mechanism (manifesting as separate phase\ntransitions) phenomenon is not just an artifact of our toy setting, we used\nOLMo training checkpoints on an ICL translation task to see a similar\nphenomenon: a decisive gap in the emergence of first-task-token performance vs\nsecond-task-token performance.", "AI": {"tldr": "The paper introduces a toy problem combining linear-regression-style continuous in-context learning (ICL) with discrete associative recall, analyzing transformer models' ability to recall and predict sequences.", "motivation": "To understand how transformer models develop capabilities for associative recall and sequence prediction in a controlled setting.", "method": "Pretraining transformer models on symbolic-labeled interleaved state observations from linear dynamical systems, analyzing training dynamics and mechanistic behaviors.", "result": "Two distinct mechanisms emerge: one for associative recall using symbolic labels, and another for Bayesian-style prediction. These mechanisms have different learning dynamics.", "conclusion": "The findings suggest multi-mechanism learning in transformers, supported by similar observations in a translation task, indicating broader applicability."}}
{"id": "2507.01924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01924", "abs": "https://arxiv.org/abs/2507.01924", "authors": ["Samirah Bakker", "Yao Ma", "Seyed Sahand Mohammadi Ziabari"], "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection", "comment": null, "summary": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings.", "AI": {"tldr": "A hybrid deep learning approach combining LSTM and Transformers, with pseudo-labeling via iForest and AE, is proposed for anomaly detection in mental healthcare billing, showing high recall but lower precision.", "motivation": "The complexity of mental healthcare billing and challenges like class imbalance, label scarcity, and sequential patterns motivate the exploration of hybrid models for anomaly detection.", "method": "The study uses a hybrid approach with LSTM and Transformers, leveraging pseudo-labeling via iForest and AE, evaluated on two real-world mental healthcare billing datasets.", "result": "The iForest LSTM baseline achieves high recall (0.963) on declaration-level data, while the hybrid iForest-based model achieves 0.744 recall on operation-level data, though with lower precision.", "conclusion": "The hybrid approach with pseudo-labeling shows promise for complex, imbalanced anomaly detection in healthcare billing."}}
{"id": "2507.01469", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01469", "abs": "https://arxiv.org/abs/2507.01469", "authors": ["Alessio Ferrato", "Fabio Gasparetti", "Carla Limongelli", "Stefano Mastandrea", "Giuseppe Sansonetti", "Joaqu\u00edn Torres-Sospedra"], "title": "Cross-platform Smartphone Positioning at Museums", "comment": "Accepted at the 2025 International Conference on Indoor Positioning\n  and Indoor Navigation (IPIN), Tampere, Finland, September 15-18, 2025", "summary": "Indoor Positioning Systems (IPSs) hold significant potential for enhancing\nvisitor experiences in cultural heritage institutions. By enabling personalized\nnavigation, efficient artifact organization, and better interaction with\nexhibits, IPSs can transform the modalities of how individuals engage with\nmuseums, galleries and libraries. However, these institutions face several\nchallenges in implementing IPSs, including environmental constraints, technical\nlimits, and limited experimentation. In other contexts, Received Signal\nStrength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have\nemerged as preferred solutions due to their non-invasive nature and minimal\ninfrastructure requirements. Nevertheless, the lack of publicly available RSS\ndatasets that specifically reflect museum environments presents a substantial\nbarrier to developing and evaluating positioning algorithms designed for the\nintricate spatial characteristics typical of cultural heritage sites. To\naddress this limitation, we present BAR, a novel RSS dataset collected in front\nof 90 artworks across 13 museum rooms using two different platforms, i.e.,\nAndroid and iOS. Additionally, we provide an advanced position classification\nbaseline taking advantage of a proximity-based method and $k$-NN algorithms. In\nour analysis, we discuss the results and offer suggestions for potential\nresearch directions.", "AI": {"tldr": "The paper introduces BAR, a novel RSS dataset for indoor positioning in museums, addressing the lack of public datasets for cultural heritage sites. It also provides a baseline classification method using proximity and k-NN algorithms.", "motivation": "Enhancing visitor experiences in cultural heritage institutions through IPSs is hindered by the lack of suitable RSS datasets for museum environments.", "method": "The authors collected RSS data in front of 90 artworks across 13 museum rooms using Android and iOS platforms, and proposed a baseline classification method combining proximity-based techniques and k-NN algorithms.", "result": "The study presents the BAR dataset and evaluates the baseline method, discussing its performance and implications.", "conclusion": "The BAR dataset fills a critical gap in IPS research for museums, and the proposed baseline method offers a foundation for future algorithm development in this context."}}
{"id": "2507.01516", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01516", "abs": "https://arxiv.org/abs/2507.01516", "authors": ["Dibyanshu Kumar", "Philipp Vaeth", "Magda Gregorov\u00e1"], "title": "Loss Functions in Diffusion Models: A Comparative Study", "comment": "Accepted to ECML 2025", "summary": "Diffusion models have emerged as powerful generative models, inspiring\nextensive research into their underlying mechanisms. One of the key questions\nin this area is the loss functions these models shall train with. Multiple\nformulations have been introduced in the literature over the past several years\nwith some links and some critical differences stemming from various initial\nconsiderations. In this paper, we explore the different target objectives and\ncorresponding loss functions in detail. We present a systematic overview of\ntheir relationships, unifying them under the framework of the variational lower\nbound objective. We complement this theoretical analysis with an empirical\nstudy providing insights into the conditions under which these objectives\ndiverge in performance and the underlying factors contributing to such\ndeviations. Additionally, we evaluate how the choice of objective impacts the\nmodel ability to achieve specific goals, such as generating high-quality\nsamples or accurately estimating likelihoods. This study offers a unified\nunderstanding of loss functions in diffusion models, contributing to more\nefficient and goal-oriented model designs in future research.", "AI": {"tldr": "This paper explores and unifies various loss functions in diffusion models under the variational lower bound framework, analyzing their relationships, performance differences, and impact on model goals like sample quality and likelihood estimation.", "motivation": "To address the key question of which loss functions diffusion models should train with, given the multiple formulations in literature with varying links and differences.", "method": "A systematic overview of target objectives and loss functions, unified under the variational lower bound framework, complemented by empirical studies on performance divergence and goal impact.", "result": "Provides insights into when and why objectives diverge in performance and how the choice of objective affects model goals like sample quality and likelihood estimation.", "conclusion": "Offers a unified understanding of loss functions in diffusion models, aiding more efficient and goal-oriented future designs."}}
{"id": "2507.01544", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01544", "abs": "https://arxiv.org/abs/2507.01544", "authors": ["Benjamin Feuer", "Lennart Purucker", "Oussama Elachqar", "Chinmay Hegde"], "title": "MARVIS: Modality Adaptive Reasoning over VISualizations", "comment": null, "summary": "Scientific applications of machine learning often rely on small, specialized\nmodels tuned to particular domains. Such models often achieve excellent\nperformance, but lack flexibility. Foundation models offer versatility, but\ntypically underperform specialized approaches, especially on non-traditional\nmodalities and long-tail domains. We propose MARVIS (Modality Adaptive\nReasoning over VISualizations), a training-free method that enables even small\nvision-language models to predict any data modality with high accuracy. MARVIS\ntransforms latent embedding spaces into visual representations and then\nleverages the spatial and fine-grained reasoning skills of VLMs to successfully\ninterpret and utilize them. MARVIS achieves competitive performance on vision,\naudio, biological, and tabular domains using a single 3B parameter model,\nachieving results that beat Gemini by 16\\% on average and approach specialized\nmethods, without exposing personally identifiable information (P.I.I.) or\nrequiring any domain-specific training. We open source our code and datasets at\nhttps://github.com/penfever/marvis", "AI": {"tldr": "MARVIS enables small vision-language models to predict any data modality accurately without training, outperforming Gemini by 16% and approaching specialized methods.", "motivation": "Address the trade-off between specialized models' high performance but lack of flexibility and foundation models' versatility but underperformance, especially in non-traditional domains.", "method": "Transforms latent embedding spaces into visual representations, leveraging VLMs' spatial and fine-grained reasoning skills for interpretation.", "result": "Achieves competitive performance across vision, audio, biological, and tabular domains using a single 3B parameter model, outperforming Gemini by 16%.", "conclusion": "MARVIS offers a training-free, flexible solution for multi-modal prediction without P.I.I. exposure or domain-specific training, with open-sourced code and datasets."}}
{"id": "2507.01559", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01559", "abs": "https://arxiv.org/abs/2507.01559", "authors": ["Lapo Frati", "Neil Traft", "Jeff Clune", "Nick Cheney"], "title": "How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks", "comment": null, "summary": "Recent work in continual learning has highlighted the beneficial effect of\nresampling weights in the last layer of a neural network (``zapping\"). Although\nempirical results demonstrate the effectiveness of this approach, the\nunderlying mechanisms that drive these improvements remain unclear. In this\nwork, we investigate in detail the pattern of learning and forgetting that take\nplace inside a convolutional neural network when trained in challenging\nsettings such as continual learning and few-shot transfer learning, with\nhandwritten characters and natural images. Our experiments show that models\nthat have undergone zapping during training more quickly recover from the shock\nof transferring to a new domain. Furthermore, to better observe the effect of\ncontinual learning in a multi-task setting we measure how each individual task\nis affected. This shows that, not only zapping, but the choice of optimizer can\nalso deeply affect the dynamics of learning and forgetting, causing complex\npatterns of synergy/interference between tasks to emerge when the model learns\nsequentially at transfer time.", "AI": {"tldr": "Zapping (resampling weights in the last layer) improves continual learning, but its mechanisms are unclear. This study explores its effects in challenging settings like few-shot transfer learning.", "motivation": "To understand how zapping and optimizer choices impact learning and forgetting in continual learning scenarios.", "method": "Experiments with convolutional neural networks in continual and few-shot transfer learning, using handwritten characters and natural images.", "result": "Zapping helps models recover faster from domain shifts. Optimizer choice also influences learning dynamics, causing complex task interactions.", "conclusion": "Zapping and optimizer selection significantly affect continual learning, revealing intricate patterns of task synergy/interference."}}
{"id": "2507.01581", "categories": ["cs.LG", "cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01581", "abs": "https://arxiv.org/abs/2507.01581", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang"], "title": "A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning", "comment": null, "summary": "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems.", "AI": {"tldr": "Proposes a Federated Learning (FL)-based approach for indoor localization using DNN, addressing privacy and efficiency issues of centralized methods.", "motivation": "Traditional indoor localization methods have high errors and privacy concerns due to centralized data collection. ML techniques, while promising, also face privacy and reliability issues.", "method": "Uses Federated Learning (FL) with a Deep Neural Network (DNN) model to enable privacy-preserving, efficient indoor localization.", "result": "FL achieves performance close to centralized models while ensuring data privacy, bandwidth efficiency, and server reliability.", "conclusion": "The FL approach is a viable solution for privacy-enhanced indoor localization, advancing secure and efficient systems."}}
{"id": "2507.01598", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01598", "abs": "https://arxiv.org/abs/2507.01598", "authors": ["Naoki Sato", "Hiroki Naganuma", "Hideaki Iiduka"], "title": "Analysis of Muon's Convergence and Critical Batch Size", "comment": null, "summary": "This paper presents a theoretical analysis of Muon, a new optimizer that\nleverages the inherent matrix structure of neural network parameters. We\nprovide convergence proofs for four practical variants of Muon: with and\nwithout Nesterov momentum, and with and without weight decay. We then show that\nadding weight decay leads to strictly tighter bounds on both the parameter and\ngradient norms, and we clarify the relationship between the weight decay\ncoefficient and the learning rate. Finally, we derive Muon's critical batch\nsize minimizing the stochastic first-order oracle (SFO) complexity, which is\nthe stochastic computational cost, and validate our theoretical findings with\nexperiments.", "AI": {"tldr": "The paper analyzes Muon, a new optimizer for neural networks, proving convergence for its variants and showing tighter bounds with weight decay. It also derives Muon's critical batch size and validates findings experimentally.", "motivation": "To theoretically analyze Muon, a novel optimizer leveraging neural network parameter structures, and explore its variants, convergence, and practical implications.", "method": "Theoretical analysis of Muon's convergence for four variants (with/without Nesterov momentum and weight decay), derivation of bounds, and relationship between weight decay and learning rate. Also, derivation of critical batch size for SFO complexity.", "result": "Weight decay leads to tighter bounds on parameter and gradient norms. The relationship between weight decay and learning rate is clarified. Critical batch size for minimizing SFO complexity is derived.", "conclusion": "Muon's theoretical properties are validated, showing practical benefits of weight decay and optimal batch size for efficient training."}}
{"id": "2507.01636", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01636", "abs": "https://arxiv.org/abs/2507.01636", "authors": ["Ghasem Alipoor", "Karl Skretting"], "title": "Kernel Recursive Least Squares Dictionary Learning Algorithm", "comment": "Published in Digital Signal Processing, Volume 141, 2023. DOI:\n  https://doi.org/10.1016/j.dsp.2023.104159 12 pages, 8 figures. Code and data\n  available at: https://github.com/G-Alipoor/kernel-rls-dictionary-learning", "summary": "We propose an efficient online dictionary learning algorithm for kernel-based\nsparse representations. In this framework, input signals are nonlinearly mapped\nto a high-dimensional feature space and represented sparsely using a virtual\ndictionary. At each step, the dictionary is updated recursively using a novel\nalgorithm based on the recursive least squares (RLS) method. This update\nmechanism works with single samples or mini-batches and maintains low\ncomputational complexity. Experiments on four datasets across different domains\nshow that our method not only outperforms existing online kernel dictionary\nlearning approaches but also achieves classification accuracy close to that of\nbatch-trained models, while remaining significantly more efficient.", "AI": {"tldr": "An efficient online dictionary learning algorithm for kernel-based sparse representations, using RLS for updates, outperforms existing methods and matches batch-trained accuracy with lower complexity.", "motivation": "To improve efficiency and performance of online kernel dictionary learning for sparse representations in high-dimensional feature spaces.", "method": "Recursive least squares (RLS)-based dictionary update mechanism, working with single samples or mini-batches, maintaining low computational complexity.", "result": "Outperforms existing online kernel dictionary learning methods and achieves classification accuracy close to batch-trained models, with higher efficiency.", "conclusion": "The proposed method is effective and efficient for online kernel dictionary learning, balancing performance and computational cost."}}
{"id": "2507.01644", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01644", "abs": "https://arxiv.org/abs/2507.01644", "authors": ["Miguel O'Malley"], "title": "Dance Dance ConvLSTM", "comment": "15 pages, 9 figures, 4 tables", "summary": "\\textit{Dance Dance Revolution} is a rhythm game consisting of songs and\naccompanying choreography, referred to as charts. Players press arrows on a\ndevice referred to as a dance pad in time with steps determined by the song's\nchart. In 2017, the authors of Dance Dance Convolution (DDC) developed an\nalgorithm for the automatic generation of \\textit{Dance Dance Revolution}\ncharts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM\n(DDCL), a new method for the automatic generation of DDR charts using a\nConvLSTM based model, which improves upon the DDC methodology and substantially\nincreases the accuracy of chart generation.", "AI": {"tldr": "DDCL improves DDR chart generation using ConvLSTM, outperforming the previous CNN-LSTM method (DDC).", "motivation": "To enhance the accuracy of automatic DDR chart generation.", "method": "Uses a ConvLSTM-based model for chart generation.", "result": "Substantially increases accuracy compared to DDC.", "conclusion": "DDCL is a superior method for automatic DDR chart generation."}}
{"id": "2507.01695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01695", "abs": "https://arxiv.org/abs/2507.01695", "authors": ["Omkar Shende", "Gayathri Ananthanarayanan", "Marcello Traiola"], "title": "PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution", "comment": null, "summary": "Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable\nability to model complex patterns across various domains such as computer\nvision, speech recognition, robotics, etc. While large DNN models are often\nmore accurate than simpler, lightweight models, they are also resource- and\nenergy-hungry. Hence, it is imperative to design methods to reduce reliance on\nsuch large models without significant degradation in output accuracy. The high\ncomputational cost of these models is often necessary only for a reduced set of\nchallenging inputs, while lighter models can handle most simple ones. Thus,\ncarefully combining properties of existing DNN models in a dynamic, input-based\nway opens opportunities to improve efficiency without impacting accuracy.\n  In this work, we introduce PERTINENCE, a novel online method designed to\nanalyze the complexity of input features and dynamically select the most\nsuitable model from a pre-trained set to process a given input effectively. To\nachieve this, we employ a genetic algorithm to explore the training space of an\nML-based input dispatcher, enabling convergence towards the Pareto front in the\nsolution space that balances overall accuracy and computational efficiency.\n  We showcase our approach on state-of-the-art Convolutional Neural Networks\n(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers\n(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's\nability to provide alternative solutions to existing state-of-the-art models in\nterms of trade-offs between accuracy and number of operations. By\nopportunistically selecting among models trained for the same task, PERTINENCE\nachieves better or comparable accuracy with up to 36% fewer operations.", "AI": {"tldr": "PERTINENCE is an online method that dynamically selects the most suitable pre-trained DNN model for a given input, balancing accuracy and computational efficiency using a genetic algorithm. It achieves comparable accuracy with up to 36% fewer operations.", "motivation": "Large DNNs are resource-intensive, but their high computational cost is often unnecessary for simpler inputs. Combining models dynamically can improve efficiency without sacrificing accuracy.", "method": "PERTINENCE uses a genetic algorithm to train an ML-based dispatcher that selects models based on input complexity, optimizing for accuracy and efficiency.", "result": "Tested on CNNs (CIFAR-10, CIFAR-100) and ViTs (TinyImageNet), PERTINENCE matches or exceeds accuracy while reducing operations by up to 36%.", "conclusion": "PERTINENCE demonstrates that dynamic model selection can significantly enhance efficiency without compromising accuracy, offering a practical solution for resource-constrained applications."}}
{"id": "2507.01699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01699", "abs": "https://arxiv.org/abs/2507.01699", "authors": ["Illia Oleksiienko", "Juho Kanniainen", "Alexandros Iosifidis"], "title": "Variational Graph Convolutional Neural Networks", "comment": "This work has been submitted to the IEEE for possible publication. 9\n  pages, 6 figures", "summary": "Estimation of model uncertainty can help improve the explainability of Graph\nConvolutional Networks and the accuracy of the models at the same time.\nUncertainty can also be used in critical applications to verify the results of\nthe model by an expert or additional models. In this paper, we propose\nVariational Neural Network versions of spatial and spatio-temporal Graph\nConvolutional Networks. We estimate uncertainty in both outputs and layer-wise\nattentions of the models, which has the potential for improving model\nexplainability. We showcase the benefits of these models in the social trading\nanalysis and the skeleton-based human action recognition tasks on the Finnish\nboard membership, NTU-60, NTU-120 and Kinetics datasets, where we show\nimprovement in model accuracy in addition to estimated model uncertainties.", "AI": {"tldr": "Proposes Variational Neural Network versions of Graph Convolutional Networks to estimate uncertainty, improving explainability and accuracy in tasks like social trading and human action recognition.", "motivation": "Enhancing model explainability and accuracy by estimating uncertainty in Graph Convolutional Networks, useful for critical applications requiring expert verification.", "method": "Introduces Variational Neural Network versions of spatial and spatio-temporal Graph Convolutional Networks, estimating uncertainty in outputs and layer-wise attentions.", "result": "Demonstrates improved model accuracy and uncertainty estimation on Finnish board membership, NTU-60, NTU-120, and Kinetics datasets.", "conclusion": "The proposed method effectively enhances model explainability and accuracy through uncertainty estimation, validated on diverse datasets."}}
{"id": "2507.01714", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01714", "abs": "https://arxiv.org/abs/2507.01714", "authors": ["Kevin Innerebner", "Franz M. Rohrhofer", "Bernhard C. Geiger"], "title": "B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling", "comment": null, "summary": "Training physics-informed neural networks (PINNs) for forward problems often\nsuffers from severe convergence issues, hindering the propagation of\ninformation from regions where the desired solution is well-defined.\nHaitsiukevich and Ilin (2023) proposed an ensemble approach that extends the\nactive training domain of each PINN based on i) ensemble consensus and ii)\nvicinity to (pseudo-)labeled points, thus ensuring that the information from\nthe initial condition successfully propagates to the interior of the\ncomputational domain.\n  In this work, we suggest replacing the ensemble by a Bayesian PINN, and\nconsensus by an evaluation of the PINN's posterior variance. Our experiments\nshow that this mathematically principled approach outperforms the ensemble on a\nset of benchmark problems and is competitive with PINN ensembles trained with\ncombinations of Adam and LBFGS.", "AI": {"tldr": "The paper proposes replacing PINN ensembles with Bayesian PINNs, using posterior variance for evaluation, showing improved performance over ensembles.", "motivation": "Address convergence issues in PINNs for forward problems by ensuring information propagation from well-defined regions.", "method": "Replace ensemble approach with Bayesian PINNs, evaluating posterior variance instead of consensus.", "result": "Outperforms ensemble methods on benchmarks and competes with Adam-LBFGS-trained ensembles.", "conclusion": "Bayesian PINNs offer a principled, effective alternative to ensembles for improving PINN convergence."}}
{"id": "2507.01724", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01724", "abs": "https://arxiv.org/abs/2507.01724", "authors": ["Micha Henheik", "Theresa Eimer", "Marius Lindauer"], "title": "Revisiting Learning Rate Control", "comment": null, "summary": "The learning rate is one of the most important hyperparameters in deep\nlearning, and how to control it is an active area within both AutoML and deep\nlearning research. Approaches for learning rate control span from classic\noptimization to online scheduling based on gradient statistics. This paper\ncompares paradigms to assess the current state of learning rate control. We\nfind that methods from multi-fidelity hyperparameter optimization,\nfixed-hyperparameter schedules, and hyperparameter-free learning often perform\nvery well on selected deep learning tasks but are not reliable across settings.\nThis highlights the need for algorithm selection methods in learning rate\ncontrol, which have been neglected so far by both the AutoML and deep learning\ncommunities. We also observe a trend of hyperparameter optimization approaches\nbecoming less effective as models and tasks grow in complexity, even when\ncombined with multi-fidelity approaches for more expensive model trainings. A\nfocus on more relevant test tasks and new promising directions like finetunable\nmethods and meta-learning will enable the AutoML community to significantly\nstrengthen its impact on this crucial factor in deep learning.", "AI": {"tldr": "The paper evaluates learning rate control methods in deep learning, finding current approaches inconsistent across tasks and advocating for algorithm selection and new directions like meta-learning.", "motivation": "To assess the effectiveness of various learning rate control paradigms in deep learning and identify gaps in current research.", "method": "Comparison of paradigms including multi-fidelity hyperparameter optimization, fixed schedules, and hyperparameter-free learning across tasks.", "result": "Current methods perform well on specific tasks but lack reliability across settings; hyperparameter optimization becomes less effective with complex models.", "conclusion": "Algorithm selection and new approaches like meta-learning are needed to improve learning rate control in deep learning."}}
{"id": "2507.01740", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01740", "abs": "https://arxiv.org/abs/2507.01740", "authors": ["Trung-Dung Hoang", "Alceu Bissoto", "Vihangkumar V. Naik", "Tim Fl\u00fchmann", "Artemii Shlychkov", "Jos\u00e9 Garcia-Tirado", "Lisa M. Koch"], "title": "A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference", "comment": null, "summary": "Accurately estimating parameters of physiological models is essential to\nachieving reliable digital twins. For Type 1 Diabetes, this is particularly\nchallenging due to the complexity of glucose-insulin interactions. Traditional\nmethods based on Markov Chain Monte Carlo struggle with high-dimensional\nparameter spaces and fit parameters from scratch at inference time, making them\nslow and computationally expensive. In this study, we propose a\nSimulation-Based Inference approach based on Neural Posterior Estimation to\nefficiently capture the complex relationships between meal intake, insulin, and\nglucose level, providing faster, amortized inference. Our experiments\ndemonstrate that SBI not only outperforms traditional methods in parameter\nestimation but also generalizes better to unseen conditions, offering real-time\nposterior inference with reliable uncertainty quantification.", "AI": {"tldr": "Proposes a Simulation-Based Inference (SBI) method using Neural Posterior Estimation for efficient parameter estimation in Type 1 Diabetes models, outperforming traditional MCMC methods.", "motivation": "Accurate parameter estimation for physiological models, especially in Type 1 Diabetes, is challenging due to complex glucose-insulin interactions and computational inefficiency of traditional methods.", "method": "Uses Simulation-Based Inference with Neural Posterior Estimation to model relationships between meal intake, insulin, and glucose levels, enabling faster, amortized inference.", "result": "SBI outperforms traditional MCMC methods in parameter estimation, generalizes better to unseen conditions, and provides real-time posterior inference with uncertainty quantification.", "conclusion": "The proposed SBI approach offers a computationally efficient and reliable solution for parameter estimation in Type 1 Diabetes models, improving over traditional methods."}}
{"id": "2507.01803", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01803", "abs": "https://arxiv.org/abs/2507.01803", "authors": ["Leyang Xue", "Meghana Madhyastha", "Randal Burns", "Myungjin Lee", "Mahesh K. Marina"], "title": "Towards Decentralized and Sustainable Foundation Model Training with the Edge", "comment": null, "summary": "Foundation models are at the forefront of AI research, appealing for their\nability to learn from vast datasets and cater to diverse tasks. Yet, their\nsignificant computational demands raise issues of environmental impact and the\nrisk of centralized control in their development. We put forward a vision\ntowards decentralized and sustainable foundation model training that leverages\nthe collective compute of sparingly used connected edge AI devices. We present\nthe rationale behind our vision, particularly in support of its sustainability\nbenefit. We further outline a set of challenges that need to be addressed to\nturn this vision into reality.", "AI": {"tldr": "Proposes decentralized and sustainable foundation model training using edge AI devices to address environmental and centralization concerns.", "motivation": "Addresses the environmental impact and centralized control risks of current foundation models by leveraging underutilized edge AI devices.", "method": "Envisions decentralized training using collective compute of connected edge devices, emphasizing sustainability.", "result": "Identifies challenges to realize decentralized, sustainable foundation model training.", "conclusion": "Advocates for a shift to decentralized training to mitigate environmental and centralization issues."}}
{"id": "2507.01823", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01823", "abs": "https://arxiv.org/abs/2507.01823", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents", "comment": "Preprint of a manuscript submitted for peer review", "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.", "AI": {"tldr": "A novel knowledge transfer method in model-based reinforcement learning efficiently distills a large multi-task agent into a compact model, improving performance and reducing size for resource-constrained deployment.", "motivation": "Addressing the challenge of deploying large world models in resource-constrained environments by enabling efficient knowledge transfer.", "method": "Distills a high-capacity multi-task agent (317M parameters) into a compact model (1M parameters) and optimizes it with FP16 post-training quantization.", "result": "Achieves a state-of-the-art normalized score of 28.45, surpassing the original 1M parameter model score of 18.93, and reduces model size by ~50%.", "conclusion": "The method effectively consolidates complex multi-task knowledge, offering practical deployment solutions and insights for efficient multi-task reinforcement learning systems."}}
{"id": "2507.01831", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01831", "abs": "https://arxiv.org/abs/2507.01831", "authors": ["Yucen Lily Li", "Daohan Lu", "Polina Kirichenko", "Shikai Qiu", "Tim G. J. Rudner", "C. Bayan Bruss", "Andrew Gordon Wilson"], "title": "Out-of-Distribution Detection Methods Answer the Wrong Questions", "comment": "Extended version of ICML 2025 paper", "summary": "To detect distribution shifts and improve model safety, many\nout-of-distribution (OOD) detection methods rely on the predictive uncertainty\nor features of supervised models trained on in-distribution data. In this\npaper, we critically re-examine this popular family of OOD detection\nprocedures, and we argue that these methods are fundamentally answering the\nwrong questions for OOD detection. There is no simple fix to this misalignment,\nsince a classifier trained only on in-distribution classes cannot be expected\nto identify OOD points; for instance, a cat-dog classifier may confidently\nmisclassify an airplane if it contains features that distinguish cats from\ndogs, despite generally appearing nothing alike. We find that uncertainty-based\nmethods incorrectly conflate high uncertainty with being OOD, while\nfeature-based methods incorrectly conflate far feature-space distance with\nbeing OOD. We show how these pathologies manifest as irreducible errors in OOD\ndetection and identify common settings where these methods are ineffective.\nAdditionally, interventions to improve OOD detection such as feature-logit\nhybrid methods, scaling of model and data size, epistemic uncertainty\nrepresentation, and outlier exposure also fail to address this fundamental\nmisalignment in objectives. We additionally consider unsupervised density\nestimation and generative models for OOD detection, which we show have their\nown fundamental limitations.", "AI": {"tldr": "The paper critiques current OOD detection methods, highlighting their fundamental misalignment with the task of identifying out-of-distribution data. It shows that uncertainty-based and feature-based methods fail due to inherent flaws, and even advanced interventions don't resolve these issues. Unsupervised methods also have limitations.", "motivation": "The motivation is to expose the flaws in popular OOD detection methods, which misalign with the actual goal of identifying OOD data, leading to unreliable results.", "method": "The paper critically analyzes uncertainty-based and feature-based OOD detection methods, demonstrating their limitations through theoretical and empirical evidence. It also evaluates interventions like hybrid methods and unsupervised approaches.", "result": "The results reveal irreducible errors in current OOD detection methods, showing they conflate uncertainty or feature distance with OOD status. Advanced fixes fail to address the core misalignment.", "conclusion": "The conclusion is that current OOD detection methods are fundamentally flawed due to misaligned objectives, and new approaches are needed to address these limitations."}}
{"id": "2507.01841", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.01841", "abs": "https://arxiv.org/abs/2507.01841", "authors": ["Yihang Gao", "Vincent Y. F. Tan"], "title": "Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization", "comment": null, "summary": "In this paper, we propose SubLoRA, a rank determination method for Low-Rank\nAdaptation (LoRA) based on submodular function maximization. In contrast to\nprior approaches, such as AdaLoRA, that rely on first-order (linearized)\napproximations of the loss function, SubLoRA utilizes second-order information\nto capture the potentially complex loss landscape by incorporating the Hessian\nmatrix. We show that the linearization becomes inaccurate and ill-conditioned\nwhen the LoRA parameters have been well optimized, motivating the need for a\nmore reliable and nuanced second-order formulation. To this end, we reformulate\nthe rank determination problem as a combinatorial optimization problem with a\nquadratic objective. However, solving this problem exactly is NP-hard in\ngeneral. To overcome the computational challenge, we introduce a submodular\nfunction maximization framework and devise a greedy algorithm with\napproximation guarantees. We derive a sufficient and necessary condition under\nwhich the rank-determination objective becomes submodular, and construct a\nclosed-form projection of the Hessian matrix that satisfies this condition\nwhile maintaining computational efficiency. Our method combines solid\ntheoretical foundations, second-order accuracy, and practical computational\nefficiency. We further extend SubLoRA to a joint optimization setting,\nalternating between LoRA parameter updates and rank determination under a rank\nbudget constraint. Extensive experiments on fine-tuning physics-informed neural\nnetworks (PINNs) for solving partial differential equations (PDEs) demonstrate\nthe effectiveness of our approach. Results show that SubLoRA outperforms\nexisting methods in both rank determination and joint training performance.", "AI": {"tldr": "SubLoRA introduces a second-order rank determination method for LoRA, using submodular function maximization and Hessian matrix, outperforming prior methods like AdaLoRA.", "motivation": "Prior methods rely on inaccurate first-order approximations of the loss function, especially when LoRA parameters are well-optimized, necessitating a more reliable second-order approach.", "method": "SubLoRA reformulates rank determination as a combinatorial optimization problem with a quadratic objective, solved via submodular function maximization and a greedy algorithm with approximation guarantees.", "result": "SubLoRA outperforms existing methods in rank determination and joint training performance, validated through experiments on physics-informed neural networks for PDEs.", "conclusion": "SubLoRA combines theoretical rigor, second-order accuracy, and computational efficiency, offering a superior alternative to first-order methods for LoRA rank determination."}}
{"id": "2507.01951", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01951", "abs": "https://arxiv.org/abs/2507.01951", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "title": "Test-Time Scaling with Reflective Generative Model", "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.", "AI": {"tldr": "MetaStone-S1 is a reflective generative model achieving OpenAI o3's performance via SPRM, integrating policy and reward models efficiently. It supports test-time scaling and open-sourced for research.", "motivation": "To create an efficient, unified model combining policy and process reward without extra annotations, reducing parameters and enabling scalable reasoning.", "method": "Uses SPRM with shared backbone and task-specific heads for next token prediction and process scoring, enabling efficient reasoning and test-time scaling.", "result": "Achieves OpenAI-o3-mini performance with 32B parameters and establishes a scaling law for thinking computation.", "conclusion": "MetaStone-S1 is a scalable, efficient model open-sourced for community use, matching high-performance benchmarks with fewer parameters."}}
