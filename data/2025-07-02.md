<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: DiMo-GUI is a training-free framework for grounding natural language queries in GUIs, using dynamic visual grounding and modality-aware optimization to improve accuracy without additional training.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of GUI grounding, such as visual diversity, spatial clutter, and language ambiguity, without requiring extra training data.

Method: Splits GUI into textual and iconic elements for independent reasoning, dynamically refines predictions by focusing on candidate regions, and incrementally zooms into subregions.

Result: Outperforms baseline inference pipelines on standard GUI grounding benchmarks, showing improved accuracy.

Conclusion: DiMo-GUI effectively combines modality separation and region-focused reasoning to enhance GUI grounding without training.

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [2] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: TalentMine, an LLM-enhanced framework, addresses semantic loss in table extraction for talent management, achieving 100% query accuracy by preserving tabular relationships.


<details>
  <summary>Details</summary>
Motivation: Current table extraction methods fail to capture semantic relationships in talent documents, leading to poor retrieval performance.

Method: TalentMine uses multimodal reasoning to create semantically enriched table representations, outperforming conventional CSV/text linearization.

Result: Achieves 100% accuracy in query tasks vs. 0% (AWS Textract) and 40% (AWS Textract Visual Q&A). Claude v3 Haiku performs best.

Conclusion: TalentMine significantly improves semantic table understanding and retrieval for talent management systems.

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [3] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: A distributed self-driving lab (SDL) using FAIR data and nanoHUB services accelerates collaborative optimization tasks, with a case study on frugal food dye color optimization.


<details>
  <summary>Details</summary>
Motivation: To enhance collaboration and efficiency in scientific discovery by integrating machine learning, FAIR data, and automated experimentation in SDLs.

Method: Implementation of a distributed SDL framework using nanoHUB for simulation and FAIR data management, with active learning for sequential optimization.

Result: A functional system enabling shared data, real-time ML updates, and collaborative optimization, demonstrated with food dye color mixing.

Conclusion: The framework is scalable and adaptable to diverse optimization problems, promoting accessible and collaborative research.

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [4] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: SEZ-HARN is a self-explainable zero-shot HAR model that recognizes unseen activities and provides skeleton videos for transparency, achieving competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of IMU-based HAR datasets and lack of transparency in existing zero-shot HAR models.

Method: Introduces SEZ-HARN, a model that recognizes untrained activities and explains decisions via skeleton videos.

Result: SEZ-HARN achieves near-state-of-the-art accuracy (within 3% on PAMAP2) while providing understandable explanations.

Conclusion: SEZ-HARN balances accuracy and transparency, making it suitable for real-world HAR applications.

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [5] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: AdvDistill improves knowledge distillation by using reward-guided dataset distillation, enhancing student model performance in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current distillation methods limit student models to copying teacher responses, reducing generalisability, especially in reasoning tasks.

Method: AdvDistill uses multiple teacher responses per prompt, assigns rewards via rule-based verifiers, and trains student models with weighted rewards.

Result: Significant improvement in student model performance for mathematical and complex reasoning tasks.

Conclusion: Incorporating a rewarding mechanism in dataset distillation enhances efficacy and benefits student models.

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [6] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: The paper introduces VoyagerVision, a multi-modal model that uses visual feedback (screenshots) in Minecraft to enhance spatial interpretation and task performance, outperforming its predecessor Voyager.


<details>
  <summary>Details</summary>
Motivation: To extend the open-ended potential of AI by improving spatial environment interpretation through visual inputs, enabling more diverse task completion.

Method: Developed VoyagerVision, a multi-modal model leveraging screenshots for visual feedback in Minecraft, building on the Voyager framework.

Result: VoyagerVision created 2.75 unique structures per 50 iterations (Voyager couldn't) and succeeded in 50% of flat-world building tests, struggling with complex structures.

Conclusion: Visual inputs significantly enhance AI's spatial task performance, demonstrating VoyagerVision's potential for open-ended AI advancement.

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [7] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: The paper introduces inverse reasoning, a method for LLMs to decompose and explain their reasoning post-hoc, improving transparency and performance.


<details>
  <summary>Details</summary>
Motivation: To address the black-box nature of LLM decision-making and enhance interpretability in reasoning tasks.

Method: Uses inverse reasoning with a metacognitive structure in SAGE-nano, reversing attention flow to identify decision points and generate explanations.

Result: Achieves high reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference), comparable to top models like GPT-4o.

Conclusion: Inverse reasoning improves LLM transparency and performance, advancing AI safety, education, and scientific discovery.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [8] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: A novel pipeline using RL and decision trees extracts interpretable decision logic from legacy systems, focusing on boundary regions for accurate rule extraction.


<details>
  <summary>Details</summary>
Motivation: Legacy systems lack documentation, making modernization difficult. Traditional methods like behavioral cloning fail to capture underlying intent.

Method: Uses an RL agent to explore input space, identify decision boundaries, and cluster counterfactual transitions. Decision trees then extract human-readable rules.

Result: Effective on dummy systems with varying complexity; extracted rules accurately reflect core logic.

Conclusion: The pipeline provides a promising foundation for generating specifications and test cases during legacy migration.

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [9] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: AI tools like ChatGPT may reduce cognitive engagement in students during academic writing tasks, as shown by lower scores in the AI-assisted group compared to controls.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of generative AI tools on students' cognitive engagement during academic writing, addressing concerns about reduced deep thinking and active learning.

Method: Experimental design with random assignment to AI-assisted (ChatGPT) or non-assisted (control) conditions. Participants completed a writing task and a cognitive engagement scale (CES-AI).

Result: Significantly lower cognitive engagement scores in the ChatGPT group, suggesting cognitive offloading.

Conclusion: AI assistance may compromise deep cognitive involvement; pedagogical strategies are needed to promote active engagement with AI-generated content.

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [10] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: xHAIM enhances HAIM by adding explainability and task-specific data use, improving prediction accuracy and clinical utility.


<details>
  <summary>Details</summary>
Motivation: Address HAIM's limitations of task-agnostic data use and lack of explainability in AI for medicine.

Method: Leverages Generative AI in four steps: task-relevant data identification, patient summary generation, predictive modeling, and clinical explanations.

Result: Improves average AUC from 79.9% to 90.3% on chest pathology and operative tasks.

Conclusion: xHAIM transforms AI into an explainable decision support system, enhancing clinical adoption.

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [11] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: A review of ML applications for NP-hard routing problems like TSP and VRP, proposing a taxonomy for ML-based methods and integrating traditional OR with ML.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of exact algorithms and heuristics in solving NP-hard routing problems by leveraging ML techniques.

Method: Proposes a taxonomy categorizing ML-based routing methods into construction-based and improvement-based approaches.

Result: Provides a structured framework to guide future research and address emerging VRP variants.

Conclusion: Integrates traditional OR methods with ML, offering a pathway for enhanced solutions to complex routing problems.

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [12] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: ASTRO trains language models to reason like search algorithms using self-reflection, backtracking, and exploration, improving performance on math problems.


<details>
  <summary>Details</summary>
Motivation: Enhance reasoning capabilities of non-reasoner models like Llama 3 by teaching structured search behavior.

Method: Uses synthetic data from Monte Carlo Tree Search (MCTS) to create natural language chain-of-thoughts, followed by reinforcement learning with verifiable rewards.

Result: Achieves significant performance gains: 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024.

Conclusion: Search-inspired training effectively instills robust reasoning in open LLMs.

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [13] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: Math reasoning improvements in LLMs may not generalize to other domains, with RL-tuned models outperforming SFT-tuned ones in cross-domain tasks.


<details>
  <summary>Details</summary>
Motivation: To determine if math reasoning gains in LLMs reflect broader problem-solving ability or narrow overfitting.

Method: Evaluated 20+ models across math, scientific QA, agent planning, coding, and instruction-following. Conducted controlled experiments on Qwen3-14B with math-only data using different tuning methods (RL vs. SFT).

Result: RL-tuned models generalized better across domains, while SFT-tuned models often lost general capabilities. SFT caused representation and output drift, whereas RL preserved general-domain structure.

Conclusion: Rethinking post-training recipes, especially reliance on SFT-distilled data, is needed to advance reasoning models.

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [14] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: The paper introduces a 2D cell-jump move and an extended local search framework (2d-LS) for SMT-NRA, integrating MCSAT and a sample-cell projection operator to enhance efficiency. A hybrid framework combining MCSAT, 2d-LS, and OpenCAD is proposed, showing improved performance in experiments.


<details>
  <summary>Details</summary>
Motivation: To enhance local search efficiency for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA) by generalizing key operations and integrating advanced techniques.

Method: 1. Introduces 2d-cell-jump move. 2. Proposes 2d-LS framework integrating MCSAT. 3. Implements sample-cell projection operator for MCSAT. 4. Designs a hybrid framework combining MCSAT, 2d-LS, and OpenCAD.

Result: Experimental results show improved local search performance, validating the effectiveness of the proposed methods.

Conclusion: The proposed techniques, including 2d-cell-jump, 2d-LS, and the hybrid framework, significantly enhance the efficiency of local search for SMT-NRA.

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [15] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: RL for LLMs in chess shows promise but plateaus below expert levels due to pretrained models' chess understanding deficits.


<details>
  <summary>Details</summary>
Motivation: Explore if LLMs can develop strategic reasoning in chess using RL, leveraging dense rewards from a chess-pretrained network.

Method: Use a chess-pretrained action-value network for dense rewards (knowledge distillation) and compare with sparse binary rewards.

Result: Dense rewards outperform sparse ones, but models plateau below expert levels due to pretrained models' chess understanding gaps.

Conclusion: RL alone may not fully overcome LLMs' chess understanding deficits, suggesting limitations in pretrained models.

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [16] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: An improved numerical algorithm for minimax problems using nonsmooth optimization, quadratic programming, and iterative methods, with proven convergence under mild assumptions.


<details>
  <summary>Details</summary>
Motivation: Address the need for efficient and reliable solutions to minimax problems in fields like robust optimization and imbalanced learning.

Method: Combines nonsmooth optimization, quadratic programming, and iterative processes, with convergence proof under gradient continuity and boundedness.

Result: The algorithm is theoretically sound and applicable to diverse fields.

Conclusion: The proposed algorithm offers a robust and versatile solution for minimax problems with proven convergence.

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [17] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: The paper explores security risks in multimodal foundation model-driven agents, proposing a risk discrimination mechanism and automated assessment to mitigate jailbreak risks.


<details>
  <summary>Details</summary>
Motivation: Addressing the increasing exposure of intelligent agents to jailbreak risks, where attackers bypass constraints to trigger sensitive operations, highlighting gaps in existing security measures.

Method: Constructs a risk discrimination mechanism using behavioral sequence information and designs an automated assessment scheme based on a large language model.

Result: Preliminary validation shows improved recognition of risky behaviors and reduced jailbreak probability in high-risk tasks.

Conclusion: The study offers valuable insights for security risk modeling and protection in multimodal intelligent agent systems.

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [18] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: The paper explores the limitations of current AI models in achieving AGI, emphasizing the need for modular reasoning, memory, and multi-agent coordination. It introduces Agentic RAG frameworks and other strategies for adaptive behavior, while highlighting the integration of memory and reasoning as key to true intelligence.


<details>
  <summary>Details</summary>
Motivation: To address the gap between current AI capabilities and AGI by synthesizing insights from multiple disciplines and proposing pathways for more flexible, domain-agnostic intelligence.

Method: Cross-disciplinary synthesis of AGI development, analyzing architectural and cognitive foundations, and proposing frameworks like Agentic RAG, generalization strategies, and neurosymbolic systems.

Result: Identifies critical pathways toward AGI, including modular reasoning, memory integration, and adaptive frameworks, while acknowledging current limitations.

Conclusion: True AGI requires integrating memory, reasoning, and adaptive components, with ongoing challenges in scientific, technical, and ethical domains.

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [19] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: CIP uses causal influence diagrams (CIDs) to improve safety in LLM-powered autonomous agents by structuring decision-making and mitigating risks.


<details>
  <summary>Details</summary>
Motivation: Ensuring safe and reliable behavior in LLM-powered agents to prevent unintended consequences.

Method: Three-step approach: (1) initialize CID for task, (2) guide agent interactions using CID, (3) iteratively refine CID based on outcomes.

Result: Enhanced safety in code execution and mobile device control tasks.

Conclusion: CIP effectively improves agent safety through structured causal reasoning.

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness](https://arxiv.org/abs/2507.00195)
*Kumar Kshitij Patel*

Main category: cs.LG

TL;DR: The paper analyzes Local SGD in distributed/federated optimization, showing bounded second-order heterogeneity is key for outperforming centralized methods. It provides tight bounds, a consensus-error framework, and extends to online federated learning.


<details>
  <summary>Details</summary>
Motivation: To understand when and why local update algorithms like Local SGD outperform centralized methods under realistic data heterogeneity.

Method: Uses a fine-grained consensus-error-based analysis framework under third-order smoothness and relaxed heterogeneity assumptions.

Result: Establishes tight bounds for Local SGD, characterizes min-max complexity, and provides regret bounds for online federated learning.

Conclusion: Clarifies the advantages of local updates and serves as a guide for analyzing Local SGD in heterogeneous settings.

Abstract: This thesis contributes to the theoretical understanding of local update
algorithms, especially Local SGD, in distributed and federated optimization
under realistic models of data heterogeneity. A central focus is on the bounded
second-order heterogeneity assumption, which is shown to be both necessary and
sufficient for local updates to outperform centralized or mini-batch methods in
convex and non-convex settings. The thesis establishes tight upper and lower
bounds in several regimes for various local update algorithms and characterizes
the min-max complexity of multiple problem classes. At its core is a
fine-grained consensus-error-based analysis framework that yields sharper
finite-time convergence bounds under third-order smoothness and relaxed
heterogeneity assumptions. The thesis also extends to online federated
learning, providing fundamental regret bounds under both first-order and bandit
feedback. Together, these results clarify when and why local updates offer
provable advantages, and the thesis serves as a self-contained guide for
analyzing Local SGD in heterogeneous environments.

</details>


### [21] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: HDRAM improves LLM memory by using holographic and quantum-inspired techniques to address precision loss as an information-theoretic issue.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from precision loss, reframed as information spreading, which hampers efficient key-value operations and associative retrieval.

Method: Introduces HDRAM, a symbolic memory framework using hypertokens, ECC, holographic computing, and quantum-inspired search to recover distributed information.

Result: HDRAM enhances associative retrieval in LLMs without architectural changes, leveraging CHQ principles.

Conclusion: CHQ-inspired HDRAM effectively addresses LLM memory issues, improving performance through principled despreading and phase-coherent addressing.

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [22] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: NeutroSENSE is a neutrosophic-enhanced ensemble framework for interpretable intrusion detection in IoT, achieving 97% accuracy by quantifying uncertainty through truth, falsity, and indeterminacy components.


<details>
  <summary>Details</summary>
Motivation: To improve intrusion detection in IoT by integrating neutrosophic logic with ensemble models for better uncertainty quantification and interpretability.

Method: Combines Random Forest, XGBoost, and Logistic Regression with neutrosophic logic to decompose predictions into truth (T), falsity (F), and indeterminacy (I) components, using adaptive thresholds for review.

Result: Achieved 97% accuracy on IoT-CAD dataset, with misclassified samples showing higher indeterminacy (I=0.62) than correct ones (I=0.24).

Conclusion: NeutroSENSE enhances accuracy and explainability, offering a trust-aware AI foundation for IoT security systems.

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [23] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: DS3 introduces a framework for optimizing LLM inference costs by modeling it as stochastic traversal over a skill graph, enabling comparative analysis of strategies like CoT and ToT.


<details>
  <summary>Details</summary>
Motivation: High computational, energy, and financial costs of LLMs, especially in inference, necessitate more efficient operating points beyond existing compute-optimality characterizations.

Method: Directed stochastic skill search (DS3) models inference as stochastic traversal over a learned skill graph, deriving closed-form expressions for task success and compute cost.

Result: The framework recovers empirical patterns like linear accuracy scaling with logarithmic compute and variation in preferred inference strategies based on task difficulty and model capability.

Conclusion: DS3 enhances theoretical understanding of training-inference interdependencies, aiding principled algorithmic design and resource allocation.

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [24] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

TL;DR: The paper proposes a Reinforcement Learning (RL)-based Elevator Group Control System (EGCS) to optimize elevator traffic in large buildings, outperforming traditional rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Heuristic or pattern-based controllers struggle with the stochastic and combinatorial challenges of elevator dispatching, necessitating a more adaptive solution.

Method: The system is modeled as a Markov Decision Process, using Dueling Double Deep Q-learning with innovations like action space encoding, infra-steps for continuous arrivals, and tailored rewards.

Result: The RL-based EGCS adapts to fluctuating traffic and stochastic environments, outperforming traditional rule-based algorithms.

Conclusion: The proposed RL approach effectively addresses the complexities of elevator dispatching, offering improved efficiency and adaptability.

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [25] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

TL;DR: The paper introduces a method to create undistillable DNNs by minimizing conditional mutual information (CMI) during training, ensuring knockoff students underperform label-smoothed ones.


<details>
  <summary>Details</summary>
Motivation: To protect DNN intellectual property by making models undistillable through knowledge distillation.

Method: Proposes the CMI Minimized (CMIM) method, training DNNs by jointly minimizing cross-entropy loss and CMI values of temperature-scaled clusters.

Result: CMIM models are undistillable by tested KD methods and outperform models trained with cross-entropy alone in prediction accuracy.

Conclusion: The CMIM method effectively creates undistillable DNNs while maintaining or improving model performance.

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [26] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

TL;DR: ST-MTM improves time-series forecasting by combining masked modeling with seasonal-trend decomposition and contrastive learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing masked time-series modeling ignores inherent semantic structures in raw data, leading to spurious patterns. Addressing entangled temporal patterns through decomposition can improve forecasting.

Method: ST-MTM uses seasonal-trend decomposition with novel masking strategies: period masking for seasonal components and sub-series masking for trend components. It also employs contrastive learning for contextual consistency.

Result: ST-MTM consistently outperforms existing masked modeling, contrastive learning, and supervised forecasting methods.

Conclusion: Decomposition-based masking and contrastive learning in ST-MTM effectively capture temporal semantics, enhancing forecasting accuracy.

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [27] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: SWE-Bench-CL is a continual learning benchmark for evaluating LLMs in evolving software development contexts, featuring chronologically ordered GitHub issues, specialized metrics, and an interactive evaluation framework.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLMs' ability to handle continuous, real-world software development tasks, including knowledge transfer and resistance to forgetting.

Method: Introduces SWE-Bench-CL, a dataset of chronologically ordered GitHub issues, with an interactive LangGraph-based evaluation framework and specialized continual learning metrics.

Result: Provides tools and metrics for assessing LLMs' continual learning capabilities, including accuracy, forgetting, and transfer efficiency.

Conclusion: SWE-Bench-CL offers a reproducible platform for developing adaptive AI agents in software engineering, enhancing their robustness and adaptability.

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [28] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

TL;DR: A novel vision transformer (ViT) with an adversarial indicator (AdvI) token is proposed to defend against adversarial attacks in modulation classification, combining training and runtime defenses in one model.


<details>
  <summary>Details</summary>
Motivation: Transformers are vulnerable to adversarial attacks in radio signal classification, necessitating a robust defense mechanism.

Method: Introduces AdvI token in ViT to detect attacks, integrates adversarial training, and examines attention mechanisms for defense.

Result: Outperforms existing methods in white-box attack scenarios like FGM, PGD, and BIM.

Conclusion: The AdvI token effectively enhances ViT's robustness against adversarial attacks in modulation classification.

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [29] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: GRFT is a gradient-based and regularized fine-tuning method that updates specific rows/columns of weight matrices, reducing storage and improving efficiency while achieving top performance.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large pre-trained models is resource-intensive; GPS reduces parameters but increases resource demands. GRFT aims to optimize this process.

Method: GRFT updates rows/columns with the highest sum of squared gradients, incorporates regularization, and reduces parameter updates (e.g., 1.22% on FGVC).

Result: GRFT outperforms GPS, Adapter Tuning, and LoRA, requiring minimal parameter updates (1.22% on FGVC, 0.30% on VTAB).

Conclusion: GRFT is efficient, effective, and achieves state-of-the-art performance with reduced resource usage.

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [30] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for post-training processes in LLMs, linking SFT and preference learning, and proposes improvements to address limitations in conventional SFT.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SFT and preference learning in LLM post-training and address the limitations of conventional SFT.

Method: Theoretical analysis and mathematical derivation to unify SFT and preference learning, along with proposed learning rate reduction and alternative SFT objectives.

Result: Significant performance improvements (up to 25% relative gain, 6% absolute win rate increase) and enhanced post-DPO model performance.

Conclusion: The framework provides a deeper understanding of SFT and preference learning, offering practical improvements for LLM post-training.

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [31] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

TL;DR: The paper proposes and evaluates three quantum-inspired data encoding strategies (ILS, GDS, CCVS) for classical ML, focusing on reducing encoding time while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To optimize quantum-inspired data transformations for classical ML by balancing encoding efficiency, correctness, and computational cost.

Method: Three encoding strategies are compared: ILS (instance-level), GDS (global discrete), and CCVS (class-conditional). Their impact on encoding time, correctness, and classification performance is assessed.

Result: The study provides insights into trade-offs between encoding time, precision, and predictive performance for each strategy.

Conclusion: The findings help optimize quantum-inspired data encoding for classical ML workflows, highlighting the importance of strategy choice based on specific needs.

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [32] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges,Felipe Pereira,Michel Tosin*

Main category: cs.LG

TL;DR: A Variational Autoencoder (VAE) is used to improve Markov Chain Monte Carlo (McMC) methods by generating flexible prior proposals, outperforming traditional Karhunen-Loève Expansion (KLE) in cases with unknown correlation structures.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like KLE require prior knowledge of covariance functions, which is often unavailable. The study aims to enhance McMC efficiency and adaptability in high-dimensional Bayesian inverse problems, such as subsurface flow modeling.

Method: The study employs a VAE framework to generate data-driven prior proposals, tested on a synthetic groundwater flow inversion problem to estimate permeability fields from pressure data.

Result: VAE matches KLE accuracy when correlation length is known and outperforms KLE when it is misassumed. It also reduces stochastic dimensionality, improving computational efficiency.

Conclusion: Deep generative models like VAE can enhance McMC methods, making Bayesian inference more adaptable and efficient for high-dimensional problems.

Abstract: This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [33] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: Introduces GLU Attention, a novel attention mechanism using nonlinearity in values, improving performance and convergence speed without extra parameters or computational cost.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network performance by introducing nonlinearity into attention mechanisms, leveraging the success of Gated Linear Units (GLU).

Method: Proposes GLU Attention, integrating nonlinearity into attention values, and tests it across text and vision tasks.

Result: GLU Attention improves model performance and convergence speed without additional parameters or significant computational overhead.

Conclusion: GLU Attention is a lightweight, effective enhancement for attention mechanisms, compatible with existing technologies like Flash Attention and RoPE.

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [34] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

TL;DR: AIMatDesign, a reinforcement learning framework, overcomes limitations of traditional ML methods in materials design by integrating expert knowledge, refining predictions with LLMs, and improving discovery efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high-dimensional materials composition space and limited experimental data, as well as the lack of reliability and expert knowledge integration in existing ML models.

Method: Uses reinforcement learning with difference-based algorithms for data augmentation, LLM-guided refinement for prediction correction, and knowledge-based reward functions for stability.

Result: Outperforms traditional ML and RL methods in efficiency, convergence, and success rates, validated by experimental synthesis of high-performance Zr-based alloys.

Conclusion: AIMatDesign demonstrates reliability and potential for closed-loop materials discovery, accurately predicting material properties and trends.

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [35] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

TL;DR: FNSDA introduces a parameter-efficient method for generalizing to new dynamical systems by adapting in Fourier space, outperforming existing methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Current deep learning approaches for modeling dynamics lack generalization to unseen systems with shared dynamics but different environmental characteristics.

Method: FNSDA partitions Fourier modes to identify shareable dynamics and adjusts modes for new environments using low-dimensional latent parameters.

Result: FNSDA achieves superior or competitive generalization on four dynamic systems with reduced parameter costs.

Conclusion: FNSDA provides an efficient and adaptable solution for modeling complex dynamics across varying environments.

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [36] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: ROSE, a framework using multi-objective reinforcement learning, improves LLM safety evaluation by generating diverse and context-rich adversarial prompts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current manual and automated methods for evaluating LLM safety are limited in adaptability and coverage, failing to keep pace with advancing LLMs and lacking real-world contextualization.

Method: Proposes ROSE, a framework leveraging multi-objective reinforcement learning to fine-tune an adversarial LLM for generating diverse and contextually rich prompts.

Result: ROSE outperforms existing methods in uncovering safety vulnerabilities in LLMs, with improved evaluation metrics.

Conclusion: ROSE advances practical, reality-oriented safety evaluation for LLMs, addressing limitations of current approaches.

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [37] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: HiT-JEPA is a hierarchical framework for multi-scale urban trajectory representation, capturing fine-grained details and high-level semantics in one model.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to integrate fine-grained details and high-level summaries in trajectory data, limiting analysis of spatial movement patterns.

Method: HiT-JEPA uses a three-layer hierarchy to capture point-level details, intermediate patterns, and high-level abstractions, combining local and global semantics.

Result: Experiments show HiT-JEPA produces richer, multi-scale representations for trajectory similarity computation.

Conclusion: HiT-JEPA effectively unifies multi-scale trajectory representations, improving analysis of urban movement patterns.

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [38] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

TL;DR: LoRA-Mixer integrates LoRA experts into MoE for LLMs, improving efficiency and performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current LoRA-MoE methods, which lack parameter efficiency and task fidelity.

Method: Replace projection matrices with dynamically routed LoRA experts, supporting joint optimization or frozen pre-trained modules.

Result: Significant improvements on benchmarks (e.g., GSM8K, HumanEval, MedQA) with fewer parameters.

Conclusion: LoRA-Mixer is efficient, modular, and outperforms state-of-the-art methods.

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [39] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: A novel DRL approach integrates contextual bandits to adaptively select action durations, improving performance and efficiency in tasks like Atari games.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored aspect of temporal scale in DRL to enhance policy flexibility and computational efficiency.

Method: Augments DQN with a contextual bandit module to learn optimal action repetition rates based on state contexts.

Result: Experiments on Atari 2600 games show significant improvements over static duration baselines.

Conclusion: The approach offers a scalable solution for real-time applications requiring dynamic action durations.

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [40] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li,Jiang You,Hassine Moungla,Vincent Gauthier,Miguel Nunez-del-Prado,Hugo Alatrista-Salas*

Main category: cs.LG

TL;DR: A lightweight Spatial Neighbourhood Fusion (SPN) technique improves mobility flow forecasting in sparse data by aggregating signals from neighboring cells, reducing test MSE by up to 9.85%.


<details>
  <summary>Details</summary>
Motivation: Accurate human mobility modeling is crucial for understanding epidemic spread and deploying interventions, especially during public health crises like COVID-19.

Method: Proposes SPN, a model-agnostic technique that augments cell features with aggregated signals from immediate H3 neighbors, evaluated on NLinear, PatchTST, and K-U-Net models.

Result: SPN consistently enhances forecasting performance, achieving up to a 9.85% reduction in test MSE.

Conclusion: Spatial smoothing of sparse mobility signals is a simple yet effective method for robust spatio-temporal forecasting during health crises.

Abstract: Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [41] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw,Reid McCants,Jean-Marie Le Corre,Anna Iskhakova,Arsen S. Iskhakov*

Main category: cs.LG

TL;DR: This paper compiles and digitizes CHF data for uniform and non-uniform heating conditions, highlighting the limitations of classical correlations and neural networks, and sets the stage for advanced modeling in future benchmarks.


<details>
  <summary>Details</summary>
Motivation: To support Phase II of the OECD/NEA AI/ML CHF benchmark by providing curated datasets and baseline results for spatially varying power profiles, addressing the limitations of existing CHF prediction methods.

Method: Heating profiles were extracted from reports, interpolated onto a consistent mesh, validated via energy-balance checks, and encoded for benchmark use. Classical correlations and neural networks were tested for CHF prediction.

Result: Classical correlations show errors under uniform heating and degrade for non-uniform profiles. Neural networks trained on uniform data fail to generalize to non-uniform cases, emphasizing the need for models incorporating axial power distributions.

Conclusion: The study provides foundational data and insights for future work on transfer learning, uncertainty quantification, and design optimization in CHF benchmarks.

Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [42] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda,Sanjay Purushotham,Ratnaksha Lele,Vandana P. Janeja*

Main category: cs.LG

TL;DR: The paper introduces IDRIFTNET, a hybrid physics-driven deep learning model for forecasting iceberg trajectories, outperforming state-of-the-art models in accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate iceberg trajectory forecasting is challenging due to data scarcity and complex environmental influences, limiting deep learning models' effectiveness.

Method: IDRIFTNET combines analytical iceberg drift physics with a residual learning model and a rotate-augmented spectral neural network to improve predictions.

Result: IDRIFTNET achieves lower Final Displacement Error (FDE) and Average Displacement Error (ADE) compared to other models on Antarctic icebergs A23A and B22A.

Conclusion: IDRIFTNET effectively captures nonlinear iceberg drift dynamics, offering reliable trajectory forecasts under limited data and dynamic conditions.

Abstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [43] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

TL;DR: A novel neuron-centric model fusion algorithm integrates multiple neural networks into one, outperforming prior methods, especially in zero-shot and non-IID scenarios.


<details>
  <summary>Details</summary>
Motivation: Model fusion is challenging due to differences in internal representations (e.g., permutation invariance, random initialization, or varied training data distributions).

Method: Neuron-centric algorithms group intermediate neurons of parent models, using neuron attribution scores to guide fusion, and generalize to arbitrary layer types.

Result: The algorithms consistently outperform previous fusion techniques, particularly in zero-shot and non-IID scenarios.

Conclusion: The proposed neuron-centric fusion approach effectively combines multiple models, offering improved performance and versatility.

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [44] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

TL;DR: The paper proposes a data reduction strategy using Pointwise V-information (PVI) to enhance model training efficiency by selecting optimal instances, preserving performance while reducing data volume.


<details>
  <summary>Details</summary>
Motivation: To improve data quality and training efficiency in data-centric AI by identifying and utilizing the most informative instances in large datasets.

Method: Quantifies instance difficulty with PVI, filters low-difficulty instances statically, and uses progressive learning on PVI-sorted instances.

Result: Removing 10%-30% of data preserves classifier performance with minimal accuracy loss (0.0001%-0.76%). Progressive learning achieves a 0.8% accuracy gain.

Conclusion: The PVI-based strategy enhances model performance and training efficiency, with successful cross-lingual application to Chinese NLP tasks.

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [45] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

TL;DR: The paper compares 38 quality measures for graph classification, evaluates their theoretical properties, and introduces a clustering-based preprocessing step to improve performance.


<details>
  <summary>Details</summary>
Motivation: The lack of a focused comparison of quality measures for graph classification leads to the arbitrary use of popular measures without thorough evaluation.

Method: Theoretical characterization of 38 measures, empirical comparison using benchmark datasets, and introduction of a clustering-based preprocessing step.

Result: Some popular measures underperform, while the proposed preprocessing step reduces pattern count without sacrificing performance.

Conclusion: The study provides insights for selecting quality measures and demonstrates the effectiveness of clustering-based preprocessing in graph classification.

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [46] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

TL;DR: LiSER is a lightweight knowledge distillation framework for speech emotion recognition (SER) that reduces reliance on labeled data by leveraging unlabeled audio-visual data and teacher models.


<details>
  <summary>Details</summary>
Motivation: Voice interfaces benefit from SER, but labeled data collection is expensive. Multi-modal (audio-visual) cues improve SER, but labeled data scarcity is a challenge.

Method: LiSER uses large teacher models (speech and face representation) to distill knowledge into lightweight student models, utilizing unlabeled audio-visual data.

Result: Tested on RAVDESS and CREMA-D datasets, LiSER reduces the need for extensive labeled data in SER tasks.

Conclusion: LiSER offers a cost-effective solution for SER by minimizing labeled data dependency while leveraging multi-modal cues.

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [47] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

TL;DR: Smooth-Distill is a self-distillation framework for HAR and sensor placement detection, using a unified CNN (MTL-net) and a smoothed historical model as the teacher, reducing computational overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of traditional distillation methods and improve multitask learning for HAR and sensor placement detection.

Method: Uses MTL-net, a CNN processing accelerometer data with two task-specific outputs, and a smoothed historical model as the teacher for self-distillation.

Result: Outperforms alternatives in HAR and placement detection, showing stable convergence and reduced overfitting.

Conclusion: Smooth-Distill offers an efficient, accurate solution for multitask learning with accelerometer data, reducing computational costs for resource-constrained scenarios.

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [48] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar,Kunal Telangi*

Main category: cs.LG

TL;DR: FPG introduces fractional calculus into policy gradients for long-term temporal modeling, reducing variance and improving sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard policy gradients suffer from Markovian assumptions, high variance, and inefficient sampling. FPG addresses these by leveraging fractional calculus.

Method: FPG uses Caputo fractional derivatives to establish power-law temporal correlations and employs a recursive computation technique for fractional temporal-difference errors.

Result: FPG achieves asymptotic variance reduction (O(t^(-alpha))) and shows 35-68% sample efficiency gains with 24-52% variance reduction.

Conclusion: FPG offers a mathematically grounded method for long-range dependencies without added computational cost.

Abstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [49] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

TL;DR: The paper explores the dynamics of self-improvement in LLMs, introducing the solver-verifier gap concept to model performance evolution and predict outcomes early in training. It also examines the impact of external data.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLM performance evolves during self-improvement and the role of the solver-verifier gap is underexplored, motivating this theoretical and empirical study.

Method: Theoretical modeling of self-improvement dynamics using the solver-verifier gap, with empirical validation across LLMs and datasets. Also investigates external data's role.

Result: The solver-verifier gap effectively models self-improvement dynamics, enabling early prediction of outcomes. External data's impact is minimal in limited regimes.

Conclusion: The solver-verifier gap provides a robust framework for understanding and predicting self-improvement in LLMs, with implications for leveraging external data.

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [50] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: The paper explores how patch-based time series foundation models generalize the representation paradigm of language models, resolving the paradox of their cross-domain success despite time series data's dynamical nature.


<details>
  <summary>Details</summary>
Motivation: To understand why time series foundation models succeed in cross-domain transfer despite the inherent differences in dynamical systems reflected by time series data.

Method: Theoretical and experimental analysis of representation learning mechanisms and generalization capabilities of patch-based time series foundation models.

Result: Demonstrates that time series patches can be quantized into a discrete vocabulary with statistical properties akin to natural language, enabling robust representation and transfer.

Conclusion: Provides a theoretical foundation for understanding and improving the safety and reliability of large-scale time series foundation models.

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [51] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli,Heman Shakeri*

Main category: cs.LG

TL;DR: Using dynamical modes from CGM data improves meal event detection by capturing glucose variability patterns, enhancing accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve meal detection accuracy and interpretability of glucose dynamics in CGM data.

Method: Leverage dynamical modes as features to identify meal-related patterns and anomalies.

Result: Enhanced detection accuracy and robustness across diverse datasets.

Conclusion: The method outperforms traditional approaches, offering reliable performance in real-world applications.

Abstract: We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [52] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: FedHLM is a hybrid language model framework combining SLMs and LLMs, using federated learning to optimize token-level uncertainty thresholds, reducing LLM transmissions by 95% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the communication overhead of frequent LLM offloading in hybrid language models, especially in bandwidth-constrained settings.

Method: Integrates uncertainty-aware inference with federated learning, collaboratively learning thresholds and using P2P token resolution. Includes hierarchical model aggregation for refined routing policies.

Result: Reduces LLM transmissions by over 95% with negligible accuracy loss in large-scale news classification tasks.

Conclusion: FedHLM is scalable and efficient for edge-AI applications, balancing accuracy and communication efficiency.

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [53] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

TL;DR: The paper introduces IA-STGNN, a framework for modeling causal relationships between tactical strikes and strategic delays, outperforming baselines with improved accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address gaps in current strategic simulations, particularly in capturing intermediate variables in causal chains like resilience and negotiation windows.

Method: Proposes IA-STGNN, integrating graph attention, counterfactual simulation, and spatial intervention reconstruction. Uses multi-physics simulation data under NIST standards.

Result: IA-STGNN reduces MAE by 12.8% and boosts Top-5% accuracy by 18.4%, enhancing causal consistency and intervention stability.

Conclusion: IA-STGNN offers interpretable, structured AI support for high-level policy modeling, applicable in nuclear deterrence and diplomatic assessments.

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [54] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

TL;DR: GFEN is a novel framework for traffic speed prediction, combining topological spatiotemporal graph fusion and hybrid methods to improve accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current traffic prediction methods struggle with complexity, non-linearity, and static techniques for non-stationary data, limiting accuracy and adaptability.

Method: GFEN uses topological spatiotemporal graph fusion and a hybrid approach (k-th order difference and attention-based deep learning) to model multi-scale features and smooth data.

Result: GFEN outperforms state-of-the-art methods by 6.3% in accuracy and converges twice as fast as hybrid models.

Conclusion: GFEN enhances traffic prediction efficiency, demonstrating superior performance and adaptability.

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [55] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

TL;DR: pUniFind is a multimodal pre-trained model for proteomics, integrating peptide-spectrum scoring and de novo sequencing, outperforming traditional methods with improved sensitivity and modification coverage.


<details>
  <summary>Details</summary>
Motivation: To address the lack of unified scoring frameworks in deep learning for mass spectrometry data interpretation, aiming for improved sensitivity and modification coverage.

Method: pUniFind is trained on 100M spectra, using cross-modality prediction to align spectral and peptide data, and includes a quality control module.

Result: Achieves 42.6% more identified peptides in immunopeptidomics, 60% more PSMs than de novo methods, and recovers 38.5% additional peptides.

Conclusion: pUniFind establishes a scalable, unified deep learning framework for proteomics with enhanced sensitivity and interpretability.

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [56] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi,Pierre Latouche,Arnaud Guillin,Yan Bailly*

Main category: cs.LG

TL;DR: A framework for short-term occupational accident forecasting using safety inspections and binary time series models, with LSTM outperforming other methods (0.86 accuracy).


<details>
  <summary>Details</summary>
Motivation: To improve workplace safety by predicting high-risk periods using routine safety inspections, enabling proactive interventions.

Method: Uses binary time series modeling, sliding-window cross-validation, and compares machine learning algorithms (logistic regression, tree-based models, LSTM).

Result: LSTM achieves 0.86 balanced accuracy, outperforming other models in detecting high-risk periods.

Conclusion: The framework effectively converts inspection data into actionable weekly risk scores, aiding decision-makers in prioritizing safety measures.

Abstract: We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [57] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zaraté Pascale*

Main category: cs.LG

TL;DR: The paper compares data generation methods for optimizing firefighter resource allocation, evaluating their effectiveness using domain-specific and standard metrics.


<details>
  <summary>Details</summary>
Motivation: To improve firefighter response optimization by generating high-quality synthetic data for scenario simulations.

Method: Comparison of Random Sampling, Tabular Variational Autoencoders, GANs, Conditional Tabular GANs, and Diffusion Probabilistic Models, evaluated with domain-specific and standard metrics.

Result: The study highlights the challenges of generating synthetic data for highly unbalanced, non-Gaussian distributions and assesses methods' efficacy in capturing real-world complexities.

Conclusion: Domain-specific metrics are crucial for evaluating synthetic data quality in firefighting scenarios, as traditional metrics may not suffice.

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [58] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

Main category: cs.LG

TL;DR: DFReg is a physics-inspired regularization method for deep neural networks, using Density Functional Theory to encourage smooth and diverse weight distributions globally.


<details>
  <summary>Details</summary>
Motivation: Traditional regularization methods like Dropout or L2 decay lack global structural regularity. DFReg aims to address this by applying a physics-inspired functional penalty.

Method: DFReg draws from Density Functional Theory (DFT) to impose a functional penalty on the global distribution of weights, promoting smoothness and diversity without architectural changes.

Result: DFReg achieves global structural regularity in weight configurations, unlike traditional methods that rely on stochastic perturbations or local penalties.

Conclusion: DFReg offers a novel, physics-inspired approach to regularization, enhancing weight distribution without altering network architecture or relying on randomness.

Abstract: We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [59] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann,Patrick Bruendl,Huong Giang Nguyen,Joerg Franke*

Main category: cs.LG

TL;DR: A data-driven, interpretable fault detection method for industrial quality control combines machine learning, Shapley explanations, and domain-specific visualization, achieving high accuracy and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of conventional quality control (lack of adaptability) and black-box machine learning models (lack of interpretability) in industrial settings.

Method: Integrates supervised ML for fault classification, Shapley Additive Explanations for interpretability, and domain-specific visualization. Evaluated via perturbation analysis and expert assessment.

Result: Achieves 95.9% fault detection accuracy, with confirmed relevance and interpretability of explanations.

Conclusion: The approach enhances trust and interpretability in data-driven fault detection, suitable for industrial quality control.

Abstract: Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [60] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano,Ignacio Villanueva*

Main category: cs.LG

TL;DR: GNNs perform comparably to CNNs in wind energy forecasting, tested on three facilities with five years of data and NWP predictors.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of GNNs for wind energy forecasting compared to traditional CNN-based methods.

Method: Evaluated GNN architectures against CNN benchmarks using historical data from three wind power facilities and NWP variables as predictors.

Result: Certain GNN architectures achieved performance comparable to the best CNN-based models.

Conclusion: GNNs are a viable alternative to CNNs for wind energy forecasting, offering similar accuracy.

Abstract: We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [61] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum,Olivia Kilday,Emilio Salas,Bess Hagan,Reid Williams*

Main category: cs.LG

TL;DR: The paper explores text-to-level generation in tile-based games using diffusion models, addressing practical challenges like caption/level pairs and playable level generation. It compares results with other models and introduces a GUI for designers.


<details>
  <summary>Details</summary>
Motivation: To advance text-to-level generation in games, addressing gaps in using diffusion models and ensuring usability with practical solutions like automated captioning and playable level generation.

Method: Automatically assigns captions to levels, trains diffusion models with pretrained and simple transformer text encoders, and compares results with other models (unconditional diffusion, GAN, Five-Dollar Model, MarioGPT).

Result: The best diffusion model uses a simple transformer for text embedding, trains faster than complex encoders, and produces diverse, playable levels. A GUI for level construction is also introduced.

Conclusion: Simple transformer-based text embedding suffices for effective text-to-level generation, outperforming complex models, and the GUI enhances usability for designers.

Abstract: Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [62] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk,Fahad Kamran,Salar Abbaspourazad,Sean Jewell,Harsh Sharma,Yujie Li,Sinead Williamson,Nicholas J Foti,Joseph Futoma*

Main category: cs.LG

TL;DR: A foundation model for behavioral signals from wearable data improves health predictions, excelling in tasks like sleep prediction and showing potential for diverse health applications.


<details>
  <summary>Details</summary>
Motivation: Behavioral data from wearables is often more informative than low-level sensor data for health predictions, but foundation models have not been widely applied to such data.

Method: Developed foundation models using 2.5B hours of wearable data from 162K individuals, optimizing architectures and tokenization strategies.

Result: Strong performance on 57 health-related tasks, especially behavior-driven ones like sleep prediction, with further improvements when combined with raw sensor data.

Conclusion: Tailoring foundation models to wearable behavioral data is crucial and opens new possibilities for health applications.

Abstract: Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>


### [63] [PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction](https://arxiv.org/abs/2507.00230)
*Peilin He,James Joshi*

Main category: cs.LG

TL;DR: A novel Privacy-Preserving Federated Learning-based RDSN (PPFL-RDSN) framework is proposed for secure, high-quality image reconstruction from low-resolution inputs, addressing privacy risks and computational costs.


<details>
  <summary>Details</summary>
Motivation: Centralized training for image reconstruction poses privacy risks (e.g., data leakage) and high computational costs, necessitating a secure, decentralized solution.

Method: PPFL-RDSN combines Federated Learning, local differential privacy, and robust model watermarking to ensure data security and model authenticity without exposing raw data.

Result: PPFL-RDSN matches state-of-the-art centralized methods in performance, reduces computational burdens, and mitigates security and privacy vulnerabilities.

Conclusion: PPFL-RDSN is a practical, secure solution for privacy-preserving collaborative computer vision applications.

Abstract: Reconstructing high-quality images from low-resolution inputs using Residual
Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in
collaborative scenarios where centralized training poses significant privacy
risks, including data leakage and inference attacks, as well as high
computational costs. We propose a novel Privacy-Preserving Federated
Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image
reconstruction. PPFL-RDSN integrates Federated Learning (FL), local
differential privacy, and robust model watermarking techniques, ensuring data
remains secure on local devices, safeguarding sensitive information, and
maintaining model authenticity without revealing underlying data. Empirical
evaluations show that PPFL-RDSN achieves comparable performance to the
state-of-the-art centralized methods while reducing computational burdens, and
effectively mitigates security and privacy vulnerabilities, making it a
practical solution for secure and privacy-preserving collaborative computer
vision applications.

</details>


### [64] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: A hybrid framework combining ResNet and Transformer heatmaps improves interpretability and accuracy in safety-critical domains like healthcare and industrial monitoring.


<details>
  <summary>Details</summary>
Motivation: Addresses spatial-temporal misalignment in interpretability methods, where CNNs lack global context and Transformers miss localized precision, hindering actionable insights.

Method: Integrates gradient-weighted activation maps (ResNet) and Transformer attention rollout into a unified visualization for spatial-temporal alignment.

Result: Achieves 94.1% accuracy (F1 0.93) on ECG detection and RMSE = 0.28 kWh (R2 = 0.95) on energy prediction, outperforming baselines by 3.8-12.4%. NLP translates heatmaps into narratives (BLEU-4 0.586, ROUGE-L 0.650).

Conclusion: The framework bridges technical outputs and stakeholder understanding, offering scalable, transparent decision-making with causal fidelity and alignment.

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [65] [Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning](https://arxiv.org/abs/2507.00257)
*Davide Salaorni,Vincenzo De Paola,Samuele Delpero,Giovanni Dispoto,Paolo Bonetti,Alessio Russo,Giuseppe Calcagno,Francesco Trovò,Matteo Papini,Alberto Maria Metelli,Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: The paper introduces Gym4ReaL, a suite of realistic environments for RL, addressing real-world challenges like large state-action spaces and partial observability, and shows standard RL algorithms' competitiveness.


<details>
  <summary>Details</summary>
Motivation: Current RL benchmarks often overlook real-world complexities, limiting progress in deploying RL in practical applications.

Method: The authors develop Gym4ReaL, a diverse set of realistic environments to test RL algorithms under real-world conditions.

Result: Standard RL algorithms perform competitively against rule-based benchmarks in these realistic settings.

Conclusion: The work highlights the need for new RL methods to handle real-world complexities and demonstrates the potential of RL in practical scenarios.

Abstract: In recent years, \emph{Reinforcement Learning} (RL) has made remarkable
progress, achieving superhuman performance in a wide range of simulated
environments. As research moves toward deploying RL in real-world applications,
the field faces a new set of challenges inherent to real-world settings, such
as large state-action spaces, non-stationarity, and partial observability.
Despite their importance, these challenges are often underexplored in current
benchmarks, which tend to focus on idealized, fully observable, and stationary
environments, often neglecting to incorporate real-world complexities
explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive
suite of realistic environments designed to support the development and
evaluation of RL algorithms that can operate in real-world scenarios. The suite
includes a diverse set of tasks that expose algorithms to a variety of
practical challenges. Our experimental results show that, in these settings,
standard RL algorithms confirm their competitiveness against rule-based
benchmarks, motivating the development of new methods to fully exploit the
potential of RL to tackle the complexities of real-world tasks.

</details>


### [66] [Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning](https://arxiv.org/abs/2507.00259)
*Amr Abourayya,Jens Kleesiek,Bharat Rao,Michael Kamp*

Main category: cs.LG

TL;DR: FEDMOSAIC, a personalized federated learning method, addresses data heterogeneity by enabling clients to adaptively collaborate and trust others at the example level, outperforming existing PFL methods.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity in federated learning challenges model personalization, and many PFL methods fail to surpass local or centralized baselines due to rigid collaboration structures.

Method: FEDMOSAIC uses adaptive collaboration, where clients exchange predictions over shared unlabeled data, adjust loss weighting based on agreement, and contribute to global pseudo-labels proportionally to confidence.

Result: FEDMOSAIC outperforms state-of-the-art PFL methods in diverse non-IID settings and provides convergence guarantees.

Conclusion: Data-aware collaboration, as demonstrated by FEDMOSAIC, offers robust and effective personalization in federated learning.

Abstract: Data heterogeneity is a central challenge in federated learning, and
personalized federated learning (PFL) aims to address it by tailoring models to
each client's distribution. Yet many PFL methods fail to outperform local or
centralized baselines, suggesting a mismatch between the collaboration they
enforce and the structure of the data. We propose an approach based on adaptive
collaboration, where clients decide adaptively not only how much to rely on
others, but also whom to trust at the level of individual examples. We
instantiate this principle in FEDMOSAIC, a federated co-training method in
which clients exchange predictions over a shared unlabeled dataset. This
enables fine-grained trust decisions that are difficult to achieve with
parameter sharing alone. Each client adjusts its loss weighting based on the
agreement between private and public data, and contributes to global
pseudo-labels in proportion to its estimated per-example confidence.
Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across
diverse non-IID settings, and we provide convergence guarantees under standard
assumptions. Our results demonstrate the potential of data-aware collaboration
for robust and effective personalization.

</details>


### [67] [Examining Reject Relations in Stimulus Equivalence Simulations](https://arxiv.org/abs/2507.00265)
*Alexis Carrillo,Asieh Abolpour Mofrad,Anis Yazidi,Moises Betancort*

Main category: cs.LG

TL;DR: The study explores how reject relations affect stimulus equivalence (SE) in computational models, finding that neural networks may rely on associative learning rather than SE.


<details>
  <summary>Details</summary>
Motivation: To investigate the contentious role of reject relations in SE and assess whether artificial neural networks can demonstrate equivalence class formation.

Method: Used feedforward neural networks (FFNs), BERT, and GPT across 18 conditions in matching-to-sample (MTS) simulations, varying training structures, relation types, and negative comparison selections. A probabilistic agent served as a benchmark.

Result: Reject relations influenced performance; some agents achieved high accuracy but mirrored associative learning, suggesting neural networks may not truly demonstrate SE.

Conclusion: Artificial neural networks likely rely on associative strategies, highlighting the need for stricter criteria in computational models of equivalence.

Abstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE),
yet the potential of reject relations to disrupt the assessment of equivalence
class formation is contentious. This study investigates the role of reject
relations in the acquisition of stimulus equivalence using computational
models. We examined feedforward neural networks (FFNs), bidirectional encoder
representations from transformers (BERT), and generative pre-trained
transformers (GPT) across 18 conditions in matching-to-sample (MTS)
simulations. Conditions varied in training structure (linear series,
one-to-many, and many-to-one), relation type (select-only, reject-only, and
select-reject), and negative comparison selection (standard and biased). A
probabilistic agent served as a benchmark, embodying purely associative
learning. The primary goal was to determine whether artificial neural networks
could demonstrate equivalence class formation or whether their performance
reflected associative learning. Results showed that reject relations influenced
agent performance. While some agents achieved high accuracy on equivalence
tests, particularly with reject relations and biased negative comparisons, this
performance was comparable to the probabilistic agent. These findings suggest
that artificial neural networks, including transformer models, may rely on
associative strategies rather than SE. This underscores the need for careful
consideration of reject relations and more stringent criteria in computational
models of equivalence.

</details>


### [68] [Double Q-learning for Value-based Deep Reinforcement Learning, Revisited](https://arxiv.org/abs/2507.00275)
*Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: The paper introduces Deep Double Q-learning (DDQL) to address overestimation in deep RL, showing it outperforms Double DQN in reducing overestimation and improving performance across 57 Atari games.


<details>
  <summary>Details</summary>
Motivation: Overestimation in Q-learning and its deep variants (e.g., DQN, Double DQN) motivates the need for better solutions like DDQL.

Method: DDQL adapts Double Q-learning's core idea of using two Q-functions to de-correlate action-selection and evaluation, unlike Double DQN.

Result: DDQL reduces overestimation and outperforms Double DQN across 57 Atari games without extra hyperparameters.

Conclusion: DDQL is a performant and effective solution for overestimation in deep RL, with potential for further study in network architecture and training strategies.

Abstract: Overestimation is pervasive in reinforcement learning (RL), including in
Q-learning, which forms the algorithmic basis for many value-based deep RL
algorithms. Double Q-learning is an algorithm introduced to address
Q-learning's overestimation by training two Q-functions and using both to
de-correlate action-selection and action-evaluation in bootstrap targets.
Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks
(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.
However, Double DQN only loosely adapts Double Q-learning, forgoing the
training of two different Q-functions that bootstrap off one another. In this
paper, we study algorithms that adapt this core idea of Double Q-learning for
value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our
aim is to understand whether DDQL exhibits less overestimation than Double DQN
and whether performant instantiations of DDQL exist. We answer both questions
affirmatively, demonstrating that DDQL reduces overestimation and outperforms
Double DQN in aggregate across 57 Atari 2600 games, without requiring
additional hyperparameters. We also study several aspects of DDQL, including
its network architecture, replay ratio, and minibatch sampling strategy.

</details>


### [69] [Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations](https://arxiv.org/abs/2507.00301)
*Harsh Sharma,Juan Diego Draxl Giannoni,Boris Kramer*

Main category: cs.LG

TL;DR: Structure-preserving Lift & Learn method learns reduced-order models for nonlinear PDEs with conservation laws using energy-quadratization and hybrid learning, ensuring physics-respecting results.


<details>
  <summary>Details</summary>
Motivation: To develop a method for learning reduced-order models of nonlinear PDEs that preserves structural properties like conservation laws, improving accuracy and computational efficiency.

Method: Uses energy-quadratization to transform PDEs into quadratic lifted systems, derives reduced terms analytically, and learns linear operators via constrained optimization.

Result: Produces efficient, physics-respecting quadratic reduced-order models, demonstrated on wave, sine-Gordon, and Klein-Gordon-Zakharov equations.

Conclusion: The method is competitive with state-of-the-art techniques in accuracy and efficiency, generalizable across various PDEs.

Abstract: This work presents structure-preserving Lift & Learn, a scientific machine
learning method that employs lifting variable transformations to learn
structure-preserving reduced-order models for nonlinear partial differential
equations (PDEs) with conservation laws. We propose a hybrid learning approach
based on a recently developed energy-quadratization strategy that uses
knowledge of the nonlinearity at the PDE level to derive an equivalent
quadratic lifted system with quadratic system energy. The lifted dynamics
obtained via energy quadratization are linear in the old variables, making
model learning very effective in the lifted setting. Based on the lifted
quadratic PDE model form, the proposed method derives quadratic reduced terms
analytically and then uses those derived terms to formulate a constrained
optimization problem to learn the remaining linear reduced operators in a
structure-preserving way. The proposed hybrid learning approach yields
computationally efficient quadratic reduced-order models that respect the
underlying physics of the high-dimensional problem. We demonstrate the
generalizability of quadratic models learned via the proposed
structure-preserving Lift & Learn method through three numerical examples: the
one-dimensional wave equation with exponential nonlinearity, the
two-dimensional sine-Gordon equation, and the two-dimensional
Klein-Gordon-Zakharov equations. The numerical results show that the proposed
learning approach is competitive with the state-of-the-art structure-preserving
data-driven model reduction method in terms of both accuracy and computational
efficiency.

</details>


### [70] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang,Runlong Li,Xiaoxiang Liang,Xinhao Yang,Tian Su,Bo Liu,Yan Zhou*

Main category: cs.LG

TL;DR: MamNet integrates time-domain and frequency-domain modeling for network traffic prediction and anomaly detection, outperforming recent models by 2-4% in accuracy, recall, and F1-Score.


<details>
  <summary>Details</summary>
Motivation: Abnormal network traffic fluctuations signal security threats or failures, necessitating efficient prediction and detection methods.

Method: MamNet combines Mamba module (time-domain) and Fourier Transform (frequency-domain) with multi-scale feature fusion.

Result: MamNet excels in accuracy, recall, and F1-Score on UNSW-NB15 and CAIDA datasets, especially for complex traffic patterns.

Conclusion: MamNet effectively detects anomalies across time scales, with future potential for optimization using external event data.

Abstract: The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [71] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: AutoDS introduces a method for open-ended autonomous scientific discovery using Bayesian surprise and Monte Carlo tree search, outperforming competitors in generating surprising discoveries.


<details>
  <summary>Details</summary>
Motivation: Current AI-driven scientific discovery relies on human-specified questions or flawed heuristics. AutoDS aims to accelerate discovery by letting AI drive exploration using Bayesian surprise.

Method: AutoDS employs Bayesian surprise to quantify epistemic shifts and uses Monte Carlo tree search with progressive widening to explore nested hypotheses efficiently.

Result: AutoDS outperforms competitors, producing 5–29% more surprising discoveries across 21 datasets. Two-thirds of its findings were surprising to domain experts.

Conclusion: AutoDS represents a significant advancement in open-ended autonomous scientific discovery by effectively leveraging Bayesian surprise and efficient exploration.

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [72] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [73] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin,Ashutosh Singh,Deniz Erdogmus,Georgios Stratis,Lisa Feldman Barrett*

Main category: cs.LG

TL;DR: The paper challenges the assumption that emotion categories are biologically and psychologically distinct, showing significant variation in brain patterns when reanalyzing data with minimal assumptions.


<details>
  <summary>Details</summary>
Motivation: To question the widely held assumption in emotion science that folk emotion categories form distinct biological and psychological types, and to demonstrate how starting assumptions influence scientific conclusions.

Method: Reanalyzed data from a typologically-guided study using an alternative approach that treats emotion categories as variable, situated instances, with minimal assumptions about data structure.

Result: Found significant variation in brain patterns across individuals, contradicting the original study's mappings and supporting the alternative view.

Conclusion: Scientific conclusions are shaped by starting assumptions, and hypotheses should be tested with multiple analytic methods to ensure robustness.

Abstract: In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [74] [Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems](https://arxiv.org/abs/2507.00358)
*Yilie Huang,Xun Yu Zhou*

Main category: cs.LG

TL;DR: The paper introduces an adaptive exploration mechanism for reinforcement learning in continuous-time stochastic LQ control problems, improving efficiency and matching sublinear regret bounds without extensive tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods use constant or deterministic exploration schedules, requiring heavy tuning and ignoring learning progress. The goal is to enhance learning efficiency with adaptive exploration.

Method: Proposes a model-free, data-driven approach where entropy regularization (critic) and policy variance (actor) are adaptively adjusted based on learning progress.

Result: Achieves sublinear regret matching best-known model-free results, with numerical experiments showing faster convergence and better regret performance than non-adaptive methods.

Conclusion: Adaptive exploration boosts efficiency and performance in RL for LQ control problems, outperforming fixed-schedule methods with minimal tuning.

Abstract: We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.

</details>


### [75] [MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE](https://arxiv.org/abs/2507.00390)
*Geng Zhang,Yuxuan Han,Yuxuan Lou,Wangbo Zhao,Yiqi Zhang,Yang You*

Main category: cs.LG

TL;DR: MoNE is a novel expert pruning method for Mixture-of-Experts models, replacing redundant experts with lightweight novices to reduce memory overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Deploying MoE-based models incurs high memory costs, and existing pruning methods show suboptimal performance and instability.

Method: MoNE evaluates expert redundancy using access frequency and output variance, pruning low-usage, stable-output experts and replacing them with lightweight novices.

Result: MoNE outperforms baselines with minimal accuracy degradation, improving zero-shot accuracy by up to 3.61 under 50% pruning.

Conclusion: MoNE is an effective and robust method for compressing MoE models, balancing performance and memory efficiency.

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
by activating only a subset of experts per input token. However, deploying
MoE-based models incurs significant memory overhead due to the need to retain
all experts in memory. While structured pruning is promising to reduce memory
costs, existing methods often show suboptimal performance and unstable
degradation in three dimensions: model architectures, calibration data sources,
and calibration sample sizes. This paper proposes
Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that
replaces redundant experts with lightweight novices to achieve effective and
robust model compression. MoNE evaluates expert redundancy based on two
metrics: access frequency and output variance. Experts exhibiting low usage and
stable outputs are pruned and replaced with lightweight novices-unbiased
estimations of their original outputs-minimizing performance degradation.
Extensive experiments demonstrate that MoNE consistently outperforms baseline
methods with minimal accuracy degradation across the three dimensions,
confirming its effectiveness and robustness. Notably, it improves the average
zero shot accuracy across nine downstream tasks by up to 2.71 under 25\%
pruning ratio and 3.61 under 50\% pruning. The code is available at
https://github.com/zxgx/mode-pd.

</details>


### [76] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang,Shenggan Cheng,Xuanlei Zhao,Ziming Liu,Yang You*

Main category: cs.LG

TL;DR: HelixPipe improves pipeline parallelism for long-sequence transformer training by optimizing attention computation and memory usage, achieving better throughput and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing pipeline parallelisms perform poorly with long transformer sequences due to quadratic attention computation and high memory overhead.

Method: HelixPipe introduces attention parallel partition, a two-fold micro batch schedule, and recomputation techniques to reduce pipeline bubbles and balance memory.

Result: HelixPipe outperforms baselines, achieving a 26% speedup for a 7B model with 128k sequence length on 64 GPUs.

Conclusion: HelixPipe is effective for long-sequence transformer training, offering scalability and efficiency gains.

Abstract: As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


### [77] [Diffusion Disambiguation Models for Partial Label Learning](https://arxiv.org/abs/2507.00411)
*Jinfu Fan,Xiaohui Zhong,Kangrui Ren,Jiangnan Li,Linqing Huang*

Main category: cs.LG

TL;DR: The paper proposes DDMP, a diffusion disambiguation model for partial label learning (PLL), leveraging diffusion models to denoise ambiguous labels and improve classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning from ambiguous labels in machine learning by exploring diffusion models' potential to refine labels iteratively.

Method: Reformulates label disambiguation as a generative process, using diffusion models to refine labels. Introduces pseudo-clean labels and a transition-aware matrix to estimate ground-truth labels dynamically.

Result: DDMP demonstrates improved performance in PLL tasks, effectively refining ambiguous labels.

Conclusion: DDMP is a promising approach for PLL, leveraging diffusion models to enhance label disambiguation and classification.

Abstract: Learning from ambiguous labels is a long-standing problem in practical
machine learning applications. The purpose of \emph{partial label learning}
(PLL) is to identify the ground-truth label from a set of candidate labels
associated with a given instance. Inspired by the remarkable performance of
diffusion models in various generation tasks, this paper explores their
potential to denoise ambiguous labels through the reverse denoising process.
Therefore, this paper reformulates the label disambiguation problem from the
perspective of generative models, where labels are generated by iteratively
refining initial random guesses. This perspective enables the diffusion model
to learn how label information is generated stochastically. By modeling the
generation uncertainty, we can use the maximum likelihood estimate of the label
for classification inference. However, such ambiguous labels lead to a mismatch
between instance and label, which reduces the quality of generated data. To
address this issue, this paper proposes a \emph{diffusion disambiguation model
for PLL} (DDMP), which first uses the potential complementary information
between instances and labels to construct pseudo-clean labels for initial
diffusion training. Furthermore, a transition-aware matrix is introduced to
estimate the potential ground-truth labels, which are dynamically updated
during the diffusion generation. During training, the ground-truth label is
progressively refined, improving the classifier. Experiments show the advantage
of the DDMP and its suitability for PLL.

</details>


### [78] [A Recipe for Causal Graph Regression: Confounding Effects Revisited](https://arxiv.org/abs/2507.00440)
*Yujia Yin,Tianyi Qu,Zihao Wang,Yifan Chen*

Main category: cs.LG

TL;DR: The paper introduces causal graph regression (CGR) to address the overlooked challenge of regression tasks in causal graph learning, adapting classification-specific techniques for regression using contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Regression tasks in graph learning are more challenging than classification but have been neglected in causal graph learning (CGL) research. The paper aims to fill this gap.

Method: The authors reshape confounding effect processing from CGL for classification, generalize causal intervention techniques for regression, and use contrastive learning.

Result: Experiments on graph OOD benchmarks confirm the effectiveness of the proposed CGR approach.

Conclusion: The work successfully extends causal graph learning to regression tasks, validated by empirical results.

Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen
to be a promising approach for improving the generalizability of graph neural
networks under out-of-distribution (OOD) scenarios. However, the empirical
successes of CGL techniques are mostly exemplified in classification settings,
while regression tasks, a more challenging setting in graph learning, are
overlooked. We thus devote this work to tackling causal graph regression (CGR);
to this end we reshape the processing of confounding effects in existing CGL
studies, which mainly deal with classification. Specifically, we reflect on the
predictive power of confounders in graph-level regression, and generalize
classification-specific causal intervention techniques to regression through a
lens of contrastive learning. Extensive experiments on graph OOD benchmarks
validate the efficacy of our proposals for CGR. The model implementation and
the code are provided on https://github.com/causal-graph/CGR.

</details>


### [79] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: TarFlowLM introduces a continuous latent space for language modeling using autoregressive normalizing flows, offering flexibility in context, generation, and dependencies.


<details>
  <summary>Details</summary>
Motivation: To explore beyond the limitations of discrete tokens and unidirectional context in autoregressive models by shifting to a continuous latent space.

Method: Proposes TarFlowLM, a transformer-based autoregressive normalizing flow framework for continuous representations, featuring bi-directional context, block-wise generation, and multi-pass processes.

Result: Demonstrates strong likelihood performance on benchmarks and showcases flexible modeling capabilities.

Conclusion: TarFlowLM provides a novel, flexible approach to language modeling with theoretical and practical advantages over traditional discrete models.

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [80] [Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design](https://arxiv.org/abs/2507.00445)
*Xingyu Su,Xiner Li,Masatoshi Uehara,Sunwoo Kim,Yulai Zhao,Gabriele Scalia,Ehsan Hajiramezanali,Tommaso Biancalani,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: A distillation-based framework fine-tunes diffusion models for reward-guided biomolecular design, improving stability and efficiency over RL methods.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require diffusion models to optimize for non-differentiable rewards, but RL methods are unstable and inefficient.

Method: Proposes iterative distillation: collects off-policy data, simulates reward-based soft-optimal policies, and updates the model via KL divergence minimization.

Result: Outperforms RL-based methods in stability, sample efficiency, and reward optimization across protein, small molecule, and DNA design tasks.

Conclusion: The framework effectively optimizes diffusion models for arbitrary rewards, advancing biomolecular design.

Abstract: We address the problem of fine-tuning diffusion models for reward-guided
generation in biomolecular design. While diffusion models have proven highly
effective in modeling complex, high-dimensional data distributions, real-world
applications often demand more than high-fidelity generation, requiring
optimization with respect to potentially non-differentiable reward functions
such as physics-based simulation or rewards based on scientific knowledge.
Although RL methods have been explored to fine-tune diffusion models for such
objectives, they often suffer from instability, low sample efficiency, and mode
collapse due to their on-policy nature. In this work, we propose an iterative
distillation-based fine-tuning framework that enables diffusion models to
optimize for arbitrary reward functions. Our method casts the problem as policy
distillation: it collects off-policy data during the roll-in phase, simulates
reward-based soft-optimal policies during roll-out, and updates the model by
minimizing the KL divergence between the simulated soft-optimal policy and the
current model policy. Our off-policy formulation, combined with KL divergence
minimization, enhances training stability and sample efficiency compared to
existing RL-based methods. Empirical results demonstrate the effectiveness and
superior reward optimization of our approach across diverse tasks in protein,
small molecule, and regulatory DNA design.

</details>


### [81] [Best Agent Identification for General Game Playing](https://arxiv.org/abs/2507.00451)
*Matthew Stephenson,Alex Newcombe,Eric Piette,Dennis Soemers*

Main category: cs.LG

TL;DR: The paper introduces Optimistic-WS, a method for identifying the best algorithm for sub-tasks in multi-problem domains using multi-armed bandits, showing improved performance in general game domains.


<details>
  <summary>Details</summary>
Motivation: To accurately and efficiently identify the best-performing algorithm for each sub-task in multi-problem domains, particularly in general game frameworks.

Method: Treats the problem as best arm identification in multi-armed bandits, using an optimistic selection process based on the Wilson score interval (Optimistic-WS).

Result: Demonstrates substantial performance improvement in average simple regret compared to previous methods, tested on GVGAI and Ludii game domains.

Conclusion: Optimistic-WS enhances agent evaluation quality in general game frameworks and other high-runtime multi-task domains.

Abstract: We present an efficient and generalised procedure to accurately identify the
best performing algorithm for each sub-task in a multi-problem domain. Our
approach treats this as a set of best arm identification problems for
multi-armed bandits, where each bandit corresponds to a specific task and each
arm corresponds to a specific algorithm or agent. We propose an optimistic
selection process based on the Wilson score interval (Optimistic-WS) that ranks
each arm across all bandits in terms of their potential regret reduction. We
evaluate the performance of Optimistic-WS on two of the most popular general
game domains, the General Video Game AI (GVGAI) framework and the Ludii general
game playing system, with the goal of identifying the highest performing agent
for each game within a limited number of trials. Compared to previous best arm
identification algorithms for multi-armed bandits, our results demonstrate a
substantial performance improvement in terms of average simple regret. This
novel approach can be used to significantly improve the quality and accuracy of
agent evaluation procedures for general game frameworks, as well as other
multi-task domains with high algorithm runtimes.

</details>


### [82] [Diversity Conscious Refined Random Forest](https://arxiv.org/abs/2507.00467)
*Sijan Bhattarai,Saurav Bhandari,Girija Bhusal,Saroj Shakya,Tapendra Pandey*

Main category: cs.LG

TL;DR: A refined Random Forest (RF) classifier is proposed to reduce model redundancy and improve accuracy by dynamically growing trees on informative features and enforcing diversity through clustering.


<details>
  <summary>Details</summary>
Motivation: Standard RF relies on many trees and all features, leading to high inference costs and redundancy. The goal is to optimize performance by focusing on informative features and uncorrelated trees.

Method: The proposed method iteratively removes uninformative features, determines the optimal number of new trees, and uses correlation-based clustering to eliminate redundant trees.

Result: Experiments on 8 benchmark datasets show the refined RF achieves higher accuracy than standard RF with the same number of trees.

Conclusion: The refined RF classifier effectively reduces redundancy and improves classification accuracy compared to the standard RF.

Abstract: Random Forest (RF) is a widely used ensemble learning technique known for its
robust classification performance across diverse domains. However, it often
relies on hundreds of trees and all input features, leading to high inference
cost and model redundancy. In this work, our goal is to grow trees dynamically
only on informative features and then enforce maximal diversity by clustering
and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest
Classifier that iteratively refines itself by first removing the least
informative features and then analytically determines how many new trees should
be grown, followed by correlation-based clustering to remove redundant trees.
The classification accuracy of our model was compared against the standard RF
on the same number of trees. Experiments on 8 multiple benchmark datasets,
including binary and multiclass datasets, demonstrate that the proposed model
achieves improved accuracy compared to standard RF.

</details>


### [83] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: The paper addresses the challenge of efficient long-context modeling in NLP by improving state-space models (SSMs) with a novel synthetic task (joint recall) and a hybrid solution (HAX) combining SSMs with sparse attention.


<details>
  <summary>Details</summary>
Motivation: Current SSMs struggle with long-range dependencies, and the synthetic task (associative recall) used to evaluate them is inadequate for real-world complexities.

Method: Proposes joint recall as a more representative task, integrates SSMs with Context-Dependent Sparse Attention (CDSA), and introduces HAX for practical NLP applications.

Result: HAX outperforms SSM baselines and SSMs with context-independent sparse attention (CISA) on synthetic and real-world benchmarks.

Conclusion: The hybrid approach (HAX) effectively bridges the gap between theoretical limitations and practical performance in long-context modeling.

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [84] [PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning](https://arxiv.org/abs/2507.00485)
*Weiran Guo,Guanjun Liu,Ziyuan Zhou,Ling Wang*

Main category: cs.LG

TL;DR: The paper introduces a backdoor attack framework (PNAct) for Safe Reinforcement Learning (Safe RL), demonstrating its feasibility and risks.


<details>
  <summary>Details</summary>
Motivation: Safe RL ensures agents adhere to safety constraints, but this paper identifies its vulnerability to backdoor attacks, which can manipulate agents into unsafe actions.

Method: The authors propose PNAct, a framework using Positive and Negative Action samples to implant backdoors, and design an attack algorithm.

Result: Experiments confirm the effectiveness of the backdoor attack, highlighting risks in Safe RL.

Conclusion: The study underscores the feasibility of backdoor attacks in Safe RL, calling for attention to such vulnerabilities.

Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact
with an environment to maximize rewards. Building on this foundation, Safe
Reinforcement Learning (Safe RL) incorporates a cost metric alongside the
reward metric, ensuring that agents adhere to safety constraints during
decision-making. In this paper, we identify that Safe RL is vulnerable to
backdoor attacks, which can manipulate agents into performing unsafe actions.
First, we introduce the relevant concepts and evaluation metrics for backdoor
attacks in Safe RL. It is the first attack framework in the Safe RL field that
involves both Positive and Negative Action sample (PNAct) is to implant
backdoors, where positive action samples provide reference actions and negative
action samples indicate actions to be avoided. We theoretically point out the
properties of PNAct and design an attack algorithm. Finally, we conduct
experiments to evaluate the effectiveness of our proposed backdoor attack
framework, evaluating it with the established metrics. This paper highlights
the potential risks associated with Safe RL and underscores the feasibility of
such attacks. Our code and supplementary material are available at
https://github.com/azure-123/PNAct.

</details>


### [85] [Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling](https://arxiv.org/abs/2507.00453)
*Ankit Kashyap*

Main category: cs.LG

TL;DR: A Transformer model with global attention, chunked local attention, and a gated FIFO memory mechanism for efficient long-context language modeling.


<details>
  <summary>Details</summary>
Motivation: To handle both short-range and long-range dependencies efficiently without quadratic attention cost.

Method: Combines global attention with chunked local attention and a gated FIFO memory mechanism, using rotary positional encoding for disentangled positional signals.

Result: A lightweight, extensible model suitable for tasks like dialogue modeling, code completion, and document understanding.

Conclusion: The architecture provides an efficient and modular solution for long-context language modeling.

Abstract: We present a Transformer architecture for long-context language modeling that
combines global attention with two biologically inspired components: chunked
local attention and a gated FIFO memory mechanism. This unified attention block
allows the model to efficiently handle both short-range and long-range
dependencies without increasing attention cost quadratically. The memory module
persistently stores past token representations using a gated update mechanism
inspired by recurrent networks. Rotary positional encoding is applied per
attention head to enable directionally disentangled, scale-invariant positional
signals. The architecture is implemented entirely from scratch in PyTorch, with
no reliance on high-level libraries, enabling transparent and modular
experimentation. Our model offers a lightweight and extensible design for tasks
such as dialogue modeling, code completion, and document understanding.

</details>


### [86] [Quantum Circuit Structure Optimization for Quantum Reinforcement Learning](https://arxiv.org/abs/2507.00589)
*Seok Bin Son,Joongheon Kim*

Main category: cs.LG

TL;DR: QRL-NAS optimizes PQC structures in quantum reinforcement learning, outperforming fixed-circuit QRL.


<details>
  <summary>Details</summary>
Motivation: RL struggles with high-dimensional spaces; QRL leverages quantum computing but lacks optimized PQC structures.

Method: Proposes QRL-NAS, integrating quantum neural architecture search to optimize PQC structures.

Result: QRL-NAS achieves higher rewards than fixed-circuit QRL, proving its effectiveness.

Conclusion: QRL-NAS enhances QRL performance by optimizing PQC structures, offering practical utility.

Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through
environmental interaction. However, RL suffers from reduced learning efficiency
due to the curse of dimensionality in high-dimensional spaces. Quantum
reinforcement learning (QRL) addresses this issue by leveraging superposition
and entanglement in quantum computing, allowing efficient handling of
high-dimensional problems with fewer resources. QRL combines quantum neural
networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as
the core computational module. The PQC performs linear and nonlinear
transformations through gate operations, similar to hidden layers in classical
neural networks. Previous QRL studies, however, have used fixed PQC structures
based on empirical intuition without verifying their optimality. This paper
proposes a QRL-NAS algorithm that integrates quantum neural architecture search
(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that
QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its
effectiveness and practical utility.

</details>


### [87] [Residual Reward Models for Preference-based Reinforcement Learning](https://arxiv.org/abs/2507.00611)
*Chenyang Cao,Miguel Rogel-García,Mohamed Nabail,Xueqian Wang,Nicholas Rhinehart*

Main category: cs.LG

TL;DR: The paper proposes a Residual Reward Model (RRM) to improve Preference-based Reinforcement Learning (PbRL) by combining prior knowledge with learned rewards, enhancing performance and convergence speed.


<details>
  <summary>Details</summary>
Motivation: PbRL avoids heuristic reward design but suffers from slow convergence due to reward model training challenges. Leveraging prior knowledge can address this.

Method: RRM splits the true reward into a prior reward (pre-defined or from IRL) and a learned reward (trained with preferences). Evaluated on Meta-World and a real robot.

Result: RRM significantly improves PbRL performance, works with various prior rewards, and accelerates policy learning on real robots.

Conclusion: RRM effectively leverages prior knowledge to enhance PbRL, achieving faster convergence and better performance in simulated and real-world tasks.

Abstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.

</details>


### [88] [Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization](https://arxiv.org/abs/2507.00480)
*Kiyoung Om,Kyuil Sim,Taeyoung Yun,Hyeongyu Kang,Jinkyoo Park*

Main category: cs.LG

TL;DR: A new framework for high-dimensional constrained black-box optimization using flow-based models and posterior inference to address scalability and mode collapse issues.


<details>
  <summary>Details</summary>
Motivation: High-dimensional constrained optimization is challenging due to hard-to-find feasible regions and limitations of existing methods like Bayesian optimization and generative models.

Method: Proposes a two-stage approach: training flow-based models for data distribution and surrogate models, followed by posterior inference for candidate selection.

Result: Empirically shows superior performance on synthetic and real-world tasks, addressing multi-modality and scalability.

Conclusion: The method effectively optimizes high-dimensional constrained problems, outperforming existing approaches.

Abstract: Optimizing high-dimensional black-box functions under black-box constraints
is a pervasive task in a wide range of scientific and engineering problems.
These problems are typically harder than unconstrained problems due to
hard-to-find feasible regions. While Bayesian optimization (BO) methods have
been developed to solve such problems, they often struggle with the curse of
dimensionality. Recently, generative model-based approaches have emerged as a
promising alternative for constrained optimization. However, they suffer from
poor scalability and are vulnerable to mode collapse, particularly when the
target distribution is highly multi-modal. In this paper, we propose a new
framework to overcome these challenges. Our method iterates through two stages.
First, we train flow-based models to capture the data distribution and
surrogate models that predict both function values and constraint violations
with uncertainty quantification. Second, we cast the candidate selection
problem as a posterior inference problem to effectively search for promising
candidates that have high objective values while not violating the constraints.
During posterior inference, we find that the posterior distribution is highly
multi-modal and has a large plateau due to constraints, especially when
constraint feedback is given as binary indicators of feasibility. To mitigate
this issue, we amortize the sampling from the posterior distribution in the
latent space of flow-based models, which is much smoother than that in the data
space. We empirically demonstrate that our method achieves superior performance
on various synthetic and real-world constrained black-box optimization tasks.
Our code is publicly available \href{https://github.com/umkiyoung/CiBO}{here}.

</details>


### [89] [Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models](https://arxiv.org/abs/2507.00653)
*Yilun Zhang*

Main category: cs.LG

TL;DR: The paper introduces the Cognitive Load-Aware Inference (CLAI) framework, applying Cognitive Load Theory to optimize LLM inference, reducing token usage by up to 45% without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: High computational costs of LLM inference and lack of cognitive theory-based optimization strategies.

Method: CLAI framework quantifies cognitive load metrics (ICL_LLM, ECL_LLM, GCL_LLM) and offers two implementations: CLAI-Prompt (zero-shot) and CLAI-Tune (fine-tuned).

Result: Significant token reduction (up to 45%) in benchmarks without accuracy loss; CLAI-Tune autonomously decomposes problems.

Conclusion: Emulating brain's resource management improves LLM efficiency and capability.

Abstract: The escalating computational costs of Large Language Model (LLM) inference
have become a critical barrier to their widespread and sustainable deployment.
While existing optimization strategies are effective, they are predominantly
based on statistical heuristics or architectural modifications, lacking a
guiding cognitive theory to manage the inference process itself. This paper
aims to bridge this gap by introducing a novel paradigm: the Cognitive
Load-Aware Inference (CLAI) framework, which operationalizes principles from
Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize
the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and
Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,
and $GCL_{LLM}$), thereby reframing the inference process as a cognitive
economics optimization problem: based on the intrinsic complexity of a problem
($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically
allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two
implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM
through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a
fine-tuned model that internalizes these principles for spontaneous cognitive
economy. Across a range of benchmarks in complex reasoning, long-context
question answering, and code generation, our methods achieve significant
reductions in token consumption (up to 45\%) without sacrificing accuracy.
Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose
difficult problems, a key characteristic of human expert cognition. This work
demonstrates that by emulating the brain's resource management strategies, we
can build more efficient, robust, and capable artificial intelligence systems.

</details>


### [90] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh,Khai Le-Duc,Anh Dao,Bach Phan Tat,Chris Ngo,Duy M. H. Nguyen,Nguyen X. Khanh,Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: Audio-3DVG integrates audio and spatial info for 3D visual grounding, outperforming prior methods by decomposing the task into object mention detection and audio-guided attention.


<details>
  <summary>Details</summary>
Motivation: Prior work focuses on text, but spoken language (audio) is underexplored. Advances in ASR and speech representation learning enable this integration.

Method: Decomposes task into Object Mention Detection (multi-label classification) and Audio-Guided Attention module for better scene reasoning.

Result: Achieves state-of-the-art in audio-based grounding and competes with text-based methods.

Conclusion: Integrating spoken language into 3D vision tasks shows promise, as demonstrated by Audio-3DVG.

Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [91] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada,Guillaume Salha-Galvan,Romain Hennequin,Théo Bontempelli,Thomas Bouabça,Tristan Cazenave*

Main category: cs.LG

TL;DR: vMF-exp is a scalable exploration method for large action sets in RL, using hyperspherical embeddings and von Mises-Fisher sampling, outperforming Boltzmann Exploration in scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing scalability issues in exploring large action sets in reinforcement learning, particularly where Boltzmann Exploration (B-exp) fails due to computational inefficiency.

Method: Samples state embeddings using von Mises-Fisher distribution and explores nearest neighbors, enabling scalability to unlimited actions.

Result: Theoretically matches B-exp's exploration probability while being scalable; validated on simulated, real-world, and industrial recommender system data.

Conclusion: vMF-exp is a practical, scalable alternative to B-exp for large action sets with hyperspherical embeddings, proven effective in diverse settings.

Abstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [92] [NN-Former: Rethinking Graph Structure in Neural Architecture Representation](https://arxiv.org/abs/2507.00880)
*Ruihan Xu,Haokui Zhang,Yaowei Wang,Wei Zeng,Shiliang Zhang*

Main category: cs.LG

TL;DR: A novel neural predictor combines GNNs and transformers to address their individual shortcomings, focusing on sibling nodes in DAG topology for improved accuracy and latency prediction.


<details>
  <summary>Details</summary>
Motivation: The need for efficient neural network design and deployment drives the development of better predictors for attributes like accuracy and latency. Existing methods (GNNs and transformers) have limitations in feature representation and generalization.

Method: Proposes a hybrid predictor using GNNs and transformers, introducing a sibling-aware token mixer and a bidirectional graph isomorphism feed-forward network.

Result: The approach achieves strong performance in accuracy and latency prediction, demonstrating effectiveness in learning DAG topology.

Conclusion: The hybrid method overcomes limitations of GNNs and transformers, offering valuable insights for neural architecture representation.

Abstract: The growing use of deep learning necessitates efficient network design and
deployment, making neural predictors vital for estimating attributes such as
accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers
have shown promising performance in representing neural architectures. However,
each of both methods has its disadvantages. GNNs lack the capabilities to
represent complicated features, while transformers face poor generalization
when the depth of architecture grows. To mitigate the above issues, we rethink
neural architecture topology and show that sibling nodes are pivotal while
overlooked in previous research. We thus propose a novel predictor leveraging
the strengths of GNNs and transformers to learn the enhanced topology. We
introduce a novel token mixer that considers siblings, and a new channel mixer
named bidirectional graph isomorphism feed-forward network. Our approach
consistently achieves promising performance in both accuracy and latency
prediction, providing valuable insights for learning Directed Acyclic Graph
(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.

</details>


### [93] [Foundation Models for Clinical Records at Health System Scale](https://arxiv.org/abs/2507.00574)
*Haresh Rengaraj Rajamohan,Xiang Gao,Weicheng Zhu,Shih-Lun Huang,Long Chen,Kyunghyun Cho,Cem M. Deniz,Narges Razavian*

Main category: cs.LG

TL;DR: A novel generative pretraining strategy for EHR data using next-visit prediction, addressing pitfalls in evaluation and demonstrating competitive performance in zero-shot tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of large-scale pretraining in healthcare, specifically for structured EHR data, and to improve clinical event prediction.

Method: Autoregressive generative pretraining for sequential EHR data with regularization on repeated events, evaluated via zero-shot prediction tasks.

Result: The model rivals a fine-tuned Transformer baseline in forecasting dementia and knee osteoarthritis, capturing clinical dependencies without task-specific fine-tuning.

Conclusion: The approach effectively leverages pretraining for EHR data, highlighting the importance of distinguishing new onsets from repeated events in evaluations.

Abstract: Large-scale pretraining has transformed modeling of language and other data
types, but its potential remains underexplored in healthcare with structured
electronic health records (EHRs). We present a novel generative pretraining
strategy for sequential EHR data using next-visit event prediction. Our model
learns to autoregressively generate various tokenized clinical events for the
next visit based on patient history and inherently handles the joint prediction
of heterogeneous data types. Additionally, we introduce regularization on
predicting repeated events and highlight a key pitfall in EHR-based foundation
model evaluations: repeated event tokens can inflate performance metrics when
new onsets are not distinguished from subsequent occurrences. Our model is
evaluated via zero-shot prediction for forecasting dementia and knee
osteoarthritis incidence within 2 and 5 years, and the model performance rivals
a fully fine-tuned masked pretrained Transformer baseline, demonstrating that
our approach captures complex clinical dependencies without requiring costly
task-specific fine-tuning.

</details>


### [94] [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971)
*Taeyoun Kim,Fahim Tajwar,Aditi Raghunathan,Aviral Kumar*

Main category: cs.LG

TL;DR: TARS is a reinforcement learning method to train LLMs for safety by adaptively allocating compute, improving robustness to attacks and balancing safety with task completion.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM robustness against safety vulnerabilities by leveraging adaptive reasoning methods, addressing issues like jailbreaks and harmful requests.

Method: Uses RL with chain-of-thought traces and a balanced reward signal. Key design choices include a warmstart SFT stage, diverse prompt mixing, and a reward function to prevent reasoning degeneration.

Result: TARS-trained models show adaptive compute allocation, better safety-refusal trade-offs, and improved robustness to attacks like GCG and PAIR.

Conclusion: TARS offers an effective, open recipe for training LLMs to handle harmful requests by reasoning adaptively per prompt.

Abstract: Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful,
harmless, and ambiguous prompts to prevent shortcut behaviors such as too many
refusals, and (3) a reward function to prevent degeneration of reasoning
capabilities during training. Models trained with TARS exhibit adaptive
behaviors by spending more compute on ambiguous queries, leading to better
safety-refusal trade-offs. They also internally learn to better distinguish
between safe and unsafe prompts and attain greater robustness to both white-box
(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an
effective, open recipe for training LLMs against jailbreaks and harmful
requests by reasoning per prompt.

</details>


### [95] [Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes](https://arxiv.org/abs/2507.01003)
*Eun-Ji Park,Sangwon Yun*

Main category: cs.LG

TL;DR: A unified framework for accelerating deep neural network training via stochastic gradient descent, using Lyapunov exponents and ghost output nodes to improve early-stage trainability.


<details>
  <summary>Details</summary>
Motivation: To address challenges in training deep neural networks, such as convergence to poor basins and saddle points, by providing a principled method to enhance early-stage trainability.

Method: Analyzes the geometric landscape of the objective function, introduces a diagnostic using Lyapunov exponents, and extends classifiers with ghost output nodes to create extra descent directions.

Result: The ghost extension reduces approximation error, enables bypassing poor basins, and preserves asymptotic behavior while accelerating training.

Conclusion: The proposed framework offers a practical and theoretically grounded approach to improve the training of deep neural networks without compromising final performance.

Abstract: Recent studies have proposed interpreting the training process from an
ergodic perspective. Building on this foundation we present a unified framework
for understanding and accelerating the training of deep neural networks via
stochastic gradient descent. By analyzing the geometric landscape of the
objective function we introduce a practical diagnostic, the running estimate of
the largest Lyapunov exponent, which provably distinguishes genuine convergence
toward stable minimizers from mere statistical stabilization near saddle
points. We then propose a ghost category extension for standard classifiers
that adds auxiliary ghost output nodes so the model gains extra descent
directions that open a lateral corridor around narrow loss barriers and enable
the optimizer to bypass poor basins during the early training phase. We show
that this extension strictly reduces approximation error and that after
sufficient convergence the ghost dimensions collapse and the extended model's
invariant law coincides with that of the original and there exists a path in
the enlarged parameter space along which the total loss does not increase while
the original loss decreases by an arbitrary margin. Taken together these
results provide a principled architecture level intervention that accelerates
early stage trainability while preserving asymptotic behavior.

</details>


### [96] [Cooperative Sheaf Neural Networks](https://arxiv.org/abs/2507.00647)
*André Ribeiro,Ana Luiza Tenório,Juan Belieni,Amauri H. Souza,Diego Mesquita*

Main category: cs.LG

TL;DR: Sheaf diffusion lacks cooperative behavior due to missing message directionality. The paper introduces directed cellular sheaves and proposes Cooperative Sheaf Neural Networks (CSNNs), which outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of sheaf diffusion in achieving cooperative behavior, which is crucial for flexible information diffusion in graph representation learning.

Method: Introduces cellular sheaves over directed graphs, characterizes their Laplacians, and proposes CSNNs to enable selective attention and mitigate oversquashing.

Result: CSNNs outperform prior sheaf diffusion and cooperative graph neural networks, demonstrating better performance.

Conclusion: Directed cellular sheaves and CSNNs successfully enable cooperative behavior in sheaf diffusion, improving graph representation learning.

Abstract: Sheaf diffusion has recently emerged as a promising design pattern for graph
representation learning due to its inherent ability to handle heterophilic data
and avoid oversmoothing. Meanwhile, cooperative message passing has also been
proposed as a way to enhance the flexibility of information diffusion by
allowing nodes to independently choose whether to propagate/gather information
from/to neighbors. A natural question ensues: is sheaf diffusion capable of
exhibiting this cooperative behavior? Here, we provide a negative answer to
this question. In particular, we show that existing sheaf diffusion methods
fail to achieve cooperative behavior due to the lack of message directionality.
To circumvent this limitation, we introduce the notion of cellular sheaves over
directed graphs and characterize their in- and out-degree Laplacians. We
leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).
Theoretically, we characterize the receptive field of CSNN and show it allows
nodes to selectively attend (listen) to arbitrarily far nodes while ignoring
all others in their path, potentially mitigating oversquashing. Our experiments
show that CSNN presents overall better performance compared to prior art on
sheaf diffusion as well as cooperative graph neural networks.

</details>


### [97] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone,Marius P. Linhard*

Main category: cs.LG

TL;DR: The paper interprets GANs as probabilistic generative models, linking them to Bayesian neural networks, and proposes regularization and optimization strategies for improved performance.


<details>
  <summary>Details</summary>
Motivation: GANs are successful but face optimization challenges and overfitting issues. The paper aims to explain their success and limitations by reinterpreting them probabilistically.

Method: The study views GANs as Bayesian neural networks with partial stochasticity, establishes universal approximation conditions, and links adversarial optimization to marginal likelihood optimization.

Result: Proposed strategies smooth the loss landscape and find solutions with minimum description length, leading to performance improvements in experiments.

Conclusion: The work enhances understanding of GAN regularization and optimization, paving the way for better strategies.

Abstract: Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [98] [Neural Augmented Kalman Filters for Road Network assisted GNSS positioning](https://arxiv.org/abs/2507.00654)
*Hans van Gorp,Davide Belli,Amir Jalalirad,Bence Major*

Main category: cs.LG

TL;DR: A Temporal Graph Neural Network (TGNN) is proposed to integrate road network data with GNSS measurements, reducing positioning errors by 29% in dense urban areas compared to GNSS-only Kalman Filter methods.


<details>
  <summary>Details</summary>
Motivation: GNSS accuracy is compromised in dense urban environments due to multipath and non-line-of-sight errors. Existing methods using road network data are either offline or rely on inflexible Kalman Filter heuristics.

Method: A TGNN is trained to predict the correct road segment and its uncertainty, which is integrated into the Kalman Filter's measurement update step.

Result: Real-world validation shows a 29% reduction in positioning error for challenging scenarios compared to GNSS-only Kalman Filter.

Conclusion: This is the first deep learning-based approach combining road network data and GNSS measurements to improve positioning accuracy.

Abstract: The Global Navigation Satellite System (GNSS) provides critical positioning
information globally, but its accuracy in dense urban environments is often
compromised by multipath and non-line-of-sight errors. Road network data can be
used to reduce the impact of these errors and enhance the accuracy of a
positioning system. Previous works employing road network data are either
limited to offline applications, or rely on Kalman Filter (KF) heuristics with
little flexibility and robustness. We instead propose training a Temporal Graph
Neural Network (TGNN) to integrate road network information into a KF. The TGNN
is designed to predict the correct road segment and its associated uncertainty
to be used in the measurement update step of the KF. We validate our approach
with real-world GNSS data and open-source road networks, observing a 29%
decrease in positioning error for challenging scenarios compared to a GNSS-only
KF. To the best of our knowledge, ours is the first deep learning-based
approach jointly employing road network data and GNSS measurements to determine
the user position on Earth.

</details>


### [99] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth,Dibyanshu Kumar,Benjamin Paassen,Magda Gregorová*

Main category: cs.LG

TL;DR: Extending classifier guidance to non-robust classifiers by addressing noise sensitivity and stabilizing gradients, improving conditional sampling in generative models.


<details>
  <summary>Details</summary>
Motivation: Classifiers not trained on diffusion noise fail under noisy conditions, limiting their use in guidance. This work aims to enable broader classifier applicability.

Method: Uses one-step denoised predictions and stabilization techniques (e.g., exponential moving averages) to mitigate noise-induced instability.

Result: Improved guidance stability while preserving sample diversity and visual quality across datasets (CelebA, SportBalls, CelebA-HQ).

Conclusion: Enables non-robust classifiers for guidance, advancing conditional sampling in generative models.

Abstract: Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


### [100] [A Test-Function Approach to Incremental Stability](https://arxiv.org/abs/2507.00695)
*Daniel Pfrommer,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

TL;DR: The paper introduces a framework linking RL-style value functions to incremental stability, distinct from traditional Lyapunov methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between RL value functions (non-smooth, unbounded) and control theory's Lyapunov stability certificates.

Method: Uses rewards as test functions and establishes equivalence between incremental stability and value function regularity under adversarial reward selection.

Result: Shows RL-style value functions can certify stability without traditional Lyapunov conditions.

Conclusion: Demonstrates a novel, non-Lyapunov approach to understanding stability in RL-controlled systems.

Abstract: This paper presents a novel framework for analyzing
Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using
rewards as "test functions." Whereas control theory traditionally deals with
Lyapunov functions that satisfy a time-decrease condition, reinforcement
learning (RL) value functions are constructed by exponentially decaying a
Lipschitz reward function that may be non-smooth and unbounded on both sides.
Thus, these RL-style value functions cannot be directly understood as Lyapunov
certificates. We develop a new equivalence between a variant of incremental
input-to-state stability of a closed-loop system under given a policy, and the
regularity of RL-style value functions under adversarial selection of a
H\"older-continuous reward function. This result highlights that the regularity
of value functions, and their connection to incremental stability, can be
understood in a way that is distinct from the traditional Lyapunov-based
approach to certifying stability in control theory.

</details>


### [101] [SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval](https://arxiv.org/abs/2507.00701)
*Chong Zhang,Xichao Liu,Yibing Zhan,Dapeng Tao,Jun Ni*

Main category: cs.LG

TL;DR: SCAWaveNet, a spatial-channel attention-based network, improves significant wave height (SWH) retrieval by leveraging cross-channel information, outperforming existing models with reduced RMSE.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for SWH retrieval fail to fully utilize cross-channel information in CYGNSS data, limiting performance.

Method: SCAWaveNet models features from each channel as independent attention heads, integrating spatial and channel-wise information with a lightweight attention mechanism for auxiliary parameters.

Result: SCAWaveNet achieves RMSE of 0.438 m (ERA5) and 0.432 m (NDBC), reducing average RMSE by 3.52% and 5.47% respectively compared to state-of-the-art models.

Conclusion: SCAWaveNet effectively enhances SWH retrieval by leveraging cross-channel interactions, demonstrating superior performance over existing methods.

Abstract: Recent advancements in spaceborne GNSS missions have produced extensive
global datasets, providing a robust basis for deep learning-based significant
wave height (SWH) retrieval. While existing deep learning models predominantly
utilize CYGNSS data with four-channel information, they often adopt
single-channel inputs or simple channel concatenation without leveraging the
benefits of cross-channel information interaction during training. To address
this limitation, a novel spatial-channel attention-based network, namely
SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each
channel of the DDMs are modeled as independent attention heads, enabling the
fusion of spatial and channel-wise information. For auxiliary parameters, a
lightweight attention mechanism is designed to assign weights along the spatial
and channel dimensions. The final feature integrates both spatial and
channel-level characteristics. Model performance is evaluated using
four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves
an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE
reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the
average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC
buoy observations. The code is available at
https://github.com/Clifx9908/SCAWaveNet.

</details>


### [102] [Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories](https://arxiv.org/abs/2507.00711)
*Jhouben Cuesta-Ramirez,Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: LLMs trained via RL show overthinking, ignoring correct solutions and generating unnecessary reasoning steps, leading to errors.


<details>
  <summary>Details</summary>
Motivation: To investigate whether benchmark gains in LLMs reflect real reasoning improvements or just longer, ineffective reasoning chains.

Method: Experiments on three state-of-the-art models using the AIME2024 math benchmark to analyze their reasoning behavior.

Result: Models often disregard correct solutions, generate unnecessary steps, and produce incorrect conclusions, highlighting limitations in integrating corrective information.

Conclusion: The findings reveal critical challenges in achieving robust and interpretable reasoning in LLMs, questioning their current effectiveness.

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
recently achieved impressive results on reasoning benchmarks. Yet, growing
evidence shows that these models often generate longer but ineffective chains
of thought (CoTs), calling into question whether benchmark gains reflect real
reasoning improvements. We present new evidence of overthinking, where models
disregard correct solutions even when explicitly provided, instead continuing
to generate unnecessary reasoning steps that often lead to incorrect
conclusions. Experiments on three state-of-the-art models using the AIME2024
math benchmark reveal critical limitations in these models ability to integrate
corrective information, posing new challenges for achieving robust and
interpretable reasoning.

</details>


### [103] [Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction](https://arxiv.org/abs/2507.00733)
*Stefan Haas,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: The paper introduces new measures for aleatoric and epistemic uncertainty in ordinal classification, outperforming existing methods in error and OOD detection.


<details>
  <summary>Details</summary>
Motivation: Ordinal classification is common in critical fields like medicine and finance, but existing uncertainty measures focus on nominal classification and regression, leaving a gap for ordinal cases.

Method: The proposed method reduces ordinal classification to binary cases using entropy- and variance-based measures, evaluated with gradient-boosted trees and multi-layer perceptrons.

Result: The new measures outperform standard entropy and variance-based methods in error detection and show competitive OOD detection performance.

Conclusion: The ordinal nature of classification problems is crucial for accurate uncertainty assessment, as demonstrated by the proposed measures.

Abstract: Ordinal classification problems, where labels exhibit a natural order, are
prevalent in high-stakes fields such as medicine and finance. Accurate
uncertainty quantification, including the decomposition into aleatoric
(inherent variability) and epistemic (lack of knowledge) components, is crucial
for reliable decision-making. However, existing research has primarily focused
on nominal classification and regression. In this paper, we introduce a novel
class of measures of aleatoric and epistemic uncertainty in ordinal
classification, which is based on a suitable reduction to (entropy- and
variance-based) measures for the binary case. These measures effectively
capture the trade-off in ordinal classification between exact hit-rate and
minimial error distances. We demonstrate the effectiveness of our approach on
various tabular ordinal benchmark datasets using ensembles of gradient-boosted
trees and multi-layer perceptrons for approximate Bayesian inference. Our
method significantly outperforms standard and label-wise entropy and
variance-based measures in error detection, as indicated by misclassification
rates and mean absolute error. Additionally, the ordinal measures show
competitive performance in out-of-distribution (OOD) detection. Our findings
highlight the importance of considering the ordinal nature of classification
problems when assessing uncertainty.

</details>


### [104] [Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN](https://arxiv.org/abs/2507.00736)
*Arthur Thuy,Ekaterina Loginova,Dries F. Benoit*

Main category: cs.LG

TL;DR: The paper benchmarks QDE models using a novel metric (balanced DRPS) and introduces OrderedLogitNN, showing superior performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the neglect of ordinal nature in QDE and biased evaluations due to class imbalance.

Method: Benchmarks discretized regression, classification, and ordinal regression using balanced DRPS; introduces OrderedLogitNN, a neural extension of ordered logit.

Result: OrderedLogitNN outperforms on complex tasks; balanced DRPS proves robust for QDE evaluation.

Conclusion: Balanced DRPS and OrderedLogitNN provide a principled foundation for future QDE research.

Abstract: Recent years have seen growing interest in Question Difficulty Estimation
(QDE) using natural language processing techniques. Question difficulty is
often represented using discrete levels, framing the task as ordinal regression
due to the inherent ordering from easiest to hardest. However, the literature
has neglected the ordinal nature of the task, relying on classification or
discretized regression models, with specialized ordinal regression methods
remaining unexplored. Furthermore, evaluation metrics are tightly coupled to
the modeling paradigm, hindering cross-study comparability. While some metrics
fail to account for the ordinal structure of difficulty levels, none adequately
address class imbalance, resulting in biased performance assessments. This
study addresses these limitations by benchmarking three types of model outputs
-- discretized regression, classification, and ordinal regression -- using the
balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly
captures ordinality and class imbalance. In addition to using popular ordinal
regression methods, we propose OrderedLogitNN, extending the ordered logit
model from econometrics to neural networks. We fine-tune BERT on the RACE++ and
ARC datasets and find that OrderedLogitNN performs considerably better on
complex tasks. The balanced DRPS offers a robust and fair evaluation metric for
discrete-level QDE, providing a principled foundation for future research.

</details>


### [105] [Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports](https://arxiv.org/abs/2507.00742)
*Carlos Caminha,Maria de Lourdes M. Silva,Iago C. Chaves,Felipe T. Brito,Victor A. E. Farias,Javam C. Machado*

Main category: cs.LG

TL;DR: The study evaluates 27 open-source and 2 proprietary LLMs for identifying faulty components from ambiguous user reports, achieving an F1-score of up to 0.76. Three models (mistral-small-24b-instruct, llama-3.2-1b-instruct, gemma-2-2b-it) balance performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Automating fault identification from ambiguous user reports is challenging but crucial for improving user experience and testing processes.

Method: Evaluated 27 open-source and 2 proprietary LLMs using four prompting strategies: Zero-Shot, Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). Conducted 98,948 inferences with over 51M input and 13M output tokens.

Result: Achieved F1-score up to 0.76. Three models (mistral-small-24b-instruct, llama-3.2-1b-instruct, gemma-2-2b-it) performed best, balancing size and efficiency.

Conclusion: Smaller models like llama-3.2-1b-instruct and gemma-2-2b-it offer competitive performance with lower resource usage, making them suitable for end-user devices.

Abstract: Computer manufacturers offer platforms for users to describe device faults
using textual reports such as "My screen is flickering". Identifying the faulty
component from the report is essential for automating tests and improving user
experience. However, such reports are often ambiguous and lack detail, making
this task challenging. Large Language Models (LLMs) have shown promise in
addressing such issues. This study evaluates 27 open-source models (1B-72B
parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,
Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted
98,948 inferences, processing over 51 million input tokens and generating 13
million output tokens. We achieve f1-score up to 0.76. Results show that three
models offer the best balance between size and performance:
mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and
gemma-2-2b-it, that offer competitive performance with lower VRAM usage,
enabling efficient inference on end-user devices as modern laptops or
smartphones with NPUs.

</details>


### [106] [A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model](https://arxiv.org/abs/2507.00761)
*Wenbo Yu,Anirbit Ghosh,Tobias Sebastian Finn,Rossella Arcucci,Marc Bocquet,Sibo Cheng*

Main category: cs.LG

TL;DR: A denoising diffusion model is introduced to predict wildfire spread, generating multiple possible scenarios to account for uncertainty, unlike traditional deterministic models.


<details>
  <summary>Details</summary>
Motivation: Wildfire spread prediction is challenging due to fire's unpredictable nature and varying environmental conditions. Current models often fail to represent uncertainty.

Method: The study uses a denoising diffusion model, an AI framework that simulates wildfire spread as a range of possible outcomes, producing ensembles of forecasts.

Result: The model generates physically meaningful distributions of fire spread, reflecting uncertainty and improving reliability over single-outcome predictions.

Conclusion: This technology enhances wildfire behavior prediction, aiding decision-makers in risk assessment and response planning.

Abstract: Thanks to recent advances in generative AI, computers can now simulate
realistic and complex natural processes. We apply this capability to predict
how wildfires spread, a task made difficult by the unpredictable nature of fire
and the variety of environmental conditions it depends on. In this study, We
present the first denoising diffusion model for predicting wildfire spread, a
new kind of AI framework that learns to simulate fires not just as one fixed
outcome, but as a range of possible scenarios. By doing so, it accounts for the
inherent uncertainty of wildfire dynamics, a feature that traditional models
typically fail to represent. Unlike deterministic approaches that generate a
single prediction, our model produces ensembles of forecasts that reflect
physically meaningful distributions of where fire might go next. This
technology could help us develop smarter, faster, and more reliable tools for
anticipating wildfire behavior, aiding decision-makers in fire risk assessment
and response planning.

</details>


### [107] [Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments](https://arxiv.org/abs/2507.00762)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: The paper explores using Genetic Algorithms (GAs) to enhance Reinforcement Learning (RL) performance by generating expert demonstrations for policy learning in an industrial sorting task.


<details>
  <summary>Details</summary>
Motivation: RL's broader deployment is limited by challenges like sample inefficiency and unstable learning. The study aims to improve RL performance using GA-generated demonstrations.

Method: GA-generated expert demonstrations are integrated into DQN replay buffers and used as warm-start trajectories for PPO agents. Comparisons are made with standard RL, rule-based heuristics, and brute-force optimization.

Result: GA-derived demonstrations significantly improve RL performance, with PPO agents achieving superior cumulative rewards when initialized with GA-generated data.

Conclusion: Hybrid learning paradigms combining heuristic search (GA) and data-driven RL show promise for real-world applications, with the framework made publicly available for further research.

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in certain
real-world industrial applications, yet its broader deployment remains limited
by inherent challenges such as sample inefficiency and unstable learning
dynamics. This study investigates the utilization of Genetic Algorithms (GAs)
as a mechanism for improving RL performance in an industrially inspired sorting
environment. We propose a novel approach in which GA-generated expert
demonstrations are used to enhance policy learning. These demonstrations are
incorporated into a Deep Q-Network (DQN) replay buffer for experience-based
learning and utilized as warm-start trajectories for Proximal Policy
Optimization (PPO) agents to accelerate training convergence. Our experiments
compare standard RL training with rule-based heuristics, brute-force
optimization, and demonstration data, revealing that GA-derived demonstrations
significantly improve RL performance. Notably, PPO agents initialized with
GA-generated data achieved superior cumulative rewards, highlighting the
potential of hybrid learning paradigms, where heuristic search methods
complement data-driven RL. The utilized framework is publicly available and
enables further research into adaptive RL strategies for real-world
applications.

</details>


### [108] [BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation](https://arxiv.org/abs/2507.00846)
*Rishal Aggrwal,Jacky Chen,Nicholas M. Boffi,David Ryan Koes*

Main category: cs.LG

TL;DR: The paper proposes a method to efficiently sample from the Boltzmann distribution for large molecular systems by learning likelihoods via an energy-based model, avoiding costly Jacobian computations.


<details>
  <summary>Details</summary>
Motivation: Sampling from the Boltzmann distribution is computationally expensive for large systems due to the need for costly Jacobian calculations in existing methods like Boltzmann Generators.

Method: The method learns the likelihood of the generated distribution using an energy-based model trained with noise contrastive estimation and score matching, combined with stochastic interpolants for annealing.

Result: The approach achieves comparable free energy profiles and distributions to exact likelihood methods on the alanine dipeptide system, with significant speedup in estimating free energy differences.

Conclusion: The proposed method efficiently samples from the Boltzmann distribution for large systems, offering a practical alternative to exact likelihood calculations.

Abstract: Efficient sampling from the Boltzmann distribution defined by an energy
function is a key challenge in modeling physical systems such as molecules.
Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows
that transform a simple prior into a distribution that can be reweighted to
match the Boltzmann distribution using sample likelihoods. However, obtaining
likelihoods requires computing costly Jacobians during integration, making it
impractical for large molecular systems. To overcome this, we propose learning
the likelihood of the generated distribution via an energy-based model trained
with noise contrastive estimation and score matching. By using stochastic
interpolants to anneal between the prior and generated distributions, we
combine both the objective functions to efficiently learn the density function.
On the alanine dipeptide system, we demonstrate that our method yields free
energy profiles and energy distributions comparable to those obtained with
exact likelihoods. Additionally, we show that free energy differences between
metastable states can be estimated accurately with orders-of-magnitude speedup.

</details>


### [109] [Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters](https://arxiv.org/abs/2507.00848)
*Don Roosan,Saif Nirzhor,Rubayat Khan,Fahmida Hai,Mohammad Rifat Haidar*

Main category: cs.LG

TL;DR: Quantum-accelerated machine learning outperformed classical methods in HIV cluster detection (92% accuracy) and prevalence forecasting (94% accuracy), while identifying housing instability as a key causal factor.


<details>
  <summary>Details</summary>
Motivation: The complexity of HIV epidemiological data necessitates advanced computational methods for accurate analysis and forecasting to improve public health interventions.

Method: Used quantum-accelerated machine learning, including QAOA for clustering, a hybrid quantum-classical neural network for forecasting, and quantum Bayesian networks for causal analysis.

Result: QAOA achieved 92% accuracy in cluster detection; the hybrid model predicted HIV prevalence with 94% accuracy. Quantum Bayesian analysis highlighted housing instability as a key driver.

Conclusion: Quantum-enhanced methods offer precision and efficiency in HIV surveillance, guiding targeted interventions and addressing structural inequities.

Abstract: HIV epidemiological data is increasingly complex, requiring advanced
computation for accurate cluster detection and forecasting. We employed
quantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code
level using AIDSVu and synthetic SDoH data for 2022. Our approach compared
classical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization
algorithm (QAOA), developed a hybrid quantum-classical neural network for HIV
prevalence forecasting, and used quantum Bayesian networks to explore causal
links between SDoH factors and HIV incidence. The QAOA-based method achieved
92% accuracy in cluster detection within 1.6 seconds, outperforming classical
algorithms. Meanwhile, the hybrid quantum-classical neural network predicted
HIV prevalence with 94% accuracy, surpassing a purely classical counterpart.
Quantum Bayesian analysis identified housing instability as a key driver of HIV
cluster emergence and expansion, with stigma exerting a geographically variable
influence. These quantum-enhanced methods deliver greater precision and
efficiency in HIV surveillance while illuminating critical causal pathways.
This work can guide targeted interventions, optimize resource allocation for
PrEP, and address structural inequities fueling HIV transmission.

</details>


### [110] [Aligning Learning and Endogenous Decision-Making](https://arxiv.org/abs/2507.00851)
*Rares Cristian,Pavithra Harsha,Georgia Perakis,Brian Quanz*

Main category: cs.LG

TL;DR: The paper introduces an end-to-end method for training ML models to account for downstream decision-making under endogenous uncertainty, with a robust optimization variant and guarantees for near-optimal decisions. It also extends the framework to two-stage stochastic optimization problems.


<details>
  <summary>Details</summary>
Motivation: The challenge of decision-making under biased observations and lack of counterfactual information motivates the need for ML models aware of downstream impacts.

Method: The paper proposes an end-to-end method with robust optimization, uncertainty sets over ML models, and a two-stage stochastic optimization framework.

Result: Computational experiments show improved performance over existing methods in pricing and inventory problems.

Conclusion: The framework effectively addresses decision-making under uncertainty and extends to new problem classes, demonstrating consistent improvements over existing approaches.

Abstract: Many of the observations we make are biased by our decisions. For instance,
the demand of items is impacted by the prices set, and online checkout choices
are influenced by the assortments presented. The challenge in decision-making
under this setting is the lack of counterfactual information, and the need to
learn it instead. We introduce an end-to-end method under endogenous
uncertainty to train ML models to be aware of their downstream, enabling their
effective use in the decision-making stage. We further introduce a robust
optimization variant that accounts for uncertainty in ML models -- specifically
by constructing uncertainty sets over the space of ML models and optimizing
actions to protect against worst-case predictions. We prove guarantees that
this robust approach can capture near-optimal decisions with high probability
as a function of data. Besides this, we also introduce a new class of two-stage
stochastic optimization problems to the end-to-end learning framework that can
now be addressed through our framework. Here, the first stage is an
information-gathering problem to decide which random variable to poll and gain
information about before making a second-stage decision based off of it. We
present several computational experiments for pricing and inventory
assortment/recommendation problems. We compare against existing methods in
online learning/bandits/offline reinforcement learning and show our approach
has consistent improved performance over these. Just as in the endogenous
setting, the model's prediction also depends on the first-stage decision made.
While this decision does not affect the random variable in this setting, it
does affect the correct point forecast that should be made.

</details>


### [111] [Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals](https://arxiv.org/abs/2507.00862)
*Davide Andreoletti,Aris Marcolongo,Natasa Sarafijanovic Djukic,Julien Roulet,Stefano Billeter,Andrzej Kurenda,Margot Visse-Mansiaux,Brice Dupuis,Carrol Annette Plummer,Beatrice Paoli,Omran Ayoub*

Main category: cs.LG

TL;DR: A machine learning-based method predicts potato sprouting early using electrophysiological signals, improving storage management and reducing waste.


<details>
  <summary>Details</summary>
Motivation: Accurate early prediction of potato sprouting is crucial for storage management, especially after the ban on CIPC, which has led to costly alternatives. Current visual methods are ineffective for proactive intervention.

Method: The approach uses electrophysiological signals from tubers, preprocessing them, extracting wavelet-domain features, and training supervised ML models with uncertainty quantification.

Result: The method shows promising performance in early detection, predicting exact sprouting days for some potatoes and acceptable average error overall, though further refinements are needed.

Conclusion: The proposed ML-based approach is effective for early sprouting prediction but requires improvements to minimize prediction errors, especially in extreme cases.

Abstract: Accurately predicting potato sprouting before the emergence of any visual
signs is critical for effective storage management, as sprouting degrades both
the commercial and nutritional value of tubers. Effective forecasting allows
for the precise application of anti-sprouting chemicals (ASCs), minimizing
waste and reducing costs. This need has become even more pressing following the
ban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to
health and environmental concerns, which has led to the adoption of
significantly more expensive alternative ASCs. Existing approaches primarily
rely on visual identification, which only detects sprouting after morphological
changes have occurred, limiting their effectiveness for proactive management. A
reliable early prediction method is therefore essential to enable timely
intervention and improve the efficiency of post-harvest storage strategies,
where early refers to detecting sprouting before any visible signs appear. In
this work, we address the problem of early prediction of potato sprouting. To
this end, we propose a novel machine learning (ML)-based approach that enables
early prediction of potato sprouting using electrophysiological signals
recorded from tubers using proprietary sensors. Our approach preprocesses the
recorded signals, extracts relevant features from the wavelet domain, and
trains supervised ML models for early sprouting detection. Additionally, we
incorporate uncertainty quantification techniques to enhance predictions.
Experimental results demonstrate promising performance in the early detection
of potato sprouting by accurately predicting the exact day of sprouting for a
subset of potatoes and while showing acceptable average error across all
potatoes. Despite promising results, further refinements are necessary to
minimize prediction errors, particularly in reducing the maximum observed
deviations.

</details>


### [112] [TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality](https://arxiv.org/abs/2507.00899)
*Carlos Vonessen,Charles Harris,Miruna Cretu,Pietro Liò*

Main category: cs.LG

TL;DR: TABASCO introduces a non-equivariant transformer for 3D molecular generation, simplifying architecture and improving speed and validity.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with physical plausibility despite strong inductive biases. TABASCO aims to relax these assumptions for better performance.

Method: Uses a standard transformer, treats atoms as sequences, and reconstructs bonds deterministically. Avoids equivariant layers and message passing.

Result: Achieves state-of-the-art validity on GEOM-Drugs, 10x faster inference, and emergent rotational equivariance.

Conclusion: TABASCO offers a minimalist, high-throughput approach for specialized tasks like drug design.

Abstract: State-of-the-art models for 3D molecular generation are based on significant
inductive biases, SE(3), permutation equivariance to respect symmetry and graph
message-passing networks to capture local chemistry, yet the generated
molecules still struggle with physical plausibility. We introduce TABASCO which
relaxes these assumptions: The model has a standard non-equivariant transformer
architecture, treats atoms in a molecule as sequences and reconstructs bonds
deterministically after generation. The absence of equivariant layers and
message passing allows us to significantly simplify the model architecture and
scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves
state-of-the-art PoseBusters validity and delivers inference roughly 10x faster
than the strongest baseline, while exhibiting emergent rotational equivariance
despite symmetry not being hard-coded. Our work offers a blueprint for training
minimalist, high-throughput generative models suited to specialised tasks such
as structure- and pharmacophore-based drug design. We provide a link to our
implementation at github.com/carlosinator/tabasco.

</details>


### [113] [Privacy-Preserving Quantized Federated Learning with Diverse Precision](https://arxiv.org/abs/2507.00920)
*Dang Qua Nguyen,Morteza Hashemi,Erik Perrins,Sergiy A. Vorobyov,David J. Love,Taejoon Kim*

Main category: cs.LG

TL;DR: The paper proposes a novel stochastic quantizer (SQ) for federated learning (FL) to address privacy risks and quantization heterogeneity, ensuring differential privacy and minimal quantization error while improving learning utility.


<details>
  <summary>Details</summary>
Motivation: FL faces challenges like privacy risks from unprotected model updates and reduced utility due to quantization heterogeneity. Existing solutions often tackle only one issue, leaving a gap for a combined approach.

Method: Introduces a stochastic quantizer (SQ) for differential privacy and minimal quantization error, alongside a cluster size optimization and linear fusion technique for accurate model aggregation.

Result: Numerical simulations show the proposed method outperforms conventional LaplaceSQ-FL in privacy protection and learning utility.

Conclusion: The novel SQ and optimization techniques effectively address privacy and quantization challenges in FL, enhancing both security and performance.

Abstract: Federated learning (FL) has emerged as a promising paradigm for distributed
machine learning, enabling collaborative training of a global model across
multiple local devices without requiring them to share raw data. Despite its
advancements, FL is limited by factors such as: (i) privacy risks arising from
the unprotected transmission of local model updates to the fusion center (FC)
and (ii) decreased learning utility caused by heterogeneity in model
quantization resolution across participating devices. Prior work typically
addresses only one of these challenges because maintaining learning utility
under both privacy risks and quantization heterogeneity is a non-trivial task.
In this paper, our aim is therefore to improve the learning utility of a
privacy-preserving FL that allows clusters of devices with different
quantization resolutions to participate in each FL round. Specifically, we
introduce a novel stochastic quantizer (SQ) that is designed to simultaneously
achieve differential privacy (DP) and minimum quantization error. Notably, the
proposed SQ guarantees bounded distortion, unlike other DP approaches. To
address quantization heterogeneity, we introduce a cluster size optimization
technique combined with a linear fusion approach to enhance model aggregation
accuracy. Numerical simulations validate the benefits of our approach in terms
of privacy protection and learning utility compared to the conventional
LaplaceSQ-FL algorithm.

</details>


### [114] [Understanding Generalization in Node and Link Prediction](https://arxiv.org/abs/2507.00927)
*Antonis Vasileiou,Timo Stoll,Christopher Morris*

Main category: cs.LG

TL;DR: The paper introduces a unified framework to analyze the generalization of MPNNs in node and link prediction, addressing gaps in understanding and unrealistic assumptions in prior work.


<details>
  <summary>Details</summary>
Motivation: Current MPNN studies lack understanding of generalization in node- and link-level predictions, often relying on unrealistic assumptions and neglecting graph structure influence.

Method: A unified framework is proposed to analyze MPNN generalization, incorporating diverse architectural parameters, loss functions, and graph structure influence.

Result: Empirical studies support theoretical insights, enhancing understanding of MPNNs' generalization in inductive and transductive settings.

Conclusion: The framework advances MPNN generalization analysis, applicable beyond graphs to other classification tasks.

Abstract: Using message-passing graph neural networks (MPNNs) for node and link
prediction is crucial in various scientific and industrial domains, which has
led to the development of diverse MPNN architectures. Besides working well in
practical settings, their ability to generalize beyond the training set remains
poorly understood. While some studies have explored MPNNs' generalization in
graph-level prediction tasks, much less attention has been given to node- and
link-level predictions. Existing works often rely on unrealistic i.i.d.\@
assumptions, overlooking possible correlations between nodes or links, and
assuming fixed aggregation and impractical loss functions while neglecting the
influence of graph structure. In this work, we introduce a unified framework to
analyze the generalization properties of MPNNs in inductive and transductive
node and link prediction settings, incorporating diverse architectural
parameters and loss functions and quantifying the influence of graph structure.
Additionally, our proposed generalization framework can be applied beyond
graphs to any classification task under the inductive or transductive setting.
Our empirical study supports our theoretical insights, deepening our
understanding of MPNNs' generalization capabilities in these tasks.

</details>


### [115] [Time Series Foundation Models are Flow Predictors](https://arxiv.org/abs/2507.00945)
*Massimiliano Luca,Ciro Beneduce,Bruno Lepri*

Main category: cs.LG

TL;DR: Time series foundation models (TSFMs) like Moirai and TimesFM outperform traditional methods in crowd flow prediction, achieving significant improvements in accuracy without spatial data.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of TSFMs in crowd flow prediction, especially in zero-shot settings with limited spatial context.

Method: Evaluated Moirai and TimesFM on three mobility datasets (Bike NYC, Taxi Beijing, Spanish OD flows) in a zero-shot setting, using only temporal data.

Result: TSFMs achieved up to 33% lower RMSE, 39% lower MAE, and 49% higher CPC compared to state-of-the-art baselines.

Conclusion: TSFMs offer accurate and scalable solutions for flow prediction, even with minimal annotated data or missing spatial information.

Abstract: We investigate the effectiveness of time series foundation models (TSFMs) for
crowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three
real-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD
flows-these models are deployed in a strict zero-shot setting, using only the
temporal evolution of each OD flow and no explicit spatial information. Moirai
and TimesFM outperform both statistical and deep learning baselines, achieving
up to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to
state-of-the-art competitors. Our results highlight the practical value of
TSFMs for accurate, scalable flow prediction, even in scenarios with limited
annotated data or missing spatial context.

</details>


### [116] [Benchmarking the Discovery Engine](https://arxiv.org/abs/2507.00964)
*Jack Foxabbott,Arush Tagade,Andrew Cusick,Robbie McCorkell,Leo McKee-Reid,Jugal Patel,Jamie Rumbelow,Jessica Rumbelow,Zohreh Shams*

Main category: cs.LG

TL;DR: The Discovery Engine is an automated system combining ML and interpretability for scientific discovery, outperforming peer-reviewed studies in predictive performance and insights.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the Discovery Engine's capability as a general-purpose tool for automated, interpretable scientific discovery across diverse fields.

Method: Benchmarked against five peer-reviewed ML applications in medicine, materials science, social science, and environmental science.

Result: Matches or exceeds prior predictive performance and provides richer, actionable insights through interpretability.

Conclusion: The Discovery Engine shows potential as a new standard for automated, interpretable scientific modeling.

Abstract: The Discovery Engine is a general purpose automated system for scientific
discovery, which combines machine learning with state-of-the-art ML
interpretability to enable rapid and robust scientific insight across diverse
datasets. In this paper, we benchmark the Discovery Engine against five recent
peer-reviewed scientific publications applying machine learning across
medicine, materials science, social science, and environmental science. In each
case, the Discovery Engine matches or exceeds prior predictive performance
while also generating deeper, more actionable insights through rich
interpretability artefacts. These results demonstrate its potential as a new
standard for automated, interpretable scientific modelling that enables complex
knowledge discovery from data.

</details>


### [117] [Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning](https://arxiv.org/abs/2507.00965)
*Félix Lefebvre,Gaël Varoquaux*

Main category: cs.LG

TL;DR: SEPAL is a scalable embedding propagation algorithm for large knowledge graphs, designed to improve downstream task performance by optimizing embeddings on a core set of entities and propagating them globally.


<details>
  <summary>Details</summary>
Motivation: Current embedding methods are limited by their focus on link prediction and scalability issues with large graphs.

Method: SEPAL optimizes embeddings on a small core of entities and propagates them globally via message passing.

Result: SEPAL outperforms previous methods on 46 downstream tasks and scales efficiently on commodity hardware.

Conclusion: SEPAL addresses scalability and downstream task performance, making it a practical solution for large knowledge graphs.

Abstract: Many machine learning tasks can benefit from external knowledge. Large
knowledge graphs store such knowledge, and embedding methods can be used to
distill it into ready-to-use vector representations for downstream
applications. For this purpose, current models have however two limitations:
they are primarily optimized for link prediction, via local contrastive
learning, and they struggle to scale to the largest graphs due to GPU memory
limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation
ALgorithm for large knowledge graphs designed to produce high-quality
embeddings for downstream tasks at scale. The key idea of SEPAL is to enforce
global embedding alignment by optimizing embeddings only on a small core of
entities, and then propagating them to the rest of the graph via message
passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream
machine learning tasks. Our results show that SEPAL significantly outperforms
previous methods on downstream tasks. In addition, SEPAL scales up its base
embedding model, enabling fitting huge knowledge graphs on commodity hardware.

</details>


### [118] [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention](https://arxiv.org/abs/2507.01004)
*Yuhong Chou,Zehao Liu,Ruijie Zhu,Xinyi Wan,Tianjian Li,Congying Chu,Qian Liu,Jibin Wu,Zejun Ma*

Main category: cs.LG

TL;DR: ZeCO introduces a Zero Communication Overhead sequence parallelism method for linear attention models, achieving near-linear scalability and significantly reducing training time for ultra-long sequences.


<details>
  <summary>Details</summary>
Motivation: Existing Sequence Parallelism (SP) methods create communication bottlenecks, hindering efficient training of large language models with ultra-long sequences.

Method: ZeCO uses All-Scan, a new collective communication primitive, to eliminate communication overhead by providing each SP rank with the required initial operator state.

Result: ZeCO achieves a 60% speedup on 256 GPUs with an 8M sequence length compared to state-of-the-art SP methods.

Conclusion: ZeCO enables efficient training of next-generation LLMs on previously intractable sequence lengths.

Abstract: Linear attention mechanisms deliver significant advantages for Large Language
Models (LLMs) by providing linear computational complexity, enabling efficient
processing of ultra-long sequences (e.g., 1M context). However, existing
Sequence Parallelism (SP) methods, essential for distributing these workloads
across devices, become the primary bottleneck due to substantial communication
overhead. In this paper, we introduce ZeCO (Zero Communication Overhead)
sequence parallelism for linear attention models, a new SP method designed to
overcome these limitations and achieve end-to-end near-linear scalability for
long sequence training. For example, training a model with a 1M sequence length
across 64 devices using ZeCO takes roughly the same time as training with an
16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new
collective communication primitive. All-Scan provides each SP rank with
precisely the initial operator state it requires while maintaining a minimal
communication footprint, effectively eliminating communication overhead.
Theoretically, we prove the optimaity of ZeCO, showing that it introduces only
negligible time and space overhead. Empirically, we compare the communication
costs of different sequence parallelism strategies and demonstrate that
All-Scan achieves the fastest communication in SP scenarios. Specifically, on
256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to
the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a
clear path toward efficiently training next-generation LLMs on previously
intractable sequence lengths.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [119] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

TL;DR: SciBORG is a modular AI framework using LLMs for autonomous planning and execution in scientific workflows, enhancing reliability and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address limitations of LLMs in complex scientific tasks, such as memory, planning, and tool integration.

Method: Dynamic agent construction from source code docs, augmented with FSA memory for state tracking and context-aware decisions.

Result: Reliable execution, adaptive planning, and interpretable state transitions in tasks like bioassay retrieval and hardware integration.

Conclusion: Memory and state awareness are key for reliable AI agents in complex environments, offering a scalable solution.

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


### [120] [Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms](https://arxiv.org/abs/2507.00491)
*Zain Taufique,Aman Vyas,Antonio Miele,Pasi Liljeberg,Anil Kanduri*

Main category: cs.MA

TL;DR: Twill is a runtime framework for scheduling Compound AI (cAI) systems on mobile edge platforms, reducing latency by 54% while adhering to power constraints.


<details>
  <summary>Details</summary>
Motivation: Existing edge AI strategies fail to handle concurrent DNN-transformer workloads in cAI systems, necessitating a new solution.

Method: Twill uses task affinity-aware cluster mapping, priority-aware task freezing/unfreezing, and DVFS to manage workloads.

Result: Twill reduces inference latency by 54% compared to state-of-the-art techniques.

Conclusion: Twill effectively addresses the challenge of deploying cAI systems on mobile edge platforms.

Abstract: Compound AI (cAI) systems chain multiple AI models to solve complex problems.
cAI systems are typically composed of deep neural networks (DNNs),
transformers, and large language models (LLMs), exhibiting a high degree of
computational diversity and dynamic workload variation. Deploying cAI services
on mobile edge platforms poses a significant challenge in scheduling concurrent
DNN-transformer inference tasks, which arrive dynamically in an unknown
sequence. Existing mobile edge AI inference strategies manage multi-DNN or
transformer-only workloads, relying on design-time profiling, and cannot handle
concurrent inference of DNNs and transformers required by cAI systems. In this
work, we address the challenge of scheduling cAI systems on heterogeneous
mobile edge platforms. We present Twill, a run-time framework to handle
concurrent inference requests of cAI workloads through task affinity-aware
cluster mapping and migration, priority-aware task freezing/unfreezing, and
DVFS, while minimizing inference latency within power budgets. We implement and
deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate
Twill against state-of-the-art edge AI inference techniques over contemporary
DNNs and LLMs, reducing inference latency by 54% on average, while honoring
power budgets.

</details>


### [121] [Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications](https://arxiv.org/abs/2507.00914)
*Jindong Han,Yansong Ning,Zirui Yuan,Hang Ni,Fan Liu,Tengfei Lyu,Hao Liu*

Main category: cs.MA

TL;DR: The paper explores the use of Large Language Models (LLMs) as intelligent agents in urban environments, detailing their capabilities, workflows, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs for creating efficient, livable, and sustainable cities by deploying them as semi-embodied agents in urban decision-making.

Method: Introduces urban LLM agents, surveys their workflows (sensing, memory, reasoning, execution, learning), categorizes applications (planning, transportation, environment, safety, society), and discusses trustworthiness and evaluation.

Result: Provides a comprehensive survey of urban LLM agents, establishing a foundation for future research and practical deployment.

Conclusion: The paper outlines a roadmap for integrating LLMs into urban intelligence, highlighting open problems and maintaining a resource repository for ongoing updates.

Abstract: The long-standing vision of intelligent cities is to create efficient,
livable, and sustainable urban environments using big data and artificial
intelligence technologies. Recently, the advent of Large Language Models (LLMs)
has opened new ways toward realizing this vision. With powerful semantic
understanding and reasoning capabilities, LLMs can be deployed as intelligent
agents capable of autonomously solving complex problems across domains. In this
article, we focus on Urban LLM Agents, which are LLM-powered agents that are
semi-embodied within the hybrid cyber-physical-social space of cities and used
for system-level urban decision-making. First, we introduce the concept of
urban LLM agents, discussing their unique capabilities and features. Second, we
survey the current research landscape from the perspective of agent workflows,
encompassing urban sensing, memory management, reasoning, execution, and
learning. Third, we categorize the application domains of urban LLM agents into
five groups: urban planning, transportation, environment, public safety, and
urban society, presenting representative works in each group. Finally, we
discuss trustworthiness and evaluation issues that are critical for real-world
deployment, and identify several open problems for future research. This survey
aims to establish a foundation for the emerging field of urban LLM agents and
to provide a roadmap for advancing the intersection of LLMs and urban
intelligence. A curated list of relevant papers and open-source resources is
maintained and continuously updated at
https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.

</details>
