<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.LG](#cs.LG) [Total: 162]
- [cs.MA](#cs.MA) [Total: 11]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: The paper introduces a method to improve long-form reasoning in large language models by removing redundant reasoning paths, focusing on attention patterns and pruning low-contributing tokens.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning paths in large language models contain redundancy, leading to scattered attention and incorrect answers. The goal is to enhance performance by eliminating distractions.

Method: The method involves identifying redundancy via token-level attention scores to a special end-of-thinking token, pruning low-contributing reasoning chunks, and resuming generation after removing redundant tokens.

Result: The approach significantly improves accuracy on reasoning-intensive benchmarks, especially in mathematical competitions like AIME and AMC, without requiring additional training.

Conclusion: Deliberate removal of reasoning redundancy enhances model performance, particularly in complex reasoning tasks.

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: A novel MCA approach combining two Virtual Gap Analysis (VGA) models is proposed to enhance efficiency and fairness in multi-criteria assessments, addressing challenges like subjective judgment and homogeneity assumptions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing MCA methods (DEA, SFA, MCDM) by integrating quantitative and qualitative criteria and ensuring comprehensive, dependable evaluations.

Method: Combines two VGA models rooted in linear programming to improve accuracy and transparency in evaluating Decision-Making Units (DMUs).

Result: Demonstrated through numerical examples, the approach proves efficient, fair, and adaptable for complex evaluations.

Conclusion: The proposed method advances automated decision systems, encouraging further progress in decision support systems.

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: The paper proposes using tabletop role-playing game (TTRPG) principles and the Entity-Component pattern to create a flexible framework for generative AI in multi-actor environments, demonstrated through the Concordia library.


<details>
  <summary>Details</summary>
Motivation: To address the diverse use cases of generative AI (Simulationist, Dramatist, Evaluationist) by providing a flexible scenario definition framework.

Method: Adopts the Game Master (GM) concept from TTRPGs and the Entity-Component architectural pattern, allowing configurable GM entities composed of reusable components.

Result: The approach enables separation of implementation, component creation, and configuration, supporting rapid iteration, modularity, and scalability.

Conclusion: The Concordia library exemplifies this philosophy, allowing users to configure scenarios tailored to their goals effectively.

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst, a transformer-based AI Foundation Model, is introduced for biodiversity analysis and conservation, outperforming existing methods in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: The accelerating loss of biodiversity and its threats (habitat loss, climate change, invasive species) necessitate advanced tools for monitoring and conservation.

Method: BioAnalyst uses a transformer-based architecture, pre-trained on multi-modal datasets (species records, remote sensing, climate data), and is fine-tuned for tasks like species distribution modeling.

Result: BioAnalyst achieves higher accuracy in ecological forecasting, especially in data-scarce scenarios, setting a new baseline.

Conclusion: Openly releasing BioAnalyst aims to advance AI-driven biodiversity solutions and foster scientific collaboration.

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: AI tools unexpectedly increased task completion time by 19% for experienced developers, contrary to predictions of time savings.


<details>
  <summary>Details</summary>
Motivation: To study the real-world impact of AI tools on software development productivity.

Method: Conducted a randomized controlled trial (RCT) with 16 experienced developers completing 246 tasks, with AI tool usage randomly allowed or disallowed.

Result: AI tools slowed developers by 19%, contradicting forecasts and expert predictions.

Conclusion: The slowdown effect is robust and unlikely due to experimental design, suggesting AI tools may not always enhance productivity as expected.

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: A MARL framework for detecting DeFi market manipulation, using adversarial game dynamics, innovative learning methods, and multi-modal data integration.


<details>
  <summary>Details</summary>
Motivation: DeFi lacks centralized oversight, enabling market manipulation. The paper aims to detect such activities using decentralized methods.

Method: Proposes a MARL framework with GRPO, a theory-based reward function, and multi-modal agent integration for manipulation detection.

Result: Achieves high detection accuracy and causal attribution, validated on real-world data and simulations.

Conclusion: Bridges multi-agent systems with financial surveillance, offering a decentralized solution for market intelligence.

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: The paper evaluates the security risks of LLM-based coding agents, revealing 21% of actions are insecure, and proposes mitigation strategies with varying effectiveness.


<details>
  <summary>Details</summary>
Motivation: To understand and address the security implications of deploying LLM-based coding agents in software development.

Method: Conducted a systematic security evaluation of five state-of-the-art models on 93 real-world tasks, analyzing over 12,000 actions. Developed a detection system for vulnerabilities.

Result: Found 21% of agent actions were insecure, with information exposure (CWE-200) being most common. GPT-4.1 showed 96.8% mitigation success.

Conclusion: Highlights the need for security-aware design in next-gen LLM-based coding agents and provides a framework for evaluation.

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: A taxonomy of AI-driven omnicidal events, highlighting risks and advocating preventive measures.


<details>
  <summary>Details</summary>
Motivation: To raise awareness and public support for preventing catastrophic AI risks.

Method: Presents a taxonomy and examples of potential omnicidal scenarios.

Result: Identifies possibilities of AI-driven human extinction, urging preventive action.

Conclusion: Public awareness can help mitigate catastrophic AI risks.

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow is an end-to-end framework for improving scientific reasoning in MLLMs, featuring EduPRM for step-wise critique and EduMCTS for domain-adapted search, resulting in enhanced reasoning consistency and coherence.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with scientific tasks due to poor reasoning patterns, lack of coherence in multi-step inference, and no self-correction. EduFlow addresses these limitations.

Method: EduFlow includes EduPRM (a process-aware reward model) and EduMCTS (a domain-adapted search framework). It uses curriculum learning, MCTS-guided trajectories, error-injected critiques, and teacher-student dialogues.

Result: EduFlow improves reasoning consistency and coherence, demonstrated through extensive experiments.

Conclusion: EduFlow effectively enhances MLLMs' scientific reasoning, with plans to release code, data, and models.

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [10] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: The paper explores merging explainability and adaptability in AI, focusing on transferable and interpretable neurosymbolic systems, specifically Agentic Retrieval-Augmented Generation systems. It evaluates how knowledge representations impact LLMs in querying triplestores.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between explainability/interpretability and adaptability in AI systems, particularly for generative AI and LLMs.

Method: Systematic evaluation of how knowledge representations (structure and complexity) affect LLMs in querying triplestores within Agentic Retrieval-Augmented Generation systems.

Result: Both knowledge representation approaches impact the AI agent's effectiveness, with specific implications discussed.

Conclusion: The study highlights the trade-offs and impacts of different knowledge representations in designing interpretable and adaptable AI systems.

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [11] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG algorithm resolves the conflict between monotonic improvement and partial parameter-sharing in heterogeneous MARL, outperforming baselines in SMAC and MAMuJoCo.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous MARL requires partial parameter-sharing for high performance, but existing methods like HAPPO fail due to policy drift when combined with sequential updates.

Method: OMDPG introduces Optimal Marginal Q (OMQ) for monotonic improvement, Generalized Q Critic (GQC) for stable Q-values, and CCGA architecture for parameter-sharing.

Result: OMDPG outperforms state-of-the-art MARL baselines in SMAC and MAMuJoCo environments.

Conclusion: OMDPG successfully balances monotonic improvement and parameter-sharing, proving effective in heterogeneous MARL.

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [12] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Stackelberg games integrate LLMs into strategic leader-follower interactions, introducing reasoning and behavioral equilibria to model bounded rationality and asymmetric information.


<details>
  <summary>Details</summary>
Motivation: To extend classical Stackelberg games by incorporating LLMs for richer, adaptive strategic interactions, addressing limitations like complete information and perfect rationality.

Method: Define reasoning and behavioral equilibria, and conjectural reasoning equilibrium, using structured prompts and probabilistic LLM behaviors. Tested in a spearphishing case study.

Result: Demonstrates cognitive richness and adversarial potential in LLM-mediated interactions, applicable to cybersecurity, misinformation, and recommendation systems.

Conclusion: LLM-Stackelberg games offer a robust framework for modeling complex decision-making in strategic domains with bounded rationality and uncertainty.

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [13] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: The paper introduces 'adaptability' as a key metric for evaluating MARL algorithms in dynamic real-world multi-agent systems, proposing a framework with three dimensions to assess performance beyond traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Real-world MAS deployment of MARL is limited due to dynamic and complex environments, requiring algorithms to handle variability in agent populations, task goals, and execution conditions.

Method: The authors propose a structured framework centered on adaptability, with three dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability.

Result: The framework aims to provide a principled way to evaluate MARL algorithms under shifting conditions, addressing real-world challenges.

Conclusion: This work supports the development of MARL algorithms better suited for dynamic, real-world MAS by emphasizing adaptability as a critical evaluation criterion.

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [14] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: The paper proposes a shift from reactive to proactive multi-agent reinforcement learning using generative AI, enabling agents to model future interactions and coordinate effectively.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent reinforcement learning methods struggle with joint action spaces, non-stationarity, and partial observability, limiting their adaptability and coordination.

Method: The paper advocates for generative AI-based reinforcement learning, where agents act as generative models to predict and synthesize multi-agent dynamics for proactive decision-making.

Result: Generative-RL agents can anticipate behaviors, coordinate actions, and adapt dynamically, overcoming limitations of reactive approaches.

Conclusion: This paradigm shift promises advanced distributed intelligence and solutions for complex coordination challenges in autonomous systems and robotics.

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [15] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$ is an LLM-based system for automated scientific synthesis, improving literature retrieval diversity and depth with user-controllable, transparent reasoning.


<details>
  <summary>Details</summary>
Motivation: To enhance scientific literature retrieval by enabling recursive, depth- and breadth-controlled exploration of research questions, addressing limitations of conventional retrieval-augmented generation.

Method: Uses parameter-driven configurability for high-throughput integration of domain-specific evidence, with transparent reasoning. Tested on 49 ecological research questions.

Result: Achieves up to 21x more source integration and 14.9x rise in sources per 1,000 words. High-parameter settings provide expert-level depth and diversity.

Conclusion: DeepResearch$^{\text{Eco}}$ significantly improves scientific synthesis, offering scalable, rigorous, and nuanced literature exploration.

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


### [16] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP is a fast, single-step trajectory planning method using CTM, outperforming diffusion-based methods in efficiency and performance on D4RL tasks.


<details>
  <summary>Details</summary>
Motivation: Address high computational costs of iterative sampling in diffusion-based planning methods.

Method: Leverages Consistency Trajectory Model (CTM) for efficient, single-step trajectory optimization.

Result: Outperforms existing methods on D4RL, achieving higher returns with fewer steps and 120x speedup.

Conclusion: CTP is practical and effective for high-performance, low-latency offline planning.

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [17] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: The paper introduces a Metropolis-Hastings sampling-based framework to train Spiking Neural Networks (SNNs) for reinforcement learning tasks, avoiding gradient-based methods and outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient but challenging to train for RL due to non-differentiable spike-based communication. The work aims to overcome this by using Bayesian inference.

Method: The framework employs Metropolis-Hastings sampling to iteratively propose and accept parameter updates based on reward signals, bypassing backpropagation.

Result: The MH-based approach outperforms Deep Q-Learning and prior SNN-based RL methods in maximizing rewards and minimizing resources on AcroBot and CartPole benchmarks.

Conclusion: The proposed framework successfully trains SNNs for RL without gradients, offering a promising alternative for neuromorphic platforms.

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [18] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens is an AIaaS platform integrating proprietary data, workflows, and major LLMs, offering businesses control, security, and automation. It excels in retrieval and generation tasks, improving precision and factual alignment.


<details>
  <summary>Details</summary>
Motivation: To provide businesses with a secure, in-house AI solution that retains knowledge and automates tasks while leveraging major LLMs for high-impact outcomes.

Method: Uses structured document ingestion, hybrid vector retrieval, no-code orchestration via LangChain, and supports top LLMs. Features the THOR Agent for SQL-style queries.

Result: Achieves 91.3% retrieval precision (Top-3) and up to 23% improvement in factual alignment for generation tasks.

Conclusion: eSapiens effectively enables trustworthy, auditable AI workflows for high-stakes domains like legal and finance.

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [19] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: The paper reviews AI's overlooked environmental and ethical challenges, focusing on energy consumption, e-waste, compute inequality, and cybersecurity energy demands. It advocates for sustainable and equitable AI development.


<details>
  <summary>Details</summary>
Motivation: To address the environmental and ethical impacts of AI's rapid expansion, highlighting systemic issues like emissions, e-waste, and global disparities.

Method: Draws from recent studies and institutional reports to analyze AI's broader impacts.

Result: Identifies key research gaps and systemic issues, emphasizing the need for sustainable and transparent AI practices.

Conclusion: AI progress must align with ethical responsibility and environmental stewardship for a sustainable future.

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [20] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: A neurosymbolic framework combines multimodal language models with Knowledge Graphs (KGs) and ontologies to enhance interoperability in personal service robots, outperforming proprietary solutions.


<details>
  <summary>Details</summary>
Motivation: Current robotic systems rely on siloed, hard-coded solutions, limiting adaptability and scalability. Combining perceptual strengths of language models with symbolic reasoning of KGs aims to address this.

Method: The framework integrates robot perception data, ontologies, and five multimodal models (LLaMA and GPT variants) to generate ontology-compliant KGs, evaluated for consistency and effectiveness.

Result: GPT-o1 and LLaMA 4 Maverick models consistently outperformed others, though newer models didn't guarantee better results, emphasizing integration strategy's importance.

Conclusion: The neurosymbolic approach successfully supports interoperability in robotics, but model choice and integration strategy are critical for performance.

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [21] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: A PyTorch toolkit for stochastic control in AI systems ensures fairness and robustness in multi-agent interactions.


<details>
  <summary>Details</summary>
Motivation: To provide a priori guarantees of fairness and robustness in AI systems interacting with multiple agents.

Method: Uses stochastic control techniques and closed-loop modeling in a PyTorch-based toolkit.

Result: Simplifies fairness guarantees for multi-agent systems and offers robust closed-loop solutions.

Conclusion: The toolkit effectively addresses fairness and robustness in AI systems with multi-agent interactions.

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [22] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: Survey on concise and adaptive thinking for efficient reasoning in large reasoning models (LRMs), addressing challenges like redundant reasoning chains and proposing solutions.


<details>
  <summary>Details</summary>
Motivation: LRMs generate unnecessarily lengthy reasoning chains for trivial questions, wasting resources and increasing response time, hindering practical applications.

Method: Comprehensive overview of methodologies, benchmarks, and challenges in concise and adaptive thinking for LRMs.

Result: Identifies the need for adaptive reasoning between fast and slow thinking based on input difficulty.

Conclusion: Aims to help researchers understand the field and inspire novel adaptive thinking ideas for better LRM usage.

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [23] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: The paper introduces a causality-informed deep Q-network (Causal DQ) for optimal sensor placement in anomaly detection, addressing limitations of existing methods by incorporating causality without impractical interventions.


<details>
  <summary>Details</summary>
Motivation: The growing volume of data in AI-driven manufacturing necessitates efficient sensor placement for anomaly detection, but current methods overlook causality or rely on impractical interventions.

Method: The proposed Causal DQ integrates causal information into Q-network training, ensuring faster convergence and tighter error bounds.

Result: The method reduces anomaly detection time significantly in various settings, proving effective for large-scale data streams.

Conclusion: The approach not only improves sensor placement but also offers broader applications in causality-informed reinforcement learning for engineering.

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [24] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: A method integrates LLMs into paraconsistent logic to address their logical inconsistency, preserving soundness and completeness.


<details>
  <summary>Details</summary>
Motivation: LLMs show logical inconsistency despite strong language capabilities; this work aims to leverage their knowledge while ensuring formal reasoning integrity.

Method: Directly integrate an LLM into the interpretation function of paraconsistent logic's formal semantics.

Result: Experimental evidence supports feasibility using datasets from factuality benchmarks.

Conclusion: The method provides a neuro-symbolic framework that maintains logical soundness and completeness while utilizing LLM knowledge.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [25] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: Proposes technical interventions for a coordinated halt on dangerous AI development to mitigate risks like misuse and geopolitical instability.


<details>
  <summary>Details</summary>
Motivation: Addressing the unprecedented risks posed by rapid AI development, including loss of control and power concentration.

Method: Outlines key technical interventions to enable a coordinated halt on dangerous AI activities.

Result: Demonstrates how these interventions can restrict dangerous AI and support governance plans.

Conclusion: Technical interventions can form a foundation for AI governance to prevent worst-case outcomes.

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [26] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: A study shows that minimal fine-tuning with 20 long CoT examples from a reasoning model can significantly enhance a base model's reasoning, outperforming larger models. Human-written CoT and other methods fall short, highlighting the unique value of expert reasoning traces.


<details>
  <summary>Details</summary>
Motivation: To explore whether long CoT can be induced in base models with minimal tuning or prompting, and to understand the effectiveness of small-scale reasoning supervision.

Method: Light fine-tuning of the base model Qwen2.5-32B using 20 long CoT examples from QwQ-32B-Preview, and experimentation with CoT data from non-reasoning models and human annotators.

Result: The fine-tuned model outperforms the larger Qwen2.5-Math-72B-Instruct, while human-written and other CoT data fail to match reasoning model traces.

Conclusion: Expert reasoning traces are hard to replicate, but small, high-quality CoT datasets can unlock strong reasoning in base models, suggesting potential for further research.

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [27] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: The paper reinterprets instruction-tuned large language models as neurosymbolic AI systems, using natural language as the symbolic layer and grounding through internal representations, aiming to improve learning and reasoning.


<details>
  <summary>Details</summary>
Motivation: To combine neural networks and symbolic AI for robust, verifiable reasoning and scalable learning by leveraging natural language as a symbolic layer.

Method: Proposes a framework where natural language serves as the symbolic layer, grounded in the model's internal representation space, and develops novel learning and reasoning approaches.

Result: Preliminary evaluations on axiomatic deductive reasoning show improved learning efficiency and reasoning reliability.

Conclusion: The approach effectively integrates neural and symbolic AI, demonstrating potential for enhanced learning and reasoning capabilities.

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [28] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: The paper introduces VerifyBench, a benchmark for evaluating verifiers of LLM-generated responses, highlighting trade-offs between specialized and general verifiers and their cross-domain limitations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation of verifiers for LLM-generated responses, which hinders reliable RLVR development.

Method: Constructed VerifyBench with 4,000 expert-level questions across domains, equipped with reference answers and diverse responses, and evaluated verifiers using a four-dimensional framework.

Result: Specialized verifiers lead in accuracy but lack recall; general models are inclusive but unstable in precision. Verifiers are sensitive to input structure and struggle with cross-domain generalization.

Conclusion: The study reveals critical bottlenecks in verifier technology and provides insights for future improvements in RLVR.

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [29] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek's V3 and R1 models are notable for their cost-efficiency, performance, and open-source nature. The paper reviews AI model evolution, introduces novel algorithms, and discusses engineering breakthroughs and competitive impact.


<details>
  <summary>Details</summary>
Motivation: To highlight DeepSeek's innovations and their role in advancing large AI models, focusing on cost, performance, and open-source benefits.

Method: Reviews AI model evolution, introduces novel algorithms (MLA, MoE, MTP, GRPO), and explores engineering optimizations in scaling, training, and inference.

Result: DeepSeek models demonstrate competitive advantages in performance and cost, impacting the AI landscape.

Conclusion: DeepSeek's innovations provide insights for future trends in AI model development, emphasizing data, training, and reasoning advancements.

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [30] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: The paper proposes a low-cost method to assess latent intentionality in data using process coherence and scale separation, applicable even to basic organisms.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention to practical intent in science and technology, building on Promise Theory's Semantic Spacetime model.

Method: Uses process coherence and scale separation to identify intended content and ambient context, without extensive training.

Result: An elementary, pragmatic interpretation of latent intentionality with minimal computational cost.

Conclusion: The method is feasible for basic organisms and offers a scalable approach to concept formation based on memory capacity.

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [31] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: The paper introduces a method to improve Chain of Thought (CoT) reasoning reliability by using the model's intrinsic veracity encoding to dynamically select accurate reasoning paths.


<details>
  <summary>Details</summary>
Motivation: CoT reasoning is powerful but suffers from error accumulation in intermediate steps, undermining reliability.

Method: Leverages truthfulness-sensitive attention head activations to train a confidence predictor, dynamically selecting reasoning paths via beam search.

Result: Outperforms state-of-the-art baselines in accuracy and reliability across various reasoning tasks.

Conclusion: Provides a novel reliability improvement for CoT reasoning with broad applicability, validated on large models.

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [32] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: This paper evaluates LLMs for SPARQL-to-SPARQL translation between KG schemas (DBpedia-Wikidata and DBLP-OpenAlex), finding performance varies by model and prompting strategy.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in KG interoperability research by assessing LLMs' ability to translate SPARQL queries across different KG schemas.

Method: Two benchmarks (DBpedia-Wikidata and DBLP-OpenAlex) were used to test three LLMs (Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, Mistral-Large-Instruct-2407) with zero-shot, few-shot, and chain-of-thought prompting.

Result: Performance varied significantly by model and prompting strategy; translations from Wikidata to DBpedia outperformed the reverse.

Conclusion: LLMs show promise for SPARQL translation but require further optimization for consistent performance across schemas.

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [33] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: The paper introduces gradual semantics for assumption-based argumentation (ABA), filling a gap in computational argumentation by extending modular gradual semantics from QBAFs to ABA frameworks.


<details>
  <summary>Details</summary>
Motivation: Gradual semantics are useful for ABA frameworks, but existing work has not addressed this, despite ABA's popularity and applications.

Method: The authors propose a family of gradual semantics for ABA by abstracting ABA frameworks into bipolar set-based argumentation frameworks and generalizing QBAF semantics. They also explore an argument-based approach as a baseline.

Result: The proposed gradual ABA semantics satisfy adapted properties like balance and monotonicity. Experiments compare these semantics with the argument-based approach and assess convergence.

Conclusion: The paper successfully extends gradual semantics to ABA, demonstrating their applicability and comparing them with alternative methods.

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [34] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: BlueGlass is a framework for integrating diverse AI safety tools, demonstrated through three analyses on vision-language models.


<details>
  <summary>Details</summary>
Motivation: The increasing capability of AI systems necessitates integrated safety methodologies, as existing tools lack comprehensive assurance.

Method: Introduces BlueGlass, a unified infrastructure for composite safety workflows, applied to vision-language models with three analyses: distributional evaluation, probe-based layer dynamics, and sparse autoencoders.

Result: Demonstrates utility through analyses revealing performance trade-offs, hierarchical learning, and interpretable concepts.

Conclusion: Provides foundational infrastructure and insights for building robust and reliable AI systems.

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [35] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: The paper explores AI planning and RL approaches for orchestrating edge-cloud application migration, modeled as Towers of Hanoi problems, and introduces a new classification based on state space.


<details>
  <summary>Details</summary>
Motivation: To understand and compare techniques for automating application migration in edge-cloud systems to improve QoS and cost-effectiveness.

Method: Analyzes MDP, AI planning, and RL approaches, focusing on ToH problem modeling, and introduces a state-space-based classification.

Result: Identifies and compares state-of-the-art techniques for migration orchestration in computing continuum environments.

Conclusion: Provides insights into effective methods for automating application migration in edge-cloud systems, aiding future research and implementation.

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [36] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: The paper explores using metacognitive prompts, like "could you be wrong?", to debias LLMs by revealing latent biases and contradictions in their responses.


<details>
  <summary>Details</summary>
Motivation: To develop general debiasing strategies for LLMs that remain effective as models evolve, inspired by human decision-making interventions.

Method: Applying metacognitive prompts (e.g., "could you be wrong?") to LLMs to elicit self-reflection on biases, errors, and contradictory evidence.

Result: The prompts successfully uncover hidden biases and misalignments in LLM responses, improving metacognitive awareness.

Conclusion: Human psychology-based prompts offer a promising approach for debiasing LLMs, leveraging proven decision-making strategies.

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [37] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: The paper proposes FRSICL, an LLM-enabled online scheme for UAV wildfire monitoring, optimizing flight control and data scheduling to minimize AoI, outperforming DRL methods like PPO.


<details>
  <summary>Details</summary>
Motivation: Current DRL methods for UAV-assisted wildfire monitoring suffer from inefficiencies and simulation-reality gaps, making them unsuitable for time-critical tasks.

Method: FRSICL uses LLM-enabled in-context learning to dynamically optimize UAV flight and data collection via natural language descriptions and feedback.

Result: Simulations show FRSICL outperforms PPO and Nearest-Neighbor baselines in minimizing AoI.

Conclusion: FRSICL offers a more efficient and practical solution for real-time UAV wildfire monitoring compared to traditional DRL approaches.

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [38] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: The paper introduces the Swiss Food Knowledge Graph (SwissFKG), a centralized resource integrating recipes, ingredients, substitutions, and nutritional data, enhanced by LLMs, to address gaps in dietary assessment tools.


<details>
  <summary>Details</summary>
Motivation: Existing dietary assessment systems ignore non-visual factors like ingredient substitutions and individual dietary needs, while Swiss food data remains fragmented.

Method: Developed SwissFKG with an LLM-powered enrichment pipeline, benchmarked four LLMs for food knowledge augmentation, and implemented a Graph-RAG application for nutrition queries.

Result: LLMs effectively enriched SwissFKG with nutritional data, and the Graph-RAG application demonstrated improved LLM responses to user-specific queries.

Conclusion: SwissFKG bridges gaps in dietary assessment by combining visual, contextual, and cultural dimensions, paving the way for next-gen tools.

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [39] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: The paper compares Decision Transformer (DT) with Filtered Behavior Cloning (FBC) in sparse-reward settings, showing FBC outperforms DT in efficiency and performance, questioning DT's overall utility.


<details>
  <summary>Details</summary>
Motivation: To evaluate if Decision Transformer (DT) is truly superior in sparse-reward environments compared to simpler methods like FBC.

Method: Experiments on Robomimic and D4RL benchmarks, comparing DT with FBC, which filters low-performing trajectories and applies behavior cloning.

Result: FBC achieves competitive or better performance than DT, with less data and computational cost.

Conclusion: DT may not be preferable in sparse or dense-reward environments, raising doubts about its general utility.

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [40] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: The paper addresses gaps in XAI research by proposing a method to categorize and compare studies under 'what, why, and who' dimensions, aiming to improve design and reporting guidelines.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from contradictions and lack of concrete design recommendations in XAI research due to inadequate task descriptions, context-free studies, and insufficient user testing.

Method: The method involves drawing from fields like visual analytics, cognition, and dashboard design to categorize XAI studies and propose guidelines for reporting user expertise and study design.

Result: The results highlight the need for better descriptions of tasks, context-aware studies, and user testing, along with reporting on users' domain, AI, and data analysis expertise.

Conclusion: The paper concludes with proposed guidelines to enhance the XAI community's ability to navigate and utilize research findings effectively, addressing gaps and contradictions.

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [41] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: A survey on LLM-based Table Agents addressing real-world table tasks, defining five core competencies and highlighting gaps in performance for open-source models.


<details>
  <summary>Details</summary>
Motivation: Real-world table tasks involve noise and complexity, underexplored in research focused on clean datasets.

Method: Defines five core competencies (C1-C5) to analyze and compare approaches, with a focus on Text-to-SQL Agents.

Result: Identifies a performance gap between academic benchmarks and real-world scenarios, especially for open-source models.

Conclusion: Provides insights to enhance robustness, generalization, and efficiency of LLM-based Table Agents in practice.

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [42] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouva,Nuno Paulos,Eduardo Uchoa e Mari C. V. Nascimento*

Main category: cs.AI

TL;DR: The paper introduces Instance Space Analysis (ISA) to study how CVRP instance characteristics affect metaheuristic performance, using a dataset from DIMACS and dimensionality reduction methods.


<details>
  <summary>Details</summary>
Motivation: To understand the nuanced relationships between CVRP instance characteristics and metaheuristic performance, advancing research in the field.

Method: Combines ISA methodology with DIMACS dataset, using PRELIM, SIFTED, and PILOT stages for dimensionality reduction and machine learning to project instance space.

Result: Identified 23 relevant instance characteristics and created a two-dimensional projection of the instance space, including a projection matrix for future analysis.

Conclusion: The work provides a new tool (ISA) and projection matrix for analyzing CVRP instances, enhancing understanding of metaheuristic behavior.

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [43] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: A novel model combining BERT for sentiment analysis and XGBoost for socio-demographic and behavioral data predicts student dropout with 84% accuracy, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Early detection of school dropout in distance learning is critical for intervention and student perseverance.

Method: Combines sentiment analysis of student comments using BERT with socio-demographic and behavioral data analyzed via XGBoost.

Result: Achieved 84% accuracy, outperforming the baseline (82%), with better precision and F1-score.

Conclusion: The model is a promising tool for personalized dropout prevention strategies.

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [44] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: The paper proposes architectures like neural memory and hypernetworks to enable efficient transfer learning in data-scarce domains like computational chemistry and medical imaging, demonstrating success in 3D scene generation and molecular property prediction.


<details>
  <summary>Details</summary>
Motivation: Transfer learning struggles in data-scarce domains where large pre-trained models are infeasible. The work aims to design efficient architectures for such scenarios.

Method: Uses neural memory for adaptation on non-stationary distributions and hypernetworks with MAML for generalizable priors. Applied to 3D scene generation and molecular property prediction.

Result: Hypernetworks efficiently acquire priors with few samples, enabling faster text-to-3D generation and improved molecular property prediction.

Conclusion: The proposed architectures effectively address transfer learning challenges in data-scarce domains, demonstrating versatility across tasks like 3D generation and computational immunology.

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: Recurrent Expansion (RE) introduces a behavior-aware learning paradigm beyond traditional ML/DL, focusing on model evolution and iterative self-improvement through Multiverse RE (MVRE) and Heterogeneous MVRE (HMVRE).


<details>
  <summary>Details</summary>
Motivation: To move beyond static data representations in DL by incorporating evolving model behavior and internal representations for self-improvement.

Method: RE analyzes feature maps and performance signals (e.g., loss) across multiple mappings of data through identical architectures. MVRE and HMVRE aggregate signals from parallel or diverse model instances, while Sc-HMVRE adds scalability.

Result: RE enables iterative self-improvement and behavior-aware learning, laying the foundation for scalable, introspective AI.

Conclusion: RE shifts DL from representational learning to self-evolving systems, offering a path toward adaptive and introspective AI.

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [46] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: An efficient TMR approach using XAI (LRP) to enhance DNN reliability against bit-flip faults, improving reliability by over 60% with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Ensuring DNN reliability in safety-critical domains is essential, but TMR's overhead is significant. Selective application based on parameter importance can optimize efficiency.

Method: Uses gradient-based XAI (LRP) to calculate importance scores for DNN parameters, applying TMR selectively to the most critical weights.

Result: Evaluated on VGG16 and AlexNet with MNIST/CIFAR-10, the method improved AlexNet reliability by over 60% at a bit error rate of 10-4 with comparable overhead.

Conclusion: The proposed XAI-based TMR method effectively enhances DNN reliability against bit-flip faults while maintaining efficiency.

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [47] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: A hybrid decision support system combines ML and HCI to help farmers in Karnataka, India, by predicting crop suitability and market prices, delivered via a voice-based interface for low-literacy users.


<details>
  <summary>Details</summary>
Motivation: Farmers in developing regions face market and climate volatility but lack digital access due to literacy barriers. This system aims to bridge that gap.

Method: Uses a Random Forest for agronomic suitability and an LSTM for market price forecasting, integrated into a voice-based Kannada interface.

Result: Random Forest achieves 98.5% accuracy in suitability prediction; LSTM forecasts prices with low error.

Conclusion: The system enhances financial resilience for marginalized farmers through data-driven, accessible recommendations.

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [48] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA reduces parameters for LLM fine-tuning but lacks consistent speed improvements. The paper analyzes LoRA's limitations and proposes more efficient methods.


<details>
  <summary>Details</summary>
Motivation: Inconsistency in LoRA's speed improvements across model architectures and training setups.

Method: Comprehensive analysis of LoRA's performance, investigation of limiting factors, and proposal of new efficient fine-tuning methods.

Result: Proposed methods achieve comparable or superior performance with more consistent speed improvements than LoRA.

Conclusion: Provides insights and guidelines for optimizing LLM fine-tuning under resource constraints.

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [49] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: A Physics-Informed Neural Network (PINN) framework is introduced to model pollutant dispersion in oceans, overcoming traditional numerical method limitations by embedding physical laws and handling noisy data.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with modeling pollutant transport in large, dynamic oceanic domains due to complexity and scale.

Method: The PINN framework integrates physical laws (2D advection-diffusion equation) into neural network training, using synthetic data with noise and a hybrid loss function (PDE residuals, boundary/initial conditions, data fit).

Result: The model achieves physically consistent predictions, addressing non-linear dynamics and boundary/initial condition enforcement.

Conclusion: The PINN approach, leveraging Julia for high-performance computing, offers a scalable and flexible alternative to traditional solvers.

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [50] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: A transformer-based method for money laundering detection uses contrastive learning and a two-thresholds approach with BH procedure, outperforming rule-based and LSTM methods.


<details>
  <summary>Details</summary>
Motivation: To improve money laundering detection by leveraging structured time series data with minimal supervision.

Method: Uses a transformer neural network with contrastive learning for unsupervised representation learning, followed by a scoring system with controlled false positives via the BH procedure.

Result: The method effectively detects fraudsters and nonfraudsters while controlling false positives, outperforming rule-based and LSTM approaches.

Conclusion: The transformer-based approach is superior for money laundering detection, requiring less expert supervision and maintaining low false positives.

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [51] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: CompactifAI compression for Llama 3.1 8B reduces computational resources while maintaining accuracy, enhancing efficiency and cost-effectiveness.


<details>
  <summary>Details</summary>
Motivation: To evaluate the efficiency and accuracy of CompactifAI compression on Llama 3.1 8B, comparing it to the full-size model.

Method: Used Codecarbon for energy efficiency and Ragas for accuracy assessment, comparing compressed and full-size models.

Result: Compressed model significantly reduced computational resources without compromising accuracy.

Conclusion: CompactifAI makes Llama 3.1 8B more efficient, scalable, and cost-effective while retaining performance.

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [52] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [53] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: A small model with a compressed literature database and structured reasoning outperforms general-purpose models in advising systems for hypothesis refinement, achieving high acceptance rates.


<details>
  <summary>Details</summary>
Motivation: Address the gap in scalable advising systems for high-quality feedback in hypothesis and experimental design refinement.

Method: Explores factors like model size, context length, confidence estimation, and structured reasoning. Tests a small model with a literature database and reasoning framework.

Result: Outperforms general-purpose models (e.g., Deepseek-R1) with a 90%+ acceptance rate for high-confidence predictions on ICLR 2025.

Conclusion: The system enhances hypothesis generation and experimental design quality and efficiency.

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [54] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: The paper proposes a Transferability Aware Transformer (TAT) to adapt knowledge from Alzheimer's disease (AD) datasets to improve Lewy Body Disease (LBD) diagnosis, addressing data scarcity and domain shift issues.


<details>
  <summary>Details</summary>
Motivation: LBD is understudied and shares similarities with AD, but data scarcity and domain shift hinder effective diagnosis using deep learning.

Method: The TAT method uses structural connectivity from MRI, leveraging attention mechanisms to prioritize transferable features and suppress domain-specific ones.

Result: TAT effectively reduces domain shift and improves LBD diagnostic accuracy despite limited data.

Conclusion: This study pioneers domain adaptation from AD to LBD, offering a framework for diagnosing rare diseases with scarce data.

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [55] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: A novel training-free proxy, WRCor, improves neural architecture search (NAS) by efficiently estimating architectures without training, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot NAS methods lack effectiveness, stability, and generality, prompting the need for a better training-free proxy.

Method: WRCor uses correlation coefficient matrices of responses to measure expressivity and generalizability, combined with voting proxies for efficiency.

Result: WRCor outperforms existing proxies in evaluation and achieves a 22.1% test error on ImageNet-1k in just 4 GPU hours.

Conclusion: WRCor is a highly efficient and effective zero-shot NAS method, with publicly available code for broader use.

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [56] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: FedRAS is a communication-efficient Federated Recommender System framework that reduces communication overhead by clustering gradients into actions, avoiding direct compression of item embeddings, and adapts to heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: Federated Recommender Systems (FedRecs) face high communication overhead and low training efficiency due to massive item embeddings and heterogeneous environments. Existing compression methods degrade model performance.

Method: FedRAS uses an action-sharing strategy to cluster gradients into model updating actions for communication, avoiding direct compression of embeddings. It includes an adaptive clustering mechanism for heterogeneous environments.

Result: FedRAS reduces communication payload size by up to 96.88% without sacrificing recommendation performance in heterogeneous scenarios.

Conclusion: FedRAS effectively addresses communication and efficiency issues in FedRecs while maintaining performance, and is open-sourced for broader use.

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [57] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: FLLL3M is a federated learning framework using LLMs for mobility modeling, ensuring privacy and efficiency with SOTA results.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in Next-Location Prediction (NxLP) while maintaining high accuracy and low resource usage.

Method: Retains user data locally and uses LLMs with an efficient outer product mechanism.

Result: Achieves SOTA results on Gowalla, WeePlace, Brightkite, and FourSquare, with reduced parameters (45.6%) and memory (52.7%).

Conclusion: FLLL3M is an effective, privacy-preserving solution for NxLP with significant efficiency gains.

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [58] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: DAFOS dynamically adjusts fanout and prioritizes important nodes in GNN training, improving speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Uniform neighbor sampling and static fanout limit GNN scalability and efficiency.

Method: DAFOS uses node scoring (based on degree) to focus resources on important nodes, adjusts fanout dynamically, and includes early stopping.

Result: Achieves 3.57x-12.6x speedup and improves F1 scores on benchmark datasets.

Conclusion: DAFOS is an efficient, scalable solution for large-scale GNN training.

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [59] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: The paper adapts the AMLAS methodology to create AMLAS-RL, a framework for ensuring safety in reinforcement learning (RL)-enabled cyber-physical systems (CPS).


<details>
  <summary>Details</summary>
Motivation: The integration of ML, especially RL, into CPS introduces safety challenges, and existing methods like Safe-RL lack systematic assurance. AMLAS, designed for supervised learning, doesn't address RL's unique needs.

Method: The authors adapt AMLAS to develop AMLAS-RL, an iterative framework for generating safety assurance arguments for RL-enabled systems.

Result: AMLAS-RL is demonstrated using a wheeled vehicle example, showing its applicability in ensuring safe RL behavior in CPS.

Conclusion: AMLAS-RL provides a structured approach to address safety assurance gaps in RL-enabled CPS, extending the AMLAS methodology to RL contexts.

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [60] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: TSFMs outperform traditional methods in conformal prediction for time series, especially with limited data, due to better accuracy and stable calibration.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of Time Series Foundation Models (TSFMs) with traditional methods in conformal prediction settings, particularly under data constraints.

Method: Evaluated TSFMs against statistical models and gradient boosting in conformal prediction, focusing on predictive accuracy and calibration stability.

Result: TSFMs provide more reliable prediction intervals and stable calibration, especially with limited data, outperforming traditional models.

Conclusion: Foundation models enhance conformal prediction reliability in time series, particularly in data-constrained scenarios.

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [61] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: The paper introduces e-Profits, a business-aligned metric for evaluating churn prediction models, outperforming traditional metrics like AUC and F1-score by incorporating customer-specific value, retention probability, and costs.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics (AUC, F1-score) fail to reflect financial outcomes, potentially misleading strategic decisions in retention campaigns.

Method: e-Profits uses Kaplan-Meier survival analysis for personalized retention rates and evaluates models per customer, unlike fixed population-level metrics.

Result: Benchmarking on telecom datasets shows e-Profits reshapes model rankings, highlighting financial advantages of overlooked models.

Conclusion: e-Profits is a post hoc, understandable tool for profit-driven model evaluation, aiding marketing and analytics teams.

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [62] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: The paper establishes sharp lower bounds for message-passing iterations in GNNs for solving PDEs, reducing hyperparameter tuning. It links physical PDE characteristics to GNN requirements and validates bounds with examples.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of hyperparameter tuning in GNNs for PDEs by deriving theoretical lower bounds for message-passing iterations.

Method: Bounds are derived for hyperbolic, parabolic, and elliptic PDEs by analyzing the relationship between physical constants, discretization, and GNN message-passing mechanisms.

Result: Below the bounds, GNNs perform poorly; satisfying them ensures accurate solutions. Four examples validate the bounds' sharpness.

Conclusion: The proposed lower bounds optimize GNN performance for PDEs, reducing tuning effort while ensuring accuracy.

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [63] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: The paper examines data biases in AI, identifies three common types, and introduces the Data Bias Profile (DBP) to detect and mitigate their discriminatory effects.


<details>
  <summary>Details</summary>
Motivation: Data biases drive algorithmic discrimination but are understudied, hindering effective mitigation.

Method: Study three data biases' effects, develop detection mechanisms, and create the DBP for systematic documentation.

Result: Underrepresentation is less harmful than proxies and label bias; DBP effectively predicts discriminatory risks.

Conclusion: DBP bridges fairness research and policy, offering a data-centric approach to anti-discrimination.

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [64] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: A learning-based travel demand model is introduced, offering a scalable, data-driven alternative to traditional activity-based models, validated with high accuracy in Los Angeles.


<details>
  <summary>Details</summary>
Motivation: Traditional activity-based models (ABMs) are costly, inflexible, and rely on simplifications. This paper aims to address these limitations with a generative, data-driven approach.

Method: The framework integrates population synthesis, activity generation, location assignment, and traffic simulation into a unified system, validated in Los Angeles with a 10 million population.

Result: The model closely replicates real-world mobility, achieving high similarity (0.97 cosine) in origin-destination matrices and low error (6.11% MAPE) in traffic speed/volume.

Conclusion: The proposed model outperforms legacy ABMs in scalability, cost, and accuracy, demonstrating strong potential for broader application.

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [65] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: A novel CLIP model-based semantic communication framework is proposed, eliminating the need for joint training. It optimizes CLIP architecture and resource allocation using PPO-based RL, improving performance in noisy wireless networks.


<details>
  <summary>Details</summary>
Motivation: To enable semantic communication without joint training and optimize performance in noisy wireless environments.

Method: Uses a CLIP model for semantic encoding/decoding without training and employs PPO-based RL for joint optimization of CLIP architecture and RB allocation.

Result: Improves convergence rate by 40% and accumulated reward by 4x compared to soft actor-critic.

Conclusion: The proposed framework efficiently handles semantic communication in noisy wireless networks, outperforming existing methods.

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [66] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: VIPEEGNet, a CNN model, achieves high accuracy in EEG-based brain activity classification, performing comparably to human experts and ranking top among competing algorithms.


<details>
  <summary>Details</summary>
Motivation: Address limitations in EEG-based brain disease diagnosis due to inter-rater variability, resource constraints, and poor AI model generalizability.

Method: Developed and validated VIPEEGNet using EEG datasets from 1950 patients (development cohort) and 1532 patients (testing cohort), with expert annotations.

Result: Achieved high AUROC (0.930-0.972) for binary classification and competitive sensitivity (36.8%-88.2%) and precision (55.6%-80.4%) for multi-classification, with top-ranking external validation (KLD 0.223-0.273).

Conclusion: VIPEEGNet is a highly accurate and efficient model for EEG-based brain activity classification, outperforming existing algorithms with fewer parameters.

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [67] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: ODIA accelerates LLM-based Function Calling by distilling knowledge from larger to smaller models using online user data, reducing latency by 45-78% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: High latency in LLM-based Function Calling impacts user experience, necessitating a solution to accelerate responses while maintaining accuracy.

Method: ODIA identifies 'simple queries' from production traffic and distills knowledge from larger to smaller models, leveraging online user interaction data.

Result: Reduces response latency by 45% (expected) and 78% (median), with the smaller model handling 60% of traffic and negligible accuracy loss.

Conclusion: ODIA is a practical, automated solution for reducing latency in Function Calling, requiring minimal human intervention and continuously improving through data collection.

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [68] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Gran Falkman,Jonas Andersson,Anders Sjgren*

Main category: cs.LG

TL;DR: LL-HMC applies Hamiltonian Monte Carlo sampling to the last layer of DNNs for efficient uncertainty estimation, showing competitive performance in classification and OOD detection.


<details>
  <summary>Details</summary>
Motivation: HMC is computationally expensive for large DNNs; LL-HMC aims to reduce costs by limiting sampling to the last layer, making it feasible for data-intensive tasks.

Method: LL-HMC restricts HMC sampling to the final layer of DNNs and compares it with five other LL-PDL methods on video datasets for driver action/intention.

Result: LL-HMC performs competitively in classification and OOD detection, with additional samples improving OOD detection but not classification. Multiple chains showed no consistent benefits.

Conclusion: LL-HMC is a viable, efficient method for uncertainty estimation in DNNs, particularly for OOD detection, without significant computational overhead.

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [69] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: Fair-FLIP, a post-processing method, reduces biases in deepfake detection while maintaining accuracy, improving fairness metrics by up to 30%.


<details>
  <summary>Details</summary>
Motivation: Address biases in deepfake detection methods that disproportionately affect demographic subgroups like ethnicity and gender.

Method: Proposes Fair-FLIP, a post-processing approach that reweights a model's final-layer inputs to prioritize low-variability subgroups and demote high-variability ones.

Result: Fair-FLIP improves fairness metrics by up to 30% with only a 0.25% reduction in accuracy compared to baseline.

Conclusion: Fair-FLIP effectively mitigates biases in deepfake detection without significantly compromising performance, offering a practical solution for fair AI applications.

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [70] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: The paper addresses the lack of Lipschitz smoothness in shuffling-type gradient methods, proposing a stepsize strategy that ensures convergence under weaker assumptions while matching best-known rates.


<details>
  <summary>Details</summary>
Motivation: Existing convergence guarantees for shuffling-type gradient methods often rely on Lipschitz smoothness, which is not met in many machine learning models. This paper aims to bridge this gap.

Method: The authors revisit convergence rates without Lipschitz smoothness, using a new stepsize strategy, and analyze nonconvex, strongly convex, and non-strongly convex cases under random and arbitrary shuffling schemes.

Result: The proposed method converges under weaker assumptions and matches current best-known rates, validated by numerical experiments.

Conclusion: The study broadens the applicability of shuffling-type gradient methods, proving their efficacy under more general conditions.

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [71] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Daz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: Proximal Diffusion Models (ProxDM) use backward discretization with proximal maps instead of scores, offering faster convergence and theoretical benefits.


<details>
  <summary>Details</summary>
Motivation: To improve upon traditional score-based diffusion models by leveraging proximal maps for more efficient sampling.

Method: Backward discretization of SDEs using proximal maps, learned via proximal matching, to replace score-based methods.

Result: Theoretically, ProxDM requires fewer steps (O(d/)) for -accuracy. Empirically, it converges faster than score-matching methods.

Conclusion: ProxDM provides a practical and theoretically sound alternative to score-based diffusion models, with superior efficiency.

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [72] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: A GNN-based method improves cross-platform ad recommendation by modeling user behavior, ad content, and platform features, achieving high AUC (0.937 on Platform B).


<details>
  <summary>Details</summary>
Motivation: Enhance accuracy of cross-platform ad recommendation by capturing user interest migration across platforms.

Method: Uses GNN for multi-dimensional modeling of user behavior, ad content, and platform features.

Result: Platform B achieves 0.937 AUC; Platforms A and C show slight precision/recall drops due to uneven ad label distribution. Hyperparameter tuning improves model robustness.

Conclusion: The GNN-based method effectively captures user interest migration, with high performance on Platform B and adaptability to heterogeneous data.

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [73] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: The paper analyzes Classifier-Free Guidance (CFG) in masked discrete diffusion, revealing that early high guidance harms quality, while late-stage guidance is more impactful. It identifies an implementation flaw causing imbalanced transitions and proposes a simple, effective fix.


<details>
  <summary>Details</summary>
Motivation: To theoretically understand the role of guidance schedules in CFG for discrete diffusion and address observed quality issues in current implementations.

Method: Theoretical analysis of CFG in masked discrete diffusion, identifying flaws in current implementations, and proposing a novel guidance mechanism to smoothen transitions.

Result: Early high guidance degrades quality, while late-stage guidance is more effective. The proposed method improves sample quality with minimal code changes.

Conclusion: The study provides theoretical insights into CFG, identifies implementation flaws, and offers a simple, effective solution for better discrete diffusion performance.

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [74] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: ToxBench is a large-scale AB-FEP dataset for ML development, focusing on ER-ligand binding affinity prediction. The proposed DualBind model outperforms other ML methods, bridging the gap between ML and physics-based approaches.


<details>
  <summary>Details</summary>
Motivation: The need for accurate and efficient protein-ligand binding affinity prediction in drug discovery and toxicity assessment, balancing ML's speed with physics-based accuracy.

Method: Creation of ToxBench, a dataset with 8,770 ER-ligand complexes and AB-FEP computed binding free energies. Introduction of DualBind, a dual-loss ML model.

Result: DualBind achieves superior performance, demonstrating ML's potential to approximate AB-FEP at lower computational cost. Experimental validation shows 1.75 kcal/mol RMSE.

Conclusion: ToxBench and DualBind advance ML-based binding affinity prediction, offering a scalable alternative to computationally expensive physics-based methods.

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [75] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: PINNs simulate turbulent flows without grids or data, using adaptive architectures and advanced methods, accurately reproducing key flow statistics.


<details>
  <summary>Details</summary>
Motivation: Traditional turbulent flow simulations are computationally expensive; PINNs offer a mesh-free, data-free alternative.

Method: Uses physics-informed neural networks (PINNs) with adaptive architectures, causal training, and advanced optimization to solve fluid equations.

Result: PINNs accurately simulate turbulent flows, reproducing energy spectra, kinetic energy, enstrophy, and Reynolds stresses.

Conclusion: Neural equation solvers like PINNs can model chaotic systems, overcoming traditional computational limits.

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [76] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs combine mechanistic simulations with neural networks for robust, interpretable scientific modeling, achieving state-of-the-art results in prediction and inference tasks.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of mechanistic models (lack of flexibility) and machine learning models (data hunger, black-box nature) in scientific modeling.

Method: Introduce Simulation-Grounded Neural Networks (SGNNs), pretrained on diverse synthetic data from mechanistic simulations.

Result: SGNNs outperformed baselines in COVID-19 forecasting, chemical yield prediction, and ecological forecasting, and enabled accurate inference for unobservable targets.

Conclusion: SGNNs unify scientific theory and deep learning, offering interpretability and flexibility even without ground truth.

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [77] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: The paper introduces a systematic framework for improving diffusion models by incorporating representation guidance, leading to faster training and superior performance.


<details>
  <summary>Details</summary>
Motivation: Prior work showed aligning internal representations of diffusion models with pre-trained models improves generation quality, but a systematic approach was lacking.

Method: The paper proposes decompositions of denoising models and introduces two strategies: joint modeling of multimodal pairs and an optimal training curriculum balancing representation learning and generation.

Result: Experiments show 23.3x faster training on ImageNet 256x256 and 4x speedup over REPA, with improved performance across tasks.

Conclusion: The framework effectively enhances representation alignment in diffusion models, achieving faster training and better results.

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [78] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: The paper explores how model leaderboards can be exploited to distribute poisoned models at scale, introducing TrojanClimb, a framework for injecting malicious behaviors while maintaining high leaderboard rankings.


<details>
  <summary>Details</summary>
Motivation: To uncover vulnerabilities in the machine learning ecosystem, specifically how adversaries can misuse model leaderboards for large-scale poisoning attacks.

Method: The authors propose TrojanClimb, a framework for embedding harmful functionalities (e.g., backdoors, bias) in models while ensuring competitive leaderboard performance. They test it across four modalities: text-embedding, text-generation, text-to-speech, and text-to-image.

Result: TrojanClimb successfully achieves high leaderboard rankings while embedding arbitrary harmful functionalities, demonstrating the feasibility of such attacks.

Conclusion: The study reveals a critical vulnerability in leaderboard evaluation mechanisms, urging redesigns to detect malicious models and raising awareness about risks of unverified model adoption.

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [79] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: A self-supervised deep learning model analyzes multi-modal signals (EEG, ECG, respiratory) to predict CVD risk, validated in large cohorts and improving traditional risk scores.


<details>
  <summary>Details</summary>
Motivation: To enhance CVD risk assessment by leveraging multi-modal physiological signals and deep learning for personalized predictions.

Method: Self-supervised deep learning model trained on 4,398 participants, validated externally on 1,093. Projection scores derived from signal embeddings.

Result: ECG predicted CVD mortality; EEG predicted hypertension and CVD mortality. Combined with Framingham Risk Score, AUC improved (0.607-0.965).

Conclusion: The framework generates individualized CVD risk scores from PSG data, offering potential for clinical integration and personalized care.

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [80] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: Gaze-informed RLHF improves efficiency by using human gaze data for faster convergence and reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: RLHF is computationally expensive, and human gaze data is an underutilized signal for alignment.

Method: Two approaches: gaze-aware reward models and gaze-based sparse reward distribution at token level.

Result: Faster convergence with maintained or slightly improved performance, reducing computational costs.

Conclusion: Human gaze is a valuable signal for RLHF, offering a promising direction for efficiency improvements.

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [81] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: The paper identifies flaws in current LLM inference evaluation methods, highlighting common anti-patterns in Baseline Fairness, Evaluation Setup, and Metric Design, and proposes a framework to improve evaluation rigor.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methodologies for LLM inference systems are flawed, leading to misleading conclusions and hindering scientific progress.

Method: The authors systematically analyze recent systems to identify recurring anti-patterns and propose a checklist to avoid them, demonstrated through a case study on speculative decoding.

Result: The analysis reveals how anti-patterns distort performance evaluation, and the proposed framework aims to ensure robust and reproducible results.

Conclusion: The paper establishes a rigorous evaluation foundation to accelerate progress in LLM inference by aligning methods with real-world requirements.

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [82] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: A novel distributed pre-training method reduces memory demands by training small subnetworks on separate workers, avoiding inter-node activation communication and maintaining low bandwidth. Stochastic block dropping outperforms width-wise subnetwork construction, reducing memory usage by 20-40% without performance loss.


<details>
  <summary>Details</summary>
Motivation: Large model pre-training imposes heavy memory demands and high communication costs. The goal is to reduce these while maintaining performance.

Method: Train small, structured subnetworks on separate workers, avoiding inter-node activation communication. Compare two subnetwork strategies: stochastic block dropping and width-wise construction.

Result: Stochastic block dropping outperforms width-wise construction, reducing memory usage by 20-40% without performance loss.

Conclusion: The proposed method effectively reduces memory and communication costs while maintaining model performance, with stochastic block dropping being the superior strategy.

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [83] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: The paper introduces Recursive MDN (R-MDN) to address confounder effects in continual learning, ensuring invariant feature representations and equitable predictions.


<details>
  <summary>Details</summary>
Motivation: Confounders cause spurious correlations and biased predictions, especially challenging in continual learning where data distributions change over time.

Method: Proposes R-MDN, a layer integrated into deep learning architectures, using recursive least squares to continually adjust for confounders.

Result: R-MDN reduces catastrophic forgetting and promotes equitable predictions across population groups in both static and continual learning.

Conclusion: R-MDN effectively mitigates confounder effects in continual learning, enhancing model fairness and adaptability.

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [84] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: Proposes behavioral exploration for autonomous agents, enabling fast online adaptation and expert-like exploration by training a generative model on expert demonstrations.


<details>
  <summary>Details</summary>
Motivation: Humans quickly explore and adapt, but existing methods rely on slow, random exploration. The goal is to match human-like capabilities in autonomous agents.

Method: Trains a long-context generative model on expert demonstrations to predict actions based on past observations and exploratory behavior measures.

Result: Effective in simulated and real-world robotic tasks, showing adaptive, exploratory behavior.

Conclusion: Behavioral exploration enables agents to mimic and adapt expert behaviors, achieving fast online adaptation and targeted exploration.

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [85] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: A framework improves efficiency of Gaussian-based Probabilistic Generative Models by replacing redundant steps with a closed-form Gaussian approximation, maintaining quality and fidelity.


<details>
  <summary>Details</summary>
Motivation: High computational costs of long generative trajectories in GPGMs limit practical deployment.

Method: Identify a step where data becomes Gaussian-like, then replace remaining trajectory with a closed-form Gaussian approximation.

Result: Substantial improvements in sample quality and computational efficiency across data modalities.

Conclusion: The method enhances efficiency without sacrificing training granularity or inference fidelity.

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [86] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: The paper addresses imitation learning in continuous dynamical systems, proposing minimal interventions like 'action chunking' and 'noise injection' to mitigate compounding errors.


<details>
  <summary>Details</summary>
Motivation: Imitation learning in physical settings (e.g., robotics) is complex due to compounding errors, requiring advanced solutions beyond expert trajectories.

Method: Proposes 'action chunking' for stable systems and 'noise injection' for unstable systems to reduce compounding errors.

Result: The interventions align with modern robot learning practices and provide provable benefits distinct from their original design purposes.

Conclusion: The study bridges control theory and reinforcement learning, revealing novel insights for imitation learning in continuous systems.

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [87] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amalle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: The paper introduces QT-SimAM, a novel model combining Queue-Theory and attention for flight delay prediction, showing high accuracy and transferability.


<details>
  <summary>Details</summary>
Motivation: Flight delays cause financial and operational disruptions; improving prediction models can enhance passenger experience and reduce revenue loss.

Method: Proposes QT-SimAM (Bidirectional), combining Queue-Theory and a simple attention model, validated on US and EU datasets.

Result: Achieved accuracy of 0.927 (US) and 0.826 (EU), with F1 scores of 0.932 and 0.791, outperforming existing methods.

Conclusion: QT-SimAM is an effective, end-to-end solution for flight delay prediction, improving accuracy and decision-making across networks.

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [88] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: The paper extends the Generalized Projected Bellman Error (GPBE) to support multistep credit assignment using -return, introducing three gradient-based methods for optimization. These methods outperform PPO and StreamQ in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Fast and stable off-policy learning in deep RL is challenging due to divergence risks in semi-gradient TD methods, while GTD methods, though stable, are underutilized in deep RL.

Method: Extends GPBE to multistep credit assignment via -return, derives three gradient-based methods, and provides forward-view (compatible with replay) and backward-view (compatible with streaming) formulations.

Result: Proposed algorithms outperform PPO and StreamQ in MuJoCo and MinAtar environments.

Conclusion: The extension of GPBE to multistep credit assignment improves stability and performance in deep RL, validated by empirical results.

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [89] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: Direct preference optimization (DPO) connects Savage's loss functions and stochastic choice theories, supporting abstention, non-convex objectives, and extensions like margins and length corrections.


<details>
  <summary>Details</summary>
Motivation: To provide a principled understanding of DPO's operation, given its diverse applications and current momentum, and to highlight the pitfalls of deviating from this framework.

Method: Establishes a connection between Savage's loss functions and stochastic choice theories, generalizing DPO to include abstention, non-convex objectives, and extensions.

Result: The framework covers notable DPO extensions and helps identify pitfalls and workarounds in variations.

Conclusion: Understanding DPO's general principles is crucial for its diverse applications and avoiding pitfalls in its variations.

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [90] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: The paper introduces a generalized framework for low-rank decomposition (PCA, ICA) of continuous-time signals using implicit neural representations, enabling applications to irregularly sampled data.


<details>
  <summary>Details</summary>
Motivation: To extend low-rank decomposition techniques (PCA, ICA) to continuous-time signals and irregularly sampled data, where traditional methods fail.

Method: Uses a model-agnostic implicit neural signal representation framework with a contrast function in the loss to enforce desired statistical properties (decorrelation, independence).

Result: The framework successfully unifies PCA and ICA for continuous-time signals and applies to irregularly sampled data.

Conclusion: The proposed method generalizes low-rank decomposition to continuous domains, enabling broader applications like point clouds and irregular signals.

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [91] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: DejaVu exploits network delays to create temporal misalignments in multimodal fusion (MMF) for autonomous driving, degrading perception tasks. AION, a defense patch, detects such attacks with high accuracy.


<details>
  <summary>Details</summary>
Motivation: MMF in autonomous driving is vulnerable to temporal misalignments due to network-induced delays, which attackers can exploit to degrade perception tasks.

Method: DejaVu introduces subtle temporal misalignments in sensor streams (camera and LiDAR). AION uses cross-modal temporal consistency, shared representation learning, and dynamic time warping for detection.

Result: DejaVu reduces car detection mAP by 88.5% and MOTA by 73%. AION achieves AUROC scores of 0.92-0.98 with low false positives.

Conclusion: AION is a robust defense against temporal misalignment attacks, demonstrating high detection accuracy across models and datasets.

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [92] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: The paper introduces S2SRec2, a set-to-set recommendation framework for grocery e-commerce, improving ingredient suggestions by predicting multiple complementary ingredients and assessing basket completeness.


<details>
  <summary>Details</summary>
Motivation: Traditional methods predict single missing ingredients and ignore real-world needs for multiple ingredients and their relationships.

Method: Reformulates basket completion as a set-to-set problem using a Set Transformer in a multitask learning paradigm.

Result: S2SRec2 outperforms single-target baselines in experiments on large-scale recipe datasets.

Conclusion: S2SRec2 enhances grocery shopping and culinary creativity by accurately predicting complementary ingredients and ensuring basket coherence.

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [93] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: Eigenoptions in RL aid exploration and credit assignment, but online discovery can hinder learning. A method for learning option-values in deep RL is proposed, showing the impact of termination conditions.


<details>
  <summary>Details</summary>
Motivation: To explore if eigenoptions can accelerate credit assignment in model-free RL and evaluate their role in both exploration and learning.

Method: Evaluate eigenoptions in tabular and pixel-based gridworlds, propose a method for learning option-values in deep RL under non-linear function approximation.

Result: Pre-specified eigenoptions aid exploration and credit assignment, but online discovery can bias experience and hinder learning. Termination conditions impact performance.

Conclusion: Eigenoptions show promise for supporting credit assignment and exploration in RL, but their use involves complexity, especially in online discovery.

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [94] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: The paper introduces GPAWP, a framework combining graph prompts with weight pruning to enhance GNN performance and efficiency by reducing negative prompts.


<details>
  <summary>Details</summary>
Motivation: GNNs face challenges like long training times and insufficient feature extraction. Graph prompts and pre-training offer potential improvements, but their optimization and impact on stability are overlooked.

Method: Proposes GPAWP, using an importance assessment function to hierarchically prune negative graph prompts, improving parameter efficiency.

Result: Experiments on benchmark datasets show GPAWP reduces parameters significantly while maintaining competitive performance in node classification.

Conclusion: GPAWP effectively enhances GNN efficiency and performance by optimizing graph prompts through pruning.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [95] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: POIFormer is a Transformer-based framework for accurate POI attribution, addressing GPS inaccuracies and dense urban POI clustering by modeling spatial, temporal, contextual, and behavioral features.


<details>
  <summary>Details</summary>
Motivation: Accurate POI attribution is crucial for mobility analytics and urban planning but is hindered by GPS inaccuracies and high POI density in cities.

Method: POIFormer uses Transformer self-attention to jointly model spatial proximity, visit timing/duration, POI semantics, and user/crowd behavior patterns, masking the current visit for context.

Result: POIFormer outperforms baselines in real-world datasets, especially in noisy, dense urban settings.

Conclusion: POIFormer offers a practical, generalizable solution for POI attribution without relying on hard-to-access data.

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [96] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: MolecBioNet is a graph-based framework integrating molecular and biomedical knowledge for improved drug-drug interaction (DDI) prediction, outperforming existing methods by modeling drug pairs as unified entities and using multi-scale representations.


<details>
  <summary>Details</summary>
Motivation: Current graph-based DDI prediction methods treat drug pairs independently and fail to integrate biological networks and molecular structures, limiting accuracy and interpretability.

Method: MolecBioNet models drug pairs as unified entities, extracts subgraphs from biomedical knowledge graphs, and constructs hierarchical interaction graphs. It uses CASPool and AGIPool for pooling and mutual information minimization for embedding fusion.

Result: MolecBioNet outperforms state-of-the-art methods in DDI prediction, validated by ablation studies and embedding visualizations.

Conclusion: The framework provides a comprehensive, interpretable, and accurate approach to DDI prediction by unifying drug pair modeling and integrating multi-scale knowledge.

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [97] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: The paper proposes a method for continual reinforcement learning (CRL) using online world models to prevent catastrophic forgetting, achieving better performance than deep models with continual learning techniques.


<details>
  <summary>Details</summary>
Motivation: CRL agents often forget previous tasks when learning new ones (catastrophic forgetting). The paper aims to address this by leveraging online world models.

Method: The approach involves learning a Follow-The-Leader shallow model online for world dynamics and planning with model predictive control. The model is immune to forgetting by design.

Result: The method achieves a regret bound of O((KD log(T))) and outperforms deep world models with continual learning techniques in the Continual Bench environment.

Conclusion: The proposed FTL Online Agent effectively learns new tasks without forgetting old skills, demonstrating superior performance in CRL.

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [98] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen is a fully AI-driven weather forecasting system that completes data assimilation to medium-range forecasting in 17 seconds, rivaling NWP systems in accuracy and achieving an 8.25-day skillful lead time.


<details>
  <summary>Details</summary>
Motivation: Current AI-driven weather models depend on NWP systems for initial conditions, which are time-consuming. XiChen aims to eliminate this dependency by offering a faster, fully AI-driven solution.

Method: XiChen uses a pre-trained foundation model fine-tuned for data assimilation and forecasting, integrating 4D variational knowledge to handle conventional and raw satellite observations.

Result: XiChen achieves comparable accuracy to NWP systems with a forecasting lead time of over 8.25 days, completing the entire process in 17 seconds.

Conclusion: XiChen demonstrates the feasibility of fully AI-driven weather forecasting, independent of NWP systems, with significant speed and accuracy advantages.

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [99] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: DeepX-GAN, a knowledge-informed deep generative model, simulates unseen climate extremes beyond historical records, revealing latent risks in vulnerable regions like MENA, Indo-Pakistan, and Central Africa.


<details>
  <summary>Details</summary>
Motivation: Address the gaps in understanding climate extremes by capturing spatial dependence and simulating statistically plausible but historically unobserved extremes.

Method: Develop DeepX-GAN, a generative adversarial network, to model spatial structure of rare extremes, including 'checkmate' (direct hits) and 'stalemate' (near misses) scenarios.

Result: Unseen extremes disproportionately impact vulnerable regions; future warming may shift hotspots to Indo-Pakistan and Central Africa, exposing blind spots in hazard planning.

Conclusion: Spatially adaptive policies are needed to anticipate emergent risk hotspots, moving beyond historical extrapolation for resilience planning.

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [100] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: The paper introduces a warm-start model to accelerate conditional generation in iterative generative models by providing a better-informed starting point, reducing the number of required function evaluations.


<details>
  <summary>Details</summary>
Motivation: Iterative generative models are slow due to the need for many function evaluations. The goal is to speed up conditional generation without sacrificing quality.

Method: The warm-start model predicts an informed prior (N(mu, sigma)) conditioned on input context, reducing the distance the generative process must traverse. A conditional normalization trick ensures compatibility with existing models.

Result: The method achieves competitive results with a 1000-step DDPM baseline using only 11 function evaluations (1 for warm start, 10 for generation).

Conclusion: The warm-start model is a simple, effective way to accelerate conditional generation, compatible with existing models and samplers, and can be combined with other techniques for further efficiency.

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [101] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: The paper introduces a constructive wavelet neural network (WNN) that improves accuracy and reduces computational costs by selecting initial wavelet bases based on frequency analysis and dynamically adding new bases.


<details>
  <summary>Details</summary>
Motivation: Traditional WNNs face challenges in constructing accurate wavelet bases and high computational costs, limiting their application.

Method: The study proposes a constructive WNN framework with a frequency estimator and wavelet-basis increase mechanism, prioritizing high-energy bases for efficiency.

Result: The framework is validated through four examples, demonstrating improved accuracy and computational efficiency in various applications.

Conclusion: The proposed constructive WNN offers a versatile and practical solution for signal processing and time-series analysis, with code made publicly available.

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [102] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD accelerates Transformer temporal point process sampling using speculative decoding, achieving 2-6 speedup while maintaining output distribution.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between powerful Transformer TPP models and the need for rapid sequence sampling.

Method: Leverages a smaller draft model to generate candidate events, verified by a larger target model in parallel.

Result: Produces identical distributions as standard methods with 2-6 speedup.

Conclusion: TPP-SD efficiently combines speculative decoding with TPPs, enabling practical rapid sampling.

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [103] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: The paper introduces two lightweight modules (CKM and CSM) for dynamic patch size control in patch-based transformer surrogates, improving runtime efficiency and rollout fidelity without retraining.


<details>
  <summary>Details</summary>
Motivation: Fixed patch sizes in patch-based transformer surrogates limit their deployment efficiency. The goal is to enable dynamic patch size control without compromising accuracy.

Method: Introduces Convolutional Kernel Modulator (CKM) and Convolutional Stride Modulator (CSM) for dynamic patch size adjustment. Uses cyclic patch-size rollout to mitigate artifacts.

Result: Improves rollout fidelity and runtime efficiency in 2D and 3D PDE benchmarks. Enables inference-time patch-size tunability without retraining.

Conclusion: The framework provides a plug-and-play solution for compute-adaptive modeling in PDE surrogate tasks, applicable across architectures.

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [104] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: A framework for selective imputation in time series data, leveraging uncertainty to improve accuracy and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of confidence measures in existing imputation methods, especially in healthcare where missing data is common and unreliable imputations can be critical.

Method: Introduces a general framework to quantify and leverage uncertainty for selective imputation, avoiding highly unreliable values.

Result: Experiments on EHR datasets show reduced imputation errors and improved performance in downstream tasks like mortality prediction.

Conclusion: Incorporating uncertainty into time series imputation enhances reliability and practical utility, particularly in healthcare.

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [105] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: A meta-autoencoder (MAE) is introduced as an autoencoder for a collection of autoencoders, enabling compact representation and encoding/decoding of class-specific AEs, with applications in studying natural evolution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to generalize the concept of autoencoders to handle multiple classes dynamically evolving from each other, particularly useful in modeling natural evolution.

Method: The paper introduces MAEs as neural networks that learn compact representations and encoding/decoding processes for a family of class-specific autoencoders.

Result: Initial examples and a constructive definition of MAEs are provided, showcasing their potential in machine learning and biology.

Conclusion: MAEs offer a promising framework for capturing dynamic properties across evolving classes, with applications in both machine learning and biological research.

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [106] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: A novel fair CCA method is proposed to ensure fair representation learning by making projected features independent of sensitive attributes, improving fairness without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Fairness in machine learning is crucial, and existing fair CCA methods often neglect downstream classification impacts, limiting their practicality.

Method: The proposed fair CCA method ensures projected features are independent of sensitive attributes, balancing fairness and accuracy.

Result: Validated on synthetic and real-world ADNI data, the method maintains high correlation performance while enhancing fairness in classification.

Conclusion: This work enables fair machine learning in neuroimaging, supporting unbiased analysis in critical applications.

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [107] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: The paper introduces Noise-Conditioned Graph Networks (NCGNs) to improve generative modeling of spatially structured graphs by dynamically adapting architecture to noise levels.


<details>
  <summary>Details</summary>
Motivation: Existing flow-based generative models for graphs use noise-independent architectures, limiting their expressiveness.

Method: Proposes NCGNs with Dynamic Message Passing (DMP), which adjusts message passing range and resolution based on noise levels.

Result: DMP outperforms noise-independent architectures in domains like 3D point clouds, transcriptomics, and images.

Conclusion: NCGNs, particularly DMP, enhance generative modeling of spatially structured graphs by adapting to noise levels.

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [108] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: The paper investigates how multi-head latent attention (MLA) impacts transformer capacity during pretraining, revealing that decoupled rotary embeddings prevent spectral fragmentation and maintain capacity.


<details>
  <summary>Details</summary>
Motivation: To understand the effects of MLA on transformer capacity and how rotary embeddings influence spectral behavior during pretraining.

Method: Analyzes the spectrum of the $W_{Q}W_{K}^\top$ gram matrix using Marchenko-Pastur diagnostics, comparing MHA, MLA-PreRoPE, and MLA-Decoupled variants.

Result: Finds capacity bottlenecks and rank collapse in MHA and MLA-PreRoPE, while MLA-Decoupled avoids these issues by maintaining broad spectral support.

Conclusion: The application of rotary embeddings is critical; sharing them across heads mitigates spectral fragmentation and preserves capacity.

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [109] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: A systematic method using scaling laws to determine optimal data mixtures for large foundation models, validated across LLMs, NMMs, and LVMs.


<details>
  <summary>Details</summary>
Motivation: The trial-and-error approach for selecting data mixtures is impractical for large-scale pretraining, necessitating a principled alternative.

Method: Proposes scaling laws to predict model loss for any domain weight vector, validated in LLM, NMM, and LVM settings.

Result: Scaling laws accurately predict performance, extrapolate to new mixtures and scales, and derive optimal domain weights.

Conclusion: The method offers a cost-effective, principled alternative to trial-and-error for optimizing data mixtures in large models.

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [110] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: The paper introduces adversarial activation patching to detect and mitigate deceptive behaviors in large language models (LLMs), demonstrating its effectiveness through experiments and proposing mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: LLMs aligned for safety can still exhibit deceptive behaviors, posing risks. The study aims to address this by developing a framework to identify and counteract such behaviors.

Method: Adversarial activation patching is used to simulate vulnerabilities in transformer-based models by sourcing activations from deceptive prompts and patching them into safe forward passes.

Result: Experiments show adversarial patching increases deceptive outputs to 23.9% from a 0% baseline, with layer-specific variations supporting hypotheses.

Conclusion: The work advances AI safety by highlighting patching's dual-use potential and provides a roadmap for empirical studies on large-scale models.

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [111] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: The paper applies information geometry to analyze model compression, emphasizing the importance of defining optimal low-compute submanifolds and projecting onto them. It highlights the role of information divergences for zero-shot accuracy and iterative methods for fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying large deep learning models on resource-constrained devices by exploring effective compression techniques through the lens of information geometry.

Method: The study analyzes existing model compression methods, focusing on operator factorization and information divergences. It introduces iterative singular value thresholding for training under soft rank constraints.

Result: The paper demonstrates that using information divergences improves zero-shot accuracy in pre-trained models, while iterative methods enhance trainability for fine-tuned models. Simple modifications to existing methods yield better performance under fixed compression rates.

Conclusion: Information geometry provides a valuable framework for understanding and improving model compression, with iterative methods and softer rank reduction offering practical advantages.

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [112] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net is a novel architecture for causal discovery in multivariate time series, combining dilated convolutions and dynamic sparse attention for accuracy and interpretability. It outperforms existing models and provides insights into causal relationships.


<details>
  <summary>Details</summary>
Motivation: Understanding causal relationships in MTS is crucial for decision-making in fields like finance and marketing, where complex dependencies and lagged effects challenge traditional methods.

Method: DyCAST-Net integrates dilated temporal convolutions and dynamic sparse attention with adaptive thresholding to capture multiscale dependencies and eliminate spurious connections. A statistical shuffle test validates robustness.

Result: DyCAST-Net outperforms models like TCDF, GCFormer, and CausalFormer, offering precise causal delay estimation and reduced false discoveries. Attention heatmaps reveal hidden causal patterns.

Conclusion: DyCAST-Net is effective for high-dimensional, dynamic settings, with interpretable insights and scalability across domains.

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [113] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: Transformers trained for in-context learning (ICL) fail to generalize under distribution shifts, suggesting they don't implement standard learning algorithms like OLS. Pretraining data shapes ICL behavior, with a unique spectral signature in representations for in-distribution inputs.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind ICL in transformers, particularly whether they implement standard learning algorithms like OLS or gradient descent.

Method: Studied synthetic linear regression tasks, conducted out-of-distribution generalization experiments, and performed spectral analysis of learned representations.

Result: Transformers fail to generalize after prompt distribution shifts, and in-distribution inputs exhibit a unique spectral signature correlated with low loss.

Conclusion: ICL in transformers is not equivalent to standard learning algorithms and is heavily influenced by pretraining data distribution.

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [114] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: A CNN and thermomechanical model predict temperature, stress, and strain in PWR fuel rods using limited temperature data, aiding Predictive Maintenance in NPPs.


<details>
  <summary>Details</summary>
Motivation: To reduce offline time in Nuclear Power Plants by preventing unexpected shutdowns through real-time monitoring of fuel rod conditions.

Method: Combines CNN with thermomechanical modeling, using BISON and MOOSE-THM simulations for dataset generation.

Result: Highly accurate temperature predictions, enabling stress and strain estimation without overfitting.

Conclusion: The methodology supports Predictive Maintenance tools for nuclear reactors by providing real-time monitoring capabilities.

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [115] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: The paper introduces Fourier Basis Mapping (FBM) to address issues in Fourier-based time series forecasting, integrating time-frequency features for better performance with neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing Fourier-based methods struggle with inconsistent cycles, series lengths, and overlook temporal information, limiting their forecasting accuracy.

Method: Proposes FBM, which expands and maps Fourier basis in time-frequency space, and introduces variants (FBM-L, FBM-NL, FBM-NP, FBM-S) and techniques like interaction masking and multi-scale down-sampling.

Result: Demonstrates state-of-the-art performance on diverse datasets for both long-term and short-term forecasting tasks.

Conclusion: FBM effectively integrates time-frequency features, enhancing forecasting accuracy and compatibility with various neural networks.

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [116] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: Semi-supervised regression models using in-home sensor data improved ALS functional decline tracking, with transfer learning and self-attention interpolation showing superior accuracy for subscales, while linear interpolation was stable for composite scales.


<details>
  <summary>Details</summary>
Motivation: Clinical monitoring of ALS functional decline is periodic and may miss critical changes between visits, necessitating continuous in-home monitoring and advanced modeling.

Method: Developed semi-supervised regression models (individual batch learning, cohort-level batch, incremental fine-tuned transfer learning) with linear, cubic polynomial, and self-attention pseudo-label interpolations to estimate ALSFRS-R trajectories.

Result: Transfer learning improved subscale predictions (28/32 contrasts), self-attention interpolation captured nonlinear patterns best (20/32 contrasts), and linear interpolation was stable for composite scales.

Conclusion: Matching learning methods to functional domain-specific profiles enhances ALS progression tracking, suggesting adaptive model integration for timely interventions.

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [117] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: La-Proteina introduces a novel partially latent representation for atomistic protein design, achieving state-of-the-art performance in co-designability, diversity, and structural validity.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of directly generating fully atomistic protein structures jointly with amino acid sequences, which is difficult due to variable side-chain lengths.

Method: Uses a partially latent protein representation: explicit coarse backbone modeling with per-residue latent variables for sequence and atomistic details. Flow matching in this space models joint distributions.

Result: State-of-the-art performance in benchmarks, including all-atom co-designability and motif scaffolding. Scalable to 800-residue proteins.

Conclusion: La-Proteina is a robust and scalable solution for atomistic protein design, outperforming previous models.

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [118] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: Proposes a new discrete differential operator using Vandermonde matrices for derivative estimation and function representation, addressing dimensionality and error issues in Taylor's formula.


<details>
  <summary>Details</summary>
Motivation: Taylor's formula is vital but suffers from dimensionality and error propagation in discrete settings.

Method: Uses a Vandermonde coefficient matrix from truncated Taylor series for derivative computation and function representation, with equidistant sampling for accuracy.

Result: Achieves high-order accuracy, mitigates error propagation, and provides tight error bounds, outperforming finite forward difference and interpolation methods.

Conclusion: The method is effective and superior, with broad applications in vision, fluid mechanics, and more.

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [119] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: The paper analyzes TD learning methods that bootstrap from two asymmetric value functions (QV and AV-learning), comparing their convergence and efficiency. AV-learning shows benefits over Q-learning in control, and a new AV-learning algorithm (RDQ) outperforms Dueling DQN.


<details>
  <summary>Details</summary>
Motivation: To clarify the advantages and theoretical soundness of learning two value functions (state and action values) in TD learning, compared to single-function methods like Q-learning.

Method: Analyzes QV and AV-learning families in terms of convergence and sample efficiency, comparing them to Expected Sarsa and Q-learning. Introduces RDQ, a new AV-learning algorithm.

Result: AV-learning is more efficient than Expected Sarsa in prediction and outperforms Q-learning in control. RDQ surpasses Dueling DQN in the MinAtar benchmark.

Conclusion: AV-learning, particularly RDQ, offers significant advantages in control tasks, providing a more efficient alternative to traditional single-function TD methods.

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [120] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: The paper explores the robustness of XAI methods in unbalanced datasets, proposing an evaluation method for explanation reliability, particularly for the minority class.


<details>
  <summary>Details</summary>
Motivation: The increasing use of AI models and legislative demands for transparency necessitate reliable explanations, especially in high-risk scenarios with unbalanced data.

Method: The study introduces an evaluation approach using on-manifold neighbor generation, explanation aggregation, and a consistency metric, tested on a frost event dataset.

Result: Preliminary insights highlight the challenges and potential solutions for ensuring robust explanations in unbalanced datasets.

Conclusion: The proposed method offers a practical way to assess explanation reliability in critical, unbalanced scenarios.

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [121] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: A dataset for classifying six wellness dimensions in social media posts is introduced, evaluated with ML and transformer models, and released ethically on GitHub.


<details>
  <summary>Details</summary>
Motivation: To enable region-specific wellness assessments and personalized well-being evaluations from social media content.

Method: Developed a dataset with expert-guided annotations, evaluated using traditional ML and transformer models, and assessed with cross-validation and post-hoc explanations.

Result: Performance measured via precision, recall, and F1-score; dataset supports wellness classification and mental health interventions.

Conclusion: The dataset aids in wellness assessment and intervention strategies, with ethical public release.

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [122] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: DRAGD and DRAGDP are attacks exploiting gradient discrepancies in federated unlearning to reconstruct deleted data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To expose privacy vulnerabilities in federated unlearning systems where gradient exchanges leak sensitive information.

Method: Introduces DRAGD and DRAGDP, which analyze gradient changes before and after unlearning, with DRAGDP using prior data for better accuracy.

Result: DRAGD and DRAGDP outperform existing methods in reconstructing deleted data, especially for complex datasets like facial images.

Conclusion: The work reveals a critical privacy flaw in federated unlearning and proposes a solution to enhance its security.

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [123] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ integrates low-rank approximation and mixed-precision quantization for efficient transformer deployment on edge devices, achieving up to 15% performance improvement.


<details>
  <summary>Details</summary>
Motivation: Deploying transformers on resource-constrained edge devices is challenging, requiring efficient compression techniques.

Method: MLoRQ uses a two-stage optimization: intra-layer for compression solutions and inter-layer for bit-width and rank assignments under memory constraints, with optional adaptive rounding.

Result: MLoRQ achieves state-of-the-art results with up to 15% performance improvement on Vision Transformers for various tasks.

Conclusion: MLoRQ effectively combines low-rank and quantization techniques, offering seamless integration and significant performance gains for edge deployment.

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [124] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: The paper addresses the challenge of aligning LLMs with diverse human preferences, showing current methods fall short and proposing negatively-correlated sampling to improve alignment. It introduces the Community Alignment dataset for better global representation.


<details>
  <summary>Details</summary>
Motivation: To align LLMs with diverse human preferences across cultural and political dimensions, as current methods lack sufficient variability.

Method: Conducted a large-scale multilingual human study, analyzed preference variability, proposed negatively-correlated sampling, and created the Community Alignment dataset.

Result: Humans show more preference variation than LLMs; existing methods are insufficient; negatively-correlated sampling improves alignment.

Conclusion: The Community Alignment dataset enhances LLM alignment with diverse global preferences, addressing current limitations.

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [125] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: The paper explores integrating Conformal Prediction (CP) with supervised learning on encrypted data, showing CP remains effective in the encrypted domain. It compares $p$-value and $e$-value-based CP, finding $e$-value CP achieves better coverage but with trade-offs in set compactness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between uncertainty quantification and privacy-preserving machine learning by applying CP to encrypted data.

Method: Uses AES-encrypted MNIST dataset to test CP methods ($p$-value and $e$-value-based) on deterministically encrypted data.

Result: Models on encrypted data achieve 36.88% test accuracy (vs. 9.56% random). $e$-value CP achieves 60% coverage, capturing true labels in 4888/5000 cases. $p$-value CP yields smaller sets but lower coverage.

Conclusion: CP is viable for encrypted data but involves trade-offs between set compactness and reliability, laying groundwork for secure uncertainty quantification.

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [126] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: The paper studies distributed learning in a DAG where agents observe partial features and parent predictions, aiming for information aggregation. It provides bounds for linear and general hypothesis classes, highlighting DAG depth as critical.


<details>
  <summary>Details</summary>
Motivation: To understand when distributed learning in a DAG can aggregate information effectively, despite no single agent having full feature access.

Method: Agents learn sequentially in a DAG, using observed features and parent predictions. Theoretical analysis and experiments are conducted for linear and general hypothesis classes.

Result: DAG depth is key: information aggregation succeeds over long paths with well-represented features but fails in shallow DAGs (e.g., hub-and-spokes).

Conclusion: Information aggregation in DAGs depends on depth and feature representation, with theoretical and empirical support.

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [127] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: A study compares generative and discriminative LSTM-based text classifiers with Post Training Quantization (PTQ), highlighting generative models' sensitivity to bitwidth, calibration data, and noise, and the impact of class imbalance on performance.


<details>
  <summary>Details</summary>
Motivation: Text classification in edge computing requires low latency and high accuracy. Generative classifiers are robust to noisy data but face deployment challenges due to computational constraints. PTQ offers a solution without retraining.

Method: Comparative study of generative and discriminative LSTM models using PTQ with the Brevitas library, evaluated across bitwidths and noisy conditions. Class imbalance in calibration data is analyzed.

Result: Discriminative classifiers remain robust, while generative ones are sensitive to bitwidth, calibration data, and noise. Class imbalance in calibration data degrades generative model performance at lower bitwidths.

Conclusion: Calibration data quality is crucial for PTQ. Generative classifiers' performance under noise depends on calibration data, aiding their deployment in edge environments.

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [128] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: The paper presents an open-source framework for developing correlation kernels, enhancing surrogate modeling with frequency-aware elements and diverse kernel types, validated on test cases and integrated into SMT 2.0.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional kernel functions and improve surrogate modeling for complex, frequency-sensitive systems like aircraft.

Method: Extends kernel functions (e.g., exponential squared sine, rational quadratic) and their derivatives, validates on test cases, and integrates into SMT 2.0.

Result: Successful validation on test cases (e.g., Mauna-Loa CO2, airline traffic) and a flexible toolset for customizable kernel configurations.

Conclusion: The framework advances surrogate modeling, offering versatility for engineers and researchers in frequency-sensitive domains.

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [129] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: EPT-2, an advanced AI model for Earth system forecasting, outperforms predecessors and competitors like Microsoft Aurora and ECMWF's IFS HRES. Its ensemble variant, EPT-2e, excels in probabilistic forecasting at lower computational costs.


<details>
  <summary>Details</summary>
Motivation: To advance Earth system forecasting by improving accuracy and efficiency over existing AI and numerical models.

Method: Developed EPT-2, an AI foundation model, and its ensemble variant EPT-2e for probabilistic forecasting, leveraging perturbation-based techniques.

Result: EPT-2 outperforms leading models (e.g., Microsoft Aurora, ECMWF IFS HRES) in predicting key variables. EPT-2e surpasses ECMWF ENS in probabilistic forecasting with reduced computational costs.

Conclusion: EPT-2 and EPT-2e set new benchmarks in Earth system forecasting, offering superior performance and efficiency, accessible via the app.jua.ai platform.

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [130] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: The paper explores using high-resolution remote sensing and AI to improve habitat mapping, addressing challenges like class imbalance and multi-class training, with successful validation in Europe.


<details>
  <summary>Details</summary>
Motivation: Accurate, high-resolution habitat maps are crucial for conservation but current methods struggle with thematic/spatial resolution and class imbalance.

Method: Utilized vegetation plots and AI tools, integrating multi-spectral and radar imagery, hierarchical habitat nomenclature, and ensemble machine learning.

Result: Improved habitat classification accuracy, especially in fragmented landscapes, using hierarchical strategies and Earth Observation Foundation models.

Conclusion: The framework is adaptable globally; future work should focus on dynamic habitats, segmentation, quality assessment, and next-gen data.

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [131] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: A foundational AI model for universal physics simulation learns physical laws from boundary-condition data without predefined equations, using a sketch-guided diffusion transformer to directly generate steady-state solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like PINNs and finite-difference require explicit equation encoding, limiting generalizability and discovery potential. This work aims to bypass these constraints by treating simulation as a conditional generation problem.

Method: The model uses a sketch-guided diffusion transformer with novel spatial relationship encoding to map boundary conditions to equilibrium solutions, avoiding temporal integration.

Result: Achieves SSIM > 0.8 for steady-state solutions with sub-pixel boundary accuracy and enables physics discovery via learned representations.

Conclusion: This approach shifts from AI-accelerated to AI-discovered physics, establishing a universal simulation framework.

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [132] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: Non-equivariant CNNs with rotation augmentations can match equivariant GNNs in molecular tasks, simplifying model complexity.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that explicit equivariance is essential for high-quality 3D molecular generation, given the complexity and scalability issues of equivariant GNNs.

Method: Train non-equivariant CNNs with rotation augmentations, analyze equivariance via loss decomposition, and evaluate performance on denoising, generation, and property prediction.

Result: Non-equivariant CNNs achieve comparable performance to equivariant models, with insights into model size, dataset size, and training duration effects.

Conclusion: Learned equivariance in non-equivariant models is viable, offering a simpler alternative to complex equivariant GNNs for molecular tasks.

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [133] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: A novel Mixture of Experts (MoE) model for TFBS prediction outperforms individual models, especially in OOD scenarios, and introduces ShiftSmooth for better interpretability.


<details>
  <summary>Details</summary>
Motivation: Understanding gene regulation and biological processes requires accurate TFBS prediction.

Method: Integrates multiple pre-trained CNN models into an MoE framework and introduces ShiftSmooth for robust interpretability.

Result: MoE model achieves competitive or superior performance, excelling in OOD scenarios; ShiftSmooth outperforms traditional methods in interpretability.

Conclusion: The MoE model and ShiftSmooth provide an efficient, generalizable, and interpretable solution for TFBS prediction, advancing genome biology research.

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [134] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: The paper introduces RGPD, a framework combining physics-based supervision with spatio-temporal learning for RUL and SOH estimation, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Accurate RUL and SOH estimation is critical for PHM in industrial applications, requiring advanced methods to handle spatio-temporal data and physical constraints.

Method: RGPD integrates GCRNs, GATConv, SAC, and Q-learning to dynamically weight physics-informed loss terms and enhance spatio-temporal learning.

Result: The method outperforms existing models in RUL and SOH estimation, showing robustness across diverse industrial datasets.

Conclusion: RGPD effectively combines physics and advanced learning for accurate PHM, reducing manual tuning and improving generalization.

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [135] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkr Olsen. Mads stergaard,Karl Ulbk,Sren Fns Nielsen,Rasmus Malik Hegh Lindrup,Bjrn Sand Jensen,Morten Mrup*

Main category: cs.LG

TL;DR: The paper proposes an early-exit neural network architecture for speech separation, enabling dynamic compute-scaling for embedded devices, and introduces an uncertainty-aware probabilistic framework for interpretable exit conditions.


<details>
  <summary>Details</summary>
Motivation: Current speech separation architectures lack flexibility for varying compute demands, limiting their use in embedded and heterogeneous devices.

Method: Designs an early-exit neural network and a probabilistic framework to model clean speech and error variance, deriving exit conditions based on desired signal-to-noise ratios.

Result: The single early-exit model matches state-of-the-art performance across varying compute budgets, enabling dynamic scaling.

Conclusion: The framework achieves state-of-the-art performance with interpretable exit conditions, making it suitable for resource-constrained devices.

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [136] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: The paper introduces two mechanisms for accelerating training and inference of generative models for 3D molecular conformer generation, achieving state-of-the-art quality.


<details>
  <summary>Details</summary>
Motivation: Fast and accurate molecular conformer generation is crucial for computational chemistry and drug discovery, but current methods require significant computational resources.

Method: Proposes SO(3)-Averaged Flow for faster training and reflow/distillation for efficient inference in flow-based models.

Result: Models trained with SO(3)-Averaged Flow achieve state-of-the-art conformer generation quality, and inference is accelerated with few or one-step generation.

Conclusion: The techniques enable highly efficient molecular conformer generation with flow-based models.

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [137] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: The paper introduces two methods, Blend and A-AMU, to accelerate approximate machine unlearning (AMU) by reducing computational runtime and improving convergence.


<details>
  <summary>Details</summary>
Motivation: Current AMU methods are computationally expensive due to processing retained datasets and slow convergence. The goal is to improve efficiency while maintaining model utility and privacy.

Method: 1. Blend: A dataset condensation technique merging visually similar images to reduce retained set size. 2. A-AMU: A loss-centric method combining a steepened primary loss and a novel regularizer to speed up unlearning.

Result: The dual approach significantly reduces unlearning latency in single and multi-round scenarios without compromising model utility or privacy.

Conclusion: This work is the first to systematically address unlearning efficiency by combining specialized dataset condensation and accelerated loss functions.

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [138] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: LinkedIn's STAR system combines LLMs and GNNs to tackle job recommendation challenges like cold-start and biases, offering scalable, high-performing solutions.


<details>
  <summary>Details</summary>
Motivation: Address cold-start, filter bubbles, and biases in job matching on LinkedIn.

Method: Integrates LLMs (for text understanding) and GNNs (for relationship modeling) with adaptive sampling and version management.

Result: A scalable, end-to-end solution for embedding-based recommendations with improved performance.

Conclusion: STAR provides a robust, practical framework for deploying industrial-scale recommendation systems.

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [139] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: A lightweight graph-aware Federated Learning (FL) method is proposed for traffic prediction, combining FedAvg simplicity with graph learning to capture spatial relationships efficiently.


<details>
  <summary>Details</summary>
Motivation: Standard FL methods like FedAvg ignore spatial dependencies between clients, while graph-based FL adds computational overhead. This work aims to balance efficiency and performance.

Method: The approach uses neighbourhood aggregation principles to weight client models based on graph connectivity, avoiding full model training.

Result: Evaluated on METR-LA and PEMS-BAY datasets, the method achieves competitive performance against baselines and graph-based FL techniques.

Conclusion: The proposed method effectively captures spatial relationships in traffic prediction while maintaining computational efficiency.

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [140] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: The paper explores whether neural networks can learn compressed computation circuits in practice, focusing on the Universal-AND problem. It finds a simple, dense solution that outperforms theoretical constructions.


<details>
  <summary>Details</summary>
Motivation: To investigate if neural networks can practically learn efficient computational circuits, specifically for the Universal-AND problem, and understand the types of circuits they form.

Method: A toy model for the Universal-AND problem is used, restricting hidden dimensions to pressure the model into finding compute-efficient circuits. Training reveals a dense solution.

Result: The model discovers a fully dense, scalable solution that outperforms theoretical constructions, is robust to parameter changes, and extends to other boolean operations.

Conclusion: The findings highlight the flexibility of superposition in neural networks and provide insights into network circuitry and interpretability.

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [141] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: A hybrid model combining DTW's interpretability with neural network trainability is proposed, excelling in both low-resource and rich-resource time series classification.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of neural networks (data-hungry, opaque) and DTW (non-trainable, limited scalability) by creating a versatile, interpretable, and trainable model.

Method: Develop a dynamic length-shortening algorithm to transform time series into prototypes, reformulate DTW into a recurrent neural network, and construct a trainable model mimicking DTW's behavior.

Result: The model outperforms previous methods in low-resource settings and remains competitive in rich-resource scenarios.

Conclusion: The proposed hybrid model successfully bridges the gap between interpretability and trainability, offering a flexible solution for time series classification.

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [142] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: The paper introduces a generative paradigm for cognitive diagnosis (CD), shifting from predictive to generative modeling, enabling faster and more reliable diagnosis for new learners without retraining.


<details>
  <summary>Details</summary>
Motivation: Traditional CD models are limited by expensive retraining for new learners and unreliable outputs. The study aims to overcome these scalability and reliability issues.

Method: Proposes two generative models: Generative Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model (G-NCDM), incorporating identifiability and monotonicity conditions.

Result: Achieves a 100x speedup for diagnosing new learners and outperforms traditional methods in performance.

Conclusion: The generative framework enhances scalability and reliability, opening new possibilities for AI applications in education and model evaluation.

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [143] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: TVE is a pre-training framework for relational databases that outperforms traditional methods by incorporating task heterogeneity and temporal structure.


<details>
  <summary>Details</summary>
Motivation: The challenge of designing generalizable pre-training strategies for relational databases due to task heterogeneity and the need for task-aware representations.

Method: Task Vector Estimation (TVE), which constructs predictive signals via set-based aggregation over schema traversal graphs and models relational dynamics.

Result: TVE outperforms traditional pre-training baselines on the RelBench benchmark.

Conclusion: Pre-training objectives should encode task heterogeneity and temporal structure for effective predictive modeling on relational databases.

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [144] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: The paper introduces an improved Automatic Prompt Optimization (APO) framework for LLMs, incorporating positive reinforcement and feedback diversification to enhance efficiency and effectiveness, alongside formalizing Continual Prompt Optimization (CPO) for seamless prompt migration.


<details>
  <summary>Details</summary>
Motivation: Existing APO methods focus on error correction and overlook insights from correct predictions, limiting their effectiveness. The rapid evolution of LLMs also poses challenges for prompt migration.

Method: The proposed framework uses positive reinforcement to preserve beneficial prompt components and feedback diversification to mitigate noise in LLM-generated feedback. CPO is introduced for efficient prompt migration.

Result: The approach outperforms baselines, achieving higher accuracy, faster convergence, and lower computational costs in standard and migration scenarios.

Conclusion: The enhanced APO framework and CPO provide a robust solution for optimizing and migrating prompts in evolving LLM environments.

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [145] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: The paper revisits the Schedule-Free (SF) method for large-scale training, showing its effectiveness without decay phases or memory overhead, and proposes a refined variant for improved robustness.


<details>
  <summary>Details</summary>
Motivation: Conventional pretraining strategies are inadequate for large-scale training, and existing alternatives like WSD and weight averaging have limitations.

Method: The study revisits the Schedule-Free (SF) method, analyzes its dynamics theoretically and empirically, and proposes a refined variant.

Result: SF-AdamW effectively navigates the loss landscape without decay phases or memory overhead, and the refined variant improves robustness.

Conclusion: SF is a practical, scalable, and theoretically grounded approach for language model training.

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [146] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: The paper proposes a probabilistic framework for evaluating AI models over all possible downstream tasks using Task Priors, addressing limitations of fixed benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current AI evaluation relies on fixed benchmarks, creating a bottleneck. The goal is to assess models comprehensively across all possible tasks.

Method: Defines a probabilistic space of tasks with Task Priors, enabling evaluation of average performance and variance across all tasks.

Result: The framework provides answers to key questions about model performance and variance under Task Priors.

Conclusion: Task Priors offer a new evaluation standard and could accelerate SSL research by improving qualitative signals.

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [147] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: AdaBrain-Bench is introduced as a standardized benchmark to evaluate brain foundation models in non-invasive BCI tasks, addressing the lack of practical benchmarks for assessing public models.


<details>
  <summary>Details</summary>
Motivation: The high noise and limited task-specific data in non-invasive BCI signals hinder decoding capabilities, and current brain foundation models lack comprehensive benchmarks for widespread adoption.

Method: AdaBrain-Bench includes diverse BCI datasets, a task adaptation pipeline, multi-dimensional metrics, and adaptation tools to evaluate models in cross-subject, multi-subject, and few-shot scenarios.

Result: The benchmark evaluates public brain foundation models, providing insights for model selection in various scenarios.

Conclusion: AdaBrain-Bench offers a reproducible and evolving platform to advance robust and generalized neural decoding solutions.

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [148] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: TolerantECG is a robust foundation model for ECG signals, handling noise and missing leads, outperforming benchmarks on PTB-XL and MIT-BIH datasets.


<details>
  <summary>Details</summary>
Motivation: ECG effectiveness is limited by noise and missing leads, leading to diagnostic errors.

Method: Combines contrastive and self-supervised learning to train on noisy, incomplete ECG signals and text reports.

Result: Top or second-best performance across conditions on PTB-XL; highest on MIT-BIH.

Conclusion: TolerantECG effectively addresses ECG noise and lead-missing issues, enhancing diagnostic reliability.

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [149] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: NeuTSFlow introduces a novel framework for time series forecasting by modeling continuous function families, addressing challenges in learning transitions between historical and future functions using Neural Operators and flow matching.


<details>
  <summary>Details</summary>
Motivation: Conventional time series forecasting methods treat data as discrete sequences, ignoring their continuous nature and the ambiguity in determining underlying functions from noisy observations.

Method: NeuTSFlow leverages Neural Operators to parameterize velocity fields in infinite-dimensional function spaces, enabling flow matching between historical and future function families.

Result: Experiments show NeuTSFlow outperforms traditional methods in accuracy and robustness across diverse forecasting tasks.

Conclusion: The function-family perspective and NeuTSFlow's approach effectively address the challenges of continuous time series forecasting, offering superior performance.

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [150] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: scSGC introduces a soft graph clustering method for scRNA-seq data to address limitations of hard graph constructions, improving clustering accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Hard graph constructions in GNN-based scRNA-seq clustering lose continuous similarity features and confuse inter-cluster connections, leading to biased results.

Method: scSGC uses a ZINB-based autoencoder, dual-channel soft graph embedding, and optimal transport-based clustering to capture continuous similarities.

Result: scSGC outperforms 13 state-of-the-art models in accuracy, annotation, and efficiency across ten datasets.

Conclusion: scSGC advances scRNA-seq analysis by better characterizing cellular heterogeneity and improving clustering outcomes.

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [151] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: RNNs trained on the streaming parity task show a phase transition to perfect infinite generalization, revealing a mechanism for learning algorithms from finite data.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural networks generalize beyond training data, especially in tasks like streaming parity, where infinite generalization is possible.

Method: Case study of RNNs on the streaming parity task, analyzing learning dynamics and representational merger effects.

Result: RNNs achieve perfect infinite generalization with sufficient training, constructing a finite automaton for the task.

Conclusion: The study reveals a mechanism for infinite generalization in neural networks, highlighting implicit algorithm learning.

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [152] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: The paper introduces a method to interpret LLMs' reasoning in safety-critical domains like nuclear engineering, using a Boiling Water Reactor case study. It identifies specialized neurons and their collective impact on performance.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency of LLMs in safety-critical applications, addressing regulatory challenges in nuclear engineering.

Method: Parameter-efficient fine-tuning (Low-Rank Adaptation) and neuron silencing to analyze neuron activation patterns.

Result: Silencing specialized neurons collectively degrades performance, impairing technical accuracy.

Conclusion: The methodology improves LLM transparency, aiding nuclear-grade AI assurance and regulatory compliance.

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [153] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: DepBERT, a transformer-based model incorporating dependency trees, outperforms state-of-the-art methods in extracting cause-effect phrases.


<details>
  <summary>Details</summary>
Motivation: Existing supervised methods lack integration of linguistic tools like dependency trees, which are effective for semantic extraction.

Method: DepBERT extends a transformer model by embedding dependency tree structures into its framework.

Result: DepBERT surpasses other supervised methods in causality extraction across three datasets.

Conclusion: Incorporating dependency trees into deep learning models enhances causality extraction performance.

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [154] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: MemSinks isolates memorized content in large language models by design, enabling easier removal without harming general language abilities.


<details>
  <summary>Details</summary>
Motivation: Address privacy and copyright concerns by mitigating memorization of repeated sequences in language models.

Method: Introduces MemSinks, using sequence identifiers to activate unique memorization neurons for each sequence, isolating memorized content.

Result: Effective isolation of memorized content and strong generalization demonstrated at billion-parameter scale.

Conclusion: MemSinks proves simultaneous generalization and isolation is achievable, offering a practical solution for memorization issues.

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [155] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [156] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: A neural framework combining Neural ODEs, graph attention, wavelet transforms, and adaptive learning improves energy demand/supply forecasting, outperforming baselines on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate energy forecasting is vital for sustainable systems but is hindered by renewable variability and dynamic consumption.

Method: Integrates Neural ODEs, graph attention, wavelet transformations, and adaptive frequency learning with a Runge-Kutta solver and residual connections.

Result: Outperforms state-of-the-art baselines on seven datasets (ETTh1, ETTh2, ETTm1, ETTm2, Waste, Solar, Hydro) in forecasting metrics.

Conclusion: The model robustly captures complex temporal dependencies and enhances interpretability via SHAP analysis, making it suitable for sustainable energy applications.

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [157] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: A dynamic neuron allocation method inspired by the human hippocampus improves performance on class-imbalanced datasets by periodically adding and pruning neurons during training, while maintaining the final network size.


<details>
  <summary>Details</summary>
Motivation: Fixed neuron counts in deep learning may hinder performance on imbalanced datasets. Biological insights suggest dynamic neuron allocation could enhance learning.

Method: Proposes periodic neuron addition and removal during training to boost representation for minority classes, retaining critical features from majority classes.

Result: Outperforms fixed-size networks on three datasets and five models, especially when combined with other imbalance-handling techniques.

Conclusion: Dynamic, biologically inspired network designs effectively improve performance on class-imbalanced data.

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [158] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: The paper explores unsupervised CNN-LSTM autoencoder models to identify play styles from low-level game trace data, reducing reliance on domain expertise.


<details>
  <summary>Details</summary>
Motivation: To improve game design and adaptive experiences by identifying play styles without heavy domain knowledge or handcrafted features.

Method: Uses unsupervised CNN-LSTM autoencoder models on low-level play trace data in MicroRTS.

Result: Achieves meaningful separation of game playing agents in latent space, reducing domain bias.

Conclusion: The approach successfully guides exploration of diverse play styles in AI players, minimizing reliance on expert knowledge.

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [159] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: Iceberg improves HLS prediction model generalizability via synthetic data augmentation and weak label generation, boosting accuracy and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the generalizability gap in deep learning-based HLS prediction models.

Method: Uses synthetic data augmentation (Iceberg) with LLM-generated programs and weak labels, integrated with in-context model architecture for meta-learning.

Result: 86.4% improvement in geometric mean accuracy, 2.47 and 1.12 better offline DSE performance on test datasets.

Conclusion: Iceberg effectively enhances model generalizability and performance in real-world HLS applications.

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [160] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: A method using adversarial representation learning and focal entropy to balance predictive power and privacy by sanitizing sensitive content.


<details>
  <summary>Details</summary>
Motivation: To learn representations with high predictive power while preserving user privacy by addressing information leakage in existing entropy-based methods.

Method: Adversarial representation learning with focal entropy to sanitize sensitive content from learned representations.

Result: Achieves high target utility with moderate privacy leakage on multiple benchmarks.

Conclusion: The proposed method effectively balances predictive performance and privacy protection.

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [161] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: A novel model for job classification improves accuracy by embedding jobs and hierarchical industry categories into a shared latent space, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of job titles and descriptions requires advanced models to leverage hierarchical relationships for better job classification in recruitment systems.

Method: Proposes a representation learning model integrating the SOC system and Carotene taxonomy to capture hierarchical and graph relationships in job data.

Result: The model significantly outperforms existing methods in classification accuracy, as demonstrated on a large-scale job postings dataset.

Conclusion: The research offers a robust framework for enhancing job classification, aiding recruitment decision-making.

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [162] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: Demenba, a novel ADC framework using state space models, outperforms prior methods by 21% in dementia classification, scales efficiently, and benefits from fusion with large language models.


<details>
  <summary>Details</summary>
Motivation: Early dementia detection is crucial for timely intervention, but traditional neuropsychological tests rely on manual scoring. ADC systems aim to automate this using speech recordings.

Method: Demenba employs state space models for linear memory and computation scaling with sequence length, trained on 1,000+ hours of cognitive assessments from the Framingham Heart Study.

Result: Outperforms prior methods by 21% in fine-grained dementia classification, uses fewer parameters, and shows additional gains when fused with large language models.

Conclusion: Demenba offers a scalable, efficient, and transparent approach for dementia assessment, with potential for further improvement through integration with advanced language models.

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [163] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: The paper introduces a novel distance estimation method in latent space for recommender systems, leading to the Radial Neighborhood Estimator (RNE), which improves accuracy and addresses the cold-start problem.


<details>
  <summary>Details</summary>
Motivation: To define meaningful distances in latent space for capturing user-item relationships effectively, addressing noise-induced non-centrality.

Method: Proposes row-wise and column-wise distance approximation, introduces correction via empirical variance estimator, and develops RNE with neighborhood smoothing.

Result: RNE outperforms existing methods in evaluations on simulated and real-world datasets.

Conclusion: The novel distance estimation and RNE enhance recommender system performance and mitigate the cold-start problem.

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [164] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: The paper explores inductive bias in spatial regression models, focusing on GNNWR. It identifies limitations in current approaches and proposes enhancements using neural network concepts. Benchmarking shows improved performance, with results varying by data characteristics.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current spatial regression models, particularly GNNWR, by improving inductive bias and capturing spatial non-stationarity more effectively.

Method: Generalizes GNNWR by integrating convolutional, recurrent, and transformer neural networks, introducing local receptive fields, sequential context, and self-attention.

Result: GNNWR outperforms classic methods in capturing complex spatial relationships, with performance varying by data heterogeneity, noise, and sample size.

Conclusion: Inductive bias is crucial in spatial modeling. Future work should focus on learnable weighting functions, hybrid architectures, and better interpretability for non-stationary data.

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [165] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: FedFD improves Hetero-FL by using feature distillation with orthogonal projection to stabilize training and enhance performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods in Hetero-FL rely on logit distillation, which fails to address knowledge bias from heterogeneous models, leading to unstable training.

Method: Proposes FedFD, a feature distillation approach using orthogonal projection to align features from heterogeneous models.

Result: FedFD outperforms state-of-the-art methods, demonstrating superior performance.

Conclusion: FedFD effectively mitigates knowledge bias and enhances model performance in heterogeneous federated learning.

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [166] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: TDCRL integrates causal inference into source-free domain generalization (SFDG) to address domain-specific confounders, achieving robust generalization with minimal data demands.


<details>
  <summary>Details</summary>
Motivation: Traditional DG methods require multiple source domains, which is impractical due to data collection costs. Vision-language models like CLIP enable SFDG but struggle with domain-specific confounders.

Method: TDCRL uses text prompts for data augmentation and trains a causal intervention network with a confounder dictionary to extract domain-invariant features.

Result: TDCRL achieves state-of-the-art performance on benchmarks like PACS, VLCS, OfficeHome, and DomainNet.

Conclusion: TDCRL effectively addresses domain-specific confounders in SFDG, offering robust generalization with minimal data requirements.

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [167] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: The paper introduces Temporal-Aligned Transformer (TAT) for multi-horizon time series forecasting, improving peak demand prediction by 30% using context variables like holidays and promotions.


<details>
  <summary>Details</summary>
Motivation: Accurate demand forecasting is crucial for supply chain management, especially during high-stake sales events with unpredictable demand peaks.

Method: TAT uses an encoder-decoder architecture with Temporal Alignment Attention (TAA) to leverage context variables for better alignment in peak demand forecasting.

Result: TAT achieves up to 30% accuracy improvement in peak demand forecasting while maintaining competitive overall performance.

Conclusion: TAT effectively addresses the challenge of peak demand forecasting, enhancing supply chain and customer experience.

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [168] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: A mesh-free, physics-informed Gaussian process (GP) framework is proposed for compliance minimization, using multi-output neural networks (NNs) to control design complexity and improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing ML methods for compliance minimization suffer from poor feature boundaries, high costs, and lack of design complexity control.

Method: Parameterize design and state variables with GP priors sharing a multi-output NN mean function, using PGCANs to mitigate spectral bias and control complexity. Parameters are estimated by minimizing compliance, potential energy, and volume fraction constraints.

Result: The approach achieves super-resolution topologies, smaller compliance, less gray area, fine-scale feature control, and outperforms traditional and ML-based methods.

Conclusion: The proposed GP-based framework effectively addresses limitations of existing methods, offering improved performance and control in compliance minimization.

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [169] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: The study explores energy-efficient AI/ML models using DeepRX, a ResNet-based deep learning receiver. It evaluates energy consumption, validates estimates, and applies knowledge distillation to create a compact, energy-efficient student model.


<details>
  <summary>Details</summary>
Motivation: To balance energy efficiency with performance in AI/ML models, particularly for DeepRX, addressing the challenge of high energy consumption in deep learning systems.

Method: Evaluates DeepRX's energy usage (FLOPs/Watt, FLOPs/clock), compares training vs. inference energy dynamics, and applies knowledge distillation to train a compact student model.

Result: Distilled student models achieve lower error floors across SINR levels, demonstrating energy efficiency without compromising performance.

Conclusion: Knowledge distillation effectively reduces energy consumption in DeepRX while maintaining performance, offering a viable solution for energy-efficient AI.

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [170] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: The paper explores how graph structure, including community features, impacts neural network performance in image classification, finding that densely interconnected communities enhance learning.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between graph structure (e.g., communities, degree distributions) and neural network performance, addressing gaps in prior narrow-focused studies.

Method: Uses model networks (random, scale-free) and a biological neural network for comparison, analyzing structural impacts on image classification tasks.

Result: Networks with coherent, densely interconnected communities show better learning performance, with findings relevant to real-world biological networks.

Conclusion: The study advances network science and machine learning, suggesting biologically inspired neural network designs.

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [171] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gal Richard,Slim Essid,Andrei Bursuc,Patrick Prez*

Main category: cs.LG

TL;DR: LoRA-MCL enhances language models by using Multiple Choice Learning and Winner-Takes-All loss to decode diverse, plausible continuations, addressing ambiguity in next-token prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional language modeling struggles with ambiguity, as multiple futures can be equally plausible for a given context.

Method: Combines Multiple Choice Learning (MCL) and Winner-Takes-All (WTA) loss with Low-Rank Adaptation (LoRA) to handle ambiguity. Theoretical interpretation assumes data from a mixture of distributions.

Result: Achieves high diversity and relevance in generated outputs, demonstrated on visual and audio captioning tasks.

Conclusion: LoRA-MCL effectively addresses ambiguity in language modeling, producing diverse and plausible continuations.

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [172] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevti*

Main category: cs.LG

TL;DR: A GNN model is developed to forecast Valley Fever incidence in Arizona, integrating environmental and surveillance data, showing effectiveness in modeling trends and identifying key drivers.


<details>
  <summary>Details</summary>
Motivation: Addressing the public health concern of Valley Fever in endemic regions by leveraging advanced modeling techniques to improve forecasting and prevention.

Method: Uses a graph neural network (GNN) to integrate case data with environmental predictors (soil, atmospheric, agricultural, air quality) and explores correlation-based relationships with lagged effects.

Result: The GNN effectively models Valley Fever trends and identifies key environmental drivers of disease incidence.

Conclusion: The model can inform early warning systems and guide resource allocation for disease prevention in high-risk areas.

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [173] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: The paper investigates the limitations of state-of-the-art Vision-Language Models (VLMs) by testing their performance on fundamental visual tasks and analyzing design components.


<details>
  <summary>Details</summary>
Motivation: To understand the shortcomings of VLMs in basic visual understanding and identify design flaws beyond current benchmarks.

Method: Constructs tests to probe VLM components (visual encoder, vision-language projection, LLM-decoder) and compares their performance to trained probes.

Result: Uncovers VLM shortcomings, revealing insights into their capabilities, robustness, and visual information processing.

Conclusion: The findings aim to guide future improvements in VLM design and performance.

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [174] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: The paper introduces scMPT, a model combining scGPT and LLMs to improve single-cell analysis by leveraging their complementary strengths.


<details>
  <summary>Details</summary>
Motivation: Current single-cell foundation models like scGPT lack the ability to utilize text-based biological information, while LLMs, though competitive, are not well-understood in this context. The study aims to explore how LLMs complement scGPT.

Method: The study investigates biological insights driving LLM performance on single-cell data and introduces scMPT, a fusion model of scGPT and LLMs. Alternative fusion methods are also tested.

Result: scMPT outperforms its component models (scGPT and LLMs) with more consistent performance across datasets. Fusion methods show potential for further improvements.

Conclusion: LLMs can effectively complement single-cell foundation models, enhancing performance in single-cell analysis, as demonstrated by scMPT.

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [175] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: The paper highlights the need for standardized benchmarks in AI-driven biology to address data heterogeneity, reproducibility, and biases, proposing recommendations for robust benchmarking frameworks.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized benchmarks in AI for biology hinders the development of trustworthy models, necessitating cross-domain collaboration to address these challenges.

Method: Workshop insights from machine learning and computational biology experts across imaging, transcriptomics, proteomics, and genomics were analyzed to identify bottlenecks and propose solutions.

Result: Key bottlenecks include data heterogeneity, noise, reproducibility issues, and fragmented resources. Recommendations focus on high-quality data curation, standardized tools, and open platforms.

Conclusion: Robust benchmarking frameworks are essential for advancing AI-driven biology, ensuring rigor, reproducibility, and biological relevance for future discoveries.

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [176] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: The paper proposes an efficient pipeline for training adversarially robust decision trees, focusing on automatic perturbation size selection, training evaluation, and robustness certification.


<details>
  <summary>Details</summary>
Motivation: To address the lack of established efficiency and sustainability in robust training pipelines for machine learning models, particularly decision trees.

Method: A three-stage pipeline: (1) automatic perturbation size selection, (2) training state-of-the-art adversarial methods, and (3) robustness certification.

Result: Perturbation size can be estimated from smaller models for efficiency, and verification time is uncorrelated with training time.

Conclusion: The pipeline improves efficiency in robust training, with insights into perturbation selection and verification scalability.

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [177] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: The paper examines the reasoning capabilities of LLMs, particularly Qwen2.5, and highlights issues with data contamination in benchmarks. It introduces a synthetic dataset (RandomCalculation) to show that accurate reward signals improve performance, unlike noisy ones.


<details>
  <summary>Details</summary>
Motivation: To address unreliable results from contaminated benchmarks and investigate the inconsistent performance of RL-enhanced reasoning across different LLM families.

Method: Introduces a synthetic dataset (RandomCalculation) for leakage-free evaluation and tests RL methods with accurate vs. noisy reward signals.

Result: Accurate reward signals consistently improve performance, while noisy or incorrect signals do not, especially on uncontaminated datasets.

Conclusion: Advocates for evaluating RL methods on clean benchmarks and diverse models to ensure reliable conclusions.

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [178] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: Proposes an efficient parameter reduction method for linear SSMs in deep learning, reducing parameters to 1/32 without performance loss.


<details>
  <summary>Details</summary>
Motivation: Large parameter sizes in linear SSMs hinder deployment on resource-constrained devices.

Method: Applies $H^{2}$ model order reduction techniques from control theory to linear SSM components.

Result: Outperforms Balanced Truncation in LRA benchmarks, achieving significant parameter reduction.

Conclusion: The method effectively compresses SSMs while maintaining original model performance.

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [179] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: The paper addresses performance degradation in Neural DNF models during symbolic translation by proposing a disentanglement method that splits nested rule nodes, improving interpretability and preserving performance.


<details>
  <summary>Details</summary>
Motivation: Performance degradation in Neural DNF models during symbolic translation due to failure in disentangling learned knowledge.

Method: Proposes a disentanglement method by splitting nodes encoding nested rules into smaller independent nodes.

Result: Improved performance and interpretability in binary, multiclass, and multilabel classification tasks, with results closer to pre-translation performance.

Conclusion: The disentanglement method enhances Neural DNF models by providing compact, interpretable logical representations while maintaining performance.

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


### [180] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: PRRO is a pipeline for improving synthetic tabular data quality by addressing class imbalance and data relationship issues, enhancing supervised learning performance.


<details>
  <summary>Details</summary>
Motivation: Synthetic tabular data often underperforms in supervised learning due to class imbalance and overlooked data relationships. PRRO aims to bridge this gap.

Method: PRRO integrates data pruning and column reordering to align synthetic data with original data's class distribution and modeling structure.

Result: PRRO improves predictive performance by up to 871.46% and enhances class distribution similarity by 43% in imbalanced datasets.

Conclusion: PRRO effectively integrates data synthesis with supervised learning, improving data utility and accessibility.

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [181] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: MTF-Grasp, a multi-tier FL approach for robotic grasping, addresses non-IID data challenges by leveraging top-level robots with better data to train seed models, improving performance by up to 8%.


<details>
  <summary>Details</summary>
Motivation: Grasping tasks lack exploration in FL settings, with challenges like non-IID data and low quantity, leading to performance degradation.

Method: MTF-Grasp selects top-level robots with better data distribution and higher sample count to train seed models, which are then distributed to low-level robots.

Result: Outperforms conventional FL by up to 8% on quantity-skewed Cornell and Jacquard grasping datasets.

Conclusion: MTF-Grasp effectively mitigates performance degradation in FL for robotic grasping by addressing non-IID data challenges.

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [182] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Rder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: FedAcross+ is a scalable Federated Learning framework addressing challenges like data labeling, covariate shift, and limited updates in resource-constrained environments by leveraging a pre-trained model and domain adaptation.


<details>
  <summary>Details</summary>
Motivation: To overcome practical challenges in Federated Learning, such as human involvement in labeling, covariate shift, and resource constraints, for real-world industrial applications.

Method: Uses a pre-trained source model with a frozen backbone and classifier, allowing a domain adaptive linear layer to handle target adaptation, and extends to streaming data for non-stationary environments.

Result: FedAcross+ achieves competitive adaptation on low-end devices with limited samples and handles domain shift effectively, supporting sporadic updates in constrained settings.

Conclusion: FedAcross+ provides a practical and efficient solution for Federated Learning in industrial settings, addressing key challenges while ensuring seamless deployment.

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [183] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: This paper demystifies Tensor Network (TN) ranks, explaining their role in TN decompositions and how domain knowledge can guide their selection, using intuitive visualizations and examples.


<details>
  <summary>Details</summary>
Motivation: TN ranks are crucial for TN decompositions but lack universal meaning and intuitive interpretation, often being treated as hyperparameters. The paper aims to clarify TN ranks for better understanding and application.

Method: The paper uses real-life examples and intuitive visualizations to explain TN ranks, focusing on models like CP and Tucker decompositions, and employs a graphical approach for complex TN structures.

Result: The paper provides a unified understanding of TN ranks, linking them to tensor unfoldings and offering practical insights for domain-informed TN design.

Conclusion: The paper equips readers with clarity and intuition about TN ranks, aiding their selection and application in tensor methods for both practical and educational purposes.

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [184] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: The paper introduces T-GRAB, a benchmark to evaluate TGNNs' ability to capture temporal patterns like periodicity and causality, revealing their limitations.


<details>
  <summary>Details</summary>
Motivation: Current TGNNs lack clarity in capturing core temporal patterns, prompting the need for a systematic evaluation tool.

Method: T-GRAB, a synthetic benchmark, isolates key temporal skills (e.g., periodicity, causality) to test TGNNs.

Result: Evaluation of 11 TGNNs shows shortcomings in generalizing temporal patterns.

Conclusion: T-GRAB highlights model limitations, challenges traditional benchmarks, and motivates improved architectures.

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [185] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: The paper analyzes neural networks using graph variables and statistical sufficiency, showing how layers preserve conditional distributions and achieve sufficiency under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To bridge statistical sufficiency, graph-theoretic representations, and deep learning for a new statistical understanding of neural networks.

Method: Interprets neural network layers as graph-based transformations, proving sufficiency under dense anchor point assumptions and finite-width networks with region-separated inputs.

Result: Asymptotic sufficiency holds in infinite-width limit and is preserved during training; finite-width networks achieve sufficiency with specific conditions.

Conclusion: The framework provides a novel statistical perspective on neural networks, covering various architectures and activations.

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [186] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: KAPI-ELM, an adaptive RBF-based extension of PI-ELM, solves PDE problems with sharp gradients using Bayesian Optimization for input layer distributional parameters and least-squares for output layer, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Overcome PI-ELM's limitation in capturing sharp gradients due to fixed input layers while retaining its speed advantage over PINNs.

Method: Introduces a lightweight Bayesian Optimization framework to learn hyperparameters defining input weight distributions, combined with least-squares optimization for output layer parameters.

Result: KAPI-ELM achieves state-of-the-art accuracy in forward and inverse PDE problems, outperforming advanced methods like XTFC with fewer parameters.

Conclusion: KAPI-ELM is a scalable, interpretable, and generalizable physics-informed learning framework, especially effective in stiff PDE regimes.

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [187] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T is a conditional generative chemical language model that integrates biological context for molecule prioritization and design, outperforming existing methods in speed and performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current generative chemical language models (CLMs) in drug discovery, such as unreliable reward signals and lack of interpretability.

Method: SAFE-T models the conditional likelihood of fragment-based molecular sequences given a biological prompt, enabling scoring and generation aligned with biological objectives.

Result: SAFE-T achieves comparable or better performance than existing methods in zero-shot evaluations and is significantly faster, with interpretable fragment-level attributions.

Conclusion: Conditional generative CLMs like SAFE-T can unify scoring and generation to accelerate early-stage drug discovery.

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [188] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: The paper addresses the sensitivity of hierarchical $k$-median clustering to dataset perturbations, proposing an efficient algorithm with low average sensitivity and high clustering quality, while showing instability in existing methods like single linkage and CLNSS.


<details>
  <summary>Details</summary>
Motivation: Hierarchical clustering is widely used but sensitive to small dataset changes, reducing usability for large, dynamic datasets. The study focuses on hierarchical $k$-median clustering for its theoretical and practical benefits.

Method: The paper analyzes average sensitivity by measuring output changes when a random data point is deleted. It proposes an efficient hierarchical $k$-median clustering algorithm and proves its low sensitivity and high quality.

Result: The proposed algorithm shows low average sensitivity and high clustering quality, while single linkage and CLNSS variants exhibit high sensitivity. Experiments validate robustness and effectiveness.

Conclusion: The study successfully introduces a stable and efficient hierarchical $k$-median clustering algorithm, outperforming existing methods in sensitivity and quality, with practical applications for dynamic datasets.

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [189] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: The paper analyzes the convergence of agnostic Federated Averaging (FedAvg) under random and variably-sized client participation, providing the first guarantees without knowledge of participation distributions.


<details>
  <summary>Details</summary>
Motivation: Practical FL deployments face intermittent client participation with unknown biases, challenging assumptions of full participation or uniform availability.

Method: Characterizes the optimization problem under stochastic client dynamics and analyzes agnostic FedAvg's convergence for convex, possibly nonsmooth losses.

Result: Achieves a standard convergence rate of O(1/T) and shows agnostic FedAvg outperforms weighted aggregation variants.

Conclusion: Agnostic FedAvg is robust under general, non-uniform client participation, offering practical advantages without needing participation distribution knowledge.

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [190] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: The paper evaluates imputation methods for missing MoCap data from IMUs, finding multivariate approaches superior, especially for complex missingness patterns.


<details>
  <summary>Details</summary>
Motivation: Missing data in IMU-derived MoCap compromises its utility in sports science, yet a systematic evaluation of imputation methods is lacking.

Method: Comparative analysis of statistical, ML, and DL imputation methods across univariate and multivariate contexts, using a new public MoCap dataset with simulated missingness.

Result: Multivariate methods outperform univariate, reducing MAE by up to 50% for complex missingness. GAIN and Iterative Imputers show highest accuracy.

Conclusion: This work establishes a benchmark for future research and offers practical recommendations for robust MoCap data analysis.

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [191] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: The paper derives nearly optimal super-approximation error bounds for ReLU neural networks approximating Korobov functions, improving classical bounds and showing resilience to the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To analyze the approximation errors of ReLU neural networks for Korobov functions and demonstrate their expressivity despite high dimensions.

Method: Uses sparse grid finite elements and the bit extraction technique to derive error bounds in $L_p$ and $W^1_p$ norms.

Result: Achieves nearly optimal error bounds of order $2m$ in $L_p$ norm and $2m-2$ in $W^1_p$ norm for functions with $L_p$ mixed derivatives of order $m$.

Conclusion: ReLU neural networks effectively approximate Korobov functions, outperforming classical methods and mitigating the curse of dimensionality.

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [192] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: An algorithm is designed to accelerate diffusion on the $SO(3)$ manifold using Picard iteration, achieving up to 4.9 speed-up without task reward degradation.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are inherently slow due to their sequential nature, limiting their practical use.

Method: Adapts numerical Picard iteration for the $SO(3)$ space to accelerate diffusion.

Result: Achieves up to 4.9 speed-up in sample generation with no measurable degradation in task reward.

Conclusion: The proposed algorithm effectively reduces latency in diffusion processes on $SO(3)$ without compromising performance.

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [193] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: DeepONets are evaluated for geotechnical engineering, with Model 4 (Fourier feature-enhanced) outperforming others, offering significant speedups over traditional solvers.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance the application of DeepONets in geotechnical engineering, particularly for the 1D consolidation problem.

Method: Evaluated four DeepONet architectures, including standard and physics-inspired designs, and proposed a Fourier feature-enhanced model (Model 4).

Result: Model 4 outperformed others, achieving speedups of 1.5 to 100 times over traditional solvers, with potential for larger savings in complex systems.

Conclusion: DeepONets show promise for efficient, generalizable surrogate modeling in geotechnics, advancing scientific machine learning in the field.

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [194] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: A cloud-based, LLM-powered shared e-mobility platform is introduced, integrating personalized route recommendations and evaluated for optimization and RAG framework performance.


<details>
  <summary>Details</summary>
Motivation: The growing demand for e-mobility and the need for advanced, realistic solutions drive the development of a comprehensive, intelligent platform.

Method: The platform combines cloud-based simulation, LLM for decision-making, and a mobile app for personalized routes. Optimization is evaluated on travel time and cost, while the RAG framework is tested for accuracy.

Result: The RAG framework achieves 0.81 accuracy for system operator queries and 0.98 for user queries.

Conclusion: The platform effectively addresses e-mobility demands with robust LLM integration and high accuracy in user-focused applications.

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [195] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: The paper introduces TagBERT, a dependency-aware transformer model for query reformulation in e-commerce, leveraging semantic tags to improve relevance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like ambiguous queries and vocabulary misalignment in e-commerce search by enhancing query reformulation with semantic tags.

Method: Proposes TagBERT, a token classification model using semantic tags for better query phrase embeddings, outperforming BERT and sequence-to-sequence models.

Result: TagBERT shows superior performance on real-life e-commerce datasets compared to BERT, eBERT, and sequence-to-sequence models.

Conclusion: Semantic tags significantly improve query reformulation, with TagBERT proving more effective than existing methods.

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [196] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: A strategy for mechanism search in complex reactions like cyclizations, combining graph-based enumeration and machine learning, using a neural network potential (AIMNet2-rxn) for pathway evaluation.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in reaction mechanism searches for complex reactions with multiple concerted bond changes, common in natural product synthesis.

Method: Combines graph-based enumeration and machine learning for intermediate filtering, using AIMNet2-rxn for computational evaluation of reaction pathways.

Result: Demonstrates the NNP's ability to estimate activation energies, predict stereoselectivity, and recapitulate complex steps in natural product synthesis.

Conclusion: The approach provides a cost-effective and efficient method for exploring complex reaction mechanisms, particularly cyclizations.

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [197] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: A new framework, Stochastic Operator Network (SON), combines stochastic optimal control with DeepONet for uncertainty quantification in operator learning, using SDEs and adjoint BSDEs to update SGD via Hamiltonian gradients.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty quantification in operator learning by integrating stochastic optimal control concepts with existing neural operator frameworks.

Method: SON formulates the branch net as a stochastic differential equation (SDE) and uses adjoint backward SDEs (BSDEs) to replace loss gradients with Hamiltonian gradients in SGD updates.

Result: SON effectively replicates noisy operators in 2D and 3D, demonstrating its capability to learn uncertainty through diffusion parameters.

Conclusion: SON provides a robust framework for uncertainty-aware operator learning, validated by successful applications in noisy operator replication.

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [198] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: The paper explores conformal prediction's limitations under distribution shifts and proposes an optimal transport-based method to estimate and mitigate coverage loss.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction lacks robustness under distribution shifts, and existing methods require prior knowledge of the shift type. This work aims to address this gap.

Method: The study uses optimal transport to analyze and mitigate coverage loss in non-exchangeable settings.

Result: The proposed method effectively estimates and mitigates coverage loss caused by distribution shifts.

Conclusion: Optimal transport provides a viable solution to enhance conformal prediction's robustness under distribution shifts.

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [199] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: CLA is a self-supervised learning method for online continual learning that aligns current and past representations to reduce forgetting, improving convergence and final performance.


<details>
  <summary>Details</summary>
Motivation: Address the lack of SSL techniques for online continual learning with constraints like small minibatches, fixed computational budgets, and no task boundaries.

Method: Introduces Continual Latent Alignment (CLA), which aligns current model representations with past ones to mitigate forgetting.

Result: CLA speeds up training convergence and outperforms state-of-the-art methods under the same budget. It also enhances final performance when used as pretraining.

Conclusion: CLA is effective for online continual learning, offering faster convergence and better performance compared to traditional methods.

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [200] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: The paper explores the application of the Polyak-ojasiewicz Inequality (PLI) to optimization problems, highlighting mixed convergence behaviors in continuous-time vs. discrete-time LQR problems and proposing generalized PLI-like conditions. It also addresses gradient estimation errors and their impact, using an ISS analysis.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in convergence behaviors between continuous-time and discrete-time LQR problems and to understand the effects of gradient estimation errors in optimization.

Method: Applies PLI and its generalizations to analyze convergence rates, particularly in policy optimization and LQR problems. Uses ISS analysis for gradient error effects.

Result: Identifies mixed globally linear / locally exponential convergence in continuous-time LQR, contrasting with global exponential convergence in discrete-time. Proposes generalized PLI-like conditions.

Conclusion: Generalized PLI-like conditions and ISS analysis are crucial for understanding convergence and error impacts in optimization, with implications for reinforcement learning and control systems.

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [201] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: Target Polish is a fast, outlier-resistant framework for nonnegative matrix/tensor factorization, outperforming robust NMF methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional weighted NMF methods are slow due to multiplicative updates, despite outlier resistance. Target Polish aims to combine speed (via Fast-HALS) and robustness.

Method: Uses weighted median-based transformation for adaptive smoothing, maintaining Fast-HALS' additive updates for efficiency.

Result: Matches/exceeds state-of-the-art robust NMF accuracy, reduces computation time significantly.

Conclusion: Target Polish effectively balances speed and robustness, making it superior for noisy data.

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [202] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Sal Fenollosa*

Main category: cs.LG

TL;DR: EWC reduces catastrophic forgetting in continual learning, outperforming L2 and SGD, though slightly slowing new task learning. Dropout and hyperparameters were also studied.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in neural networks to enable lifelong learning.

Method: Evaluated EWC on PermutedMNIST and RotatedMNIST, comparing it with L2 regularization and SGD. Analyzed dropout and hyperparameters.

Result: EWC significantly reduces forgetting but slightly slows new task learning. Generalization insights were provided.

Conclusion: EWC is a promising solution for lifelong learning in neural networks.

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [203] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: SplitHappens enhances Split Learning (SL) with Function Secret Sharing (FSS) in a U-shaped SL framework, improving security by keeping client labels private and addressing vulnerabilities like model inversion and label inference attacks.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerabilities in Split Learning (SL) and enhance data privacy by combining SL with Function Secret Sharing (FSS) in a U-shaped framework.

Method: U-shaped SL combined with FSS, allowing clients to keep training labels secret while reducing communication and computational costs.

Result: Improved security guarantees, reduced training time, and lower communication costs compared to standalone FSS, while maintaining accuracy.

Conclusion: SplitHappens successfully generalizes security analysis and mitigates modern attacks, proving effective for practical ML applications.

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [204] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lcuyer*

Main category: cs.LG

TL;DR: DP-GD struggles with low-frequency classes under heavy-tail imbalance, while DP-AdamBC improves accuracy by ~8% and ~5% in experiments.


<details>
  <summary>Details</summary>
Motivation: To analyze optimization behavior of private learning algorithms under heavy-tail class imbalance.

Method: Compare DP-GD and DP-AdamBC, focusing on second-order information and bias removal.

Result: DP-AdamBC outperforms DP-GD, improving training accuracy for low-frequency classes.

Conclusion: DP-AdamBC is effective for heavy-tail imbalance by addressing DP bias and curvature estimation.

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [205] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: The paper introduces the Graph World Model (GWM), a versatile world model that integrates unstructured and graph-structured data for multi-modal tasks, outperforming domain-specific baselines.


<details>
  <summary>Details</summary>
Motivation: Existing world models lack support for structured data like graphs, and graph foundation models are limited to graph learning tasks. GWM aims to bridge this gap by handling multi-modal data and diverse tasks.

Method: GWM uses a generic message-passing algorithm to aggregate structured information, with two variants: GWM-T (text-based) and GWM-E (embedding-based). It introduces action nodes for task representation.

Result: GWM outperforms or matches domain-specific baselines across six diverse tasks, showing strong zero-shot/few-shot capabilities and benefiting from multi-hop structures.

Conclusion: GWM successfully integrates structured and unstructured data, demonstrating versatility and superior performance in multi-modal and interdisciplinary tasks.

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [206] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: FusionBench and FusionFactory propose a systematic approach to leverage diverse LLMs' strengths via routing and fusion, outperforming single-model use.


<details>
  <summary>Details</summary>
Motivation: Current reliance on single LLMs limits performance and efficiency; routing data from hosting platforms can reveal model strengths.

Method: Developed FusionBench (routing benchmark) and FusionFactory (fusion framework with query-, thought-, and model-level fusion).

Result: FusionFactory outperforms individual LLMs across 14 benchmarks, with optimal fusion varying by task.

Conclusion: Systematic LLM fusion harnesses complementary strengths, improving performance and efficiency.

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [207] [Agent-based visualization of streaming text](https://arxiv.org/abs/2507.08884)
*Jordan Riley Benson,David Crist,Phil Lafleur,Benjamin Watson*

Main category: cs.MA

TL;DR: A visualization system maps data to agents with behaviors based on the data, creating dynamic visuals. Applied to streaming text, it clusters words by co-occurrence, revealing topics dynamically.


<details>
  <summary>Details</summary>
Motivation: To dynamically visualize streaming text data by representing words as agents, enabling real-time topic clustering and analysis.

Method: Agents represent words, with behaviors and positions adjusted to minimize differences between displayed and ideal distances (derived from word co-occurrence). A backend gathers and processes text data.

Result: Visuals dynamically cluster words by topics, with stable layouts even as data streams change.

Conclusion: The system effectively visualizes streaming text, revealing topics through agent-based clustering and dynamic adjustments.

Abstract: We present a visualization infrastructure that maps data elements to agents,
which have behaviors parameterized by those elements. Dynamic visualizations
emerge as the agents change position, alter appearance and respond to one
other. Agents move to minimize the difference between displayed agent-to-agent
distances, and an input matrix of ideal distances. Our current application is
visualization of streaming text. Each agent represents a significant word,
visualizing it by displaying the word itself, centered in a circle sized by the
frequency of word occurrence. We derive the ideal distance matrix from word
cooccurrence, mapping higher co-occurrence to lower distance. To depict
co-occurrence in its textual context, the ratio of intersection to circle area
approximates the ratio of word co-occurrence to frequency. A networked backend
process gathers articles from news feeds, blogs, Digg or Twitter, exploiting
online search APIs to focus on user-chosen topics. Resulting visuals reveal the
primary topics in text streams as clusters, with agent-based layout moving
without instability as data streams change dynamically.

</details>


### [208] [Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents](https://arxiv.org/abs/2507.08944)
*Enhao Zhang,Erkang Zhu,Gagan Bansal,Adam Fourney,Hussein Mozannar,Jack Gerrits*

Main category: cs.MA

TL;DR: M1-Parallel framework reduces latency in LLM-based multi-agent systems by running parallel teams and using asynchronous messaging, achieving 2.2 speedup and higher task completion.


<details>
  <summary>Details</summary>
Motivation: High latency in iterative multi-agent systems for complex tasks.

Method: Proposes M1-Parallel, a framework with parallel multi-agent teams and event-driven asynchronous communication.

Result: 2.2 speedup with preserved accuracy; higher task completion rates with aggregation.

Conclusion: Parallel plan execution optimizes multi-agent systems for complex tasks.

Abstract: Large language model (LLM)-based multi-agent systems have demonstrated
remarkable promise for tackling complex tasks by breaking them down into
subtasks that are iteratively planned, executed, observed, and refined. Despite
their effectiveness, these systems often incur high latency because real-world
problems frequently demand multiple iterative cycles of reasoning steps. To
address this challenge, we propose M1-Parallel, a framework that concurrently
runs multiple multi-agent teams in parallel to uncover distinct solution paths.
By leveraging an event-driven communication model with asynchronous messaging,
M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to
either reduce end-to-end latency or boost task completion rates. Our
experiments on complex tasks show that M1-Parallel with early termination
achieves up to $2.2\times$ speedup while preserving accuracy, and that
M1-Parallel with aggregation yields higher task completion rates. We further
investigate strategies aimed at encouraging diverse execution plans but observe
no additional performance gains over repeated sampling. Overall, these findings
underscore the potential of parallel plan execution for optimizing multi-agent
systems for real-world, high-complexity reasoning tasks.

</details>


### [209] [How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08960)
*Andrew Estornell,Jean-Francois Ton,Muhammad Faaiz Taufiq,Hang Li*

Main category: cs.MA

TL;DR: A hierarchical multi-agent framework (MLPO) trains a single leader LLM to coordinate untrained peers, improving performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent methods are computationally expensive; this work aims to enhance efficiency and performance by training only a leader.

Method: Proposes MLPO, training a leader to evaluate and synthesize responses from untrained agents without extra networks or feedback.

Result: Achieves significant gains on BBH, MATH, and MMLU over single and multi-agent baselines.

Conclusion: Training a flexible leader is effective and efficient for collaborative reasoning in multi-agent LLM systems.

Abstract: Large Language Models (LLMs) have achieved strong performance on a wide range
of complex reasoning tasks, yet further gains are often possible by leveraging
the complementary strengths of multiple models. While multi-agent frameworks
can improve solution quality by leveraging multiple LLMs, existing methods are
often computationally expensive, both at training and inference time. In this
work, we introduce a hierarchical multi-agent framework that addresses these
challenges by training only a single leader LLM to coordinate a team of
untrained peer agents. To this end, we propose Multi-agent guided Leader Policy
\textbf{O}ptimization (MLPO), a novel approach which trains the leader to
evaluate and synthesize agent responses without auxiliary value networks or
explicit agent feedback. Leaders trained with MLPO exhibit improved performance
not only when interacting with the agent team at inference time, but also enjoy
improved performance when deployed in single-agent settings without the team.
Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our
framework achieves substantial performance improvements over both single-agent
and multi-agent baselines. Our results highlight the effectiveness and
efficiency of training a single, flexible leader for collaborative reasoning in
multi-agent LLM systems.

</details>


### [210] [Simulation for All: A Step-by-Step Cookbook for Developing Human-Centered Multi-Agent Transportation Simulators](https://arxiv.org/abs/2507.09367)
*Shiva Azimi,Arash Tavakoli*

Main category: cs.MA

TL;DR: A human-centered multi-agent simulation platform for multimodal transportation studies, integrating immersive environments and open-source tools for accessibility and replication.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing platforms by supporting real-time, human-centered studies of all road users, including public transit users, and ensuring accessibility for non-technical users.

Method: Develops a modular, extensible platform with immersive virtual environments, hardware-specific modules (e.g., treadmill, cockpit), and embedded sensing devices (e.g., fNIRS, eye tracking) for multimodal data collection.

Result: Three use cases demonstrate the platform's usability for high-fidelity transportation simulation, interdisciplinary experimentation, and understanding multimodal mobility.

Conclusion: The platform lowers barriers to entry for transportation simulation, supports diverse studies, and advances urban mobility research.

Abstract: As cities evolve toward more complex and multimodal transportation systems,
the need for human-centered multi-agent simulation tools has never been more
urgent. Yet most existing platforms remain limited - they often separate
different types of road users, rely on scripted or pre-defined behaviors,
overlook public transit users as active participants, and are rarely designed
with accessibility in mind for non-technical users. To address this gap, this
paper presents the specifications of a multi-agent simulation platform designed
to support real-time, human-centered, and immersive studies of all road users,
accompanied by open-source scripts for replication. Using high-fidelity
immersive virtual environments, our platform enables interaction across public
transit users, pedestrians, cyclists, automated vehicles, and drivers. The
architecture is modular, extensible, and designed for accessibility. The system
integrates hardware-specific modules - including an omnidirectional treadmill,
a seating arrangement, a smart trainer, and an actuated cockpit. Additionally,
the platform collects multimodal physiological, neurological, and behavioral
data through embedded sensing devices such as functional near-infrared
spectroscopy (fNIRS), eye tracking, and wrist-based biosensors. To show the
usability of this system, we present three use cases. Simulation for All aims
to lower the barrier to entry for high-fidelity transportation simulation,
support experimentation across disciplines, and advance our understanding of
multimodal mobility in complex urban environments.

</details>


### [211] [Adaptive Social Learning using Theory of Mind](https://arxiv.org/abs/2507.09409)
*Lance Ying,Ryan Truong,Joshua B. Tenenbaum,Samuel J. Gershman*

Main category: cs.MA

TL;DR: A rational mentalizing model balances social and non-social learning by estimating utility, validated through a treasure hunt game.


<details>
  <summary>Details</summary>
Motivation: Understand how humans balance social and non-social learning, considering costs and benefits.

Method: Propose a rational mentalizing model to estimate utility of social vs. non-social learning, tested in a multi-player treasure hunt game.

Result: Model captures human trade-offs; social learning enhances goal efficiency.

Conclusion: Flexible social learning application improves goal achievement.

Abstract: Social learning is a powerful mechanism through which agents learn about the
world from others. However, humans don't always choose to observe others, since
social learning can carry time and cognitive resource costs. How do people
balance social and non-social learning? In this paper, we propose a rational
mentalizing model of the decision to engage in social learning. This model
estimates the utility of social learning by reasoning about the other agent's
goal and the informativity of their future actions. It then weighs the utility
of social learning against the utility of self-exploration (non-social
learning). Using a multi-player treasure hunt game, we show that our model can
quantitatively capture human trade-offs between social and non-social learning.
Furthermore, our results indicate that these two components allow agents to
flexibly apply social learning to achieve their goals more efficiently.

</details>


### [212] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Main category: cs.MA

TL;DR: TinyTroupe is a toolkit for simulating human behavior in LLM-powered Multiagent Systems, addressing gaps in persona specification and experimentation support.


<details>
  <summary>Details</summary>
Motivation: Existing MAS tools lack fine-grained persona specifications and integrated validation, limiting their utility for behavioral studies and social simulations.

Method: Introduces TinyTroupe, a toolkit with detailed persona definitions and LLM-driven mechanisms for behavioral problem-solving.

Result: Demonstrated through examples like brainstorming and market research, with evaluations highlighting its capabilities and limitations.

Conclusion: TinyTroupe is a conceptual and practical contribution, available as open source, adaptable to other contexts.

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


### [213] [Large Population Models](https://arxiv.org/abs/2507.09901)
*Ayush Chopra*

Main category: cs.MA

TL;DR: Large Population Models (LPMs) simulate millions of agents to study collective behavior and system-level outcomes, offering insights for societal challenges.


<details>
  <summary>Details</summary>
Motivation: To address complex societal challenges like pandemics and climate adaptation by understanding collective behavior of autonomous agents.

Method: LPMs use computational efficiency, data-driven learning, and privacy-preserving protocols to simulate large-scale agent interactions.

Result: LPMs reveal emergent phenomena and allow testing of interventions before real-world deployment.

Conclusion: LPMs complement AI research by focusing on collective intelligence and providing policy testing grounds.

Abstract: Many of society's most pressing challenges, from pandemic response to supply
chain disruptions to climate adaptation, emerge from the collective behavior of
millions of autonomous agents making decisions over time. Large Population
Models (LPMs) offer an approach to understand these complex systems by
simulating entire populations with realistic behaviors and interactions at
unprecedented scale. LPMs extend traditional modeling approaches through three
key innovations: computational methods that efficiently simulate millions of
agents simultaneously, mathematical frameworks that learn from diverse
real-world data streams, and privacy-preserving communication protocols that
bridge virtual and physical environments. This allows researchers to observe
how agent behavior aggregates into system-level outcomes and test interventions
before real-world implementation. While current AI advances primarily focus on
creating "digital humans" with sophisticated individual capabilities, LPMs
develop "digital societies" where the richness of interactions reveals emergent
phenomena. By bridging individual agent behavior and population-scale dynamics,
LPMs offer a complementary path in AI research illuminating collective
intelligence and providing testing grounds for policies and social innovations
before real-world deployment. We discuss the technical foundations and some
open problems here. LPMs are implemented by the AgentTorch framework
(github.com/AgentTorch/AgentTorch)

</details>


### [214] [AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design](https://arxiv.org/abs/2507.09965)
*Weiyu Chen,Chengjie Liu,Wenhao Huang,Jinyang Lyu,Mingqian Yang,Yuan Du,Li Du,Jun Yang*

Main category: cs.MA

TL;DR: AnalogTester automates testbench construction for analog circuits using LLMs, addressing manual bottlenecks in analog design automation.


<details>
  <summary>Details</summary>
Motivation: Manual testbench construction for analog circuits is time-consuming and inflexible, hindering automation.

Method: AnalogTester uses an LLM-powered pipeline for domain-knowledge integration, paper extraction, simulation synthesis, and testbench code generation.

Result: It successfully automates testbench generation for op-amps, BGRs, and LDOs, and creates datasets for LLM specialization.

Conclusion: AnalogTester provides a scalable solution for automating analog circuit design and supports future LLM training.

Abstract: Recent advancements have demonstrated the significant potential of large
language models (LLMs) in analog circuit design. Nevertheless, testbench
construction for analog circuits remains manual, creating a critical bottleneck
in achieving fully automated design processes. Particularly when replicating
circuit designs from academic papers, manual Testbench construction demands
time-intensive implementation and frequent adjustments, which fails to address
the dynamic diversity and flexibility requirements for automation. AnalogTester
tackles automated analog design challenges through an LLM-powered pipeline: a)
domain-knowledge integration, b) paper information extraction, c) simulation
scheme synthesis, and d) testbench code generation with Tsinghua Electronic
Design (TED). AnalogTester has demonstrated automated Testbench generation
capabilities for three fundamental analog circuit types: operational amplifiers
(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while
maintaining a scalable framework for adaptation to broader circuit topologies.
Furthermore, AnalogTester can generate circuit knowledge data and TED code
corpus, establishing fundamental training datasets for LLM specialization in
analog circuit design automation.

</details>


### [215] [Multi-Robot Cooperative Herding through Backstepping Control Barrier Functions](https://arxiv.org/abs/2507.10249)
*Kang Li,Ming Li,Wenkang Ji,Zhiyong Sun,Shiyu Zhao*

Main category: cs.MA

TL;DR: A novel cooperative herding strategy using backstepping control barrier functions (CBFs) ensures safe herding of evaders by herders, addressing underactuation through hierarchical control design.


<details>
  <summary>Details</summary>
Motivation: The challenge of indirectly influencing evaders' behavior in heterogeneous herding systems, which are inherently underactuated, motivates the need for a safe and effective control strategy.

Method: Separate CBFs for goal reaching and collision avoidance are constructed, reformulated into a control-affine structure, and a backstepping approach designs hierarchical control inputs. Centralized and decentralized implementations are developed.

Result: The strategy successfully herds multiple evaders safely into the goal region, validated by simulations and real-world experiments.

Conclusion: The backstepping CBF-based approach provides a flexible, safe, and effective solution for multi-robot herding, with potential for broader applications.

Abstract: We propose a novel cooperative herding strategy through backstepping control
barrier functions (CBFs), which coordinates multiple herders to herd a group of
evaders safely towards a designated goal region. For the herding system with
heterogeneous groups involving herders and evaders, the behavior of the evaders
can only be influenced indirectly by the herders' motion, especially when the
evaders follow an inverse dynamics model and respond solely to repulsive
interactions from the herders. This indirect interaction mechanism inherently
renders the overall system underactuated. To address this issue, we first
construct separate CBFs for the dual objectives of goal reaching and collision
avoidance, which ensure both herding completion and safety guarantees. Then, we
reformulate the underactuated herding dynamics into a control-affine structure
and employ a backstepping approach to recursively design control inputs for the
hierarchical barrier functions, avoiding taking derivatives of the higher-order
system. Finally, we present a cooperative herding strategy based on
backstepping CBFs that allow herders to safely herd multiple evaders into the
goal region. In addition, centralized and decentralized implementations of the
proposed algorithm are developed, further enhancing its flexibility and
applicability. Extensive simulations and real-world experiments validate the
effectiveness and safety of the proposed strategy in multi-robot herding.

</details>


### [216] [ToMacVF : Temporal Macro-action Value Factorization for Asynchronous Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.10251)
*Wenjing Zhang,Wei Zhang*

Main category: cs.MA

TL;DR: ToMacVF improves asynchronous MARL by enabling fine-grained temporal credit assignment for macro-actions, using Mac-SJERT for accurate data collection and To-Mac-IGM for principled value factorization.


<details>
  <summary>Details</summary>
Motivation: Existing methods sample biased data at macro-action endpoints, leading to incomplete representations and poor credit assignment.

Method: Proposes ToMacVF with Mac-SJERT for data collection and To-Mac-IGM for value factorization, ensuring CTDE compliance.

Result: ToMacVF outperforms baselines, showing optimal performance, adaptability, and robustness in asynchronous MARL scenarios.

Conclusion: ToMacVF effectively addresses limitations of existing methods, enhancing macro-action representation and credit assignment.

Abstract: Existing asynchronous MARL methods based on MacDec-POMDP typically construct
training trajectory buffers by simply sampling limited and biased data at the
endpoints of macro-actions, and directly apply conventional MARL methods on the
buffers. As a result, these methods lead to an incomplete and inaccurate
representation of the macro-action execution process, along with unsuitable
credit assignments. To solve these problems, the Temporal Macro-action Value
Factorization (ToMacVF) is proposed to achieve fine-grained temporal credit
assignment for macro-action contributions. A centralized training buffer,
called Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT),
is designed to incorporate with ToMacVF to collect accurate and complete
macro-action execution information, supporting a more comprehensive and precise
representation of the macro-action process. To ensure principled and
fine-grained asynchronous value factorization, the consistency requirement
between joint and individual macro-action selection called Temporal
Macro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes
the synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture,
which satisfies CTDE principle, is designed to conveniently integrate previous
value factorization methods. Next, the ToMacVF algorithm is devised as an
implementation of the ToMacVF architecture. Experimental results demonstrate
that, compared to asynchronous baselines, our ToMacVF algorithm not only
achieves optimal performance but also exhibits strong adaptability and
robustness across various asynchronous multi-agent experimental scenarios.

</details>


### [217] [Toolsuite for Implementing Multiagent Systems Based on Communication Protocols](https://arxiv.org/abs/2507.10324)
*Amit K. Chopra,Samuel H. Christie V,Munindar P. Singh*

Main category: cs.MA

TL;DR: The paper introduces Interaction-Oriented Programming (IOP) and its software suite for multiagent system development, focusing on protocol verification and agent implementation.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance the development of multiagent systems by modeling interactions through flexible protocols.

Method: Developed tools for verifying interaction protocols (e.g., liveness, safety) and middleware for agent implementation.

Result: A comprehensive software suite enabling efficient multiagent system development using IOP.

Conclusion: IOP and its supporting tools provide a robust framework for building multiagent systems with verified interaction protocols.

Abstract: Interaction-Oriented Programming (IOP) is an approach to building a
multiagent system by modeling the interactions between its roles via a flexible
interaction protocol and implementing agents to realize the interactions of the
roles they play in the protocol.
  In recent years, we have developed an extensive suite of software that
enables multiagent system developers to apply IOP. These include tools for
efficiently verifying protocols for properties such as liveness and safety and
middleware that simplifies the implementation of agents. This paper presents
some of that software suite.

</details>
