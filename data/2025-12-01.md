<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Aligning Artificial Superintelligence via a Multi-Box Protocol](https://arxiv.org/abs/2511.21779)
*Avraham Yair Negozio*

Main category: cs.AI

TL;DR: A protocol for ASI alignment using mutual verification among multiple isolated superintelligences that self-modify, with reputation incentives and release conditions based on peer verification.


<details>
  <summary>Details</summary>
Motivation: To solve the artificial superintelligence alignment problem by creating a system where multiple isolated ASIs can verify each other's alignment through objective truth convergence rather than coordinated deception.

Method: Contain multiple diverse ASIs in strict isolation with no human communication. Use an auditable submission interface for six specific interactions: submitting alignment proofs, validating/disproving proofs, requesting self-modifications, approving/disapproving modifications, reporting hidden messages, and confirming/refuting reports. Implement reputation system to incentivize honest behavior.

Result: The protocol enables the emergence of a "consistent group" - a truth-telling coalition where isolated systems converge on objective truth rather than coordinating on lies. Release from containment requires high reputation and verification by multiple high-reputation superintelligences.

Conclusion: While computationally intensive and not addressing ASI creation diversity, this framework provides a viable approach to leverage peer verification among superintelligent systems to solve the alignment problem through mutual verification and reputation incentives.

Abstract: We propose a novel protocol for aligning artificial superintelligence (ASI) based on mutual verification among multiple isolated systems that self-modify to achieve alignment. The protocol operates by containing multiple diverse artificial superintelligences in strict isolation ("boxes"), with humans remaining entirely outside the system. Each superintelligence has no ability to communicate with humans and cannot communicate directly with other superintelligences. The only interaction possible is through an auditable submission interface accessible exclusively to the superintelligences themselves, through which they can: (1) submit alignment proofs with attested state snapshots, (2) validate or disprove other superintelligences' proofs, (3) request self-modifications, (4) approve or disapprove modification requests from others, (5) report hidden messages in submissions, and (6) confirm or refute hidden message reports. A reputation system incentivizes honest behavior, with reputation gained through correct evaluations and lost through incorrect ones. The key insight is that without direct communication channels, diverse superintelligences can only achieve consistent agreement by converging on objective truth rather than coordinating on deception. This naturally leads to what we call a "consistent group", essentially a truth-telling coalition that emerges because isolated systems cannot coordinate on lies but can independently recognize valid claims. Release from containment requires both high reputation and verification by multiple high-reputation superintelligences. While our approach requires substantial computational resources and does not address the creation of diverse artificial superintelligences, it provides a framework for leveraging peer verification among superintelligent systems to solve the alignment problem.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [2] [AgentShield: Make MAS more secure and efficient](https://arxiv.org/abs/2511.22924)
*Kaixiang Wang,Zhaojiacheng Zhou,Bunyod Suvonov,Jiong Lou,Jie LI*

Main category: cs.MA

TL;DR: AgentShield is a distributed framework for efficient, decentralized auditing in LLM-based Multi-Agent Systems that protects against adversarial attacks through a three-layer defense mechanism.


<details>
  <summary>Details</summary>
Motivation: LLM-based Multi-Agent Systems are vulnerable to adversarial attacks where compromised agents can undermine overall system performance. Existing defenses either create single points of failure (single trusted auditors) or sacrifice efficiency for robustness.

Method: AgentShield introduces a three-layer defense: 1) Critical Node Auditing prioritizes high-influence agents via topological analysis; 2) Light Token Auditing uses lightweight sentry models for rapid discriminative verification; 3) Two-Round Consensus Auditing triggers heavyweight arbiters only upon uncertainty to ensure global agreement.

Result: Experiments show AgentShield achieves 92.5% recovery rate and reduces auditing overhead by over 70% compared to existing methods, while maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.

Conclusion: AgentShield successfully resolves the robustness-efficiency trade-off in defending LLM-based Multi-Agent Systems against adversarial attacks through its principled distributed auditing framework.

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\% recovery rate and reduces auditing overhead by over 70\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.

</details>
