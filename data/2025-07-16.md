<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: The paper challenges traditional inductive biases in MARL by introducing the AIM framework, showing agents can achieve effective communication without external biases, aligning with neuroscience and LLM research.


<details>
  <summary>Details</summary>
Motivation: To question if artificial inductive biases in MARL are over-engineering and explore if agents can develop communication naturally.

Method: Uses the AIM framework with VQ-VAE to enable endogenous symbol systems, analyzing semantic compression and convergence.

Result: AIM achieves efficient communication without biases, with symbol usage following a power-law distribution, leading to new theoretical insights.

Conclusion: The findings bridge symbolism and connectionism, suggesting future work with HQ-VAE and RL pre-training for enhanced capabilities.

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [2] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: The paper provides a comprehensive evolutionary overview of the Web of Agents (WoA), linking modern protocols to historical standards and introducing a taxonomy to unify agent architectures. It highlights a paradigm shift in intelligence locus and identifies socio-technical challenges as the next research frontier.


<details>
  <summary>Details</summary>
Motivation: The fragmentation of research across communities obscures the intellectual lineage of modern systems, hindering a holistic understanding of the WoA's evolution.

Method: The study introduces a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism) to analyze and compare agent architectures across generations.

Result: The analysis reveals a paradigm shift in the 'locus of intelligence' and identifies modern protocols as evolutionary responses to earlier limitations. It also highlights the need for addressing socio-technical challenges.

Conclusion: New protocols alone are insufficient for a robust WoA ecosystem; the next research frontier involves solving decentralized identity, economic models, security, and governance challenges.

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [3] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: AgentOps is a framework for managing uncertainty in LLM-powered agentic systems, addressing the needs of developers, testers, SREs, and business users through a six-stage automation pipeline.


<details>
  <summary>Details</summary>
Motivation: Traditional observability and operations practices are inadequate for the unique uncertainties introduced by LLM-powered agentic systems, necessitating a specialized framework.

Method: The paper introduces AgentOps, a six-stage automation pipeline (observation, metric collection, issue detection, root cause analysis, optimized recommendations, runtime automation) tailored for agentic AI systems.

Result: AgentOps provides a structured approach to manage uncertainty, enabling safe and adaptive operation of LLM-powered systems.

Conclusion: AgentOps leverages automation to tame uncertainty, ensuring effective and self-improving AI systems without eliminating their inherent unpredictability.

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [4] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP is a protocol enabling secure, persistent, and semantically searchable memory sharing among AI agents, improving collaboration and reducing redundancy.


<details>
  <summary>Details</summary>
Motivation: Addresses ephemeral memory limitations in AI agents, hindering effective collaboration and knowledge sharing across sessions.

Method: Introduces SAMEP with distributed memory repository, vector-based semantic search, cryptographic access controls, and standardized APIs.

Result: Shows 73% reduction in redundant computations, 89% improvement in context relevance, and regulatory compliance.

Conclusion: SAMEP enables persistent, collaborative AI ecosystems with security and privacy guarantees.

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [5] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: A modular Agentic AI framework for visual classification integrates multimodal agents, a reasoning orchestrator, and RAG to improve trust and accuracy in zero-shot settings, achieving 85.63% accuracy in apple leaf disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: Addressing trust challenges in multi-agent AI systems, especially in zero-shot settings without fine-tuning, by developing a scalable and interpretable framework.

Method: Combines generalist multimodal agents with a non-visual reasoning orchestrator and RAG module, tested in three configurations: zero-shot, fine-tuned, and trust-calibrated orchestration.

Result: 77.94% accuracy improvement in zero-shot setting, with overall accuracy of 85.63%. GPT-4o showed better calibration, while Qwen-2.5-VL was overconfident. Image-RAG improved prediction grounding.

Conclusion: The framework separates perception from meta-reasoning, enabling scalable and interpretable multi-agent AI, applicable to trust-critical domains like diagnostics and biology. All components are open-sourced for reproducibility.

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [6] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: LLMs show fluency but fail in symbolic reasoning, arithmetic, and logic due to a gap between comprehension and competence, termed 'split-brain syndrome.'


<details>
  <summary>Details</summary>
Motivation: To diagnose why LLMs fail in tasks requiring principled reasoning despite their fluency.

Method: Controlled experiments and architectural analysis to identify the gap between comprehension and execution.

Result: LLMs articulate correct principles but fail to apply them due to computational execution issues, not knowledge access.

Conclusion: LLMs lack scaffolding for compositional reasoning, suggesting future models need metacognitive control and structural grounding.

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [7] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data integrates knowledge graphs, LLMs, and tool-use technologies to improve API call accuracy and domain-specific data handling in meteorology, outperforming existing systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the underexplored ability of LLMs to use tools via API calls in knowledge-intensive domains like meteorology, where domain-specific knowledge is critical.

Method: KG2data combines knowledge graphs, LLMs, ReAct agents, and tool-use technologies, evaluated via a virtual API for accuracy in name recognition, hallucination, and call correctness.

Result: KG2data outperforms RAG2data and chat2data with metrics (1.43%, 0%, 88.57%) vs. (16%, 10%, 72.14%) and (7.14%, 8.57%, 71.43%).

Conclusion: KG2data offers a novel, adaptable solution for intelligent question answering and data analysis in high-knowledge domains, mitigating LLM limitations and fine-tuning costs.

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [8] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: A rule-based method for music generation by mutating grammars derived from tunes, analyzing changes over multiple mutations.


<details>
  <summary>Details</summary>
Motivation: To explore how tunes evolve through systematic grammar mutations and measure the impact of each mutation type.

Method: Parse tunes into grammars using Sequitur, apply random mutations (19 types), expand grammars into new tunes, and analyze changes.

Result: Tracks gradual tune changes using edit distance, structural complexity, and length; evaluates mutation effects and musical quality.

Conclusion: Grammar-based mutation effectively generates related tunes, with measurable changes and insights into mutation impacts.

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [9] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: The paper examines AI's energy consumption and GHG emissions impact, projecting near-term strain but long-term potential for CO2 reduction.


<details>
  <summary>Details</summary>
Motivation: To assess whether AI will have a net positive, neutral, or negative impact on CO2 emissions by 2035, considering its growing role in various sectors.

Method: Analysis of energy consumption scenarios for data centers, including near-term (up to 2030) and long-term (2035+) projections, and evaluation of AI's role in optimizing energy workflows.

Result: Near-term: AI's growth may increase CO2 emissions due to high energy demands. Long-term: AI could significantly reduce emissions by optimizing processes across industries.

Conclusion: AI may initially increase emissions but has the potential to support climate mitigation efforts in the long run.

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [10] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: The paper evaluates deep learning and graph-based models for detecting IoT malicious attacks, with BERT achieving the highest accuracy (99.94%) and other models like Multi-Head Attention and GraphSAGE showing varying performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting malicious network traffic in IoT systems, which exhibit sequential and diverse traffic patterns.

Method: Utilizes GraphSAGE, BERT, TCN, Multi-Head Attention, BI-LSTM, and LSTM models to capture temporal patterns and feature significance.

Result: BERT outperformed others with 99.94% accuracy, while Multi-Head Attention provided interpretable results but required more processing time. GraphSAGE had the shortest training time but lower accuracy.

Conclusion: BERT is the most effective for IoT malicious attack detection, though other models like Multi-Head Attention offer trade-offs between interpretability and performance.

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [11] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: The paper proposes using neural networks to detect AI assistance in abstract tasks by preprocessing data into machine learning-friendly formats, including image and time-series formulations, and evaluates their effectiveness across various architectures.


<details>
  <summary>Details</summary>
Motivation: Detecting AI assistance is crucial as AI becomes ubiquitous, but it's challenging for humans, especially with abstract data. Neural networks offer a solution due to their classification capabilities.

Method: The study constructs four neural network-friendly image formulations and a time-series formulation to encode user behavior. It benchmarks these using three deep learning architectures and a hybrid CNN-RNN model.

Result: Common models can effectively classify AI-assisted data when preprocessed appropriately. The hybrid CNN-RNN model, leveraging temporal and spatial data, maximizes performance.

Conclusion: Preprocessing abstract data into suitable formats and using architectures that encode temporal and spatial information are key to detecting AI assistance effectively.

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [12] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: SigmaScheduling dynamically schedules decision points in mHealth interventions based on behavior time uncertainty, improving intervention timeliness compared to fixed-interval methods.


<details>
  <summary>Details</summary>
Motivation: Fixed-interval scheduling in mHealth interventions often fails for individuals with irregular routines, leading to ineffective interventions.

Method: SigmaScheduling adjusts decision points dynamically: closer to predicted behavior times when timing is predictable, and earlier when uncertain.

Result: In a trial with 68 participants, SigmaScheduling ensured decision points preceded brushing events in ≥70% of cases.

Conclusion: SigmaScheduling enhances precision in mHealth, especially for time-sensitive habitual behaviors like oral hygiene.

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [13] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: LLMs like GPT-4o can effectively replicate expert-driven thematic analysis of social media data using few-shot prompting, achieving high accuracy and F1-scores.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of LLMs in inductive thematic analysis, particularly for tasks requiring deep interpretive and domain-specific expertise.

Method: Evaluated five LLMs on Reddit datasets using binary classifications with zero-, single-, and few-shot prompting, measuring performance via accuracy, precision, recall, and F1-score.

Result: GPT-4o with two-shot prompting performed best (90.9% accuracy, F1-score: 0.71), closely mirroring expert classifications for high-prevalence themes.

Conclusion: Few-shot LLM-based approaches can automate thematic analyses, providing a scalable supplement for qualitative research.

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [14] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY is an open-source toolkit for analyzing and visualizing argumentation frameworks (AFs) in legal reasoning, addressing ambiguity and aiding non-experts.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in identifying ambiguity sources and explaining argument acceptance in legal reasoning for non-experts.

Method: AF-XRAY introduces layered visualizations, attack edge classification, overlay visualizations, and critical attack set identification.

Result: The tool transforms ambiguous scenarios into grounded solutions, revealing causes of ambiguity and enabling exploration of alternative resolutions.

Conclusion: AF-XRAY supports teleological legal reasoning by showing how assumptions impact conclusions, demonstrated with real-world cases.

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [15] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer generates high-quality navigation instructions by decomposing and recomposing semantic entities, while NavInstrCritic evaluates them without expert annotations.


<details>
  <summary>Details</summary>
Motivation: Expert-provided navigation instructions are scarce, and synthesized ones often lack quality, limiting large-scale research.

Method: NavComposer decomposes semantic entities (actions, scenes, objects) and recomposes them into instructions. NavInstrCritic evaluates instructions on contrastive matching, semantic consistency, and linguistic diversity.

Result: The framework produces rich, accurate instructions and offers a holistic evaluation, enabling scalable research.

Conclusion: NavComposer and NavInstrCritic provide a scalable, generalizable solution for language-guided navigation research.

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [16] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: The study explores using an LLM-based multi-agent system (MAS) for therapy recommendations in multimorbidity patients, finding single-agent GP models perform comparably to MDTs but with incomplete or conflicting advice.


<details>
  <summary>Details</summary>
Motivation: Therapy recommendations for chronic multimorbidity patients are complex due to treatment conflicts, and existing systems lack scalability. The study aims to leverage LLM-based MAS to simulate MDT decision-making for safer recommendations.

Method: A single-agent and MAS framework were designed to simulate MDT collaboration. Systems were evaluated on therapy planning tasks using benchmark cases, comparing MAS with single-agent approaches and real-world benchmarks.

Result: Single-agent GP models performed as well as MDTs, but recommendations were incomplete or included unnecessary medications, leading to conflicts.

Conclusion: LLM-based MAS shows promise for therapy recommendations, but current models need refinement to avoid incomplete or conflicting advice.

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [17] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: A framework called Knowledge-guided Preference Optimization (KPO) is proposed to mitigate the risks of harmful protein sequence generation by protein language models, ensuring safety without compromising functionality.


<details>
  <summary>Details</summary>
Motivation: Protein language models pose biosafety risks by potentially generating harmful sequences, necessitating a solution to balance functionality and safety.

Method: KPO integrates prior knowledge via a Protein Safety Knowledge Graph, uses graph pruning to identify safe sequences, and employs reinforcement learning to minimize harmful outputs.

Result: KPO successfully reduces hazardous sequence generation while maintaining high functionality.

Conclusion: KPO provides a robust safety framework for generative protein models in biotechnology.

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [18] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: The paper proposes a hybrid model combining CNNs and tabular data to predict bird species presence in shifting habitats due to climate change, achieving 85% accuracy.


<details>
  <summary>Details</summary>
Motivation: Climate-induced habitat shifts necessitate accurate methods to track bird species presence in new geographic locations.

Method: Uses CNNs for spatial features from satellite imagery and tabular data for environmental features (temperature, precipitation, elevation).

Result: The hybrid model achieves 85% accuracy in predicting bird distribution.

Conclusion: The scalable and reliable method aids in understanding bird migration amid climate change.

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [19] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec is a framework for personalized exercise recommendation using semantically-grounded knowledge tracing, addressing gaps in semantic content and structured learning progression. It combines KT models with RL, enhanced by model-based value estimation, and shows effectiveness in real-world math learning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing exercise recommendation methods often ignore semantic content and structured learning progression, limiting personalization.

Method: ExRec uses an end-to-end pipeline: annotating KCs, learning semantic representations, training KT models, and optimizing RL methods with model-based value estimation.

Result: Validated across four real-world math learning tasks, ExRec generalizes to new questions and produces interpretable learning trajectories.

Conclusion: KT-guided RL holds promise for effective personalization in education, as demonstrated by ExRec.

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [20] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: A vision-language model-based commander is proposed for autonomous multi-agent tactical decisions, combining scene understanding and strategic reasoning, achieving high win rates.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based and reinforcement learning methods lack adaptability and interpretability for strategic decisions in complex battlefield environments.

Method: Integrates a vision-language model for scene understanding and a lightweight large language model for strategic reasoning, unifying perception and decision-making.

Result: Achieves a win rate of over 80% in simulations compared to baseline models.

Conclusion: The proposed method offers a human-like cognitive process for tactical decisions, with strong adaptability and interpretability.

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [21] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: F2STrans improves LLM-based code translation by combining functional and style learning, outperforming larger models like Qwen-32B and GPT-4.


<details>
  <summary>Details</summary>
Motivation: Ensuring correctness and readability in code translation by LLMs remains a challenge, limiting real-world adoption.

Method: F2STrans uses functional learning (correctness) and style learning (readability) with mined code pairs and style examples.

Result: F2STrans significantly improves performance, enabling smaller models to outperform larger ones like Qwen-32B and GPT-4.

Conclusion: F2STrans advances code translation by addressing correctness and readability, demonstrating superior performance.

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [22] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS is an AI-driven decentralized system for tokenizing and trading physical gold on blockchain, achieving fast, secure, and compliant transactions.


<details>
  <summary>Details</summary>
Motivation: To bridge physical asset custody with blockchain, ensuring compliance, liquidity, and risk management for decentralized trading of alternative assets like gold.

Method: Combines on-chain smart contracts for risk control with off-chain AI agents (Compliance, Token Issuance, Market Making, Risk Control) coordinated by a core system.

Result: Achieves 1.2s token issuance, tight liquidity (spreads <0.5%), resilience to attacks, and scalability (5000 TPS, 10000 users).

Conclusion: AI agent-based decentralized exchanges can meet performance and safety needs, democratizing access to illiquid assets with transparent governance.

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [23] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: The paper introduces a formal definition for neurosymbolic AI, unifying logical and neural representations through an integral computation over a product of logical and belief functions.


<details>
  <summary>Details</summary>
Motivation: The field lacks a generally accepted formal definition of neurosymbolic AI, despite numerous existing systems.

Method: Proposes a formal definition of neurosymbolic inference as an integral over a product of logical and belief functions.

Result: The definition abstracts key ingredients of representative neurosymbolic AI systems.

Conclusion: The paper provides a foundational formalization for neurosymbolic AI, bridging learning and reasoning.

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [24] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: A collaborative approach for trustworthy decision-making in autonomous systems, using quality attributes and BDDs for efficient reasoning.


<details>
  <summary>Details</summary>
Motivation: Ensuring safe and correct behavior of autonomous systems in dynamic environments is challenging, requiring reliable decision-making.

Method: Proposes a collaborative approach using quality attributes (e.g., perception quality) for trustworthiness, with BDDs for belief aggregation and propagation.

Result: Introduces reduction rules for BDDs to enable efficient automated reasoning in collaborative settings.

Conclusion: The approach enhances trustworthiness and reliability in autonomous systems by leveraging collaborative data sharing and formal reasoning.

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [25] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: The paper addresses the challenge of computing the actual maximum delay in integrated circuits, using Answer Set Programming (ASP) for accurate results.


<details>
  <summary>Details</summary>
Motivation: Static Timing Analysis provides an upper bound for delay, leading to suboptimal processor speeds. The goal is to compute the actual maximum delay for better performance.

Method: The problem is modeled in Answer Set Programming (ASP), with non-trivial encodings proposed to handle its computational hardness.

Result: Experimental results demonstrate ASP's viability for solving complex hardware design problems.

Conclusion: ASP offers a promising solution for accurately determining maximum delays in integrated circuits, improving performance.

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [26] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph is a new KG reasoning method that addresses score over-smoothing by separating global and local information processing and using a coarse-to-fine optimization strategy, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing KG reasoning methods suffer from score over-smoothing, which reduces reasoning effectiveness by blurring distinctions between correct and incorrect answers.

Method: DuetGraph uses dual-pathway global-local fusion (separating local message passing and global attention) and coarse-to-fine optimization (partitioning entities into high- and low-score subsets).

Result: DuetGraph improves reasoning quality by up to 8.7% and accelerates training efficiency by 1.8x, outperforming existing methods.

Conclusion: DuetGraph effectively mitigates over-smoothing and enhances KG reasoning, demonstrating superior performance and efficiency.

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [27] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: The paper introduces the Opus Prompt Intention Framework to enhance workflow generation with LLMs by adding an intermediate intention capture layer.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability and scalability of workflow generation from complex user queries using LLMs.

Method: Proposes the Opus Workflow Intention Framework, which extracts workflow signals, interprets them into structured intentions, and generates workflows based on these.

Result: Shows consistent improvements in semantic workflow similarity metrics on a benchmark of 1,000 multi-intent query-workflow pairs.

Conclusion: The framework significantly enhances workflow generation quality, especially for mixed intention elicitation, compared to direct generation.

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [28] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: The paper explores using Edge-Weighted Quantitative Bipolar Argumentation Frameworks (EW-QBAFs) to align AI decisions with human preferences, introducing a contestability problem and solving it with gradient-based explanations and an iterative algorithm.


<details>
  <summary>Details</summary>
Motivation: To ensure AI-driven decisions are contestable and align with human preferences, focusing on EW-QBAFs as an underexplored method.

Method: Proposes gradient-based relation attribution explanations (G-RAEs) and an iterative algorithm to adjust edge weights for achieving desired argument strength.

Result: Experimental evaluation on synthetic EW-QBAFs shows the approach effectively solves the contestability problem.

Conclusion: The method successfully enables contestability in AI decisions using EW-QBAFs, validated by synthetic simulations.

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [29] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: CogDDN is a VLM-based framework for demand-driven navigation that integrates fast and slow thinking systems, improving navigation accuracy by 15% over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Mobile robots need to navigate unknown environments by understanding human intent, but traditional data-driven methods lack generalization in unseen scenarios.

Method: CogDDN uses semantic alignment of objects with instructions and a dual-process decision-making module (Heuristic and Analytic Processes) enhanced by Chain of Thought reasoning.

Result: CogDDN outperforms single-view camera-only methods by 15% in navigation accuracy and adaptability, as shown in AI2Thor simulator evaluations.

Conclusion: CogDDN effectively emulates human cognitive mechanisms, enhancing robot navigation and adaptability in unstructured environments.

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [30] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: A neurosymbolic framework combines natural-language dialogue with verifiable guarantees for logistics planning, improving accuracy and speed over traditional and LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Logistics decisions require rapid, expert replanning under uncertainty, but current methods (integer programming and LLMs) are either slow or unreliable.

Method: The framework converts user requests into structured plans, quantifies uncertainty, and uses an interactive clarification loop for low-confidence cases.

Result: A lightweight model fine-tuned on 100 examples outperforms GPT-4.1 in zero-shot tasks and reduces latency by 50%.

Conclusion: The approach offers a practical solution for certifiable, real-time logistics decision-making.

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [31] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: A novel approach combines code-as-text modeling with structured analysis to enhance Code LLMs' reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models struggle with structured code properties like control and data flow, while existing structured approaches lack generative scalability.

Method: Combines code-as-text modeling with structured forms to leverage both generative and analytical strengths.

Result: Proposes a hybrid approach to improve reasoning in Code LLMs without sacrificing generative capabilities.

Conclusion: The new method bridges the gap between generative and structured analysis in code modeling.

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [32] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: AI systems using human language for thought chains (CoT) can be monitored for misbehavior, though imperfectly. Further research and investment in CoT monitoring are recommended, alongside caution in development to preserve monitorability.


<details>
  <summary>Details</summary>
Motivation: To enhance AI safety by leveraging human-like thought processes for monitoring misbehavior, despite inherent limitations.

Method: Proposes monitoring chains of thought (CoT) in AI systems to detect intent to misbehave, acknowledging its imperfections.

Result: CoT monitoring shows promise but is not foolproof; some misbehavior may evade detection.

Conclusion: Recommends further research into CoT monitorability, investment in monitoring, and careful development decisions to maintain its effectiveness.

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [33] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR integrates Perspective-Aware AI with XR for adaptive, context-aware immersive experiences using user identity models called Chronicles.


<details>
  <summary>Details</summary>
Motivation: Current XR systems lack deep user modeling and cognitive context, limiting adaptive experiences.

Method: PAiR uses Chronicles—multimodal identity models—in a closed-loop system to link user states with XR environments.

Result: Two proof-of-concept scenarios in Unity-based OpenDome demonstrate PAiR's utility.

Conclusion: PAiR advances human-AI interaction by embedding perspective-based identity models in XR.

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [34] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: The paper critiques three core tenets of RL (agency, learning objectives, reward hypothesis) and proposes an evolutionary-inspired framework to rethink them, addressing biological plausibility and practical implications.


<details>
  <summary>Details</summary>
Motivation: To challenge and revise foundational assumptions in RL, making it more biologically plausible and theoretically robust.

Method: Uses evolutionary theory to revisit RL dogmas, integrating insights from evolutionary dynamics and origins-of-life theory.

Result: Proposes a framework to enrich RL theory, addressing agency, learning objectives, and reward hypotheses with evolutionary analogies.

Conclusion: Evolutionary insights can refine RL, but agency requires additional integration with origins-of-life thermodynamics.

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [35] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: DrafterBench is a benchmark for evaluating LLM agents in technical drawing revision tasks in civil engineering, featuring 12 task types, 46 tools, and 1920 tasks.


<details>
  <summary>Details</summary>
Motivation: The need for systematic evaluation of LLM agents in industrial tasks like civil engineering, particularly technical drawing revision.

Method: Proposed DrafterBench, an open-source toolkit with diverse tasks and tools to assess LLM agents' capabilities in structured data comprehension, function execution, and more.

Result: DrafterBench provides detailed accuracy and error analysis, offering insights into agent performance and areas for improvement.

Conclusion: DrafterBench aims to enhance LLM agent integration in engineering by identifying improvement targets and rigorously testing capabilities.

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [36] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: IFScale benchmark evaluates LLMs' instruction-following at high densities, revealing performance degradation patterns and biases, with top models achieving only 68% accuracy at 500 instructions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack evaluation of LLMs' performance at high instruction densities, limiting understanding of their real-world applicability.

Method: Introduces IFScale, a benchmark with 500 keyword-inclusion instructions for business report writing, tested on 20 state-of-the-art models.

Result: Best models achieve 68% accuracy at 500 instructions, with performance degradation linked to model size and reasoning capability.

Conclusion: IFScale highlights tradeoffs in instruction-dense prompts and provides insights for real-world LLM system design.

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: The paper introduces novel tool-to-tool matching (TTTM) pipelines for semiconductor manufacturing, addressing limitations of traditional methods like reliance on static data or golden references, and poor performance in heterogeneous equipment settings.


<details>
  <summary>Details</summary>
Motivation: Traditional TTTM methods struggle with dynamic manufacturing environments and heterogeneous equipment, necessitating more adaptable solutions.

Method: Proposes univariate and multivariate TTTM pipelines, leveraging variance and modes in data to identify mismatched equipment.

Result: Univariate methods achieve high correlation (>0.95 with variance, >0.5 with modes); multivariate methods correlate >0.75 with top univariate methods.

Conclusion: The proposed methods effectively address TTTM challenges, with multivariate methods showing robustness to hyper-parameters.

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [38] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK is a reinforcement learning framework that teaches language models diverse tool usage, optimizing for answer quality and tool diversity, and outperforms baselines in performance and exploration.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning capabilities of language models by encouraging systematic exploration of diverse tool usage beyond conventional methods.

Method: Uses a dual-objective reward system and rarity-first exploitation strategy, training a Llama-3.1 8B model via offline PPO on synthetic MMLU-Pro trajectories.

Result: Achieves competitive performance on MMLU-Pro with higher tool selection entropy, indicating better exploration without accuracy loss.

Conclusion: Explicit tool diversity in reinforcement learning can improve reasoning while maintaining accuracy.

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [39] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: A novel Linearly Adaptive Cross Entropy Loss function is introduced, outperforming standard cross entropy in classification tasks while maintaining simplicity.


<details>
  <summary>Details</summary>
Motivation: To enhance optimization in classification tasks with one-hot encoded labels by incorporating a term dependent on the predicted probability of the true class.

Method: Derived from information theory, the proposed loss function adds an adaptive term to the standard cross entropy. Evaluated using a ResNet-based model on CIFAR-100.

Result: Consistently outperforms standard cross entropy in classification accuracy with similar efficiency.

Conclusion: The proposed loss function shows promise for future research in loss function design.

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [40] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: VolSched is a novel adaptive learning rate scheduler inspired by volatility in stochastic processes, improving model generalization by dynamically adjusting LR based on accuracy volatility.


<details>
  <summary>Details</summary>
Motivation: Pre-defined and adaptive LR schedulers often lead to suboptimal generalization, prompting the need for a more dynamic approach.

Method: VolSched calculates the ratio between long-term and short-term accuracy volatility to adjust LR, escaping plateaus and stabilizing training.

Result: On CIFAR-100 with ResNet-18/34, VolSched improves top-1 accuracy by 1.4/1.3 percentage points and finds flatter minima (38% flatter than baselines).

Conclusion: VolSched enhances exploration and generalization, achieving better performance and wider minima compared to existing schedulers.

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [41] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: The paper explores the theoretical foundations of deep learning and Transformers, proving a universal approximation theorem for single-layer Transformers.


<details>
  <summary>Details</summary>
Motivation: Despite the success of deep learning and Transformers, their theoretical understanding is limited. This paper aims to bridge this gap.

Method: The authors review key mathematical concepts, analyze self-attention and backpropagation, and prove a universal approximation theorem for Transformers.

Result: A single-layer Transformer can approximate any continuous sequence-to-sequence mapping on a compact domain to arbitrary precision.

Conclusion: The findings enhance the theoretical understanding of Transformers and connect theory with practical applications.

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [42] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: The paper introduces MH-FSF, a modular framework for feature selection, addressing reproducibility issues by benchmarking 17 methods on 10 public Android malware datasets. Results show performance variations, emphasizing the need for data preprocessing and unified evaluation.


<details>
  <summary>Details</summary>
Motivation: Current feature selection research lacks reproducibility due to limited benchmarking and proprietary datasets, impacting performance and consistency.

Method: The MH-FSF framework is developed, offering 17 feature selection methods (11 classical, 6 domain-specific) and systematic evaluation on 10 public Android malware datasets.

Result: Performance varies across balanced and imbalanced datasets, underscoring the importance of preprocessing and selection criteria.

Conclusion: MH-FSF fosters methodological consistency, broadens literature, and opens new research directions in feature selection, especially for Android malware detection.

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [43] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: OL-MDISF addresses challenges in online learning with mixed, drifted, and incomplete features using latent copula-based representation, drift detection, and pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: Challenges include heterogeneous data, distribution shifts, and lack of labels in streaming features.

Method: OL-MDISF uses latent copula-based representation, ensemble entropy for drift detection, and structure-aware pseudo-labeling.

Result: Tested on 14 datasets, showing CER trends, ablation studies, and temporal dynamics.

Conclusion: Provides a reproducible benchmark for weakly supervised online learning.

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [44] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: DTRGC is a novel method for deep graph clustering in attribute-missing graphs, using hierarchical imputation and clustering feedback to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing imputation methods for attribute-missing graphs often fail due to varying neighborhood information, leading to unreliable clustering results.

Method: DTRGC employs Dynamic Cluster-Aware Feature Propagation (DCFP), Hierarchical Neighborhood-aware Imputation (HNAI), and Hop-wise Representation Enhancement (HRE) to iteratively impute missing attributes and refine clustering.

Result: DTRGC significantly improves clustering performance on six benchmark datasets compared to existing methods.

Conclusion: DTRGC effectively addresses the challenges of attribute-missing graphs by leveraging clustering feedback and hierarchical imputation, offering a robust solution for unsupervised clustering tasks.

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [45] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: RedOne, a domain-specific LLM for SNS, outperforms single-task baselines by 14.02% on average across 8 tasks and reduces harmful content exposure by 11.23%.


<details>
  <summary>Details</summary>
Motivation: Address challenges in SNS content management and interaction quality by overcoming limitations of isolated task-focused LLMs.

Method: Three-stage training: continued pretraining, supervised fine-tuning, and preference optimization using large-scale real-world data.

Result: 14.02% average improvement in SNS tasks, 7.56% in bilingual benchmarks, 11.23% reduction in harmful content exposure, and 14.95% boost in post-view search.

Conclusion: RedOne is a robust, generalizable domain-specific LLM for SNS with real-world applicability.

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [46] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD is a scalable framework using diffusion models to generate synthetic layout heatmaps for ML in physical design, addressing dataset limitations.


<details>
  <summary>Details</summary>
Motivation: Overcoming the scarcity of high-quality, large-scale datasets for ML in physical design tasks due to computational costs and IP constraints.

Method: Uses a diffusion model to generate diverse synthetic layout heatmaps (power, IR drop, congestion, etc.) quickly.

Result: Created a dataset of 20,000+ layout configurations resembling real layouts, improving ML accuracy for tasks like IR drop prediction.

Conclusion: DALI-PD provides an efficient solution for generating synthetic datasets, enhancing ML research in physical design.

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [47] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: The paper proposes a predictive modeling framework for optimizing desalination performance in the UAE, addressing challenges like AOD and climate uncertainties, achieving 98% accuracy and offering a decision-support dashboard.


<details>
  <summary>Details</summary>
Motivation: The UAE's heavy reliance on energy-intensive desalination, contributing to CO2 emissions and facing climate-related challenges like AOD, necessitates innovative solutions for sustainability.

Method: A two-stage predictive model forecasts AOD and desalination efficiency losses, followed by dust-aware control logic for system adjustments. SHAP analysis identifies degradation drivers.

Result: The framework achieved 98% accuracy, with SHAP revealing key degradation factors. An interactive dashboard was developed for scenario analytics.

Conclusion: The study provides a climate-adaptive decision-support system for desalination plants, enhancing sustainability and operational efficiency.

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [48] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA is a novel federated learning framework designed to handle label noise and data imbalance in medical image classification, improving model robustness and stability.


<details>
  <summary>Details</summary>
Motivation: Label noise and data imbalance in federated learning degrade model performance, especially in medical contexts.

Method: FedGSCA uses a Global Sample Selector to aggregate noise knowledge and a Client Adaptive Adjustment mechanism for dynamic class distribution handling.

Result: Outperforms state-of-the-art methods in extreme and heterogeneous noise scenarios, improving model stability.

Conclusion: FedGSCA is effective for real-world medical federated learning, addressing noise and imbalance challenges.

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [49] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: The paper revisits scaling laws in NLP, identifying data quality and training strategies as key factors in sub-scaling. It proposes a sub-optimal scaling law to better predict performance.


<details>
  <summary>Details</summary>
Motivation: Recent studies show deviations from traditional scaling laws, with performance improvements slowing in large models (sub-scaling). This paper aims to understand why.

Method: Empirical analysis of over 400 models to study the impact of data quality (density, diversity) and resource allocation.

Result: High data density causes diminishing returns, and non-optimal resource allocation worsens sub-scaling. A new sub-optimal scaling law is proposed.

Conclusion: Data quality and diversity, along with optimal resource allocation, are critical for sustained performance improvements in large models.

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [50] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: The paper explores fine-tuning LLMs for algorithm design, introducing a Diversity-Aware Rank-based sampling strategy and direct preference optimization to enhance performance. Results show task-specific LLMs outperform general models and generalize well.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs tailored for algorithm design are necessary and how to obtain them, given the reliance on general-purpose LLMs in current methods.

Method: Proposes a Diversity-Aware Rank-based (DAR) sampling strategy and uses direct preference optimization to fine-tune LLMs (Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct) for algorithm design tasks.

Result: Fine-tuned LLMs outperform general models, with smaller models matching larger ones in some tasks, and show promising generalization across related tasks.

Conclusion: Task-specific adaptation of LLMs is valuable for algorithm design, offering improved performance and generalization, and opens new research directions.

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [51] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: Comparative analysis of RL and SFT for LLM reasoning training shows RL has minor in-domain gains, while SFT causes more pronounced changes and out-of-domain degradation. Freezing parts of the model yields inconclusive results.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics of RL and SFT for LLM reasoning, as their effects on model performance are unclear.

Method: Comparative analysis of RL and SFT on the same maths problems with the same model and similar hyperparameters, including parameter updates and freezing experiments.

Result: RL shows minor in-domain gains and slight degradation on knowledge benchmarks, while SFT has more pronounced effects and greater parameter updates. Freezing parts of the model yields mixed results.

Conclusion: RL amplifies existing capabilities, while SFT replaces old skills with new ones, with freezing experiments providing inconclusive mitigation.

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [52] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: The paper investigates compute requirements for algorithmic innovations in large language model pretraining, analyzing 36 innovations in Llama 3 and DeepSeek-V3. It finds that compute caps may not significantly slow AI progress.


<details>
  <summary>Details</summary>
Motivation: To understand the compute resources needed for algorithmic innovations in pretraining and assess the impact of compute caps on AI progress.

Method: Catalog and analyze 36 pretraining algorithmic innovations, estimating their FLOP requirements and hardware utilization.

Result: Compute requirements for innovations double yearly. Even stringent compute caps could allow half of the innovations.

Conclusion: Compute caps alone are unlikely to dramatically slow AI algorithmic progress.

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [53] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: A meta-learning framework is proposed for dynamic spectrum allocation in 5G/6G networks, outperforming traditional DRL methods in throughput, safety, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional DRL methods are inefficient and unsafe for dynamic spectrum allocation due to high sample complexity and unguided exploration risks.

Method: Three meta-learning architectures (MAML, RNN, attention-enhanced RNN) are implemented and compared to PPO in a simulated IAB environment.

Result: The attention-based meta-learning agent achieves 48 Mbps peak throughput, reduces SINR/latency violations by >50%, and shows better fairness (0.7 index) compared to PPO (10 Mbps).

Conclusion: Meta-learning is a safer and more effective solution for intelligent control in complex wireless systems.

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [54] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: This tutorial overviews LLM-based cross-modal time series analytics, classifying approaches into conversion, alignment, and fusion, and discusses applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for time series analytics due to shared sequential nature with text, but a cross-modality gap exists. This tutorial aims to bridge this gap and expand LLM applications in real-world problems.

Method: Introduces a taxonomy classifying approaches into conversion, alignment, and fusion strategies, and reviews their applications in downstream tasks.

Result: Provides a comprehensive overview of current methodologies, applications, and open challenges in cross-modal time series analytics.

Conclusion: The tutorial enhances understanding of LLM applications in time series analytics, balancing effectiveness and efficiency, and highlights future research directions.

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [55] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: Extends diffusion and flow-based generative models to weight space learning, leveraging optimization dynamics for structural priors and unifying trajectory inference techniques under gradient flow matching.


<details>
  <summary>Details</summary>
Motivation: To advance generative models by applying them to weight space learning, using optimization dynamics as inductive bias for improved performance.

Method: Models gradient descent trajectories as inference problems, incorporates autoencoders for latent weight representation, and uses reward fine-tuning and task-specific conditioning.

Result: Matches or surpasses baselines in generating in-distribution weights, improves downstream training initialization, and excels in detecting harmful covariate shifts.

Conclusion: The method effectively applies generative models to weight space learning, offering practical benefits in performance and safety-critical applications.

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [56] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: HIGFormer, a graph-augmented transformer model, improves soccer match outcome prediction by capturing multi-level player and team interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to account for the heterogeneous interactions among players and teams, which are crucial for accurate match outcome prediction.

Method: HIGFormer uses a multi-level interaction framework: Player Interaction Network (heterogeneous graphs), Team Interaction Network (team-to-team graphs), and Match Comparison Transformer (joint analysis).

Result: HIGFormer achieves superior prediction accuracy on the WyScout dataset and offers insights for player evaluation and team strategy.

Conclusion: HIGFormer advances soccer outcome prediction by modeling complex interactions, with potential applications in talent scouting and strategy analysis.

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [57] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: RLVR improves LLMs in reasoning tasks but faces instability. GHPO, a difficulty-aware framework, dynamically adjusts task difficulty, balancing imitation and exploration, achieving 5% better performance.


<details>
  <summary>Details</summary>
Motivation: Address training instability and inefficiency in RLVR due to capacity-difficulty mismatch, especially in smaller LLMs.

Method: Introduces GHPO, which uses adaptive prompt refinement to balance imitation learning and exploration-based RL.

Result: 5% average performance gain on math benchmarks, outperforming baselines in stability and reasoning.

Conclusion: GHPO offers a scalable, efficient solution for robust reasoning models.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [58] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: RFF-GP-HSMM is a fast unsupervised time-series segmentation method using random Fourier features to reduce computational costs of GP-HSMM, achieving comparable performance with 278x speedup.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of GP-HSMM due to kernel matrix inversion for large datasets motivates the need for a faster alternative.

Method: The method approximates Gaussian processes with linear regression using RFF, avoiding kernel matrix inversion while maintaining expressive power.

Result: Experiments show comparable segmentation performance to conventional methods with a 278x speedup on 39,200-frame data.

Conclusion: RFF-GP-HSMM effectively balances performance and computational efficiency for large-scale time-series segmentation.

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [59] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: GeoHopNet is a novel Hopfield-augmented sparse spatial attention network for dynamic UAV site selection, addressing computational bottlenecks with innovations like distance-biased attention and K-nearest neighbor sparsity.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of urban UAV economy demands efficient solutions for dynamic site selection, but traditional methods struggle with computational complexity for large-scale problems.

Method: Proposes GeoHopNet with four innovations: distance-biased multi-head attention, K-nearest neighbor sparse attention, Hopfield external memory, and memory regularization.

Result: GeoHopNet handles 1,000-node problems in under 0.1s with a 0.22% optimality gap, outperforming baselines in speed (1.8× faster) and quality (22.2% better).

Conclusion: GeoHopNet advances large-scale UAV site selection by combining spatial awareness and computational efficiency, setting new benchmarks for solvable problem sizes.

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [60] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP introduces ReLUDown and Decreasing Backpropagation for balanced continual learning, outperforming state-of-the-art methods with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Address the imbalance between plasticity and stability in continual learning for computer vision.

Method: Combines ReLUDown (activation modification) and Decreasing Backpropagation (gradient-scheduling).

Result: Matches/exceeds state-of-the-art performance on Continual ImageNet with reduced computational cost.

Conclusion: RDBP is a practical, efficient benchmark for future continual learning strategies.

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [61] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: ZClassifier replaces deterministic logits with Gaussian-distributed logits, unifying uncertainty calibration and latent control via KL divergence minimization, improving robustness and calibration.


<details>
  <summary>Details</summary>
Motivation: To address temperature scaling and manifold approximation in classification while unifying uncertainty calibration and latent control.

Method: Uses diagonal Gaussian-distributed logits, minimizing KL divergence between predicted Gaussians and a unit isotropic Gaussian.

Result: Outperforms softmax classifiers in robustness, calibration, and latent separation on CIFAR-10 and CIFAR-100.

Conclusion: ZClassifier provides a principled probabilistic framework for classification and demonstrates effectiveness in classifier-guided generation.

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [62] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: An AI model using Hopfield neural networks for bioacoustic analysis is proposed, addressing data scarcity, environmental impact, and hardware demands. It's fast, lightweight, and accurate.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of analyzing large bioacoustic datasets with limited training data, high energy consumption, and hardware requirements.

Method: Uses associative memory via a transparent Hopfield neural network, requiring minimal training data (one signal per target sound).

Result: Achieves 86% precision, processes 10,384 bat recordings in 5.4s, and uses only 144.09MB RAM. No disagreements with expert identifications.

Conclusion: The model is a sustainable, efficient, and accurate solution for bioacoustic analysis, suitable for deployment on standard devices.

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [63] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: The paper explores how neural networks can achieve radical generalization in learning symmetry functions, using base addition as an example. It analyzes alternative carry functions, trains networks with them, and finds learning speed depends on carry structure.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing neural networks capable of radical generalization, particularly in learning symmetry functions like base addition.

Method: Group theoretic analysis of base addition, introducing alternative carry functions, and training neural networks with different carries to study learning efficacy.

Result: Simple neural networks can achieve radical generalization with suitable input formats and carry functions, with learning speed tied to carry structure.

Conclusion: The findings highlight the importance of carry function structure in symmetry learning, offering insights for cognitive science and machine learning.

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [64] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: A neural-surrogate framework using a 1D Convolutional Residual Network is introduced for parameter estimation in Stochastic Petri Nets (SPNs) with covariate-dependent rates, outperforming traditional methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation in SPNs is challenging, especially with covariate-dependent rates and unavailable explicit likelihoods, necessitating a robust, likelihood-free solution.

Method: A lightweight 1D Convolutional Residual Network is trained on Gillespie-simulated SPN data to predict rate-function coefficients from noisy, partially observed trajectories, using Monte Carlo dropout for uncertainty bounds.

Result: The surrogate achieves RMSE = 0.108 on synthetic SPNs with 20% missing events and runs faster than traditional Bayesian methods.

Conclusion: The neural-surrogate framework enables accurate, robust, and real-time parameter recovery in complex, partially observed discrete-event systems.

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [65] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: The paper introduces a method to address outliers and distributional uncertainty in Distributionally Robust Optimization (DRO) for generalized linear models, achieving an estimation error of O(√ε) with contaminated data.


<details>
  <summary>Details</summary>
Motivation: To tackle the dual challenges of data contamination and distributional shifts in DRO, ensuring robust decision-making.

Method: A novel modeling framework integrates robustness against data corruption and distributional shifts, with an efficient algorithm inspired by robust statistics.

Result: The method achieves an estimation error of O(√ε) for the true DRO objective value using contaminated data under bounded covariance.

Conclusion: This work provides the first rigorous guarantees for learning under data contamination and distributional shifts, with efficient computation.

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [66] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: A neurosymbolic framework, Ground-Compose-Reinforce, is proposed for grounding formal language in perception and action, enabling RL agents to follow language instructions efficiently without manual design.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of grounding language in complex perception and action for situated agents, avoiding manual design or massive datasets.

Method: Uses a neurosymbolic framework combining data-driven learning and compositional formal language semantics for efficient grounding and generalization.

Result: Demonstrates reliable mapping of formal language instructions to behaviors in image-based gridworld and MuJoCo robotics, outperforming end-to-end data-driven methods.

Conclusion: The framework efficiently grounds language and generalizes to arbitrary compositions, offering a scalable solution for language-based agent control.

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [67] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: A benchmarking framework in NVIDIA PhysicsNeMo-CFD is introduced to evaluate AI models for automotive aerodynamics, focusing on accuracy, performance, scalability, and generalization.


<details>
  <summary>Details</summary>
Motivation: To standardize and improve the assessment of AI models for automotive aerodynamics, enhancing transparency and consistency in performance evaluation.

Method: The framework incorporates diverse metrics and evaluates three AI models (DoMINO, X-MeshGraphNet, FIGConvNet) using the DrivAerML dataset, with guidelines for extensibility.

Result: Demonstrates utility by assessing surface and volumetric flow field predictions, enabling model comparison and integration of new models/datasets.

Conclusion: The framework accelerates AI-driven aerodynamic research, aiding in the selection and refinement of models for more efficient and accurate solutions.

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [68] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners is a framework for spatial reasoning using generative denoising models, simplifying research with easy interfaces for variable mapping, model paradigms, and inference strategies.


<details>
  <summary>Details</summary>
Motivation: The complexity of generative reasoning with denoising models due to varied formulations, samplers, and inference strategies necessitates a streamlined framework.

Method: The framework provides interfaces for variable mapping, generative model paradigms, and inference strategies, leveraging denoising models for spatial reasoning.

Result: Spatial Reasoners is developed and made openly available to facilitate research in generative spatial reasoning.

Conclusion: The framework addresses the high effort in generative reasoning, enabling easier exploration of denoising models for spatial tasks.

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [69] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: Proposes Phy-SSM, a method integrating partial physics knowledge into state space models for long-term dynamic forecasting in noisy, irregularly sampled environments.


<details>
  <summary>Details</summary>
Motivation: SSMs capture long-range dependencies, and adding physics knowledge improves generalization for long-term forecasting in complex scenarios.

Method: Decomposes partially known dynamics into known/unknown state matrices, integrates them into Phy-SSM, and adds physics state regularization for alignment.

Result: Outperforms baselines in vehicle motion, drone state, and COVID-19 forecasting tasks.

Conclusion: Phy-SSM effectively combines physics and SSMs for superior long-term forecasting in complex environments.

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [70] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: The paper introduces multi-armed sampling, a framework analogous to multi-armed bandits but for sampling. It explores the exploration-exploitation trade-off, defines regret notions, and proposes an optimal algorithm. Unlike optimization, sampling doesn't require exploration. The work connects sampling and bandit problems via a temperature parameter and has implications for neural samplers and RLHF.


<details>
  <summary>Details</summary>
Motivation: To rigorously study the exploration-exploitation trade-off in sampling, contrasting it with optimization, and to unify sampling and bandit problems.

Method: Defines regret notions for multi-armed sampling, establishes lower bounds, and proposes an algorithm achieving optimal regret. Introduces a temperature parameter to interpolate between sampling and bandit problems.

Result: Shows sampling doesn't require exploration, unlike optimization. The algorithm achieves optimal regret bounds. The framework unifies sampling and bandit problems.

Conclusion: The multi-armed sampling framework is foundational for studying sampling, with implications for neural samplers, entropy-regularized RL, and RLHF. It clarifies exploration needs and algorithm convergence.

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [71] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: A new causal framework is proposed to handle out-of-domain interventions in temporal sequences, improving ATE estimation with a Transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Existing causal inference methods ignore out-of-domain interventions, which can significantly alter causal dynamics in real-world settings.

Method: A new causal framework extends Rubin's model to capture causal shifts under out-of-domain interventions, using an unbiased ATE estimator and a Transformer-based neural network.

Result: The method outperforms baselines in ATE estimation and goodness-of-fit on simulated and real-world datasets.

Conclusion: The proposed framework effectively addresses the impact of out-of-domain interventions, enhancing causal inference in temporal processes.

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [72] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: The paper shows Semantic Context (SC) is key for tool orchestration, introducing SC-LinUCB for lower regret, validating SC's role in LLMs, and proposing the FiReAct pipeline for large-scale tool orchestration.


<details>
  <summary>Details</summary>
Motivation: To establish SC as a foundational component for robust tool orchestration, addressing efficiency and adaptability in dynamic action spaces.

Method: Theoretical foundation with SC-LinUCB (contextual bandits), empirical validation with LLMs, and the FiReAct pipeline for large-scale tool retrieval.

Result: SC-LinUCB achieves lower regret; SC enhances LLM learning and adaptation; FiReAct enables effective orchestration over 10,000 tools.

Conclusion: SC is essential for sample-efficient, adaptive, and scalable orchestration agents.

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [73] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: The paper proposes using Graph Convolutional Networks (GCNs) to solve constrained assortment optimization efficiently, achieving high performance on large-scale instances.


<details>
  <summary>Details</summary>
Motivation: Assortment optimization is NP-hard and challenging due to its combinatorial and non-linear nature, requiring efficient solutions.

Method: Develop a graph representation of the problem, train a GCN to learn optimal patterns, and propose two inference policies.

Result: GCNs achieve 90%+ optimality on large-scale instances (up to 2,000 products) within seconds, outperforming existing heuristics.

Conclusion: The GCN-based approach is effective and scalable, even in model-free settings with unknown choice models.

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [74] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: The paper proposes a Wasserstein distance-based method for offline RL to address distributional shift, using ICNNs for stable learning without adversarial training, showing strong performance on D4RL.


<details>
  <summary>Details</summary>
Motivation: Offline RL faces challenges like distributional shift, where policies deviate from dataset distributions, risking unreliable actions. Existing methods use density ratios, but the authors propose a more robust alternative.

Method: The approach leverages the Wasserstein distance for regularization, modeled via input-convex neural networks (ICNNs) to compute distances without adversarial training.

Result: The method achieves comparable or better performance than existing techniques on the D4RL benchmark.

Conclusion: The proposed Wasserstein-based approach is effective for offline RL, offering stability and robustness against distributional shift.

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [75] [Collaboration Promotes Group Resilience in Multi-Agent RL](https://arxiv.org/abs/2111.06614)
*Ilai Shraga,Guy Azran,Matthias Gerstgrasser,Ofir Abu,Jeffrey S. Rosenschein,Sarah Keren*

Main category: cs.LG

TL;DR: The paper introduces 'group resilience' in multi-agent RL, showing collaborative protocols outperform non-collaborative ones in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resilience frameworks in multi-agent RL and explore how collaboration enhances adaptability.

Method: Formalized group resilience, tested various collaboration protocols in MARL, and compared their performance.

Result: Collaborative approaches consistently achieved higher group resilience than non-collaborative ones.

Conclusion: Collaboration is key to achieving group resilience in multi-agent RL, with empirical support for its effectiveness.

Abstract: To effectively operate in various dynamic scenarios, RL agents must be
resilient to unexpected changes in their environment. Previous work on this
form of resilience has focused on single-agent settings. In this work, we
introduce and formalize a multi-agent variant of resilience, which we term
group resilience. We further hypothesize that collaboration with other agents
is key to achieving group resilience; collaborating agents adapt better to
environmental perturbations in multi-agent reinforcement learning (MARL)
settings. We test our hypothesis empirically by evaluating different
collaboration protocols and examining their effect on group resilience. Our
experiments show that all the examined collaborative approaches achieve higher
group resilience than their non-collaborative counterparts.

</details>


### [76] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: A visually augmented cognitive reappraisal method using AI-generated images reduces negative affect more effectively than traditional verbal approaches.


<details>
  <summary>Details</summary>
Motivation: Standard cognitive reappraisal methods are often abstract and verbally demanding, limiting effectiveness for individuals with trauma or depression.

Method: Integrates text-to-image diffusion models to transform spoken reappraisals into supportive visualizations, tested in a within-subject experiment.

Result: AI-assisted reappraisal significantly reduced negative affect, with sentiment alignment between reappraisals and images enhancing efficacy.

Conclusion: Generative visual input supports cognitive reappraisal, offering new therapeutic and affective computing applications.

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [77] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: The paper introduces GALDS, a Graph-Autoencoder-based Latent Dynamics Surrogate model, to efficiently simulate material transport in neural trees, achieving high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Accurate simulation of material transport in neuron trees is computationally challenging due to their complex geometries, necessitating optimized methods.

Method: GALDS uses a graph autoencoder to encode network geometry, velocity fields, and concentration profiles into latent representations, predicting dynamics via a Neural ODE-inspired model.

Result: GALDS achieves a mean relative error of 3%, max error <8%, and a 10x speed improvement over prior methods on unseen and abnormal transport cases.

Conclusion: GALDS offers an efficient, accurate solution for simulating material transport in neural trees, addressing computational challenges of traditional approaches.

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [78] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: A domain-adaptive small language model (SLM) with encoder-decoder architecture is proposed for predicting hierarchical tax codes, outperforming flat classifiers and other architectures.


<details>
  <summary>Details</summary>
Motivation: Accurate tax code prediction is crucial for compliance, avoiding penalties, and handling unstructured product/service data.

Method: Uses an encoder-decoder SLM to capture hierarchical dependencies in tax codes, tested on HSN and other tax systems.

Result: Encoder-decoder SLMs outperform flat classifiers, decoder-only, and encoder-only architectures in tax code prediction.

Conclusion: The approach is scalable to other tax codes and demonstrates the potential of SLMs in structured sequence generation tasks.

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [79] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: SiGMoID is a simulation-based generative model for inferring nonlinear dynamic systems from noisy, sparse, or partial data, combining physics-informed neural networks and Wasserstein GANs.


<details>
  <summary>Details</summary>
Motivation: Inferring nonlinear dynamic models from imperfect data is challenging, especially with noise, sparsity, or partial observability.

Method: Integrates physics-informed neural networks (for ODE solving) and Wasserstein GANs (for parameter estimation from noisy data).

Result: SiGMoID effectively quantifies noise, estimates parameters, and infers unobserved components, validated in realistic experiments.

Conclusion: SiGMoID is broadly applicable for discovering full system dynamics in scientific and engineered systems.

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [80] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: The paper explores adversarial unlearning, where malicious unlearn requests degrade model performance, and proposes a method to protect models from such effects.


<details>
  <summary>Details</summary>
Motivation: AI models require unlearning for legal compliance (e.g., GDPR) and to address issues like toxic content or data shifts, but unlearning can harm performance.

Method: Investigates adversarial unlearning, analyzing factors like model backbone and data selection strategies, and introduces a protection method.

Result: Demonstrates that adversarial unlearning can significantly degrade performance, depending on model and data factors.

Conclusion: Proposes a novel method to safeguard model performance against adversarial and spontaneous unlearning effects.

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [81] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: The paper addresses forecasting inventory drain and shipping costs for RL-based regional inventory planning, proposing a probabilistic model and validation scheme for robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of inventory drain and shipping costs is crucial for RL-driven inventory planning, but existing methods are non-differentiable and inefficient.

Method: A probabilistic forecasting model is developed to predict joint distributions of drain and costs, conditioned on inventory and demand, with a validation scheme for RL robustness.

Result: Preliminary results show the model's accuracy in in-distribution scenarios.

Conclusion: The proposed model and validation approach offer a scalable and differentiable solution for RL-based inventory planning.

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [82] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: The paper introduces class-proportional coreset selection methods to address class-difficulty separability, improving data efficiency and performance in high-stakes domains like security and medical imaging.


<details>
  <summary>Details</summary>
Motivation: Existing coreset methods assume class-wise homogeneity in data difficulty, neglecting variations across classes, leading to performance degradation in real-world applications.

Method: The authors propose the Class Difficulty Separability Coefficient (CDSC) and class-proportional variants of sampling strategies, evaluated on diverse datasets.

Result: Class-proportional methods outperform class-agnostic ones, e.g., CCS-CP shows minimal performance drops (e.g., 2.58% accuracy loss) at 99% pruning, while baselines degrade significantly.

Conclusion: Modeling class-difficulty separability enhances data pruning effectiveness, robustness, and generalizability, especially in noisy or imbalanced datasets.

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [83] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: The paper explores diffusion decoders for peptide de novo sequencing, finding they improve amino acid recall over traditional autoregressive methods, though challenges remain.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning methods like Casanovo suffer from cascading errors and inefficient use of high-confidence regions in peptide sequencing.

Method: The study tests three diffusion decoder designs, knapsack beam search, and various loss functions, comparing them to autoregressive decoders.

Result: The best diffusion decoder with DINOISER loss improved amino acid recall by 0.373 over Casanovo, though peptide precision/recall remained 0.

Conclusion: Diffusion decoders show promise for enhancing sensitivity and advancing peptide de novo sequencing, despite mixed results.

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [84] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: The paper reviews ML applications, especially Physics-Informed Neural Networks (PINNs), for improving semiconductor film deposition processes, identifying trends, gaps, and proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in semiconductor film deposition (e.g., control, quality) using ML, particularly PINNs, for better precision and efficiency.

Method: Thematic analysis of ML applications in film deposition, focusing on PINNs, their integration with physical laws, and neural network architectures.

Result: Identified key trends, limitations, and gaps in current ML methodologies, proposing novel research directions for PINNs in film deposition.

Conclusion: PINNs offer significant potential to enhance semiconductor manufacturing; future research should focus on integrating physics-informed ML to improve precision and scalability.

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [85] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: StellarF introduces a parameter-efficient model for stellar flare forecasting using LoRA and Adapter techniques, achieving state-of-the-art results on Kepler and TESS datasets.


<details>
  <summary>Details</summary>
Motivation: The field lacks large-scale predictive models and suffers from sparse flare event data, hindering progress in stellar activity research.

Method: StellarF combines a flare statistical module with historical records for multi-scale pattern recognition, using LoRA and Adapter for efficient learning.

Result: StellarF outperforms existing methods on self-constructed datasets from Kepler and TESS light curves.

Conclusion: The model provides a novel framework for advancing astrophysical research and cross-disciplinary applications.

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [86] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: ClusterEnv is a modular, learner-agnostic framework for distributed RL, decoupling simulation and training via the DETACH pattern and addressing policy staleness with AAPS.


<details>
  <summary>Details</summary>
Motivation: Existing RL frameworks lack modularity by entangling simulation, learning, and orchestration, limiting reusability.

Method: ClusterEnv uses the DETACH pattern to offload simulation to remote workers and introduces AAPS for efficient policy synchronization.

Result: AAPS reduces synchronization overhead while maintaining performance, achieving high sample efficiency with fewer weight updates.

Conclusion: ClusterEnv offers a lightweight, flexible solution for distributed RL, compatible with existing pipelines and requiring minimal code changes.

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [87] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: The paper highlights how reward functions in reinforcement learning often mix terminal and instrumental goals, leading to misalignment and poor performance when optimized.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of reward functions in reinforcement learning being imperfect due to the conflation of terminal (end goals) and instrumental (means to an end) goals, which can result in severe misalignment.

Method: The authors formulate a simple example demonstrating how slight conflation of instrumental and terminal goals leads to misalignment, distilling key properties of environments sensitive to this issue.

Result: The results show that optimizing misspecified reward functions results in poor performance when evaluated against the true reward function.

Conclusion: The conclusion emphasizes the sensitivity of reinforcement learning to goal conflation and discusses implications for reward learning and real-world environments.

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [88] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: The paper proposes a VAE-based framework for generating imperceptible adversarial examples on tabular data, addressing challenges like heterogeneous features and distributional consistency.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks on tabular data are challenging due to mixed categorical/numerical features and lack of intuitive similarity metrics. Traditional methods often produce detectable examples.

Method: A mixed-input VAE integrates categorical embeddings and numerical features into a unified latent manifold for perturbations that preserve statistical consistency.

Result: The method achieves lower outlier rates and more consistent performance across datasets and models, with IDSR as a key metric.

Conclusion: VAE-based attacks are effective for realistic adversarial examples on tabular data, emphasizing on-manifold perturbations and reconstruction quality.

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [89] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: AdaMuon is an adaptive learning-rate framework based on Muon, enhancing it with per-parameter second-moment modulation and RMS-aligned rescaling for better convergence and stability.


<details>
  <summary>Details</summary>
Motivation: To improve upon Muon's efficiency gains over AdamW in large-scale model training by introducing adaptive features without added tuning complexity.

Method: AdaMuon adds two modules to Muon: (1) per-parameter second-moment modulation for update-level adaptivity, and (2) RMS-aligned rescaling to regulate update magnitude.

Result: AdaMuon outperforms Muon in convergence speed and stability across various model scales and learning-rate regimes.

Conclusion: AdaMuon enhances Muon's performance without extra tuning, making it easy to integrate into existing pipelines.

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [90] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: The study uses machine learning to predict turbulent kinetic energy (TKE) from temperature data in a fire environment, achieving accurate results despite weak correlations.


<details>
  <summary>Details</summary>
Motivation: To explore relationships between temperature and TKE in fire environments, aiding fire research and management.

Method: Employed machine learning models (Deep Neural Networks, Random Forest, Gradient Boosting, Gaussian Process) on concurrent temperature and turbulence data.

Result: Successful TKE predictions, especially with regression models, revealed new temperature-airflow relationships.

Conclusion: Machine learning can advance fire research by analyzing complex datasets and improving fire and smoke models.

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [91] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM is a novel PTQ method for LLMs that incorporates first-order gradient terms to improve quantization error compensation, outperforming GPTQ and reducing perplexity and accuracy gaps significantly.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods assume negligible first-order terms in quantization error, but accumulated deviations make this flawed. FOEM addresses this by explicitly including first-order gradients.

Method: FOEM approximates gradients by computing differences between latent and full-precision weights, avoiding costly backpropagation. It uses precomputed Cholesky factors for efficient Hessian submatrix inversion.

Result: FOEM reduces perplexity by 89.6% for Llama3-8B and improves MMLU accuracy from 51.7% to 74.9% for Llama3-70B, nearing full-precision performance (78.6%).

Conclusion: FOEM outperforms GPTQ, integrates with advanced techniques, and significantly narrows accuracy gaps, making it a robust PTQ solution for LLMs.

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [92] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: The paper introduces REPPO, an on-policy algorithm combining pathwise policy gradients' efficiency with on-policy learning's simplicity, reducing variance and improving stability.


<details>
  <summary>Details</summary>
Motivation: Address the high variance of score-function policy gradients and the unreliability of pathwise policy gradients without accurate value functions.

Method: Proposes REPPO, which trains Q-value models purely from on-policy data, balancing exploration and stable updates.

Result: REPPO shows strong performance, reduced sample needs, faster training, and high robustness in benchmarks.

Conclusion: REPPO effectively merges pathwise policy gradients' benefits with on-policy learning, offering a practical and efficient solution.

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [93] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: GATE, a novel GNN-based framework, improves indoor localization by addressing non-Euclidean RSS noise and device heterogeneity, outperforming existing methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: Indoor localization is vital for smart environments, but current DL models fail to handle non-Euclidean RSS noise and device heterogeneity, limiting accuracy.

Method: GATE uses adaptive graph representations, AHV for message passing, MDHV to mitigate GNN blind spots, and RTEC for dynamic graph adaptation.

Result: GATE reduces mean localization errors by 1.6x to 4.72x and worst-case errors by 1.85x to 4.57x compared to state-of-the-art methods.

Conclusion: GATE effectively addresses challenges in indoor localization, offering superior accuracy and robustness across diverse environments and devices.

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [94] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: The paper introduces a mathematical distance metric for MILP instances to improve similarity comparison, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: MILP lacks a structured way to compare instances, limiting solver guidance and evaluation of heterogeneity. Existing metrics are imprecise or rely on labeled data.

Method: Proposes a distance metric derived from MILP formulations, discretizing components and using Earth mover's distance for constraint comparisons. Evaluates exact and greedy variants.

Result: The greedy variant is nearly as accurate as the exact version but 200x faster. It outperforms non-learned baselines and rivals supervised classifiers.

Conclusion: The new metric effectively compares MILP instances, enhancing solver guidance and instance set evaluation without needing labeled data.

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [95] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: The paper proposes parameter-efficient finetuning methods (LoRA and adapters) for log anomaly detection, outperforming traditional approaches with significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Log anomaly detection is challenging due to large, complex log sequences, necessitating efficient methods for system maintenance.

Method: Uses LoRA and adapter-based finetuning on tiny LLMs, tested on the Thunderbird dataset.

Result: LoRA achieves 97.76%-98.83% accuracy, a 18-19% improvement over LogBert's 79.37%.

Conclusion: Parameter-efficient finetuning, especially LoRA, is highly effective for log anomaly detection.

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [96] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian online change point detection (BOCPD) method using RL critic networks to detect subtle drift-evasive spoofing attacks on UAVs, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: UAVs rely on GNSS for navigation, making them vulnerable to stealthy spoofing attacks that evade conventional detection. Rapid and accurate detection is crucial for resilience.

Method: A BOCPD approach monitors temporal shifts in RL critic network value estimates to detect behavioral deviations in UAV navigation.

Result: The method outperforms traditional GNSS spoofing detectors and other temporal frameworks, achieving higher accuracy and lower error rates.

Conclusion: The proposed temporal value-based framework enhances UAV resilience against drift-evasive spoofing attacks by enabling rapid detection and response.

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [97] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: The paper proposes GRNGC, a Gradient Regularization-based Neural Granger Causality model, to address computational inefficiency and complexity limitations in existing neural network-based Granger causality methods.


<details>
  <summary>Details</summary>
Motivation: Existing neural network-based Granger causality models are computationally expensive and struggle with capturing complex interactions due to component-wise architectures and sparsity-inducing penalties.

Method: GRNGC uses a single time series prediction model and applies L1 regularization to gradients between input and output to infer causality. It is compatible with various architectures like KAN, MLP, and LSTM.

Result: GRNGC outperforms baselines in numerical simulations (DREAM, Lorenz-96, fMRI BOLD, CausalTime) and reduces computational costs. Real-world datasets (DNA, Yeast, HeLa, bladder cancer) confirm its effectiveness in gene regulatory network reconstruction.

Conclusion: GRNGC offers a flexible, efficient, and accurate alternative for Granger causality analysis, overcoming key limitations of existing methods.

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [98] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: A review of Mixture-of-Experts (MoE) in large language models, highlighting its performance benefits, architectural designs, and applications, while addressing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of MoE architectures in enhancing large language models efficiently, focusing on their theoretical and practical advantages.

Method: Systematic analysis of MoE's theoretical foundations, core designs, expert gating, routing mechanisms, hierarchical/sparse configurations, meta-learning, and real-world applications.

Result: MoE offers superior model capacity, task-specific performance, and scalable efficiency, but requires expert diversity, calibration, and reliable inference.

Conclusion: The review identifies MoE's strengths and challenges, providing a roadmap for future research and innovation in its architecture and applications.

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [99] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: A communication-efficient federated learning scheme using low-rank approximation and quantization to reduce network load while maintaining model accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of high communication overhead in federated learning due to frequent model updates.

Method: Proposes using low-rank approximation of neural network gradients and quantization to minimize data exchange.

Result: Significantly reduces network load with minimal impact on model accuracy.

Conclusion: The scheme effectively balances communication efficiency and model performance in federated learning.

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [100] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: A machine learning framework combining classification and regression models for heart disease diagnosis and risk prediction, using SMOTE for class imbalance and achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Heart disease is a global health issue with traditional diagnostic methods often failing, necessitating more accurate and efficient solutions.

Method: Proposed framework uses classification (e.g., Random Forest) and regression (e.g., Linear Regression) models, SMOTE for data balancing, and Explainable AI for interpretability.

Result: Random Forest achieved 97.2% accuracy on real data; Linear Regression had R2 of 0.992. Synthetic data improved performance further.

Conclusion: Machine learning can revolutionize heart disease diagnosis and risk prediction, aiding early intervention and clinical decisions.

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [101] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: The paper addresses privacy and quality concerns in online collaborative medical prediction platforms by proposing a privacy-preserving mechanism integrated into a one-shot distributed learning framework, ensuring optimal performance under privacy constraints.


<details>
  <summary>Details</summary>
Motivation: Growing privacy concerns and low prediction quality deter patient and doctor participation in online collaborative medical prediction platforms.

Method: The paper clarifies privacy attacks (attribute and model extraction attacks), specifies privacy principles, and proposes a privacy-preserving mechanism within a one-shot distributed learning framework.

Result: The framework theoretically achieves optimal prediction performance under privacy requirements, validated by simulations and real-world data.

Conclusion: The proposed solution effectively balances privacy and performance, enhancing trust and utility in collaborative medical prediction.

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [102] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: The paper investigates whether equal-magnitude data ensures global convergence of gradient descent (GD) in logistic regression, proving it works in 1D but not in higher dimensions.


<details>
  <summary>Details</summary>
Motivation: To understand if equal-magnitude data can prevent cycling behaviour in GD for logistic regression, especially under large step sizes.

Method: Analyzes GD behaviour in logistic regression, focusing on datasets with equal-magnitude data, and proves results for 1D and higher dimensions.

Result: Equal-magnitude data ensures global convergence in 1D, but cycling can still occur in higher dimensions.

Conclusion: Further research is needed to quantify cycling frequency in real datasets and identify sufficient conditions for global convergence with large step sizes.

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [103] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: A novel model combines generative and discriminative approaches for CTR prediction, improving accuracy through a two-stage training process and validated via experiments and A/B testing.


<details>
  <summary>Details</summary>
Motivation: Current CTR prediction models rely on discriminative methods, but generative models like GPT show potential for enhanced expressive power.

Method: Two-stage training: generative pre-training for next-item prediction, followed by fine-tuning within a discriminative CTR framework.

Result: Improved CTR prediction accuracy, validated by experiments and online A/B testing; deployed on a major e-commerce platform.

Conclusion: The hybrid generative-discriminative model effectively enhances CTR prediction, with plans to release code and dataset.

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [104] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: LyAm is a novel optimizer combining Adam's adaptive moment estimation with Lyapunov stability to improve deep learning convergence and robustness.


<details>
  <summary>Details</summary>
Motivation: Address noisy gradients and unstable convergence in deep neural networks, particularly for computer vision tasks.

Method: Integrates Adam with Lyapunov-based stability mechanisms to dynamically adjust learning rates.

Result: Outperforms state-of-the-art optimizers in accuracy, convergence speed, and stability on datasets like CIFAR-10 and CIFAR-100.

Conclusion: LyAm is a robust optimizer for deep learning, supported by theoretical guarantees and empirical results.

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [105] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: A novel DRL method using the Neyman-Rubin framework improves sample efficiency by reducing replay buffer size and boosting rewards.


<details>
  <summary>Details</summary>
Motivation: DRL agents demand high computational resources due to large training steps and replay buffers. This work aims to enhance efficiency.

Method: Leverages the Neyman-Rubin potential outcomes framework to bound factual loss, reusing past value network outputs in the replay buffer.

Result: Achieves up to 2,427% higher reward ratio and reduces replay buffer size by 96%, improving efficiency.

Conclusion: The method significantly enhances DRL performance with minimal cost, making it practical for resource-intensive tasks.

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [106] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: The paper analyzes the convergence of SGD for smooth convex objectives in the interpolation regime, providing improved rates for the last iterate under specific conditions.


<details>
  <summary>Details</summary>
Motivation: Understanding the behavior of SGD in over-parameterized models and its implications for continual learning and solving linear systems.

Method: Analyzes SGD with large stepsizes on β-smooth convex loss functions, deriving expected excess risk bounds for the last iterate.

Result: Establishes improved convergence rates, including a near-optimal rate for well-tuned stepsizes and a better rate when noise at optimum is zero.

Conclusion: The results extend and improve upon prior work, offering tighter bounds for SGD convergence in the interpolation regime.

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [107] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: A framework for training a Fairness Reward Model (FRM) is proposed to mitigate bias in LLM reasoning for high-stakes decisions, improving fairness without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the amplification of unfair bias in LLM reasoning for high-stakes decisions like bail or loans.

Method: Training a generalizable FRM to score fairness in LLM reasoning, down-weighting biased trajectories.

Result: The FRM improves fairness across tasks (e.g., recidivism prediction, social media moderation) without additional fine-tuning, matching or surpassing baseline accuracy.

Conclusion: The FRM enables trustworthy LLM use in high-stakes decision-making by balancing fairness and accuracy.

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [108] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: The paper challenges the independence assumption in neurosymbolic (NeSy) predictors, showing it limits uncertainty modeling and causes reasoning shortcuts.


<details>
  <summary>Details</summary>
Motivation: To address skepticism about the impact of the independence assumption in NeSy systems and its limitations in modeling uncertainty.

Method: Formal analysis demonstrating that independence among symbolic concepts prevents representation of uncertainty over certain combinations.

Result: Independence assumption leads to reasoning shortcuts, where models predict correctly but for wrong reasons.

Conclusion: The independence assumption in NeSy predictors is problematic as it hinders proper uncertainty modeling and reasoning.

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [109] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: A novel RL method trains neural networks layer-wise using local signals during forward passes, eliminating backpropagation and activation storage, achieving competitive performance and improved stability.


<details>
  <summary>Details</summary>
Motivation: Backpropagation in RL requires storing activations and suffers from vanishing/exploding gradients, degrading learning performance and stability.

Method: Proposes layer-wise training with local losses based on multi-dimensional scaling, optionally guided by rewards, enabling forward-pass-only updates.

Result: Competitive performance vs. BP-based methods, enhanced stability, and improved performance in challenging environments.

Conclusion: The method offers a viable, efficient alternative to BP in RL, improving stability and performance without backward passes.

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [110] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: The paper proposes a neurally plausible continual learning model combining VAEs and MHNs to mimic human memory functions, achieving ~90% accuracy on Split-MNIST with reduced forgetting.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in neural networks by emulating human memory mechanisms (pattern separation and completion) described in CLS theory.

Method: Combines variational autoencoders (VAEs) for pattern completion and Modern Hopfield networks (MHNs) for pattern separation into a continual learning model.

Result: Achieves ~90% accuracy on Split-MNIST, significantly reducing forgetting, with VAEs and MHNs functionally dissociated.

Conclusion: The model provides a scalable template for memory consolidation and continual learning in biological and artificial systems.

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [111] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: R-MTGB is a robust multi-task gradient boosting framework that handles outlier tasks while improving performance across all tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world MTL often includes outlier tasks that degrade performance, requiring a method to handle task heterogeneity.

Method: R-MTGB uses three blocks: learning shared patterns, partitioning tasks into outliers/non-outliers, and fine-tuning task-specific predictors.

Result: R-MTGB isolates outliers, transfers knowledge, and reduces prediction errors, achieving overall performance gains.

Conclusion: R-MTGB is robust, adaptable, and reliable in challenging MTL environments.

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [112] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: The study evaluates activation functions for fNIRS classification, finding symmetrical functions like Tanh and Abs(x) outperform ReLU, with MAF further supporting symmetry's role in performance.


<details>
  <summary>Details</summary>
Motivation: Activation functions' impact on DL performance in fNIRS is underexplored, especially given challenges like nonlinearity and low SNR in fNIRS data.

Method: Tested conventional and field-specific activation functions on fNIRSNet, AbsoluteNet, MDNN, and shallowConvNet using standardized preprocessing and training on an auditory task dataset.

Result: Symmetrical activation functions (Tanh, Abs(x)) outperformed ReLU, with MAF analysis reinforcing symmetry's effectiveness.

Conclusion: Proper activation function selection, aligned with fNIRS signal characteristics, is crucial for DL performance in this domain.

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [113] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: DAIF introduces a real-time data augmentation method for iTransformer to address its limitations in capturing temporal interdependency and noise from nonsignificant variable correlations.


<details>
  <summary>Details</summary>
Motivation: iTransformer's inverted framework for MTS forecasting diminishes temporal interdependency and introduces noise in nonsignificant variable correlations, prompting the need for improvement.

Method: Proposes DAIF with two strategies: Frequency Filtering and Cross-variation Patching, tailored for the inverted framework.

Result: Experiments show DAIF effectively enhances iTransformer's performance across multiple datasets.

Conclusion: DAIF successfully addresses iTransformer's limitations, proving its utility in MTS forecasting.

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [114] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: LRMR, an LLM-driven framework, improves lymph node metastasis assessment in rectal cancer by combining multimodal analysis and relational ranking, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional MRI and black-box AI models lack interpretability and patient-level context for lymph node metastasis evaluation.

Method: LRMR uses a two-stage LLM approach: multimodal analysis of LN images for structured reports and pairwise comparisons for risk ranking.

Result: LRMR achieved an AUC of 0.7917 and F1-score of 0.7200, outperforming deep learning baselines like ResNet50.

Conclusion: The two-stage LLM framework provides an interpretable and effective paradigm for lymph node metastasis assessment.

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [115] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM introduces a hierarchical diffusion policy framework for efficient LoRA adaptation in edge-based LLMs, combining PPO and DDIM to optimize rank configurations and reduce transmission costs.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in existing LoRA approaches for edge-based LLMs due to fixed rank configurations and high transmission costs.

Method: Uses a PPO agent for coarse-grained decisions and DDIM for refinement, optimizing rank vectors adaptively.

Result: Improves fine-tuning performance and reduces transmission costs under varying signal-to-noise ratios.

Conclusion: AirLLM demonstrates scalable and efficient remote fine-tuning via reinforcement-driven, diffusion-refined rank adaptation.

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [116] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: The paper explores federated learning (FL) for forecasting non-linear, non-stationary IoT time-series data, comparing it to centralized methods and evaluating detrending techniques.


<details>
  <summary>Details</summary>
Motivation: Centralized data analysis for IoT introduces delays and costs; FL offers a distributed alternative. Non-linear data variations challenge prediction accuracy.

Method: Synthetic and real-world datasets with non-linear distributions were used to train LSTM models in FL and centralized setups, with detrending techniques applied.

Result: FL underperforms centralized methods for non-linear data but improves with detrending.

Conclusion: Detrending enhances FL performance for non-linear time-series data, though FL still lags behind centralized approaches.

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [117] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: The paper explores extensions of the RL-based TractOracle-RL framework for tractography, introducing Iterative Reward Training (IRT) to improve accuracy and anatomical validity.


<details>
  <summary>Details</summary>
Motivation: To enhance tractography by integrating RL advancements and reducing false positives with anatomical priors.

Method: Extends TractOracle-RL with RL techniques, evaluates on five datasets, and introduces IRT for iterative reward refinement.

Result: RL methods with oracle feedback outperform traditional techniques in accuracy and anatomical validity.

Conclusion: Combining RL with oracle guidance yields robust tractography, with IRT further enhancing performance.

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [118] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: A novel parametric activation function using Wendland RBFs is introduced for deep neural networks, offering tunable locality, improved gradients, and stability, with competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional activation functions (ReLU, sigmoid, tanh) by leveraging Wendland RBFs' compact support and smoothness.

Method: Enhanced Wendland activation combines standard Wendland RBFs with linear and exponential terms, analyzed theoretically and tested on synthetic tasks and benchmarks.

Result: Superior accuracy in regression tasks and competitive performance on MNIST/Fashion-MNIST, with computational efficiency.

Conclusion: Wendland activations bridge RBF theory and deep learning, improving generalization and suggesting future hybrid architectures.

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [119] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: LangevinFlow is a physics-inspired VAE using Langevin dynamics for modeling neural population dynamics, outperforming baselines in synthetic and real datasets.


<details>
  <summary>Details</summary>
Motivation: To capture intrinsic and external dynamics in neural systems, leveraging physical priors like inertia and stochastic forces.

Method: Uses a sequential VAE with underdamped Langevin dynamics, a recurrent encoder, Transformer decoder, and oscillator-based potential function.

Result: Outperforms baselines on synthetic Lorenz attractor data and NLB datasets, achieving high accuracy in firing rates and behavioral metrics.

Conclusion: LangevinFlow is a flexible, high-performing framework for neural dynamics modeling, inspired by physics.

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [120] [A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge](https://arxiv.org/abs/2507.10913)
*Shuangyao Huang,Haibo Zhang,Zhiyi Huang*

Main category: cs.MA

TL;DR: A MARL framework for UAV swarm collision avoidance uses domain knowledge-driven rewards from image processing, avoiding complex interactions and credit assignment.


<details>
  <summary>Details</summary>
Motivation: To simplify cooperative collision avoidance in UAV swarms by leveraging domain knowledge and minimizing agent interactions.

Method: Uses image processing-inspired rewards to model obstacles as maxima, ensuring smooth, energy-efficient paths without collisions.

Result: The framework outperforms state-of-the-art MARL algorithms and adapts to complex environments.

Conclusion: The approach effectively avoids collisions and reduces training complexity for large UAV swarms.

Abstract: This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.

</details>
