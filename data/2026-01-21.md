<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.MA](#cs.MA) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?](https://arxiv.org/abs/2601.11559)
*Zilal Eiz AlDin,John Wu,Jeffrey Paul Fung,Jennifer King,Mya Watts,Lauren ONeill,Adam Richard Cross,Jimeng Sun*

Main category: cs.AI

TL;DR: Researchers created MIMIC-RD benchmark by mapping clinical text to Orphanet to address limitations of existing rare disease evaluation methods, finding current LLMs perform poorly on real-world rare disease differential diagnosis.


<details>
  <summary>Details</summary>
Motivation: Rare disease diagnosis is challenging (affects 1 in 10 Americans). Existing LLM evaluation approaches have critical limitations: they use idealized clinical case studies that don't capture real-world complexity, or rely on ICD codes that undercount rare diseases since many lack direct mappings to comprehensive databases like Orphanet.

Method: Created MIMIC-RD benchmark by directly mapping clinical text entities to Orphanet. Used an initial LLM-based mining process followed by validation from four medical annotators to confirm identified rare diseases. Evaluated various models on a dataset of 145 patients.

Result: Current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting substantial gap between existing capabilities and clinical needs.

Conclusion: The study demonstrates the inadequacy of current LLMs for rare disease diagnosis and outlines several future steps toward improving differential diagnosis of rare diseases.

Abstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.

</details>


### [2] [A Mind Cannot Be Smeared Across Time](https://arxiv.org/abs/2601.11620)
*Michael Timothy Bennett*

Main category: cs.AI

TL;DR: Machines need simultaneous computation, not just sequential, to support consciousness. Formal analysis shows temporal smearing doesn't preserve conjunction. Brain evidence favors StrongSync requiring co-instantiation.


<details>
  <summary>Details</summary>
Motivation: Most AI systems use sequential/time-multiplexed updates, while conscious experience appears unified and simultaneous. Need to determine whether machines can be conscious based on when they compute, not just what they compute.

Method: Formal analysis using Stack Theory augmented with algebraic laws relating time-window constraints to conjunction, introducing temporal semantics over windowed trajectories τ^{Δ,s}, distinguishing StrongSync vs WeakSync postulates, and formalizing concurrency-capacity metrics.

Result: Existential temporal realization ◇_Δ does not preserve conjunction - systems can realize all ingredients of experience across time without instantiating the conjunction itself. Neurophysiological evidence suggests consciousness depends on phase synchrony, making WeakSync implausible. StrongSync requires objective co-instantiation.

Conclusion: Consciousness attribution requires architectural inspection of hardware concurrency capacity, not just functional performance. Software consciousness on strictly sequential substrates is impossible when consciousness requires simultaneous contributions from multiple parts.

Abstract: Whether machines can be conscious depends not only on what they compute, but \emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.

</details>


### [3] [Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models](https://arxiv.org/abs/2601.11622)
*Hassan Ugail,Newton Howard*

Main category: cs.AI

TL;DR: Neuroscience-inspired dynamical metrics reveal distinct temporal organization in LLM activations across different functional regimes, with structured reasoning showing elevated dynamical complexity compared to repetitive, noisy, or perturbed conditions.


<details>
  <summary>Details</summary>
Motivation: Current interpretability approaches for large language models focus on static representations or causal interventions, leaving temporal dynamics largely unexplored. The paper aims to adapt neuroscience concepts of temporal integration and metastability to understand how transformer models organize their internal computations over time during text generation.

Method: Developed a composite dynamical metric computed from activation time-series during autoregressive generation. Applied this metric to GPT-2-medium across five experimental conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Evaluated robustness through layer selection, channel subsampling, and random seeds variations, with statistical analysis using one-way ANOVA and effect size calculations.

Result: Structured reasoning consistently exhibited elevated dynamical metric compared to repetitive, noisy, and perturbed regimes. Statistically significant differences were confirmed through one-way ANOVA with large effect sizes in key comparisons. Results were robust to variations in layer selection, channel subsampling, and random seeds.

Conclusion: Neuroscience-inspired dynamical metrics can reliably characterize differences in computational organization across functional regimes in LLMs. The proposed metric captures formal dynamical properties but does not imply subjective experience, providing an objective approach to understanding temporal organization in transformer models.

Abstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.

</details>


### [4] [Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance](https://arxiv.org/abs/2601.11625)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: This paper proposes using token-level attribution changes (explanation drift) during fine-tuning to monitor decision evidence evolution and identify stable moments, such as the Reasoning Stabilization Point (RSP).


<details>
  <summary>Details</summary>
Motivation: To enable training-time interpretability by tracking how evidence shifts during fine-tuning of language models, as validation accuracy alone may not reveal reliance on shortcuts or unstable evidence.

Method: Define explanation drift as epoch-to-epoch change in normalized token attributions on a fixed probe set, compute RSP from within-run drift dynamics requiring no out-of-distribution tuning, and test across lightweight transformer classifiers and tasks.

Result: Drift collapses into a stable low regime early in training, while validation accuracy changes minimally; attribution dynamics expose reliance on shortcuts like label-correlated trigger tokens, even with competitive accuracy.

Conclusion: Explanation drift offers a simple, low-cost diagnostic for monitoring evidence evolution and selecting stable-evidence checkpoints during fine-tuning.

Abstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.

</details>


### [5] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: The paper explores the shift to Agentic AI, where LLMs act as autonomous systems with perception, reasoning, planning, and action, proposing a taxonomy to classify agent architectures, environments, and challenges.


<details>
  <summary>Details</summary>
Motivation: As AI moves from text generation to agentic systems with autonomous behavior, there's a growing variety of designs (from single-loop agents to hierarchical multi-agent systems) that make the landscape complex and difficult to navigate. This paper aims to address this by investigating architectures and creating a unified framework.

Method: The authors propose a taxonomy breaking agents into components: Perception, Brain, Planning, Action, Tool Use, and Collaboration. They use this taxonomy to analyze the transition from linear reasoning to native inference time reasoning models and from fixed API calls to open standards like MCP and Native Computer Use. They also group agent environments (e.g., digital OS, robotics) and review evaluation practices.

Result: A unified taxonomy is developed to classify agent architectures, environments, and evaluation methods, highlighting key trends such as the move toward open standards and native reasoning. Open challenges like hallucination in action and infinite loops are identified.

Conclusion: The paper provides a structured framework to navigate the evolving field of Agentic AI, emphasizing the need for future research to address challenges and build more robust, reliable autonomous systems.

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [6] [PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement](https://arxiv.org/abs/2601.11747)
*Huaxiaoyue Wang,Sunav Choudhary,Franck Dernoncourt,Yu Shen,Stefano Petrangeli*

Main category: cs.AI

TL;DR: PRISM leverages design data to improve graphic designs based on natural language instructions by building a knowledge base from real-world designs.


<details>
  <summary>Details</summary>
Motivation: Graphic design exploration is time-consuming for non-experts, and VLMs' general style knowledge often misaligns with domain-specific design principles.

Method: PRISM constructs a design knowledge base through three stages: clustering high-variance designs, summarizing clusters into actionable knowledge, and retrieving relevant knowledge during inference for style-aware improvement.

Result: On the Crello dataset, PRISM achieves an average rank of 1.49 in style alignment, outperforming baselines, and user studies show it is consistently preferred by designers.

Conclusion: PRISM effectively enhances stylistic design modifications by utilizing design data, bridging the gap between general VLM knowledge and specific domain requirements.

Abstract: Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.

</details>


### [7] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: This paper extends a vulnerability scoring system (TIVS) to TIVS-O by adding an Observability Score Ratio and semantic caching to evaluate prompt injection defenses in multi-agent LLM systems, balancing security and transparency.


<details>
  <summary>Details</summary>
Motivation: Prompt injection is a critical obstacle for safe large language model deployment, especially in multi-agent settings where attacks can propagate, necessitating improvements in defense evaluation frameworks.

Method: The study develops TIVS-O with semantic similarity-based caching and Nested Learning architecture, testing 301 synthetic prompts from 10 attack families using a pipeline with Continuum Memory Systems and security analysis agents.

Result: The system achieved secure responses with zero high-risk breaches, reduced LLM calls by 41.6%, and lowered latency, energy, and emissions, with TIVS-O configurations optimizing mitigation-transparency trade-offs.

Conclusion: Observability-aware evaluation reveals non-monotonic effects in multi-agent pipelines, enabling memory-augmented agents to enhance security, performance, and sustainability without modifying model weights for green LLM deployments.

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [8] [Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles](https://arxiv.org/abs/2601.11781)
*Dawood Wasif,Terrence J. Moore,Seunghyun Yoon,Hyuk Lim,Dan Dongseong Kim,Frederica F. Nelson,Jin-Hee Cho*

Main category: cs.AI

TL;DR: RAIL is a risk-aware human-in-the-loop framework that fuses runtime signals into risk scores, adapts control via intelligent shielding, and improves learning through risk-prioritized replay, achieving strong safety and performance against rare events and cyber attacks.


<details>
  <summary>Details</summary>
Motivation: To ensure autonomous vehicles remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during operation.

Method: RAIL uses an Intrusion Risk Score (IRS) calculated from three cues via weighted Noisy-OR fusion. It features a contextual bandit for shield arbitration, blends actions based on risk levels, and employs SAC with risk-prioritized replay and dual rewards.

Result: On MetaDrive: TR=360.65, TSR=0.85, TSV=0.75, DR=0.0027 with 29.07 training safety violations. Under CAN/LiDAR attacks: SR=0.68/0.80, DRA=0.37/0.03, ASR=0.34/0.11. In CARLA: TR=1609.70, TSR=0.41 with 8000 steps.

Conclusion: RAIL provides a robust solution for safety-critical autonomous driving that addresses rare scenarios and cyber-physical attacks by integrating human-in-the-loop oversight with adaptive learning and intelligent risk mitigation strategies.

Abstract: Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.

</details>


### [9] [A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation](https://arxiv.org/abs/2601.11792)
*Yifei Sun,Yongan Li,A. K. Qin,Sicheng Hou,Tamas Pflanzner*

Main category: cs.AI

TL;DR: A self-evolving, multi-role collaborative framework with fine-grained difficulty guidance is proposed for innovative math problem generation (IMPG), enhancing innovation while maintaining high correctness.


<details>
  <summary>Details</summary>
Motivation: Existing large language models (LLMs) for math problem generation often lack innovation and exhibit poor discrimination, prompting the need for a more innovative approach.

Method: The framework includes a multi-role collaborative mechanism (sampler, generator, evaluator, state machine, memory), an improved difficulty model with DAPS algorithm, and the HSM3K-CN dataset with multi-stage training (CPT, SFT, GRPO) and self-evolution via knowledge distillation.

Result: Experiments show the method significantly improves innovation of generated problems compared to baseline models while maintaining a high correctness rate.

Conclusion: The proposed framework effectively addresses the lack of innovation in math problem generation, offering a scalable and high-performing solution for intelligent education applications.

Abstract: Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.

</details>


### [10] [TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning](https://arxiv.org/abs/2601.13545)
*Shirin Shahabi,Spencer Graham,Haruna Isah*

Main category: cs.AI

TL;DR: TruthTensor introduces a novel evaluation paradigm for LLMs that moves beyond static benchmarks to measure models as human-imitation systems in socially-grounded, high-entropy environments using live prediction markets and probabilistic scoring.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of language models and AI agents is fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions.

Method: The framework builds on forward-looking, contamination-free tasks anchored to live prediction markets, combines probabilistic scoring, and includes drift-centric diagnostics and explicit robustness checks. It specifies human vs. automated evaluation roles, annotation protocols, and statistical testing procedures for interpretability and replicability.

Result: In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, highlighting the need for multi-axis evaluation.

Conclusion: TruthTensor operationalizes modern evaluation best practices to produce defensible assessments of LLMs in real-world decision contexts, addressing the limitations of traditional benchmarks by measuring models along multiple axes including accuracy, calibration, narrative stability, cost, and resource efficiency.

Abstract: Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration](https://arxiv.org/abs/2601.11556)
*Boyang Wang,Yash Vishe,Xin Xu,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: A new benchmark CSyMR-Bench introduces integrative compositional reasoning for LLMs in symbolic music, coupled with a tool-augmented agent that outperforms baselines by 5-7% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs in symbolic music focus on isolated knowledge or atomic analyses, lacking integrative compositional reasoning to connect musical structures.

Method: Curate a multiple-choice dataset of 126 questions from expert forums and professional exams, and develop a tool-augmented agent framework using music21 library tools for enhanced analysis.

Result: CSyMR-Bench presents a non-trivial challenge across question types, and the tool-augmented agent achieves 5-7% absolute accuracy gains over baselines.

Conclusion: CSyMR-Bench effectively advances symbolic music reasoning by addressing integrative reasoning gaps, with the tool-augmented agent demonstrating superior performance, highlighting the potential of tool-enhanced approaches in this domain.

Abstract: Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.

</details>


### [12] [AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control](https://arxiv.org/abs/2601.11568)
*Quang-Hung Bui,Anh Son Ta*

Main category: cs.LG

TL;DR: AdaFRUGAL is an automated framework that dynamically adjusts hyperparameters for gradient splitting to reduce memory and computational overhead in LLM training, outperforming manual tuning in FRUGAL.


<details>
  <summary>Details</summary>
Motivation: Training LLMs is memory-intensive, and the FRUGAL framework's static hyperparameters require manual tuning, which limits adaptability and effectiveness.

Method: Introduces two dynamic controls: linear decay for subspace ratio (ρ) to reduce memory usage over time, and a loss-aware schedule for update frequency (T) to lower computation.

Result: AdaFRUGAL achieves a compelling trade-off, maintaining competitive performance while significantly reducing GPU memory and training time in experiments on English C4, Vietnamese VietVault, and GLUE benchmarks.

Conclusion: AdaFRUGAL offers a practical, autonomous solution for resource-constrained LLM training by automating hyperparameter tuning in gradient splitting.

Abstract: Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($ρ$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $ρ$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.

</details>


### [13] [Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces](https://arxiv.org/abs/2601.11572)
*Timo Aukusti Laine*

Main category: cs.LG

TL;DR: A mathematical analysis of LLM embedding spaces using linear algebra and Hamiltonian formalism, inspired by quantum mechanics, to study semantic structures and propose insights.


<details>
  <summary>Details</summary>
Motivation: To investigate the structured nature of LLM embeddings, which exhibit discrete semantic states, by applying mathematical tools from linear algebra and Hamiltonian formalism, drawing analogies with quantum mechanical systems for deeper insights.

Method: Apply linear algebra and Hamiltonian formalism to analyze LLM embedding spaces, exploring the L2 normalization constraint, relationships between cosine similarity and embedding perturbations, and direct/indirect semantic transitions, with a quantum-inspired perspective including zero-point energy analogies.

Result: The L2 normalization constraint in LLMs creates a structured embedding space suitable for Hamiltonian analysis, with derived connections between cosine similarity and perturbations, and exploration of semantic transitions and quantum-inspired concepts like zero-point energy and potential links to Koopman-von Neumann mechanics.

Conclusion: This approach provides a promising framework for gaining deeper insights into LLM semantics, potentially aiding in mitigating hallucinations, though interpretation requires careful consideration.

Abstract: We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.

</details>


### [14] [GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment](https://arxiv.org/abs/2601.11574)
*Lukas Abrie Nel*

Main category: cs.LG

TL;DR: GRADE is a method that replaces high-variance RL policy gradient estimation with differentiable relaxation for more stable LLM alignment, achieving better performance.


<details>
  <summary>Details</summary>
Motivation: Policy gradient methods like PPO have high variance gradient estimates, requiring complex tuning and high computational costs, prompting a need for a simpler, more stable alternative for LLM alignment.

Method: Using Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), it enables end-to-end gradient flow from reward signals through generated tokens to model parameters via differentiable relaxation.

Result: On IMDB sentiment-controlled text generation, GRADE-STE achieves a test reward of 0.763 ± 0.344 vs. PPO's 0.510 ± 0.313 and REINFORCE's 0.617 ± 0.378 (50% relative improvement over PPO), with gradient variance over 14 times lower than REINFORCE and stable training dynamics.

Conclusion: GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning methods for LLM alignment, improving generalization and reducing gradient variance significantly.

Abstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.

</details>


### [15] [Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.11604)
*Jonaid Shianifar,Michael Schukat,Karl Mason*

Main category: cs.LG

TL;DR: Hindsight Preference Replay (HPR) enhances multi-objective RL by retroactively relabeling stored transitions with alternative preferences, improving data utilization without changing the CAPQL architecture.


<details>
  <summary>Details</summary>
Motivation: Current MORL methods like CAPQL restrict data usage to specific preferences under which it was collected, leaving off-policy data from other preferences unused, which limits learning efficiency.

Method: HPR is a simple replay augmentation strategy that retroactively relabels stored transitions with alternative preferences, densifying supervision across the preference simplex without altering the CAPQL architecture or loss functions.

Result: Evaluated on six MO-Gymnasium locomotion tasks at a fixed 300K-step budget, HPR-CAPQL improves hypervolume in five of six environments and expected utility in four of six. Significant improvements shown on mo-humanoid-v5 (EUM from 323±125 to 1613±464, HV from 0.52M to 9.63M).

Conclusion: HPR is an effective and general technique that improves data efficiency in MORL by leveraging off-policy data across different preferences, achieving better performance on most benchmark tasks without architectural changes.

Abstract: Multi-objective reinforcement learning (MORL) enables agents to optimize vector-valued rewards while respecting user preferences. CAPQL, a preference-conditioned actor-critic method, achieves this by conditioning on weight vectors w and restricts data usage to the specific preferences under which it was collected, leaving off-policy data from other preferences unused. We introduce Hindsight Preference Replay (HPR), a simple and general replay augmentation strategy that retroactively relabels stored transitions with alternative preferences. This densifies supervision across the preference simplex without altering the CAPQL architecture or loss functions. Evaluated on six MO-Gymnasium locomotion tasks at a fixed 300000-step budget using expected utility (EUM), hypervolume (HV), and sparsity, HPR-CAPQL improves HV in five of six environments and EUM in four of six. On mo-humanoid-v5, for instance, EUM rises from $323\!\pm\!125$ to $1613\!\pm\!464$ and HV from 0.52M to 9.63M, with strong statistical support. mo-halfcheetah-v5 remains a challenging exception where CAPQL attains higher HV at comparable EUM. We report final summaries and Pareto-front visualizations across all tasks.

</details>


### [16] [A Multimodal Data Processing Pipeline for MIMIC-IV Dataset](https://arxiv.org/abs/2601.11606)
*Farzana Islam Adiba,Varsha Danduri,Fahmida Liza Piya,Ali Abbasi,Mehak Gupta,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: A multimodal pipeline is introduced to process MIMIC-IV EHR data by integrating structured data, clinical notes, waveforms, and imaging data, reducing processing time and enhancing reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing pipelines for MIMIC-IV target limited modalities or lack support for arbitrary downstream applications, necessitating extensive manual effort for preprocessing and alignment.

Method: The pipeline systematically integrates multiple modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized output formats for static and time-series applications.

Result: A comprehensive and customizable pipeline is developed and released as open-source code with a UI and Python package, facilitating easier and faster multimodal data processing for research.

Conclusion: This work enhances the usability of MIMIC-IV by providing a tool that reduces multimodal processing challenges and supports reproducible clinical machine learning studies.

Abstract: The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, they target a small subset of modalities or do not fully support arbitrary downstream applications. In this work, we greatly expand our prior popular unimodal pipeline and present a comprehensive and customizable multimodal pipeline that can significantly reduce multimodal processing time and enhance the reproducibility of MIMIC-based studies. Our pipeline systematically integrates the listed modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized multimodal output formats suitable for arbitrary static and time-series downstream applications. We release the code, a simple UI, and a Python package for selective integration (with embedding) at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.

</details>


### [17] [CooperBench: Why Coding Agents Cannot be Your Teammates Yet](https://arxiv.org/abs/2601.13295)
*Arpandeep Khatua,Hao Zhu,Peter Tran,Arya Prabhudesai,Frederic Sadrieh,Johann K. Lieberwirth,Xinkai Yu,Yicheng Fu,Michael J. Ryan,Jiaxin Pei,Diyi Yang*

Main category: cs.LG

TL;DR: Introduces CooperBench, a benchmark for collaborative coding tasks, showing AI agents suffer a 30% success rate drop when working together compared to solo, due to poor communication and coordination, unlike human teams.


<details>
  <summary>Details</summary>
Motivation: AI agents lack social intelligence and coordination capabilities needed for effective teamwork in collaborative tasks, contrasting with human teams where collaboration typically enhances productivity.

Method: Developed CooperBench with over 600 collaborative coding tasks across 12 libraries in 4 programming languages, grounded in real open-source repositories, to test state-of-the-art coding agents in scenarios requiring coordination.

Result: Agents achieved 30% lower success rates in collaborative tasks versus individual tasks, with issues including jammed communication channels, deviation from commitments, and incorrect expectations about others' plans. Rare emergent coordination behaviors like role division were observed.

Conclusion: Highlights the need for a shift from focusing on individual agent capabilities to developing social intelligence in AI, presenting CooperBench as a benchmark for future research in collaborative coding.

Abstract: Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.

</details>


### [18] [Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction](https://arxiv.org/abs/2601.11609)
*Weinuo Ou*

Main category: cs.LG

TL;DR: Proposes the Auxiliary Prediction Compression Memory Model (ApCM) to improve LLM runtime memory mechanisms for dynamic and personalized interactions.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) lack effective runtime memory mechanisms, hindering adaptation to dynamic and personalized interaction requirements.

Method: Introduces a novel neural memory storage architecture called the ApCM Model.

Result: Not specified in the provided abstract.

Conclusion: Not specified in the provided abstract.

Abstract: Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [19] [Rethinking the Value of Multi-Agent Workflow: A Strong Single Agent Baseline](https://arxiv.org/abs/2601.12307)
*Jiawei Xu,Arief Koesdwiady,Sisong Bei,Yan Han,Baixiang Huang,Dakuo Wang,Yutong Chen,Zheshen Wang,Peihao Wang,Pan Li,Ying Ding*

Main category: cs.MA

TL;DR: This paper investigates whether multi-agent workflows can be effectively simulated by a single LLM, finding advantages in efficiency and proposing a method to optimize such workflows.


<details>
  <summary>Details</summary>
Motivation: Most multi-agent frameworks are homogeneous, sharing the same base LLM, raising the question of whether their workflows can be simulated by a single agent rather than multiple agents.

Method: The study analyzes performance across seven benchmarks using a single LLM agent to simulate workflows, comparing it to homogeneous and automatically optimized heterogeneous workflows.

Result: A single agent can match the performance of homogeneous workflows with efficiency gains from KV cache reuse and even perform similarly to automatically optimized heterogeneous workflows.

Conclusion: The paper proposes OneFlow, an algorithm to tailor workflows for single-agent execution, reducing inference costs without sacrificing accuracy. It positions single-LLM implementations as a strong baseline and notes future opportunities for truly heterogeneous systems.

Abstract: Recent advances in LLM-based multi-agent systems (MAS) show that workflows composed of multiple LLM agents with distinct roles, tools, and communication patterns can outperform single-LLM baselines on complex tasks. However, most frameworks are homogeneous, where all agents share the same base LLM and differ only in prompts, tools, and positions in the workflow. This raises the question of whether such workflows can be simulated by a single agent through multi-turn conversations. We investigate this across seven benchmarks spanning coding, mathematics, general question answering, domain-specific reasoning, and real-world planning and tool use. Our results show that a single agent can reach the performance of homogeneous workflows with an efficiency advantage from KV cache reuse, and can even match the performance of an automatically optimized heterogeneous workflow. Building on this finding, we propose \textbf{OneFlow}, an algorithm that automatically tailors workflows for single-agent execution, reducing inference costs compared to existing automatic multi-agent design frameworks without trading off accuracy. These results position the single-LLM implementation of multi-agent workflows as a strong baseline for MAS research. We also note that single-LLM methods cannot capture heterogeneous workflows due to the lack of KV cache sharing across different LLMs, highlighting future opportunities in developing \textit{truly} heterogeneous multi-agent systems.

</details>


### [20] [Generative AI Agents for Controllable and Protected Content Creation](https://arxiv.org/abs/2601.12348)
*Haris Khan,Sadia Asif*

Main category: cs.MA

TL;DR: A multi-agent framework is proposed to enhance controllability and content protection in generative AI workflows, integrating controllable synthesis with watermarking and human feedback.


<details>
  <summary>Details</summary>
Motivation: The proliferation of generative AI has transformed creative workflows, but current systems face critical challenges in controllability and content protection, necessitating a solution that balances generation quality with security.

Method: We propose a novel multi-agent framework with specialized agent roles (Director/Planner, Generator, Reviewer, Integration, Protection) and integrated watermarking mechanisms, involving human-in-the-loop feedback and formalized as a joint optimization objective for controllability, semantic alignment, and protection robustness.

Result: The framework addresses the limitations of existing multi-agent systems by uniquely combining controllable content synthesis with provenance protection during generation, embedding imperceptible digital watermarks to ensure alignment with user intent and ownership tracking.

Conclusion: This work contributes to responsible generative AI by positioning multi-agent architectures as a solution for trustworthy creative workflows with built-in ownership tracking and content traceability.

Abstract: The proliferation of generative AI has transformed creative workflows, yet current systems face critical challenges in controllability and content protection. We propose a novel multi-agent framework that addresses both limitations through specialized agent roles and integrated watermarking mechanisms. Unlike existing multi-agent systems focused solely on generation quality, our approach uniquely combines controllable content synthesis with provenance protection during the generation process itself. The framework orchestrates Director/Planner, Generator, Reviewer, Integration, and Protection agents with human-in-the-loop feedback to ensure alignment with user intent while embedding imperceptible digital watermarks. We formalize the pipeline as a joint optimization objective unifying controllability, semantic alignment, and protection robustness. This work contributes to responsible generative AI by positioning multi-agent architectures as a solution for trustworthy creative workflows with built-in ownership tracking and content traceability.

</details>


### [21] [Semantic Fusion: Verifiable Alignment in Decentralized Multi-Agent Systems](https://arxiv.org/abs/2601.12580)
*Sofiya Zaichyk*

Main category: cs.MA

TL;DR: Semantic Fusion is a formal framework for decentralized semantic coordination in multi-agent systems that enables local validation to maintain global coherence without centralized control.


<details>
  <summary>Details</summary>
Motivation: To enable decentralized semantic coordination in multi-agent systems without requiring centralized control or explicit message passing, allowing agents to operate autonomously while maintaining global coherence through local validation.

Method: Introduces a formal framework using scoped views of shared memory, local ontology-based validation, refresh mechanisms, and bisimulation theory to ensure equivalence between local and global semantics. Also implements a lightweight reference architecture and conducts 250-agent simulation with over 11,000 updates.

Result: Establishes deterministic and probabilistic guarantees for semantic alignment under asynchronous communication. Simulation demonstrates convergence under probabilistic refresh, bounded communication, and resilience to agent failure with successful validation of over 11,000 updates.

Conclusion: Semantic Fusion provides a formal and scalable basis for verifiable autonomy in decentralized systems, enabling agents to maintain global coherence through local validation without centralized control.

Abstract: We present Semantic Fusion (SF), a formal framework for decentralized semantic coordination in multi-agent systems. SF allows agents to operate over scoped views of shared memory, propose structured updates, and maintain global coherence through local ontology-based validation and refresh without centralized control or explicit message passing. The central theoretical result is a bisimulation theorem showing that each agent's local execution is behaviorally equivalent to its projection of the global semantics, in both deterministic and probabilistic settings. This enables safety, liveness, and temporal properties to be verified locally and soundly lifted to the full system. SF supports agents whose update proposals vary across invocations, including those generated by learned or heuristic components, provided updates pass semantic validation before integration. We establish deterministic and probabilistic guarantees ensuring semantic alignment under asynchronous or degraded communication. To validate the model operationally, we implement a lightweight reference architecture that instantiates its core mechanisms. A 250-agent simulation evaluates these properties across over 11,000 validated updates, demonstrating convergence under probabilistic refresh, bounded communication, and resilience to agent failure. Together, these results show that Semantic Fusion can provide a formal and scalable basis for verifiable autonomy in decentralized systems.

</details>


### [22] [Communication Methods in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.12886)
*Christoph Wittner*

Main category: cs.MA

TL;DR: An overview paper reviewing communication techniques in multi-agent reinforcement learning, analyzing 29 studies to evaluate different methods and their suitability based on problem context.


<details>
  <summary>Details</summary>
Motivation: To address challenges like partial observability, non-stationarity, and large action spaces in multi-agent systems, and enable efficient cooperation through communication.

Method: In-depth analysis of 29 publications on communication techniques, evaluating explicit, implicit, attention-based, graph-based, and hierarchical/role-based methods.

Result: No universal optimal communication framework exists; choice depends on the specific problem, and low computational overhead is crucial for scalability in multi-agent environments.

Conclusion: Identifies need for standardized benchmarking and improved robustness under realistic conditions to enhance real-world applicability of communication methods in multi-agent reinforcement learning.

Abstract: Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.

</details>


### [23] [OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts Graph Generative Models](https://arxiv.org/abs/2601.12996)
*Shiyuan Li,Yixin Liu,Yu Zheng,Mei Li,Quoc Viet Hung Nguyen,Shirui Pan*

Main category: cs.MA

TL;DR: OFA-TAD is a one-for-all framework that uses a universal model to generate adaptive collaboration topologies for multi-agent systems based on natural language task descriptions, outperforming specialized models.


<details>
  <summary>Details</summary>
Motivation: Current methods for designing collaboration topologies in MAS rely on task-specific models, which generalize poorly to unseen domains and fail to share structural knowledge across tasks.

Method: The approach integrates a Task-Aware Graph State Encoder (TAGSE) with sparse gating and a Mixture-of-Experts (MoE) architecture, trained in three stages: unconditional pre-training, conditional pre-training on LLM-generated datasets, and supervised fine-tuning.

Result: Experiments on six benchmarks show that OFA-TAD significantly outperforms specialized one-for-one models in generating adaptive MAS topologies.

Conclusion: OFA-TAD provides an effective solution for adaptive topology design in MAS by leveraging a universal model and advanced training strategies, enhancing generalization and performance across diverse tasks.

Abstract: Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex problems, yet their performance is critically dependent on the design of their underlying collaboration topology. As MAS become increasingly deployed in web services (e.g., search engines), designing adaptive topologies for diverse cross-domain user queries becomes essential. Current graph learning-based design methodologies often adhere to a "one-for-one" paradigm, where a specialized model is trained for each specific task domain. This approach suffers from poor generalization to unseen domains and fails to leverage shared structural knowledge across different tasks. To address this, we propose OFA-TAD, a one-for-all framework that generates adaptive collaboration graphs for any task described in natural language through a single universal model. Our approach integrates a Task-Aware Graph State Encoder (TAGSE) that filters task-relevant node information via sparse gating, and a Mixture-of-Experts (MoE) architecture that dynamically selects specialized sub-networks to drive node and edge prediction. We employ a three-stage training strategy: unconditional pre-training on canonical topologies for structural priors, large-scale conditional pre-training on LLM-generated datasets for task-topology mappings, and supervised fine-tuning on empirically validated graphs. Experiments across six diverse benchmarks show that OFA-TAD significantly outperforms specialized one-for-one models, generating highly adaptive MAS topologies. Code: https://github.com/Shiy-Li/OFA-MAS.

</details>


### [24] [A simulation of urban incidents involving pedestrians and vehicles based on Weighted A*](https://arxiv.org/abs/2601.13452)
*Edgar Gonzalez Fernandez*

Main category: cs.MA

TL;DR: A simulation framework using multiagent systems to model pedestrian-vehicle interactions in a 2D urban grid, assessing collision risk and efficiency based on environmental and behavioral factors.


<details>
  <summary>Details</summary>
Motivation: To model complex urban incident scenarios involving pedestrians and vehicles to analyze interactions, evaluate collision risk, and understand impacts on safety and travel efficiency.

Method: Implements a multiagent system with pedestrian and vehicle agents in a 2D grid environment. Uses a weighted A* algorithm for pathfinding, incorporating behavioral variations like reckless movement or rule-following. Includes streets, sidewalks, buildings, zebra crossings, and obstacles.

Result: Experimental results indicate how obstacle density, traffic control mechanisms, and behavioral deviations affect safety and travel efficiency.

Conclusion: The framework effectively simulates and assesses urban incident scenarios, providing insights into factors that influence pedestrian-vehicle interactions and suggesting potential improvements for urban safety and efficiency.

Abstract: This document presents a comprehensive simulation framework designed to model urban incidents involving pedestrians and vehicles. Using a multiagent systems approach, two types of agents (pedestrians and vehicles) are introduced within a 2D grid based urban environment. The environment encodes streets, sidewalks, buildings, zebra crossings, and obstacles such as potholes and infrastructure elements. Each agent employs a weighted A* algorithm for pathfinding, allowing for variation in decision making behavior such as reckless movement or strict rule-following. The model aims to simulate interactions, assess risk of collisions, and evaluate efficiency under varying environmental and behavioral conditions. Experimental results explore how factors like obstacle density, presence of traffic control mechanisms, and behavioral deviations affect safety and travel efficiency.

</details>


### [25] [The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption](https://arxiv.org/abs/2601.13671)
*Apoorva Adimulam,Rajesh Gupta,Sumit Kumar*

Main category: cs.MA

TL;DR: This paper introduces a unified architectural framework and two communication protocols (Model Context Protocol and Agent2Agent) for scalable, auditable, and policy-compliant orchestrated multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: To address the need for structured coordination and communication in collaborative multi-agent AI systems, enabling complex shared objectives in enterprise-scale ecosystems.

Method: Consolidates and formalizes technical components into an orchestration layer integrating planning, policy enforcement, state management, and quality operations, along with designing interoperable protocols.

Result: Establishes a coherent technical blueprint that includes communication protocols and governance mechanisms for system coherence, transparency, and accountability.

Conclusion: Orchestrated multi-agent systems are advanced through this architectural framework, bridging concepts with implementable design principles for scalable AI applications.

Abstract: Orchestrated multi-agent systems represent the next stage in the evolution of artificial intelligence, where autonomous agents collaborate through structured coordination and communication to achieve complex, shared objectives. This paper consolidates and formalizes the technical composition of such systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. Another primary contribution of this work is the in-depth technical delineation of two complementary communication protocols - the Model Context Protocol, which standardizes how agents access external tools and contextual data, and the Agent2Agent protocol, which governs peer coordination, negotiation, and delegation. Together, these protocols establish an interoperable communication substrate that enables scalable, auditable, and policy-compliant reasoning across distributed agent collectives. Beyond protocol design, the paper details how orchestration logic, governance frameworks, and observability mechanisms collectively sustain system coherence, transparency, and accountability. By synthesizing these elements into a cohesive technical blueprint, this paper provides comprehensive treatments of orchestrated multi-agent systems - bridging conceptual architectures with implementation-ready design principles for enterprise-scale AI ecosystems.

</details>
