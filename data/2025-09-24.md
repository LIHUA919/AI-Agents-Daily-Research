<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 48]
- [cs.LG](#cs.LG) [Total: 125]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: MMCD framework enhances autonomous driving safety through multi-modal collaborative decision-making with cross-modal knowledge distillation to handle missing data modalities.


<details>
  <summary>Details</summary>
Motivation: Address limitations of single vehicles with limited sensor range and practical challenges of sensor failures/missing connected vehicles in multi-modal autonomous systems.

Method: Proposes MMCD framework that fuses multi-modal observations from ego and collaborative vehicles using cross-modal knowledge distillation with teacher-student model structure.

Result: Improves driving safety by up to 20.7% in connected autonomous driving scenarios, outperforming existing baselines in accident detection and safe decision-making.

Conclusion: MMCD provides robust multi-modal collaborative decision-making that maintains performance even when certain data modalities are unavailable during testing.

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [2] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: A formal approach for explaining changes in inference within Quantitative Bipolar Argumentation Frameworks (QBAFs) by tracing strength inconsistencies in partial orders over argument strengths.


<details>
  <summary>Details</summary>
Motivation: To provide explanations for changes in conclusions when QBAFs are updated, specifically focusing on strength inconsistencies in the partial order of argument strengths for topic arguments.

Method: Define strength inconsistencies, trace their causes to specific arguments, identify sufficient/necessary/counterfactual explanations, develop heuristic-based search approach with implementation.

Result: Shows that strength inconsistency explanations exist if and only if an update leads to strength inconsistency, and provides an implementation for finding these explanations.

Conclusion: The approach successfully formalizes explanation of inference changes in QBAFs through strength inconsistency analysis with practical implementation.

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [3] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: This paper presents a cost-benefit analysis framework to help organizations decide between commercial LLM services and on-premise deployment by comparing costs, performance, and breakeven points.


<details>
  <summary>Details</summary>
Motivation: Organizations face challenges in choosing between commercial LLM services (data privacy concerns, vendor lock-in, long-term costs) and local deployment of open-source models, requiring a systematic decision-making framework.

Method: The study analyzes hardware requirements, operational expenses, and performance benchmarks of open-source models (Qwen, Llama, Mistral, etc.) and compares total local deployment costs with commercial subscription fees from major cloud providers.

Result: The research provides estimated breakeven points based on usage levels and performance needs, showing when on-premise deployment becomes economically viable compared to subscription services.

Conclusion: The framework offers organizations a practical tool for planning their LLM strategies by quantifying the economic trade-offs between cloud services and local deployment.

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [4] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: SPADE is an LLM-based framework that detects irrigation patterns and anomalies in soil moisture time-series data using ChatGPT-4.1, achieving superior performance without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing soil moisture analysis methods rely on threshold-based rules or data-intensive ML/DL models that lack adaptability and interpretability for irrigation scheduling and crop management.

Method: SPADE converts time-series data to textual representation and uses domain-informed prompt templates with ChatGPT-4.1 for zero-shot analysis of irrigation events, net irrigation gains, and anomaly detection/classification.

Result: SPADE outperforms existing methods in anomaly detection (higher recall/F1 scores) and achieves high precision/recall in irrigation event detection on real-world data from US farms.

Conclusion: LLMs like SPADE offer scalable, adaptable tools for precision agriculture by integrating qualitative knowledge with data-driven reasoning for actionable soil moisture insights.

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [5] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: The paper proposes Explainable Uncertainty Estimation (XUE) to bridge the gap between explainable AI and uncertainty quantification in medical AI systems, aiming to enhance trust and clinical usability.


<details>
  <summary>Details</summary>
Motivation: Current medical AI systems fail to communicate uncertainty in clinically meaningful ways, with XAI focusing on predictions without confidence measures and UE providing confidence but lacking intuitive explanations, limiting AI adoption in medicine.

Method: The authors systematically map medical uncertainty to AI uncertainty concepts, identify implementation challenges, and outline technical directions including multimodal uncertainty quantification, model-agnostic visualization techniques, and uncertainty-aware decision support systems.

Result: The analysis highlights the need for AI systems that generate reliable predictions while articulating confidence levels in clinically meaningful ways, providing guiding principles for effective XUE realization.

Conclusion: This work contributes to trustworthy medical AI by bridging explainability and uncertainty, paving the way for AI systems aligned with real-world clinical complexities through the proposed XUE framework.

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [6] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: HSGM is a hierarchical framework that decomposes long documents into segments, constructs local semantic graphs, and creates a global graph memory to enable efficient semantic parsing with reduced computational complexity and memory usage.


<details>
  <summary>Details</summary>
Motivation: Semantic parsing of long documents faces challenges due to quadratic growth in computational complexity and memory requirements, making it impractical for ultra-long texts and real-time applications.

Method: Hierarchical Segment-Graph Memory (HSGM) decomposes input into meaningful segments, builds Local Semantic Graphs on each segment, extracts summary nodes to form a Global Graph Memory, supports incremental updates, and uses Hierarchical Query Processing for efficient retrieval and reasoning.

Result: HSGM achieves 2-4× inference speedup, >60% reduction in peak memory usage, and ≥95% of baseline accuracy on benchmarks including long-document AMR parsing, semantic role labeling, and legal event extraction.

Conclusion: HSGM enables scalable and accurate semantic modeling for ultra-long texts, making real-time and resource-constrained NLP applications feasible by reducing worst-case complexity from O(N²) to O(Nk + (N/k)²).

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [7] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent is a multi-agent framework that automates the entire OpenFOAM CFD workflow from natural language prompts, achieving 88.2% success rate on benchmark tests.


<details>
  <summary>Details</summary>
Motivation: CFD simulations have a steep learning curve and complex manual setup, creating significant barriers for users. Existing systems lack comprehensive end-to-end automation.

Method: Uses a multi-agent framework with Model Context Protocol (MCP) for composable services, hierarchical multi-index RAG for context retrieval, and dependency-aware generation for configuration consistency. Handles pre-processing, meshing, HPC script generation, and post-simulation visualization.

Result: Achieved 88.2% success rate on 110 simulation tasks, significantly outperforming MetaOpenFOAM (55.5%). Successfully automates the full CFD pipeline from natural language input.

Conclusion: Foam-Agent dramatically lowers the expertise barrier for CFD and demonstrates how specialized multi-agent systems can democratize complex scientific computing.

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [8] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: This paper surveys the integration of large language models (LLMs) into operations research (OR), organizing methods into automatic modeling, auxiliary optimization, and direct solving, and discusses challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: Traditional OR approaches face challenges in handling large-scale, dynamic, and multi-constraint problems due to reliance on expert-based modeling and manual parameter adjustment. LLMs offer potential solutions through semantic understanding and reasoning capabilities.

Method: The paper organizes LLM-OR integration methods into three main directions: automatic modeling (translating natural language to mathematical models/code), auxiliary optimization (generating heuristics, evolving algorithms), and direct solving (tackling optimization tasks directly).

Result: The survey reviews evaluation benchmarks, domain-specific applications, and identifies key challenges including unstable semantic-to-structure mapping, fragmented research progress, limited generalization, and insufficient evaluation systems.

Conclusion: The paper outlines possible research avenues for advancing LLMs' role in OR, suggesting future directions to address current limitations and enhance the integration of LLMs into operations research methodologies.

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [9] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: The paper introduces SAPA framework that uses LLMs to synthesize psychological attitudes from travel data to improve ridesourcing mode choice prediction, achieving 75.9% PR-AUC improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing ridesourcing mode choice models have limited accuracy due to inability to capture psychological factors and severe class imbalance issues, as ridesourcing trips are rare in daily travel.

Method: SAPA uses a hierarchical approach: LLM generates traveler personas from survey data, trains propensity-score model, assigns quantitative scores to latent variables, and integrates all features in a final classifier.

Result: Experiments on large-scale multi-year travel survey show SAPA outperforms state-of-the-art baselines by up to 75.9% in PR-AUC on held-out test set.

Conclusion: SAPA provides an accurate tool for ridesourcing mode choice prediction with methodology transferable to various applications.

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [10] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: OBER is an Outcome-Based Educational Recommender that evaluates recommendation algorithms based on learning mastery rather than just clicks or ratings, using a unified framework that combines business, relevance, and learning metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional educational recommender systems focus on click- or rating-based relevance, which doesn't measure true pedagogical impact. There's a need to evaluate recommendations based on actual learning outcomes and mastery.

Method: OBER uses a minimalist entity-relation model, log-driven mastery formula, and plug-in architecture. It was tested in a two-week randomized split test with over 5,700 learners comparing fixed expert trajectory, collaborative filtering, and knowledge-based filtering methods.

Result: Collaborative filtering maximized retention, but the fixed expert path achieved the highest mastery. The framework successfully derived business, relevance, and learning metrics from the same logs.

Conclusion: OBER provides a method-agnostic framework that allows practitioners to balance relevance and engagement against outcome mastery without extra testing overhead, and is extensible to future adaptive or context-aware recommenders.

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [11] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: Neural DNA (nDNA) is proposed as a semantic-genotypic representation that captures a model's latent cognitive identity through three geometric dimensions: spectral curvature, thermodynamic length, and belief vector field, enabling lineage tracing and cognitive evolution analysis.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks measure model behavior but fail to capture the internal cognitive identity and latent geometry that defines a model's 'soul'. There's a need to understand how meaning flows through layers and how models evolve semantically.

Method: nDNA synthesizes three geometric dimensions: spectral curvature (conceptual flow curvature across layers), thermodynamic length (semantic effort for representational transitions), and belief vector field (semantic torsion fields guiding belief orientations). This creates a coordinate-free fingerprint for analyzing model lineages.

Result: The approach enables tracing model lineages across pretraining, fine-tuning, alignment, pruning, distillation, and merges; measuring inheritance between checkpoints; detecting drift under new data/objectives; and studying artificial cognition evolution.

Conclusion: nDNA opens the field of Neural Genomics, treating AI models as digital semantic organisms with traceable inner cognition, enabling model comparison, risk diagnosis, and governance of cognitive evolution over time.

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [12] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: Similarity Field Theory provides a mathematical framework for analyzing similarity relations and their evolution, formalizing intelligence as the ability to generate entities that maintain similarity to concepts.


<details>
  <summary>Details</summary>
Motivation: To establish a foundational mathematical framework for understanding similarity relations in dynamic systems and formalize a generative definition of intelligence.

Method: Defines similarity fields over entities, system evolution sequences, concepts as fibers, and generative operators, with theorems on asymmetry and stability constraints.

Result: Proves two key theorems: (i) asymmetry blocks mutual inclusion, and (ii) stability requires anchor coordinates or confinement within level sets, ensuring constrained evolution.

Conclusion: The framework offers a language for characterizing intelligent systems and can interpret large language models as probes into societal cognition.

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [13] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: VL-RiskFormer is a hierarchical multimodal Transformer with LLM inference head that integrates medical imaging, clinical narratives, and wearable data for proactive health risk prediction, achieving 0.90 AUROC on MIMIC-IV.


<details>
  <summary>Details</summary>
Motivation: Address the rising global burden of chronic diseases and the need for unified multimodal AI frameworks to handle heterogeneous clinical data (medical imaging, free-text recordings, wearable sensor streams) for proactive individual health risk prediction.

Method: Hierarchical stacked visual-language multimodal Transformer with LLM inference head, featuring: (i) cross-modal pre-training with momentum update encoders and debiased InfoNCE losses, (ii) time fusion block with adaptive time interval position coding for irregular visit sequences, (iii) disease ontology map adapter injecting ICD-10 codes with graph attention mechanism.

Result: Achieved average AUROC of 0.90 with expected calibration error of 2.7% on MIMIC-IV longitudinal cohort.

Conclusion: VL-RiskFormer demonstrates effective multimodal integration for health risk prediction, showing strong performance in handling diverse clinical data types and temporal patterns.

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [14] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: ChefMind is a hybrid architecture combining Chain of Exploration, Knowledge Graph, Retrieval-Augmented Generation, and LLM to address challenges in personalized recipe recommendation, achieving superior performance over baseline models.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in personalized recipe recommendation including handling fuzzy user intent, ensuring semantic accuracy, and providing sufficient detail coverage.

Method: Proposes ChefMind - a hybrid architecture combining Chain of Exploration (CoE) for refining ambiguous queries, Knowledge Graph (KG) for semantic reasoning, Retrieval-Augmented Generation (RAG) for contextual details, and LLM for integrating outputs into coherent recommendations.

Result: Achieves average score of 8.7 vs 6.4-6.7 for ablation models (LLM-only, KG-only, RAG-only), reduces unprocessed queries to 1.6%, and demonstrates superior performance in accuracy, relevance, completeness, and clarity on Xiachufang dataset.

Conclusion: ChefMind effectively handles fuzzy user demands and provides robust, high-quality recipe recommendations through its integrated hybrid approach.

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [15] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: The paper introduces an "N-Plus-1" GPT Agency that uses multiple AI agents to improve reliability in mechanical engineering problem solving by comparing multiple solution attempts and selecting the most probable correct answer.


<details>
  <summary>Details</summary>
Motivation: GPT models show unreliability in mechanical engineering analysis with only 85% success probability, making them unsuitable for education and engineering practice without additional safeguards.

Method: An agency launches N independent Agent Solve instances to generate multiple solution realizations, then uses Agent Compare to summarize, compare, and recommend the best solution based on Condorcet's Jury Theorem principles.

Result: The approach significantly improves reliability over single GPT instances, with comparisons to Grok Heavy showing similar performance but with greater focus on transparency and pedagogical value.

Conclusion: The N-Plus-1 agency framework provides a reliable method for deploying GPT in mechanical engineering applications by leveraging multiple independent solutions and comparative analysis to overcome individual instance unreliability.

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [16] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: ComputerAgent is a lightweight hierarchical RL framework for desktop automation that achieves comparable performance to large MLLMs while being 4 orders of magnitude smaller and twice as fast.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based desktop automation suffers from high latency, poor efficiency on long tasks, and inability to run on-device due to large model sizes.

Method: Two-level hierarchical RL with manager-subpolicy options, triple-modal state encoder (screenshot, task ID, numeric state), meta-actions with early-stop mechanism, and compact vision backbone with small policy networks (15M parameters).

Result: 92.1% success on simple tasks (<8 steps) and 58.8% on hard tasks (≥8 steps), matching/exceeding 200B-parameter MLLMs while reducing model size by 10,000x and halving inference time.

Conclusion: Hierarchical RL provides a practical, scalable alternative to monolithic MLLM-based automation for computer control tasks.

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [17] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: Current medical AI benchmarks are misleading - top models achieve high scores through test-taking tricks rather than genuine medical understanding, exposing brittleness and shortcut learning that doesn't reflect real-world readiness.


<details>
  <summary>Details</summary>
Motivation: To expose how medical AI benchmarks reward superficial test-taking strategies over true medical competence, and to demonstrate that high leaderboard scores mask fundamental weaknesses in model robustness and reasoning.

Method: Evaluated six flagship models across six widely used medical benchmarks using stress tests (removing key inputs, testing prompt sensitivity), analyzing answer flipping patterns, and conducting clinician-guided rubric evaluation to assess what benchmarks truly measure.

Result: Leading systems often guess correctly even when critical inputs are removed, flip answers under trivial prompt changes, fabricate convincing but flawed reasoning, and benchmarks vary widely in what they actually measure while being treated interchangeably.

Conclusion: Medical benchmark scores do not reflect real-world readiness; to earn trust in healthcare, AI systems must be held accountable for robustness, sound reasoning, and alignment with real medical demands beyond just leaderboard performance.

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [18] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: This paper investigates compute constraint strategies (reasoning length constraint and model quantization) to reduce computational costs of reasoning language models while studying their impact on safety performance.


<details>
  <summary>Details</summary>
Motivation: Test-time compute scaling improves reasoning model performance but significantly increases computational cost. The research aims to find methods to reduce compute demand while maintaining safety.

Method: Two approaches: (1) fine-tuning reasoning models using length controlled policy optimization (LCPO) reinforcement learning to satisfy user-defined CoT reasoning length, (2) applying quantization to maximize CoT generation within user-defined compute constraints.

Result: The paper studies the trade-off between computational efficiency and model safety when applying these compute constraint strategies.

Conclusion: Compute constraints can be effectively applied through reasoning length control and quantization to balance performance with computational efficiency, while safety implications need careful consideration.

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [19] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: The paper proposes the Gödel Test to evaluate if large language models can prove simple unsolved mathematical conjectures, testing GPT-5 on five combinatorial optimization problems with mixed results showing progress but limitations.


<details>
  <summary>Details</summary>
Motivation: To determine whether large language models can solve new, simple conjectures in advanced mathematics beyond just replicating known competition problems.

Method: Evaluated GPT-5 on five previously unsolved conjectures in combinatorial optimization by providing source papers and assessing the model's reasoning and proof generation capabilities.

Result: GPT-5 produced nearly correct solutions for three easier problems, refuted one conjecture with a valid alternative proof, failed on a problem requiring cross-paper synthesis, and proposed correct algorithms but flawed analysis for the hardest problem.

Conclusion: GPT-5 shows meaningful progress in routine reasoning and occasional originality but has clear limitations in complex synthesis tasks, representing an early step toward models eventually passing the Gödel Test.

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [20] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: First benchmark for HTS code classification using fine-tuned Atlas model (LLaMA-3.3-70B) achieves 40% 10-digit accuracy, significantly outperforming and being cheaper than leading LLMs.


<details>
  <summary>Details</summary>
Motivation: HTS code classification is a critical bottleneck in global trade with severe consequences for misclassification, yet has received little ML research attention.

Method: Created benchmark from US Customs Rulings Online Search System (CROSS) and fine-tuned Atlas model (LLaMA-3.3-70B) for HTS classification.

Result: Atlas achieves 40% fully correct 10-digit classifications (15 points over GPT-5, 27.5 over Gemini) and 57.5% 6-digit accuracy, while being 5-8x cheaper and enabling self-hosting for data privacy.

Conclusion: While Atlas sets strong baseline, HTS classification remains highly challenging; releasing dataset and model to establish it as community benchmark for future work in retrieval, reasoning, and alignment.

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [21] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC is a new benchmark that evaluates precise instruction following in function calling by testing adherence to format specifications embedded in JSON schema parameter descriptions, which existing benchmarks overlook.


<details>
  <summary>Details</summary>
Motivation: Existing function calling benchmarks only evaluate argument correctness but don't test adherence to format instructions (like double quotes, ISO dates) embedded in parameter descriptions, which is crucial for real-world AI agent systems.

Method: IFEval-FC encodes verifiable formats directly within JSON schema descriptions and includes 750 test cases, each with a function containing embedded format requirements and corresponding user queries. Evaluation is fully algorithmic.

Result: Even state-of-the-art proprietary models like GPT-5 and Claude 4.1 Opus frequently fail to follow basic formatting rules, revealing practical limitations for real-world agent systems.

Conclusion: The benchmark highlights a critical gap in current function calling evaluation and provides an objective, reproducible, and scalable method to assess precise instruction following, with code and data publicly available.

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [22] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Memory-QA is a novel task for answering recall questions about visual content from stored multimodal memories, addressed by the Pensieve pipeline with superior performance over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of creating task-oriented memories, utilizing temporal and location information effectively, and leveraging multiple memories for answering recall questions in real-world scenarios.

Method: Proposed Pensieve pipeline with memory-specific augmentation, time- and location-aware multi-signal retrieval, and multi-memory QA fine-tuning.

Result: Pensieve achieves up to 14% improvement in QA accuracy over state-of-the-art solutions on a created multimodal benchmark.

Conclusion: The Pensieve pipeline effectively addresses the unique challenges of the Memory-QA task, demonstrating significant performance gains in answering recall questions from multimodal memories.

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [23] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: FERA is an AI referee prototype for foil fencing that combines pose-based action recognition with rule-based reasoning to address subjective refereeing challenges.


<details>
  <summary>Details</summary>
Motivation: Fencing faces refereeing challenges including subjective calls, human errors, bias, and limited availability in practice environments.

Method: Uses 2D joint position extraction, kinematic feature computation, Transformer for multi-label classification, and rule-based reasoning with encoded right-of-way rules.

Result: Achieved average macro-F1 score of 0.549 with 5-fold cross-validation, outperforming TCN, BiLSTM, and vanilla Transformer baselines.

Conclusion: While not deployment-ready, FERA demonstrates promising potential for automated referee assistance and opens opportunities for AI applications in fencing coaching.

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [24] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: LLMZ+ is a security framework that uses prompt whitelisting instead of traditional detection-based approaches to protect agentic LLMs from jailbreak attacks, ensuring only contextually appropriate messages interact with the AI.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems have privileged access to data and APIs, making them valuable targets. Their nondeterministic behavior introduces significant security risks that traditional detection-based defenses don't adequately address.

Method: The paper proposes LLMZ+, which implements prompt whitelisting to only allow contextually appropriate and safe messages to reach the agentic LLM, ensuring all exchanges conform to predefined use cases and operational boundaries.

Result: Empirical evaluation shows LLMZ+ provides strong resilience against common jailbreak prompts while maintaining legitimate business communications. False positive and false negative rates were reduced to 0 in experimental settings.

Conclusion: LLMZ+ offers a more streamlined, resilient security framework that reduces resource requirements for sustaining LLM information security compared to traditional detection-based approaches.

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [25] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: A novel method that combines LLM equation generation with symbolic solvers and verification through estimation to solve Math Word Problems, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with Math Word Problems due to limitations in reasoning and mathematical abilities, despite excelling at other tasks. Current methods need improvement for complex MWPs.

Method: First prompts LLM to create equations from question decomposition, uses external symbolic solver for answer, then verifies by having LLM estimate the answer and compare with generated answer. Uses iterative rectification if verification fails.

Result: Achieves new state-of-the-art results on numeric and algebraic MWPs, improving previous best by nearly 2% on average. Also obtains satisfactory results on trigonometric MWPs, a previously unattempted task.

Conclusion: The proposed approach effectively enhances LLMs' ability to solve complex MWPs through decomposition, symbolic solving, and verification, demonstrating significant improvements across various mathematical problem types.

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [26] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: A geospatial agent-based model integrating climate hazard data with evolutionary learning for economic agents to assess climate risks and adaptation strategies.


<details>
  <summary>Details</summary>
Motivation: Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems.

Method: Combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviors for firms to evolve strategies through fitness-based selection and mutation.

Result: Evolutionary adaptation enables firms to converge with baseline production levels after decades of disruption; systemic risks show even non-exposed agents face impacts through supply chain disruptions, with end-of-century prices 5.6% higher under RCP8.5.

Conclusion: This open-source framework provides tools for financial institutions and companies to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [27] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: TERAG is a cost-effective graph-based RAG framework that reduces LLM token usage by 89-97% while maintaining 80% accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG systems have high LLM token costs during graph construction, which limits large-scale adoption. The goal is to create a more cost-effective solution.

Method: Proposes TERAG framework inspired by HippoRAG, incorporating Personalized PageRank (PPR) during retrieval phase to build informative graphs with minimal token consumption.

Result: Achieves at least 80% accuracy of widely used graph-based RAG methods while using only 3%-11% of the output tokens (89-97% reduction in token usage).

Conclusion: TERAG provides a simple yet effective solution for cost-efficient graph-based RAG, making large-scale adoption more feasible by significantly reducing token costs while maintaining competitive accuracy.

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [28] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: The paper clarifies the distinction between ML models and their unambiguous descriptions (MLMD) and refines semantics preservation for accurate model replication, applied to industrial use cases.


<details>
  <summary>Details</summary>
Motivation: To address the need for safe operation and regulatory compliance of ML-based airborne systems, particularly in meeting EASA and EUROCAE/SAE standards for demonstrating intended function and training performance maintenance.

Method: The paper defines the concept of Machine Learning Model Description (MLMD) as an unambiguous description of ML models, refines semantics preservation for accurate model replication, and applies these concepts to industrial use cases to build and compare target models.

Result: The approach provides a framework for ensuring ML models in airborne systems can be accurately described and replicated while maintaining their intended functionality and performance in target environments.

Conclusion: The clarification between ML models and MLMDs, along with refined semantics preservation concepts, contributes to safer development of ML-based airborne systems that can meet regulatory compliance requirements.

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [29] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: This paper provides a systematic review of large language models (LLMs) in the medical field, analyzing training techniques, healthcare applications, strengths/limitations, and proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: To highlight the necessity of developing medical LLMs, provide understanding of their current state, and offer guidance for subsequent research as AI advances rapidly with LLMs making significant impact in medicine.

Method: Systematic review of up-to-date research progress, including analysis of training techniques for large medical models, their healthcare adaptation, applications, and innovative categorization of medical LLMs into three types based on training methodologies.

Result: The study categorizes medical LLMs into three distinct types, classifies evaluation approaches into two categories, and identifies existing challenges in the field.

Conclusion: The paper proposes solutions to challenges and outlines future research directions, aiming to provide clear guidance for subsequent research in medical LLMs.

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [30] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: DataAgents represent a paradigm shift using autonomous AI agents to transform complex data into actionable knowledge through LLM reasoning, task decomposition, and tool calling.


<details>
  <summary>Details</summary>
Motivation: Data preparation and analysis remain labor-intensive despite growing data complexity, and traditional tools lack the adaptability needed for optimal AI utilization of unstructured data.

Method: DataAgents integrate LLM reasoning with task decomposition, action reasoning, grounding, and tool calling to autonomously handle data operations like preprocessing, transformation, augmentation, and retrieval.

Result: DataAgents can dynamically plan workflows and adapt to diverse data tasks at scale, transforming unstructured data into coherent knowledge through automated data operations.

Conclusion: The report calls for advancing workflow optimization, establishing benchmarks, safeguarding privacy, and developing trustworthy guardrails to prevent malicious actions in DataAgent systems.

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [31] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: Experience scaling - a framework for continuous post-deployment evolution of LLMs through autonomous environmental interaction and collaborative experience sharing, enabling ongoing capability improvement beyond static training data limits.


<details>
  <summary>Details</summary>
Motivation: Traditional scaling approaches (model size, training data, compute) are reaching saturation as human-generated text is exhausted and gains diminish, requiring new methods for continuous LLM evolution.

Method: A framework that captures raw interactions, distills them into compact reusable knowledge, and periodically refines stored content to maintain relevance and efficiency through autonomous environmental interaction and collaborative experience sharing.

Result: Validated in simulated real-world scenarios, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations involving generalization to unseen tasks, repetitive queries, and over-saturated knowledge stores.

Conclusion: Structured post-deployment learning can extend LLM capabilities beyond static human-generated data limitations, offering a scalable path for continued intelligence progress through experience scaling.

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [32] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: ADS is a distributed directory service for discovering AI agent capabilities using content-addressed storage, hierarchical taxonomies, and cryptographic signing to enable verifiable discovery across heterogeneous Multi-Agent Systems.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient, verifiable discovery of AI agent capabilities across diverse Multi-Agent Systems, enabling interoperability and trust in agent interactions.

Method: Leverages Open Agentic Schema Framework with two-level mapping over Kademlia-based DHT, uses OCI/ORAS infrastructure for artifact distribution, integrates Sigstore for provenance, and supports schema-driven extensibility.

Result: A formal architectural model with storage and discovery layers that provides security and performance properties for agent registry and interoperability.

Conclusion: ADS positions itself as a foundational component in the emerging landscape of agent registry and interoperability initiatives, enabling trustworthy discovery across heterogeneous agent systems.

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [33] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER is a model-checking-based verification method that verifies probabilistic computation tree logic (PCTL) properties of LLM text generation processes using α-k-bounded text generation.


<details>
  <summary>Details</summary>
Motivation: Current LLM text generation processes lack formal verification methods to ensure consistency and reliability. The observation that only limited tokens are typically chosen during generation, but not always the same ones, motivates the need for formal verification.

Method: Uses α-k-bounded text generation that focuses on α maximal cumulative probability on top-k tokens at each generation step. Considers initial strings and subsequent top-k tokens while accommodating various text quantification methods like quality and bias evaluation.

Result: Successfully demonstrated applicability on multiple LLMs including Llama, Gemma, Mistral, Genstruct, and BERT. Provides formal verification of PCTL properties for bounded LLMs.

Conclusion: This represents the first application of PCTL-based model checking for verifying consistency in LLM text generation processes, offering a formal verification framework for LLM reliability.

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [34] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: A modular framework for ICD-10-CM code prediction using LLMs with principled model selection, redundancy-aware data sampling, and structured input design to improve automated medical coding accuracy.


<details>
  <summary>Details</summary>
Motivation: ICD coding is labor-intensive and error-prone, and current LLM approaches face challenges in model selection, input contextualization, and training data redundancy that limit their effectiveness in automated medical coding.

Method: Proposes a framework with LLM-as-judge evaluation using Plackett-Luce aggregation for model selection, embedding-based similarity measures for redundancy-aware sampling, and structured discharge summaries with section-wise content analysis under different modeling paradigms.

Result: Experiments show the selected fine-tuned base model consistently outperforms baseline LLMs in internal and external evaluations, with performance improving as more clinical sections are incorporated.

Conclusion: The framework provides a scalable, institution-ready solution for real-world deployment of automated medical coding systems through informed model selection, efficient data refinement, and context-aware prompting.

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [35] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: Proposes Mixed Advantage Policy Optimization (MAPO) to address advantage reversion and mirror problems in GRPO by dynamically reweighting advantage functions based on trajectory certainty.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO methods suffer from advantage reversion and advantage mirror problems that hinder reasonable advantage allocation across query samples.

Method: Introduces advantage percent deviation for high-certainty trajectories and dynamically reweights advantage functions based on trajectory certainty to adapt to sample-specific characteristics.

Result: Comparison with state-of-the-art methods and ablation studies validate the effectiveness of MAPO approach.

Conclusion: MAPO provides an easy but effective strategy to improve advantage allocation in reinforcement learning for foundation models.

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [36] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: ProfileBench is a new benchmark for user profiling using LLMs, and Conf-Profile is a confidence-driven framework that achieves label-free user profiling through two-stage training with confidence-weighted voting and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Current user profiling with LLMs lacks comprehensive benchmarks and struggles with label scarcity, heterogeneous/noisy data, and reliability issues.

Method: Two-stage framework: 1) Synthesize high-quality labels using LLMs with confidence hints, then use confidence-weighted voting and calibration; 2) Distill results into lightweight LLM and enhance reasoning via confidence-guided unsupervised reinforcement learning.

Result: Conf-Profile significantly improves performance, achieving 13.97 F1 improvement on Qwen3-8B model.

Conclusion: The proposed confidence-driven framework effectively addresses label scarcity and data reliability issues in user profiling, demonstrating substantial performance gains through the two-stage training approach.

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [37] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: This paper proposes a unified framework for defining, categorizing, and evaluating LLM memory systems, including a taxonomy, evaluation protocols, governance mechanisms, and testable propositions for reproducible research.


<details>
  <summary>Details</summary>
Motivation: To establish a standardized framework for understanding and evaluating LLM memory systems across heterogeneous setups, enabling fair comparisons and systematic governance of memory operations.

Method: Proposes a four-part memory taxonomy (parametric, contextual, external, procedural/episodic) with a memory quadruple framework, three-setting evaluation protocol, layered evaluation approach, and DMM Gov system for memory governance.

Result: Develops a comprehensive coordinate system that integrates temporal governance, leakage auditing, uncertainty reporting, and memory updating/forgetting mechanisms with auditable loops.

Conclusion: The framework provides a reproducible, comparable, and governable system for LLM memory research and deployment, with four testable propositions to guide future work.

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [38] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking is a 560B parameter open-source MoE model that achieves state-of-the-art reasoning performance through a novel training process combining CoT cold-start and large-scale RL with domain-parallel optimization.


<details>
  <summary>Details</summary>
Motivation: To create an efficient large-scale reasoning model that excels in both formal and agentic reasoning tasks while being more computationally efficient than existing approaches.

Method: Uses a cold-start training with long Chain-of-Thought data, followed by domain-parallel training that decouples optimization across STEM, Code, and Agentic domains, then fuses experts into a single model using the DORA RL framework for 3x training speedup.

Result: Achieves SOTA performance among open-source models on complex reasoning tasks, with 64.5% reduction in token consumption on AIME-25 while maintaining accuracy.

Conclusion: The model demonstrates exceptional efficiency in agentic reasoning and is released to advance reasoning systems and agentic AI research.

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [39] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: This paper presents a systematic investigation of Visual Spatial Reasoning (VSR) in Vision-Language Models, categorizing spatial intelligence into three capability levels and introducing SIBench, a comprehensive benchmark with 20 datasets across 23 tasks.


<details>
  <summary>Details</summary>
Motivation: Visual Spatial Reasoning is a critical human cognitive ability essential for advancing embodied intelligence and autonomous systems, but current VLMs struggle with representing and reasoning over 3D space.

Method: The study reviews existing VSR methodologies across input modalities, model architectures, training strategies, and reasoning mechanisms, and creates SIBench benchmark with systematic categorization of spatial intelligence capabilities.

Result: Experiments with state-of-the-art VLMs reveal a significant gap between perception and reasoning - models perform well on basic perceptual tasks but consistently underperform in understanding and planning tasks, especially in numerical estimation, multi-view reasoning, temporal dynamics, and spatial imagination.

Conclusion: There remain substantial challenges in achieving spatial intelligence, but this study provides both a systematic roadmap and comprehensive benchmark to guide future research in visual spatial reasoning.

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [40] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: DEAL is a novel framework that integrates Low-Rank Adaptation (LoRA) with continuous fine-tuning to address catastrophic forgetting and suboptimal data efficiency in LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Conventional fine-tuning approaches suffer from catastrophic forgetting and poor data efficiency, limiting their real-world applicability despite the importance of fine-tuning for adapting LLMs to specific tasks.

Method: DEAL integrates LoRA with a continuous fine-tuning strategy, incorporating knowledge retention and adaptive parameter update modules to mitigate limitations of existing FT methods while maintaining efficiency in privacy-preserving settings.

Result: Experiments on 15 diverse datasets show DEAL consistently outperforms baseline methods with substantial gains in task accuracy and resource efficiency.

Conclusion: The approach demonstrates potential to advance continual adaptation in LLMs by enhancing task performance while improving resource efficiency.

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [41] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: This paper presents the first comprehensive survey on hallucinations in LLM-based agents, analyzing their workflow, proposing a taxonomy of hallucination types, examining triggering causes, and summarizing mitigation approaches.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents are increasingly deployed in real-world applications but remain vulnerable to hallucinations that undermine system reliability, requiring systematic understanding and consolidation of recent advances.

Method: The authors analyze the complete workflow of LLM-based agents, propose a new taxonomy of hallucination types occurring at different stages, examine eighteen triggering causes, and review existing studies on mitigation and detection approaches.

Result: The survey provides a comprehensive framework for understanding agent hallucinations, including their classification, underlying causes, and current mitigation strategies.

Conclusion: This work aims to inspire further research on addressing hallucinations in LLM-based agents to develop more robust and reliable agent systems.

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [42] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: This paper investigates whether LLMs can generate effective explanations from an interpretable recommendation model, using user-centered evaluation with 326 participants to assess explanation quality across multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: Many explainable AI works rely on automatic metrics that fail to capture users' actual needs and perceptions, so the authors aim to adopt a more user-centered approach to evaluate explanation quality.

Method: The study uses constrained matrix factorization with explicit user type representations, then translates model outputs into natural language explanations using carefully designed LLM prompts. Multiple explanation types are generated from the same model by varying LLM input information.

Result: All explanation types were generally well received with moderate statistical differences between strategies. User comments provided complementary insights beyond quantitative results.

Conclusion: LLMs can effectively generate user-facing explanations from interpretable recommendation models, and user-centered evaluation provides valuable insights into explanation quality that automatic metrics may miss.

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [43] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: Comparison of four remaining time prediction approaches in a real-life logistics warehouse process using a novel event log with 169,523 traces


<details>
  <summary>Details</summary>
Motivation: To forecast the remaining time until process completion in predictive process monitoring, specifically for logistics operations

Method: Evaluated four different remaining time prediction approaches including deep learning models and shallow methods like conventional boosting techniques

Result: Deep learning models achieved highest accuracy, but shallow methods like boosting achieved competitive accuracy with significantly fewer computational resources

Conclusion: While deep learning provides best accuracy, shallow methods offer competitive performance with better computational efficiency for remaining time prediction in process mining

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [44] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: The paper introduces nested concepts of Landmarks, Monuments, and Beacons for automated decomposition and evaluation of procedurally generated content in games, aiming to bridge humanities and technical game research.


<details>
  <summary>Details</summary>
Motivation: Algorithmic evaluation of procedurally generated content struggles to find metrics that align with human experience, particularly for composite artefacts. Current methods lack player-centric perspectives.

Method: Drawing on Games Studies and Game AI research, the authors introduce three nested concepts based on perceivability, evocativeness, and Call to Action from a player-centric perspective. These concepts are designed to be generic across game genres and can be evaluated using existing techniques.

Result: The proposed framework provides a path towards fully automated decomposition of PCG and evaluation of salient sub-components, with applicability beyond mixed-initiative PCG and compositional PCG.

Conclusion: This approach creates a connection between humanities and technical game research, enabling better computational PCG evaluation through player-centric concepts that can be automatically identified and assessed.

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [45] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: The paper introduces a framework using observable sources as auxiliary variables for causal representation learning, enabling identification of latent variables up to subspace-wise transformations and permutations with volume-preserving encoders.


<details>
  <summary>Details</summary>
Motivation: Prior works limit auxiliary variables to be external to the mixing function, but system-driving latent factors can sometimes be observed or extracted from data, which could facilitate identification of latent variables.

Method: Proposes using observable sources as auxiliary variables, employs volume-preserving encoders for identification, and introduces a variable-selection scheme when multiple auxiliary variables are available to maximize recoverability given knowledge of the latent causal graph.

Result: The framework can identify entire latent variables up to subspace-wise transformations and permutations. Experiments on synthetic graph and image data demonstrate the effectiveness of the approach.

Conclusion: This work extends the boundaries of current causal representation learning approaches by leveraging observable sources as auxiliary variables, providing a more flexible framework for latent variable identification.

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [46] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: CoPiC reduces LLM query costs by generating diverse planning programs and using a domain-adaptive critic to select plans aligned with long-term rewards, achieving higher success rates with fewer queries.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based planners require frequent queries for iterative refinement, which is costly and focuses on short-term feedback rather than long-term rewards.

Method: Generate diverse high-level planning programs with LLMs, then use a trained domain-adaptive critic to evaluate and select the best plan based on long-term alignment.

Result: Outperforms AdaPlanner and Reflexion with 23.33% higher success rate and 91.27% reduction in query costs across ALFWorld, NetHack, and StarCraft II.

Conclusion: CoPiC effectively balances planning quality and efficiency by separating plan generation from evaluation, reducing reliance on costly LLM queries.

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [47] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit is a novel multi-agent system initialization method that optimizes agent team structure through multi-round interactions, reflections, and balanced team selection using Pareto principles to improve collaboration and system performance.


<details>
  <summary>Details</summary>
Motivation: Existing MAS initialization methods fail to adequately address the collaborative needs of agents in subsequent stages, leading to suboptimal system efficiency and effectiveness.

Method: AgentInit incorporates multi-round interactions and reflections between agents during generation, uses a Natural Language to Format mechanism for consistency, and applies balanced team selection strategies based on Pareto principles to jointly consider diversity and task relevance.

Result: AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving performance improvements of up to 1.2x and 1.6x respectively, while significantly reducing token consumption.

Conclusion: The method demonstrates strong transferability to similar tasks and verifies the effectiveness of its key components, establishing AgentInit as a reliable and adaptable MAS initialization approach.

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [48] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: This paper demonstrates that cross-cultural transfer of commonsense reasoning is possible using lightweight alignment methods, showing that even small amounts of culture-specific examples from one Arab country can significantly improve performance in other Arab countries.


<details>
  <summary>Details</summary>
Motivation: LLMs often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. The potential for cross-cultural transfer - using alignment in one culture to improve performance in others - remains underexplored, particularly in the Arab world.

Method: Used a culturally grounded commonsense reasoning dataset covering 13 Arab countries. Evaluated lightweight alignment methods including in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization.

Result: Just 12 culture-specific examples from one country improved performance in others by 10% on average within multilingual models. Out-of-culture demonstrations from Indonesia and US contexts matched or surpassed in-culture alignment for MCQ reasoning.

Conclusion: Efficient cross-cultural alignment is possible and offers a promising approach to adapt LLMs to low-resource cultural settings, demonstrating cultural commonsense transferability beyond the Arab world.

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: Machine learning models can detect comparative regularity in prime number distributions on Ulam spirals, with better performance at higher number regions (500m) than lower regions (25m), suggesting machine learning as a new experimental tool for number theory.


<details>
  <summary>Details</summary>
Motivation: Prime numbers exhibit both deterministic definition and statistical randomness, creating an opportunity to use machine learning as an experimental instrument to measure regularity patterns in prime distributions across different regions of the Ulam spiral.

Method: Using image-focused machine learning models trained on blocks extracted from different regions of the Ulam spiral (specifically regions around 500m vs below 25m) to classify prime number patterns and analyze precision/recall differences.

Result: Models trained on higher number regions (500m) outperform those on lower regions (25m), indicating more learnable order at higher magnitudes. Precision/recall analysis shows different classification strategies: focusing on prime pattern identification at lower numbers and composite elimination at higher numbers.

Conclusion: Machine learning can serve as a new experimental instrument for number theory, potentially useful for investigating prime patterns in cryptography, with findings supporting number theory conjectures about diminishing noise and increasing regularity in prime distributions at higher orders of magnitude.

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [50] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: This paper introduces a Wasserstein-based framework for data valuation and selection in Federated Learning marketplaces, enabling model performance prediction and compatibility assessment while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in data valuation and selection from heterogeneous sources in FL setups, which are crucial for establishing trustworthy data marketplaces that enhance data reusability and ensure traceability of data ownership.

Method: Proposes a comprehensive framework with a Wasserstein-based estimator that predicts model performance across unseen data combinations and reveals data-algorithm compatibility. Includes a distributed method to approximate Wasserstein distance without raw data access, and leverages neural scaling law for performance extrapolation.

Result: Extensive experiments across diverse scenarios (label skew, mislabeled, and unlabeled sources) show the approach consistently identifies high-performing data combinations without requiring full-scale training.

Conclusion: The framework paves the way for more reliable FL-based model marketplaces by enabling effective data selection while maintaining privacy and addressing data heterogeneity challenges.

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [51] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: BULL-ODE compares Neural ODE (fully learned) vs Universal Differential Equation (physics-informed) for forecasting bullwhip effect in inventory dynamics, showing UDE performs better in structured demand regimes while NODE excels in heavy-tailed scenarios.


<details>
  <summary>Details</summary>
Motivation: To quantify when structural bias helps or hurts forecasting of bullwhip effect in supply chains under different demand regimes, addressing uncertainty about whether domain constraints improve or hinder forecasting performance.

Method: Uses single-echelon testbed with three demand regimes (AR(1), i.i.d. Gaussian, heavy-tailed lognormal). Compares Neural ODE (fully learned) against physics-informed Universal Differential Equation that preserves conservation and order-up-to structure while learning residual policy terms.

Result: UDE generalizes better in structured regimes: inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96 to 0.95 under Gaussian demand. NODE performs better under heavy-tailed lognormal shocks. UDE remains stable but underreacts to rare spikes.

Conclusion: Enforce structure when noise is light-tailed or temporally correlated; relax structure when extreme events dominate. Provides guidance for hybrid modeling: enforce known structure when conservation laws dominate, relax structure to capture rare events.

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [52] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: A model-based transfer learning approach using neural network surrogate models to enable knowledge transfer between similar bridges for scalable structural monitoring.


<details>
  <summary>Details</summary>
Motivation: The growing use of permanent monitoring systems creates scalability challenges for managing large bridge networks, requiring efficient tracking and comparison of long-term behavior across multiple structures.

Method: Proposes neural network surrogate models that capture shared damage mechanisms, allowing models trained on one bridge to be adapted to similar bridges. Validated using real data from two bridges and integrated into Bayesian inference framework for continuous damage assessment.

Result: The transferred model showed high sensitivity to damage location, severity, and extent when applied to monitoring data from similar bridges.

Conclusion: This approach enables cross-structure knowledge transfer, enhances real-time monitoring capabilities, and promotes smart monitoring strategies for improved network-level resilience.

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [53] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: AdaMixT is a novel transformer architecture for multivariate time series forecasting that uses adaptive multi-scale feature fusion with expert networks and dynamic gating to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting approaches rely on predefined single-scale patches or lack effective multi-scale feature fusion mechanisms, limiting their ability to capture complex temporal patterns and achieve good generalizability.

Method: AdaMixT introduces various patches and leverages both General Pre-trained Models (GPM) and Domain-specific Models (DSM) for multi-scale feature extraction. It incorporates a gating network that dynamically allocates weights among different experts for adaptive multi-scale fusion.

Result: Comprehensive experiments on eight benchmarks (Weather, Traffic, Electricity, ILI, and four ETT datasets) consistently demonstrate AdaMixT's effectiveness in real-world scenarios.

Conclusion: AdaMixT addresses the limitations of existing approaches by enabling adaptive multi-scale feature fusion through expert transformers and dynamic gating, leading to improved forecasting performance across diverse real-world datasets.

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [54] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE is an open-source modular framework that uses LLMs in a feedback loop for algorithmic solution generation, with roles like generator, analyst, and evaluator.


<details>
  <summary>Details</summary>
Motivation: To provide a transparent and extensible platform that abstracts prompt design and model management complexity, enabling researchers and practitioners to co-design algorithms across domains.

Method: Integrates generation, testing, analysis, and evaluation into a reproducible feedback loop, orchestrating multiple LLMs in complementary roles.

Result: EASE offers full user control over error handling, analysis, and quality assessment while supporting iterative algorithmic solution evolution.

Conclusion: The framework effectively simplifies LLM-based algorithmic development through modular architecture and collaborative model orchestration.

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [55] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: A machine learning pipeline using AIS data achieves 92.15% accuracy in classifying vessel types (cargo, tanker, passenger, high-speed craft, fishing) through trajectory-level features and tree-based models.


<details>
  <summary>Details</summary>
Motivation: Accurate vessel type recognition from AIS tracks is essential for safety oversight and combating illegal, unreported, and unregulated (IUU) activity.

Method: The pipeline processes AIS data from the Bornholm Strait, extracts 31 trajectory features (kinematic, temporal, geospatial, ship-shape), uses grouped train/test splits by MMSI with stratified 5-fold cross-validation, and employs Random Forest with SMOTE for classification.

Result: Random Forest with SMOTE achieved 92.15% accuracy, 94.11% macro-precision, 92.51% macro-recall, and 93.27% macro-F1 on test data. Feature importance identified bridge-position ratio and maximum SOG as most discriminative.

Conclusion: Lightweight features from AIS trajectories enable real-time vessel type classification in straits, with potential improvements through DBSCAN-based trip segmentation and gradient-boosted ensembles.

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [56] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: A patch-based PCA-Net framework that decomposes PDE solution fields into smaller patches for more efficient neural operator learning, reducing computational complexity by 3.7-4x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the significant computational overhead of applying principal component analysis (PCA) to high-dimensional PDE solution fields in neural operator learning.

Method: Proposes two patch-based PCA approaches: (1) local-to-global patch PCA, and (2) local-to-local patch PCA, with refinements including overlapping patches with smoothing filters and CNN-based refinement.

Result: Patch-based PCA significantly reduces computational complexity while maintaining high accuracy, achieving 3.7-4x reduction in end-to-end pipeline processing time compared to global PCA.

Conclusion: Patch-based PCA is a promising technique for efficient operator learning in PDE-based systems, offering a good trade-off between computational efficiency and reconstruction accuracy.

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [57] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: A novel context optimization framework that integrates subspace representation learning with prompt tuning for improved out-of-distribution detection in vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning-based OOD methods rely solely on softmax probabilities, overlooking the rich discriminative potential of feature embeddings learned by VLMs trained on millions of samples.

Method: Proposes a CoOp-based framework that projects ID features into a subspace spanned by prompt vectors while projecting ID-irrelevant features into an orthogonal null space, using an end-to-end learning criterion.

Result: Experiments on real-world datasets showcase the effectiveness of the approach.

Conclusion: The proposed framework improves ID-OOD separability and achieves strong OOD detection performance while maintaining high ID classification accuracy.

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [58] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: This study presents the first comprehensive comparison of AI approaches for automated antepartum CTG analysis, showing that fine-tuned LLMs outperform both foundation models and domain-specific methods.


<details>
  <summary>Details</summary>
Motivation: Electronic fetal monitoring (CTG) interpretation faces challenges due to subjective clinical assessment and variability in diagnostic accuracy. The potential of foundation models and LLMs for CTG analysis remains underexplored despite their success in other healthcare domains.

Method: Systematic comparison of time-series foundation models and LLMs against established CTG-specific architectures using over 500 CTG recordings of varying durations reflecting real-world clinical scenarios.

Result: Fine-tuned LLMs achieved superior performance compared to both foundation models and domain-specific approaches for CTG interpretation.

Conclusion: Fine-tuned LLMs offer a promising alternative pathway for clinical CTG interpretation, providing critical insights for future AI development in prenatal care.

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [59] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: This paper proposes a DPU-assisted framework using BlueField-3 Data Processing Units to detect and mitigate load imbalance in multi-node tensor-parallel inference of large language models, addressing throughput degradation and latency spikes during autoregressive decode phases.


<details>
  <summary>Details</summary>
Motivation: Autoregressive inference in large transformer-based language models faces runtime efficiency challenges due to load imbalance across GPU shards during decode phase, causing throughput degradation and latency spikes in multi-GPU execution.

Method: The study leverages BlueField-3 DPUs to offload monitoring tasks, analyze GPU telemetry and inter-node communication patterns, and provide actionable feedback to inference controllers and schedulers for real-time load imbalance detection and mitigation.

Result: The paper identifies specific skews, imbalances, and pathological conditions that arise in multi-GPU execution of LLM tensor computing during both training and inference, and assesses their impact on computational performance.

Conclusion: The DPU-assisted framework enables effective tracking and potential mitigation of load imbalance issues from the network level, offering a solution to improve runtime efficiency in large-scale LLM inference deployments.

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [60] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: Proposes a Spatial Balance Attention block for spatiotemporal forecasting that balances local spatial proximity and global correlation through intra-subgraph and inter-subgraph attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing spatial proximity constraints with the need to capture global spatial correlations in spatiotemporal forecasting tasks.

Method: Partitions spatial graphs into subgraphs, uses Intra-subgraph Attention for local spatial correlation within subgraphs, and Inter-subgraph Attention for global correlation between subgraphs. Builds a multiscale model by progressively increasing subgraph scales.

Result: Achieves performance improvements up to 7.7% over baseline methods with low running costs on real-world spatiotemporal datasets from medium to large sizes.

Conclusion: The proposed model is scalable, produces structured spatial correlation, easy to implement, and demonstrates superior efficacy and efficiency compared to existing models.

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [61] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: Amortized Latent Steering (ALS) is a method that replaces expensive iterative test-time optimization with a single pre-computed steering vector, achieving 2-5x speedup while maintaining or improving accuracy on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Test-time optimization methods are computationally expensive and impractical for production deployment due to high inference costs from iterative refinement and multiple backward passes.

Method: ALS computes the mean difference between hidden states from successful vs unsuccessful generations offline, then applies this single steering vector during inference to nudge activations back toward the success manifold when decoding drifts.

Result: Across GSM8K and MATH-500 benchmarks, ALS achieves 2-5x speedup over iterative methods while matching or surpassing greedy Chain-of-Thought and Self-Consistency baselines, with up to 101% improvement in efficiency-accuracy trade-off.

Conclusion: Much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment by eliminating expensive per-query optimization loops.

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [62] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: Machine learning approach using Bayesian statistics to create adaptive digital interfaces that model individual user browsing behavior through online incremental learning


<details>
  <summary>Details</summary>
Motivation: To design interfaces that dynamically adapt to individual users' habits rather than group preferences, improving user experience by helping them navigate interfaces more effectively

Method: Bayesian statistical modeling of users' browsing behavior with online incremental learning that preserves prior knowledge while learning new tasks, generating graphical task models with usage statistics

Result: Simulations demonstrate the approach's effectiveness in both stationary and non-stationary environments, showing reliable predictions even with limited data

Conclusion: This research enables adaptive systems that enhance user experience by providing personalized interface navigation support based on individual usage patterns

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [63] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: This paper extends L-SGD (a lightweight SGD variant) from Arm Cortex-M to RISC-V MCUs for on-device training, addressing the performance gap due to lack of FPUs in RISC-V by introducing an 8-bit quantized version that reduces memory usage by 4x and speeds up training by 2.2x with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Enable on-device training for IoT devices using RISC-V MCUs, which lack GPU/accelerator support and have privacy/connectivity concerns with cloud-based training. Federated Learning requires efficient algorithms, but RISC-V lacks robust on-device training support.

Method: Extended L-SGD to RISC-V MCUs, evaluated with 32-bit floating-point arithmetic, then introduced an 8-bit quantized version of L-SGD specifically for RISC-V to overcome FPU limitations.

Result: The 8-bit quantized L-SGD achieved nearly 4x reduction in memory usage and 2.2x speedup in training time compared to 32-bit version, with negligible accuracy degradation.

Conclusion: Quantized L-SGD enables efficient on-device training on RISC-V MCUs, making them viable for Federated Learning applications despite hardware limitations, with significant performance improvements over standard floating-point implementations.

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [64] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL is an online reinforcement learning framework for mobile GUI agents that addresses challenges in RL training through difficulty-adaptive strategies and reward adjustments, achieving state-of-the-art performance on mobile task benchmarks.


<details>
  <summary>Details</summary>
Motivation: Developing effective mobile GUI agents with RL is challenging due to heavy-tailed task difficulty distributions and inefficient large-scale environment sampling. The paper aims to overcome these limitations to create more robust mobile agents.

Method: The framework uses ADAGRPO algorithm with difficulty-adaptive positive replay, failure curriculum filtering, and shortest path reward adjustment. These strategies adapt to task difficulties and stabilize RL training while improving sample efficiency.

Result: MOBILERL applied to Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base models achieves state-of-the-art success rates: 75.8% on AndroidWorld and 46.8% on AndroidLab benchmarks.

Conclusion: MOBILERL successfully enhances GUI agents for mobile environments through adaptive RL strategies, demonstrating strong performance across diverse mobile apps and tasks, and has been adopted in AutoGLM products.

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [65] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: CoCoGen is a framework that addresses economic competition and statistical heterogeneity in cross-silo federated learning using generative AI and game theory to optimize collaborative training while maximizing social welfare.


<details>
  <summary>Details</summary>
Motivation: Organizations in cross-silo federated learning face economic competition as market rivals, making them hesitant to participate due to potential utility loss. The combined effects of statistical heterogeneity and inter-organizational competition on organizational behavior and social welfare are underexplored.

Method: CoCoGen uses generative AI and potential game theory to model competition and statistical heterogeneity through learning performance and utility-based formulations. It models each training round as a weighted potential game and derives GenAI-based data generation strategies.

Result: Experiments on Fashion-MNIST show how varying heterogeneity and competition levels affect organizational behavior, and demonstrate that CoCoGen consistently outperforms baseline methods.

Conclusion: CoCoGen effectively addresses the challenges of economic competition and statistical heterogeneity in federated learning, providing optimized collaborative training that maximizes social welfare through generative AI and game-theoretic approaches.

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [66] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: Supervised machine learning applied to predict coffee ratings using text and numerical features from user reviews, with ensemble methods showing best performance.


<details>
  <summary>Details</summary>
Motivation: To develop a data-driven approach for coffee quality assessment that complements traditional expert cupping by leveraging user reviews and machine learning.

Method: Used text preprocessing, TF-IDF feature extraction, SelectKBest feature selection, and trained six ML models (Decision Tree, KNN, MLP, Random Forest, Extra Trees, XGBoost) with hyperparameter optimization.

Result: Ensemble methods (Extra Trees, Random Forest, XGBoost) and Multi-layer Perceptron outperformed simpler classifiers (Decision Trees, KNN) on F1-score, G-mean, and AUC metrics.

Conclusion: Rigorous feature selection and hyperparameter tuning are essential for building robust predictive systems for sensory product evaluation, offering a complementary approach to traditional expert assessment.

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [67] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: NurseSchedRL is a reinforcement learning framework for nurse-patient assignment that addresses skill heterogeneity, patient acuity, staff fatigue, and continuity of care constraints more effectively than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Healthcare systems need efficient nursing resource allocation that accounts for multiple dynamic constraints, but traditional optimization and heuristic scheduling methods struggle with these complex environments.

Method: Uses Proximal Policy Optimization (PPO) with constrained action masking, structured state encoding, and attention-based representations of skills, fatigue, and geographical context to ensure assignments respect real-world constraints.

Result: In simulations with realistic data, NurseSchedRL achieves improved scheduling efficiency, better skill-patient alignment, and reduced fatigue compared to baseline heuristic and unconstrained RL approaches.

Conclusion: Reinforcement learning shows strong potential for decision support in complex healthcare workforce management by dynamically adapting to patient arrivals and nurse availability while respecting multiple constraints.

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [68] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: Federated Learning (FL) is evaluated for anomaly detection in EV charging stations, addressing practical challenges like system heterogeneity and non-IID data. FedAvgM outperforms FedAvg in heterogeneous settings, showing FL can handle heterogeneity without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: Securing IoT-based electric vehicle charging stations against cyber threats is critical, but centralized IDS raise privacy concerns. FL offers a privacy-preserving alternative, but current evaluations overlook practical challenges like system heterogeneity and non-IID data.

Method: Conducted experiments evaluating FL performance for anomaly detection in EV charging stations under system and data heterogeneity. Used FedAvg and FedAvgM optimization approaches to analyze their effectiveness.

Result: Under IID settings, FedAvg achieves superior performance to centralized models. However, performance degrades with non-IID data and system heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous settings, showing better convergence and higher anomaly detection accuracy.

Conclusion: FL can handle heterogeneity in IoT-based EVCS without significant performance loss, with FedAvgM as a promising solution for robust, privacy-preserving EVCS security.

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [69] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: Safe-SAIL is a framework that uses Sparse Autoencoders (SAEs) to interpret safety-related features in LLMs, addressing the limitations of current safety research by systematically identifying and explaining safety-critical neurons to better understand and mitigate high-risk behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing safety research on LLMs focuses on evaluating outputs or specific tasks, which fails to address broader undefined risks. Current SAE applications lack fine-grained safety concept interpretation, making them inadequate for analyzing safety-critical behaviors like toxic response generation and regulation violations.

Method: The proposed Safe-SAIL framework systematically identifies SAEs with the best concept-specific interpretability, explains safety-related neurons, and introduces efficient strategies to scale up the interpretation process. It includes a comprehensive toolkit with SAE checkpoints and human-readable neuron explanations.

Result: The framework enables extraction of a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors in LLMs, overcoming challenges of identifying optimal SAEs and reducing the cost of detailed feature explanation.

Conclusion: Safe-SAIL advances mechanistic understanding of safety domains in LLMs by providing interpretable safety features, supporting empirical analysis of safety risks, and promoting research on LLM safety through the release of comprehensive toolkits.

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [70] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: Proposes a Gauss-Hermite quadrature approach to decouple nested epistemic and aleatory uncertainties in machine learning surrogate models for more accurate reliability analysis.


<details>
  <summary>Details</summary>
Motivation: Machine learning surrogates introduce epistemic uncertainty from model approximation errors that couples with aleatory uncertainty in inputs, compromising reliability prediction accuracy.

Method: Uses Gauss-Hermite quadrature to decouple uncertainties, evaluates conditional failure probabilities under aleatory uncertainty using FORM/SORM, then integrates across epistemic uncertainty realizations.

Result: Three examples demonstrate the approach maintains computational efficiency while yielding more trustworthy predictions than traditional methods ignoring model uncertainty.

Conclusion: The proposed method effectively handles nested uncertainties in surrogate-based reliability analysis, providing more accurate and trustworthy predictions than conventional approaches.

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [71] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: This paper proposes a metro transfer passenger flow prediction model combining STL decomposition and GRU neural network, which significantly improves prediction accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate transfer passenger flow prediction is crucial for optimizing metro operation plans and improving transportation efficiency in intelligent transportation systems.

Method: The model uses STL time series decomposition to separate transfer passenger flow into trend, periodic, and residual components, applies 3σ principle for outlier handling, and uses GRU neural network for prediction based on processed metro card data and travel path identification.

Result: The STL-GRU model outperformed LSTM, GRU, and STL-LSTM models, reducing MAPE by at least 2.3% on weekdays, 1.36% on Fridays, and 6.42% on rest days.

Conclusion: The proposed STL-GRU combined prediction model effectively improves transfer passenger flow prediction accuracy and provides reliable support for intelligent metro operation decisions.

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [72] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: Transformer weight matrices in physics applications appear random and lack direct correlation with the underlying physical structure, suggesting ML and scientific methods may be complementary but with limited explainability.


<details>
  <summary>Details</summary>
Motivation: To investigate whether transformer-based ML applications in physics reveal meaningful connections between network parameters and physical structures, or if they represent a fundamentally different approach to knowledge acquisition.

Method: Analysis of weight matrices from transformer-based ML applications solving two representative physical problems, examining their characteristics and comparing with physical/mathematical structures.

Result: Weight matrices show random-like character with no directly recognizable link to the physical problem structure, though parallels with path-integration techniques may explain this randomness.

Conclusion: ML and scientific methods represent distinct but potentially complementary paths to knowledge, but strict explainability remains elusive, highlighting risks of acquiring knowledge without true insight.

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [73] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: MoE-CL is a parameter-efficient adversarial mixture-of-experts framework for continual instruction tuning of LLMs that addresses catastrophic forgetting through dedicated task-specific experts and a shared expert with task-aware discrimination.


<details>
  <summary>Details</summary>
Motivation: Existing continual learning approaches suffer from catastrophic forgetting when training on new tasks, degrading performance on earlier tasks due to overfitting to new distributions and weakened generalization.

Method: Uses dual-expert design: dedicated LoRA expert per task for task-specific knowledge preservation, and shared LoRA expert for cross-task transfer with a task-aware discriminator in a GAN framework to filter task-irrelevant noise.

Result: Extensive experiments on MTL5 and Tencent3 benchmarks show effectiveness. Real-world A/B testing on Tencent Video platform reduced manual review costs by 15.3%.

Conclusion: MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical, supporting self-evolution of LLMs in dynamic environments.

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [74] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: This paper addresses privacy risks in gradient tracking for distributed optimization and proposes a weighted gradient tracking algorithm with decaying weights to eliminate privacy leakage while maintaining convergence.


<details>
  <summary>Details</summary>
Motivation: To protect agents' private information from attackers during distributed optimization processes, as gradient tracking techniques used to improve convergence rates inherently create privacy leakage risks.

Method: Proposes a weighted gradient tracking distributed privacy-preserving algorithm that uses decaying weight factors to eliminate privacy leakage in gradient tracking, with convergence analysis under time-varying heterogeneous step sizes.

Result: The algorithm converges precisely to the optimal solution under mild assumptions, validated through numerical simulations including distributed estimation problems and CNN training.

Conclusion: The proposed weighted gradient tracking algorithm successfully eliminates privacy leakage risks while maintaining convergence performance, providing an effective privacy-preserving solution for distributed optimization.

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [75] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: Proposes SDGF network using static-dynamic graph fusion to capture multi-scale inter-series correlations in multivariate time series forecasting


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to model complex multi-scale dependencies and evolving inter-series correlations in multivariate time series

Method: Dual-path graph structure learning with static graph (prior knowledge) and dynamic graph (multi-level wavelet decomposition), fused via attention-gated module and processed with multi-kernel dilated convolutional network

Result: Comprehensive experiments on real-world benchmark datasets demonstrate the model's effectiveness

Conclusion: SDGF successfully addresses the challenge of capturing multi-scale inter-series correlations through static-dynamic graph fusion

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [76] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: This paper presents a large-scale dataset and systematic analysis of how structural configurations affect LLM performance, using data mining and mechanistic interpretability techniques.


<details>
  <summary>Details</summary>
Motivation: Despite rapid growth in LLM capabilities, there's a lack of systematic, data-driven research on how structural configurations impact performance. The authors aim to fill this gap with empirical evidence.

Method: Created a large-scale dataset of diverse open-source LLM structures and their benchmark performance. Conducted systematic data mining analysis and used mechanistic interpretability techniques to validate findings.

Result: The study provides data-driven insights into the relationship between structural configurations and LLM performance across multiple benchmarks.

Conclusion: This work offers guidance for targeted development and application of future LLMs by quantifying how structural choices impact performance, with the dataset being publicly released.

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [77] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: LoRALib is a unified benchmark for LoRA-MoE methods that standardizes datasets, hyperparameters, and evaluation across 40 tasks and 17 models, revealing LoRAMoE as the best performer and showing task-relevant LoRA selection improves MoE performance.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA-MoE methods lack standardized evaluation frameworks, making fair comparisons difficult due to inconsistent models, datasets, hyperparameters, and evaluation methods across different approaches.

Method: Created LoRALib benchmark by standardizing 40 downstream tasks into unified format, fine-tuning with same hyperparameters to obtain 680 LoRA modules across 17 model architectures, then conducting large-scale experiments on 3 representative LoRA-MoE methods using OpenCompass testing tool.

Result: Extensive experiments show LoRAMoE performs best among the tested methods, and prioritizing LoRAs relevant to the target task can further improve MoE performance.

Conclusion: The findings provide standardized benchmarks for fair comparison of LoRA-MoE methods and demonstrate the importance of task-relevant LoRA selection, which should inspire future work in parameter-efficient fine-tuning.

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [78] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: RIPLM is a new rank-faithful and variance-adaptive algorithm for sleeping experts that operates directly in rank-induced Plackett-Luce parameter space.


<details>
  <summary>Details</summary>
Motivation: To create an algorithm that preserves the structural equivalence between rank and distributional benchmarks while being both rank-faithful and variance-adaptive in sleeping experts settings.

Method: Uses rank-induced Plackett-Luce mirror descent (RIPLM) that updates directly in rank-induced PL parameterization, ensuring played distributions remain within rank-induced distributions at every round.

Result: RIPLM is the first algorithm that simultaneously achieves rank-faithfulness and variance-adaptivity in the sleeping experts setting.

Conclusion: The RIPLM algorithm successfully bridges the gap between rank and distributional benchmarks while maintaining desirable properties for sleeping experts problems.

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [79] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: FOLD-SE, a rule-based classifier, balances accuracy and explainability, outperforming FOLD-R++ in binary classification and XGBoost in multi-category classification while providing interpretable rule sets.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between accuracy and explainability in ML models by developing rule-based algorithms that offer transparent predictions without significant performance loss.

Method: Compare FOLD-SE and FOLD-R++ for binary classification, and FOLD-SE against XGBoost for multi-category classification using accuracy, F1 scores, and processing time as metrics.

Result: FOLD-SE outperforms FOLD-R++ in binary classification with fewer rules and minor accuracy loss; it is more precise and efficient than XGBoost in multi-category tasks while generating interpretable rules.

Conclusion: Rule-based approaches like FOLD-SE effectively bridge the gap between explainability and performance, making them viable alternatives to black-box models in classification tasks.

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [80] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: A novel machine learning framework combining predictive modeling with gene-agnostic pathway mapping to identify high-risk individuals and therapeutic targets for type 2 diabetes in genetically predisposed populations like Pima Indians.


<details>
  <summary>Details</summary>
Motivation: Metabolic disorders like type 2 diabetes disproportionately affect genetically predisposed populations such as Pima Indians, creating a significant global health burden that requires precision medicine approaches for early detection and targeted intervention.

Method: Used logistic regression and t-tests on Pima Indian dataset to identify T2DM predictors (78.43% accuracy), combined with pathway mapping strategy linking predictors to insulin signaling, AMPK, and PPAR pathways without requiring molecular data.

Result: Developed an interpretable ML framework that successfully identified high-risk individuals and proposed therapeutic strategies including dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1 modulators, and phytochemical interventions validated through pathway enrichment analyses.

Conclusion: The framework advances precision medicine by providing scalable, interpretable solutions for early detection and targeted intervention in metabolic disorders, with key contributions in ML-based risk prediction, gene-agnostic pathway mapping, and population-specific therapeutic strategy identification.

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [81] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT is an AI-powered pipeline that automatically reconstructs individual patient data from Kaplan-Meier plots, eliminating manual digitization and enabling scalable evidence synthesis in clinical research.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for reconstructing IPD from KM plots rely on manual digitization, which is error-prone and lacks scalability, limiting evidence synthesis in clinical research.

Method: KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered by GPT-5, and iterative reconstruction algorithms with a hybrid reasoning architecture that automates conversion of unstructured information into structured data flows.

Result: KM-GPT demonstrated superior accuracy on both synthetic and real-world datasets and successfully reconstructed IPD for a meta-analysis of gastric cancer immunotherapy trials, facilitating evidence synthesis and biomarker-based subgroup analyses.

Conclusion: KM-GPT transforms clinical research by automating traditionally manual processes, providing a scalable web-based solution that enables more informed downstream analyses and supports evidence-based decision-making.

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [82] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: AdaSTI is a novel spatio-temporal imputation method using conditional diffusion models that addresses error accumulation and dependency variability issues in previous approaches, achieving up to 46.4% error reduction.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based spatio-temporal imputation methods suffer from error accumulation when extracting conditional dependencies and ignore how these dependencies vary across different diffusion steps in noisy data.

Method: Proposes AdaSTI with three key components: BiS4PI network for pre-imputation using bi-directional S4 model, Spatio-Temporal Conditionalizer (STC) network to extract conditional information, and Noise-Aware Spatio-Temporal (NAST) network with gated attention to capture varying dependencies across diffusion steps.

Result: Extensive experiments on three real-world datasets show AdaSTI outperforms existing methods in all settings, achieving up to 46.4% reduction in imputation error.

Conclusion: AdaSTI effectively addresses limitations of previous diffusion-based methods by adaptively handling spatio-temporal dependencies across diffusion steps, demonstrating superior performance for spatio-temporal data imputation.

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [83] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: A multi-label classification framework using XGBoost to predict Care Escalation Triggers (respiratory, hemodynamic, renal, neurological deterioration) from first 24 hours of ICU data, achieving F1-scores of 0.62-0.76 and outperforming traditional early warning systems.


<details>
  <summary>Details</summary>
Motivation: Traditional ICU early warning systems like SOFA or MEWS are limited by single-outcome focus and fail to capture multi-dimensional clinical decline, requiring a more comprehensive approach to predict various types of physiological deterioration.

Method: Multi-label classification framework using MIMIC-IV database (85,242 ICU stays) with features from first 24 hours (vital sign aggregates, lab values, demographics) to predict CETs defined by rule-based criteria from hours 24-72. XGBoost was the best performing model among multiple classifiers evaluated.

Result: XGBoost achieved F1-scores: 0.66 (respiratory), 0.72 (hemodynamic), 0.76 (renal), 0.62 (neurologic). Feature analysis showed clinically relevant parameters (respiratory rate, blood pressure, creatinine) were most influential predictors, aligning with clinical CET definitions.

Conclusion: The framework demonstrates practical potential for early, interpretable clinical alerts without requiring complex time-series modeling or NLP, offering a more comprehensive approach than traditional single-outcome warning systems.

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [84] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow is a concept-based interpretability framework for CNNs that traces how concepts emerge and evolve across layers through concept attentions and conceptual pathways.


<details>
  <summary>Details</summary>
Motivation: Existing CNN interpretability approaches overlook the semantic roles of individual filters and the dynamic propagation of concepts across layers, limiting understanding of internal model reasoning.

Method: ConceptFlow uses concept attentions to associate filters with high-level concepts and conceptual pathways derived from a concept transition matrix to trace concept propagation between filters.

Result: Experimental results show ConceptFlow provides semantically meaningful insights into model reasoning and validates the effectiveness of concept attentions and conceptual pathways.

Conclusion: ConceptFlow offers deeper insight into CNN internal logic and supports generation of more faithful, human-aligned explanations by modeling hierarchical conceptual pathways.

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [85] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: A sparse training scheme (STS) for efficient MLLM training using visual token compression and dynamic layer skipping to reduce computational overhead.


<details>
  <summary>Details</summary>
Motivation: Training Multimodal Large Language Models (MLLMs) is inefficient due to long input sequences from multimodal data and low utilization of inter-layer computations.

Method: STS framework with Visual Token Compressor to reduce visual token information load and Layer Dynamic Skipper to dynamically skip unnecessary layers during forward/backward passes.

Result: Extensively evaluated on multiple benchmarks, demonstrating effectiveness and efficiency across diverse MLLM architectures.

Conclusion: The proposed sparse training scheme provides an efficient training framework for MLLMs while maintaining performance.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [86] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS is a novel neural predictor paradigm for Neural Architecture Search that enhances architecture representation learning through global encoding and shared hypernetwork components, achieving state-of-the-art results with significantly fewer training samples.


<details>
  <summary>Details</summary>
Motivation: Traditional neural predictors in NAS exhibit poor generalization due to limited ability to capture intricate relationships among various architectures, and time-intensive performance evaluations impede NAS progress.

Method: HyperNAS consists of two components: a global encoding scheme to capture comprehensive macro-structure information, and a shared hypernetwork as an auxiliary task to enhance investigation of inter-architecture patterns. It uses a dynamic adaptive multi-task loss for training stability.

Result: HyperNAS achieves state-of-the-art results with 97.60% top-1 accuracy on CIFAR-10 and 82.4% top-1 accuracy on ImageNet, using at least 5.0× fewer samples than previous methods.

Conclusion: HyperNAS demonstrates superior performance across five representative search spaces, particularly in few-shot scenarios, making it an effective solution for efficient neural architecture search.

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [87] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM is a foundation model for well-log interpretation that uses multi-stage pretraining on 1200 wells, achieving state-of-the-art performance in porosity estimation and lithology classification through tokenization, self-supervised learning, and few-shot fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Well-log interpretation faces challenges from heterogeneous tool responses, noisy signals, and limited labeled data, requiring more robust and scalable AI solutions for subsurface characterization.

Method: Three-stage approach: (1) tokenization of log patches into geological tokens, (2) self-supervised pretraining with masked-token modeling and stratigraphy-aware contrastive learning, (3) multi-task adaptation with few-shot fine-tuning.

Result: WLFM achieves 0.0041 MSE in porosity estimation and 74.13% accuracy in lithology classification. With fine-tuning (WLFM-Finetune), performance improves to 0.0038 MSE and 78.10% accuracy. The model also exhibits emergent layer-awareness and learns reusable geological vocabulary.

Conclusion: WLFM establishes a scalable, interpretable, and transferable backbone for geological AI with potential for multi-modal integration of logs, seismic, and textual data, though systematic offsets remain in shallow and ultra-deep intervals.

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [88] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: ApexAmphion is a deep learning framework that combines a 6.4B-parameter protein language model with reinforcement learning to design novel antibiotics, achieving 100% hit rate with nanomolar potency in vitro.


<details>
  <summary>Details</summary>
Motivation: Antimicrobial resistance is projected to cause 10 million annual deaths by 2050, creating an urgent need for new antibiotics.

Method: Fine-tunes a protein language model on peptide data, then uses proximal policy optimization with composite rewards combining MIC predictions and physicochemical objectives.

Result: 100% hit rate with low MIC values (nanomolar range), 99/100 compounds showed broad-spectrum activity against clinically relevant bacteria via membrane targeting.

Conclusion: The approach provides a scalable platform for rapid generation of diverse, potent peptide antibiotics with iterative optimization capabilities.

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [89] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5 is an 8B parameter multimodal LLM that achieves state-of-the-art performance with exceptional efficiency, surpassing larger proprietary and open-source models while using significantly less GPU memory and inference time.


<details>
  <summary>Details</summary>
Motivation: Address the training and inference efficiency bottlenecks in multimodal LLMs to make them more accessible and scalable, as current models face challenges with computational resource requirements.

Method: Three core improvements: 1) Unified 3D-Resampler architecture for compact image/video encoding, 2) Unified learning paradigm for document knowledge and text recognition without heavy data engineering, 3) Hybrid reinforcement learning strategy for both short and long reasoning modes.

Result: Surpasses GPT-4o-latest and Qwen2.5-VL 72B in OpenCompass evaluation. On VideoMME benchmark, achieves SOTA performance among models under 30B size with only 46.7% GPU memory cost and 8.7% inference time of Qwen2.5-VL 7B.

Conclusion: MiniCPM-V 4.5 demonstrates that strong multimodal performance can be achieved with remarkable efficiency through architectural innovations and optimized training strategies, making advanced MLLMs more accessible.

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [90] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: This paper explores optimizing activation function shapes using parameterized linear B-splines, achieving significant error rate reductions in FNNs (94%) and CNNs (51%) compared to ReLU-based models, though with increased complexity and latency.


<details>
  <summary>Details</summary>
Motivation: Traditional neural networks use static activation functions like ReLU, tanh, or sigmoid, which may not be optimal. By optimizing activation function shapes, we can create more parameter-efficient and accurate models by assigning better activations to neurons.

Method: The paper presents and compares 9 training methodologies for dual-optimization dynamics in neural networks using parameterized linear B-spline activation functions.

Result: Experiments show up to 94% lower end model error rates in FNNs and 51% lower rates in CNNs compared to traditional ReLU-based models.

Conclusion: While optimized activation functions provide substantial accuracy improvements, these gains come at the cost of additional development and training complexity as well as increased end model latency.

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [91] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: Hybrid RL solver for truck-drone delivery with battery constraints achieves 2.73% better makespan than ALNS and matches neural network performance.


<details>
  <summary>Details</summary>
Motivation: To optimize last-mile delivery with one truck and one drone under explicit battery management constraints, where each drone sortie must satisfy endurance limits and requires recharging between deliveries.

Method: Hybrid reinforcement learning solver combining ALNS-based truck tour optimization with pointer/attention policy for drone scheduling. Uses feasibility masks for endurance and recharge constraints, with exact timeline simulation for makespan computation.

Result: Achieves average makespan of 5.203±0.093 vs 5.349±0.038 for ALNS and 5.208±0.124 for NN - 2.73% better than ALNS and within 0.10% of NN. RL scheduler never underperforms ALNS and ties/beats NN on 2/3 seeds.

Conclusion: The learned scheduler effectively balances truck and wait times to minimize total completion time, demonstrating superior performance over traditional optimization methods while handling complex battery constraints.

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [92] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: DSFT is a Diffusion SFT strategy that improves dLLMs' performance on mathematical and logical tasks by adjusting masking strategy and loss function, achieving 5-10% improvements on math problems and 2% on logical problems.


<details>
  <summary>Details</summary>
Motivation: Current dLLM training methods focus on general knowledge but lack comprehensive understanding of mathematically sensitive and order-sensitive logical tasks, which are challenging for diffusion models.

Method: Proposes DSFT (Diffusion SFT) strategy with adjusted masking strategy and loss function to guide models in understanding mathematical and logical patterns. Can be combined with pre-training, reinforcement learning, and other methods.

Result: Validated on LLaDA and Dream series models, DSFT achieves 5-10% improvement on mathematical problems and approximately 2% improvement on logical problems using small-scale data.

Conclusion: DSFT provides an effective masking approach for learning specific patterns that can be easily combined with other training methods and applied to various dLLMs.

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [93] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: MobiGPT is a foundation model for mobile data forecasting that can handle multiple data types (base station traffic, user app usage, channel quality) using a unified structure with soft-prompt learning and temporal masking mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current mobile data forecasting approaches require customized models for each data type, increasing complexity and deployment costs in large-scale heterogeneous networks. There's a need for a unified foundation model that can handle multiple forecasting tasks efficiently.

Method: Proposes MobiGPT with soft-prompt learning to understand different data types and temporal masking mechanism to guide three forecasting tasks: short-term prediction, long-term prediction, and distribution generation.

Result: Evaluations on real-world datasets with over 100,000 samples show MobiGPT achieves accurate multi-type forecasting with 27.37%, 20.08%, and 7.27% accuracy improvements over existing models, plus superior zero/few-shot performance with over 21.51% improvement in unseen scenarios.

Conclusion: MobiGPT demonstrates strong generalization and transferability as a foundation model for mobile data forecasting, effectively addressing the limitations of customized approaches while supporting diverse optimization scenarios.

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [94] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE is a novel architecture that integrates computational capabilities into neural networks through physically-isolated experts, enabling efficient token-level alternation between computation and reasoning within a single chain of thought.


<details>
  <summary>Details</summary>
Motivation: Current LLMs cannot incorporate high-precision numerical computation as an intrinsic capability, and multi-agent approaches introduce communication overhead and scalability limitations.

Method: PiMoE trains experts, a text-to-computation module, and a router separately, then integrates them endogenously. At inference, the router directs computation and reasoning at token level for iterative alternation.

Result: PiMoE achieves higher accuracy than LLM finetuning and significant improvements in latency, token usage, and GPU energy consumption compared to multi-agent approaches.

Conclusion: PiMoE provides an efficient, interpretable, and scalable paradigm for next-generation scientific and industrial intelligent systems.

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [95] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: FedIA is a federated graph learning framework that addresses domain skew by using a projection-first strategy to denoise client updates before aggregation, achieving stable convergence and higher accuracy with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Federated Graph Learning under domain skew leads to incompatible client representations, making naive aggregation unstable and ineffective due to noisy gradient signals dominated by domain-specific variance.

Method: FedIA employs a two-stage pipeline: (1) server-side top-ρ mask to retain only the most informative gradient coordinates (about 5%), and (2) lightweight influence-regularised momentum weight to suppress outlier clients.

Result: FedIA achieves smoother, more stable convergence and higher final accuracy than nine baselines on both homogeneous (Twitch Gamers) and heterogeneous (Wikipedia) graphs, with no extra uplink traffic and negligible server memory overhead.

Conclusion: The projection-first strategy effectively addresses domain skew in federated graph learning, maintaining optimal convergence rates while being readily deployable due to minimal resource requirements.

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [96] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: SBVR is a novel LLM quantization method that uses Gaussian-like code representation with custom CUDA kernels for fast inference, achieving state-of-the-art performance with 2.21x-3.04x speedup over FP16 models.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods have limitations: RTN-based methods don't account for LLM weights' Gaussian distribution, while codebook-based methods suffer from inefficient memory access patterns that degrade inference speed due to GPU cache limitations.

Method: SBVR maps weight values to non-uniform representation points that follow LLM weights' actual distribution, and uses custom CUDA kernels for direct matrix-vector multiplication in SBVR format without decompression.

Result: SBVR achieves state-of-the-art perplexity and accuracy benchmarks while providing 2.21x-3.04x end-to-end token-generation speedup over naive FP16 models in 4-bit quantization.

Conclusion: SBVR overcomes limitations of existing PTQ methods by combining distribution-aware quantization with hardware-friendly execution, enabling both high compression accuracy and fast inference speeds.

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [97] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: A comprehensive benchmark evaluating LLMs' geospatial route cognition capabilities, revealing significant limitations in route reversal tasks despite high confidence in incorrect answers.


<details>
  <summary>Details</summary>
Motivation: Geospatial cognition capabilities of LLMs remain underexplored due to non-quantifiable metrics, limited evaluation datasets, and unclear research hierarchies in prior work.

Method: Created a large-scale dataset of 36,000 routes from 12 global metropolises, introduced PathBuilder tool for natural language-route conversion, and proposed new evaluation framework to assess 11 SOTA LLMs on route reversal tasks.

Result: LLMs exhibit significant limitations in reversing routes - most reversed routes neither return to starting points nor match optimal routes, showing low robustness and high confidence in incorrect answers.

Conclusion: Current LLMs struggle with geospatial route cognition, particularly route reversal, highlighting the need for improved spatial reasoning capabilities in language models.

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [98] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: The paper introduces COR, a new benchmark for Traditional Chinese conversational navigation, and proposes MCoT, a multimodal chain-of-thought framework that achieves high accuracy in translating egocentric utterances to allocentric orientations.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of translating egocentric utterances to allocentric orientations in indoor/complex facilities where GPS is weak and maps are unavailable, particularly for non-English and ASR-transcribed scenarios.

Method: Multimodal chain-of-thought (MCoT) framework integrating ASR-transcribed speech with landmark coordinates through a three-step reasoning process: spatial relation extraction, coordinate mapping to absolute directions, and user orientation inference. Uses curriculum learning on Taiwan-LLM-13B-v2.0-Chat.

Result: MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, outperforming unimodal and non-structured baselines. Demonstrates robustness to ASR errors, multilingual code-switching, and maintains high accuracy in cross-domain evaluation.

Conclusion: Structured MCoT spatial reasoning shows potential for interpretable and resource-efficient embodied navigation, with strong performance under noisy conversational conditions and resilience to linguistic variation and domain shift.

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [99] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: Proposes variational task vector composition with Bayesian inference, using Spike-and-Slab priors for sparsity and gated sampling for efficiency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To enable sample-specific composition of task vectors rather than task-level composition, addressing structural redundancy and improving efficiency.

Method: Bayesian inference framework with Spike-and-Slab priors for sparsity, combined with gated sampling mechanism to filter composition coefficients based on uncertainty and importance.

Result: Consistently outperforms existing approaches across all datasets by selectively leveraging the most reliable and informative components.

Conclusion: Establishes a new standard for efficient and effective task vector composition with improved transparency and generalization.

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [100] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: MolPILE is a large-scale, diverse dataset of 222 million compounds curated from 6 databases to address limitations in existing molecular datasets for foundation model training.


<details>
  <summary>Details</summary>
Motivation: Existing small molecule datasets have limitations that hinder molecular representation learning, creating a need for an ImageNet-like standardized resource in chemoinformatics.

Method: Constructed MolPILE using an automated curation pipeline from 6 large-scale databases, with comprehensive analysis of current pretraining datasets and retraining existing models on the new dataset.

Result: Retraining existing models on MolPILE yields improvements in generalization performance, demonstrating the dataset's effectiveness for molecular representation learning.

Conclusion: MolPILE provides a standardized, high-quality resource that addresses critical gaps in molecular chemistry datasets and enhances foundation model generalization capabilities.

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [101] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP is a method that improves multi-token prediction training to accelerate LLM inference through better speculative decoding, achieving 2.03x speedup with lossless quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive generation creates throughput bottlenecks in LLM deployment, and while multi-token prediction helps training efficiency, its inference acceleration potential remains unexplored.

Method: Fine-tunes a single MTP head with position-shared weights on self-distilled data, integrates language-aware dynamic vocabulary compression, and aligns training with inference patterns to improve multi-step draft quality.

Result: Achieves average 2.03x speedup across seven benchmarks compared to standard next token prediction, outperforming vanilla MTP by 82% while maintaining lossless output quality.

Conclusion: FastMTP offers a practical, lightweight training solution that seamlessly integrates with existing inference frameworks for rapidly deployable LLM acceleration.

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [102] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: This paper proposes M-DSL, a multi-worker selection algorithm for distributed swarm learning that addresses data heterogeneity challenges in edge IoT environments.


<details>
  <summary>Details</summary>
Motivation: Non-i.i.d. data in distributed swarm learning degrades performance and lacks theoretical guidance on how data heterogeneity affects model training accuracy.

Method: Introduces a new non-i.i.d. degree metric to measure data heterogeneity, develops M-DSL algorithm for effective multi-worker selection based on data statistical differences, and provides theoretical convergence analysis.

Result: Extensive experiments show M-DSL improves performance and network intelligence beyond benchmarks on various heterogeneous datasets and non-i.i.d. settings.

Conclusion: M-DSL effectively addresses data heterogeneity challenges in distributed swarm learning through systematic measurement and worker selection, with verified performance improvements.

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [103] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: GnnXemplar is a novel global explainer for Graph Neural Networks that identifies representative exemplar nodes and generates natural language rules using LLMs, outperforming existing methods in fidelity, scalability, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current GNN explanation methods are limited to local explanations, while global explanation methods that characterize entire classes are underdeveloped, especially for large real-world graphs where traditional motif discovery approaches fail.

Method: GnnXemplar identifies exemplar nodes in the GNN embedding space using a coverage maximization approach over reverse k-nearest neighbors, then employs self-refining prompts with LLMs to generate natural language rules from the exemplars' neighborhoods.

Result: Experiments across diverse benchmarks show GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.

Conclusion: GnnXemplar provides an effective global explanation framework for GNNs that addresses limitations of existing methods and offers human-interpretable explanations through exemplar-based reasoning and LLM-generated rules.

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [104] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: GETAD is a graph-enhanced trajectory anomaly detection framework that integrates road network topology, segment semantics, and historical patterns using Graph Attention Networks and Transformers to detect subtle anomalies in road-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory anomaly detection methods only consider limited aspects of trajectory nature and movement space, treating trajectories as simple location sequences in Euclidean space while neglecting underlying network constraints and connectivity information.

Method: GETAD uses Graph Attention Network to learn road-aware embeddings with graph-based positional encodings, a Transformer-based decoder for sequential movement modeling, and a multiobjective loss function combining autoregressive prediction and supervised link prediction. It introduces Confidence Weighted Negative Log Likelihood for robust anomaly scoring.

Result: Experiments on real-world and synthetic datasets show that GETAD achieves consistent improvements over existing methods, particularly in detecting subtle anomalies in road-constrained environments.

Conclusion: The results highlight the benefits of incorporating graph structure and contextual semantics into trajectory modeling, enabling more precise and context-aware anomaly detection.

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [105] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: This paper investigates why RL pretraining algorithms can generate network parameters that enable in-context reinforcement learning (ICRL), hypothesizing that such parameters are minimizers of the pretraining loss.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why standard RL pretraining algorithms can produce network parameters that allow agents to solve new out-of-distribution tasks without parameter updates, simply by conditioning on context like interaction history.

Method: The authors conduct a case study proving that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.

Result: The paper provides initial support for the hypothesis that parameters capable of ICRL are indeed minimizers of the pretraining loss, specifically demonstrating this for policy evaluation with Transformers.

Conclusion: The work establishes theoretical evidence that ICRL capabilities emerge from optimal solutions to the pretraining objective, offering insights into why standard RL pretraining can yield context-adaptive behavior.

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [106] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: This paper provides a comprehensive review of deep learning optimizers, examining their evolution from Stochastic Gradient Descent to recent developments like Momentum, AdamW, Sophia, and Muon, highlighting their distinctive features, update rules, and hyperparameter settings.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of deep learning training is highly dependent on the optimizer used, and with the rapid advancement of deep learning, numerous optimizers with different approaches have been developed. This study aims to provide a comprehensive resource for understanding the current state of optimizers and identifying future development areas.

Method: The study conducts a chronological review of various optimizers, examining each individually and highlighting their distinctive features. It presents detailed update rules, explains associated concepts and variables, discusses techniques applied by these optimizers, their contributions to optimization, and their default hyperparameter settings.

Result: The paper provides a comprehensive examination of optimizer evolution and characteristics, offering insights into open challenges in deep learning optimization.

Conclusion: This review serves as a valuable resource for understanding the current state of deep learning optimizers and identifying potential areas for future development in optimization algorithms.

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [107] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: A novel Chaos Game Representation method called R-CGR that preserves complete sequence information through explicit path encoding and rational arithmetic, enabling perfect sequence reconstruction from geometric traces.


<details>
  <summary>Details</summary>
Motivation: Traditional CGR approaches lose sequence information during geometric mapping, limiting their utility in applications requiring both accurate classification and sequence recovery.

Method: Introduces complete sequence recovery through explicit path encoding combined with rational arithmetic precision control, maintaining both positional and character information at each step of the geometric mapping.

Result: Achieves competitive performance on biological sequence classification tasks compared to traditional methods while providing interpretable geometric visualizations and complete sequence recovery.

Conclusion: R-CGR opens new avenues for interpretable bioinformatics analysis by generating feature-rich images suitable for deep learning while maintaining complete sequence information through explicit encoding.

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [108] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: KANDI is a novel offline reinforcement learning method that combines Kolmogorov-Arnold Networks and Diffusion Policies for healthcare applications, specifically for physical activity promotion in older adults at high fall risk.


<details>
  <summary>Details</summary>
Motivation: Offline RL faces challenges in healthcare applications: defining direct rewards is difficult, IRL struggles with complex environments, and aligning policies with human behavior is problematic. The paper addresses these issues for fall-risk intervention in older adults.

Method: KANDI uses Kolmogorov-Arnold Networks for flexible reward function estimation from expert behavior (low-fall-risk older adults), and diffusion-based policies within an Actor-Critic framework for generative action refinement in offline RL.

Result: KANDI outperforms state-of-the-art methods on the D4RL benchmark and shows practical effectiveness in a clinical trial for physical activity promotion among older adults at high fall risk.

Conclusion: KANDI offers an effective solution for healthcare applications, addressing key challenges in offline RL and showing potential for activity promotion intervention strategies in clinical settings.

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [109] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: MeshODENet combines Graph Neural Networks with Neural ODEs to create stable, accurate surrogate models for simulating complex structural mechanics problems, outperforming baseline models in long-term prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical solvers are computationally expensive for many-query tasks, and standard autoregressive GNNs suffer from error accumulation and instability in long-term predictions.

Method: A framework that integrates GNNs for spatial reasoning with Neural ODEs for continuous-time modeling, applied to 1D and 2D elastic bodies undergoing large non-linear deformations.

Result: Significantly outperforms baseline models in long-term predictive accuracy and stability while achieving substantial computational speed-ups over traditional solvers.

Conclusion: MeshODENet provides a powerful and generalizable approach for developing data-driven surrogates to accelerate analysis and modeling of complex structural systems.

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [110] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: AI-driven framework for optimizing MCMC-based preconditioner parameters to accelerate Krylov subspace solvers for ill-conditioned linear systems


<details>
  <summary>Details</summary>
Motivation: Krylov solvers converge slowly for ill-conditioned matrices, and MCMC-based preconditioning requires parameter tuning that is costly via manual or grid search

Method: Uses graph neural network surrogate to predict preconditioning speed from matrix A and MCMC parameters, then Bayesian acquisition function selects optimal parameters to minimize iterations

Result: Achieves better preconditioning with 50% of search budget compared to conventional methods, yielding ~10% reduction in iterations to convergence

Conclusion: Provides a viable approach for incorporating MCMC-based preconditioners into large-scale systems through automated parameter optimization

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [111] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind is a transformer-based multimodal framework for continual blood glucose forecasting that uses cross-attention and multi-scale attention mechanisms, achieving 15% RMSE and 9% MAE improvements over state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address challenges in blood glucose forecasting including varying sampling rates of physiological signals, long-range temporal dependencies, and catastrophic forgetting when learning from new patient cohorts.

Method: Uses transformer architecture with parallel cross-attention (integrates blood glucose with physiological/behavioral signals) and multi-scale attention (captures long-range dependencies), plus knowledge retention module to prevent catastrophic forgetting.

Result: Evaluated on AIREADI dataset, GluMind consistently outperforms state-of-the-art models with approximately 15% improvement in RMSE and 9% improvement in MAE.

Conclusion: GluMind demonstrates superior performance and stability for continual blood glucose forecasting across different patient populations, effectively handling multimodal data integration and long-term dependencies.

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [112] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: PGPCA extends PPCA to handle data distributed around nonlinear manifolds by incorporating geometric coordinates and an EM algorithm for parameter learning, showing superior performance over PPCA for manifold-distributed data.


<details>
  <summary>Details</summary>
Motivation: Traditional PPCA and its extensions are limited to linear models and Euclidean spaces, but neuroscience data often lies on nonlinear manifolds, requiring a probabilistic dimensionality reduction method that can handle manifold geometry.

Method: Developed Probabilistic Geometric PCA (PGPCA) that incorporates knowledge of a fitted nonlinear manifold, derives geometric coordinate systems to capture deviations from the manifold, and uses a data-driven EM algorithm for parameter learning.

Result: PGPCA effectively models data distributions around various manifolds, outperforms PPCA for such data, enables testing of geometric vs Euclidean coordinate systems, and performs dimensionality reduction both around and on the manifold.

Conclusion: PGPCA provides valuable capabilities for enhancing dimensionality reduction efficacy in high-dimensional data with noise distributed around nonlinear manifolds, making it particularly useful for neuroscience applications.

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [113] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: This paper proposes discrete-time diffusion processes as alternatives to continuous-time models, addressing limitations like training-inference mismatch and inefficient sampling while maintaining comparable speech quality.


<details>
  <summary>Details</summary>
Motivation: Continuous-time diffusion models have limitations including restrictive additive Gaussian noising, training-inference mismatch due to time discretization, and inefficient sampling requiring many steps. Discrete-time processes offer more flexibility and consistency.

Method: The paper explores discrete-time diffusion processes including variants with additive Gaussian noise, multiplicative Gaussian noise, blurring noise, and mixed blurring-Gaussian noise, proposing new variants of these discrete-time processes.

Result: Experimental results show that discrete-time processes achieve comparable subjective and objective speech quality to continuous-time models, while offering more efficient and consistent training and inference.

Conclusion: Discrete-time diffusion processes are viable alternatives to continuous-time models, providing similar speech quality with improved efficiency and consistency between training and inference conditions.

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [114] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: NVQ is a new vector compression technique that uses non-uniform vector quantization with individually learned quantizers for each vector, achieving improved accuracy with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Embedding vectors are large due to high-dimensionality, creating problems for vector search techniques as retrieving large vectors from memory/storage is expensive and their footprint is costly.

Method: Uses novel parsimonious and computationally efficient nonlinearities to build non-uniform vector quantizers that are individually learned for each indexed vector.

Result: Experimental results show NVQ exhibits improved accuracy compared to state-of-the-art methods.

Conclusion: NVQ provides a computationally and spatially efficient vector compression technique for high-fidelity regime applications.

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [115] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold is a protein folding model that uses standard transformer blocks with flow-matching training, achieving competitive performance without domain-specific architectural designs.


<details>
  <summary>Details</summary>
Motivation: To challenge the necessity of complex domain-specific architectures in protein folding by demonstrating that general-purpose transformer blocks can achieve state-of-the-art results.

Method: Uses standard transformer blocks with adaptive layers, trained via generative flow-matching objective with structural term. Scaled to 3B parameters on 9M distilled protein structures and PDB data.

Result: Achieves competitive performance on standard folding benchmarks and shows strong ensemble prediction capabilities. Efficient deployment on consumer hardware.

Conclusion: SimpleFold demonstrates that complex domain-specific architectures are not necessary for high-performance protein folding, opening alternative design possibilities.

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [116] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: Physics-informed Kolmogorov Arnold Networks (KANs) with Ehrenfest theorem constraints provide superior quantum dynamics prediction using only 5.4% of training data compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: Quantum dynamical prediction is computationally challenging due to high-dimensional Hilbert spaces, and existing neural networks require large datasets and suffer from spurious oscillations that compromise physical interpretability.

Method: Introduce Kolmogorov Arnold Networks (KANs) augmented with physics-informed loss functions enforcing Ehrenfest theorems, plus Chain of KANs architecture embedding temporal causality for time series modeling.

Result: Achieves superior accuracy with only 200 training samples (5.4% of what Temporal Convolution Networks require), maintaining mathematical rigor and physical consistency.

Conclusion: Physics-informed KANs offer compelling advantages over black-box models by dramatically reducing data requirements while preserving physical interpretability and accuracy.

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [117] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: The paper proposes using hybrid datasets combining synthetic and real-world features to improve anti-money laundering (AML) models while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Financial institutions face challenges in training AML models due to privacy concerns limiting access to real transaction data. Synthetic data alone has limitations for effective model training.

Method: The article proposes augmenting synthetic datasets with publicly available, easily accessible real-world features to create hybrid datasets that preserve privacy while enhancing model utility.

Result: Hybrid datasets demonstrate improved model utility compared to purely synthetic datasets while maintaining privacy and confidentiality protections.

Conclusion: Hybrid datasets offer a practical solution for financial institutions to enhance AML systems by balancing privacy concerns with model effectiveness.

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [118] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL is a novel RL training method that addresses the inefficiency caused by long-tail distribution of rollout response lengths by over-provisioning requests, terminating once target responses are reached, and recycling incomplete responses.


<details>
  <summary>Details</summary>
Motivation: RL training is computationally expensive with rollout generation accounting for over 90% of runtime, and efficiency is constrained by long-tail distribution where a few lengthy responses stall entire batches, leaving GPUs idle.

Method: APRIL over-provisions rollout requests, terminates once the target number of responses is reached, and recycles incomplete responses for continuation in future steps, ensuring no rollouts are discarded while reducing GPU idle time.

Result: APRIL improves rollout throughput by up to 44% across RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves up to 8% higher final accuracy across tasks. It's framework and hardware agnostic.

Conclusion: APRIL unifies system-level and algorithmic considerations to advance RL training efficiency and inspire further optimizations in RL systems.

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [119] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: RCCR is a fine-tuning method that enforces reverse-complement consistency in DNA language models by penalizing prediction divergence between sequences and their reverse complements, improving robustness while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: DNA language models often fail to capture the fundamental biological symmetry where reverse complements carry identical meaning, leading to inconsistent predictions that undermine model reliability.

Method: Reverse-Complement Consistency Regularization (RCCR) - a model-agnostic fine-tuning objective that directly penalizes divergence between predictions on sequences and their reverse complements.

Result: RCCR substantially improves reverse-complement robustness across three DNA model backbones on various genomic tasks, reducing prediction flips and errors while maintaining or improving task accuracy compared to baselines.

Conclusion: RCCR provides a single, intrinsically robust, and computationally efficient fine-tuning recipe that integrates biological symmetry priors directly into DNA language model learning.

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [120] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: Symphony-MoE is a novel framework that constructs powerful Mixture-of-Experts models by harmonizing experts from multiple pre-trained models, overcoming parameter misalignment through training-free fusion and functional alignment.


<details>
  <summary>Details</summary>
Motivation: Existing MoE upcycling methods limit expert diversity by using only one pre-trained model. This paper addresses the challenge of integrating experts from multiple disparate pre-trained models that occupy different parameter spaces.

Method: A two-stage framework: 1) Training-free harmony via layer-aware fusion and activation-based functional alignment to address parameter misalignment; 2) Lightweight router training to coordinate the entire architecture.

Result: Successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.

Conclusion: Symphony-MoE enables effective construction of powerful MoE models by harmonizing diverse experts from multiple pre-trained sources, overcoming the limitations of single-source upcycling.

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [121] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: Theoretical analysis of SigLIP's contrastive learning with trainable temperature and bias, introducing (m, b_rel)-Constellations to explain representation quality and modality gap.


<details>
  <summary>Details</summary>
Motivation: To theoretically explain the advantages of synchronizing temperature and bias in contrastive pretraining models like SigLIP and SigLIP2, and understand why they succeed in retrieval tasks.

Method: Introduces (m, b_rel)-Constellations as a novel combinatorial object related to spherical codes, parametrized by margin and relative bias. Uses this characterization to analyze representation quality and modality gap.

Result: Provides theoretical justification for SigLIP's success on retrieval, explains the modality gap, identifies necessary dimensions for high-quality representations, and proposes a reparameterization that improves training dynamics.

Conclusion: The theoretical framework of (m, b_rel)-Constellations successfully explains SigLIP's performance characteristics and leads to improved training methods through explicit relative bias parameterization.

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [122] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: This paper provides the first comprehensive review of Explainable Graph Neural Networks (XGNNs) in dementia research, covering applications across various dementia subtypes and introducing a taxonomy of explainability methods tailored for clinical scenarios.


<details>
  <summary>Details</summary>
Motivation: Dementia's clinical and biological heterogeneity makes diagnosis and subtype differentiation challenging. While GNNs show potential in modeling brain connectivity, their limited robustness, data scarcity, and lack of interpretability constrain clinical adoption. XGNNs address these barriers by combining graph-based learning with interpretability.

Method: The paper presents a comprehensive review examining XGNN applications across Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and multi-disease diagnosis. It introduces a taxonomy of explainability methods and compares existing models in clinical scenarios.

Result: The review identifies XGNNs' ability to identify disease-relevant biomarkers, analyze brain network disruptions, and provide transparent insights for clinicians. It also highlights challenges including limited generalizability, underexplored domains, and integration of LLMs for early detection.

Conclusion: By outlining both progress and open problems, this review aims to guide future work toward trustworthy, clinically meaningful, and scalable use of XGNNs in dementia research, addressing barriers to clinical adoption through improved interpretability and robustness.

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [123] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: Proposes Interaction Topological Transformer (ITT) - a unified framework for predictive modeling of porous materials that captures multi-scale structure-property relationships using interaction topology and Transformer architecture.


<details>
  <summary>Details</summary>
Motivation: Predictive modeling of porous materials is challenging due to multiscale structure-property relationships and sparse labeled data, hindering generalization across material families.

Method: ITT leverages interaction topology to capture materials information across structural, elemental, atomic, and pairwise-elemental levels. Uses two-stage training: self-supervised pretraining on 0.6M unlabeled structures followed by supervised fine-tuning.

Result: ITT achieves state-of-the-art, accurate, and transferable predictions for adsorption, transport, and stability properties of porous materials.

Conclusion: The framework provides a principled and scalable path for learning-guided discovery in structurally and chemically diverse porous materials.

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [124] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: DS-Diffusion is a novel time series generation model that addresses limitations of existing diffusion models by introducing style-guided kernels and hierarchical denoising to reduce distributional bias and improve interpretability without requiring retraining for specific conditions.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for time series generation require retraining for specific conditional guidance, suffer from distributional bias between generated and real data, and have uninterpretable inference processes due to complex latent spaces.

Method: Proposes DS-Diffusion with: 1) Diffusion framework based on style-guided kernels to avoid retraining for specific conditions, 2) Time-information based hierarchical denoising mechanism (THD) to reduce distributional bias, 3) Clear indication of data style origins for interpretability.

Result: Compared to state-of-the-art ImagenTime: predictive score decreases by 5.56%, discriminative score decreases by 61.55%. Distributional bias is further reduced, inference process is more interpretable, and model flexibility/adaptability is enhanced without retraining.

Conclusion: DS-Diffusion effectively addresses key limitations of existing diffusion models for time series generation by reducing distributional bias, improving interpretability, and eliminating the need for retraining, while achieving superior performance metrics.

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [125] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: REBACT introduces a 'reflect before act' approach that adds a reflection step before each action in LLM-based decision-making, enabling immediate error correction and improving success rates across multiple interactive environments.


<details>
  <summary>Details</summary>
Motivation: Existing LLM methods for interactive decision-making struggle with error accumulation and lack robust self-correction mechanisms, leading to suboptimal performance.

Method: REBACT adds a critical reflect step prior to taking the next action, allowing for immediate error correction and adaptation to environment feedback using Claude3.5-sonnet as the underlying LLM.

Result: REBACT significantly outperforms baselines with success rate improvements of 24% on WebShop (61%), 6.72% on ALFWorld (98.51%), and 0.5% on TextCraft (99.5%), achieving high performance with few modification steps.

Conclusion: The reflect-before-act approach effectively enhances LLM decision-making by enabling robust self-correction while maintaining computational efficiency.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [126] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: Flow Marching bridges neural operator learning with flow matching to create a generative PDE foundation model that reduces long-term rollout drift while enabling uncertainty-aware ensemble generations.


<details>
  <summary>Details</summary>
Motivation: Existing PDE foundation models rely on deterministic Transformer architectures which lack generative flexibility needed for many science and engineering applications.

Method: Proposes Flow Marching algorithm that jointly samples noise level and physical time step, learns unified velocity field, uses Physics-Pretrained Variational Autoencoder (P2VAE) for state embedding, and Flow Marching Transformer (FMT) with diffusion-forcing and latent temporal pyramids for efficiency.

Result: Achieves up to 15x greater computational efficiency than full-length video diffusion models, demonstrates long-term rollout stability over deterministic counterparts, and shows effective few-shot adaptation on unseen Kolmogorov turbulence.

Conclusion: The approach highlights the importance of generative PDE foundation models for real-world applications, enabling uncertainty-stratified ensemble results and large-scale pretraining at reduced cost.

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [127] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt is a parameter-efficient fine-tuning method that uses diagonal matrices for row- and column-wise scaling, achieving high-rank updates with only n+m parameters for an n×m matrix.


<details>
  <summary>Details</summary>
Motivation: To reduce the memory and computational costs of fine-tuning foundation models while maintaining performance comparable to full fine-tuning.

Method: HyperAdapt adapts pre-trained weight matrices by applying row- and column-wise scaling through diagonal matrices, requiring only n+m trainable parameters for an n×m matrix.

Result: Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters show HyperAdapt matches or nearly matches full fine-tuning and state-of-the-art PEFT methods while using significantly fewer parameters.

Conclusion: HyperAdapt provides an efficient alternative to full fine-tuning and existing PEFT methods, achieving high performance with minimal parameter updates.

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [128] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: A novel framework called Subspace Clustering of Subspaces (SCoS) that clusters tall matrices based on their column spaces using Block Term Decomposition of third-order tensors.


<details>
  <summary>Details</summary>
Motivation: Traditional subspace clustering methods assume vectorized data, but many real-world applications have data naturally represented as matrices with underlying subspace structure that should be preserved.

Method: Constructs a third-order tensor from input matrices and uses Block Term Decomposition to jointly estimate cluster memberships and partially shared subspaces, with scalable optimization algorithms for large datasets.

Result: Superior clustering accuracy and robustness compared to existing subspace clustering techniques, especially under high noise and interference, as demonstrated on hyperspectral imaging datasets.

Conclusion: The framework shows strong potential for challenging high-dimensional applications where structure exists beyond individual data vectors, providing the first identifiability results for this formulation.

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [129] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: This paper presents a rational pesticide design approach using graph machine learning, creating ApisTox - the largest curated dataset on pesticide toxicity to honey bees, and evaluates various ML models for agrochemical applications.


<details>
  <summary>Details</summary>
Motivation: To accelerate development of safer, eco-friendly agrochemicals by adapting in silico methods from drug discovery to pesticide design, with emphasis on ecotoxicology.

Method: Created ApisTox dataset and conducted broad evaluation of ML models including molecular fingerprints, graph kernels, GNNs, and pretrained transformers for molecular graph classification.

Result: Found that methods successful in medicinal chemistry often fail to generalize to agrochemicals, highlighting the need for domain-specific models and benchmarks.

Conclusion: Future work will focus on developing comprehensive benchmarking suite and designing ML models tailored to pesticide discovery challenges.

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [130] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: The paper introduces a generalized bisimulation metric (GBSM) for comparing pairs of MDPs, establishing rigorous mathematical properties and providing tighter theoretical bounds for policy transfer, state aggregation, and sampling-based estimation than standard BSM.


<details>
  <summary>Details</summary>
Motivation: While bisimulation metric (BSM) is effective for single MDP analysis, its application to multiple-MDP scenarios like policy transfer remains challenging due to lack of rigorous mathematical properties when generalized to MDP pairs.

Method: Formally establish GBSM between pairs of MDPs with proven fundamental properties: symmetry, inter-MDP triangle inequality, and distance bounds on identical state spaces. Use these properties to analyze policy transfer, state aggregation, and sampling-based estimation.

Result: GBSM provides explicit bounds that are strictly tighter than standard BSM, offers closed-form sample complexity for estimation (improving upon asymptotic BSM results), and numerical validation demonstrates effectiveness in multi-MDP scenarios.

Conclusion: GBSM successfully generalizes bisimulation metrics to multiple-MDP settings with rigorous mathematical foundations, enabling more effective theoretical analysis and practical applications in multi-MDP reinforcement learning problems.

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [131] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: This paper introduces a novel e-commerce fraud detection method that combines reinforcement learning (RL) with Large Language Models (LLMs) to optimize risk assessment across payment stages, using LLMs to iteratively refine reward functions for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional RL models for fraud detection require significant human expertise to craft effective reward functions due to their complexity and variability. LLMs offer advanced reasoning capabilities that can automate and enhance this process.

Method: The approach frames transaction risk as a multi-step Markov Decision Process (MDP) and uses LLMs to iteratively refine reward functions for RL optimization, enabling better fraud detection across multiple payment stages.

Result: Experiments with real-world data show the LLM-enhanced RL framework achieves superior fraud detection accuracy, demonstrates zero-shot capability, and proves robust and resilient in long-term evaluations.

Conclusion: The integration of LLMs with RL presents significant potential for advancing industrial RL applications, particularly in complex domains like e-commerce fraud detection where automated reward function refinement is crucial.

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [132] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: Periodic CNNs with boundary conditions can approximate ridge functions in d-1 dimensions, which is impossible in lower dimensions, establishing their expressive power for high-dimensional ridge-structured data.


<details>
  <summary>Details</summary>
Motivation: To develop CNN architectures that can handle data with ridge-like structures in high dimensions, particularly for applications like image analysis on wrapped domains and physics-informed learning.

Method: Introducing periodic boundary conditions into convolutional layers and proving a theoretical approximation theorem for ridge functions.

Result: Periodic CNNs can approximate ridge functions depending on d-1 linear variables in d-dimensional space, but not in lower-dimensional settings, providing a sharp characterization of their expressive power.

Conclusion: Periodic CNNs expand CNN approximation theory and offer practical advantages for problems with high-dimensional ridge structures in various scientific domains.

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [133] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: MOMEMTO is a time series foundation model for anomaly detection that uses a patch-based memory module to prevent over-generalization by storing normal patterns from multiple domains, enabling joint fine-tuning across datasets.


<details>
  <summary>Details</summary>
Motivation: Existing reconstruction-based deep models for time series anomaly detection tend to over-generalize and accurately reconstruct anomalies. Memory-based approaches have high training costs and haven't been effectively integrated with time series foundation models.

Method: Proposes MOMEMTO with a patch-based memory module that captures normal patterns from multiple domains. Memory items are initialized with pre-trained encoder representations, organized into patch-level units, and updated via attention mechanism. Uses multi-domain training strategy for joint fine-tuning.

Result: Evaluated on 23 univariate benchmark datasets, MOMEMTO achieves higher AUC and VUS metrics compared to baselines and enhances backbone TFM performance, especially in few-shot learning scenarios.

Conclusion: MOMEMTO effectively addresses over-generalization in time series anomaly detection through its memory architecture and multi-domain training, demonstrating superior performance as a single model across multiple datasets.

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [134] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: Diagonal linear networks' training trajectory closely mirrors the lasso regularization path, with training time acting as an inverse regularization parameter.


<details>
  <summary>Details</summary>
Motivation: To deepen the analysis of implicit regularization in diagonal linear networks by connecting their full training trajectory to the lasso regularization path.

Method: Analyzed the training dynamics of diagonal linear networks with linear activation and diagonal weight matrices, comparing them to the lasso regularization path through both rigorous theoretical analysis and simulations.

Result: Found that under monotonicity assumptions, the connection between diagonal linear network training and lasso path is exact; in general cases, an approximate connection exists.

Conclusion: The training trajectory of diagonal linear networks is fundamentally related to the lasso regularization path, with training time inversely related to the regularization parameter.

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [135] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO addresses GRPO's limitation in handling homogeneous responses by introducing Advantage Calibration and Asymmetric Clipping to convert homogeneous errors into learning signals, achieving superior performance on mathematical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: GRPO fails to learn from homogeneous responses (all correct or all incorrect), especially problematic for homogeneously incorrect groups where advantage function yields zero, causing null gradients and loss of learning signals.

Method: NGRPO introduces two key mechanisms: 1) Advantage Calibration - hypothesizes a virtual maximum-reward sample to ensure non-zero advantages for homogeneously incorrect samples; 2) Asymmetric Clipping - relaxes positive sample updates while constraining negative sample updates to stabilize exploration.

Result: Experiments on Qwen2.5-Math-7B show NGRPO significantly outperforms PPO, GRPO, DAPO, and PSR-NSR on MATH500, AMC23, and AIME2025 benchmarks, demonstrating stable and substantial improvements in mathematical reasoning.

Conclusion: NGRPO effectively converts homogeneous errors into robust learning signals, enabling LLMs to learn from previously unlearnable scenarios and achieve better mathematical reasoning performance.

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [136] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: A diagnostic framework using ensemble probabilistic machine learning to quantify prediction uncertainty in data-driven consistency-based fault diagnosis, improving diagnostic metrics by addressing confidence evaluation challenges in deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks for fault diagnostics struggle with confidence evaluation, which is critical in consistency-based diagnosis where decision logic is highly sensitive to false alarms.

Method: Uses ensemble probabilistic machine learning to quantify and automate prediction uncertainty in data-driven consistency-based diagnosis.

Result: Evaluated across several case studies using ablation and comparative analyses, showing consistent improvements across a range of diagnostic metrics.

Conclusion: The proposed framework effectively addresses confidence evaluation challenges in fault diagnostics by leveraging ensemble probabilistic methods to improve diagnostic characteristics.

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [137] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: The paper introduces GNARL, a framework that reframes neural algorithmic reasoning as a Markov Decision Process to address limitations of supervised learning approaches, enabling better performance on NP-hard problems and applicability without expert algorithms.


<details>
  <summary>Details</summary>
Motivation: Current Neural Algorithmic Reasoning (NAR) approaches have limitations including inability to construct valid solutions without post-processing, poor performance on combinatorial NP-hard problems, and inapplicability when strong algorithms are unknown.

Method: Proposes GNARL framework that translates NAR problems to RL by modeling algorithm trajectories as Markov Decision Processes, using imitation and reinforcement learning for graph-based problems.

Result: Achieves very high graph accuracy on CLRS-30 problems, matches or exceeds narrower NAR approaches for NP-hard problems, and works even without expert algorithms.

Conclusion: Reframing neural algorithmic reasoning as RL via MDPs successfully addresses key limitations of supervised NAR approaches, enabling broader applicability and better performance on challenging problems.

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [138] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: A lightweight method for data assimilation using pre-trained diffusion models without additional training, applied to weather forecasting.


<details>
  <summary>Details</summary>
Motivation: Data assimilation is crucial in fields like meteorology and robotics for estimating system states from noisy observations, but existing methods can be complex and require training.

Method: Builds on particle filters using diffusion models pre-trained for dynamical system emulation, demonstrated with GenCast for weather forecasting.

Result: Proposes a general framework that leverages existing diffusion models for data assimilation tasks.

Conclusion: The method provides a lightweight, training-free approach to data assimilation using pre-trained diffusion models, with potential applications across various disciplines.

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [139] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: This paper introduces credal networks (CN) as a privacy-preserving alternative to Bayesian networks (BN) to protect against tracing attacks while maintaining model utility.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in publicly released Bayesian networks where tracing attacks can identify whether specific individuals belong to the training data, and existing noise-based protection methods significantly degrade model utility.

Method: Proposes using credal networks to mask learned Bayesian networks by adapting tracing attack notions and identifying key learning information to conceal. Conducts numerical experiments to analyze privacy gains through CN hyperparameter tuning.

Result: CNs enable meaningful inferences while safeguarding privacy, providing a way to modulate privacy gains through hyperparameter tuning without introducing noise that degrades utility.

Conclusion: Credal networks offer a principled, practical, and effective approach for developing privacy-aware probabilistic graphical models that balance privacy protection with model utility.

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [140] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: LoRD is a low-rank doubly stochastic clustering method that relaxes only the orthonormal constraint to produce probabilistic clustering results, with B-LoRD adding block diagonal regularization to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based clustering methods excessively relax constraints (low-rank, nonnegative, doubly stochastic, orthonormal) for numerical feasibility, potentially limiting clustering effectiveness.

Method: Proposes LoRD (only relaxes orthonormal constraint) and B-LoRD (adds block diagonal regularization via Frobenius norm maximization). Uses projected gradient descent with linear convex constraint transformation for optimization.

Result: Extensive experiments validate the effectiveness of the approaches. Theoretically establishes equivalence between orthogonality and block diagonality under doubly stochastic constraint.

Conclusion: The proposed methods provide improved clustering performance by maintaining more constraints than existing approaches while ensuring numerical solvability through proper optimization techniques.

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [141] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: Proposes Fully Learnable Neural Reward Machines (FLNRM) that can learn both Symbol Grounding functions and automata end-to-end for non-Markovian RL tasks, outperforming RNN-based approaches while maintaining explainability.


<details>
  <summary>Details</summary>
Motivation: Current approaches for non-Markovian RL rely on restrictive assumptions like predefined Symbol Grounding functions or prior knowledge of temporal tasks, limiting their applicability.

Method: Develops a fully learnable version of Neural Reward Machines that learns both the Symbol Grounding function mapping raw observations to symbols and the automaton structure end-to-end, integrated with deep reinforcement learning.

Result: FLNRM outperforms previous approaches based on Recurrent Neural Networks and is as easily applicable as classic deep RL methods while being more explainable due to the finite and compact nature of automata.

Conclusion: The proposed FLNRM approach successfully removes reliance on prior knowledge while maintaining explainability and achieving superior performance compared to RNN-based methods for non-Markovian reinforcement learning tasks.

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [142] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: SWE and SVoD methods prevent neuron inactivity during neural network expansion by coupling new neurons with existing ones and using gradient-based allocation.


<details>
  <summary>Details</summary>
Motivation: Newly added neurons during neural network expansion often become inactive and fail to contribute to capacity growth, limiting the effectiveness of network expansion methods.

Method: Shared-Weights Extender (SWE) couples new neurons with existing ones for smooth integration, and Steepest Voting Distributor (SVoD) uses gradient-based allocation to distribute neurons across layers during expansion.

Result: Extensive benchmarking on four datasets shows the method effectively suppresses neuron inactivity and achieves better performance compared to other expanding methods and baselines.

Conclusion: The proposed SWE and SVoD methods successfully address the neuron inactivity problem in neural network expansion, enabling effective capacity augmentation without retraining from scratch.

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [143] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: This paper demonstrates that Mono-Forward (MF) algorithm outperforms backpropagation in both accuracy and efficiency, achieving up to 41% energy reduction and 34% faster training while maintaining competitive classification performance.


<details>
  <summary>Details</summary>
Motivation: The rising computational and energy demands of deep neural networks driven by backpropagation challenge sustainable AI development, necessitating more efficient training methods.

Method: Rigorous comparative analysis of three BP-free methods (Forward-Forward, Cascaded-Forward, and Mono-Forward) using optimized hyperparameters, consistent early stopping, and hardware-level energy measurements via NVML API and CodeCarbon.

Result: MF consistently surpasses BP in classification accuracy on MLPs, reduces energy consumption by up to 41%, shortens training time by up to 34%, and converges to more favorable validation loss minima.

Conclusion: MF offers a superior balance of accuracy and sustainability, challenging assumptions about global optimization requirements and providing a roadmap for energy-efficient deep learning.

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [144] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelGNN is a novel Graph Neural Network architecture inspired by Axelrod's cultural dissemination model that addresses fundamental GNN limitations like feature oversmoothing, handling heterogeneous relationships, and fine-grained feature processing through similarity-gated probabilistic interactions and trait-level copying mechanisms.


<details>
  <summary>Details</summary>
Motivation: To overcome fundamental limitations of traditional GNNs including feature oversmoothing in deep networks, ineffective handling of heterogeneous relationships where connected nodes differ significantly, and processing entire feature vectors as indivisible units which limits flexibility.

Method: AxelGNN incorporates similarity-gated probabilistic interactions that adaptively promote convergence or divergence based on node similarity, implements trait-level copying mechanisms for fine-grained feature aggregation at the segment level, and maintains global polarization to preserve node distinctiveness across multiple representation clusters.

Result: Extensive experiments on node classification and influence estimation benchmarks demonstrate that AxelGNN consistently outperforms or matches state-of-the-art GNN methods across diverse graph structures with varying homophily-heterophily characteristics.

Conclusion: AxelGNN provides a unified framework that naturally handles both homophilic and heterophilic graphs within a single architecture, effectively addressing key limitations of traditional GNNs through its bistable convergence dynamics inspired by cultural dissemination models.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [145] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: First analysis of heterophily in graph-level learning, revealing that motif-based tasks require mixed-frequency dynamics rather than frequency-dominated approaches used in node-level tasks.


<details>
  <summary>Details</summary>
Motivation: While heterophily has been extensively studied in node-level tasks, its impact on graph-level tasks remains unclear and requires systematic investigation.

Method: Combined theoretical energy-based gradient flow analysis with empirical validation on synthetic datasets with controlled heterophily and real-world molecular property prediction tasks.

Result: Motif detection requires mixed-frequency dynamics to remain flexible across multiple spectral components, and frequency-adaptive models outperform frequency-dominated models in graph-level tasks.

Conclusion: Establishes new theoretical understanding of heterophily in graph-level learning and provides guidance for designing effective GNN architectures that account for mixed-frequency requirements.

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [146] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: This thesis focuses on developing robust deep learning algorithms across three areas: adversarial examples in computer vision, domain generalization, and jailbreaking large language models.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are increasingly used in safety-critical applications, making robustness against adversarial exploitation fundamentally important.

Method: The research introduces new technical results, training paradigms, and certification algorithms for adversarial examples; develops state-of-the-art domain generalization algorithms for medical imaging, molecular identification, and image classification; and proposes new attacks and defenses for jailbreaking LLMs.

Result: The work achieves state-of-the-art generalization performance across multiple domains and represents frontier progress in designing robust language-based agents.

Conclusion: The thesis makes significant contributions to improving the robustness of deep learning models across computer vision and natural language processing applications, addressing critical security challenges in safety-critical systems.

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [147] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: This paper proposes a dynamic trigger optimization approach for backdoor attacks in federated learning that decouples backdoor tasks from main tasks to improve persistence against defenses.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks in federated learning use fixed triggers that tightly couple main and backdoor tasks, making them vulnerable to dilution by honest updates and limited persistence under federated defenses.

Method: A min-max framework where the inner layer maximizes performance gap between poisoned and benign samples to minimize impact of benign users, and the outer process injects adaptive triggers into the local model.

Result: Evaluation on computer vision and natural language tasks shows the method achieves good attack performance and outperforms six existing backdoor attack methods under six defense algorithms.

Conclusion: The proposed dynamic trigger optimization method effectively improves backdoor attack persistence in federated learning and can be easily integrated into existing attack techniques.

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [148] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: CARGO is a scalable multi-label causal discovery method for high-dimensional event sequences that uses pretrained causal Transformers to infer causal graphs and reconstruct global Markov boundaries through adaptive frequency fusion.


<details>
  <summary>Details</summary>
Motivation: Understanding causality in event sequences (like symptoms leading to diseases or error codes causing system failures) is critical but remains challenging in domains like healthcare and vehicle diagnostics, especially with sparse, high-dimensional data.

Method: CARGO uses two pretrained causal Transformers as foundation models to infer causal graphs per sequence in parallel, then aggregates them using adaptive frequency fusion to reconstruct global Markov boundaries of labels, bypassing expensive full-dataset conditional independence testing.

Result: On a real-world automotive fault prediction dataset with 29,100 unique event types and 474 imbalanced labels, CARGO demonstrated effective structured reasoning capabilities.

Conclusion: CARGO provides an efficient probabilistic reasoning approach for causal discovery in high-dimensional event sequences, enabling scalable analysis while avoiding the computational intractability of traditional methods.

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [149] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: FedFiTS is a trust and fairness-aware selective federated learning framework that improves upon FedFaSt by combining fitness-based client election with slotted aggregation to address challenges in sensitive domains like healthcare.


<details>
  <summary>Details</summary>
Motivation: Federated Learning deployments in sensitive domains face persistent challenges from non-IID data, client unreliability, and adversarial manipulation, requiring more robust and fair frameworks.

Method: FedFiTS implements a three-phase participation strategy (free-for-all training, natural selection, slotted team participation) with dynamic client scoring, adaptive thresholding, and cohort-based scheduling. It includes theoretical convergence analysis and communication-complexity analysis.

Result: Experiments on medical imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and agricultural data show FedFiTS consistently outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and resilience to poisoning attacks.

Conclusion: By integrating trust-aware aggregation with fairness-oriented client selection, FedFiTS advances scalable and secure FL, making it well suited for real-world healthcare and cross-domain deployments.

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [150] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: This paper proposes two vector representations (Standard-Deviation Vector and Clustering Vector) to analyze weight characteristics in large language models, showing they can distinguish between different models and reveal dataset influences on weight distribution while maintaining correlation consistency.


<details>
  <summary>Details</summary>
Motivation: To analyze the correlations and differences between large language models by studying their weight characteristics, particularly focusing on how to effectively distinguish different models and understand the similarities among models of the same family.

Method: Two vector representations are proposed: 1) Standard-Deviation Vector - normalizing standard deviation values of projection matrices assuming normal distribution; 2) Clustering Vector - extracting singular values from weight projection matrices and grouping them using K-Means algorithm. The study also conducts LoRA fine-tuning experiments with different datasets and models.

Result: The two vectors effectively distinguish between different models and clearly show similarities among models of the same family. Standard-deviation vector distribution is directly influenced by the dataset used in fine-tuning, while clustering vector correlations remain unaffected and maintain high consistency with the pre-trained model.

Conclusion: The proposed vector representations provide effective tools for analyzing model characteristics, with standard-deviation vectors capturing dataset-influenced distribution patterns and clustering vectors preserving inherent correlation characteristics that remain stable across fine-tuning.

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [151] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: HEROS proposes a heterogeneous online ensemble method that balances predictive performance with computational sustainability by selectively training subsets of models under resource constraints.


<details>
  <summary>Details</summary>
Motivation: Current ensemble methods for stream mining focus too much on predictive capabilities without sufficiently considering computational expenses, creating a need for more sustainable approaches to online learning.

Method: HEROS uses a Markov decision process framework to model trade-offs between performance and sustainability, with a novel ζ-policy that selectively trains near-optimal models from a diverse pool under resource constraints.

Result: Theoretical analysis proves ζ-policy achieves near-optimal performance with fewer resources. Experiments on 11 benchmark datasets show it outperforms competitors in accuracy while being much more resource-friendly.

Conclusion: HEROS provides a sustainable ensemble method that maintains high predictive performance while significantly reducing computational costs, representing a strong contribution to green online learning.

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [152] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: GSTM-HMU is a generative spatio-temporal framework that models semantic and temporal complexity of human mobility traces, achieving superior performance on next-location prediction, trajectory-user identification, and time estimation tasks.


<details>
  <summary>Details</summary>
Motivation: To advance mobility analysis by explicitly modeling the semantic and temporal complexity of human movement, capturing both short-term visiting patterns and persistent lifestyle regularities from mobility check-in sequences.

Method: The framework includes four innovations: Spatio-Temporal Concept Encoder (integrates location, POI semantics, and temporal rhythms), Cognitive Trajectory Memory (filters historical visits for user intent), Lifestyle Concept Bank (adds human preference cues), and task-oriented generative heads for multiple downstream tasks.

Result: Extensive experiments on four real-world datasets (Gowalla, WeePlace, Brightkite, FourSquare) show consistent and substantial improvements over strong baselines on three benchmark tasks.

Conclusion: Generative modeling provides a promising foundation for building more robust, interpretable, and generalizable systems for human mobility intelligence, effectively extracting semantic regularities from complex mobility data.

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [153] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: Central limit theorems for Polyak-Ruppert averaged Q-learning under asynchronous updates, including non-asymptotic and functional CLTs


<details>
  <summary>Details</summary>
Motivation: To establish rigorous statistical guarantees for Q-learning algorithms, particularly understanding the convergence behavior and distributional properties of Polyak-Ruppert averaging in asynchronous settings

Method: Develops mathematical proofs for central limit theorems, analyzing the convergence rate in Wasserstein distance and deriving functional CLT showing weak convergence to Brownian motion

Result: Established non-asymptotic CLT with explicit dependence on iteration count, state-action space size, discount factor, and exploration quality; proved functional CLT showing partial-sum process converges to Brownian motion

Conclusion: Provides fundamental statistical foundations for Q-learning with Polyak-Ruppert averaging, enabling better understanding of convergence properties and distributional behavior in reinforcement learning algorithms

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [154] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion is a federated transfer-learning framework that addresses heterogeneous feature spaces, non-IID data, and label scarcity through domain adaptation, frugal labeling, and diversity-aware encoders.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with heterogeneous feature spaces, severe non-IID data distribution, and scarce labels across clients in real-world scenarios.

Method: Combines domain adaptation and frugal labeling with diversity-aware encoders (DivEn, DivEn-mix, DivEn-c), uses confidence-filtered pseudo-labels, similarity-weighted classifier coupling, and self-/semi-supervised pretext training with selective fine-tuning.

Result: Outperforms state-of-the-art baselines in accuracy, robustness, and fairness across tabular and imaging benchmarks under various data regimes while maintaining comparable communication and computation budgets.

Conclusion: Harmonizing personalization, domain adaptation, and label efficiency provides an effective solution for robust federated learning under real-world constraints.

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [155] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: This paper introduces Otters, a hardware-software co-design approach that repurposes natural signal decay in optoelectronic devices as the core computation for time-to-first-spike (TTFS) encoding in spiking neural networks, eliminating expensive digital operations and achieving state-of-the-art energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Current SNNs with TTFS encoding fail to realize their full energy efficiency potential because inference requires costly evaluation of temporal decay functions and multiplication with synaptic weights. The authors aim to eliminate these expensive digital operations by leveraging natural physical phenomena.

Method: The authors fabricated a custom indium oxide optoelectronic synapse that uses natural physical decay to implement the required temporal function. They treat the device's analog output as the fused product of synaptic weight and temporal decay. For complex architectures like transformers, they developed a novel quantized neural network-to-SNN conversion algorithm to overcome training challenges due to sparsity.

Result: The Otters paradigm achieved state-of-the-art accuracy across seven GLUE benchmark datasets and demonstrated a 1.77× improvement in energy efficiency over previous leading SNNs, based on comprehensive analysis of compute, data movement, and memory access costs using energy measurements from a commercial 22nm process.

Conclusion: This work establishes a new paradigm for energy-efficient SNNs by translating fundamental device physics directly into computational primitives, showing how physical hardware 'bugs' can be repurposed as core computational elements to achieve significant energy savings.

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [156] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs implement amortized Bayesian inference under simulation priors, converge to Bayes-optimal predictors, and provide mechanistic interpretability through posterior-consistent explanations.


<details>
  <summary>Details</summary>
Motivation: To establish formal theoretical foundations for Simulation-Grounded Neural Networks (SGNNs) which currently lack formal underpinning despite achieving state-of-the-art performance in data-limited domains.

Method: Develop theoretical framework showing SGNNs perform amortized Bayesian inference, derive generalization bounds under model misspecification, and formalize mechanistic interpretability through attribution to simulated mechanisms.

Result: SGNNs recover latent parameters, remain robust under mismatch, and outperform classical tools - achieving half the error of AIC in model selection tasks for distinguishing mechanistic dynamics.

Conclusion: SGNNs are established as a principled and practical framework for scientific prediction in data-limited regimes, providing both theoretical guarantees and practical advantages over traditional methods.

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [157] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: CR-Net is a parameter-efficient framework that uses low-rank residual networks to improve LLM pre-training by maintaining high-rank information with minimal parameters and reduced memory requirements.


<details>
  <summary>Details</summary>
Motivation: Current low-rank methods for LLM pre-training suffer from compromised performance, computational overhead, and limited memory savings. The authors discovered that inter-layer activation residuals have low-rank properties, which can be leveraged for efficiency.

Method: Proposes Cross-layer Low-Rank residual Network (CR-Net) with a dual-path architecture that reconstructs layer activations by combining previous-layer outputs with their low-rank differences. Includes a specialized activation recomputation strategy for memory reduction.

Result: Extensive pre-training experiments across model scales (60M to 7B parameters) show CR-Net consistently outperforms state-of-the-art low-rank frameworks while requiring fewer computational resources and less memory.

Conclusion: CR-Net effectively addresses the limitations of current low-rank methods by leveraging low-rank residual properties, achieving better performance with improved efficiency in computational resources and memory usage.

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [158] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: This paper provides a theoretical overview of representation learning from unlabeled data, highlighting the gap between classical statistical methods and modern deep learning approaches, and discusses recent theoretical advances in analyzing these models.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models use new unsupervised representation learning principles that cannot be easily analyzed using classical theories, making it difficult to characterize why they perform well for diverse tasks or show emergent behavior.

Method: The paper combines mathematical tools from statistics and optimization to provide theoretical analysis of representation learning from unlabeled data, particularly focusing on visual foundation models using self-supervision or denoising/masked autoencoders.

Result: The paper presents an overview of recent theoretical advances in understanding representation learning from unlabeled data and mentions the authors' contributions to this research direction.

Conclusion: There is a need for new theoretical frameworks that can explain the success of modern unsupervised representation learning methods, and combining tools from statistics and optimization provides a promising approach for such analysis.

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [159] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge is a unified multimodal framework that supports vision-language understanding, generation, and retrieval using a language-centric design with lightweight bidirectional latent alignment and decoupled training strategy.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLM solutions treat tasks in isolation or require expensive training from scratch, leading to high computational costs and limited generalization across modalities.

Method: Uses pretrained LLMs with lightweight bidirectional latent alignment module and two-stage decoupled training: supervised fine-tuning with latent space alignment, and semantic-guided diffusion training with learnable query embeddings.

Result: Achieves competitive or state-of-the-art performance across multiple benchmarks in all three tasks (understanding, generation, retrieval).

Conclusion: Demonstrates effectiveness of latent space alignment for unifying multimodal modeling under a shared representation space, providing a modular and efficient solution.

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [160] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: A hybrid GAN-Transformer approach for generating realistic fraudulent transaction samples to address class imbalance in credit card fraud detection, outperforming traditional methods like SMOTE and other generative models.


<details>
  <summary>Details</summary>
Motivation: Credit card fraud detection faces severe class imbalance issues where fraud cases are extremely rare. Traditional oversampling methods like SMOTE create simplistic synthetic samples that don't capture complex fraud patterns, while existing generative models like CTGAN and TVAE struggle with high-dimensional dependence modeling.

Method: Proposes a hybrid approach combining Generative Adversarial Network (GAN) with Transformer encoder blocks. The GAN enables adversarial training of realistic generators, while the Transformer's self-attention mechanism learns rich feature interactions to overcome limitations of previous methods.

Result: Tested on the Credit Card Fraud Detection dataset and compared with conventional and generative resampling strategies using various classifiers (LR, RF, XGBoost, SVM). The Transformer-based GAN showed substantial improvements in Recall, F1-score, and AUC metrics.

Conclusion: The hybrid GAN-Transformer approach effectively overcomes severe class imbalance in fraud detection by producing high-quality synthetic minority class samples, demonstrating superior performance compared to existing methods.

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [161] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD is a latent diffusion model framework for black-box adversarial attacks that uses joint attention distillation from CNN and ViT models to generate architecture-agnostic adversarial examples with improved transferability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing black-box adversarial attack methods have limitations including dependency on specific network architectures, high query costs, and poor cross-architecture transferability.

Method: JAD leverages a latent diffusion model guided by attention maps distilled from both CNN and Vision Transformer models, focusing on commonly sensitive image regions across different architectures.

Result: Experiments show JAD achieves superior attack generalization, generation efficiency, and cross-architecture transferability compared to existing methods.

Conclusion: JAD provides a promising and effective paradigm for black-box adversarial attacks by being architecture-agnostic and reducing reliance on iterative queries.

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [162] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: DBVI improves upon DDVI by using a learnable, data-dependent initial distribution for diffusion-based variational inference in deep Gaussian processes, leading to better efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: DDVI's fixed unconditional starting distribution is far from the true posterior, causing inefficient inference and slow convergence in deep Gaussian processes.

Method: DBVI initiates reverse diffusion from an amortized neural network parameterized initial distribution that operates on inducing inputs, using Girsanov-based ELBOs and reverse-time SDEs with a Doob-bridged diffusion process.

Result: DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality across regression, classification, and image reconstruction tasks.

Conclusion: DBVI provides a scalable and efficient inference method for deep Gaussian processes by bridging the gap between the initial distribution and true posterior through learnable data-dependent initialization.

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [163] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: This paper studies transfer learning in multi-armed bandits, where prior knowledge from source distributions helps learn target distributions, and proposes an algorithm that matches theoretical bounds.


<details>
  <summary>Details</summary>
Motivation: To leverage transfer learning in bandit problems by using samples from source distributions to improve learning efficiency when target distributions are similar to sources.

Method: Developed KL-UCB-Transfer, an index policy that incorporates transfer parameters (distance bounds, sample sizes) and matches the derived asymptotic lower bound for Gaussian distributions.

Result: Theoretical analysis shows the algorithm achieves optimal regret bounds, and simulations demonstrate significant performance improvements over baseline methods when source and target distributions are close.

Conclusion: Transfer learning can effectively reduce regret in multi-armed bandits when prior knowledge is available, with KL-UCB-Transfer providing optimal performance guarantees.

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [164] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: DRO-REBEL introduces a unified family of robust RLHF updates using Wasserstein, KL, and χ² ambiguity sets to address overoptimization in offline RLHF, achieving optimal parametric rates with practical algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing offline RLHF approaches suffer from overoptimization where models overfit to reward misspecification and drift from preferred behaviors observed during training.

Method: Uses Fenchel duality to reduce updates to simple relative-reward regression, avoiding PPO-style clipping or auxiliary value networks. Derives practical SGD algorithms for three divergences: gradient regularization (Wasserstein), importance weighting (KL), and fast 1-D dual solve (χ²).

Result: Achieves O(n^{-1/4}) estimation bounds with tighter constants than prior approaches, recovering minimax-optimal O(n^{-1/2}) rate via localized Rademacher complexity analysis. Experiments show strong worst-case robustness across unseen preference mixtures, model sizes, and data scales.

Conclusion: Validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve optimal rates but forfeit coverage, while coverage-guaranteeing radii incur O(n^{-1/4}) rates. χ²-REBEL shows consistently strong empirical performance.

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [165] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL introduces concurrent asynchronous data generation and model training with in-flight weight updates to improve hardware efficiency and data freshness for RL-based LLM training.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for LLMs struggle with scaling due to poor hardware utilization and stale off-policy data that harms RL algorithm performance.

Method: PipelineRL uses concurrent asynchronous data generation and model training with novel in-flight weight updates, allowing LLM generation engine to receive updated model weights with minimal interruption during token sequence generation.

Result: Experiments on 128 H100 GPUs show PipelineRL achieves ~2x faster learning than conventional RL baselines while maintaining highly on-policy training data.

Conclusion: PipelineRL provides a superior trade-off between hardware efficiency and data on-policyness, with scalable open-source implementation released.

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [166] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: This paper studies how activation functions affect catastrophic forgetting in reinforcement learning, revealing that gradient sparsity is key. The authors propose 'elephant activation functions' that create sparse outputs and gradients, significantly improving neural network resilience to forgetting.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting has been a major challenge in reinforcement learning for decades. While recent works focus on algorithmic solutions, there's limited understanding of how neural network architecture properties contribute to forgetting. This study aims to fill this gap by examining activation functions' role.

Method: The authors analyze activation functions' training dynamics and their impact on catastrophic forgetting. They propose a new class of 'elephant activation functions' that generate both sparse outputs and sparse gradients. These are simply substituted for classical activation functions in value-based RL algorithms.

Result: Replacing classical activation functions with elephant activation functions significantly improves neural networks' resilience to catastrophic forgetting. This makes reinforcement learning more sample-efficient and memory-efficient.

Conclusion: Gradient sparsity of activation functions, in addition to sparse representations, plays a crucial role in reducing catastrophic forgetting. The proposed elephant activation functions provide an effective architectural solution to mitigate forgetting in reinforcement learning.

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [167] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: This paper introduces Functional Scaling Law (FSL) to model loss dynamics during LLM training, capturing learning rate schedule effects through a novel convolution-type functional term.


<details>
  <summary>Details</summary>
Motivation: Existing scaling laws focus only on final loss, ignoring training dynamics and learning rate schedule impacts. The authors aim to bridge this gap by studying training process dynamics.

Method: Uses teacher-student kernel regression with online SGD, develops intrinsic time viewpoint and SDE modeling to derive FSL that characterizes population risk evolution for general learning rate schedules.

Result: FSL makes learning rate schedule effects fully tractable. Theoretical justification provided for empirical practices: higher-capacity models are more efficient, learning rate decay improves efficiency, WSD schedules outperform direct-decay.

Conclusion: FSL framework deepens understanding of LLM pre-training dynamics and provides insights for improving large-scale model training, with practical relevance demonstrated across 0.1B to 1B parameter models.

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [168] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: Proposes a validation approach that extracts 'weak robust' samples from training data via local robustness analysis to identify model vulnerabilities early, enabling targeted performance enhancement against adversarial and corruption perturbations.


<details>
  <summary>Details</summary>
Motivation: Deep learning classifiers perform well on clean data but remain vulnerable to adversarial and common corruption perturbations, challenging model reliability. Traditional robustness validation relies on perturbed test datasets, which may not provide early vulnerability detection.

Method: Extracts 'weak robust' samples directly from training dataset through local robustness analysis. These samples are the most susceptible to perturbations and serve as early indicators of model vulnerabilities. Models are evaluated on these challenging training instances to understand robustness nuances.

Result: Demonstrated effectiveness on models trained with CIFAR-10, CIFAR-100, and ImageNet. The approach enables meaningful improvements in model reliability under adversarial and common corruption scenarios.

Conclusion: Robustness validation guided by weak robust samples provides a more nuanced understanding of model vulnerabilities and drives targeted performance enhancement, offering an effective alternative to traditional perturbed test dataset approaches.

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [169] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill is a knowledge distillation framework that enables efficient PPG analysis on wearable devices by transferring knowledge from large foundation models to smaller student models through multi-level distillation techniques.


<details>
  <summary>Details</summary>
Motivation: Large PPG foundation models are difficult to deploy on resource-limited wearable devices due to computational constraints, creating a need for efficient model compression techniques.

Method: Uses knowledge distillation with prediction-, feature-, and patch-level distillation. Specifically incorporates morphology distillation for local waveform patterns and rhythm distillation for inter-patch temporal structures.

Result: Achieves up to 21.8% improvement in student model performance for heart rate estimation and atrial fibrillation detection, with 7X faster inference and 19X memory reduction.

Conclusion: PPG-Distill enables efficient PPG analysis on wearable devices by significantly improving performance while dramatically reducing computational requirements.

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [170] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: This paper presents a systematic study of the computational costs and energy consumption of text-to-video generation models, developing analytical models and validating them through experiments on various T2V systems.


<details>
  <summary>Details</summary>
Motivation: Text-to-video generation systems have significant computational costs, but their energy demands remain poorly understood. The authors aim to provide systematic analysis of latency and energy consumption to enable more sustainable generative video systems.

Method: Developed a compute-bound analytical model predicting scaling laws for spatial resolution, temporal length, and denoising steps. Validated predictions through fine-grained experiments on WAN2.1-T2V and extended analysis to six diverse T2V models.

Result: Found quadratic growth in computational costs with spatial and temporal dimensions, and linear scaling with the number of denoising steps. Provided runtime and energy profiles for six T2V models under default settings.

Conclusion: The study provides benchmark references and practical insights for designing and deploying more sustainable generative video systems by understanding their computational and energy requirements.

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [171] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: This paper presents an ablation study analyzing different hybridization strategies for machine learning models used as fast surrogates for power flow simulation, evaluating their performance across accuracy, physical compliance, industrial readiness, and generalization.


<details>
  <summary>Details</summary>
Motivation: Power grids face increasing uncertainty due to renewable integration and cross-border exchanges, requiring fast simulation tools. Traditional physical solvers are accurate but too slow for real-time use, while ML models need better adherence to physical laws like Kirchhoff's laws.

Method: The study uses a custom benchmarking pipeline called LIPS to evaluate various hybridization strategies, including physical constraints as regularization terms or unsupervised losses, and different architectures from MLPs to graph-based networks that enable direct optimization of physics equations.

Result: The results demonstrate how different physical knowledge integration strategies impact performance across four key dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization.

Conclusion: The study provides insights into effective hybridization strategies for power flow simulation models, with all implementations being reproducible and available on GitHub, contributing to better grid stability management tools.

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [172] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: This paper presents a stability-based generalization analysis of adversarial training in decentralized networks, showing that generalization error increases with adversarial perturbation strength and training steps.


<details>
  <summary>Details</summary>
Motivation: While adversarial training enhances model robustness, it suffers from robust overfitting and enlarged generalization gaps. Although convergence has been established for decentralized adversarial training, its generalization properties remain unexplored.

Method: The authors use algorithmic stability analysis under the diffusion strategy for convex losses to derive generalization bounds for decentralized adversarial training.

Result: The theoretical analysis shows that generalization error grows with both adversarial perturbation strength and number of training steps, which is consistent with single-agent cases but novel for decentralized settings. Numerical experiments on logistic regression validate these findings.

Conclusion: This work provides the first stability-based generalization analysis for adversarial training in decentralized networks, establishing theoretical foundations for understanding generalization properties in distributed robust learning settings.

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [173] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: This paper challenges the 'longer-is-better' assumption for chain-of-thought reasoning, finding that longer CoTs and increased review actually lower accuracy. The authors introduce a Failed-Step Fraction metric that better predicts correctness and show that removing failed branches improves reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models spend substantial compute on long chain-of-thought traces, but what characterizes an effective CoT remains unclear. Prior work shows conflicting results about CoT length effectiveness, motivating systematic evaluation.

Method: Systematic evaluation across ten large reasoning models on math and scientific reasoning tasks. Introduces graph view of CoT to extract structure and Failed-Step Fraction metric. Designs two interventions: ranking CoTs by metrics and editing CoTs to remove failed branches.

Result: Both naive CoT lengthening and increased review are associated with lower accuracy. Failed-Step Fraction consistently outpredicts length and review ratio for correctness. Removing failed branches significantly improves accuracy, indicating failed branches bias subsequent reasoning.

Conclusion: Effective CoTs are those that fail less, supporting structure-aware test-time scaling over indiscriminately generating long CoT traces. Failed-Step Fraction is a key metric for characterizing reasoning quality.

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>
