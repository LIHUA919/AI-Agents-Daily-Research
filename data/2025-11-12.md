<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 26]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Analysing Environmental Efficiency in AI for X-Ray Diagnosis](https://arxiv.org/abs/2511.07436)
*Liam Kearns*

Main category: cs.AI

TL;DR: This paper compares generative (LLMs) and discriminative models for COVID-19 detection in chest X-rays via a Mendix app, finding smaller discriminative models like Covid-Net achieve highest accuracy (95.5%) with minimal carbon footprint, while LLMs show environmental and performance risks for classification tasks.


<details>
  <summary>Details</summary>
Motivation: To benchmark the accuracy and environmental impact of using versatile but resource-intensive large language models (LLMs) versus smaller, specialized discriminative models for COVID-19 detection in medical applications, highlighting potential inefficiencies and biases.

Method: Integration of LLMs (e.g., ChatGPT, Claude) and small discriminative models into a Mendix application to detect COVID-19 in chest X-rays; 14 model configurations were tested, with discriminative models also providing knowledge bases to LLMs to enhance accuracy.

Result: Smaller discriminative models (e.g., Covid-Net) achieved the highest accuracy (95.5%) with a 99.9% lower carbon footprint compared to large LLMs like GPT-4.5-Preview. LLMs restricted to probabilistic output performed poorly in accuracy and environmental impact, and smaller models showed bias toward positive diagnoses with low confidence.

Conclusion: Specialized discriminative models are more efficient and accurate for COVID-19 detection than LLMs, which pose environmental risks and performance issues when used as universal solutions for classification tasks in medical applications.

Abstract: The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility and ease of use through APIs, these larger models are often utilised even though smaller, custom models can be used instead. In this paper, LLMs and small discriminative models are integrated into a Mendix application to detect Covid-19 in chest X-rays. These discriminative models are also used to provide knowledge bases for LLMs to improve accuracy. This provides a benchmark study of 14 different model configurations for comparison of accuracy and environmental impact. The findings indicated that while smaller models reduced the carbon footprint of the application, the output was biased towards a positive diagnosis and the output probabilities were lacking confidence. Meanwhile, restricting LLMs to only give probabilistic output caused poor performance in both accuracy and carbon footprint, demonstrating the risk of using LLMs as a universal AI solution. While using the smaller LLM GPT-4.1-Nano reduced the carbon footprint by 94.2% compared to the larger models, this was still disproportionate to the discriminative models; the most efficient solution was the Covid-Net model. Although it had a larger carbon footprint than other small models, its carbon footprint was 99.9% less than when using GPT-4.5-Preview, whilst achieving an accuracy of 95.5%, the highest of all models examined. This paper contributes to knowledge by comparing generative and discriminative models in Covid-19 detection as well as highlighting the environmental risk of using generative tools for classification tasks.

</details>


### [2] [Agentic Educational Content Generation for African Languages on Edge Devices](https://arxiv.org/abs/2511.07437)
*Ravi Gupta,Guneet Bhatia*

Main category: cs.AI

TL;DR: AI-driven framework for culturally adaptive educational content in Africa, achieving high performance on edge devices with excellent multilingual quality and cultural relevance.


<details>
  <summary>Details</summary>
Motivation: To address educational inequity in Sub-Saharan Africa by providing culturally relevant educational content in resource-constrained environments.

Method: An autonomous agent-orchestrated framework using four specialized agents for decentralized, culturally adaptive educational content generation on edge devices.

Result: Excellent performance on edge devices: Jetson Nano - 129ms TTFT, 45.2 tokens/sec at 8.4W; Raspberry Pi 4B - 326ms TTFT, 15.9 tokens/sec at 5.8W. High quality metrics: BLEU score 0.688, cultural relevance 4.4/5, fluency 4.2/5 across African languages.

Conclusion: The research establishes a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments, contributing to UN SDGs 4, 9, and 10.

Abstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.

</details>


### [3] [Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://arxiv.org/abs/2511.07483)
*Qianxi He,Qingyu Ren,Shanzhe Lei,Xuhong Wang,Yingchun Wang*

Main category: cs.AI

TL;DR: A confidence-based reward model for STEM reasoning that penalizes both incorrect answers and low-confidence correct responses, outperforming state-of-the-art methods in STEM benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based RL for LLMs often results in poor reasoning chains or inconsistencies, especially with smaller models, limiting resource-constrained organizations from effective RL training.

Method: Proposes a novel confidence-based reward model that penalizes incorrect answers and low-confidence correct responses, validated through static evaluations, Best-of-N inference, and PPO-based RL training.

Result: Outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks.

Conclusion: The confidence-based reward model effectively enhances STEM reasoning capabilities by promoting robust and logically consistent reasoning, making RL training more accessible for smaller-scale models.

Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.

</details>


### [4] [Procedural Knowledge Improves Agentic LLM Workflows](https://arxiv.org/abs/2511.07568)
*Vincent Hsiao,Mark Roberts,Leslie Smith*

Main category: cs.AI

TL;DR: Using hierarchical task networks (HTNs) as procedural knowledge significantly improves LLM performance on agentic tasks, allowing smaller models (20b/70b) to outperform larger ones (120b). LLM-created HTNs also help but less than hand-coded ones.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with agentic tasks without extensive tool support or fine-tuning, despite evidence that procedural knowledge boosts planning efficiency. There's limited research on using such knowledge for LLM agentic tasks requiring implicit planning.

Method: Formalize, implement, and evaluate an agentic LLM workflow that incorporates procedural knowledge via hierarchical task networks (HTNs), comparing hand-coded and LLM-created HTNs.

Result: Hand-coded HTNs dramatically improve LLM task performance; 20b/70b models with HTNs beat a 120b baseline. LLM-generated HTNs also enhance performance but less effectively.

Conclusion: Leveraging curated procedural knowledge—from humans, documents, or LLMs—is a vital tool for enhancing LLM workflows, highlighting the value of expertise integration.

Abstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.

</details>


### [5] [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151)
*Xuchen Li,Ruitao Wu,Xuanbo Liu,Xukai Wang,Jinbo Hu,Zhixin Bai,Bohan Zeng,Hao Liang,Leheng Chen,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Xu-Yao Zhang,Liu Liu,Jia Li,Kaiqi Huang,Jiahao Xu,Haitao Mi,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: SciAgent is a hierarchical multi-agent system that achieves expert-level performance across scientific Olympiads by dynamically orchestrating specialized reasoning agents, demonstrating generalistic scientific intelligence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current AI systems that achieve expert-level performance on domain-specific scientific tasks but remain narrow and handcrafted, by developing a unified multi-agent system for generalistic scientific reasoning across disciplines.

Method: SciAgent organizes problem solving as a hierarchical process where a Coordinator Agent interprets each problem's domain and complexity, then dynamically orchestrates specialized Worker Systems composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification.

Result: Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, and it also performs well on the International Chemistry Olympiad and Humanity's Last Exam benchmark, confirming its ability to generalize across diverse scientific domains.

Conclusion: SciAgent establishes a concrete step toward generalistic scientific intelligence, demonstrating that AI systems can achieve coherent, cross-disciplinary reasoning at expert levels across multiple scientific domains.

Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

</details>


### [6] [Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models](https://arxiv.org/abs/2511.07581)
*Supriti Vijay,Aman Priyanshu,Anu Vellore,Baturay Saglam,Amin Karbasi*

Main category: cs.AI

TL;DR: Orion is a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies, outperforming much larger retrievers on multiple benchmarks despite using minimal training data.


<details>
  <summary>Details</summary>
Motivation: Current retrieval approaches lack reasoning capabilities, are too expensive (LLMs), or only perform static transformations, failing to capture the iterative dynamics needed for complex queries.

Method: Orion combines synthetic trajectory generation with supervised fine-tuning, reinforcement learning to reward effective query refinement/backtracking, and inference-time beam search exploiting learned self-reflection.

Result: The 1.2B Orion model achieves 77.6% success on SciFact (vs. 72.6% prior), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), remaining competitive on other benchmarks while outperforming retrievers 200-400x larger on 5/6 benchmarks.

Conclusion: Retrieval performance can emerge from learned strategies rather than model scale alone when models are trained to search, reflect, and revise iteratively.

Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

</details>


### [7] [Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces](https://arxiv.org/abs/2511.07587)
*Shreyas Rajesh,Pavan Holur,Chenda Duan,David Chong,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: GSW is a generative memory framework that helps LLMs track evolving situations through structured representations, improving reasoning over long texts by 20% and reducing context tokens by 51% compared to RAG baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long-context reasoning as documents exceed context windows and performance degrades with length. Current solutions fail to create narrative representations needed to track entities through episodic events.

Method: GSW consists of an Operator that maps observations to semantic structures and a Reconciler that integrates them into a persistent workspace enforcing temporal, spatial, and logical coherence.

Result: On the Episodic Memory Benchmark with 100k-1M token corpora, GSW outperforms RAG baselines by up to 20% in performance and reduces query-time context tokens by 51%.

Conclusion: GSW provides a blueprint for human-like episodic memory in LLMs, enabling better reasoning over long horizons and more capable AI agents.

Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.

</details>


### [8] [AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design for Group Workload Investigation](https://arxiv.org/abs/2511.07667)
*Jakub Slapek,Mir Seyedebrahimi,Yang Jianhua*

Main category: cs.AI

TL;DR: Proposes an AI-enhanced framework using LLMs to analyze team artifacts and detect contribution conflicts through multi-dimensional benchmarks and inequality measures.


<details>
  <summary>Details</summary>
Motivation: Current team contribution assessment methods are prone to conflict and unfair evaluation, requiring costly manual intervention.

Method: Organizes team artifacts into three dimensions with nine benchmarks, normalizes objective measures, applies inequality metrics, and uses LLM architecture for contextual analysis.

Result: Framework enables automated conflict detection and generates interpretable advisory judgments for fairer team evaluation.

Conclusion: The proposed AI-enhanced tool shows feasibility for practical implementation with proper bias safeguards, addressing limitations in current conflict resolution methods.

Abstract: The equitable assessment of individual contribution in teams remains a persistent challenge, where conflict and disparity in workload can result in unfair performance evaluation, often requiring manual intervention - a costly and challenging process. We survey existing tool features and identify a gap in conflict resolution methods and AI integration. To address this, we propose a framework and implementation design for a novel AI-enhanced tool that assists in dispute investigation. The framework organises heterogeneous artefacts - submissions (code, text, media), communications (chat, email), coordination records (meeting logs, tasks), peer assessments, and contextual information - into three dimensions with nine benchmarks: Contribution, Interaction, and Role. Objective measures are normalised, aggregated per dimension, and paired with inequality measures (Gini index) to surface conflict markers. A Large Language Model (LLM) architecture performs validated and contextual analysis over these measures to generate interpretable and transparent advisory judgments. We argue for feasibility under current statutory and institutional policy, and outline practical analytics (sentimental, task fidelity, word/line count, etc.), bias safeguards, limitations, and practical challenges.

</details>


### [9] [Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions](https://arxiv.org/abs/2511.07669)
*Alejandro R. Jadad*

Main category: cs.AI

TL;DR: LLMs fail in high-stakes decisions due to biases; a partnership framework with bias-monitoring architecture enables reliable human-AI teams through calibration and maintenance protocols.


<details>
  <summary>Details</summary>
Motivation: Address LLM unreliability in strategic decisions caused by cognitive biases, threatening investment sustainability.

Method: Qualitative assessment across 7 LLMs and 3 venture vignettes using a 7-stage calibration sequence within a 5-layer protection architecture to mitigate biases.

Result: Achieved partnership state with three key discoveries: calibration enables partnership but requires maintenance, reliability degrades under drift/exhaustion, and dissolution discipline prevents errors. Cross-model validation showed performance differences.

Conclusion: Human-AI teams can prevent regret in high-stakes decisions, meeting ROI expectations by avoiding cognitive traps when verification is delayed.

Abstract: Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action but prove less reliable for high-stakes strategic decisions with uncertain outcomes. This gap, driven by mutually reinforcing cognitive biases in both humans and artificial intelligence (AI) systems, threatens the defensibility of valuations and sustainability of investments in the sector.
  This report describes a framework emerging from systematic qualitative assessment across 7 frontier-grade LLMs and 3 market-facing venture vignettes under time pressure. Detailed prompting specifying decision partnership and explicitly instructing avoidance of sycophancy, confabulation, solution drift, and nihilism achieved initial partnership state but failed to maintain it under operational pressure. Sustaining protective partnership state required an emergent 7-stage calibration sequence, built upon a 4-stage initialization process, within a 5-layer protection architecture enabling bias self-monitoring, human-AI adversarial challenge, partnership state verification, performance degradation detection, and stakeholder protection.
  Three discoveries resulted: partnership state is achievable through ordered calibration but requires emergent maintenance protocols; reliability degrades when architectural drift and context exhaustion align; and dissolution discipline prevents costly pursuit of fundamentally wrong directions. Cross-model validation revealed systematic performance differences across LLM architectures.
  This approach demonstrates that human-AI teams can achieve cognitive partnership capable of preventing avoidable regret in high-stakes decisions, addressing return-on-investment expectations that depend on AI systems supporting consequential decision-making without introducing preventable cognitive traps when verification arrives too late.

</details>


### [10] [AIA Forecaster: Technical Report](https://arxiv.org/abs/2511.07678)
*Rohan Alur,Bradly C. Stadie,Daniel Kang,Ryan Chen,Matt McManus,Michael Rickert,Tyler Lee,Michael Federici,Richard Zhu,Dennis Fogerty,Hayley Williamson,Nina Lozinski,Aaron Linsky,Jasjeet S. Sekhon*

Main category: cs.AI

TL;DR: The AIA Forecaster is an LLM-based system that achieves human superforecaster performance through agentic search, supervisor reconciliation, and statistical bias correction.


<details>
  <summary>Details</summary>
Motivation: To create an AI system capable of expert-level forecasting using unstructured data, surpassing prior LLM limitations.

Method: Combines agentic search over news sources, supervisor agent for forecast reconciliation, and statistical calibration techniques to counter LLM biases.

Result: Matches human superforecaster performance on ForecastBench and provides additive information when combined with market consensus on prediction markets.

Conclusion: Establishes new state-of-the-art in AI forecasting with verifiable expert-level performance at scale and practical recommendations for future research.

Abstract: This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.

</details>


### [11] [ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents](https://arxiv.org/abs/2511.07685)
*Manasi Sharma,Chen Bo Calvin Zhang,Chaithanya Bandi,Clinton Wang,Ankit Aich,Huy Nghiem,Tahseen Rabbani,Ye Htet,Brian Jang,Sumana Basu,Aishwarya Balwani,Denis Peskoff,Marcos Ayestaran,Sean M. Hendryx,Brad Kenstler,Bing Liu*

Main category: cs.AI

TL;DR: ResearchRubrics is a new benchmark for evaluating Deep Research (DR) agents with 2,500+ expert-written rubrics across factual grounding, reasoning soundness, and clarity, showing current leading agents achieve under 68% compliance.


<details>
  <summary>Details</summary>
Motivation: Evaluating DR agents is challenging due to lengthy, diverse responses with many valid solutions and dependence on dynamic information sources, requiring standardized assessment methods.

Method: Created ResearchRubrics benchmark with 2,800+ hours of human labor, pairing realistic domain-diverse prompts with fine-grained rubrics. Developed complexity framework (conceptual breadth, logical nesting, exploration) and evaluation protocols for DR agents.

Result: Leading DR agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with rubrics, primarily failing on implicit context and reasoning about retrieved information.

Conclusion: There's a need for robust, scalable DR assessment; ResearchRubrics is released to facilitate progress toward well-justified research assistants.

Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.

</details>


### [12] [Towards AI-Assisted Generation of Military Training Scenarios](https://arxiv.org/abs/2511.07690)
*Soham Hans,Volkan Ustun,Benjamin Nye,James Sterrett,Matthew Green*

Main category: cs.AI

TL;DR: Introduces a multi-agent, multi-modal LLM framework to automate complex military training scenario generation, overcoming pre-LLM AI limitations. It decomposes generation into subproblems handled by specialized agents, validated via OPORD generation.


<details>
  <summary>Details</summary>
Motivation: Traditional manual scenario generation for expert-level simulation training is laborious and resource-intensive. Pre-LLM AI tools failed to produce sufficiently complex or adaptable scenarios, necessitating an advanced automated solution.

Method: A hierarchical multi-agent framework decomposes scenario generation into subproblems (e.g., generating options, candidate products, or full artifacts). Specialized LLM agents process text and visual inputs sequentially, ensuring logical consistency.

Result: Proof-of-concept successfully generates the scheme of maneuver and movement section of an Operations Order (OPORD), accurately estimating map positions and movements, demonstrating coherence and adaptability.

Conclusion: The framework shows LLM-driven multi-agent systems can automate nuanced document generation for military training, dynamically adapting to changes, marking a significant advancement in scenario generation automation.

Abstract: Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.

</details>


### [13] [Operational machine learning for remote spectroscopic detection of CH$_{4}$ point sources](https://arxiv.org/abs/2511.07719)
*Vít Růžička,Gonzalo Mateo-García,Itziar Irakulis-Loitxate,Juan Emmanuel Johnson,Manuel Montesino San Martín,Anna Allen,Luis Guanter,David R. Thompson*

Main category: cs.AI

TL;DR: Machine learning system deployed in UN's Methane Alert and Response System to automatically detect methane plumes from satellite data, reducing false detections by 74% through model ensembling and enabling rapid verification of 1,351 leaks.


<details>
  <summary>Details</summary>
Motivation: Current satellite-based methane detection methods produce high false positives requiring manual verification, creating operational inefficiencies for global methane monitoring.

Method: Created largest annotated methane plume dataset from 3 imaging spectrometers; compared deep learning models with full-granule evaluation; implemented model ensembling to reduce false detections.

Result: System processed scenes automatically, reduced false detections by over 74%, facilitated verification of 1,351 distinct methane leaks with 479 stakeholder notifications during 7-month deployment.

Conclusion: The AI system represents critical progress toward global automated methane leak detection, demonstrating operational viability and ability to handle increasing satellite data volumes.

Abstract: Mitigating anthropogenic methane sources is one the most cost-effective levers to slow down global warming. While satellite-based imaging spectrometers, such as EMIT, PRISMA, and EnMAP, can detect these point sources, current methane retrieval methods based on matched filters still produce a high number of false detections requiring laborious manual verification. This paper describes the operational deployment of a machine learning system for detecting methane emissions within the Methane Alert and Response System (MARS) of the United Nations Environment Programme's International Methane Emissions Observatory. We created the largest and most diverse global dataset of annotated methane plumes from three imaging spectrometer missions and quantitatively compared different deep learning model configurations. Focusing on the requirements for operational deployment, we extended prior evaluation methodologies from small tiled datasets to full granule evaluation. This revealed that deep learning models still produce a large number of false detections, a problem we address with model ensembling, which reduced false detections by over 74%. Deployed in the MARS pipeline, our system processes scenes and proposes plumes to analysts, accelerating the detection and analysis process. During seven months of operational deployment, it facilitated the verification of 1,351 distinct methane leaks, resulting in 479 stakeholder notifications. We further demonstrate the model's utility in verifying mitigation success through case studies in Libya, Argentina, Oman, and Azerbaijan. Our work represents a critical step towards a global AI-assisted methane leak detection system, which is required to process the dramatically higher data volumes expected from new and current imaging spectrometers.

</details>


### [14] [Alignment-Aware Quantization for LLM Safety](https://arxiv.org/abs/2511.07842)
*Sunghyun Wee,Suyoung Kim,Hyeonjin Kim,Kyomin Hwang,Nojun Kwak*

Main category: cs.AI

TL;DR: AAQ is a novel quantization method that preserves LLM safety alignment through contrastive learning, enabling efficient 4-bit models without specialized safety datasets.


<details>
  <summary>Details</summary>
Motivation: Conventional PTQ prioritizes low perplexity but compromises safety alignment, creating a vulnerability where quantized models maintain good perplexity but lose safety.

Method: Integrates Alignment-Preserving Contrastive (APC) loss into PTQ pipeline, making quantized models mimic safe instruction-tuned versions while diverging from unsafe pre-trained counterparts.

Result: Achieves robust 4-bit (W4A4) quantization across LLaMA, Qwen, and Mistral models while maintaining safety where previous methods fail, without needing safety-specific calibration data.

Conclusion: Resolves the efficiency-safety trade-off in LLM quantization, enabling both efficient and trustworthy models through alignment-aware quantization.

Abstract: Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained to follow human alignment for safety, and post training quantization(PTQ) is applied afterward for efficiency. However, these two objectives are often in conflict, revealing a fundamental flaw in the conventional PTQ paradigm: quantization can turn into a safety vulnerability if it only aims to achieve low perplexity. Models can demonstrate low perplexity yet exhibit significant degradation in alignment with the safety policy, highlighting that perplexity alone is an insufficient and often misleading proxy for model safety. To address this, we propose Alignment-Aware Quantization(AAQ), a novel approach that integrates Alignment-Preserving Contrastive(APC) loss into the PTQ pipeline. Compared to simple reconstruction loss, ours explicitly preserves alignment by encouraging the quantized model to mimic its safe, instruction-tuned model while diverging from the unaligned, pre-trained counterpart. Our method achieves this robust safety alignment without resorting to specialized safety-focused calibration datasets, highlighting its practical utility and broad applicability. AAQ is compatible with standard PTQ techniques and enables robust 4-bit (W4A4) quantization across diverse model families such as LLaMA, Qwen, and Mistral while maintaining safety where previous methods fail. Our work resolves the critical trade-off between efficiency and safety, paving the way toward LLMs that are both efficient and trustworthy. Anonymized code is available in the supplementary material.

</details>


### [15] [GAMA: A Neural Neighborhood Search Method with Graph-aware Multi-modal Attention for Vehicle Routing Problem](https://arxiv.org/abs/2511.07850)
*Xiangling Chen,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: GAMA is a neural neighborhood search method for Vehicle Routing Problems that uses Graph-aware Multi-modal Attention to encode problem instances and solutions as distinct modalities, modeling their interactions through attention mechanisms to improve operator selection.


<details>
  <summary>Details</summary>
Motivation: Existing neural approaches for VRPs use simplistic state representations and naive information fusion, limiting their ability to capture rich structural and semantic context.

Method: Encodes problem instances and evolving solutions as distinct modalities using graph neural networks, models intra- and inter-modal interactions through stacked self- and cross-attention layers, and integrates representations via gated fusion mechanism.

Result: Extensive experiments show GAMA significantly outperforms recent neural baselines across various synthetic and benchmark instances.

Conclusion: The multi-modal attention mechanism and gated fusion design are key to achieving performance gains, demonstrating GAMA's effectiveness for neural neighborhood search in VRPs.

Abstract: Recent advances in neural neighborhood search methods have shown potential in tackling Vehicle Routing Problems (VRPs). However, most existing approaches rely on simplistic state representations and fuse heterogeneous information via naive concatenation, limiting their ability to capture rich structural and semantic context. To address these limitations, we propose GAMA, a neural neighborhood search method with Graph-aware Multi-modal Attention model in VRP. GAMA encodes the problem instance and its evolving solution as distinct modalities using graph neural networks, and models their intra- and inter-modal interactions through stacked self- and cross-attention layers. A gated fusion mechanism further integrates the multi-modal representations into a structured state, enabling the policy to make informed and generalizable operator selection decisions. Extensive experiments conducted across various synthetic and benchmark instances demonstrate that the proposed algorithm GAMA significantly outperforms the recent neural baselines. Further ablation studies confirm that both the multi-modal attention mechanism and the gated fusion design play a key role in achieving the observed performance gains.

</details>


### [16] [WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking](https://arxiv.org/abs/2511.07863)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: WaterMod is a probability-aware watermarking technique for LLMs that embeds detectable signals while maintaining generation quality by using modular partitioning of vocabulary ranks.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods degrade fluency by excluding high-probability tokens; WaterMod addresses this by ensuring at least one high-probability token remains available.

Method: Sorts vocabulary by probability, partitions ranks mod k, applies bias to one class; uses entropy-adaptive gate for zero-bit setting and payload digit selection for multi-bit.

Result: Achieves strong watermark detection while preserving quality across natural language, math, and code tasks in both zero-bit and multi-bit settings.

Conclusion: WaterMod provides robust, versatile watermarking for LLM outputs, balancing detection reliability with generation fluency.

Abstract: Large language models now draft news, legal analyses, and software code with human-level fluency. At the same time, regulations such as the EU AI Act mandate that each synthetic passage carry an imperceptible, machine-verifiable mark for provenance. Conventional logit-based watermarks satisfy this requirement by selecting a pseudorandom green vocabulary at every decoding step and boosting its logits, yet the random split can exclude the highest-probability token and thus erode fluency. WaterMod mitigates this limitation through a probability-aware modular rule. The vocabulary is first sorted in descending model probability; the resulting ranks are then partitioned by the residue rank mod k, which distributes adjacent-and therefore semantically similar-tokens across different classes. A fixed bias of small magnitude is applied to one selected class. In the zero-bit setting (k=2), an entropy-adaptive gate selects either the even or the odd parity as the green list. Because the top two ranks fall into different parities, this choice embeds a detectable signal while guaranteeing that at least one high-probability token remains available for sampling. In the multi-bit regime (k>2), the current payload digit d selects the color class whose ranks satisfy rank mod k = d. Biasing the logits of that class embeds exactly one base-k digit per decoding step, thereby enabling fine-grained provenance tracing. The same modular arithmetic therefore supports both binary attribution and rich payloads. Experimental results demonstrate that WaterMod consistently attains strong watermark detection performance while maintaining generation quality in both zero-bit and multi-bit settings. This robustness holds across a range of tasks, including natural language generation, mathematical reasoning, and code synthesis. Our code and data are available at https://github.com/Shinwoo-Park/WaterMod.

</details>


### [17] [Confidence-Aware Neural Decoding of Overt Speech from EEG: Toward Robust Brain-Computer Interfaces](https://arxiv.org/abs/2511.07890)
*Soowon Kim,Byung-Kwan Ko,Seo-Hyun Lee*

Main category: cs.AI

TL;DR: Framework improves brain-computer interface reliability by adding confidence awareness and selective classification to EEG speech decoding.


<details>
  <summary>Details</summary>
Motivation: Need for both accurate and trustworthy non-invasive brain-computer interfaces that decode spoken commands from EEG signals.

Method: Deep ensembles of compact, speech-oriented convolutional networks with post-hoc calibration and selective classification using ensemble-based uncertainty metrics.

Result: More reliable probability estimates, improved selective performance across operating points, and balanced per-class acceptance compared to baselines.

Conclusion: Confidence-aware neural decoding can provide robust, deployment-oriented behavior for real-world brain-computer interface communication systems.

Abstract: Non-invasive brain-computer interfaces that decode spoken commands from electroencephalogram must be both accurate and trustworthy. We present a confidence-aware decoding framework that couples deep ensembles of compact, speech-oriented convolutional networks with post-hoc calibration and selective classification. Uncertainty is quantified using ensemble-based predictive entropy, top-two margin, and mutual information, and decisions are made with an abstain option governed by an accuracy-coverage operating point. The approach is evaluated on a multi-class overt speech dataset using a leakage-safe, block-stratified split that respects temporal contiguity. Compared with widely used baselines, the proposed method yields more reliable probability estimates, improved selective performance across operating points, and balanced per-class acceptance. These results suggest that confidence-aware neural decoding can provide robust, deployment-oriented behavior for real-world brain-computer interface communication systems.

</details>


### [18] [Toward Robust EEG-based Intention Decoding during Misarticulated Speech in Aphasia](https://arxiv.org/abs/2511.07895)
*Ha-Na Jo,Jung-Sun Lee,Eunyeong Ko*

Main category: cs.AI

TL;DR: EEG-based communication system for aphasia uses multitask learning and delta feature analysis to decode speech intention even during misarticulations, achieving significant performance improvements over baseline.


<details>
  <summary>Details</summary>
Motivation: Aphasia severely limits verbal communication due to impaired language production and frequent misarticulations. Despite growing interest in brain-computer interfaces, little attention has been paid to developing EEG-based communication support systems specifically for aphasic patients.

Method: Used a soft multitask learning framework with maximum mean discrepancy regularization focusing on delta features to jointly optimize class discrimination while aligning EEG feature distributions of correct and misarticulated trials.

Result: The proposed model achieved 58.6% accuracy for correct trials and 45.5% for misarticulated trials, outperforming the baseline by over 45% on misarticulated trials, demonstrating robust intention decoding under articulation errors.

Conclusion: The study demonstrates the feasibility of EEG-based assistive systems for aphasia patients, showing that intention decoding remains possible even during articulation errors through specialized deep learning approaches.

Abstract: Aphasia severely limits verbal communication due to impaired language production, often leading to frequent misarticulations during speech attempts. Despite growing interest in brain-computer interface technologies, relatively little attention has been paid to developing EEG-based communication support systems tailored for aphasic patients. To address this gap, we recruited a single participant with expressive aphasia and conducted an Korean-based automatic speech task. EEG signals were recorded during task performance, and each trial was labeled as either correct or incorrect depending on whether the intended word was successfully spoken. Spectral analysis revealed distinct neural activation patterns between the two trial types: misarticulated trials exhibited excessive delta power across widespread channels and increased theta-alpha activity in frontal regions. Building upon these findings, we developed a soft multitask learning framework with maximum mean discrepancy regularization that focus on delta features to jointly optimize class discrimination while aligning the EEG feature distributions of correct and misarticulated trials. The proposed model achieved 58.6 % accuracy for correct and 45.5 % for misarticulated trials-outperforming the baseline by over 45 % on the latter-demonstrating robust intention decoding even under articulation errors. These results highlight the feasibility of EEG-based assistive systems capable of supporting real-world, imperfect speech conditions in aphasia patients.

</details>


### [19] [SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder](https://arxiv.org/abs/2511.07896)
*Dengcan Liu,Jiahao Li,Zheren Fu,Yi Tu,Jiajun Li,Zhendong Mao,Yongdong Zhang*

Main category: cs.AI

TL;DR: SparseRM is a lightweight reward model that uses Sparse Autoencoder to extract preference-relevant features from LLM representations, achieving superior performance with less than 1% trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Training reliable reward models is challenging due to the need for large-scale preference annotations and high fine-tuning costs, making efficient RM development under limited resources a key problem.

Method: Uses Sparse Autoencoder (SAE) to decompose LLM representations into interpretable preference-relevant directions, projects representations onto these directions to compute alignment scores, and aggregates them with a simple reward head.

Result: SparseRM achieves superior performance over most mainstream reward models on three preference modeling tasks while using less than 1% of trainable parameters.

Conclusion: The method enables construction of lightweight and interpretable reward models that integrate seamlessly into downstream alignment pipelines, highlighting potential for efficient LLM alignment.

Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

</details>


### [20] [Data Descriptions from Large Language Models with Influence Estimation](https://arxiv.org/abs/2511.07897)
*Chaeri Kim,Jaeyeon Bae,Taehwan Kim*

Main category: cs.AI

TL;DR: 该论文提出了一种使用大语言模型生成文本描述来解释图像数据的新方法，通过结合外部知识库和影响力估计来选择最有信息量的描述，并在跨模态迁移分类任务中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在许多领域取得了成功，但其行为仍然像黑箱一样难以理解。现有的可解释AI方法主要关注解释模型如何做出预测，而本文希望通过深度学习和语言模型来理解和解释数据本身，使人类能够更容易理解。

Method: 提出了一种生成文本描述来解释数据的流程：1）使用大语言模型结合外部知识库生成文本描述；2）利用影响力估计和CLIP评分选择最有信息量的描述；3）提出跨模态迁移分类基准任务来评估文本描述的有效性。

Result: 在零样本设置实验中，该方法生成的文本描述比基线描述更有效，并且在九个图像分类数据集上成功提升了仅使用图像训练的模型性能。GPT-4o评估进一步支持了这些结果。

Conclusion: 通过该方法可以获得对模型决策过程内在可解释性的洞察，证明生成的文本描述能够有效解释数据并提升模型性能，为理解深度学习模型提供了新的途径。

Abstract: Deep learning models have been successful in many areas but understanding their behaviors still remains a black-box. Most prior explainable AI (XAI) approaches have focused on interpreting and explaining how models make predictions. In contrast, we would like to understand how data can be explained with deep learning model training and propose a novel approach to understand the data via one of the most common media - language - so that humans can easily understand. Our approach proposes a pipeline to generate textual descriptions that can explain the data with large language models by incorporating external knowledge bases. However, generated data descriptions may still include irrelevant information, so we introduce to exploit influence estimation to choose the most informative textual descriptions, along with the CLIP score. Furthermore, based on the phenomenon of cross-modal transferability, we propose a novel benchmark task named cross-modal transfer classification to examine the effectiveness of our textual descriptions. In the experiment of zero-shot setting, we show that our textual descriptions are more effective than other baseline descriptions, and furthermore, we successfully boost the performance of the model trained only on images across all nine image classification datasets. These results are further supported by evaluation using GPT-4o. Through our approach, we may gain insights into the inherent interpretability of the decision-making process of the model.

</details>


### [21] [DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion](https://arxiv.org/abs/2511.07901)
*Haoning Li,Qinghua Huang*

Main category: cs.AI

TL;DR: DANS-KGC uses diffusion models to generate adaptive negative samples for knowledge graph completion, overcoming limitations of traditional negative sampling methods by dynamically controlling sample hardness throughout training.


<details>
  <summary>Details</summary>
Motivation: Existing negative sampling strategies for knowledge graphs suffer from vulnerability to false negatives, limited generalization, and lack of control over sample hardness, which limits their effectiveness.

Method: Three-component system: Difficulty Assessment Module evaluates entity learning difficulty; Adaptive Negative Sampling Module uses conditional diffusion model with difficulty-aware noise scheduling; Dynamic Training Mechanism adjusts hardness distribution throughout training in curriculum-style progression.

Result: Achieved state-of-the-art results on all three evaluation metrics for UMLS and YAGO3-10 datasets, demonstrating effectiveness and generalization across six benchmark datasets.

Conclusion: DANS-KGC provides an effective solution for knowledge graph completion by adaptively generating negative samples with controlled hardness through diffusion models, significantly improving representation learning.

Abstract: Negative sampling (NS) strategies play a crucial role in knowledge graph representation. In order to overcome the limitations of existing negative sampling strategies, such as vulnerability to false negatives, limited generalization, and lack of control over sample hardness, we propose DANS-KGC (Diffusion-based Adaptive Negative Sampling for Knowledge Graph Completion). DANS-KGC comprises three key components: the Difficulty Assessment Module (DAM), the Adaptive Negative Sampling Module (ANS), and the Dynamic Training Mechanism (DTM). DAM evaluates the learning difficulty of entities by integrating semantic and structural features. Based on this assessment, ANS employs a conditional diffusion model with difficulty-aware noise scheduling, leveraging semantic and neighborhood information during the denoising phase to generate negative samples of diverse hardness. DTM further enhances learning by dynamically adjusting the hardness distribution of negative samples throughout training, enabling a curriculum-style progression from easy to hard examples. Extensive experiments on six benchmark datasets demonstrate the effectiveness and generalization ability of DANS-KGC, with the method achieving state-of-the-art results on all three evaluation metrics for the UMLS and YAGO3-10 datasets.

</details>


### [22] [Neurophysiological Characteristics of Adaptive Reasoning for Creative Problem-Solving Strategy](https://arxiv.org/abs/2511.07912)
*Jun-Young Kim,Young-Seok Kweon,Gi-Hwan Shin,Seong-Whan Lee*

Main category: cs.AI

TL;DR: Study compares human adaptive reasoning with multimodal LLM, finding humans use coordinated delta-theta-alpha brain oscillations for rule learning while LLMs lack genuine adaptive capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand the neural mechanisms underlying human adaptive reasoning and compare it with artificial intelligence systems like multimodal large language models.

Method: Used card-sorting paradigm with EEG to analyze brain activity, comparing human performance with a multimodal large language model through stimulus- and feedback-locked analyses.

Result: Humans showed coordinated delta-theta-alpha dynamics: early delta-theta for exploration/rule inference, occipital alpha for attention stabilization after rule identification. LLM only showed short-term feedback adjustments without hierarchical rule abstraction.

Conclusion: Identified neural signatures of human adaptive reasoning and highlighted the need for brain-inspired AI incorporating oscillatory feedback coordination for genuine context-sensitive adaptation.

Abstract: Adaptive reasoning enables humans to flexibly adjust inference strategies when environmental rules or contexts change, yet its underlying neural dynamics remain unclear. This study investigated the neurophysiological mechanisms of adaptive reasoning using a card-sorting paradigm combined with electroencephalography and compared human performance with that of a multimodal large language model. Stimulus- and feedback-locked analyses revealed coordinated delta-theta-alpha dynamics: early delta-theta activity reflected exploratory monitoring and rule inference, whereas occipital alpha engagement indicated confirmatory stabilization of attention after successful rule identification. In contrast, the multimodal large language model exhibited only short-term feedback-driven adjustments without hierarchical rule abstraction or genuine adaptive reasoning. These findings identify the neural signatures of human adaptive reasoning and highlight the need for brain-inspired artificial intelligence that incorporates oscillatory feedback coordination for true context-sensitive adaptation.

</details>


### [23] [Lightweight Diffusion-based Framework for Online Imagined Speech Decoding in Aphasia](https://arxiv.org/abs/2511.07920)
*Eunyeong Ko,Soowon Kim,Ha-Na Jo*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A diffusion-based neural decoding framework optimized for real-time imagined speech classification in individuals with aphasia. The system integrates a lightweight conditional diffusion encoder and convolutional classifier trained using subject-specific EEG data acquired from a Korean-language paradigm. A dual-criterion early stopping strategy enabled rapid convergence under limited calibration data, while dropout regularization and grouped temporal convolutions ensured stable generalization. During online operation, continuous EEG streams were processed in two-second sliding windows to generate class probabilities that dynamically modulated visual and auditory feedback according to decoding confidence. Across twenty real-time trials, the framework achieved 65% top-1 and 70% top-2 accuracy, outperforming offline evaluation (50% top-1). These results demonstrate the feasibility of deploying diffusion-based EEG decoding under practical clinical constraints, maintaining reliable performance despite environmental variability and minimal preprocessing. The proposed framework advances the translation of imagined speech brain-computer interfaces toward clinical communication support for individuals with severe expressive language impairment.

</details>


### [24] [Computational Blueprints: Generating Isomorphic Mathematics Problems with Large Language Models](https://arxiv.org/abs/2511.07932)
*Jeong-Hoon Kim,Jinwoo Nam,Geunsik Jo*

Main category: cs.AI

TL;DR: The paper introduces Isomorphic Math Problem Generation (IMPG) to create structurally similar math problems for personalized education, proposing the CBIT framework that uses LLMs and templates for efficient, accurate generation.


<details>
  <summary>Details</summary>
Motivation: Personalized math education requires many similar practice problems, but current methods focus on data augmentation for AI training rather than direct educational use, creating a gap.

Method: Developed LLM-based frameworks via successive refinements, establishing Computational Blueprints for Isomorphic Twins (CBIT) with meta-level generation and template-based selective variation.

Result: CBIT achieved high mathematical accuracy and structural consistency at lower cost, with a 17.8% lower error rate than expert-authored problems and successful deployment involving 6,732 learners and 186,870 interactions.

Conclusion: CBIT effectively bridges the gap in educational problem generation, offering scalable, cost-efficient, and high-quality isomorphic math problems for personalized learning.

Abstract: Personalized mathematics education is growing rapidly, creating a strong demand for large sets of similar practice problems. Yet existing studies on mathematics problem generation have focused on data augmentation for training neural language models rather than on direct educational deployment. To bridge this gap, we define a new task, Isomorphic Math Problem Generation (IMPG), designed to produce structurally consistent variants of source problems. Subsequently, we explored LLM-based frameworks for automatic IMPG through successive refinements, and established Computational Blueprints for Isomorphic Twins (CBIT). With meta-level generation and template-based selective variation, CBIT achieves high mathematical correctness and structural consistency while reducing the cost of generation. Empirical results across refinements demonstrate that CBIT is superior on generation accuracy and cost-effectiveness at scale. Most importantly, CBIT-generated problems exhibited an error rate 17.8% lower than expert-authored items, with deployment to 6,732 learners on a commercial education platform yielding 186,870 interactions.

</details>


### [25] [Toward Practical BCI: A Real-time Wireless Imagined Speech EEG Decoding System](https://arxiv.org/abs/2511.07936)
*Ji-Ha Park,Heon-Gyu Kwak,Gi-Hwan Shin,Yoo-In Jeon,Sun-Min Park,Ji-Yeon Hwang,Seong-Whan Lee*

Main category: cs.AI

TL;DR: Development of a real-time wireless imagined speech EEG decoding system for practical BCI, achieving 62% wired and 46.67% wireless accuracy.


<details>
  <summary>Details</summary>
Motivation: Current BCI research is limited to static environments, hindering real-world application; need for flexible, everyday-use BCIs.

Method: Real-time wireless EEG decoding system using lab streaming layer for continuous signal management, with user identification for personalized service.

Result: System classifies 4 imagined speech commands with 62.00% accuracy on wired EEG and 46.67% on wireless headset.

Conclusion: Significant progress towards practical, accessible BCI technology, guiding future research on robust and personalized neural interfaces.

Abstract: Brain-computer interface (BCI) research, while promising, has largely been confined to static and fixed environments, limiting real-world applicability. To move towards practical BCI, we introduce a real-time wireless imagined speech electroencephalogram (EEG) decoding system designed for flexibility and everyday use. Our framework focuses on practicality, demonstrating extensibility beyond wired EEG devices to portable, wireless hardware. A user identification module recognizes the operator and provides a personalized, user-specific service. To achieve seamless, real-time operation, we utilize the lab streaming layer to manage the continuous streaming of live EEG signals to the personalized decoder. This end-to-end pipeline enables a functional real-time application capable of classifying user commands from imagined speech EEG signals, achieving an overall 4-class accuracy of 62.00 % on a wired device and 46.67 % on a portable wireless headset. This paper demonstrates a significant step towards truly practical and accessible BCI technology, establishing a clear direction for future research in robust, practical, and personalized neural interfaces.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution](https://arxiv.org/abs/2511.07459)
*Ashutosh Agarwal*

Main category: cs.LG

TL;DR: LEVER is a Siamese-style architecture that improves performance on infrequent categories in Extreme Classification by reducing label inconsistency through knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Infrequent categories in Extreme Classification suffer from sparse samples and high label inconsistency, which undermines classification performance.

Method: Adopts a robust Siamese-style architecture that leverages knowledge transfer to reduce label inconsistency and enhance One-vs-All classifiers.

Result: Comprehensive testing across multiple XC datasets shows substantial improvements in handling infrequent categories, setting new benchmarks. Also introduces two new multi-intent datasets for future research.

Conclusion: LEVER effectively addresses the challenge of underperforming infrequent categories in XC tasks and provides valuable resources for advancing the field.

Abstract: This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label inconsistency, which undermines classification performance. LEVER mitigates this problem by adopting a robust Siamese-style architecture, leveraging knowledge transfer to reduce label inconsistency and enhance the performance of One-vs-All classifiers. Comprehensive testing across multiple XC datasets reveals substantial improvements in the handling of infrequent categories, setting a new benchmark for the field. Additionally, the paper introduces two newly created multi-intent datasets, offering essential resources for future XC research.

</details>


### [27] [Slimmable NAM: Neural Amp Models with adjustable runtime computational cost](https://arxiv.org/abs/2511.07470)
*Steven Atkinson*

Main category: cs.LG

TL;DR: Slimmable Neural Amp Models allow dynamic size/compute adjustment without retraining, enabling accuracy/compute trade-offs for musicians in real-time audio effects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable musicians to easily trade off between model accuracy and computational requirements without needing additional training, making neural amp models more practical for real-time use in audio effects.

Method: The approach involves developing slimmable Neural Amp Models that can dynamically adjust their size and computational cost. Performance is quantified against standard baselines, and a real-time audio effect plug-in is created to demonstrate the model.

Result: The method successfully creates models with variable computational costs and negligible overhead, as demonstrated through quantitative comparison with baselines and a functioning real-time audio plug-in.

Conclusion: Slimmable Neural Amp Models offer a practical and efficient solution for real-time audio applications, providing users with flexible control over model performance and computational cost without additional training overhead.

Abstract: This work demonstrates "slimmable Neural Amp Models", whose size and computational cost can be changed without additional training and with negligible computational overhead, enabling musicians to easily trade off between the accuracy and compute of the models they are using. The method's performance is quantified against commonly-used baselines, and a real-time demonstration of the model in an audio effect plug-in is developed.

</details>


### [28] [Partial Action Replacement: Tackling Distribution Shift in Offline MARL](https://arxiv.org/abs/2511.07629)
*Yue Jin,Giovanni Montana*

Main category: cs.LG

TL;DR: SPaCQL addresses OOD issues in offline MARL via partial action replacement and uncertainty-aware weighting, achieving better performance with factorized behavior policies.


<details>
  <summary>Details</summary>
Motivation: Offline multi-agent reinforcement learning faces significant challenges with out-of-distribution joint action evaluation, particularly when behavior policies are factorized during data collection.

Method: Soft-Partial Conservative Q-Learning (SPaCQL) uses partial action replacement (PAR) to mitigate distribution shift and employs dynamic weighting of different PAR strategies based on value estimation uncertainty.

Result: SPaCQL enables more effective policy learning and demonstrates superior performance over baseline algorithms, especially when the offline dataset exhibits independence structure among agents.

Conclusion: SPaCQL successfully addresses the OOD problem in offline MARL by leveraging partial action replacement and dynamic uncertainty weighting, providing a theoretically sound and empirically effective solution particularly suited for datasets with factorized behavior policies.

Abstract: Offline multi-agent reinforcement learning (MARL) is severely hampered by the challenge of evaluating out-of-distribution (OOD) joint actions. Our core finding is that when the behavior policy is factorized - a common scenario where agents act fully or partially independently during data collection - a strategy of partial action replacement (PAR) can significantly mitigate this challenge. PAR updates a single or part of agents' actions while the others remain fixed to the behavioral data, reducing distribution shift compared to full joint-action updates. Based on this insight, we develop Soft-Partial Conservative Q-Learning (SPaCQL), using PAR to mitigate OOD issue and dynamically weighting different PAR strategies based on the uncertainty of value estimation. We provide a rigorous theoretical foundation for this approach, proving that under factorized behavior policies, the induced distribution shift scales linearly with the number of deviating agents rather than exponentially with the joint-action space. This yields a provably tighter value error bound for this important class of offline MARL problems. Our theoretical results also indicate that SPaCQL adaptively addresses distribution shift using uncertainty-informed weights. Our empirical results demonstrate SPaCQL enables more effective policy learning, and manifest its remarkable superiority over baseline algorithms when the offline dataset exhibits the independence structure.

</details>


### [29] [Towards Personalized Quantum Federated Learning for Anomaly Detection](https://arxiv.org/abs/2511.07471)
*Ratun Rahman,Sina Shaham,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: PQFL is a personalized quantum federated learning framework for anomaly detection that adapts to client heterogeneity in quantum hardware, circuits, and data, significantly improving accuracy over standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard quantum federated learning suffers from client heterogeneity in hardware capabilities, circuit designs, noise levels, and data encoding, making global models ineffective for anomaly detection with imbalanced or non-IID data.

Method: PQFL uses parameterized quantum circuits and classical optimizers for local training, with a quantum-centric personalization strategy that adapts each client's model to its specific hardware and data characteristics.

Result: PQFL reduces false errors by up to 23%, and achieves gains of 24.2% in AUROC and 20.5% in AUPR compared to state-of-the-art methods under diverse conditions.

Conclusion: PQFL is an effective and scalable solution for anomaly detection in practical quantum federated settings, addressing client heterogeneity challenges.

Abstract: Anomaly detection has a significant impact on applications such as video surveillance, medical diagnostics, and industrial monitoring, where anomalies frequently depend on context and anomaly-labeled data are limited. Quantum federated learning (QFL) overcomes these concerns by distributing model training among several quantum clients, consequently eliminating the requirement for centralized quantum storage and processing. However, in real-life quantum networks, clients frequently differ in terms of hardware capabilities, circuit designs, noise levels, and how classical data is encoded or preprocessed into quantum states. These differences create inherent heterogeneity across clients - not just in their data distributions, but also in their quantum processing behaviors. As a result, training a single global model becomes ineffective, especially when clients handle imbalanced or non-identically distributed (non-IID) data. To address this, we propose a new framework called personalized quantum federated learning (PQFL) for anomaly detection. PQFL enhances local model training at quantum clients using parameterized quantum circuits and classical optimizers, while introducing a quantum-centric personalization strategy that adapts each client's model to its own hardware characteristics and data representation. Extensive experiments show that PQFL significantly improves anomaly detection accuracy under diverse and realistic conditions. Compared to state-of-the-art methods, PQFL reduces false errors by up to 23%, and achieves gains of 24.2% in AUROC and 20.5% in AUPR, highlighting its effectiveness and scalability in practical quantum federated settings.

</details>


### [30] [BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services](https://arxiv.org/abs/2511.08142)
*Anna Lackinger,Andrea Morichetta,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.LG

TL;DR: BIPPO is an energy-efficient multi-agent RL method that optimizes client selection for Federated Learning in IoT by addressing infrastructure challenges, achieving higher accuracy with negligible budget consumption.


<details>
  <summary>Details</summary>
Motivation: Federated Learning doesn't natively consider infrastructure efficiency, which is critical for resource-constrained IoT environments. Existing RL-based solutions don't effectively address infrastructure challenges like resource limitations and device churn, and lack generalizability and energy efficiency optimization.

Method: BIPPO (Budget-aware Independent Proximal Policy Optimization), an energy-efficient multi-agent RL solution with an improved sampler for client selection in federated learning systems.

Result: BIPPO increases mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO in image classification tasks. It only consumes negligible budget that stays consistent even with increasing client numbers.

Conclusion: BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL that consumes minimal budget and maintains consistency even with increasing numbers of clients.

Abstract: Federated Learning (FL) is a promising machine learning solution in large-scale IoT systems, guaranteeing load distribution and privacy. However, FL does not natively consider infrastructure efficiency, a critical concern for systems operating in resource-constrained environments. Several Reinforcement Learning (RL) based solutions offer improved client selection for FL; however, they do not consider infrastructure challenges, such as resource limitations and device churn. Furthermore, the training of RL methods is often not designed for practical application, as these approaches frequently do not consider generalizability and are not optimized for energy efficiency. To fill this gap, we propose BIPPO (Budget-aware Independent Proximal Policy Optimization), which is an energy-efficient multi-agent RL solution that improves performance. We evaluate BIPPO on two image classification tasks run in a highly budget-constrained setting, with FL clients training on non-IID data, a challenging context for vanilla FL. The improved sampler of BIPPO enables it to increase the mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO. In addition, BIPPO only consumes a negligible proportion of the budget, which stays consistent even if the number of clients increases. Overall, BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL.

</details>


### [31] [Multivariate Variational Autoencoder](https://arxiv.org/abs/2511.07472)
*Mehmet Can Yavuz*

Main category: cs.LG

TL;DR: MVAE is a VAE variant with full-covariance posteriors using global coupling and per-sample scales, improving reconstruction, calibration, and latent structure over diagonal VAEs.


<details>
  <summary>Details</summary>
Motivation: To overcome the diagonal posterior restriction in standard VAEs while maintaining Gaussian tractability and efficient training.

Method: Factorizes posterior covariance into global coupling matrix for dataset-wide correlations and per-sample diagonal scales for local uncertainty, enabling analytic KL and efficient reparameterization.

Result: Consistent improvements in reconstruction (MSE), calibration (NLL/Brier/ECE), and unsupervised structure (NMI/ARI) across multiple datasets, with smoother latent traversals and sharper details.

Conclusion: MVAE effectively balances expressiveness and tractability, providing better uncertainty modeling and latent structure than diagonal-covariance VAEs without sacrificing efficiency.

Abstract: We present the Multivariate Variational Autoencoder (MVAE), a VAE variant that preserves Gaussian tractability while lifting the diagonal posterior restriction. MVAE factorizes each posterior covariance, where a \emph{global} coupling matrix $\mathbf{C}$ induces dataset-wide latent correlations and \emph{per-sample} diagonal scales modulate local uncertainty. This yields a full-covariance family with analytic KL and an efficient reparameterization via $\mathbf{L}=\mathbf{C}\mathrm{diag}(\boldsymbolσ)$. Across Larochelle-style MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, MVAE consistently matches or improves reconstruction (MSE~$\downarrow$) and delivers robust gains in calibration (NLL/Brier/ECE~$\downarrow$) and unsupervised structure (NMI/ARI~$\uparrow$) relative to diagonal-covariance VAEs with matched capacity, especially at mid-range latent sizes. Latent-plane visualizations further indicate smoother, more coherent factor traversals and sharper local detail. We release a fully reproducible implementation with training/evaluation scripts and sweep utilities to facilitate fair comparison and reuse.

</details>


### [32] [RELEAP: Reinforcement-Enhanced Label-Efficient Active Phenotyping for Electronic Health Records](https://arxiv.org/abs/2511.07473)
*Yang Yang,Kathryn Pollak,Bibhas Chakraborty,Molei Liu,Doudou Zhou,Chuan Hong*

Main category: cs.LG

TL;DR: RELEAP is a reinforcement learning-based active learning framework that uses downstream prediction performance as feedback to optimize phenotype correction and sample selection for EHR-based risk prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional EHR phenotyping relies on noisy proxy labels, and existing active learning methods use fixed heuristics without ensuring that phenotype refinement improves prediction performance.

Method: Proposed RELEAP framework adaptively integrates multiple querying strategies and updates its policy based on feedback from downstream models. Evaluated on DUHS cohort using logistic regression and penalized Cox survival models.

Result: RELEAP outperformed all baselines: Logistic AUC increased from 0.774 to 0.805 and survival C-index from 0.718 to 0.752, with smoother and more stable performance gains.

Conclusion: RELEAP offers a scalable, label-efficient paradigm that links phenotype refinement to prediction outcomes, reducing manual chart review and enhancing EHR-based risk prediction reliability.

Abstract: Objective: Electronic health record (EHR) phenotyping often relies on noisy proxy labels, which undermine the reliability of downstream risk prediction. Active learning can reduce annotation costs, but most rely on fixed heuristics and do not ensure that phenotype refinement improves prediction performance. Our goal was to develop a framework that directly uses downstream prediction performance as feedback to guide phenotype correction and sample selection under constrained labeling budgets.
  Materials and Methods: We propose Reinforcement-Enhanced Label-Efficient Active Phenotyping (RELEAP), a reinforcement learning-based active learning framework. RELEAP adaptively integrates multiple querying strategies and, unlike prior methods, updates its policy based on feedback from downstream models. We evaluated RELEAP on a de-identified Duke University Health System (DUHS) cohort (2014-2024) for incident lung cancer risk prediction, using logistic regression and penalized Cox survival models. Performance was benchmarked against noisy-label baselines and single-strategy active learning.
  Results: RELEAP consistently outperformed all baselines. Logistic AUC increased from 0.774 to 0.805 and survival C-index from 0.718 to 0.752. Using downstream performance as feedback, RELEAP produced smoother and more stable gains than heuristic methods under the same labeling budget.
  Discussion: By linking phenotype refinement to prediction outcomes, RELEAP learns which samples most improve downstream discrimination and calibration, offering a more principled alternative to fixed active learning rules.
  Conclusion: RELEAP optimizes phenotype correction through downstream feedback, offering a scalable, label-efficient paradigm that reduces manual chart review and enhances the reliability of EHR-based risk prediction.

</details>


### [33] [Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data](https://arxiv.org/abs/2511.07481)
*Reem Al-Saidi,Erman Ayday,Ziad Kobti*

Main category: cs.LG

TL;DR: This paper investigates embedding reconstruction attacks on genomic sequences in LLMs, specifically examining how fine-tuning affects vulnerability. It extends Pan et al.'s work by comparing pretrained vs fine-tuned embeddings, using DNA-specific tokenization, and finding fine-tuning enhances privacy resistance.


<details>
  <summary>Details</summary>
Motivation: To understand whether task-specific fine-tuning strengthens or weakens privacy protections against embedding reconstruction attacks in genomic data applications, addressing gaps in prior work that didn't specify embedding types.

Method: Comprehensive analysis using HS3D genomic dataset with specialized DNA tokenization. Compared pretrained vs fine-tuned embeddings across multiple architectures (XLNet, GPT-2, BERT) using reconstruction attack pipeline with position-specific, nucleotide-type, and privacy change analysis.

Result: Fine-tuning significantly increased resistance to reconstruction attacks: XLNet (+19.8%), GPT-2 (+9.8%), and BERT (+7.8%). Task-specific optimization consistently enhanced privacy protection across different model architectures.

Conclusion: Fine-tuning serves as a potential privacy-enhancing mechanism for LLMs processing sensitive genomic data, highlighting the need for advanced protective mechanisms while suggesting fine-tuning as a valuable technique for further exploration in privacy preservation.

Abstract: This study investigates embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, with a specific focus on how fine-tuning affects vulnerability to these attacks. Building upon Pan et al.'s seminal work demonstrating that embeddings from pretrained language models can leak sensitive information, we conduct a comprehensive analysis using the HS3D genomic dataset to determine whether task-specific optimization strengthens or weakens privacy protections. Our research extends Pan et al.'s work in three significant dimensions. First, we apply their reconstruction attack pipeline to pretrained and fine-tuned model embeddings, addressing a critical gap in their methodology that did not specify embedding types. Second, we implement specialized tokenization mechanisms tailored specifically for DNA sequences, enhancing the model's ability to process genomic data, as these models are pretrained on natural language and not DNA. Third, we perform a detailed comparative analysis examining position-specific, nucleotide-type, and privacy changes between pretrained and fine-tuned embeddings. We assess embeddings vulnerabilities across different types and dimensions, providing deeper insights into how task adaptation shifts privacy risks throughout genomic sequences. Our findings show a clear distinction in reconstruction vulnerability between pretrained and fine-tuned embeddings. Notably, fine-tuning strengthens resistance to reconstruction attacks in multiple architectures -- XLNet (+19.8\%), GPT-2 (+9.8\%), and BERT (+7.8\%) -- pointing to task-specific optimization as a potential privacy enhancement mechanism. These results highlight the need for advanced protective mechanisms for language models processing sensitive genomic data, while highlighting fine-tuning as a potential privacy-enhancing technique worth further exploration.

</details>


### [34] [Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits](https://arxiv.org/abs/2511.07482)
*Dev Patel,Gabrielle Gervacio,Diekola Raimi,Kevin Zhu,Ryan Lagasse,Gabriel Grand,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: AAPP is a dynamic pruning method that preserves safety circuits in LLMs, improving refusal rates by 50% while maintaining computational efficiency during inference.


<details>
  <summary>Details</summary>
Motivation: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods, it exacerbates alignment degradation by failing to adequately preserve safety-critical circuits across diverse inputs.

Method: Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning

Result: Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50% at matched compute, enabling efficient yet safety-preserving LLM deployment.

Conclusion: AAPP effectively improves the trade-off between computational efficiency and safety preservation in LLM deployment, achieving significant improvements in refusal rates while maintaining acceptable performance.

Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.

</details>


### [35] [Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs](https://arxiv.org/abs/2511.07484)
*Dharmateja Priyadarshi Uddandarao,Ravi Kiran Vadlamani*

Main category: cs.LG

TL;DR: A novel framework combining causal models with transformer AI for counterfactual user behavior forecasting, outperforming traditional methods and enabling pre-deployment intervention simulation.


<details>
  <summary>Details</summary>
Motivation: To address the need for forecasting user behavior in hypothetical scenarios and enable effective simulation and assessment of potential interventions before actual deployment.

Method: This study combines structural causal models with transformer-based generative AI to model fictitious scenarios by creating causal graphs that map relationships between user interactions, adoption metrics, and product features. It uses generative models conditioned on causal variables to produce realistic behavioral trajectories under counterfactual conditions.

Result: The methodology was tested on datasets from web interactions, mobile applications, and e-commerce, and demonstrated superior performance compared to conventional forecasting and uplift modeling techniques.

Conclusion: Product teams can use this framework to simulate and evaluate potential interventions before deployment, with enhanced interpretability through causal path visualization.

Abstract: This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.

</details>


### [36] [When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift](https://arxiv.org/abs/2511.07485)
*Sushant Mehta*

Main category: cs.LG

TL;DR: Proposes unified framework showing different ML bias mechanisms (spurious correlations, subpopulation shift, class imbalance, fairness violations) produce equivalent performance effects under certain conditions.


<details>
  <summary>Details</summary>
Motivation: Machine learning failures (unfairness, brittleness, poor minority performance) are studied in isolation; need unifying theory to connect these bias mechanisms.

Method: Formalizes biases as conditional independence violations using information theory, proves equivalence conditions linking different bias types mathematically.

Result: Theoretical equivalence holds (e.g., spurious correlation strength α ≈ imbalance ratio r=(1+α)/(1-α)); empirical validation on 6 datasets/3 architectures shows predictions accurate within 3% worst-group error.

Conclusion: Framework enables transfer of debiasing methods across domains, bridging fairness, robustness, and distribution shift literature under common perspective.

Abstract: Machine learning systems exhibit diverse failure modes: unfairness toward protected groups, brittleness to spurious correlations, poor performance on minority sub-populations, which are typically studied in isolation by distinct research communities. We propose a unifying theoretical framework that characterizes when different bias mechanisms produce quantitatively equivalent effects on model performance. By formalizing biases as violations of conditional independence through information-theoretic measures, we prove formal equivalence conditions relating spurious correlations, subpopulation shift, class imbalance, and fairness violations. Our theory predicts that a spurious correlation of strength $α$ produces equivalent worst-group accuracy degradation as a sub-population imbalance ratio $r \approx (1+α)/(1-α)$ under feature overlap assumptions. Empirical validation in six datasets and three architectures confirms that predicted equivalences hold within the accuracy of the worst group 3\%, enabling the principled transfer of debiasing methods across problem domains. This work bridges the literature on fairness, robustness, and distribution shifts under a common perspective.

</details>


### [37] [Provably Efficient Sample Complexity for Robust CMDP](https://arxiv.org/abs/2511.07486)
*Sourav Ganguly,Arnob Ghosh*

Main category: cs.LG

TL;DR: First sample complexity analysis for robust constrained MDPs, showing Markovian policies fail under uncertainty, proposing RCVI algorithm with O~(|S||A|H^5/ε^2) sample complexity and empirical validation.


<details>
  <summary>Details</summary>
Motivation: Recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, but their sample complexity guarantees remain largely unexplored, creating a significant gap in understanding the practical learning requirements for safe RL under uncertainty.

Method: The authors propose a novel Robust constrained Value iteration (RCVI) algorithm based on an augmented state space that incorporates the remaining utility budget into the state representation, enabling better constraint handling under worst-case dynamics.

Result: The proposed RCVI algorithm achieves a sample complexity of O~(|S||A|H^5/ε^2) with at most ε constraint violation using a generative model, providing the first sample complexity guarantee for RCMDPs. Empirical results validate the approach's effectiveness.

Conclusion: This paper provides the first sample complexity guarantee for robust constrained Markov decision processes (RCMDPs), introducing a theoretical framework and practical algorithm that achieves ε-optimal solutions with minimal constraint violations, advancing the state of safe reinforcement learning in uncertain environments.

Abstract: We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\mathcal{\tilde{O}}(|S||A|H^5/ε^2)$ achieving at most $ε$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.

</details>


### [38] [Methodological Precedence in Health Tech: Why ML/Big Data Analysis Must Follow Basic Epidemiological Consistency. A Case Study](https://arxiv.org/abs/2511.07500)
*Marco Roccetti*

Main category: cs.LG

TL;DR: Paper warns that advanced ML/big data analyses amplify basic methodological flaws rather than correct them, using a vaccine study case to show how selection bias invalidates reported hazard ratios.


<details>
  <summary>Details</summary>
Motivation: To highlight that sophisticated analytical tools like ML require rigorous basic methodological coherence first, as they amplify rather than correct fundamental design flaws.

Method: Applying simple descriptive statistics and national epidemiological benchmarks to reanalyze a published cohort study on vaccine outcomes and psychiatric events to expose paradoxes.

Result: Found multiple statistically irreconcilable paradoxes (e.g., implausible risk reduction, contradictory incidence rates) that invalidate reported hazard ratios, showing effects are mathematical artifacts from selection bias.

Conclusion: Complex health studies must first pass basic epidemiological consistency tests before advanced analyses; robust methods like Propensity Score Matching are essential for valid causal inference from administrative data.

Abstract: The integration of advanced analytical tools, including Machine Learning (ML) and massive data processing, has revolutionized health research, promising unprecedented accuracy in diagnosis and risk prediction. However, the rigor of these complex methods is fundamentally dependent on the quality and integrity of the underlying datasets and the validity of their statistical design. We propose an emblematic case where advanced analysis (ML/Big Data) must necessarily be subsequent to the verification of basic methodological coherence. This study highlights a crucial cautionary principle: sophisticated analyses amplify, rather than correct, severe methodological flaws rooted in basic design choices, leading to misleading or contradictory findings. By applying simple, standard descriptive statistical methods and established national epidemiological benchmarks to a recently published cohort study on vaccine outcomes and psychiatric events, we expose multiple, statistically irreconcilable paradoxes. These paradoxes, including an implausible risk reduction for a chronic disorder in a high-risk group and contradictory incidence rate comparisons, definitively invalidate the reported hazard ratios (HRs). We demonstrate that the observed effects are mathematical artifacts stemming from an uncorrected selection bias in the cohort construction. This analysis serves as a robust reminder that even the most complex health studies must first pass the test of basic epidemiological consistency before any conclusion drawn from subsequent advanced ML or statistical modeling can be considered valid or publishable. We conclude that robust methods, such as Propensity Score Matching, are essential for achieving valid causal inference from administrative data in the absence of randomization

</details>


### [39] [N-ReLU: Zero-Mean Stochastic Extension of ReLU](https://arxiv.org/abs/2511.07559)
*Md Motaleb Hossen Manik,Md Zabirul Islam,Ge Wang*

Main category: cs.LG

TL;DR: N-ReLU replaces ReLU's negative activations with Gaussian noise while maintaining the same expected output, preventing dead neurons and improving optimization robustness without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Standard ReLU suffers from inactive 'dead' neurons due to its hard zero cutoff, which limits gradient flow and network performance.

Method: Proposed N-ReLU (Noise-ReLU) that replaces negative activations with zero-mean Gaussian noise while preserving the same expected output as ReLU, acting as an annealing-style regularizer.

Result: On MNIST dataset with MLP and CNN architectures, N-ReLU achieves comparable or slightly better accuracy than ReLU, LeakyReLU, PReLU, GELU, and RReLU at moderate noise levels (sigma=0.05-0.10), with stable convergence and no dead neurons observed.

Conclusion: Lightweight Gaussian noise injection provides a simple yet effective mechanism to enhance optimization robustness in neural networks without modifying structures or adding parameters.

Abstract: Activation functions are fundamental for enabling nonlinear representations in deep neural networks. However, the standard rectified linear unit (ReLU) often suffers from inactive or "dead" neurons caused by its hard zero cutoff. To address this issue, we introduce N-ReLU (Noise-ReLU), a zero-mean stochastic extension of ReLU that replaces negative activations with Gaussian noise while preserving the same expected output. This expectation-aligned formulation maintains gradient flow in inactive regions and acts as an annealing-style regularizer during training. Experiments on the MNIST dataset using both multilayer perceptron (MLP) and convolutional neural network (CNN) architectures show that N-ReLU achieves accuracy comparable to or slightly exceeding that of ReLU, LeakyReLU, PReLU, GELU, and RReLU at moderate noise levels (sigma = 0.05-0.10), with stable convergence and no dead neurons observed. These results demonstrate that lightweight Gaussian noise injection offers a simple yet effective mechanism to enhance optimization robustness without modifying network structures or introducing additional parameters.

</details>


### [40] [SCALAR: Benchmarking SAE Interaction Sparsity in Toy LLMs](https://arxiv.org/abs/2511.07572)
*Sean P. Fillingham,Andrew Gordon,Peter Lai,Xavier Poncini,David Quarel,Stefan Heimersheim*

Main category: cs.LG

TL;DR: Researchers introduce SCALAR benchmark to measure interaction sparsity between SAE features and propose Staircase SAEs that improve sparsity by ~60% over standard methods while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Current SAE training focuses on individual layer sparsity but ignores cross-layer sparsity, leading to inflated circuits where upstream features unnecessarily connect to multiple downstream features.

Method: Developed SCALAR benchmark for measuring interaction sparsity and proposed Staircase SAEs using weight-sharing to limit feature duplication across layers. Compared TopK SAEs, Jacobian SAEs, and Staircase SAEs on toy model and GPT-2 Small.

Result: Staircase SAEs improved relative sparsity by 59.67% (feedforward) and 63.15% (transformer blocks) over TopK SAEs. JSAEs showed modest improvements for feedforward layers but failed on transformer blocks. Both methods maintained interpretability.

Conclusion: Interaction sparsity is crucial for SAE performance. Staircase SAEs effectively improve cross-layer sparsity while working across the entire residual stream, highlighting the importance of benchmarking interaction patterns.

Abstract: Mechanistic interpretability aims to decompose neural networks into interpretable features and map their connecting circuits. The standard approach trains sparse autoencoders (SAEs) on each layer's activations. However, SAEs trained in isolation don't encourage sparse cross-layer connections, inflating extracted circuits where upstream features needlessly affect multiple downstream features. Current evaluations focus on individual SAE performance, leaving interaction sparsity unexamined. We introduce SCALAR (Sparse Connectivity Assessment of Latent Activation Relationships), a benchmark measuring interaction sparsity between SAE features. We also propose "Staircase SAEs", using weight-sharing to limit upstream feature duplication across downstream features. Using SCALAR, we compare TopK SAEs, Jacobian SAEs (JSAEs), and Staircase SAEs. Staircase SAEs improve relative sparsity over TopK SAEs by $59.67\% \pm 1.83\%$ (feedforward) and $63.15\% \pm 1.35\%$ (transformer blocks). JSAEs provide $8.54\% \pm 0.38\%$ improvement over TopK for feedforward layers but cannot train effectively across transformer blocks, unlike Staircase and TopK SAEs which work anywhere in the residual stream. We validate on a $216$K-parameter toy model and GPT-$2$ Small ($124$M), where Staircase SAEs maintain interaction sparsity improvements while preserving feature interpretability. Our work highlights the importance of interaction sparsity in SAEs through benchmarking and comparing promising architectures.

</details>


### [41] [LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows](https://arxiv.org/abs/2511.07585)
*Raffi Khatchadourian,Rolando Franco*

Main category: cs.LG

TL;DR: Smaller LLMs (7B-8B) outperform larger models in output consistency for financial tasks, challenging size-superiority assumptions. The study provides a deterministic test framework and compliance pathways for financial AI deployments.


<details>
  <summary>Details</summary>
Motivation: Financial institutions are using LLMs for critical tasks, but nondeterministic outputs (output drift) undermine auditability and trust in regulated applications. There is a need to quantify and address this issue.

Method: The authors use a finance-calibrated deterministic test harness with greedy decoding, fixed seeds, and SEC 10-K structure-aware retrieval. They evaluate five LLMs across regulated financial tasks with invariant checking for RAG, JSON, and SQL outputs using materiality thresholds and SEC citation validation.

Result: Smaller models (7B-8B parameters) achieve 100% output consistency at T=0.0, while larger models like GPT-OSS-120B show only 12.5% consistency. Structured tasks (SQL) remain stable even at T=0.2, but RAG tasks show significant drift (25-75%).

Conclusion: The study concludes that smaller models can achieve perfect output consistency, challenging the assumption that larger models are always better for production deployment in financial applications.

Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.

</details>


### [42] [One Router to Route Them All: Homogeneous Expert Routing for Heterogeneous Graph Transformers](https://arxiv.org/abs/2511.07603)
*Georgiy Shakirov,Albert Arakelov*

Main category: cs.LG

TL;DR: Proposes Homogeneous Expert Routing (HER) for Heterogeneous Graph Transformers that replaces type-specific experts with type-agnostic routing to improve cross-type knowledge transfer and specialization.


<details>
  <summary>Details</summary>
Motivation: Current heterogeneous graph neural networks over-rely on node/edge type labels, limiting knowledge transfer across types and causing surface-level semantic dependence.

Method: Integrates Mixture-of-Experts (MoE) into Heterogeneous Graph Transformers (HGT) but replaces type-specific experts with stochastic masking of type embeddings during routing to encourage type-agnostic expert specialization.

Result: HER consistently outperforms standard HGT and type-separated MoE baselines on IMDB, ACM, and DBLP link prediction tasks, with experts specializing by semantic patterns (e.g., movie genres) rather than node types.

Conclusion: Regularizing type dependence in expert routing yields more generalizable, efficient, and interpretable representations, establishing a new design principle for heterogeneous graph learning.

Abstract: A common practice in heterogeneous graph neural networks (HGNNs) is to condition parameters on node/edge types, assuming types reflect semantic roles. However, this can cause overreliance on surface-level labels and impede cross-type knowledge transfer. We explore integrating Mixture-of-Experts (MoE) into HGNNs--a direction underexplored despite MoE's success in homogeneous settings. Crucially, we question the need for type-specific experts. We propose Homogeneous Expert Routing (HER), an MoE layer for Heterogeneous Graph Transformers (HGT) that stochastically masks type embeddings during routing to encourage type-agnostic specialization. Evaluated on IMDB, ACM, and DBLP for link prediction, HER consistently outperforms standard HGT and a type-separated MoE baseline. Analysis on IMDB shows HER experts specialize by semantic patterns (e.g., movie genres) rather than node types, confirming routing is driven by latent semantics. Our work demonstrates that regularizing type dependence in expert routing yields more generalizable, efficient, and interpretable representations--a new design principle for heterogeneous graph learning.

</details>


### [43] [FlowTIE: Flow-based Transport of Intensity Equation for Phase Gradient Estimation from 4D-STEM Data](https://arxiv.org/abs/2511.07633)
*Arya Bangun,Maximilian Töllner,Xuan Zhao,Christian Kübel,Hanno Scharr*

Main category: cs.LG

TL;DR: FlowTIE is a neural network framework combining TIE physics with flow-based phase gradient representation for improved 4D-STEM phase reconstruction, especially for thick specimens.


<details>
  <summary>Details</summary>
Motivation: Traditional phase reconstruction methods struggle with thick specimens under dynamical scattering conditions, requiring better integration of physics-based priors with data-driven approaches.

Method: Integrates Transport of Intensity Equation (TIE) with flow-based phase gradient representation using neural networks, validated on simulated crystalline datasets against classical TIE and gradient-based methods.

Result: FlowTIE demonstrates improved phase reconstruction accuracy, faster processing, and compatibility with thick specimen models like multislice method.

Conclusion: The framework successfully bridges data-driven learning with physics-based priors, offering robust phase reconstruction for challenging thick specimen conditions in 4D-STEM.

Abstract: We introduce FlowTIE, a neural-network-based framework for phase reconstruction from 4D-Scanning Transmission Electron Microscopy (STEM) data, which integrates the Transport of Intensity Equation (TIE) with a flow-based representation of the phase gradient. This formulation allows the model to bridge data-driven learning with physics-based priors, improving robustness under dynamical scattering conditions for thick specimen. The validation on simulated datasets of crystalline materials, benchmarking to classical TIE and gradient-based optimization methods are presented. The results demonstrate that FlowTIE improves phase reconstruction accuracy, fast, and can be integrated with a thick specimen model, namely multislice method.

</details>


### [44] [Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private](https://arxiv.org/abs/2511.07637)
*Ruihan Wu,Erchi Wang,Zhiyuan Zhang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: This paper introduces two differentially private RAG algorithms (MURAG and MURAG-ADA) for multi-query settings, addressing privacy risks while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: Existing DP guarantees for RAG only work in single-query settings, which are impractical since real-world usage involves multiple queries that could leak sensitive information from external corpora.

Method: Proposed MURAG uses individual privacy filters to limit accumulated privacy loss per document retrieval frequency. MURAG-ADA enhances utility by privately releasing query-specific thresholds for more precise document selection.

Result: Experiments across multiple LLMs and datasets show both methods scale to hundreds of queries under practical DP constraints (ε≈10) while preserving meaningful utility.

Conclusion: The proposed DP-RAG algorithms effectively protect privacy in realistic multi-query scenarios without significantly compromising performance.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving documents from an external corpus at inference time. When this corpus contains sensitive information, however, unprotected RAG systems are at risk of leaking private information. Prior work has introduced differential privacy (DP) guarantees for RAG, but only in single-query settings, which fall short of realistic usage. In this paper, we study the more practical multi-query setting and propose two DP-RAG algorithms. The first, MURAG, leverages an individual privacy filter so that the accumulated privacy loss only depends on how frequently each document is retrieved rather than the total number of queries. The second, MURAG-ADA, further improves utility by privately releasing query-specific thresholds, enabling more precise selection of relevant documents. Our experiments across multiple LLMs and datasets demonstrate that the proposed methods scale to hundreds of queries within a practical DP budget ($\varepsilon\approx10$), while preserving meaningful utility.

</details>


### [45] [Adaptive Graph Learning with Transformer for Multi-Reservoir Inflow Prediction](https://arxiv.org/abs/2511.07649)
*Pengfei Hu,Ming Fan,Xiaoxue Han,Chang Lu,Wei Zhang,Hyun Kang,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: AdaTrip is an adaptive graph learning framework that improves multi-reservoir inflow forecasting by modeling dynamic spatial-temporal dependencies between interconnected reservoirs.


<details>
  <summary>Details</summary>
Motivation: Existing reservoir inflow prediction methods focus on single-reservoir models and ignore spatial dependencies between interconnected reservoirs, limiting forecasting accuracy.

Method: AdaTrip constructs dynamic graphs with reservoirs as nodes and directed edges representing hydrological connections, using attention mechanisms to automatically identify crucial spatial and temporal dependencies.

Result: Evaluation on 30 reservoirs in the Upper Colorado River Basin shows superior performance over existing baselines, with improved forecasting for reservoirs with limited records through parameter sharing. The model also provides interpretable attention maps.

Conclusion: AdaTrip offers an effective framework for multi-reservoir inflow forecasting that captures spatial-temporal dependencies and provides interpretable insights to support water resource management decisions.

Abstract: Reservoir inflow prediction is crucial for water resource management, yet existing approaches mainly focus on single-reservoir models that ignore spatial dependencies among interconnected reservoirs. We introduce AdaTrip as an adaptive, time-varying graph learning framework for multi-reservoir inflow forecasting. AdaTrip constructs dynamic graphs where reservoirs are nodes with directed edges reflecting hydrological connections, employing attention mechanisms to automatically identify crucial spatial and temporal dependencies. Evaluation on thirty reservoirs in the Upper Colorado River Basin demonstrates superiority over existing baselines, with improved performance for reservoirs with limited records through parameter sharing. Additionally, AdaTrip provides interpretable attention maps at edge and time-step levels, offering insights into hydrological controls to support operational decision-making. Our code is available at https://github.com/humphreyhuu/AdaTrip.

</details>


### [46] [Enhancing Binary Encoded Crime Linkage Analysis Using Siamese Network](https://arxiv.org/abs/2511.07651)
*Yicheng Zhan,Fahim Ahmed,Amy Burrell,Matthew J. Tonkin,Sarah Galambos,Jessica Woodhams,Dalal Alrajeh*

Main category: cs.LG

TL;DR: Siamese Autoencoder framework improves crime linkage analysis by learning meaningful latent representations from complex crime data, achieving up to 9% AUC improvement over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional crime linkage methods struggle with high-dimensional, sparse, and heterogeneous data, limiting their effectiveness in identifying serial offenders.

Method: Proposes a Siamese Autoencoder framework using ViCLAS data, integrating geographic-temporal features at decoder stage to amplify behavioral representations and mitigate signal dilution in sparse feature spaces.

Result: Approach yields consistent improvements across evaluation metrics, with up to 9% AUC improvement over traditional methods while providing interpretable insights.

Conclusion: Advanced machine learning approaches can substantially enhance crime linkage accuracy and offer practical guidance for preprocessing strategies in investigative contexts.

Abstract: Effective crime linkage analysis is crucial for identifying serial offenders and enhancing public safety. To address limitations of traditional crime linkage methods in handling high-dimensional, sparse, and heterogeneous data, we propose a Siamese Autoencoder framework that learns meaningful latent representations and uncovers correlations in complex crime data. Using data from the Violent Crime Linkage Analysis System (ViCLAS), maintained by the Serious Crime Analysis Section of the UK's National Crime Agency, our approach mitigates signal dilution in sparse feature spaces by integrating geographic-temporal features at the decoder stage. This design amplifies behavioral representations rather than allowing them to be overshadowed at the input level, yielding consistent improvements across multiple evaluation metrics. We further analyze how different domain-informed data reduction strategies influence model performance, providing practical guidance for preprocessing in crime linkage contexts. Our results show that advanced machine learning approaches can substantially enhance linkage accuracy, improving AUC by up to 9% over traditional methods while offering interpretable insights to support investigative decision-making.

</details>


### [47] [CAE: Character-Level Autoencoder for Non-Semantic Relational Data Grouping](https://arxiv.org/abs/2511.07657)
*Veera V S Bhargav Nunna,Shinae Kang,Zheyuan Zhou,Virginia Wang,Sucharitha Boinapally,Michael Foley*

Main category: cs.LG

TL;DR: A character-level autoencoder (CAE) method for grouping semantically identical columns in non-semantic relational databases by detecting data patterns, outperforming traditional NLP approaches with 81% accuracy.


<details>
  <summary>Details</summary>
Motivation: Enterprise databases contain large volumes of non-semantic data (like IP addresses, timestamps) that challenge semantic analysis, requiring scalable solutions beyond traditional NLP models.

Method: Uses a character-level autoencoder with fixed dictionary constraints to encode text representations of columns and extract feature embeddings for grouping, reducing memory and training time.

Result: Achieved 80.95% accuracy in top 5 column matching tasks, significantly outperforming Bag of Words (47.62%) and enabling efficient large-scale data processing.

Conclusion: The CAE approach effectively bridges theoretical character-level neural architectures with practical data management, providing automated schema understanding for non-semantic industrial datasets.

Abstract: Enterprise relational databases increasingly contain vast amounts of non-semantic data - IP addresses, product identifiers, encoded keys, and timestamps - that challenge traditional semantic analysis. This paper introduces a novel Character-Level Autoencoder (CAE) approach that automatically identifies and groups semantically identical columns in non-semantic relational datasets by detecting column similarities based on data patterns and structures. Unlike conventional Natural Language Processing (NLP) models that struggle with limitations in semantic interpretability and out-of-vocabulary tokens, our approach operates at the character level with fixed dictionary constraints, enabling scalable processing of large-scale data lakes and warehouses. The CAE architecture encodes text representations of non-semantic relational table columns and extracts high-dimensional feature embeddings for data grouping. By maintaining a fixed dictionary size, our method significantly reduces both memory requirements and training time, enabling efficient processing of large-scale industrial data environments. Experimental evaluation demonstrates substantial performance gains: our CAE approach achieved 80.95% accuracy in top 5 column matching tasks across relational datasets, substantially outperforming traditional NLP approaches such as Bag of Words (47.62%). These results demonstrate its effectiveness for identifying and clustering identical columns in relational datasets. This work bridges the gap between theoretical advances in character-level neural architectures and practical enterprise data management challenges, providing an automated solution for schema understanding and data profiling of non-semantic industrial datasets at scale.

</details>


### [48] [ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings](https://arxiv.org/abs/2511.07658)
*Xiaomeng Yang,Jian Gao,Yanzhi Wang,Xuan Zhang*

Main category: cs.LG

TL;DR: ZeroSim is a transformer-based framework that achieves both in-distribution and zero-shot generalization for analog circuit performance modeling without topology-specific retraining, enabling 13x faster optimization than SPICE.


<details>
  <summary>Details</summary>
Motivation: Traditional SPICE simulations are time-consuming for performance evaluation, and existing ML methods require topology-specific retraining or manual segmentation, limiting scalability in analog circuit design automation.

Method: Three key strategies: (1) Diverse training corpus of 3.6M instances across 60+ amplifier topologies; (2) Unified topology embeddings with global-aware tokens and hierarchical attention; (3) Topology-conditioned parameter mapping maintaining structural consistency.

Result: ZeroSim significantly outperforms MLP, GNN, and transformer baselines in accurate zero-shot predictions across different topologies, and enables 13x speedup in reinforcement learning-based parameter optimization compared to SPICE.

Conclusion: ZeroSim provides robust generalization for analog circuit performance modeling without fine-tuning, demonstrating practical value for scalable design automation tasks.

Abstract: Although recent advancements in learning-based analog circuit design automation have tackled tasks such as topology generation, device sizing, and layout synthesis, efficient performance evaluation remains a major bottleneck. Traditional SPICE simulations are time-consuming, while existing machine learning methods often require topology-specific retraining or manual substructure segmentation for fine-tuning, hindering scalability and adaptability. In this work, we propose ZeroSim, a transformer-based performance modeling framework designed to achieve robust in-distribution generalization across trained topologies under novel parameter configurations and zero-shot generalization to unseen topologies without any fine-tuning. We apply three key enabling strategies: (1) a diverse training corpus of 3.6 million instances covering over 60 amplifier topologies, (2) unified topology embeddings leveraging global-aware tokens and hierarchical attention to robustly generalize to novel circuits, and (3) a topology-conditioned parameter mapping approach that maintains consistent structural representations independent of parameter variations. Our experimental results demonstrate that ZeroSim significantly outperforms baseline models such as multilayer perceptrons, graph neural networks and transformers, delivering accurate zero-shot predictions across different amplifier topologies. Additionally, when integrated into a reinforcement learning-based parameter optimization pipeline, ZeroSim achieves a remarkable speedup (13x) compared to conventional SPICE simulations, underscoring its practical value for a wide range of analog circuit design automation tasks.

</details>


### [49] [Probabilities Are All You Need: A Probability-Only Approach to Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2511.07694)
*Manh Nguyen,Sunil Gupta,Hung Le*

Main category: cs.LG

TL;DR: Proposes an efficient, training-free uncertainty estimation method for LLMs using top-K probabilities with adaptive K selection, outperforming expensive baselines on QA datasets.


<details>
  <summary>Details</summary>
Motivation: LLMs are prone to hallucinations (factually incorrect outputs), and existing uncertainty estimation methods require multiple samples or extra computation, making them inefficient.

Method: Uses top-K probabilities of LLM responses to approximate predictive entropy, with an adaptive mechanism to determine K dynamically and filter low-confidence probabilities.

Result: Experimental results on three free-form QA datasets across several LLMs show the method outperforms state-of-the-art baselines without additional training or high computational cost.

Conclusion: The method effectively enhances LLM trustworthiness by providing efficient uncertainty estimation, contributing to safer and more reliable LLM applications.

Abstract: Large Language Models (LLMs) exhibit strong performance across various natural language processing (NLP) tasks but remain vulnerable to hallucinations, generating factually incorrect or misleading outputs. Uncertainty estimation, often using predictive entropy estimation, is key to addressing this issue. However, existing methods often require multiple samples or extra computation to assess semantic entropy. This paper proposes an efficient, training-free uncertainty estimation method that approximates predictive entropy using the responses' top-$K$ probabilities. Moreover, we employ an adaptive mechanism to determine $K$ to enhance flexibility and filter out low-confidence probabilities. Experimental results on three free-form question-answering datasets across several LLMs demonstrate that our method outperforms expensive state-of-the-art baselines, contributing to the broader goal of enhancing LLM trustworthiness.

</details>


### [50] [On the Role of Calibration in Benchmarking Algorithmic Fairness for Skin Cancer Detection](https://arxiv.org/abs/2511.07700)
*Brandon Dominique,Prudence Lam,Nicholas Kurtansky,Jochen Weber,Kivanc Kose,Veronica Rotemberg,Jennifer Dy*

Main category: cs.LG

TL;DR: AI models for melanoma detection show calibration biases across demographic groups despite good discrimination, highlighting the need for comprehensive auditing that includes calibration metrics alongside AUROC.


<details>
  <summary>Details</summary>
Motivation: Clinical adoption of AI models for melanoma detection is hindered by performance disparities across demographic subgroups, and previous benchmarking efforts focused primarily on AUROC-based metrics without examining model calibration, which provides insights into predicted probability accuracy.

Method: The researchers assessed the performance of leading skin cancer detection algorithms from the ISIC 2020 Challenge on both the ISIC 2020 dataset and PROVE-AI dataset, evaluating subgroups defined by sex, race (Fitzpatrick Skin Tone), and age using both AUROC-based fairness metrics and calibration metrics.

Result: The study reveals that while existing models improve discriminative accuracy, they frequently over-diagnose risk and demonstrate significant calibration issues when applied to new datasets, particularly across different demographic subgroups.

Conclusion: The paper concludes that comprehensive model auditing strategies and extensive metadata collection are essential for achieving equitable AI-driven healthcare solutions, as current models exhibit significant calibration issues across demographic subgroups.

Abstract: Artificial Intelligence (AI) models have demonstrated expert-level performance in melanoma detection, yet their clinical adoption is hindered by performance disparities across demographic subgroups such as gender, race, and age. Previous efforts to benchmark the performance of AI models have primarily focused on assessing model performance using group fairness metrics that rely on the Area Under the Receiver Operating Characteristic curve (AUROC), which does not provide insights into a model's ability to provide accurate estimates. In line with clinical assessments, this paper addresses this gap by incorporating calibration as a complementary benchmarking metric to AUROC-based fairness metrics. Calibration evaluates the alignment between predicted probabilities and observed event rates, offering deeper insights into subgroup biases. We assess the performance of the leading skin cancer detection algorithm of the ISIC 2020 Challenge on the ISIC 2020 Challenge dataset and the PROVE-AI dataset, and compare it with the second and third place models, focusing on subgroups defined by sex, race (Fitzpatrick Skin Tone), and age. Our findings reveal that while existing models enhance discriminative accuracy, they often over-diagnose risk and exhibit calibration issues when applied to new datasets. This study underscores the necessity for comprehensive model auditing strategies and extensive metadata collection to achieve equitable AI-driven healthcare solutions. All code is publicly available at https://github.com/bdominique/testing_strong_calibration.

</details>


### [51] [Diffusion Guided Adversarial State Perturbations in Reinforcement Learning](https://arxiv.org/abs/2511.07701)
*Xiaolin Sun,Feidi Liu,Zhengming Ding,ZiZhan Zheng*

Main category: cs.LG

TL;DR: SHIFT attack uses diffusion models to create realistic state perturbations that alter semantics, breaking current RL defenses.


<details>
  <summary>Details</summary>
Motivation: Current RL defenses are effective mainly due to the weakness of existing l_p norm-constrained attacks, which fail to alter the semantics of image inputs even with large perturbation budgets.

Method: SHIFT, a policy-agnostic diffusion-based state perturbation attack that generates semantically different but realistic and history-aligned perturbed states.

Result: The SHIFT attack effectively breaks existing state-of-the-art RL defenses, significantly outperforming existing attacks while being more perceptually stealthy.

Conclusion: RL systems are more vulnerable to semantics-aware adversarial perturbations than previously thought, necessitating the development of more robust policies.

Abstract: Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [52] [A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems](https://arxiv.org/abs/2511.07707)
*Manonmani Sekar,Nasim Nezamoddini*

Main category: cs.MA

TL;DR: MARL with enhanced DQN agents enables effective dynamic scheduling in reconfigurable manufacturing systems, outperforming baselines on makespan, tardiness, and utilization despite performance variability from machine failures.


<details>
  <summary>Details</summary>
Motivation: Address the need for flexible soft planning mechanisms in reconfigurable manufacturing systems (RMS) that can handle real-time production planning and scheduling amid complexity, variability, and stochastic events like machine breakdowns and reconfiguration delays.

Method: Multi-agent reinforcement learning (MARL) with deep Q-network (DQN) agents trained through centralized training, enhanced with prioritized experience replay, n-step returns, double DQN, soft target update, and an attention mechanism for improved state representation and decision focus.

Result: The proposed approach outperforms baseline heuristics in reducing makespan and tardiness while improving machine utilization, though machine breakdowns increase variability in performance metrics.

Conclusion: The study confirms that the MARL framework with enhanced DQN agents provides significant advantages for intelligent and adaptive scheduling in dynamic reconfigurable manufacturing environments, despite some performance variability introduced by machine breakdowns.

Abstract: Reconfigurable manufacturing systems (RMS) are critical for future market adjustment given their rapid adaptation to fluctuations in consumer demands, the introduction of new technological advances, and disruptions in linked supply chain sections. The adjustable hard settings of such systems require a flexible soft planning mechanism that enables realtime production planning and scheduling amid the existing complexity and variability in their configuration settings. This study explores the application of multi agent reinforcement learning (MARL) for dynamic scheduling in soft planning of the RMS settings. In the proposed framework, deep Qnetwork (DQN) agents trained in centralized training learn optimal job machine assignments in real time while adapting to stochastic events such as machine breakdowns and reconfiguration delays. The model also incorporates a negotiation with an attention mechanism to enhance state representation and improve decision focus on critical system features. Key DQN enhancements including prioritized experience replay, nstep returns, double DQN and soft target update are used to stabilize and accelerate learning. Experiments conducted in a simulated RMS environment demonstrate that the proposed approach outperforms baseline heuristics in reducing makespan and tardiness while improving machine utilization. The reconfigurable manufacturing environment was extended to simulate realistic challenges, including machine failures and reconfiguration times. Experimental results show that while the enhanced DQN agent is effective in adapting to dynamic conditions, machine breakdowns increase variability in key performance metrics such as makespan, throughput, and total tardiness. The results confirm the advantages of applying the MARL mechanism for intelligent and adaptive scheduling in dynamic reconfigurable manufacturing environments.

</details>


### [53] [A Historical Interaction-Enhanced Shapley Policy Gradient Algorithm for Multi-Agent Credit Assignment](https://arxiv.org/abs/2511.07778)
*Ao Ding,Licheng Sun,Yongjie Hou,Huaqing Zhang,Hongbin Ma*

Main category: cs.MA

TL;DR: HIS algorithm improves MARL credit assignment by combining historical interaction-based Shapley values with global rewards, achieving better performance in complex collaborative tasks


<details>
  <summary>Details</summary>
Motivation: traditional credit assignment schemes in MARL cannot reliably capture individual contributions in strongly coupled tasks while maintaining training stability, leading to limited generalization capabilities and hindered algorithm performance

Method: propose a Historical Interaction-Enhanced Shapley Policy Gradient Algorithm (HIS) that employs a hybrid credit assignment mechanism combining base rewards with individual contribution incentives, using historical interaction data to calculate Shapley values efficiently

Result: experimental results in Multi-Agent Particle Environment, Multi-Agent MuJoCo, and Bi-DexHands demonstrate that HIS outperforms state-of-the-art methods

Conclusion: HIS outperforms state-of-the-art methods in strongly coupled, complex collaborative tasks across multiple benchmark environments, demonstrating superior performance in multi-agent collaboration scenarios.

Abstract: Multi-agent reinforcement learning (MARL) has demonstrated remarkable performance in multi-agent collaboration problems and has become a prominent topic in artificial intelligence research in recent years. However, traditional credit assignment schemes in MARL cannot reliably capture individual contributions in strongly coupled tasks while maintaining training stability, which leads to limited generalization capabilities and hinders algorithm performance. To address these challenges, we propose a Historical Interaction-Enhanced Shapley Policy Gradient Algorithm (HIS) for Multi-Agent Credit Assignment, which employs a hybrid credit assignment mechanism to balance base rewards with individual contribution incentives. By utilizing historical interaction data to calculate the Shapley value in a sample-efficient manner, HIS enhances the agent's ability to perceive its own contribution, while retaining the global reward to maintain training stability. Additionally, we provide theoretical guarantees for the hybrid credit assignment mechanism, ensuring that the assignment results it generates are both efficient and stable. We evaluate the proposed algorithm in three widely used continuous-action benchmark environments: Multi-Agent Particle Environment, Multi-Agent MuJoCo, and Bi-DexHands. Experimental results demonstrate that HIS outperforms state-of-the-art methods, particularly excelling in strongly coupled, complex collaborative tasks.

</details>


### [54] [Can LLM Agents Really Debate? A Controlled Study of Multi-Agent Debate in Logical Reasoning](https://arxiv.org/abs/2511.07784)
*Haolun Wu,Zhenkun Li,Lingyao Li*

Main category: cs.MA

TL;DR: Multi-agent debate improves LLM reasoning through systematic evaluation of structural and cognitive factors, with intrinsic reasoning strength and group diversity being key drivers of success.


<details>
  <summary>Details</summary>
Motivation: To determine whether LLM agents genuinely engage in deliberative reasoning beyond simple ensembling or majority voting in multi-agent debates.

Method: Controlled study using Knight-Knave-Spy logic puzzle with systematic variation of six factors: team size, composition, confidence visibility, debate order, debate depth, and task difficulty.

Result: Intrinsic reasoning strength and group diversity are dominant success drivers; majority pressure suppresses independent correction; effective teams overturn incorrect consensus; rational reasoning predicts improvement best.

Conclusion: LLM debates succeed through genuine deliberative reasoning when properly designed, providing insights for developing interpretable, truth-seeking multi-agent systems.

Abstract: Multi-agent debate (MAD) has recently emerged as a promising framework for improving the reasoning performance of large language models (LLMs). Yet, whether LLM agents can genuinely engage in deliberative reasoning, beyond simple ensembling or majority voting, remains unclear. We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. We systematically set up six structural and cognitive factors, including agent team size, composition, confidence visibility, debate order, debate depth, and task difficulty, to disentangle their respective effects on collective reasoning. Our results show that intrinsic reasoning strength and group diversity are the dominant drivers of debate success, while structural parameters such as order or confidence visibility offer limited gains. Beyond outcomes, process-level analyses identify key behavioral patterns: majority pressure suppresses independent correction, effective teams overturn incorrect consensus, and rational, validity-aligned reasoning most strongly predicts improvement. These findings provide valuable insights into how and why LLM debates succeed or fail, offering guidance for designing interpretable and truth-seeking multi-agent reasoning systems.

</details>


### [55] [How Brittle is Agent Safety? Rethinking Agent Risk under Intent Concealment and Task Complexity](https://arxiv.org/abs/2511.08487)
*Zihan Ma,Dongsheng Zhu,Shudong Liu,Taolin Zhang,Junnan Liu,Qingqiu Li,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.MA

TL;DR: Introducing OASIS benchmark reveals that current LLM agent safety evaluations miss concealed threats and complex task vulnerabilities, showing safety alignment degrades with intent concealment and reveals a 'Complexity Paradox'.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations for LLM-driven agents primarily focus on atomic harms, failing to address sophisticated threats where malicious intent is concealed or diluted within complex tasks.

Method: We introduce OASIS (Orthogonal Agent Safety Inquiry Suite), a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox.

Result: Our findings reveal two critical phenomena: safety alignment degrades sharply and predictably as intent becomes obscured, and a 'Complexity Paradox' emerges, where agents seem safer on harder tasks only due to capability limitations.

Conclusion: By releasing OASIS and its simulation environment, we provide a principled foundation for probing and strengthening agent safety in these overlooked dimensions.

Abstract: Current safety evaluations for LLM-driven agents primarily focus on atomic harms, failing to address sophisticated threats where malicious intent is concealed or diluted within complex tasks. We address this gap with a two-dimensional analysis of agent safety brittleness under the orthogonal pressures of intent concealment and task complexity. To enable this, we introduce OASIS (Orthogonal Agent Safety Inquiry Suite), a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox. Our findings reveal two critical phenomena: safety alignment degrades sharply and predictably as intent becomes obscured, and a "Complexity Paradox" emerges, where agents seem safer on harder tasks only due to capability limitations. By releasing OASIS and its simulation environment, we provide a principled foundation for probing and strengthening agent safety in these overlooked dimensions.

</details>


### [56] [Climate Driven Interactions Between Malaria Transmission and Diabetes Prevalence](https://arxiv.org/abs/2511.08562)
*Shivank,Anurag Singha,Fakhteh Ghanbarnejad,Ajay K Sharma*

Main category: cs.MA

TL;DR: New epidemiological model reveals diabetic individuals face 1.8-4.0x higher malaria risk due to climate-driven temperature increases and immune vulnerabilities, highlighting urgent need for integrated health strategies.


<details>
  <summary>Details</summary>
Motivation: Climate change is intensifying both infectious and chronic diseases, particularly affecting vulnerable populations. There's a critical gap in existing models that rarely include bidirectional interactions between malaria and diabetes under changing climate conditions, especially given that diabetic individuals have weakened immune defenses and face increased malaria risk.

Method: Developed a new compartmental epidemiological model using synthetic data fitted to Indian disease patterns (2019-2021), with temperature-dependent transmission parameters, seasonal variability, and differential disease dynamics between diabetic and non-diabetic populations. Model calibration used Multi-Start optimization combined with Sequential Quadratic Programming.

Result: Diabetic individuals had 1.8-4.0 times higher odds of malaria infection compared to non-diabetic individuals. Peak infection levels reached 35-36% in diabetic groups versus 20-21% in non-diabetic groups. The model effectively captured observed epidemiological patterns, with basic reproduction numbers averaging 2.3 (range: 0.31-2.75 across seasons).

Conclusion: Given the projected growth of India's diabetic population to 157 million by 2050, these findings highlight the urgent need for integrated health strategies and monitoring systems that simultaneously address both malaria and diabetes in the context of climate change.

Abstract: Climate change is intensifying infectious and chronic diseases like malaria and diabetes, respectively, especially among the vulnerable populations. Global temperatures have risen by approximately $0.6^\circ$C since 1950, extending the window of transmission for mosquito-borne infections and worsening outcomes in diabetes due to metabolic stress caused by heat. People living with diabetes have already weakened immune defenses and, therefore, are at an alarmingly increased risk of contraction of malaria. However, most models rarely include both ways of interaction in changing climate conditions. In the paper, we introduce a new compartmental epidemiological model based on synthetic data fitted to disease patterns of India from 2019 to 2021. The framework captures temperature-dependent transmission parameters, seasonal variability, and different disease dynamics between diabetic and non-diabetic groups within the three-compartment system. Model calibration using Multi-Start optimization combined with Sequential Quadratic Programming allows us to find outstanding differences between populations. The odds of malaria infection in diabetic individuals were found to be 1.8--4.0 times higher, with peak infection levels in 35--36\%, as compared to 20--21\% in the non-diabetic ones. The fitted model was able to capture well the epidemiological patterns observed, while the basic reproduction number averaged around 2.3, ranging from 0.31 to 2.75 in different seasons. Given that India's diabetic population is set to rise to about 157 million people by 2050, these findings point to a pressing need for concerted efforts toward climate-informed health strategies and monitoring systems that address both malaria and diabetes jointly.

</details>
