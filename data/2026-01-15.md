<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: This paper introduces a dataset (ConvoLearn) for pedagogical AI tutoring, shows fine-tuning improves LLMs' knowledge-building performance, and establishes a framework for future work.


<details>
  <summary>Details</summary>
Motivation: To address fundamental pedagogical limitations of LLMs in education, such as revealing solutions rather than supporting dialogic learning.

Method: Developed the ConvoLearn dataset with six pedagogical dimensions, built a semi-synthetic dataset of tutor-student dialogues in Earth Science, and fine-tuned Mistral 7B using QLoRA.

Result: Fine-tuning on ConvoLearn significantly shifts LLM behavior toward knowledge-building strategies, with human evaluation showing the fine-tuned model outperforms base Mistral 7B and Claude Sonnet 4.5.

Conclusion: This work provides a potential framework to guide future development and evaluation of constructivist AI tutors.

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [2] [ART: Action-based Reasoning Task Benchmarking for Medical AI Agents](https://arxiv.org/abs/2601.08988)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji*

Main category: cs.AI

TL;DR: ART is a new benchmark that tests medical AI agents on complex, action-based reasoning tasks using real EHR data, showing models excel at retrieval but struggle with aggregation and threshold logic.


<details>
  <summary>Details</summary>
Motivation: To develop safer, more reliable clinical AI agents by exposing their reasoning weaknesses in action-based tasks involving electronic health records (EHRs).

Method: A four-stage pipeline involving scenario identification, task generation, quality audit, and evaluation, creating 600 diverse clinical tasks from mined EHR data to test models.

Result: GPT-4o-mini and Claude 3.5 Sonnet show near-perfect retrieval but large gaps in aggregation (28-64% error) and threshold reasoning (32-38% error).

Conclusion: ART advances medical AI by highlighting critical failure points in reasoning, laying groundwork for more reliable systems that reduce clinician burden.

Abstract: Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records (EHRs). While large language models (LLMs) show promise in healthcare, existing benchmarks inadequately assess performance on action-based tasks involving threshold evaluation, temporal aggregation, and conditional logic. We introduce ART, an Action-based Reasoning clinical Task benchmark for medical AI agents, which mines real-world EHR data to create challenging tasks targeting known reasoning weaknesses. Through analysis of existing benchmarks, we identify three dominant error categories: retrieval failures, aggregation errors, and conditional logic misjudgments. Our four-stage pipeline -- scenario identification, task generation, quality audit, and evaluation -- produces diverse, clinically validated tasks grounded in real patient data. Evaluating GPT-4o-mini and Claude 3.5 Sonnet on 600 tasks shows near-perfect retrieval after prompt refinement, but substantial gaps in aggregation (28--64%) and threshold reasoning (32--38%). By exposing failure modes in action-oriented EHR reasoning, ART advances toward more reliable clinical agents, an essential step for AI systems that reduce cognitive load and administrative burden, supporting workforce capacity in high-demand care settings

</details>


### [3] [The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments](https://arxiv.org/abs/2601.09032)
*Logan Ritchie,Sushant Mehta,Nick Heiner,Mason Yu,Edwin Chen*

Main category: cs.AI

TL;DR: On the evaluation of frontier AI models in a realistic e-commerce RL environment, the hierarchy of agentic capabilities is identified, and even top models fail ~40% of tasks, mainly on contextual inference.


<details>
  <summary>Details</summary>
Motivation: Evaluate LLM-based agents not just on single-turn responses but on multi-step task completion in interactive workplace environments to identify key capability gaps.

Method: Empirical study of frontier AI models on 150 workplace tasks in an e-commerce RL environment from Surge, with task-centric design emphasizing diversity and domain expert contributions.

Result: Hierarchy of agentic capabilities: tool use, planning and goal formation, adaptability, groundedness, common-sense reasoning; best models fail ~40%, failures cluster predictably along the hierarchy; weaker models struggle with basic tools and planning, stronger models fail on contextual inference.

Conclusion: Current frontier models show coherent multi-step behavior but have substantial gaps (e.g., in contextual inference) before achieving human-level task completion in realistic settings.

Abstract: The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.

</details>


### [4] [Human-AI Co-design for Clinical Prediction Models](https://arxiv.org/abs/2601.09072)
*Jean Feng,Avni Kothari,Patrick Vossler,Andrew Bishara,Lucas Zier,Newton Addo,Aaron Kornblith,Yan Shuo Tan,Chandan Singh*

Main category: cs.AI

TL;DR: HACHI is an AI-driven human-in-the-loop framework that accelerates the development of interpretable clinical prediction models by iteratively exploring concepts in clinical notes with expert feedback.


<details>
  <summary>Details</summary>
Motivation: Traditional development of clinical prediction models (CPMs) is time- and resource-intensive, especially when incorporating unstructured clinical notes with many concepts, limiting their practical use in clinical practice.

Method: HACHI alternates between an AI agent exploring/evaluating candidate concepts in clinical notes and clinical experts providing feedback, using concepts defined as yes-no questions in linear models for transparency.

Result: In real-world tasks (acute kidney injury and traumatic brain injury), HACHI outperforms existing approaches, surfaces new clinically relevant concepts, and improves model generalizability across sites and time periods.

Conclusion: HACHI effectively accelerates CPM development, highlights the crucial role of the clinical AI team in refining concepts, adjusting granularity, aligning objectives, and addressing data biases, making models more interpretable and generalizable.

Abstract: Developing safe, effective, and practically useful clinical prediction models (CPMs) traditionally requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines the often small but critical details of the model building process, such as which features/patients to include and how clinical categories should be defined. However, this traditional collaboration process is extremely time- and resource-intensive, resulting in only a small fraction of CPMs reaching clinical practice. This challenge intensifies when teams attempt to incorporate unstructured clinical notes, which can contain an enormous number of concepts. To address this challenge, we introduce HACHI, an iterative human-in-the-loop framework that uses AI agents to accelerate the development of fully interpretable CPMs by enabling the exploration of concepts in clinical notes. HACHI alternates between (i) an AI agent rapidly exploring and evaluating candidate concepts in clinical notes and (ii) clinical and domain experts providing feedback to improve the CPM learning process. HACHI defines concepts as simple yes-no questions that are used in linear models, allowing the clinical AI team to transparently review, refine, and validate the CPM learned in each round. In two real-world prediction tasks (acute kidney injury and traumatic brain injury), HACHI outperforms existing approaches, surfaces new clinically relevant concepts not included in commonly-used CPMs, and improves model generalizability across clinical sites and time periods. Furthermore, HACHI reveals the critical role of the clinical AI team, such as directing the AI agent to explore concepts that it had not previously considered, adjusting the granularity of concepts it considers, changing the objective function to better align with the clinical objectives, and identifying issues of data bias and leakage.

</details>


### [5] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE framework separates reasoning from execution to improve multi-constraint planning with LLMs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM approaches for multi-constraint planning have limitations: pure reasoning is prone to inconsistency and high cost, while code/solver-based strategies lack flexibility and reusability.

Method: Introduce the Scalable Code Planning Engine (SCOPE), which disentangles query-specific reasoning from generic code execution, producing consistent and reusable solver functions.

Result: SCOPE achieves 93.1% success on TravelPlanner with GPT-4o, a 61.6% gain over CoT, while cutting inference cost by 1.4x and time by ~4.67x.

Conclusion: SCOPE effectively addresses the challenges of multi-constraint planning by balancing reasoning and execution, leading to enhanced performance and efficiency.

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [6] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: A new approach called DScheLLM uses fine-tuned LLMs in a dual-system reasoning architecture to handle dynamic disruptions in production scheduling, showing high efficiency and solver-compatibility in job shop benchmarks.


<details>
  <summary>Details</summary>
Motivation: Production scheduling faces challenges from dynamic disruptions like processing time variations and machine unavailability, and conventional methods lack adaptability and generalization to unseen disturbances.

Method: DScheLLM leverages fine-tuned large language models (LLMs) within a fast-slow reasoning architecture, with training datasets generated from exact schedules, fine-tuning the Huawei OpenPangu Embedded-7B model using LoRA under hybrid reasoning paradigms.

Result: Experimental evaluations show the fast-thinking mode efficiently generates high-quality schedules, while the slow-thinking mode produces solver-compatible and well-formatted decision inputs, demonstrating potential in adaptive scheduling.

Conclusion: This work is an early application of large language models to job shop scheduling in dynamic environments, highlighting their significant promise for intelligent and adaptive optimization, providing a unified and scalable framework for handling diverse disturbances.

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [7] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: Proposes AviationLMM, a large multimodal foundation model for civil aviation to integrate heterogeneous data, enabling improved situational awareness and decision support.


<details>
  <summary>Details</summary>
Motivation: Conventional AI solutions in aviation are siloed and narrow, struggling with integrating diverse data like voice, radar, sensors, and texts, limiting adaptability and real-time support.

Method: Describes model architecture that ingests multimodal inputs (air-ground voice, surveillance, telemetry, video, structured texts) for cross-modal alignment, fusion, and flexible outputs.

Result: Identifies key research opportunities: data acquisition, alignment, pretraining, reasoning, trustworthiness, privacy, robustness, and synthetic scenario generation.

Conclusion: Aims to boost progress in civil aviation foundation models and catalyze coordinated research for an integrated, trustworthy, and privacy-preserving AI ecosystem in aviation.

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [8] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: A survey paper that provides a structured synthesis of memory mechanisms in LLMs and MLLMs, categorizing them into implicit, explicit, and agentic memory paradigms, and discusses integration across modalities, benchmarks, and challenges.


<details>
  <summary>Details</summary>
Motivation: Memory is crucial for enhancing reasoning, adaptability, and contextual fidelity in modern LLMs and MLLMs as they evolve from static predictors to interactive systems with continual learning and personalized inference, necessitating a comprehensive review of memory approaches.

Method: The survey organizes existing literature into a taxonomy with three primary memory frameworks (implicit, explicit, and agentic), detailing each through definitions, examples, and recent work, and extends analysis to multi-modal settings.

Result: The paper synthesizes memory in LLMs/MLLMs into a cohesive taxonomy, highlighting key architectural advances, benchmark tasks, and open challenges like memory capacity, factual consistency, and interoperability, without presenting new experimental results.

Conclusion: Memory mechanisms are foundational for advancing LLMs and MLLMs towards more interactive and adaptive systems, but significant challenges around capacity, alignment, and cross-modal coherence remain, guiding future research in memory-enhanced AI.

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [9] [PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?](https://arxiv.org/abs/2601.09152)
*Yiwen Tu,Xuan Liu,Lianhui Qin,Haojian Jin*

Main category: cs.AI

TL;DR: PRA is an AI agent simulating individual privacy concern formation, leveraging personal history and context to generate synthetic reasoning, with LLM-based evaluation and cross-domain transferability.


<details>
  <summary>Details</summary>
Motivation: To move beyond population-level sentiment analysis and simulate how individual users form privacy concerns in response to real-world news, based on personal history and context.

Method: PRA integrates privacy and cognitive theories to reconstruct a user's 'privacy mind' by dynamically activating memory via a contextual filter and generating synthetic comments; evaluation uses an LLM-as-a-Judge approach calibrated with a privacy concern taxonomy.

Result: Experiments on real-world Hacker News discussions show PRAs outperformed baseline agents in privacy concern prediction and captured transferable reasoning patterns across domains such as AI, e-commerce, and healthcare.

Conclusion: PRA demonstrates an effective method for simulating user-specific privacy concerns, offering insights into cross-domain transferability and practical applications for privacy adaptation.

Abstract: This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's "privacy mind", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.

</details>


### [10] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: The paper advocates for a shift from using LLMs to automatically generate reviews to employing them as tools to assist and educate human reviewers, proposing two systems for improving peer-review quality and sustainability.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of AI research creates a Reviewer Gap, threatening peer-review sustainability and leading to low-quality evaluations, highlighting the need to address this with a human-centered approach.

Method: It defines core principles of high-quality peer review and proposes two complementary systems: an LLM-assisted mentoring system for cultivating reviewer competencies and an LLM-assisted feedback system for refining review quality.

Result: The paper positions LLMs as assistants for human reviewers rather than replacements, aiming to enhance reviewer expertise and support a sustainable scholarly ecosystem through these defined systems.

Conclusion: The proposed human-centered paradigm shift leverages LLMs to strengthen reviewer skills and institutions, contributing to a more robust and sustainable peer-review process in AI academia.

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [11] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: MAXS is a meta-adaptive reasoning framework for LLM agents that uses lookahead and adaptive selection to achieve stable, efficient multi-tool reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent methods suffer from locally myopic generation and trajectory instability due to lack of lookahead, making it hard to balance global effectiveness and computational efficiency.

Method: Proposes the MAXS framework, employing a lookahead strategy to extend reasoning paths ahead, estimating tool advantage values, and using step consistency variance and trend slopes to select stable, consistent, high-value reasoning steps, plus a trajectory convergence mechanism for cost control.

Result: Experiments across three base models and five datasets show MAXS consistently outperforms existing methods in both performance and inference efficiency.

Conclusion: MAXS effectively addresses myopia and instability, balancing resource efficiency and global effectiveness in multi-tool reasoning for LLM agents.

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [12] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: CoT-Flow introduces a continuous probabilistic flow model for chain-of-thought reasoning, enabling flow-guided decoding and reinforcement learning to improve inference efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current chain-of-thought paradigms treat reasoning as an indivisible sequence, lacking mechanisms to quantify step-wise information gain, leading to inference inefficiency from redundant exploration and optimization difficulty with sparse supervision or costly verifiers.

Method: The framework reconceptualizes discrete reasoning steps as a continuous probabilistic flow, which quantifies each step's contribution toward the ground-truth answer. This enables flow-guided decoding for efficient path extraction and flow-based reinforcement learning with a verifier-free dense reward function.

Result: Experiments on challenging benchmarks show that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance compared to existing methods.

Conclusion: CoT-Flow effectively addresses granularity gaps in chain-of-thought reasoning by modeling it as a continuous flow, offering enhancements in both decoding strategies and learning processes without the need for external verification.

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [13] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: An LLM multi-agent framework enables coordinated, proactive pandemic control across regions by simulating intervention scenarios, reducing infections and deaths significantly.


<details>
  <summary>Details</summary>
Motivation: Human pandemic responses are often fragmented and reactive, hindering proactive control. To address this, a coordinated policymaking approach is needed across interdependent regions.

Method: Assign an LLM agent to each administrative region to analyze local data and communicate with others. Use real-world data, a pandemic simulator, and structured communication for counterfactual analysis through closed-loop simulations.

Result: Using US COVID-19 data from 2020, the framework reduces cumulative infections and deaths by up to 63.7% and 40.1% at the state level, and 39.0% and 27.0% across states, compared to real-world outcomes.

Conclusion: The LLM multi-agent system effectively facilitates coordinated policymaking, demonstrating potential for more proactive and effective pandemic control through simulation-based decision-making.

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [14] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER is a plug-and-play framework that uses adaptive activation steering with reusable reasoning vectors and a reinforcement-learning-optimized router to dynamically compose interventions for efficient LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing domain-specific reasoning methods for LLMs rely on training-intensive parameter updates, while activation steering alternatives use static manual interventions that don't adapt to the dynamic nature of complex reasoning.

Method: Constructs library of reusable reasoning vectors, employs lightweight router optimized via reinforcement learning under task-level rewards to dynamically compose vectors for each input, activating cognitive primitives emergently and compositionally.

Result: Achieves 3.4-6.5% average zero-shot accuracy improvements across seven diverse benchmarks, surpasses CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains, autonomously combines vectors into interpretable control strategies.

Conclusion: RISER points toward more controllable and efficient LLM reasoning through adaptive activation steering that dynamically composes latent cognitive primitives via reinforcement-learning-optimized routing.

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [15] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: TL;DR: A3-Bench is a new benchmark to evaluate scientific reasoning through memory-driven activation using anchors and attractors, with 2198 annotated problems and Dual-Scale AAUI metric for insights.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook the memory-driven mechanisms like anchor and attractor activation that underlie human scientific reasoning, which enhances consistency and stability.

Method: Annotated 2198 science reasoning problems across domains using the SAPM process and introduced a dual-scale memory evaluation framework with anchors and attractors, measured by the AAUI metric.

Result: Experiments with various models validated A3-Bench, showing how memory activation impacts reasoning performance and providing insights into memory-driven scientific reasoning.

Conclusion: The A3-Bench benchmark effectively addresses the gap by evaluating scientific reasoning based on memory-driven activation, offering a tool for future research.

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [16] [M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning](https://arxiv.org/abs/2601.09278)
*Xiaohan Yu,Chao Feng,Lang Mei,Chong Chen*

Main category: cs.AI

TL;DR: M³Searcher: a modular multimodal agent for information-seeking that decouples acquisition from answer derivation, outperforming existing methods in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing DeepResearch-style agents are limited to text modality; extending to multimodal settings introduces challenges like the specialization-generalization trade-off and scarcity of training data for multimodal search trajectories.

Method: Proposes M³Searcher, optimized with a retrieval-oriented multi-objective reward for factual accuracy, reasoning soundness, and retrieval fidelity, and develops MMSearchVQA dataset for training.

Result: Experimental results show M³Searcher outperforms existing approaches, exhibits strong transfer adaptability, and effective reasoning in multimodal tasks.

Conclusion: M³Searcher successfully addresses multimodal challenges, enabling advanced autonomous information-seeking with improved performance and adaptability.

Abstract: Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Attention Consistency Regularization for Interpretable Early-Exit Neural Networks](https://arxiv.org/abs/2601.08891)
*Yanhua Zhao*

Main category: cs.LG

TL;DR: EGT improves interpretability in early-exit neural networks via attention-based regularization, achieving high accuracy with faster inference.


<details>
  <summary>Details</summary>
Motivation: Early-exit networks reduce computation but lack interpretability and consistent feature focus across layers, limiting trust in explainable AI applications.

Method: Introduces Explanation-Guided Training (EGT), a multi-objective framework with an attention consistency loss to align early-exit attention maps with the final exit.

Result: EGT achieves 98.97% accuracy (matching baseline) with 1.97x inference speedup and improves attention consistency by up to 18.5% on image classification tasks.

Conclusion: EGT makes early-exit networks more interpretable and consistent, enhancing suitability for explainable AI in resource-constrained settings.

Abstract: Early-exit neural networks enable adaptive inference by allowing predictions at intermediate layers, reducing computational cost. However, early exits often lack interpretability and may focus on different features than deeper layers, limiting trust and explainability. This paper presents Explanation-Guided Training (EGT), a multi-objective framework that improves interpretability and consistency in early-exit networks through attention-based regularization. EGT introduces an attention consistency loss that aligns early-exit attention maps with the final exit. The framework jointly optimizes classification accuracy and attention consistency through a weighted combination of losses. Experiments on a real-world image classification dataset demonstrate that EGT achieves up to 98.97% overall accuracy (matching baseline performance) with a 1.97x inference speedup through early exits, while improving attention consistency by up to 18.5% compared to baseline models. The proposed method provides more interpretable and consistent explanations across all exit points, making early-exit networks more suitable for explainable AI applications in resource-constrained environments.

</details>


### [18] [Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models](https://arxiv.org/abs/2601.08893)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: Spectral Generative Flow Models (SGFMs) replace token-based transformers with physics-inspired continuous field dynamics using wavelet bases for generative AI.


<details>
  <summary>Details</summary>
Motivation: To develop an alternative to transformer-based large language models by grounding generative modeling in physics-inspired continuous dynamics, aiming for enhanced long-range coherence and multimodality.

Method: The framework involves constrained stochastic dynamics in a multiscale wavelet basis, replacing global attention with local operators, spectral projections, and Navier–Stokes-like transport.

Result: SGFMs introduce a field-theoretic ontology, a sparse wavelet-domain representation, and constrained stochastic flows, offering a novel generative architecture.

Conclusion: SGFMs provide a principled alternative to transformer-based and other autoregressive models, paving the way for next-generation generative AI with physically inspired inductive biases.

Abstract: We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure.
  Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.

</details>


### [19] [XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation](https://arxiv.org/abs/2601.08896)
*Sahaj Raj Malla,Shreeyash Kayastha,Rumi Suwal,Harish Chandra Bhandari,Rajendra Adhikari*

Main category: cs.LG

TL;DR: Develops XGBoost framework to forecast daily log-returns in NEPSE Index with engineered features and hyperparameter optimization, outperforming benchmarks on error and directional accuracy.


<details>
  <summary>Details</summary>
Motivation: To create a robust forecasting method for Nepal Stock Exchange Index returns, addressing nonlinear dynamics and noise in volatile emerging markets.

Method: Uses XGBoost regressor with lagged log-returns and technical indicators, optimizes hyperparameters via Optuna, evaluates with time-series cross-validation and walk-forward validation.

Result: Expanding window with 20 lags achieves lowest log-return RMSE (0.013450), MAE (0.009814), directional accuracy of 65.15%, outperforms ARIMA and Ridge regression.

Conclusion: Demonstrates gradient boosting's effectiveness for nonlinear financial time series in emerging markets, establishing a reproducible benchmark for NEPSE forecasting with enhanced interpretability.

Abstract: This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling window schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R-squared), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration, an expanding window with 20 lags, outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R-squared remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.

</details>


### [20] [DriftGuard: A Hierarchical Framework for Concept Drift Detection and Remediation in Supply Chain Forecasting](https://arxiv.org/abs/2601.08928)
*Shahnawaz Alam,Mohammed Abdul Rahman,Bareera Sadeqa*

Main category: cs.LG

TL;DR: DriftGuard is an end-to-end system for detecting, diagnosing, and automatically correcting concept drift in supply chain forecasting models, achieving 97.8% detection recall within 4.2 days and high ROI in M5 dataset.


<details>
  <summary>Details</summary>
Motivation: Supply chain forecasting models degrade over time due to concept drift from promotions, consumer preferences, and disruptions, causing stockouts or excess inventory without warnings. Manual monitoring and scheduled retraining are inefficient, and existing academic methods fail to diagnose or remediate drift, ignoring hierarchical data structures.

Method: DriftGuard uses five modules: an ensemble of four detection methods (error-based monitoring, statistical tests, autoencoder anomaly detection, CUSUM change-point analysis) with hierarchical propagation analysis for precise drift localization, SHAP analysis for root cause diagnosis, and cost-aware retraining to update only affected models.

Result: Evaluated on over 30,000 time series from the M5 retail dataset, DriftGuard achieves 97.8% detection recall within 4.2 days and delivers up to 417 return on investment through targeted remediation.

Conclusion: DriftGuard provides a comprehensive solution to supply chain concept drift, combining detection, diagnosis, and remedial actions efficiently, with demonstrated high performance and cost-effectiveness in a large-scale retail setting.

Abstract: Supply chain forecasting models degrade over time as real-world conditions change. Promotions shift, consumer preferences evolve, and supply disruptions alter demand patterns, causing what is known as concept drift. This silent degradation leads to stockouts or excess inventory without triggering any system warnings. Current industry practice relies on manual monitoring and scheduled retraining every 3-6 months, which wastes computational resources during stable periods while missing rapid drift events. Existing academic methods focus narrowly on drift detection without addressing diagnosis or remediation, and they ignore the hierarchical structure inherent in supply chain data. What retailers need is an end-to-end system that detects drift early, explains its root causes, and automatically corrects affected models. We propose DriftGuard, a five-module framework that addresses the complete drift lifecycle. The system combines an ensemble of four complementary detection methods, namely error-based monitoring, statistical tests, autoencoder anomaly detection, and Cumulative Sum (CUSUM) change-point analysis, with hierarchical propagation analysis to identify exactly where drift occurs across product lines. Once detected, Shapley Additive Explanations (SHAP) analysis diagnoses the root causes, and a cost-aware retraining strategy selectively updates only the most affected models. Evaluated on over 30,000 time series from the M5 retail dataset, DriftGuard achieves 97.8% detection recall within 4.2 days and delivers up to 417 return on investment through targeted remediation.

</details>


### [21] [Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2601.08963)
*Adrita Das,Peiran Jiang,Dantong Zhu,Barnabas Poczos,Jose Lugo-Martinez*

Main category: cs.LG

TL;DR: The paper provides a theoretical reinterpretation of the Directly Denoising Diffusion Model (DDDM) using the Reverse Transition Kernel (RTK) framework, showing deterministic denoising as an approximate kernel operator for efficient inference in molecular diffusion models.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for molecular design face inefficiencies like long sampling times, stochastic variance, and limited structural awareness, with DDDM's deterministic updates lacking clear theoretical foundations, motivating a principled reinterpretation to address these issues.

Method: The method uses the Reverse Transition Kernel (RTK) framework to reinterpret DDDM, expressing its reverse process as an approximate kernel operator, analyzing deterministic denoising as a structured transport map between noisy and clean samples.

Result: Empirical results on the GEOM-DRUGS dataset show that RTK-guided deterministic denoising achieves faster convergence, higher structural fidelity, numerical stability, improved sample consistency, and scalable symmetry-preserving denoisers while maintaining chemical validity.

Conclusion: The RTK framework unifies deterministic and stochastic diffusion, providing theoretical clarity for DDDM, resolving bottlenecks in molecular diffusion, and enabling efficient, stable, and high-quality molecule generation.

Abstract: Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic variance in the reverse process, and limited structural awareness in denoising dynamics. The Directly Denoising Diffusion Model (DDDM) mitigates these inefficiencies by replacing stochastic reverse MCMC updates with deterministic denoising step, substantially reducing inference time. Yet, the theoretical underpinnings of such deterministic updates have remained opaque. In this work, we provide a principled reinterpretation of DDDM through the lens of the Reverse Transition Kernel (RTK) framework by Huang et al. 2024, unifying deterministic and stochastic diffusion under a shared probabilistic formalism. By expressing the DDDM reverse process as an approximate kernel operator, we show that the direct denoising process implicitly optimizes a structured transport map between noisy and clean samples. This perspective elucidates why deterministic denoising achieves efficient inference. Beyond theoretical clarity, this reframing resolves several long-standing bottlenecks in molecular diffusion. The RTK view ensures numerical stability by enforcing well-conditioned reverse kernels, improves sample consistency by eliminating stochastic variance, and enables scalable and symmetry-preserving denoisers that respect SE(3) equivariance. Empirically, we demonstrate that RTK-guided deterministic denoising achieves faster convergence and higher structural fidelity than stochastic diffusion models, while preserving chemical validity across GEOM-DRUGS dataset. Code, models, and datasets are publicly available in our project repository.

</details>


### [22] [Continuous Fairness On Data Streams](https://arxiv.org/abs/2601.08976)
*Subhodeep Ghosh,Zhihui Du,Angela Bonifati,Manish Kumar,David Bader,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: The paper introduces a method for enforcing continuous group fairness at a finer granularity level within sliding windows in data streams, using efficient monitoring and reordering algorithms with millisecond-level processing and significant fairness improvements.


<details>
  <summary>Details</summary>
Motivation: Existing fairness models in data streams often enforce fairness at the window level, which can be insufficient when window sizes are large, as it fails to ensure fairness at finer granularities, leading to potential biases within windows.

Method: Proposes a novel fairness model for block-level group fairness within each sliding window. Includes sketch-based data structures for real-time monitoring with minimal overhead and develops optimal, efficient algorithms for reordering windows when fairness is violated, supported by theoretical guarantees.

Result: Evaluation on four real-world streaming scenarios shows practical effectiveness with millisecond-level processing and throughput of about 30,000 queries per second on average. The reordering algorithm improves block-level group fairness by up to 95% in certain cases and by 50-60% on average across datasets.

Conclusion: Block-level fairness offers significant advantages over window-level fairness in data streams, as demonstrated by improved fairness metrics and efficient performance, making it a valuable approach for real-time fairness enforcement in streaming applications.

Abstract: We study the problem of enforcing continuous group fairness over windows in data streams. We propose a novel fairness model that ensures group fairness at a finer granularity level (referred to as block) within each sliding window. This formulation is particularly useful when the window size is large, making it desirable to enforce fairness at a finer granularity. Within this framework, we address two key challenges: efficiently monitoring whether each sliding window satisfies block-level group fairness, and reordering the current window as effectively as possible when fairness is violated. To enable real-time monitoring, we design sketch-based data structures that maintain attribute distributions with minimal overhead. We also develop optimal, efficient algorithms for the reordering task, supported by rigorous theoretical guarantees. Our evaluation on four real-world streaming scenarios demonstrates the practical effectiveness of our approach. We achieve millisecond-level processing and a throughput of approximately 30,000 queries per second on average, depending on system parameters. The stream reordering algorithm improves block-level group fairness by up to 95% in certain cases, and by 50-60% on average across datasets. A qualitative study further highlights the advantages of block-level fairness compared to window-level fairness.

</details>


### [23] [Optimising for Energy Efficiency and Performance in Machine Learning](https://arxiv.org/abs/2601.08991)
*Emile Dos Santos Ferreira,Neil D. Lawrence,Andrei Paleyes*

Main category: cs.LG

TL;DR: ECOpt is a hyperparameter tuner that optimizes ML models for energy efficiency and performance, offering an interpretable Pareto frontier.


<details>
  <summary>Details</summary>
Motivation: Energy consumption in ML is rising with larger models, but existing tools lack actionable feedback and ignore inference costs, creating a need for energy-efficient optimization.

Method: Developed ECOpt, a hyperparameter tuner that quantifies the trade-off between energy efficiency and model performance, enabling informed decisions and compliance with regulations.

Result: Found that parameter and FLOP counts are unreliable energy proxies, observed consistent energy efficiency for Transformers across hardware, and uncovered seven improved CIFAR-10 models.

Conclusion: ECOpt facilitates energy-efficient ML with a net positive environmental impact, promoting the measurement and publication of energy metrics to address scaling challenges.

Abstract: The ubiquity of machine learning (ML) and the demand for ever-larger models bring an increase in energy consumption and environmental impact. However, little is known about the energy scaling laws in ML, and existing research focuses on training cost -- ignoring the larger cost of inference. Furthermore, tools for measuring the energy consumption of ML do not provide actionable feedback.
  To address these gaps, we developed Energy Consumption Optimiser (ECOpt): a hyperparameter tuner that optimises for energy efficiency and model performance. ECOpt quantifies the trade-off between these metrics as an interpretable Pareto frontier. This enables ML practitioners to make informed decisions about energy cost and environmental impact, while maximising the benefit of their models and complying with new regulations.
  Using ECOpt, we show that parameter and floating-point operation counts can be unreliable proxies for energy consumption, and observe that the energy efficiency of Transformer models for text generation is relatively consistent across hardware. These findings motivate measuring and publishing the energy metrics of ML models. We further show that ECOpt can have a net positive environmental impact and use it to uncover seven models for CIFAR-10 that improve upon the state of the art, when considering accuracy and energy efficiency together.

</details>


### [24] [Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction](https://arxiv.org/abs/2601.08999)
*Pranjal Patil,Anli Ji,Berkay Aydin*

Main category: cs.LG

TL;DR: A novel physics-guided counterfactual explanation framework for time series classification enhances interpretability in solar energetic particle forecasting, improving proximity and speed while ensuring physical plausibility.


<details>
  <summary>Details</summary>
Motivation: Solar energetic particle event prediction is crucial for space safety, but machine learning models often ignore domain-specific feasibility constraints, and existing counterfactual explanation methods lack physical plausibility.

Method: The paper introduces a Physics-Guided Counterfactual Explanation framework designed for time series classification tasks, enforcing consistency with physical principles in domains like solar energetic particle forecasting.

Result: The framework achieves over 80% reduction in Dynamic Time Warping distance, increases sparsity in explanations, reduces runtime by nearly 50% compared to baselines like DiCE, and ensures physically plausible counterfactual explanations.

Conclusion: The framework successfully generates valid and physically consistent counterfactual explanations, establishing a foundation for scalable counterfactual generation in big data settings.

Abstract: Accurate prediction of solar energetic particle events is vital for safeguarding satellites, astronauts, and space-based infrastructure. Modern space weather monitoring generates massive volumes of high-frequency, multivariate time series (MVTS) data from sources such as the Geostationary perational Environmental Satellites (GOES). Machine learning (ML) models trained on this data show strong predictive power, but most existing methods overlook domain-specific feasibility constraints. Counterfactual explanations have emerged as a key tool for improving model interpretability, yet existing approaches rarely enforce physical plausibility. This work introduces a Physics-Guided Counterfactual Explanation framework, a novel method for generating counterfactual explanations in time series classification tasks that remain consistent with underlying physical principles. Applied to solar energetic particles (SEP) forecasting, this framework achieves over 80% reduction in Dynamic Time Warping (DTW) distance increasing the proximity, produces counterfactual explanations with higher sparsity, and reduces runtime by nearly 50% compared to state-of-the-art baselines such as DiCE. Beyond numerical improvements, this framework ensures that generated counterfactual explanations are physically plausible and actionable in scientific domains. In summary, the framework generates counterfactual explanations that are both valid and physically consistent, while laying the foundation for scalable counterfactual generation in big data environments.

</details>


### [25] [Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers](https://arxiv.org/abs/2601.09000)
*Annalisa Belloni,Lorenzo Noci,Antonio Orvieto*

Main category: cs.LG

TL;DR: The paper compares the WSD learning rate scheduler's effects on a language model and a CNN, finding similar training dynamics and landscape geometry, suggesting shared nonconvex optimization characteristics.


<details>
  <summary>Details</summary>
Motivation: To investigate if the performance of the WSD scheduler, which uses decaying learning rates briefly, is specific to transformers or reflects broader geometric insights, by examining optimization paths and sharpness.

Method: Compares the WSD path of Adam on a Pythia-like language model with a small CNN on CIFAR10, analyzing training signals, optimizer path features, and sharpness dynamics.

Result: Observes qualitative similarities in training dynamics and landscape geometry between the two models, indicating shared geometric characteristics in nonconvex loss landscapes.

Conclusion: Suggests that the WSD scheduler's effectiveness is not unique to transformers, pointing to common optimization geometry in high-dimensional problems and hinting at future research directions.

Abstract: The Warmup Stable Decay (WSD) learning rate scheduler has recently become popular, largely due to its good performance and flexibility when training large language models. It remains an open question whether the remarkable performance of WSD - using a decaying learning rate for only a fraction of training compared to cosine decay - is a phenomenon specific to transformer-based language models that can potentially offer new theoretical insights into their training dynamics. Inspired by the usage of learning rate schedulers as a new lens into understanding landscape geometry (e.g., river valley, connected minima, progressive sharpening), in this work we compare the WSD path of the Adam optimizer on a Pythia-like language model to that of a small CNN trained to classify CIFAR10 images. We observe most training signals, optimizer path features, and sharpness dynamics to be qualitatively similar in such architectures. This consistency points to shared geometric characteristics of the loss landscapes of old and new nonconvex problems, and hints to future research questions around the geometry of high dimensional optimization problems.

</details>


### [26] [Meta-learning to Address Data Shift in Time Series Classification](https://arxiv.org/abs/2601.09018)
*Samuel Myren,Nidhi Parikh,Natalie Klein*

Main category: cs.LG

TL;DR: This paper compares traditional deep learning (TDL) with fine-tuning and meta-learning for time-series classification under data shift, showing meta-learning excels with scarce data but loses edge with more data, and introduces the SeisTask benchmark.


<details>
  <summary>Details</summary>
Motivation: Real-world data often shifts over time, causing traditional deep learning models to degrade and require costly retraining, so meta-learning is explored as an adaptive alternative for time-series tasks.

Method: The authors systematically compare TDL with fine-tuning and optimization-based meta-learning algorithms using a controlled seismic benchmark called SeisTask, varying data availability and model architectures.

Result: Meta-learning achieves faster, more stable adaptation and less overfitting with limited data and smaller models, but its advantages diminish as data increases, where TDL with fine-tuning performs similarly. Task alignment, not just diversity, drives meta-learning gains.

Conclusion: Meta-learning is superior under data-scarce conditions for time-series classification with data shift, but its benefits decrease with abundant data; the paper also contributes SeisTask as a benchmark for adaptive learning research.

Abstract: Across engineering and scientific domains, traditional deep learning (TDL) models perform well when training and test data share the same distribution. However, the dynamic nature of real-world data, broadly termed \textit{data shift}, renders TDL models prone to rapid performance degradation, requiring costly relabeling and inefficient retraining. Meta-learning, which enables models to adapt quickly to new data with few examples, offers a promising alternative for mitigating these challenges. Here, we systematically compare TDL with fine-tuning and optimization-based meta-learning algorithms to assess their ability to address data shift in time-series classification. We introduce a controlled, task-oriented seismic benchmark (SeisTask) and show that meta-learning typically achieves faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. As data availability and model capacity increase, its advantages diminish, with TDL with fine-tuning performing comparably. Finally, we examine how task diversity influences meta-learning and find that alignment between training and test distributions, rather than diversity alone, drives performance gains. Overall, this work provides a systematic evaluation of when and why meta-learning outperforms TDL under data shift and contributes SeisTask as a benchmark for advancing adaptive learning research in time-series domains.

</details>


### [27] [Layer-Parallel Training for Transformers](https://arxiv.org/abs/2601.09026)
*Shuai Jiang,Marc Salvado,Eric C. Cyr,Alena Kopaničáková,Rolf Krause,Jacob B. Schroder*

Main category: cs.LG

TL;DR: A new multilevel, layer-parallel training method for transformers to enhance parallel scalability with depth.


<details>
  <summary>Details</summary>
Motivation: To improve parallel acceleration in training large transformer models like BERT and GPT2 as network depth increases, though it introduces gradient bias near convergence.

Method: Uses a neural ODE formulation of transformers and applies a multilevel parallel-in-time algorithm for forward/backpropagation across layers.

Result: Demonstrated parallel-acceleration and accuracy matching serial pre-training on models like BERT, GPT2, ViT, and machine translation, with fine-tuning unaffected.

Conclusion: The method successfully accelerates parallel training for deep transformers while maintaining accuracy, with techniques to manage gradient bias issues.

Abstract: We present a new training methodology for transformers using a multilevel, layer-parallel approach. Through a neural ODE formulation of transformers, our application of a multilevel parallel-in-time algorithm for the forward and backpropagation phases of training achieves parallel acceleration over the layer dimension. This dramatically enhances parallel scalability as the network depth increases, which is particularly useful for increasingly large foundational models. However, achieving this introduces errors that cause systematic bias in the gradients, which in turn reduces convergence when closer to the minima. We develop an algorithm to detect this critical transition and either switch to serial training or systematically increase the accuracy of layer-parallel training. Results, including BERT, GPT2, ViT, and machine translation architectures, demonstrate parallel-acceleration as well as accuracy commensurate with serial pre-training while fine-tuning is unaffected.

</details>


### [28] [SCaLE: Switching Cost aware Learning and Exploration](https://arxiv.org/abs/2601.09042)
*Neelkamal Bhuyan,Debankur Mukherjee,Adam Wierman*

Main category: cs.LG

TL;DR: An algorithm called SCaLE is introduced for bandit online convex optimization with dynamic quadratic hitting costs and L2 switching costs, offering sub-linear dynamic regret without needing prior knowledge of the cost structure, validated experimentally.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the problem of unbounded metric movement costs in bandit settings, where agents face dynamic quadratic hitting costs and switching costs under noisy feedback, which limits practical applications without efficient algorithms.

Method: Develop the SCaLE algorithm utilizing a novel spectral regret analysis that separates eigenvalue-error and eigenbasis-perturbation driven regret to handle high-dimensional environments in a distribution-agnostic manner.

Result: SCaLE achieves a provable distribution-agnostic sub-linear dynamic regret, outperforming baselines in numerical experiments, and shows statistical consistency in noisy bandit feedback models.

Conclusion: The research provides a robust solution for dynamic regret in bandit online convex optimization, with spectral analysis offering new insights, and demonstrates practical effectiveness through simulations.

Abstract: This work addresses the fundamental problem of unbounded metric movement costs in bandit online convex optimization, by considering high-dimensional dynamic quadratic hitting costs and $\ell_2$-norm switching costs in a noisy bandit feedback model. For a general class of stochastic environments, we provide the first algorithm SCaLE that provably achieves a distribution-agnostic sub-linear dynamic regret, without the knowledge of hitting cost structure. En-route, we present a novel spectral regret analysis that separately quantifies eigenvalue-error driven regret and eigenbasis-perturbation driven regret. Extensive numerical experiments, against online-learning baselines, corroborate our claims, and highlight statistical consistency of our algorithm.

</details>


### [29] [Deep Incomplete Multi-View Clustering via Hierarchical Imputation and Alignment](https://arxiv.org/abs/2601.09051)
*Yiming Du,Ziyu Wang,Jian Li,Rui Ning,Lusi Li*

Main category: cs.LG

TL;DR: DIMVC-HIA is a deep clustering framework for incomplete multi-view data that integrates hierarchical imputation and alignment to improve clustering accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of imputing missing views accurately without bias and ensuring semantic consistency across views in incomplete multi-view clustering.

Method: The method includes view-specific autoencoders, a hierarchical imputation module using cross-view contrastive similarity, an energy-based semantic alignment module, and a contrastive assignment alignment module.

Result: Experiments on benchmarks show that DIMVC-HIA achieves superior performance under varying levels of missingness in multi-view data.

Conclusion: The proposed DIMVC-HIA framework effectively handles incomplete multi-view clustering by integrating imputation and alignment, offering better clustering results.

Abstract: Incomplete multi-view clustering (IMVC) aims to discover shared cluster structures from multi-view data with partial observations. The core challenges lie in accurately imputing missing views without introducing bias, while maintaining semantic consistency across views and compactness within clusters. To address these challenges, we propose DIMVC-HIA, a novel deep IMVC framework that integrates hierarchical imputation and alignment with four key components: (1) view-specific autoencoders for latent feature extraction, coupled with a view-shared clustering predictor to produce soft cluster assignments; (2) a hierarchical imputation module that first estimates missing cluster assignments based on cross-view contrastive similarity, and then reconstructs missing features using intra-view, intra-cluster statistics; (3) an energy-based semantic alignment module, which promotes intra-cluster compactness by minimizing energy variance around low-energy cluster anchors; and (4) a contrastive assignment alignment module, which enhances cross-view consistency and encourages confident, well-separated cluster predictions. Experiments on benchmarks demonstrate that our framework achieves superior performance under varying levels of missingness.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [30] [MACRO-LLM: LLM-Empowered Multi-Agent Collaborative Reasoning under Spatiotemporal Partial Observability](https://arxiv.org/abs/2601.09295)
*Handi Chen,Running Zhao,Xiuzhe Wu,Edith C. H. Ngai*

Main category: cs.MA

TL;DR: MACRO-LLM addresses spatiotemporal partial observability in LLM agents via CoProposer, Negotiator, and Introspector modules for improved coordination.


<details>
  <summary>Details</summary>
Motivation: Distributed LLM agents in real-world settings face limitations in local perception and temporal horizons, leading to spatiotemporal partial observability that hinders efficient coordination.

Method: The paper introduces MACRO-LLM, an architecture with three modules: CoProposer for temporal uncertainty mitigation through predictive rollouts, Negotiator for spatial conflict resolution via mean-field statistical aggregation, and Introspector for adaptive strategy refinement using semantic gradient descent on historical experience.

Result: Evaluations on cooperative adaptive cruise control and pandemic control tasks show that MACRO-LLM effectively mitigates spatiotemporal partial observability, enhancing coordination through spatial and temporal strategies.

Conclusion: MACRO-LLM provides a robust framework for multi-agent LLM collaboration by addressing spatiotemporal constraints, enabling more effective performance in complex, long-horizon real-world scenarios.

Abstract: Large Language Model (LLM) agents deployed in complex real-world scenarios typically operate as spatially distributed entities. However, this physical dispersion constrains agents to limited local perception and finite temporal horizons. We characterize this bottleneck as spatiotemporal partial observability. Given such fragmented awareness, distributed agents struggle to coordinate efficiently. To bridge this gap, we introduce MACRO-LLM, LLM-empowered multi-agent collaborative reasoning under spatiotemporal partial observability. The architecture addresses spatiotemporal constraints via three modules: (1) the CoProposer mitigates temporal uncertainty by verifying candidate actions via predictive rollouts; (2) the Negotiator overcomes spatial myopia by resolving conflicts through mean-field statistical aggregation; and (3) the Introspector ensures continuous adaptation by analyzing historical experience to refine strategies via semantic gradient descent. Extensive evaluations on two complex long-horizon tasks, cooperative adaptive cruise control and pandemic control, demonstrate that our framework effectively mitigates spatiotemporal partial observability through spatial and temporal strategies, enabling robust coordination.

</details>


### [31] [SC-MAS: Constructing Cost-Efficient Multi-Agent Systems with Edge-Level Heterogeneous Collaboration](https://arxiv.org/abs/2601.09434)
*Di Zhao,Longhui Ma,Siwei Wang,Miao Wang,Yi Kong*

Main category: cs.MA

TL;DR: Proposes SC-MAS, a heterogeneous multi-agent system framework using directed graphs with tailored collaboration strategies per agent pair, reducing costs while improving accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems often use homogeneous collaboration, limiting flexibility and increasing costs; inspired by Social Capital Theory to enable role-specific collaboration.

Method: Models MAS as directed graphs with edges representing pairwise collaboration strategies, uses a controller to select roles, assign strategies, and allocate LLM backbones per agent.

Result: Improves MMLU accuracy by 3.35% with 15.38% cost reduction, and MBPP accuracy by 3.53% with 12.13% cost reduction, validating effectiveness.

Conclusion: SC-MAS demonstrates the feasibility and effectiveness of heterogeneous collaboration in multi-agent systems, enhancing performance while reducing inference costs.

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) enhance complex problem solving through multi-agent collaboration, but often incur substantially higher costs than single-agent systems. Recent MAS routing methods aim to balance performance and overhead by dynamically selecting agent roles and language models. However, these approaches typically rely on a homogeneous collaboration mode, where all agents follow the same interaction pattern, limiting collaboration flexibility across different roles. Motivated by Social Capital Theory, which emphasizes that different roles benefit from distinct forms of collaboration, we propose SC-MAS, a framework for constructing heterogeneous and cost-efficient multi-agent systems. SC-MAS models MAS as directed graphs, where edges explicitly represent pairwise collaboration strategies, allowing different agent pairs to interact through tailored communication patterns. Given an input query, a unified controller progressively constructs an executable MAS by selecting task-relevant agent roles, assigning edge-level collaboration strategies, and allocating appropriate LLM backbones to individual agents. Experiments on multiple benchmarks demonstrate the effectiveness of SC-MAS. In particular, SC-MAS improves accuracy by 3.35% on MMLU while reducing inference cost by 15.38%, and achieves a 3.53% accuracy gain with a 12.13% cost reduction on MBPP. These results validate the feasibility of SC-MAS and highlight the effectiveness of heterogeneous collaboration in multi-agent systems.

</details>
