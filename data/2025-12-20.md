<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying](https://arxiv.org/abs/2512.15776)
*Shaun Baek,Sam Liu,Joseph Ukpong*

Main category: cs.AI

TL;DR: LLMs struggle with symbol grounding in asymmetric environments due to Privileged Information Bias, causing collaborative failures; active querying proves more effective than standard instruction.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of symbol grounding in LLMs when information is asymmetrically distributed, focusing on the Privileged Information Bias where a knowledgeable agent fails to guide a sensor-limited partner due to lack of Theory of Mind.

Method: Propose an Asymmetric Assistive Reasoning framework in AI2-THOR to quantify the bias, using experiments with Leader and Follower agents to measure success rates and test Push-based vs. Pull-based communication protocols.

Result: Experiments show a significant Success Gap: Leader perceives target in 35.0% of episodes, but team succeeds only 17.0% of the time, with Pull-based protocol (active querying) being more robust, doubling clarification requests in successful episodes.

Conclusion: Active uncertainty reduction through querying is crucial for mitigating communicative grounding errors and enabling safe human-AI and robot-robot collaboration.

Abstract: Large Language Models (LLMs) act as powerful reasoning engines but struggle with "symbol grounding" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or "Curse of Knowledge"), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant "Success Gap": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a "Pull-based" protocol (active querying) is significantly more robust than standard "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.

</details>


### [2] [Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments](https://arxiv.org/abs/2512.15736)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: Anubuddhi is an AI system that designs and simulates quantum optics experiments from natural language prompts using semantic retrieval and physics simulation with convergent refinement. It successfully handled 13 diverse experiments with high design-simulation alignment.


<details>
  <summary>Details</summary>
Motivation: To democratize computational experiment design in quantum optics by eliminating the need for specialized programming knowledge, making advanced quantum optics experiments accessible for research and education.

Method: Multi-agent AI system using semantic retrieval to arrange optical components from a three-tier toolbox, validated through physics simulation (QuTiP and FreeSim) with convergent refinement. Combines intent routing, knowledge-augmented generation, and dual-mode validation.

Result: Achieved 8-9/10 design-simulation alignment scores across 13 experiments. Free-form simulation outperformed constrained frameworks for 11/13 experiments. System produces structurally correct designs though numerical accuracy requires expert verification.

Conclusion: Anubuddhi successfully democratizes quantum optics experiment design, producing strong initial designs that users can iteratively refine. Critical distinction between structural correctness and quantitative accuracy highlights importance of expert review for numerical predictions.

Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.

</details>


### [3] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: The abstract discusses limitations of current MLLMs in multi-speaker multimodal reasoning and introduces AMUSE benchmark and RAFT framework to address these challenges.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with agentic reasoning in multi-speaker audio-visual scenarios that require tracking speakers, maintaining roles, and grounding events over time.

Method: Introduced AMUSE benchmark with three evaluation modes and six task families, plus RAFT framework combining reward optimization with multimodal self-evaluation and selective parameter adaptation.

Result: Current models show weak multi-speaker reasoning across all evaluation modes. RAFT framework achieves up to 39.52% relative improvement in accuracy on the benchmark.

Conclusion: AMUSE and RAFT provide a practical platform for examining and improving agentic reasoning capabilities in multimodal models.

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [4] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: The paper introduces the Principle of Proportional Duty (PPD), showing ethical responsibility scales with uncertainty. It formalizes how Action Duty converts to Repair Duty as uncertainty increases, validated through simulations and applications across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Traditional ethical frameworks inadequately handle decision-making under uncertainty, treating it as a constraint rather than a dynamic factor affecting moral duty.

Method: The PPD framework models duty transformation using an equation (D_total = K[(1-HI) + HI * g(C_signal)]) and employs Monte Carlo simulations with a humility coefficient to analyze duty allocation.

Result: Simulations show systems with humility coefficients (lambda > 0) achieve more stable duty allocations, reducing overconfident decisions. The framework is applied to clinical ethics, law, governance, and AI, demonstrating cross-disciplinary validity.

Conclusion: PPD provides a mathematically tractable approach to moral responsibility, balancing epistemic confidence and risk, which can stabilize complex systems and inform auditable AI development.

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [5] [Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions](https://arxiv.org/abs/2512.15743)
*David Noever*

Main category: cs.AI

TL;DR: A framework for generating physically realizable assembly instructions from natural language using discrete parts vocabulary and LDraw representation.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between semantic design intent and manufacturable output by addressing limitations of previous methods like pixel-based diffusion or CAD models that fail to support complex assembly instructions.

Method: Uses LDraw as text-rich intermediate representation with large language models guided by tools to produce valid construction sequences. Introduces Python library for programmatic model generation.

Result: Demonstrated capability to produce buildable outputs for complex designs (satellites, aircraft, architecture) with over 3000 assembly parts, showing scalability and fidelity.

Conclusion: The "bag of bricks" method functions as a physical API that bridges natural language specifications to material reality, opening new design options for manufacturing and engineering prototyping.

Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction](https://arxiv.org/abs/2512.15751)
*Wei Guan,Jian Cao,Jinyu Cai,Qiqi Cai,Jianqi Gao,See-Kiong Ng*

Main category: cs.LG

TL;DR: GLOW is a unified framework that combines graph neural networks and large language models to predict Agentic Workflows performance, overcoming limitations of existing methods by capturing both topological dependencies and semantic logic.


<details>
  <summary>Details</summary>
Motivation: Current AW performance prediction methods cannot simultaneously capture the complex topological dependencies and deep semantic logic in Agentic Workflows, creating a scalability bottleneck due to expensive execution-based evaluation.

Method: GLOW integrates GNNs for graph-structure modeling with a graph-oriented LLM (instruction-tuned on graph tasks) to extract topologically aware semantic features, fused with GNN structural representations, enhanced by contrastive alignment for latent space refinement.

Result: Extensive experiments on FLORA-Bench demonstrate GLOW surpasses state-of-the-art baselines in both prediction accuracy and ranking utility.

Conclusion: GLOW effectively addresses the scalability challenge in AW generation by providing a superior performance prediction framework that jointly leverages structural and semantic information.

Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.

</details>


### [7] [DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression](https://arxiv.org/abs/2512.15721)
*Sveinung Myhre*

Main category: cs.LG

TL;DR: DiscoverDCP is a framework that uses symbolic regression within DCP rules to automatically generate globally convex models, avoiding post-hoc convexity checks and offering more flexible, accurate, and interpretable expressions for control and optimization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational intractability of verifying convexity in discovered models, as traditional methods rely on post-hoc checks that are slow and may fail. It seeks to facilitate system identification with convex models suitable for safety-critical applications, moving beyond rigid forms like quadratic functions.

Method: Develop DiscoverDCP, integrating symbolic regression with DCP composition rules. This ensures all candidate model expressions naturally adhere to convexity rules during the discovery process, eliminating the need for separate verification. It utilizes a data-driven approach to generate expressions that are both convex and flexible in form.

Result: DiscoverDCP successfully produces globally convex expressions by construction, as validated through testing. It discovers convex surrogates with more relaxed and accurate functional forms than traditional fixed-parameter convex models (e.g., quadratics). The models are interpretable, verifiable, and adaptable to various tasks.

Conclusion: DiscoverDCP offers an efficient, scalable method for convex system identification, circumventing post-hoc convexity verification. It enables the creation of trustworthy models for safety-critical control and optimization, with potential to advance automation in data-driven convex modeling.

Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.

</details>


### [8] [Hybrid Quantum-Classical Ensemble Learning for S\&P 500 Directional Prediction](https://arxiv.org/abs/2512.15738)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: Hybrid ensemble combining quantum sentiment analysis, Decision Transformer, and strategic model selection achieves 60.14% directional accuracy on S&P 500 prediction, a 3.10% improvement over individual models.


<details>
  <summary>Details</summary>
Motivation: Address limitations in financial market prediction where most models struggle to exceed 55-57% accuracy due to high noise, non-stationarity, and market efficiency.

Method: Hybrid ensemble framework with architecture diversity (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression), 4-qubit variational quantum circuit for sentiment analysis, and smart filtering of weak predictors (accuracy <52%).

Result: 60.14% directional accuracy on S&P 500 (2020-2023 data), statistically significant improvement (McNemar's test p<0.05), preliminary backtesting shows Sharpe ratio of 1.2 vs buy-and-hold's 0.8 with confidence-based filtering.

Conclusion: The framework demonstrates practical trading potential, showing that architecture diversity dominates dataset diversity and quantum-enhanced sentiment provides meaningful gains.

Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.

</details>


### [9] [NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16408)
*Ruifeng Xu,Liang He*

Main category: cs.LG

TL;DR: Proposes Nested Dual-Agent Reinforcement Learning (NDRL) to optimize irrigation and nitrogen fertilization, achieving higher crop yield and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex water-nitrogen optimization and delayed feedback from mild stress signals, leading to poor yield results and low resource efficiency.

Method: Uses a parent agent for macro-action selection based on yield benefits and a child agent with quantified stress factors for daily strategy optimization, validated with DSSAT simulations.

Result: Simulated yield increased by 4.7% in both years; irrigation water productivity rose by 5.6% and 5.1%; nitrogen partial factor productivity improved by 6.3% and 1.0%.

Conclusion: NDRL effectively addresses complexity and precision issues in agricultural resource management, supporting sustainable development.

Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.

</details>


### [10] [SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference](https://arxiv.org/abs/2512.15742)
*Jeff Smith*

Main category: cs.LG

TL;DR: SHARe-KAN solves memory issues in Vision KANs using vector quantization and achieves 88× memory reduction while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: KANs face a memory wall due to parameter-intensive learned basis functions, making deployment difficult in memory-constrained environments. Traditional pruning fails because of KANs' holographic topology.

Method: SHARe-KAN framework uses Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving dense topology, combined with LUTHAM compiler for hardware-aware memory planning.

Result: Achieves 88× runtime memory reduction (1.13 GB to 12.91 MB), matches uncompressed baseline accuracy on PASCAL VOC, and shows >90% L2 cache residency on NVIDIA Ampere.

Conclusion: SHARe-KAN effectively decouples KAN workloads from DRAM constraints, enabling efficient deployment in memory-limited settings.

Abstract: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\times$ runtime memory reduction (1.13 GB $\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.

</details>


### [11] [Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game](https://arxiv.org/abs/2512.16626)
*Barna Pásztor,Thomas Kleine Buening,Andreas Krause*

Main category: cs.LG

TL;DR: Introduces Stackelberg Learning from Human Feedback (SLHF), a sequential game framework for preference optimization where a Leader commits to actions and a Follower responds, enabling inference-time refinement and showing strong alignment across model sizes.


<details>
  <summary>Details</summary>
Motivation: Current methods like RLHF (scalar rewards) and NLHF (simultaneous equilibrium) have limitations in capturing rich preference structures. Need for a framework that leverages sequential asymmetry for better preference optimization.

Method: Frames alignment as a sequential-move game between Leader (commits action) and Follower (responds conditionally), decomposing optimization into Follower refinement and Leader adversary optimization. Uses iterative sampling for inference-time improvements.

Result: SLHF achieves strong alignment on diverse preference datasets, scales from 0.5B to 8B parameters, and enables transferable inference-time refinements across model families without additional fine-tuning.

Conclusion: SLHF provides a robust framework with advantages in consistency, data sensitivity, and handling intransitive preferences, outperforming RLHF and NLHF in alignment tasks.

Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

</details>


### [12] [How Do Graph Signals Affect Recommendation: Unveiling the Mystery of Low and High-Frequency Graph Signals](https://arxiv.org/abs/2512.15744)
*Feng Liu,Hao Cang,Huanhuan Yuan,Jiaqing Fan,Yongjing Hao,Fuzhen Zhuang,Guanfeng Liu,Pengpeng Zhao*

Main category: cs.LG

TL;DR: This paper shows that both low-frequency and high-frequency graph signals are equally important for recommendation tasks, proposing a frequency scaler module and space flip method to enhance GNN performance.


<details>
  <summary>Details</summary>
Motivation: There is unclear understanding about the roles of low-frequency vs high-frequency graph signals in recommendation systems, despite spectral GNNs' success being attributed to low-pass filtering.

Method: Theoretical proof of equivalent effects of both frequency signals, introduction of a plug-and-play frequency signal scaler module, and development of a space flip method to restore graph embedding expressive power.

Result: Demonstrated that either frequency signal alone is sufficient for recommendations, with experimental validation across four public datasets showing method effectiveness.

Conclusion: Both low-frequency and high-frequency graph signals play equivalent roles in recommendation systems, and the proposed methods successfully enhance GNN performance by properly leveraging these signals.

Abstract: Spectral graph neural networks (GNNs) are highly effective in modeling graph signals, with their success in recommendation often attributed to low-pass filtering. However, recent studies highlight the importance of high-frequency signals. The role of low-frequency and high-frequency graph signals in recommendation remains unclear. This paper aims to bridge this gap by investigating the influence of graph signals on recommendation performance. We theoretically prove that the effects of low-frequency and high-frequency graph signals are equivalent in recommendation tasks, as both contribute by smoothing the similarities between user-item pairs. To leverage this insight, we propose a frequency signal scaler, a plug-and-play module that adjusts the graph signal filter function to fine-tune the smoothness between user-item pairs, making it compatible with any GNN model. Additionally, we identify and prove that graph embedding-based methods cannot fully capture the characteristics of graph signals. To address this limitation, a space flip method is introduced to restore the expressive power of graph embeddings. Remarkably, we demonstrate that either low-frequency or high-frequency graph signals alone are sufficient for effective recommendations. Extensive experiments on four public datasets validate the effectiveness of our proposed methods. Code is avaliable at https://github.com/mojosey/SimGCF.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [13] [Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services](https://arxiv.org/abs/2512.16167)
*Shiduo Yang,Jiye Wang,Jiayu Qin,Jianbin Li,Yu Wang,Yuanhe Zhao,Kenan Guo*

Main category: cs.MA

TL;DR: Ev-Trust: An evolutionary game theory-based trust mechanism for LLM-driven multi-agent systems that establishes trust equilibria through dynamic feedback, reducing malicious behavior and improving collective performance.


<details>
  <summary>Details</summary>
Motivation: The shift to agent-centric web paradigms using LLMs creates vulnerability to deception, fraud, and misinformation in decentralized multi-agent systems, requiring robust trust mechanisms.

Method: Proposes Ev-Trust mechanism integrating direct trust, indirect trust, and expected revenue into dynamic feedback structure within a 'Request-Response-Payment-Evaluation' framework, using evolutionary game theory and replicator dynamics.

Result: Theoretical proof of stable local equilibria existence; experimental results show effective trust modeling, reduced malicious strategies, and increased collective revenue in LLM-driven service interactions.

Conclusion: Ev-Trust provides a novel evolutionary perspective for trust modeling in agentic service web scenarios, enabling adaptive strategy adjustment and natural exclusion of malicious participants.

Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.

</details>


### [14] [Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents](https://arxiv.org/abs/2512.16614)
*Giulia Boato,Andrea Montibeller,Edward Delp,Luisa Verdoliva,Daniele Miorandi*

Main category: cs.MA

TL;DR: Proposal of AI forensic agents as reliable orchestrators for multimedia forensics, addressing pitfalls in current solutions with a unified framework.


<details>
  <summary>Details</summary>
Motivation: AI is reshaping multimedia forensics, but current solutions have pitfalls in authenticity verification, necessitating improved approaches.

Method: Introduce AI forensic agents that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments.

Result: A unified framework is proposed to improve the authenticity verification process in multimedia forensics.

Conclusion: The framework aims to address current challenges and enhance reliability in AI-driven multimedia forensics through orchestrated agents.

Abstract: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.

</details>
