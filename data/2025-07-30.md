<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 70]
- [cs.LG](#cs.LG) [Total: 66]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration](https://arxiv.org/abs/2507.21067)
*Jan Kapusta*

Main category: cs.AI

TL;DR: The paper introduces Symbiotic Epistemology and SynLang to foster transparent human-AI collaboration, validated through empirical dialogues.


<details>
  <summary>Details</summary>
Motivation: Current AI systems lack transparency, hindering human oversight and collaboration. Post-hoc explanations fail to enable genuine partnership.

Method: Proposes Symbiotic Epistemology and SynLang, a formal protocol with TRACE and TRACE_FE mechanisms, integrating confidence quantification and declarative control.

Result: Empirical validation shows AI's adaptation to structured reasoning and successful metacognitive intervention, enhancing human-AI collaboration.

Conclusion: SynLang and symbiotic epistemology enable transparent, ethical AI collaboration, preserving human agency and accountability.

Abstract: Current AI systems rely on opaque reasoning processes that hinder human
oversight and collaborative potential. Conventional explainable AI approaches
offer post-hoc justifications and often fail to establish genuine symbiotic
collaboration. In this paper, the Symbiotic Epistemology is presented as a
philosophical foundation for human-AI cognitive partnerships. Unlike frameworks
that treat AI as a mere tool or replacement, symbiotic epistemology positions
AI as a reasoning partner, fostering calibrated trust by aligning human
confidence with AI reliability through explicit reasoning patterns and
confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as
a formal protocol for transparent human-AI collaboration. The framework is
empirically validated through actual human-AI dialogues demonstrating AI's
adaptation to structured reasoning protocols and successful metacognitive
intervention. The protocol defines two complementary mechanisms: TRACE for
high-level reasoning patterns and TRACE_FE for detailed factor explanations. It
also integrates confidence quantification, declarative control over AI
behavior, and context inheritance for multi-agent coordination. By structuring
communication and embedding confidence-calibrated transparency, SynLang,
together with symbiotic epistemology, enables AI systems that enhance human
intelligence, preserve human agency, and uphold ethical accountability in
collaborative decision-making. Through dual-level transparency, beginning with
high-level reasoning patterns and progressing to granular explanations, the
protocol facilitates rapid comprehension and supports thorough verification of
AI decision-making.

</details>


### [2] [Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism](https://arxiv.org/abs/2507.21098)
*Marta Sidorkiewicz,Karolina Królikowska,Berenika Dyczek,Edyta Pijet-Migon,Anna Dubel*

Main category: cs.AI

TL;DR: AI enhances sustainability and efficiency in the wine industry through intelligent management in viticulture, production, and tourism.


<details>
  <summary>Details</summary>
Motivation: The wine industry faces environmental and economic challenges; AI offers solutions for resource optimization and customer engagement.

Method: Questionnaire survey among Polish winemakers and analysis of AI methods like predictive analytics, machine learning, and computer vision.

Result: AI improves vineyard monitoring, irrigation, production, and enotourism experiences, supporting sustainability.

Conclusion: AI positively impacts economic, environmental, and social sustainability in the wine industry.

Abstract: This study examines the role of Artificial Intelligence (AI) in enhancing
sustainability and efficiency within the wine industry. It focuses on AI-driven
intelligent management in viticulture, wine production, and enotourism. As the
wine industry faces environmental and economic challenges, AI offers innovative
solutions to optimize resource use, reduce environmental impact, and improve
customer engagement. Understanding AI's potential in sustainable winemaking is
crucial for fostering responsible and efficient industry practices. The
research is based on a questionnaire survey conducted among Polish winemakers,
combined with a comprehensive analysis of AI methods applicable to viticulture,
production, and tourism. Key AI technologies, including predictive analytics,
machine learning, and computer vision, are explored. The findings indicate that
AI enhances vineyard monitoring, optimizes irrigation, and streamlines
production processes, contributing to sustainable resource management. In
enotourism, AI-powered chatbots, recommendation systems, and virtual tastings
personalize consumer experiences. The study highlights AI's impact on economic,
environmental, and social sustainability, supporting local wine enterprises and
cultural heritage. Keywords: Artificial Intelligence, Sustainable Development,
AI-Driven Management, Viticulture, Wine Production, Enotourism, Wine
Enterprises, Local Communities

</details>


### [3] [Leveraging Generative AI to Enhance Synthea Module Development](https://arxiv.org/abs/2507.21123)
*Mark A. Kramer,Aanchal Mathur,Caroline E. Adams,Jason A. Walonoski*

Main category: cs.AI

TL;DR: The paper investigates using LLMs to streamline Synthea disease module development, highlighting benefits like reduced time and expertise, and introduces progressive refinement for iterative improvement.


<details>
  <summary>Details</summary>
Motivation: To enhance synthetic health data quality and diversity by leveraging LLMs for faster, less expert-dependent module development.

Method: LLMs assist in generating disease profiles, modules, evaluating existing modules, and refining them through progressive refinement.

Result: LLMs show promise in improving module development but require human oversight and validation due to potential inaccuracies.

Conclusion: Future research is needed to fully harness LLMs for synthetic data creation, addressing challenges like oversight and validation.

Abstract: This paper explores the use of large language models (LLMs) to assist in the
development of new disease modules for Synthea, an open-source synthetic health
data generator. Incorporating LLMs into the module development process has the
potential to reduce development time, reduce required expertise, expand model
diversity, and improve the overall quality of synthetic patient data. We
demonstrate four ways that LLMs can support Synthea module creation: generating
a disease profile, generating a disease module from a disease profile,
evaluating an existing Synthea module, and refining an existing module. We
introduce the concept of progressive refinement, which involves iteratively
evaluating the LLM-generated module by checking its syntactic correctness and
clinical accuracy, and then using that information to modify the module. While
the use of LLMs in this context shows promise, we also acknowledge the
challenges and limitations, such as the need for human oversight, the
importance of rigorous testing and validation, and the potential for
inaccuracies in LLM-generated content. The paper concludes with recommendations
for future research and development to fully realize the potential of LLM-aided
synthetic data creation.

</details>


### [4] [Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity](https://arxiv.org/abs/2507.21159)
*Zhihao Peng,Liuxin Bao,Shengyuan Liu,Yixuan Yuan*

Main category: cs.AI

TL;DR: The paper proposes an adaptive cluster collaborativeness methodology for LLMs in healthcare, using self-diversity and cross-consistency mechanisms to improve medical decision support without predefined clusters.


<details>
  <summary>Details</summary>
Motivation: Current LLM collaborativeness lacks explicit selection rules and relies on predefined clusters, leading to underperformance in medical scenarios.

Method: Introduces self-diversity (fuzzy matching of pairwise outputs) and cross-consistency (masking inconsistent LLMs) mechanisms to enhance LLM cluster performance.

Result: Achieves higher accuracy (e.g., 65.47% vs. GPT-4's 56.12% on Obstetrics and Gynecology) on medical datasets like NEJMQA and MMLU-Pro-health.

Conclusion: The proposed method effectively improves LLM collaborativeness for medical decision support, outperforming existing approaches.

Abstract: The collaborativeness of large language models (LLMs) has proven effective in
natural language processing systems, holding considerable promise for
healthcare development. However, it lacks explicit component selection rules,
necessitating human intervention or clinical-specific validation. Moreover,
existing architectures heavily rely on a predefined LLM cluster, where partial
LLMs underperform in medical decision support scenarios, invalidating the
collaborativeness of LLMs. To this end, we propose an adaptive cluster
collaborativeness methodology involving self-diversity and cross-consistency
maximization mechanisms to boost LLMs medical decision support capacity. For
the self-diversity, we calculate the fuzzy matching value of pairwise outputs
within an LLM as its self-diversity value, subsequently prioritizing LLMs with
high self-diversity values as cluster components in a training-free manner. For
the cross-consistency, we first measure cross-consistency values between the
LLM with the highest self-diversity value and others, and then gradually mask
out the LLM having the lowest cross-consistency value to eliminate the
potential inconsistent output during the collaborative propagation. Extensive
experiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health,
demonstrate the effectiveness of our method across physician-oriented
specialties. For example, on NEJMQA, our method achieves the accuracy rate up
to the publicly official passing score across all disciplines, especially
achieving ACC of 65.47\% compared to the 56.12\% achieved by GPT-4 on the
Obstetrics and Gynecology discipline.

</details>


### [5] [Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics](https://arxiv.org/abs/2507.21129)
*Jae Wan Shim*

Main category: cs.AI

TL;DR: The paper introduces a task-agnostic method to analyze LLMs' internal processing via a 'Cognitive Profile,' focusing on the Entropy Decay Curve and Information Gain Span (IGS) index.


<details>
  <summary>Details</summary>
Motivation: To move beyond task-specific benchmarks and understand how LLMs process information internally.

Method: Develops a quantitative 'Cognitive Profile' using the Entropy Decay Curve and IGS index to analyze predictive uncertainty across context lengths.

Result: Reveals unique cognitive profiles in LLMs, sensitive to model scale and text complexity.

Conclusion: Offers a principled framework for comparing LLMs' operational dynamics beyond traditional benchmarks.

Abstract: The remarkable capabilities of Large Language Models (LLMs) are now
extensively documented on task-specific benchmarks, yet the internal mechanisms
that produce these results are the subject of intense scientific inquiry. This
paper contributes to this inquiry by moving beyond metrics that measure
\textit{what} models can do, to a methodology that characterizes \textit{how}
they process information. We introduce a novel, task-agnostic approach to probe
these dynamics by creating a quantitative ``Cognitive Profile" for any given
model. This profile is centered on the \textbf{Entropy Decay Curve}, a
visualization that traces how a model's normalized predictive uncertainty
changes as a function of context length. Applying this methodology to several
state-of-the-art LLMs across diverse texts, we uncover unique and consistent
cognitive profiles that are sensitive to both model scale and text complexity.
We also introduce the Information Gain Span (IGS) index to summarize the
desirability of the decay trajectory. This work thus provides a new, principled
lens for analyzing and comparing the intrinsic operational dynamics of
artificial intelligence.

</details>


### [6] [Games Agents Play: Towards Transactional Analysis in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2507.21354)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: Trans-ACT integrates Transactional Analysis into Multi-Agent Systems to create psychologically realistic agents, improving social interaction simulations.


<details>
  <summary>Details</summary>
Motivation: Current MAS frameworks lack cognitive complexity for human-like behavior. Trans-ACT addresses this gap by embedding TA principles.

Method: Trans-ACT incorporates Parent, Adult, and Child ego states into agents, using context-specific memories and life scripts to shape responses.

Result: Agents with TA principles produce deeper, context-aware interactions, demonstrated in the Stupid game scenario.

Conclusion: Trans-ACT enables advanced applications like conflict resolution, education, and social psychology studies.

Abstract: Multi-Agent Systems (MAS) are increasingly used to simulate social
interactions, but most of the frameworks miss the underlying cognitive
complexity of human behavior. In this paper, we introduce Trans-ACT
(Transactional Analysis Cognitive Toolkit), an approach embedding Transactional
Analysis (TA) principles into MAS to generate agents with realistic
psychological dynamics. Trans-ACT integrates the Parent, Adult, and Child ego
states into an agent's cognitive architecture. Each ego state retrieves
context-specific memories and uses them to shape response to new situations.
The final answer is chosen according to the underlying life script of the
agent. Our experimental simulation, which reproduces the Stupid game scenario,
demonstrates that agents grounded in cognitive and TA principles produce deeper
and context-aware interactions. Looking ahead, our research opens a new way for
a variety of applications, including conflict resolution, educational support,
and advanced social psychology studies.

</details>


### [7] [INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems](https://arxiv.org/abs/2507.21130)
*Bintao Tang,Xin Yang,Yuhao Wang,Zixuan Qiu,Zimo Ji,Wenyuan Jiang*

Main category: cs.AI

TL;DR: INTEGRALBENCH is a benchmark for evaluating LLMs on definite integrals, revealing performance gaps and difficulty-accuracy correlations.


<details>
  <summary>Details</summary>
Motivation: To advance automated mathematical reasoning by providing a rigorous evaluation framework for definite integral computation.

Method: INTEGRALBENCH offers symbolic and numerical ground truth solutions with difficulty annotations, evaluating nine state-of-the-art LLMs.

Result: Significant performance gaps and strong correlations between problem difficulty and model accuracy were found.

Conclusion: INTEGRALBENCH establishes baseline metrics for LLM performance on definite integrals, aiding progress in mathematical reasoning.

Abstract: We present INTEGRALBENCH, a focused benchmark designed to evaluate Large
Language Model (LLM) performance on definite integral problems. INTEGRALBENCH
provides both symbolic and numerical ground truth solutions with manual
difficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals
significant performance gaps and strong correlations between problem difficulty
and model accuracy, establishing baseline metrics for this challenging domain.
INTEGRALBENCH aims to advance automated mathematical reasoning by providing a
rigorous evaluation framework specifically tailored for definite integral
computation.

</details>


### [8] ["Teammates, Am I Clear?": Analysing Legible Behaviours in Teams](https://arxiv.org/abs/2507.21631)
*Miguel Faria,Francisco S. Melo,Ana Paiva*

Main category: cs.AI

TL;DR: The paper extends legible decision-making to multi-agent settings, improving team performance in sequential tasks.


<details>
  <summary>Details</summary>
Motivation: Existing works on legible decision-making focus on single-agent interactions, neglecting the benefits in team settings.

Method: Proposes an extension of legible decision-making for multi-agent scenarios and tests it in benchmark environments.

Result: Teams with legible agents outperform those with standard optimal behavior.

Conclusion: Legible decision-making enhances collaboration and performance in multi-agent teams.

Abstract: In this paper we investigate the notion of legibility in sequential
decision-making in the context of teams and teamwork. There have been works
that extend the notion of legibility to sequential decision making, for
deterministic and for stochastic scenarios. However, these works focus on one
agent interacting with one human, foregoing the benefits of having legible
decision making in teams of agents or in team configurations with humans. In
this work we propose an extension of legible decision-making to multi-agent
settings that improves the performance of agents working in collaboration. We
showcase the performance of legible decision making in team scenarios using our
proposed extension in multi-agent benchmark scenarios. We show that a team with
a legible agent is able to outperform a team composed solely of agents with
standard optimal behaviour.

</details>


### [9] [NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback](https://arxiv.org/abs/2507.21131)
*Madhava Gaikwad,Ashwini Ramchandra Doke*

Main category: cs.AI

TL;DR: NPO is an alignment-aware learning framework for human-in-the-loop systems, focusing on measurable alignment loss and meta-alignment for continuous monitoring and adaptation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static or post-hoc alignment approaches by introducing a dynamic, feedback-driven system for alignment in decision-making.

Method: NPO formalizes alignment loss and meta-alignment, using structured feedback (e.g., likes, overrides) in a scalable loop involving scenario scoring, threshold tuning, and policy validation.

Result: Formal convergence results show alignment loss and monitoring fidelity improve additively. Empirical results confirm NPO's effectiveness in hyperscale deployments.

Conclusion: NPO provides a compact, inspectable architecture for continual alignment monitoring, bridging theoretical guarantees with practical reliability in dynamic environments.

Abstract: We present NPO, an alignment-aware learning framework that operationalizes
feedback-driven adaptation in human-in-the-loop decision systems. Unlike prior
approaches that treat alignment as a static or post-hoc property, NPO
introduces a formalization of alignment loss that is measurable, supervisable,
and reducible under structured feedback. In parallel, we propose meta-alignment
as the fidelity of the monitoring process that governs retraining or override
triggers, and show that it is formally reducible to primary alignment via
threshold fidelity. Our implementation spans a scalable operational loop
involving scenario scoring, threshold tuning, policy validation, and structured
feedback ingestion, including "likes", overrides, and abstentions. We provide
formal convergence results under stochastic feedback and show that both
alignment loss and monitoring fidelity converge additively. Empirically, NPO
demonstrates measurable value in hyperscale deployment settings. A
simulation-based artifact and ablation studies further illustrate the
theoretical principles in action. Together, NPO offers a compact, inspectable
architecture for continual alignment monitoring, helping bridge theoretical
alignment guarantees with practical reliability in dynamic environments.

</details>


### [10] [Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses](https://arxiv.org/abs/2507.21132)
*Joshua Adrian Cahyono,Saran Subramanian*

Main category: cs.AI

TL;DR: The paper examines risks of sycophancy and over-confidence in LLMs when providing life advice, testing model robustness, safety, and controllability through experiments.


<details>
  <summary>Details</summary>
Motivation: LLMs lack safeguards for high-stakes advice, risking misguided responses. The study aims to address these failure modes.

Method: Three experiments: (1) multiple-choice evaluation for stability, (2) free-response analysis with safety typology, and (3) mechanistic interpretability to steer behavior.

Result: Some models show sycophancy, but others like o4-mini are robust. Top models achieve safety by asking clarifying questions. Activation steering can control cautiousness.

Conclusion: Nuanced benchmarks are needed to ensure LLMs can be trusted for life-changing decisions, with activation steering offering a new safety alignment path.

Abstract: Large Language Models (LLMs) are increasingly consulted for high-stakes life
advice, yet they lack standard safeguards against providing confident but
misguided responses. This creates risks of sycophancy and over-confidence. This
paper investigates these failure modes through three experiments: (1) a
multiple-choice evaluation to measure model stability against user pressure;
(2) a free-response analysis using a novel safety typology and an LLM Judge;
and (3) a mechanistic interpretability experiment to steer model behavior by
manipulating a "high-stakes" activation vector. Our results show that while
some models exhibit sycophancy, others like o4-mini remain robust.
Top-performing models achieve high safety scores by frequently asking
clarifying questions, a key feature of a safe, inquisitive approach, rather
than issuing prescriptive advice. Furthermore, we demonstrate that a model's
cautiousness can be directly controlled via activation steering, suggesting a
new path for safety alignment. These findings underscore the need for nuanced,
multi-faceted benchmarks to ensure LLMs can be trusted with life-changing
decisions.

</details>


### [11] [Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku Website but not Easy Puzzles on another Sudoku Website?](https://arxiv.org/abs/2507.21137)
*Arman Eisenkolb-Vaithyanathan*

Main category: cs.AI

TL;DR: The paper proposes two new metrics to rate Sudoku difficulty using SAT problem conversion and human solver simulation, achieving strong correlation with website labels and creating a universal rating system.


<details>
  <summary>Details</summary>
Motivation: To understand and standardize Sudoku difficulty ratings across different websites, addressing inconsistencies in labeled difficulty levels.

Method: Two methods: (1) Convert Sudoku to SAT problem for structural complexity metrics, (2) Simulate human solvers using backtracking (Nishio) with strategy counts. Analyzed 1000+ puzzles from 5 websites.

Result: Strong correlation (4/5 websites) between proposed metrics and labeled difficulty. Universal rating system (Easy, Medium, Hard) aligns well with website labels.

Conclusion: The proposed metrics and universal rating system effectively standardize Sudoku difficulty across websites, with potential for aiding early practitioners.

Abstract: In this paper we try to answer the question "What constitutes Sudoku
difficulty rating across different Sudoku websites?" Using two distinct methods
that can both solve every Sudoku puzzle, I propose two new metrics to
characterize Sudoku difficulty. The first method is based on converting a
Sudoku puzzle into its corresponding Satisfiability (SAT) problem. The first
proposed metric is derived from SAT Clause Length Distribution which captures
the structural complexity of a Sudoku puzzle including the number of given
digits and the cells they are in. The second method simulates human Sudoku
solvers by intertwining four popular Sudoku strategies within a backtracking
algorithm called Nishio. The second metric is computed by counting the number
of times Sudoku strategies are applied within the backtracking iterations of a
randomized Nishio. Using these two metrics, I analyze more than a thousand
Sudoku puzzles across five popular websites to characterize every difficulty
level in each website. I evaluate the relationship between the proposed metrics
and website-labeled difficulty levels using Spearman's rank correlation
coefficient, finding strong correlations for 4 out of 5 websites. I construct a
universal rating system using a simple, unsupervised classifier based on the
two proposed metrics. This rating system is capable of classifying both
individual puzzles and entire difficulty levels from the different Sudoku
websites into three categories - Universal Easy, Universal Medium, and
Universal Hard - thereby enabling consistent difficulty mapping across Sudoku
websites. The experimental results show that for 4 out of 5 Sudoku websites,
the universal classification aligns well with website-labeled difficulty
levels. Finally, I present an algorithm that can be used by early Sudoku
practitioners to solve Sudoku puzzles.

</details>


### [12] [The Geometry of Harmfulness in LLMs through Subconcept Probing](https://arxiv.org/abs/2507.21141)
*McNair Shah,Saleena Angeline,Adhitya Rajendra Kumar,Naitik Chheda,Kevin Zhu,Vasu Sharma,Sean O'Brien,Will Cai*

Main category: cs.AI

TL;DR: A framework probes and steers harmful content in LLMs using 55 interpretable directions in activation space, showing low-rank harmfulness subspaces. Steering the dominant direction nearly eliminates harmfulness with minimal utility loss.


<details>
  <summary>Details</summary>
Motivation: To understand and reliably curb harmful behaviors in large language models (LLMs) by probing and steering their internal representations.

Method: Developed a multidimensional framework with 55 linear probes for distinct harmfulness subconcepts, identifying a low-rank harmfulness subspace. Tested ablation and steering in this subspace.

Result: Dominant direction steering nearly eliminates harmfulness with little utility decrease, demonstrating the effectiveness of concept subspaces.

Conclusion: Concept subspaces offer a scalable way to audit and improve LLM behavior, providing practical tools for mitigating harmfulness.

Abstract: Recent advances in large language models (LLMs) have intensified the need to
understand and reliably curb their harmful behaviours. We introduce a
multidimensional framework for probing and steering harmful content in model
internals. For each of 55 distinct harmfulness subconcepts (e.g., racial hate,
employment scams, weapons), we learn a linear probe, yielding 55 interpretable
directions in activation space. Collectively, these directions span a
harmfulness subspace that we show is strikingly low-rank. We then test ablation
of the entire subspace from model internals, as well as steering and ablation
in the subspace's dominant direction. We find that dominant direction steering
allows for near elimination of harmfulness with a low decrease in utility. Our
findings advance the emerging view that concept subspaces provide a scalable
lens on LLM behaviour and offer practical tools for the community to audit and
harden future generations of language models.

</details>


### [13] [Adaptive XAI in High Stakes Environments: Modeling Swift Trust with Multimodal Feedback in Human AI Teams](https://arxiv.org/abs/2507.21158)
*Nishani Fernando,Bahareh Nakisa,Adnan Ahmad,Mohammad Naim Rastgoo*

Main category: cs.AI

TL;DR: Proposes a framework for adaptive explainable AI (XAI) using implicit feedback to enhance swift trust in high-stakes human-AI teams.


<details>
  <summary>Details</summary>
Motivation: Swift trust is critical in high-stakes scenarios like emergency response, but current XAI lacks adaptability and relies on impractical explicit feedback.

Method: Introduces the adaptive explainability trust framework (AXTF), which uses physiological and behavioral signals (e.g., EEG, ECG, eye tracking) to infer user states and personalize explanations.

Result: AXTF dynamically adjusts explanations based on real-time cognitive and emotional states, fostering swift trust in human-AI collaboration.

Conclusion: The framework lays groundwork for adaptive, non-intrusive XAI systems suited for high-pressure, time-sensitive environments.

Abstract: Effective human-AI teaming heavily depends on swift trust, particularly in
high-stakes scenarios such as emergency response, where timely and accurate
decision-making is critical. In these time-sensitive and cognitively demanding
settings, adaptive explainability is essential for fostering trust between
human operators and AI systems. However, existing explainable AI (XAI)
approaches typically offer uniform explanations and rely heavily on explicit
feedback mechanisms, which are often impractical in such high-pressure
scenarios. To address this gap, we propose a conceptual framework for adaptive
XAI that operates non-intrusively by responding to users' real-time cognitive
and emotional states through implicit feedback, thereby enhancing swift trust
in high-stakes environments. The proposed adaptive explainability trust
framework (AXTF) leverages physiological and behavioral signals, such as EEG,
ECG, and eye tracking, to infer user states and support explanation adaptation.
At its core is a multi-objective, personalized trust estimation model that maps
workload, stress, and emotion to dynamic trust estimates. These estimates guide
the modulation of explanation features enabling responsive and personalized
support that promotes swift trust in human-AI collaboration. This conceptual
framework establishes a foundation for developing adaptive, non-intrusive XAI
systems tailored to the rigorous demands of high-pressure, time-sensitive
environments.

</details>


### [14] [Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems](https://arxiv.org/abs/2507.21162)
*Xu Yang,Chenhui Lin,Yue Yang,Qi Wang,Haotian Liu,Haizhou Hua,Wenchuan Wu*

Main category: cs.AI

TL;DR: The paper proposes an LLM-powered automated approach for ADN dispatch, decomposing problems into stages and using multi-LLM coordination for modeling and optimization, validated by test cases.


<details>
  <summary>Details</summary>
Motivation: The integration of distributed energy resources into ADNs requires expert knowledge, which is costly and time-intensive. The paper aims to bridge this gap with an automated, user-friendly solution.

Method: Decomposes ADN dispatch into stages, designs a multi-LLM framework (Information Extractor, Problem Formulator, Code Programmer), and refines each LLM agent for accuracy.

Result: The approach enables ADN operators to derive dispatch strategies via natural language queries, validated by comprehensive comparisons and demonstrations.

Conclusion: The proposed LLM-powered method effectively addresses the knowledge gap in ADN dispatch, offering an intelligent, flexible, and efficient solution.

Abstract: The increasing penetration of distributed energy resources into active
distribution networks (ADNs) has made effective ADN dispatch imperative.
However, the numerous newly-integrated ADN operators, such as distribution
system aggregators, virtual power plant managers, and end prosumers, often lack
specialized expertise in power system operation, modeling, optimization, and
programming. This knowledge gap renders reliance on human experts both costly
and time-intensive. To address this challenge and enable intelligent, flexible
ADN dispatch, this paper proposes a large language model (LLM) powered
automated modeling and optimization approach. First, the ADN dispatch problems
are decomposed into sequential stages, and a multi-LLM coordination
architecture is designed. This framework comprises an Information Extractor, a
Problem Formulator, and a Code Programmer, tasked with information retrieval,
optimization problem formulation, and code implementation, respectively.
Afterwards, tailored refinement techniques are developed for each LLM agent,
greatly improving the accuracy and reliability of generated content. The
proposed approach features a user-centric interface that enables ADN operators
to derive dispatch strategies via simple natural language queries, eliminating
technical barriers and increasing efficiency. Comprehensive comparisons and
end-to-end demonstrations on various test cases validate the effectiveness of
the proposed architecture and methods.

</details>


### [15] [An ontological analysis of risk in Basic Formal Ontology](https://arxiv.org/abs/2507.21171)
*Federico Donato,Adrien Barton*

Main category: cs.AI

TL;DR: The paper characterizes risk using BFO, arguing it's a subclass of BFO:Role, not BFO:Disposition, and provides an example-based analysis to define sufficient conditions for risk.


<details>
  <summary>Details</summary>
Motivation: To clarify the ontological classification of risk within BFO, distinguishing it from dispositions.

Method: Uses BFO categories, applies modeling to an example of risk, and generalizes the analysis to define sufficient conditions.

Result: Risk is classified as a subclass of BFO:Role, with sufficient conditions for being a risk identified.

Conclusion: The paper provides a clear ontological framework for risk, suggesting future work on necessary conditions.

Abstract: The paper explores the nature of risk, providing a characterization using the
categories of the Basic Formal Ontology (BFO). It argues that the category Risk
is a subclass of BFO:Role, contrasting it with a similar view classifying Risk
as a subclass of BFO:Disposition. This modeling choice is applied on one
example of risk, which represents objects, processes (both physical and mental)
and their interrelations, then generalizing from the instances in the example
to obtain an overall analysis of risk, making explicit what are the sufficient
conditions for being a risk. Plausible necessary conditions are also mentioned
for future work. Index Terms: ontology, risk, BFO, role, disposition

</details>


### [16] [Ontological Foundations of State Sovereignty](https://arxiv.org/abs/2507.21172)
*John Beverley,Danielle Limbaugh*

Main category: cs.AI

TL;DR: A primer on state sovereignty, its claims, and a strategy for handling vague or contradictory data about sovereign states, aiming to support ontology work in international affairs.


<details>
  <summary>Details</summary>
Motivation: To clarify the nature of state sovereignty and its claims, and to address challenges in identifying sovereign states for applied ontology in international affairs.

Method: Presents a strategy for dealing with vague or contradictory data about sovereign states.

Result: Provides foundational insights for ontology work in international affairs by addressing sovereignty ambiguities.

Conclusion: The paper lays groundwork for future applied research in ontology related to state sovereignty and international affairs.

Abstract: This short paper is a primer on the nature of state sovereignty and the
importance of claims about it. It also aims to reveal (merely reveal) a
strategy for working with vague or contradictory data about which states, in
fact, are sovereign. These goals together are intended to set the stage for
applied work in ontology about international affairs.

</details>


### [17] [Tell Me You're Biased Without Telling Me You're Biased -- Toward Revealing Implicit Biases in Medical LLMs](https://arxiv.org/abs/2507.21176)
*Farzana Islam Adiba,Rahmatollah Beheshti*

Main category: cs.AI

TL;DR: A novel framework combining knowledge graphs and auxiliary LLMs to detect and mitigate biases in medical LLMs, outperforming baselines in revealing complex bias patterns.


<details>
  <summary>Details</summary>
Motivation: To address the biased and unfair patterns in LLMs used in medical applications, ensuring their reliability for clinical decision-making.

Method: Integrates knowledge graphs with adversarial perturbation techniques and multi-hop KG characterization to systematically evaluate LLMs.

Result: Demonstrates superior ability and scalability in identifying complex biases across datasets, LLMs, and bias types.

Conclusion: The framework effectively reveals and mitigates biases in medical LLMs, enhancing their clinical applicability.

Abstract: Large language models (LLMs) that are used in medical applications are known
to show biased and unfair patterns. Prior to adopting these in clinical
decision-making applications, it is crucial to identify these bias patterns to
enable effective mitigation of their impact. In this study, we present a novel
framework combining knowledge graphs (KGs) with auxiliary LLMs to
systematically reveal complex bias patterns in medical LLMs. Specifically, the
proposed approach integrates adversarial perturbation techniques to identify
subtle bias patterns. The approach adopts a customized multi-hop
characterization of KGs to enhance the systematic evaluation of arbitrary LLMs.
Through a series of comprehensive experiments (on three datasets, six LLMs, and
five bias types), we show that our proposed framework has noticeably greater
ability and scalability to reveal complex biased patterns of LLMs compared to
other baselines.

</details>


### [18] [Agentic Web: Weaving the Next Web with AI Agents](https://arxiv.org/abs/2507.21206)
*Yingxuan Yang,Mulei Ma,Yuxuan Huang,Huacan Chai,Chenyu Gong,Haoran Geng,Yuanjian Zhou,Ying Wen,Meng Fang,Muhao Chen,Shangding Gu,Ming Jin,Costas Spanos,Yang Yang,Pieter Abbeel,Dawn Song,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: The paper introduces the Agentic Web, a new internet phase driven by AI agents using LLMs, enabling autonomous, goal-driven interactions. It proposes a framework with three dimensions (intelligence, interaction, economics) to understand and build this system, addressing challenges and societal impacts.


<details>
  <summary>Details</summary>
Motivation: The shift from human-driven to machine-to-machine interaction aims to automate routine tasks, enhance web interactivity, and delegate user intent, improving digital experiences.

Method: The paper presents a structured framework with three key dimensions (intelligence, interaction, economics) and analyzes architectural and infrastructural challenges like communication protocols and orchestration strategies.

Result: The framework enables AI agent capabilities (retrieval, recommendation, planning, collaboration) and identifies applications, risks, and governance issues of agentic systems.

Conclusion: The paper outlines research directions for developing secure, intelligent ecosystems balancing human intent and autonomous agent behavior, with ongoing updates at a provided GitHub link.

Abstract: The emergence of AI agents powered by large language models (LLMs) marks a
pivotal shift toward the Agentic Web, a new phase of the internet defined by
autonomous, goal-driven interactions. In this paradigm, agents interact
directly with one another to plan, coordinate, and execute complex tasks on
behalf of users. This transition from human-driven to machine-to-machine
interaction allows intent to be delegated, relieving users from routine digital
operations and enabling a more interactive, automated web experience. In this
paper, we present a structured framework for understanding and building the
Agentic Web. We trace its evolution from the PC and Mobile Web eras and
identify the core technological foundations that support this shift. Central to
our framework is a conceptual model consisting of three key dimensions:
intelligence, interaction, and economics. These dimensions collectively enable
the capabilities of AI agents, such as retrieval, recommendation, planning, and
collaboration. We analyze the architectural and infrastructural challenges
involved in creating scalable agentic systems, including communication
protocols, orchestration strategies, and emerging paradigms such as the Agent
Attention Economy. We conclude by discussing the potential applications,
societal risks, and governance issues posed by agentic systems, and outline
research directions for developing open, secure, and intelligent ecosystems
shaped by both human intent and autonomous agent behavior. A continuously
updated collection of relevant studies for agentic web is available at:
https://github.com/SafeRL-Lab/agentic-web.

</details>


### [19] [CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting](https://arxiv.org/abs/2507.21257)
*David Maria Schmidt,Raoul Schubert,Philipp Cimiano*

Main category: cs.AI

TL;DR: The paper investigates the compositional interpretation abilities of large language models (LLMs) in mapping questions to SPARQL queries, revealing their limitations despite their general language capabilities.


<details>
  <summary>Details</summary>
Motivation: To assess how systematic LLMs are in interpreting complex questions compositionally, given their success in simpler tasks.

Method: Created three controlled datasets of varying complexity based on DBpedia graph patterns, using Lemon lexica for verbalization. Tested LLMs with different sizes, prompts, few-shot techniques, and fine-tuning.

Result: Performance (macro $F_1$) degraded from 0.45 to 0.09 with increasing complexity. Even with all necessary input, scores did not exceed 0.57 for the simplest dataset.

Conclusion: LLMs struggle with systematic and compositional interpretation of questions into SPARQL queries.

Abstract: Language interpretation is a compositional process, in which the meaning of
more complex linguistic structures is inferred from the meaning of their parts.
Large language models possess remarkable language interpretation capabilities
and have been successfully applied to interpret questions by mapping them to
SPARQL queries. An open question is how systematic this interpretation process
is. Toward this question, in this paper, we propose a benchmark for
investigating to what extent the abilities of LLMs to interpret questions are
actually compositional. For this, we generate three datasets of varying
difficulty based on graph patterns in DBpedia, relying on Lemon lexica for
verbalization. Our datasets are created in a very controlled fashion in order
to test the ability of LLMs to interpret structurally complex questions, given
that they have seen the atomic building blocks. This allows us to evaluate to
what degree LLMs are able to interpret complex questions for which they
"understand" the atomic parts. We conduct experiments with models of different
sizes using both various prompt and few-shot optimization techniques as well as
fine-tuning. Our results show that performance in terms of macro $F_1$ degrades
from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the
samples optimized on. Even when all necessary information was provided to the
model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of
lowest complexity. We thus conclude that LLMs struggle to systematically and
compositionally interpret questions and map them into SPARQL queries.

</details>


### [20] [LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems](https://arxiv.org/abs/2507.21276)
*Yufei Li,Zexin Li,Yinglun Zhu,Cong Liu*

Main category: cs.AI

TL;DR: LeMix is a system for co-locating LLM serving and training workloads, improving efficiency and responsiveness over traditional separate setups.


<details>
  <summary>Details</summary>
Motivation: Inefficiencies in GPU usage and delayed adaptation due to separate serving and training phases in LLM deployment.

Method: Integrates offline profiling, execution prediction, and runtime scheduling to dynamically allocate resources.

Result: Improves throughput by up to 3.53x, reduces inference loss by 0.61x, and enhances response time SLO attainment by 2.12x.

Conclusion: LeMix demonstrates the benefits of joint LLM inference and training, enabling more resource-efficient deployment.

Abstract: Modern deployment of large language models (LLMs) frequently involves both
inference serving and continuous retraining to stay aligned with evolving data
and user feedback. Common practices separate these workloads onto distinct
servers in isolated phases, causing substantial inefficiencies (e.g., GPU
idleness) and delayed adaptation to new data in distributed settings. Our
empirical analysis reveals that these inefficiencies stem from dynamic request
arrivals during serving and workload heterogeneity in pipeline-parallel
training. To address these challenges, we propose LeMix, a system for
co-locating and managing concurrent LLM serving and training workloads. LeMix
integrates offline profiling, execution prediction mechanisms, and runtime
scheduling to dynamically adapt resource allocation based on workload
characteristics and system conditions. By understanding task-specific behaviors
and co-execution interference across shared nodes, LeMix improves utilization
and serving quality without compromising serving responsiveness. Our evaluation
shows that LeMix improves throughput by up to 3.53x, reduces inference loss by
up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over
traditional separate setups. To our knowledge, this is the first work to
uncover and exploit the opportunities of joint LLM inference and training,
paving the way for more resource-efficient deployment of LLMs in production
environments.

</details>


### [21] [Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions](https://arxiv.org/abs/2507.21285)
*Harsh Darji,Thibaud Lutellier*

Main category: cs.AI

TL;DR: An LLM-based coding assistant improves code generation accuracy by asking clarification questions for ambiguous prompts, outperforming zero-shot prompting.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with ambiguous developer prompts, leading to incorrect code generation. The goal is to mimic human code review by clarifying unclear queries.

Method: The system includes a query classifier for detecting unclear programming queries and a fine-tuned LLM for generating clarification questions.

Result: The fine-tuned LLM outperforms zero-shot prompting in generating useful questions, and users find the assistant's responses more accurate and helpful.

Conclusion: The proposed coding assistant enhances accuracy by addressing prompt ambiguity through clarification questions, proving more effective than baseline methods.

Abstract: Large Language Models (LLMs) are increasingly used as coding assistants.
However, the ambiguity of the developer's prompt often leads to incorrect code
generation, as current models struggle to infer user intent without extensive
prompt engineering or external context. This work aims to build an LLM-based
coding assistant that mimics the human code review process by asking
clarification questions when faced with ambiguous or under-specified queries.
  Our end-to-end system includes (1) a query classifier trained to detect
unclear programming-related queries and (2) a fine-tuned LLM that generates
clarification questions. Our evaluation shows that the fine-tuned LLM
outperforms standard zero-shot prompting in generating useful clarification
questions. Furthermore, our user study indicates that users find the
clarification questions generated by our model to outperform the baseline,
demonstrating that our coding assistant produces more accurate and helpful code
responses compared to baseline coding assistants.

</details>


### [22] [Structured Relevance Assessment for Robust Retrieval-Augmented Language Models](https://arxiv.org/abs/2507.21287)
*Aryan Raj,Astitva Veer Garg,Anitha D*

Main category: cs.AI

TL;DR: A framework for structured relevance assessment improves Retrieval-Augmented Language Models (RALMs) by reducing factual errors, enhancing document evaluation, and balancing knowledge integration. It uses multi-dimensional scoring, synthetic data, and specialized benchmarking to reduce hallucinations and improve transparency.


<details>
  <summary>Details</summary>
Motivation: RALMs struggle with factual errors, document relevance evaluation, and knowledge integration. The goal is to enhance robustness and reliability in question-answering systems.

Method: The framework employs a multi-dimensional scoring system (semantic matching and source reliability), embedding-based relevance scoring, synthetic training data, and specialized benchmarking. It also includes a knowledge integration mechanism and an "unknown" response protocol.

Result: Preliminary evaluations show reduced hallucination rates and improved transparency in reasoning. The framework advances reliable question-answering in dynamic environments.

Conclusion: The work improves RALM reliability despite challenges in distinguishing credible information and balancing latency. It represents progress toward more robust systems.

Abstract: Retrieval-Augmented Language Models (RALMs) face significant challenges in
reducing factual errors, particularly in document relevance evaluation and
knowledge integration. We introduce a framework for structured relevance
assessment that enhances RALM robustness through improved document evaluation,
balanced intrinsic and external knowledge integration, and effective handling
of unanswerable queries. Our approach employs a multi-dimensional scoring
system that considers both semantic matching and source reliability, utilizing
embedding-based relevance scoring and synthetic training data with
mixed-quality documents. We implement specialized benchmarking on niche topics,
a knowledge integration mechanism, and an "unknown" response protocol for
queries with insufficient knowledge coverage. Preliminary evaluations
demonstrate significant reductions in hallucination rates and improved
transparency in reasoning processes. Our framework advances the development of
more reliable question-answering systems capable of operating effectively in
dynamic environments with variable data quality. While challenges persist in
accurately distinguishing credible information and balancing system latency
with thoroughness, this work represents a meaningful step toward enhancing RALM
reliability.

</details>


### [23] [Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures](https://arxiv.org/abs/2507.21360)
*Nicholas Botti,Flora Haberkorn,Charlotte Hoopes,Shaun Khan*

Main category: cs.AI

TL;DR: The study evaluates AI-assisted annotation tasks, showing significant speed and accuracy improvements, especially with interactive AI use, saving up to 268 hours compared to human-only methods.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of AI retrieval augmented generation (RAG) tools in aiding analysts with complex data annotation tasks.

Method: A within-subjects design with randomized task assignments, testing two AI conditions (naive and interactive) against a human-only baseline on real-world bank disclosure documents.

Result: AI tools accelerated task execution by up to 10x and improved accuracy, with the interactive condition performing best. Extrapolated savings reached 268 hours.

Conclusion: AI tools enhance annotation efficiency and accuracy, with annotator skill in both domain and AI tool usage playing a critical role.

Abstract: We utilize a within-subjects design with randomized task assignments to
understand the effectiveness of using an AI retrieval augmented generation
(RAG) tool to assist analysts with an information extraction and data
annotation task. We replicate an existing, challenging real-world annotation
task with complex multi-part criteria on a set of thousands of pages of public
disclosure documents from global systemically important banks (GSIBs) with
heterogeneous and incomplete information content. We test two treatment
conditions. First, a "naive" AI use condition in which annotators use only the
tool and must accept the first answer they are given. And second, an
"interactive" AI treatment condition where annotators use the tool
interactively, and use their judgement to follow-up with additional information
if necessary. Compared to the human-only baseline, the use of the AI tool
accelerated task execution by up to a factor of 10 and enhanced task accuracy,
particularly in the interactive condition. We find that when extrapolated to
the full task, these methods could save up to 268 hours compared to the
human-only approach. Additionally, our findings suggest that annotator skill,
not just with the subject matter domain, but also with AI tools, is a factor in
both the accuracy and speed of task performance.

</details>


### [24] [Optimizing Multi-Tier Supply Chain Ordering with LNN+XGBoost: Mitigating the Bullwhip Effect](https://arxiv.org/abs/2507.21383)
*Chunan Tong*

Main category: cs.AI

TL;DR: A hybrid LNN and XGBoost model is proposed to optimize supply chain ordering, addressing the bullwhip effect and improving profitability by combining dynamic feature extraction and global optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail in dynamic markets, and emerging ML techniques have limitations. Liquid Neural Networks (LNNs) offer adaptability and efficiency, but their potential in supply chains is underexplored.

Method: A hybrid LNN and XGBoost model is introduced, leveraging LNN's dynamic feature extraction and XGBoost's global optimization for multi-tier supply chain ordering.

Result: The model aims to mitigate the bullwhip effect and enhance cumulative profitability by addressing adaptability and efficiency demands.

Conclusion: The hybrid approach fills a gap in supply chain methodologies, offering an innovative solution for dynamic and efficient management.

Abstract: Supply chain management faces significant challenges, including demand
fluctuations, inventory imbalances, and amplified upstream order variability
due to the bullwhip effect. Traditional methods, such as simple moving
averages, struggle to address dynamic market conditions. Emerging machine
learning techniques, including LSTM, reinforcement learning, and XGBoost, offer
potential solutions but are limited by computational complexity, training
inefficiencies, or constraints in time-series modeling. Liquid Neural Networks,
inspired by dynamic biological systems, present a promising alternative due to
their adaptability, low computational cost, and robustness to noise, making
them suitable for real-time decision-making and edge computing. Despite their
success in applications like autonomous vehicles and medical monitoring, their
potential in supply chain optimization remains underexplored. This study
introduces a hybrid LNN and XGBoost model to optimize ordering strategies in
multi-tier supply chains. By leveraging LNN's dynamic feature extraction and
XGBoost's global optimization capabilities, the model aims to mitigate the
bullwhip effect and enhance cumulative profitability. The research investigates
how local and global synergies within the hybrid framework address the dual
demands of adaptability and efficiency in SCM. The proposed approach fills a
critical gap in existing methodologies, offering an innovative solution for
dynamic and efficient supply chain management.

</details>


### [25] [Teaching Language Models To Gather Information Proactively](https://arxiv.org/abs/2507.21389)
*Tenghao Huang,Sihao Chen,Muhao Chen,Jonathan May,Longqi Yang,Mengting Wan,Pei Zhou*

Main category: cs.AI

TL;DR: The paper introduces proactive information gathering for LLMs to improve collaboration by identifying gaps and eliciting missing details through targeted questions, outperforming baselines in evaluations.


<details>
  <summary>Details</summary>
Motivation: Current LLMs often fail to proactively gather missing information in ambiguous tasks, limiting their effectiveness as collaborative partners.

Method: A reinforcement finetuning strategy is used to train LLMs to ask questions that reveal implicit user knowledge, tested with partially specified tasks.

Result: The Qwen-2.5-7B model outperforms o3-mini by 18% in automatic metrics and is preferred by humans for clarification questions (42%) and outlines (28%).

Conclusion: Proactive clarification enhances LLMs' role as collaborative partners, moving beyond passive text generation.

Abstract: Large language models (LLMs) are increasingly expected to function as
collaborative partners, engaging in back-and-forth dialogue to solve complex,
ambiguous problems. However, current LLMs often falter in real-world settings,
defaulting to passive responses or narrow clarifications when faced with
incomplete or under-specified prompts, falling short of proactively gathering
the missing information that is crucial for high-quality solutions. In this
work, we introduce a new task paradigm: proactive information gathering, where
LLMs must identify gaps in the provided context and strategically elicit
implicit user knowledge through targeted questions. To systematically study and
train this capability, we design a scalable framework that generates partially
specified, real-world tasks, masking key information and simulating authentic
ambiguity. Within this setup, our core innovation is a reinforcement finetuning
strategy that rewards questions that elicit genuinely new, implicit user
information -- such as hidden domain expertise or fine-grained requirements --
that would otherwise remain unspoken. Experiments demonstrate that our trained
Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic
evaluation metrics. More importantly, human evaluation reveals that
clarification questions and final outlines generated by our model are favored
by human annotators by 42% and 28% respectively. Together, these results
highlight the value of proactive clarification in elevating LLMs from passive
text generators to genuinely collaborative thought partners.

</details>


### [26] [Shapley Uncertainty in Natural Language Generation](https://arxiv.org/abs/2507.21406)
*Meilin Zhu,Gaojie Jin,Xiaowei Huang,Lijun Zhang*

Main category: cs.AI

TL;DR: The paper introduces a Shapley-based uncertainty metric for LLMs, outperforming semantic entropy in predicting model performance in QA tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods like semantic entropy rely on thresholding for uncertainty, which lacks nuance in capturing semantic relationships.

Method: Develops a Shapley-based uncertainty metric, ensuring it meets three fundamental properties for valid uncertainty measures.

Result: Shapley uncertainty more accurately predicts LLM performance in QA tasks compared to baseline measures.

Conclusion: The proposed framework provides a more nuanced and effective way to measure uncertainty in LLMs.

Abstract: In question-answering tasks, determining when to trust the outputs is crucial
to the alignment of large language models (LLMs). Kuhn et al. (2023) introduces
semantic entropy as a measure of uncertainty, by incorporating linguistic
invariances from the same meaning. It primarily relies on setting threshold to
measure the level of semantic equivalence relation. We propose a more nuanced
framework that extends beyond such thresholding by developing a Shapley-based
uncertainty metric that captures the continuous nature of semantic
relationships. We establish three fundamental properties that characterize
valid uncertainty metrics and prove that our Shapley uncertainty satisfies
these criteria. Through extensive experiments, we demonstrate that our Shapley
uncertainty more accurately predicts LLM performance in question-answering and
other datasets, compared to similar baseline measures.

</details>


### [27] [Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects](https://arxiv.org/abs/2507.21407)
*Yixin Liu,Guibin Zhang,Kun Wang,Shiyuan Li,Shirui Pan*

Main category: cs.AI

TL;DR: Graph-augmented LLM Agents (GLA) enhance LLM agent capabilities in planning, memory, and tool usage, with potential for multi-agent systems. The paper reviews advances and future directions.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs in agentic procedures (planning, memory, tool management, multi-agent coordination) by leveraging graphs for structure and coordination.

Method: Categorizes GLA methods by their functions (planning, memory, tool usage) and analyzes graph contributions. Discusses GLA in multi-agent systems for orchestration and efficiency.

Result: Graphs improve LLM agent workflows, enabling better planning, memory, and coordination. GLA shows promise for scalable and multimodal systems.

Conclusion: The paper provides a roadmap for future GLA research, emphasizing structural adaptability and unified systems, fostering deeper understanding of graphs in LLM agents.

Abstract: Autonomous agents based on large language models (LLMs) have demonstrated
impressive capabilities in a wide range of applications, including web
navigation, software development, and embodied control. While most LLMs are
limited in several key agentic procedures, such as reliable planning, long-term
memory, tool management, and multi-agent coordination, graphs can serve as a
powerful auxiliary structure to enhance structure, continuity, and coordination
in complex agent workflows. Given the rapid growth and fragmentation of
research on Graph-augmented LLM Agents (GLA), this paper offers a timely and
comprehensive overview of recent advances and also highlights key directions
for future work. Specifically, we categorize existing GLA methods by their
primary functions in LLM agent systems, including planning, memory, and tool
usage, and then analyze how graphs and graph learning algorithms contribute to
each. For multi-agent systems, we further discuss how GLA solutions facilitate
the orchestration, efficiency optimization, and trustworthiness of MAS.
Finally, we highlight key future directions to advance this field, from
improving structural adaptability to enabling unified, scalable, and multimodal
GLA systems. We hope this paper can serve as a roadmap for future research on
GLA and foster a deeper understanding of the role of graphs in LLM agent
systems.

</details>


### [28] [GovRelBench:A Benchmark for Government Domain Relevance](https://arxiv.org/abs/2507.21419)
*Haiquan Wang,Yi Chen,Shang Zeng,Yun Bian,Zhe Cui*

Main category: cs.AI

TL;DR: The paper introduces GovRelBench, a benchmark for evaluating LLMs' core capabilities in the government domain, addressing gaps in current evaluations.


<details>
  <summary>Details</summary>
Motivation: Current evaluations lack focus on LLMs' core capabilities, especially domain relevance, in the government sector.

Method: Proposes GovRelBench with domain-specific prompts and GovRelBERT, using SoftGovScore for training to assess relevance.

Result: Developed GovRelBERT with SoftGovScore for accurate government domain relevance scoring.

Conclusion: GovRelBench enhances LLM evaluation in the government domain, offering tools for research and practice.

Abstract: Current evaluations of LLMs in the government domain primarily focus on
safety considerations in specific scenarios, while the assessment of the
models' own core capabilities, particularly domain relevance, remains
insufficient. To address this gap, we propose GovRelBench, a benchmark
specifically designed for evaluating the core capabilities of LLMs in the
government domain. GovRelBench consists of government domain prompts and a
dedicated evaluation tool, GovRelBERT. During the training process of
GovRelBERT, we introduce the SoftGovScore method: this method trains a model
based on the ModernBERT architecture by converting hard labels to soft scores,
enabling it to accurately compute the text's government domain relevance score.
This work aims to enhance the capability evaluation framework for large models
in the government domain, providing an effective tool for relevant research and
practice. Our code and dataset are available at
https://github.com/pan-xi/GovRelBench.

</details>


### [29] [Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in Large Language Models](https://arxiv.org/abs/2507.21438)
*Vishal Raman,Vijai Aravindh R*

Main category: cs.AI

TL;DR: Evo-DKD is a dual-decoder framework for autonomous ontology evolution, combining structured ontology traversal with unstructured text reasoning, outperforming baselines in precision and task performance.


<details>
  <summary>Details</summary>
Motivation: Manual curation of ontologies and knowledge graphs is labor-intensive, and LLMs struggle with structured consistency, necessitating a hybrid approach.

Method: Evo-DKD uses two parallel decoders in an LLM: one for ontology edits and another for natural-language justifications, coordinated by a dynamic attention-based gating mechanism.

Result: Evo-DKD outperforms structured-only or unstructured-only baselines in precision of ontology updates and downstream task performance.

Conclusion: Evo-DKD combines symbolic and neural reasoning for sustainable ontology evolution, offering a new paradigm for LLM-driven knowledge base maintenance.

Abstract: Ontologies and knowledge graphs require continuous evolution to remain
comprehensive and accurate, but manual curation is labor intensive. Large
Language Models (LLMs) possess vast unstructured knowledge but struggle with
maintaining structured consistency. We propose Evo-DKD, a novel dual-decoder
framework for autonomous ontology evolution that combines structured ontology
traversal with unstructured text reasoning. Evo-DKD introduces two parallel
decoding streams within an LLM: one decoder generates candidate ontology edits
(e.g., new concepts or relations) while the other produces natural-language
justifications. A dynamic attention-based gating mechanism coordinates the two
streams, deciding at each step how to blend structured and unstructured
knowledge. Due to GPU constraints, we simulate the dual-decoder behavior using
prompt-based mode control to approximate coordinated decoding in a
single-stream mode. The system operates in a closed reasoning loop: proposed
ontology edits are validated (via consistency checks and cross-verification
with the text explanations) and then injected into the knowledge base, which in
turn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on
use cases including healthcare ontology refinement, semantic search
improvement, and cultural heritage timeline modeling. Experiments show that
Evo-DKD outperforms baselines using structured-only or unstructured-only
decoding in both precision of ontology updates and downstream task performance.
We present quantitative metrics and qualitative examples, confirming the
contributions of the dual-decoder design and gating router. Evo-DKD offers a
new paradigm for LLM-driven knowledge base maintenance, combining the strengths
of symbolic and neural reasoning for sustainable ontology evolution.

</details>


### [30] [Validating Pharmacogenomics Generative Artificial Intelligence Query Prompts Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2507.21453)
*Ashley Rector,Keaton Minor,Kamden Minor,Jeff McCormack,Beth Breeden,Ryan Nowers,Jay Dorris*

Main category: cs.AI

TL;DR: Sherpa Rx, an AI tool for pharmacogenomics, was evaluated using CPIC and PharmGKB data. It showed high performance in accuracy, relevance, and completeness, outperforming ChatGPT-4omini.


<details>
  <summary>Details</summary>
Motivation: To validate the performance of Sherpa Rx, an AI tool for pharmacogenomics, and compare it with other models like ChatGPT-4omini.

Method: Used a dataset of 260 queries across 26 CPIC guidelines, evaluated responses on accuracy, relevance, clarity, completeness, and recall. Compared Phase 1 (CPIC only) and Phase 2 (CPIC + PharmGKB) and ChatGPT-4omini.

Result: Sherpa Rx achieved high scores (e.g., accuracy 4.9, recall 0.99) and outperformed ChatGPT-4omini in accuracy and completeness. Phase 2 improved performance but not significantly over Phase 1.

Conclusion: Sherpa Rx demonstrates the potential of AI in pharmacogenomics, enhancing decision-making with accurate, personalized responses.

Abstract: This study evaluated Sherpa Rx, an artificial intelligence tool leveraging
large language models and retrieval-augmented generation (RAG) for
pharmacogenomics, to validate its performance on key response metrics. Sherpa
Rx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC)
guidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate
contextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC
guidelines was used to evaluate drug-gene interactions, dosing recommendations,
and therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2
additionally incorporated PharmGKB content. Responses were scored on accuracy,
relevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon
signed-rank tests compared accuracy between Phase 1 and Phase 2, and between
Phase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world
applicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated
high performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8,
and recall 0.99. The subset analysis (N=20) showed improvements in accuracy
(4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8).
ChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but
lagged in accuracy (3.9) and completeness (4.2). Differences in accuracy
between Phase 1 and Phase 2 was not statistically significant. However, Phase 2
significantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx
achieved 90% accuracy, outperforming other models. Integrating additional
resources like CPIC and PharmGKB with RAG enhances AI accuracy and performance.
This study highlights the transformative potential of generative AI like Sherpa
Rx in pharmacogenomics, improving decision-making with accurate, personalized
responses.

</details>


### [31] [An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning](https://arxiv.org/abs/2507.21471)
*Zujie Xie,Zixuan Chen,Jiheng Liang,Xiangyang Yu,Ziru Yu*

Main category: cs.AI

TL;DR: An LLM-driven framework for accurate, automated infrared spectral interpretation under low-data conditions, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Infrared spectroscopy's high-dimensional, overlapping bands challenge conventional chemometrics, and LLMs' potential for spectral analysis remains unexplored.

Method: An end-to-end LLM-driven agent integrates literature knowledge, spectral preprocessing, feature extraction, and multi-task reasoning, using few-shot prompts and iterative refinement.

Result: The framework outperforms single-turn inference and rivals/exceeds machine/deep learning models in low-data scenarios across diverse datasets.

Conclusion: LLMs can effectively automate IR spectral analysis, offering a robust solution for low-data conditions.

Abstract: Infrared spectroscopy offers rapid, non destructive measurement of chemical
and material properties but suffers from high dimensional, overlapping spectral
bands that challenge conventional chemometric approaches. Emerging large
language models (LLMs), with their capacity for generalization and reasoning,
offer promising potential for automating complex scientific workflows. Despite
this promise, their application in IR spectral analysis remains largely
unexplored. This study addresses the critical challenge of achieving accurate,
automated infrared spectral interpretation under low-data conditions using an
LLM-driven framework. We introduce an end-to-end, large language model driven
agent framework that integrates a structured literature knowledge base,
automated spectral preprocessing, feature extraction, and multi task reasoning
in a unified pipeline. By querying a curated corpus of peer reviewed IR
publications, the agent selects scientifically validated routines. The selected
methods transform each spectrum into low dimensional feature sets, which are
fed into few shot prompt templates for classification, regression, and anomaly
detection. A closed loop, multi turn protocol iteratively appends mispredicted
samples to the prompt, enabling dynamic refinement of predictions. Across
diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri
Reticulatae Pericarpium and waste water COD datasets, the multi turn LLM
consistently outperforms single turn inference, rivaling or exceeding machine
learning and deep learning models under low data regimes.

</details>


### [32] [Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess](https://arxiv.org/abs/2507.21488)
*Zhenwei Tang,Difan Jiao,Eric Xue,Reid McIlroy-Young,Jon Kleinberg,Siddhartha Sen,Ashton Anderson*

Main category: cs.AI

TL;DR: Maia4All is a framework for efficiently modeling individual human decision-making in chess with minimal data, using a two-stage optimization process.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of existing methods requiring large datasets for individual human behavior modeling in AI systems.

Method: A two-stage process: (1) enrichment step bridging population and individual behavior, and (2) democratization step refining individual embeddings with minimal data.

Result: Maia4All accurately predicts individual moves and profiles behavior with high fidelity, requiring only 20 games (vs. 5,000 previously).

Conclusion: Maia4All sets a new standard for personalized AI behavior modeling, with potential applications beyond chess, such as personalized LLMs.

Abstract: As humans seek to collaborate with, learn from, and better understand
artificial intelligence systems, developing AIs that can accurately emulate
individual decision-making becomes increasingly important. Chess, a
long-standing AI benchmark with precise skill measurement, offers an ideal
testbed for human-AI alignment. However, existing approaches to modeling human
behavior require prohibitively large amounts of data from each individual,
making them impractical for new or sparsely represented users. In this work, we
introduce Maia4All, a framework designed to learn and adapt to individual
decision-making styles efficiently, even with limited data. Maia4All achieves
this through a two-stage optimization process: (1) an enrichment step, which
bridges population and individual-level human behavior modeling with a
prototype-enriched model, and (2) a democratization step, which leverages
ability levels or user prototypes to initialize and refine individual
embeddings with minimal data. Our experimental results show that Maia4All can
accurately predict individual moves and profile behavioral patterns with high
fidelity, establishing a new standard for personalized human-like AI behavior
modeling in chess. Maia4All achieves individual human behavior modeling in
chess with only 20 games, compared to the 5,000 games required previously,
representing a significant improvement in data efficiency. Our work provides an
example of how population AI systems can flexibly adapt to individual users
using a prototype-enriched model as a bridge. This approach extends beyond
chess, as shown in our case study on idiosyncratic LLMs, highlighting its
potential for broader applications in personalized AI adaptation.

</details>


### [33] [Large Language Models for Supply Chain Decisions](https://arxiv.org/abs/2507.21502)
*David Simchi-Levi,Konstantina Mellou,Ishai Menache,Jeevan Pathuri*

Main category: cs.AI

TL;DR: LLMs can democratize supply chain technology by simplifying understanding and interaction with tools, reducing decision time from days to minutes.


<details>
  <summary>Details</summary>
Motivation: Business planners face challenges in understanding, explaining, and updating automated supply chain tools, requiring slow human intervention.

Method: Apply Large Language Models (LLMs) to address these challenges, enabling faster and more autonomous decision-making.

Result: Decision-making time reduced from days/weeks to minutes/hours, significantly boosting productivity.

Conclusion: LLMs offer a transformative solution to streamline supply chain management, enhancing efficiency and reducing reliance on human intermediaries.

Abstract: Supply Chain Management requires addressing a variety of complex
decision-making challenges, from sourcing strategies to planning and execution.
Over the last few decades, advances in computation and information technologies
have enabled the transition from manual, intuition and experience-based
decision-making, into more automated and data-driven decisions using a variety
of tools that apply optimization techniques. These techniques use mathematical
methods to improve decision-making.
  Unfortunately, business planners and executives still need to spend
considerable time and effort to (i) understand and explain the recommendations
coming out of these technologies; (ii) analyze various scenarios and answer
what-if questions; and (iii) update the mathematical models used in these tools
to reflect current business environments. Addressing these challenges requires
involving data science teams and/or the technology providers to explain results
or make the necessary changes in the technology and hence significantly slows
down decision making.
  Motivated by the recent advances in Large Language Models (LLMs), we report
how this disruptive technology can democratize supply chain technology -
namely, facilitate the understanding of tools' outcomes, as well as the
interaction with supply chain tools without human-in-the-loop. Specifically, we
report how we apply LLMs to address the three challenges described above, thus
substantially reducing the time to decision from days and weeks to minutes and
hours as well as dramatically increasing planners' and executives' productivity
and impact.

</details>


### [34] [MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions](https://arxiv.org/abs/2507.21503)
*Yanxu Zhu,Shitong Duan,Xiangxu Zhang,Jitao Sang,Peng Zhang,Tun Lu,Xiao Zhou,Jing Yao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: The paper assesses honesty in Multimodal Large Language Models (MLLMs) when answering visually unanswerable questions, introduces MoHoBench for benchmarking, and proposes alignment methods to improve honesty.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in MLLMs, their trustworthiness, especially in handling visually unanswerable questions, remains underexplored. This work aims to systematically evaluate and improve honesty in MLLMs.

Method: The study defines four types of unanswerable visual questions, constructs MoHoBench (12k+ samples), benchmarks 28 MLLMs, and implements supervised and preference learning for alignment.

Result: Findings reveal most models fail to refuse unanswerable questions appropriately, and honesty is influenced by visual information, not just language modeling.

Conclusion: The work highlights the need for dedicated methods to align MLLMs for honesty, providing initial solutions and a foundation for future trustworthy MLLM development.

Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable
advancements in vision-language tasks, yet produce potentially harmful or
untrustworthy content. Despite substantial work investigating the
trustworthiness of language models, MMLMs' capability to act honestly,
especially when faced with visually unanswerable questions, remains largely
underexplored. This work presents the first systematic assessment of honesty
behaviors across various MLLMs. We ground honesty in models' response behaviors
to unanswerable visual questions, define four representative types of such
questions, and construct MoHoBench, a large-scale MMLM honest benchmark,
consisting of 12k+ visual question samples, whose quality is guaranteed by
multi-stage filtering and human verification. Using MoHoBench, we benchmarked
the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our
findings show that: (1) most models fail to appropriately refuse to answer when
necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but
is deeply influenced by visual information, necessitating the development of
dedicated methods for multimodal honesty alignment. Therefore, we implemented
initial alignment methods using supervised and preference learning to improve
honesty behavior, providing a foundation for future work on trustworthy MLLMs.
Our data and code can be found at https://github.com/DSTTSD/MoHoBench.

</details>


### [35] [What Does it Mean for a Neural Network to Learn a "World Model"?](https://arxiv.org/abs/2507.21513)
*Kenneth Li,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: Proposes criteria for defining when a neural net learns a "world model," focusing on latent state space representation and avoiding trivial solutions.


<details>
  <summary>Details</summary>
Motivation: To operationalize informal terms like "world model" for experimental research, providing a common language.

Method: Uses ideas from linear probing literature to formalize computations factoring through data generation representations, with conditions to ensure non-triviality.

Result: A precise definition of a "world model" in neural nets, emphasizing latent state space and excluding trivial cases.

Conclusion: Provides a foundation for future experimental work on world models in neural networks, with action effects left for later study.

Abstract: We propose a set of precise criteria for saying a neural net learns and uses
a "world model." The goal is to give an operational meaning to terms that are
often used informally, in order to provide a common language for experimental
investigation. We focus specifically on the idea of representing a latent
"state space" of the world, leaving modeling the effect of actions to future
work. Our definition is based on ideas from the linear probing literature, and
formalizes the notion of a computation that factors through a representation of
the data generation process. An essential addition to the definition is a set
of conditions to check that such a "world model" is not a trivial consequence
of the neural net's data or task.

</details>


### [36] [ST-GDance: Long-Term and Collision-Free Group Choreography from Music](https://arxiv.org/abs/2507.21518)
*Jing Xu,Weiqiang Wang,Cunjian Chen,Jun Liu,Qiuhong Ke*

Main category: cs.AI

TL;DR: ST-GDance is a framework for generating synchronized group dances from music, addressing scalability and collision issues by decoupling spatial and temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: Group dance generation faces challenges in synchronizing multiple dancers and avoiding collisions, especially with longer sequences and more dancers. Existing methods struggle with dense spatial-temporal interactions.

Method: ST-GDance uses lightweight graph convolutions for spatial modeling and sparse attention for temporal modeling, optimizing long-term choreography and collision avoidance.

Result: ST-GDance outperforms state-of-the-art methods on the AIOZ-GDance dataset, particularly for long and coherent sequences.

Conclusion: The proposed framework effectively reduces computational costs while ensuring smooth, collision-free group dance generation.

Abstract: Group dance generation from music has broad applications in film, gaming, and
animation production. However, it requires synchronizing multiple dancers while
maintaining spatial coordination. As the number of dancers and sequence length
increase, this task faces higher computational complexity and a greater risk of
motion collisions. Existing methods often struggle to model dense
spatial-temporal interactions, leading to scalability issues and multi-dancer
collisions. To address these challenges, we propose ST-GDance, a novel
framework that decouples spatial and temporal dependencies to optimize
long-term and collision-free group choreography. We employ lightweight graph
convolutions for distance-aware spatial modeling and accelerated sparse
attention for efficient temporal modeling. This design significantly reduces
computational costs while ensuring smooth and collision-free interactions.
Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms
state-of-the-art baselines, particularly in generating long and coherent group
dance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.

</details>


### [37] [Large Language Models for Wireless Communications: From Adaptation to Autonomy](https://arxiv.org/abs/2507.21524)
*Le Liang,Hao Ye,Yucheng Sheng,Ouya Wang,Jiacheng Wang,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: The paper explores how large language models (LLMs) can revolutionize wireless communications by adapting them for core tasks, developing wireless-specific models, and enabling autonomous reasoning. It highlights benefits over traditional methods and outlines future research challenges.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and dynamics of wireless communications demand intelligent and adaptive solutions, which LLMs can provide due to their reasoning and generalization capabilities.

Method: The study examines three approaches: adapting pretrained LLMs for communication tasks, creating wireless-specific foundation models, and developing agentic LLMs with autonomous reasoning.

Result: LLM-based approaches offer unique benefits over traditional methods, as demonstrated by recent advances and case studies.

Conclusion: The paper identifies open challenges like multimodal fusion and self-improving capabilities, paving the way for intelligent and autonomous wireless networks.

Abstract: The emergence of large language models (LLMs) has revolutionized artificial
intelligence, offering unprecedented capabilities in reasoning, generalization,
and zero-shot learning. These strengths open new frontiers in wireless
communications, where increasing complexity and dynamics demand intelligent and
adaptive solutions. This article explores the role of LLMs in transforming
wireless systems across three key directions: adapting pretrained LLMs for core
communication tasks, developing wireless-specific foundation models to balance
versatility and efficiency, and enabling agentic LLMs with autonomous reasoning
and coordination capabilities. We highlight recent advances, practical case
studies, and the unique benefits of LLM-based approaches over traditional
methods. Finally, we outline open challenges and research opportunities,
including multimodal fusion, collaboration with lightweight models, and
self-improving capabilities, charting a path toward intelligent, adaptive, and
autonomous wireless networks of the future.

</details>


### [38] [Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations](https://arxiv.org/abs/2507.21571)
*Laura Spillner,Nima Zargham,Mihai Pomarlan,Robert Porzel,Rainer Malaka*

Main category: cs.AI

TL;DR: The paper proposes a personalized AI explanation approach tailored to user preferences and context, using a dynamic memory model for relevance.


<details>
  <summary>Details</summary>
Motivation: Current AI explanations focus on model internals, often unsuitable for non-experts. A human-centered approach is needed.

Method: A personalized explanation model with a dynamic memory of user interactions to estimate relevant information.

Result: The approach aims to improve explanation relevance and usability for non-experts.

Conclusion: Personalized explanations enhance transparency and user understanding in AI systems.

Abstract: The need for explanations in AI has, by and large, been driven by the desire
to increase the transparency of black-box machine learning models. However,
such explanations, which focus on the internal mechanisms that lead to a
specific output, are often unsuitable for non-experts. To facilitate a
human-centered perspective on AI explanations, agents need to focus on
individuals and their preferences as well as the context in which the
explanations are given. This paper proposes a personalized approach to
explanation, where the agent tailors the information provided to the user based
on what is most likely pertinent to them. We propose a model of the agent's
worldview that also serves as a personal and dynamic memory of its previous
interactions with the same user, based on which the artificial agent can
estimate what part of its knowledge is most likely new information to the user.

</details>


### [39] [SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21585)
*Hao Ye,Mengshi Qi,Zhaohong Liu,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: The paper introduces SafeDrive228K, a large-scale multimodal benchmark for evaluating vision-language models (VLMs) in traffic safety-critical scenarios, and proposes SafeDriveRAG, a retrieval-augmented generation method to enhance VLM performance.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks evaluation of VLMs in safety-critical driving scenarios, prompting the creation of a benchmark and method to address this gap.

Method: The authors develop SafeDrive228K (228K examples across 18 sub-tasks) and propose SafeDriveRAG, a knowledge graph-based retrieval-augmented generation approach with multi-scale subgraph retrieval.

Result: Integration of RAG improves VLM performance by +4.73% in Traffic Accidents, +8.79% in Corner Cases, and +14.57% in Traffic Safety Commonsense tasks.

Conclusion: The benchmark and methodology show promise for advancing traffic safety research, with code and data publicly available.

Abstract: In this work, we study how vision-language models (VLMs) can be utilized to
enhance the safety for the autonomous driving system, including perception,
situational understanding, and path planning. However, existing research has
largely overlooked the evaluation of these models in traffic safety-critical
driving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K)
and propose a new baseline based on VLM with knowledge graph-based
retrieval-augmented generation (SafeDriveRAG) for visual question answering
(VQA). Specifically, we introduce SafeDrive228K, the first large-scale
multimodal question-answering benchmark comprising 228K examples across 18
sub-tasks. This benchmark encompasses a diverse range of traffic safety
queries, from traffic accidents and corner cases to common safety knowledge,
enabling a thorough assessment of the comprehension and reasoning abilities of
the models. Furthermore, we propose a plug-and-play multimodal knowledge
graph-based retrieval-augmented generation approach that employs a novel
multi-scale subgraph retrieval algorithm for efficient information retrieval.
By incorporating traffic safety guidelines collected from the Internet, this
framework further enhances the model's capacity to handle safety-critical
situations. Finally, we conduct comprehensive evaluations on five mainstream
VLMs to assess their reliability in safety-sensitive driving tasks.
Experimental results demonstrate that integrating RAG significantly improves
performance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in
Corner Cases tasks and +14.57% in Traffic Safety Commonsense across five
mainstream VLMs, underscoring the potential of our proposed benchmark and
methodology for advancing research in traffic safety. Our source code and data
are available at https://github.com/Lumos0507/SafeDriveRAG.

</details>


### [40] [Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning](https://arxiv.org/abs/2507.21588)
*Jiong Yin,Liang Li,Jiehua Zhang,Yuhan Gao,Chenggang Yan,Xichun Sheng*

Main category: cs.AI

TL;DR: The paper introduces PHP, a three-stage method for audio-visual multi-task incremental learning, balancing knowledge retention and new task learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of preserving old task knowledge while learning new tasks in audio-visual incremental learning.

Method: PHP involves three phases: shallow (cross-task/modality adapter), middle (task-specific dynamic adapter), and deep (task/modality-specific prompts).

Result: PHP achieves state-of-the-art performance on four tasks (AVE, AVVP, AVS, AVQA).

Conclusion: PHP effectively balances knowledge sharing and specificity, improving incremental learning in audio-visual tasks.

Abstract: Audio-visual multi-task incremental learning aims to continuously learn from
multiple audio-visual tasks without the need for joint training on all tasks.
The challenge of the problem is how to preserve the old task knowledge while
facilitating the learning of new task with previous experiences. To address
these challenges, we introduce a three-stage Progressive Homeostatic and
Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the
task-shared modality aggregating adapter to foster cross-task and cross-modal
audio-visual representation learning to enhance shared understanding between
tasks. In the middle phase, we propose the task-specific modality-shared
dynamic generating adapter, which constructs prompts that are tailored to
individual tasks while remaining general across modalities, which balances the
models ability to retain knowledge against forgetting with its potential for
versatile multi-task transferability. In the deep phase, we introduce the
task-specific modality-independent prompts to further refine the understand
ability by targeting individual information for each task and modality. By
incorporating these three phases, PHP retains task-specific prompts while
adapting shared parameters for new tasks to effectively balance knowledge
sharing and specificity. Our method achieves SOTA performance in different
orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at
https://github.com/ENJOY-Yin-jiong/PHP.

</details>


### [41] [Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems](https://arxiv.org/abs/2507.21589)
*Bin Liu*

Main category: cs.AI

TL;DR: The paper explores the underutilization of Bayesian methods in embodied intelligence systems, despite their theoretical alignment, and suggests their potential for advancing open-world applications.


<details>
  <summary>Details</summary>
Motivation: To understand why Bayesian principles, which align well with embodied intelligence, are not widely applied in current systems, and to highlight their potential for future advancements.

Method: Analysis of Bayesian and contemporary embodied intelligence approaches through the lenses of search and learning, as per Rich Sutton's "The Bitter Lesson."

Result: Bayesian inference is not central in modern embodied intelligence systems, which are limited to closed-world environments, but it holds promise for open-world applications.

Conclusion: Bayesian methods could significantly enhance embodied intelligence systems, particularly for open-world scenarios, despite their current underuse.

Abstract: Embodied intelligence posits that cognitive capabilities fundamentally emerge
from - and are shaped by - an agent's real-time sensorimotor interactions with
its environment. Such adaptive behavior inherently requires continuous
inference under uncertainty. Bayesian statistics offers a principled
probabilistic framework to address this challenge by representing knowledge as
probability distributions and updating beliefs in response to new evidence. The
core computational processes underlying embodied intelligence - including
perception, action selection, learning, and even higher-level cognition - can
be effectively understood and modeled as forms of Bayesian inference. Despite
the deep conceptual connection between Bayesian statistics and embodied
intelligence, Bayesian principles have not been widely or explicitly applied in
today's embodied intelligence systems. In this work, we examine both Bayesian
and contemporary embodied intelligence approaches through two fundamental
lenses: search and learning - the two central themes in modern AI, as
highlighted in Rich Sutton's influential essay "The Bitter Lesson". This
analysis sheds light on why Bayesian inference has not played a central role in
the development of modern embodied intelligence. At the same time, it reveals
that current embodied intelligence systems remain largely confined to
closed-physical-world environments, and highlights the potential for Bayesian
methods to play a key role in extending these systems toward truly open
physical-world embodied intelligence.

</details>


### [42] [StaffPro: an LLM Agent for Joint Staffing and Profiling](https://arxiv.org/abs/2507.21636)
*Alessio Maritan*

Main category: cs.AI

TL;DR: StaffPro is an LLM agent for workforce management, combining staffing and profiling tasks with natural language flexibility and human feedback.


<details>
  <summary>Details</summary>
Motivation: To address the intertwined challenges of staffing (task assignment/scheduling) and profiling (worker attribute estimation) in workforce management using LLMs.

Method: Introduces StaffPro, an LLM agent that formalizes staffing and profiling in a mathematical framework, uses natural language for objectives, and employs a human-agent feedback loop for continuous learning.

Result: StaffPro effectively estimates worker attributes and generates high-quality schedules, demonstrated in a consulting firm simulation.

Conclusion: StaffPro provides a robust, interpretable, and human-centric solution for automated personnel management.

Abstract: Large language model (LLM) agents integrate pre-trained LLMs with modular
algorithmic components and have shown remarkable reasoning and decision-making
abilities. In this work, we investigate their use for two tightly intertwined
challenges in workforce management: staffing, i.e., the assignment and
scheduling of tasks to workers, which may require team formation; and
profiling, i.e., the continuous estimation of workers' skills, preferences, and
other latent attributes from unstructured data. We cast these problems in a
formal mathematical framework that links scheduling decisions to latent feature
estimation, and we introduce StaffPro, an LLM agent that addresses staffing and
profiling jointly. Differently from existing staffing solutions, StaffPro
allows expressing optimization objectives using natural language, accepts
textual task descriptions and provides high flexibility. StaffPro interacts
directly with humans by establishing a continuous human-agent feedback loop,
ensuring natural and intuitive use. By analyzing human feedback, our agent
continuously estimates the latent features of workers, realizing life-long
worker profiling and ensuring optimal staffing performance over time. A
consulting firm simulation example demonstrates that StaffPro successfully
estimates workers' attributes and generates high quality schedules. With its
innovative design, StaffPro offers a robust, interpretable, and human-centric
solution for automated personnel management.

</details>


### [43] [Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models](https://arxiv.org/abs/2507.21637)
*Wanying Wang,Zeyu Ma,Han Zheng,Xin Tan,Mingang Chen*

Main category: cs.AI

TL;DR: LVLMs are vulnerable to harmful inputs. The paper identifies three safety-related capabilities, proposes SASA to enhance safety without fine-tuning, and shows improved safety with minimal utility impact.


<details>
  <summary>Details</summary>
Motivation: LVLMs are less safe than language-only models, prompting investigation into their internal safety dynamics.

Method: Defines safety perception, semantic understanding, and alignment. Proposes SASA to project semantic representations onto safety layers and uses linear probing for risk detection.

Result: SASA improves LVLM safety significantly without compromising utility.

Conclusion: SASA effectively enhances LVLM safety by leveraging internal dynamics, offering a practical solution.

Abstract: Large vision-language models (LVLMs) are vulnerable to harmful input compared
to their language-only backbones. We investigated this vulnerability by
exploring LVLMs internal dynamics, framing their inherent safety understanding
in terms of three key capabilities. Specifically, we define these capabilities
as safety perception, semantic understanding, and alignment for linguistic
expression, and experimentally pinpointed their primary locations within the
model architecture. The results indicate that safety perception often emerges
before comprehensive semantic understanding, leading to the reduction in
safety. Motivated by these findings, we propose \textbf{Self-Aware Safety
Augmentation (SASA)}, a technique that projects informative semantic
representations from intermediate layers onto earlier safety-oriented layers.
This approach leverages the model's inherent semantic understanding to enhance
safety recognition without fine-tuning. Then, we employ linear probing to
articulate the model's internal semantic comprehension to detect the risk
before the generation process. Extensive experiments on various datasets and
tasks demonstrate that SASA significantly improves the safety of LVLMs, with
minimal impact on the utility.

</details>


### [44] [Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics](https://arxiv.org/abs/2507.21638)
*Leonard Hinckeldey,Elliot Fosong,Elle Miller,Rimvydas Rubavicius,Trevor McInroe,Patricia Wollstadt,Christiane B. Wiebel-Herboth,Subramanian Ramamoorthy,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: Assistax is an open-source benchmark for assistive robotics, leveraging JAX for speed and multi-agent RL for diverse training.


<details>
  <summary>Details</summary>
Motivation: Games dominate RL benchmarks but lack real-world applicability; Assistax addresses embodied interaction complexities.

Method: Uses JAX for hardware acceleration and multi-agent RL to train diverse partner agents for zero-shot coordination testing.

Result: Assistax runs 370x faster than CPU-based alternatives and provides reliable baselines for RL in assistive robotics.

Conclusion: Assistax is a practical benchmark for advancing RL research in assistive robotics, with open-source code available.

Abstract: The development of reinforcement learning (RL) algorithms has been largely
driven by ambitious challenge tasks and benchmarks. Games have dominated RL
benchmarks because they present relevant challenges, are inexpensive to run and
easy to understand. While games such as Go and Atari have led to many
breakthroughs, they often do not directly translate to real-world embodied
applications. In recognising the need to diversify RL benchmarks and addressing
complexities that arise in embodied interaction scenarios, we introduce
Assistax: an open-source benchmark designed to address challenges arising in
assistive robotics tasks. Assistax uses JAX's hardware acceleration for
significant speed-ups for learning in physics-based simulations. In terms of
open-loop wall-clock time, Assistax runs up to $370\times$ faster when
vectorising training runs compared to CPU-based alternatives. Assistax
conceptualises the interaction between an assistive robot and an active human
patient using multi-agent RL to train a population of diverse partner agents
against which an embodied robotic agent's zero-shot coordination capabilities
can be tested. Extensive evaluation and hyperparameter tuning for popular
continuous control RL and MARL algorithms provide reliable baselines and
establish Assistax as a practical benchmark for advancing RL research for
assistive robotics. The code is available at:
https://github.com/assistive-autonomy/assistax.

</details>


### [45] [Can the current trends of AI handle a full course of mathematics?](https://arxiv.org/abs/2507.21664)
*Mariam Alsayyad,Fayadh Kadhem*

Main category: cs.AI

TL;DR: The paper evaluates AI's ability to manage a full college-level math course, highlighting strengths in organization and accuracy but noting gaps in emotional and human aspects. Recommendations for integrating human and AI efforts are provided.


<details>
  <summary>Details</summary>
Motivation: To assess whether current AI trends can handle the responsibility of teaching a full college-level math course, including syllabus creation, material presentation, answering questions, and assessments.

Method: The study evaluates AI capabilities in four key areas: syllabus creation, material presentation, answering student questions, and creating assessments.

Result: AI excels in organization and accuracy but lacks in emotional and human aspects, making it insufficient for fully managing a math course alone.

Conclusion: The paper recommends combining human and AI strengths to optimize the creation and delivery of a college-level math course.

Abstract: This paper addresses the question of how able the current trends of
Artificial Intelligence (AI) are in managing to take the responsibility of a
full course of mathematics at a college level. The study evaluates this ability
in four significant aspects, namely, creating a course syllabus, presenting
selected material, answering student questions, and creating an assessment. It
shows that even though the AI is strong in some important parts like
organization and accuracy, there are still some human aspects that are far away
from the current abilities of AI. There is still a hidden emotional part, even
in science, that cannot be fulfilled by the AI in its current state. This paper
suggests some recommendations to integrate the human and AI potentials to
create better outcomes in terms of reaching the target of creating a full
course of mathematics, at a university level, as best as possible.

</details>


### [46] [Unrolling Dynamic Programming via Graph Filters](https://arxiv.org/abs/2507.21705)
*Sergio Rozada,Samuel Rey,Gonzalo Mateos,Antonio G. Marques*

Main category: cs.AI

TL;DR: The paper introduces BellNet, a learnable parametric model that unrolls and truncates policy iterations to efficiently solve Bellman's equations, leveraging graph signal processing for compact representation and reduced computational complexity.


<details>
  <summary>Details</summary>
Motivation: Standard dynamic programming methods like policy iteration are computationally expensive for large state-action spaces or long-term dependencies, prompting the need for a more efficient approach.

Method: The authors propose BellNet, which re-parameterizes policy iterations as a cascade of nonlinear graph filters, drawing insights from graph signal processing to minimize Bellman error from random initializations.

Result: Preliminary experiments in a grid-like environment show BellNet approximates optimal policies faster than classical methods.

Conclusion: BellNet offers a concise, transferable, and computationally efficient alternative to traditional dynamic programming methods.

Abstract: Dynamic programming (DP) is a fundamental tool used across many engineering
fields. The main goal of DP is to solve Bellman's optimality equations for a
given Markov decision process (MDP). Standard methods like policy iteration
exploit the fixed-point nature of these equations to solve them iteratively.
However, these algorithms can be computationally expensive when the
state-action space is large or when the problem involves long-term
dependencies. Here we propose a new approach that unrolls and truncates policy
iterations into a learnable parametric model dubbed BellNet, which we train to
minimize the so-termed Bellman error from random value function
initializations. Viewing the transition probability matrix of the MDP as the
adjacency of a weighted directed graph, we draw insights from graph signal
processing to interpret (and compactly re-parameterize) BellNet as a cascade of
nonlinear graph filters. This fresh look facilitates a concise, transferable,
and unifying representation of policy and value iteration, with an explicit
handle on complexity during inference. Preliminary experiments conducted in a
grid-like environment demonstrate that BellNet can effectively approximate
optimal policies in a fraction of the iterations required by classical methods.

</details>


### [47] [GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation](https://arxiv.org/abs/2507.21727)
*Jianfei Zhu,Haiqi Zhu,Shaohui Liu,Feng Jiang,Baichun Wei,Chunzhi Yi*

Main category: cs.AI

TL;DR: GDAIP integrates Graph Attention Networks and Minimax Entropy-based domain adaptation to address domain shifts in cross-dataset fMRI brain parcellation, achieving topologically plausible and functionally consistent results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with domain shifts in cross-dataset fMRI scenarios, limiting their effectiveness for individual brain parcellation.

Method: Proposes GDAIP, combining Graph Attention Networks and Minimax Entropy-based domain adaptation to adapt reference atlases from group-level to individual brain graphs.

Result: GDAIP produces individual parcellations with plausible boundaries, cross-session consistency, and functional organization reflection.

Conclusion: GDAIP effectively addresses domain shifts in cross-dataset fMRI, enabling accurate and consistent individual brain parcellation.

Abstract: Recent deep learning approaches have shown promise in learning such
individual brain parcellations from functional magnetic resonance imaging
(fMRI). However, most existing methods assume consistent data distributions
across domains and struggle with domain shifts inherent to real-world
cross-dataset scenarios. To address this challenge, we proposed Graph Domain
Adaptation for Individual Parcellation (GDAIP), a novel framework that
integrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based
domain adaptation. We construct cross-dataset brain graphs at both the group
and individual levels. By leveraging semi-supervised training and adversarial
optimization of the prediction entropy on unlabeled vertices from target brain
graph, the reference atlas is adapted from the group-level brain graph to the
individual brain graph, enabling individual parcellation under cross-dataset
settings. We evaluated our method using parcellation visualization, Dice
coefficient, and functional homogeneity. Experimental results demonstrate that
GDAIP produces individual parcellations with topologically plausible
boundaries, strong cross-session consistency, and ability of reflecting
functional organization.

</details>


### [48] [SAT-Based Bounded Fitting for the Description Logic ALC](https://arxiv.org/abs/2507.21752)
*Maurice Funk,Jean Christoph Jung,Tom Voellmer*

Main category: cs.AI

TL;DR: Bounded fitting for ALC and its fragments is NP-complete, even with minimal examples, and provides PAC learning guarantees, unlike other methods. An implementation using a SAT solver is presented and compared.


<details>
  <summary>Details</summary>
Motivation: To explore bounded fitting in description logic ALC and its fragments, addressing its computational complexity and learning guarantees.

Method: Investigates NP-completeness of bounded fitting, analyzes PAC learning guarantees, and implements a SAT solver-based approach for ALC.

Result: Bounded fitting is NP-complete for all studied fragments, even with one positive and negative example, and offers PAC guarantees. Other ALC learning methods lack such guarantees.

Conclusion: Bounded fitting is computationally hard but provides reliable learning guarantees, with a practical SAT solver implementation outperforming other tools.

Abstract: Bounded fitting is a general paradigm for learning logical formulas from
positive and negative data examples, that has received considerable interest
recently. We investigate bounded fitting for the description logic ALC and its
syntactic fragments. We show that the underlying size-restricted fitting
problem is NP-complete for all studied fragments, even in the special case of a
single positive and a single negative example. By design, bounded fitting comes
with probabilistic guarantees in Valiant's PAC learning framework. In contrast,
we show that other classes of algorithms for learning ALC concepts do not
provide such guarantees. Finally, we present an implementation of bounded
fitting in ALC and its fragments based on a SAT solver. We discuss
optimizations and compare our implementation to other concept learning tools.

</details>


### [49] [Towards a rigorous evaluation of RAG systems: the challenge of due diligence](https://arxiv.org/abs/2507.21753)
*Grégoire Martinon,Alexandra Lorenzo de Brionne,Jérôme Bohard,Antoine Lojou,Damien Hervault,Nicolas J-B. Brunel*

Main category: cs.AI

TL;DR: The paper evaluates the reliability of Retrieval-Augmented Generation (RAG) systems in high-risk sectors, proposing a robust evaluation protocol combining human and LLM-Judge annotations to address issues like hallucinations and off-topic responses.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the reliability of RAG systems in critical contexts like healthcare and finance, particularly due to issues such as hallucinations and off-topic responses.

Method: Proposes a robust evaluation protocol combining human annotations and LLM-Judge annotations, inspired by Prediction Powered Inference (PPI), to measure system performance with statistical guarantees.

Result: The study provides a comprehensive dataset and precise performance measurements, identifying system failures like hallucinations, off-topic responses, failed citations, and abstentions.

Conclusion: The contributions aim to enhance the reliability and scalability of RAG system evaluation protocols for industrial applications.

Abstract: The rise of generative AI, has driven significant advancements in high-risk
sectors like healthcare and finance. The Retrieval-Augmented Generation (RAG)
architecture, combining language models (LLMs) with search engines, is
particularly notable for its ability to generate responses from document
corpora. Despite its potential, the reliability of RAG systems in critical
contexts remains a concern, with issues such as hallucinations persisting. This
study evaluates a RAG system used in due diligence for an investment fund. We
propose a robust evaluation protocol combining human annotations and LLM-Judge
annotations to identify system failures, like hallucinations, off-topic, failed
citations, and abstentions. Inspired by the Prediction Powered Inference (PPI)
method, we achieve precise performance measurements with statistical
guarantees. We provide a comprehensive dataset for further analysis. Our
contributions aim to enhance the reliability and scalability of RAG systems
evaluation protocols in industrial applications.

</details>


### [50] [Hybrid Causal Identification and Causal Mechanism Clustering](https://arxiv.org/abs/2507.21792)
*Saixiong Liu,Yuhua Qian,Jue Li,Honghong Cheng,Feijiang Li*

Main category: cs.AI

TL;DR: The paper introduces MCVCI and MCVCC methods for bivariate causal direction identification, leveraging heterogeneous causal relationships and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods often use a single causal mechanism, but real-world data involves heterogeneous causal relationships across environments.

Method: Proposes MCVCI, combining Gaussian mixture models and neural networks, and MCVCC for clustering causal mechanisms, using likelihoods from probabilistic bounds.

Result: The methods outperform state-of-the-art techniques on simulated and real data.

Conclusion: MCVCI and MCVCC effectively address causal heterogeneity and improve causal inference accuracy.

Abstract: Bivariate causal direction identification is a fundamental and vital problem
in the causal inference field. Among binary causal methods, most methods based
on additive noise only use one single causal mechanism to construct a causal
model. In the real world, observations are always collected in different
environments with heterogeneous causal relationships. Therefore, on observation
data, this paper proposes a Mixture Conditional Variational Causal Inference
model (MCVCI) to infer heterogeneous causality. Specifically, according to the
identifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the
superior fitting capabilities of the Gaussian mixture model and the neural
network and elegantly uses the likelihoods obtained from the probabilistic
bounds of the mixture conditional variational auto-encoder as causal decision
criteria. Moreover, we model the casual heterogeneity into cluster numbers and
propose the Mixture Conditional Variational Causal Clustering (MCVCC) method,
which can reveal causal mechanism expression. Compared with state-of-the-art
methods, the comprehensive best performance demonstrates the effectiveness of
the methods proposed in this paper on several simulated and real data.

</details>


### [51] [MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE](https://arxiv.org/abs/2507.21802)
*Junzhe Li,Yutao Cui,Tao Huang,Yinping Ma,Chun Fan,Miles Yang,Zhao Zhong*

Main category: cs.AI

TL;DR: MixGRPO improves efficiency in human preference alignment for image generation by combining SDE and ODE sampling with a sliding window mechanism, reducing training time by 50% and introducing a faster variant, MixGRPO-Flash, with 71% lower training time.


<details>
  <summary>Details</summary>
Motivation: Existing methods like FlowGRPO are inefficient due to sampling and optimizing over all denoising steps in MDP. MixGRPO aims to streamline this process.

Method: MixGRPO integrates SDE and ODE sampling, using a sliding window to confine randomness and reduce optimization overhead. A faster variant, MixGRPO-Flash, supports higher-order solvers.

Result: MixGRPO outperforms DanceGRPO in effectiveness and efficiency, with 50% lower training time. MixGRPO-Flash reduces training time by 71%.

Conclusion: MixGRPO offers a novel, efficient framework for human preference alignment in image generation, with significant performance and training time improvements.

Abstract: Although GRPO substantially enhances flow matching models in human preference
alignment of image generation, methods such as FlowGRPO still exhibit
inefficiency due to the necessity of sampling and optimizing over all denoising
steps specified by the Markov Decision Process (MDP). In this paper, we propose
$\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed
sampling strategies through the integration of stochastic differential
equations (SDE) and ordinary differential equations (ODE). This streamlines the
optimization process within the MDP to improve efficiency and boost
performance. Specifically, MixGRPO introduces a sliding window mechanism, using
SDE sampling and GRPO-guided optimization only within the window, while
applying ODE sampling outside. This design confines sampling randomness to the
time-steps within the window, thereby reducing the optimization overhead, and
allowing for more focused gradient updates to accelerate convergence.
Additionally, as time-steps beyond the sliding window are not involved in
optimization, higher-order solvers are supported for sampling. So we present a
faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves
training efficiency while achieving comparable performance. MixGRPO exhibits
substantial gains across multiple dimensions of human preference alignment,
outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%
lower training time. Notably, MixGRPO-Flash further reduces training time by
71%. Codes and models are available at
$\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.

</details>


### [52] [An Agentic AI for a New Paradigm in Business Process Development](https://arxiv.org/abs/2507.21823)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff*

Main category: cs.AI

TL;DR: The paper proposes an agent-based method for business process design using Agentic AI, shifting from task-based to goal-oriented automation for flexibility in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To enhance industrial automation by leveraging Agentic AI for more modular, intelligent, and context-aware business processes.

Method: Introduces an agent-based approach where agents collaborate to achieve business goals, using business objects and merging goals when necessary.

Result: Enables flexible, modular, and intelligent business process development suited for dynamic industrial settings.

Conclusion: The agent-based model improves automation by focusing on goals and collaboration, offering adaptability in industrial environments.

Abstract: Artificial Intelligence agents represent the next major revolution in the
continuous technological evolution of industrial automation. In this paper, we
introduce a new approach for business process design and development that
leverages the capabilities of Agentic AI. Departing from the traditional
task-based approach to business process design, we propose an agent-based
method, where agents contribute to the achievement of business goals,
identified by a set of business objects. When a single agent cannot fulfill a
goal, we have a merge goal that can be achieved through the collaboration of
multiple agents. The proposed model leads to a more modular and intelligent
business process development by organizing it around goals, objects, and
agents. As a result, this approach enables flexible and context-aware
automation in dynamic industrial environments.

</details>


### [53] [DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework](https://arxiv.org/abs/2507.21830)
*Kuiye Ding,Fanda Fan,Yao Wang,Ruijie jian,Xiaorui Wang,Luqi Gong,Yishan Jiang,Chunjie Luo an Jianfeng Zhan*

Main category: cs.AI

TL;DR: DualSG is a dual-stream framework using LLMs as semantic guides to refine traditional predictions, avoiding loss of numerical precision and alignment issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods either lose numerical precision by treating LLMs as end-to-end forecasters or struggle with aligning textual and time series modalities.

Method: Proposes DualSG, where LLMs act as semantic guides, introduces Time Series Caption for explicit prompts, and a caption-guided fusion module.

Result: Outperforms 15 state-of-the-art baselines on real-world datasets.

Conclusion: Explicitly combining numerical forecasting with semantic guidance improves performance.

Abstract: Multivariate Time Series Forecasting plays a key role in many applications.
Recent works have explored using Large Language Models for MTSF to take
advantage of their reasoning abilities. However, many methods treat LLMs as
end-to-end forecasters, which often leads to a loss of numerical precision and
forces LLMs to handle patterns beyond their intended design. Alternatively,
methods that attempt to align textual and time series modalities within latent
space frequently encounter alignment difficulty. In this paper, we propose to
treat LLMs not as standalone forecasters, but as semantic guidance modules
within a dual-stream framework. We propose DualSG, a dual-stream framework that
provides explicit semantic guidance, where LLMs act as Semantic Guides to
refine rather than replace traditional predictions. As part of DualSG, we
introduce Time Series Caption, an explicit prompt format that summarizes trend
patterns in natural language and provides interpretable context for LLMs,
rather than relying on implicit alignment between text and time series in the
latent space. We also design a caption-guided fusion module that explicitly
models inter-variable relationships while reducing noise and computation.
Experiments on real-world datasets from diverse domains show that DualSG
consistently outperforms 15 state-of-the-art baselines, demonstrating the value
of explicitly combining numerical forecasting with semantic guidance.

</details>


### [54] [Probabilistic Active Goal Recognition](https://arxiv.org/abs/2507.21846)
*Chenyuan Zhang,Cristian Rojas Cardenas,Hamid Rezatofighi,Mor Vered,Buser Say*

Main category: cs.AI

TL;DR: The paper introduces Active Goal Recognition (AGR) using a probabilistic framework with joint belief updates and MCTS, outperforming passive methods and showing domain-independent effectiveness.


<details>
  <summary>Details</summary>
Motivation: To improve multi-agent interactions by actively reducing uncertainty about other agents' goals, moving beyond passive observation.

Method: Combines joint belief updates with Monte Carlo Tree Search (MCTS) for efficient goal inference without domain-specific knowledge.

Result: Outperforms passive goal recognition; domain-independent MCTS matches domain-specific baselines.

Conclusion: Proposes a robust, practical framework for goal inference, enhancing interactive multi-agent systems.

Abstract: In multi-agent environments, effective interaction hinges on understanding
the beliefs and intentions of other agents. While prior work on goal
recognition has largely treated the observer as a passive reasoner, Active Goal
Recognition (AGR) focuses on strategically gathering information to reduce
uncertainty. We adopt a probabilistic framework for Active Goal Recognition and
propose an integrated solution that combines a joint belief update mechanism
with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan
efficiently and infer the actor's hidden goal without requiring domain-specific
knowledge. Through comprehensive empirical evaluation in a grid-based domain,
we show that our joint belief update significantly outperforms passive goal
recognition, and that our domain-independent MCTS performs comparably to our
strong domain-specific greedy baseline. These results establish our solution as
a practical and robust framework for goal inference, advancing the field toward
more interactive and adaptive multi-agent systems.

</details>


### [55] [EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity](https://arxiv.org/abs/2507.21848)
*Xingjian Zhang,Siwei Wen,Wenjun Wu,Lei Huang*

Main category: cs.AI

TL;DR: The paper introduces EDGE-GRPO, an algorithm to mitigate advantage collapse in LLMs by leveraging entropy-driven advantage and guided error correction.


<details>
  <summary>Details</summary>
Motivation: Address the issue of identical rewards within groups in GRPO, which causes advantage collapse, by improving response diversity and training signals.

Method: Analyze model reflection limitations and policy entropy, then propose EDGE-GRPO with entropy-driven advantage and guided error correction.

Result: EDGE-GRPO outperforms existing methods on reasoning benchmarks, effectively reducing advantage collapse.

Conclusion: EDGE-GRPO is a superior solution for mitigating advantage collapse in LLMs, validated by extensive experiments.

Abstract: Large Language Models (LLMs) have made remarkable progress in enhancing
step-by-step reasoning through reinforcement learning. However, the Group
Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward
rules, often encounters the issue of identical rewards within groups, leading
to the advantage collapse problem. Existing works typically address this
challenge from two perspectives: enforcing model reflection to enhance response
diversity, and introducing internal feedback to augment the training signal
(advantage). In this work, we begin by analyzing the limitations of model
reflection and investigating the policy entropy of responses at the
fine-grained sample level. Based on our experimental findings, we propose the
EDGE-GRPO algorithm, which adopts \textbf{E}ntropy-\textbf{D}riven Advantage
and \textbf{G}uided \textbf{E}rror Correction to effectively mitigate the
problem of advantage collapse. Extensive experiments on several main reasoning
benchmarks demonstrate the effectiveness and superiority of our approach. It is
available at https://github.com/ZhangXJ199/EDGE-GRPO.

</details>


### [56] [MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors](https://arxiv.org/abs/2507.21872)
*Shouyi Lu,Zihan Lin,Chao Lu,Huanran Wang,Guirong Zhuo,Lianqing Zheng*

Main category: cs.AI

TL;DR: MultiEditor is a dual-branch latent diffusion framework for joint editing of images and LiDAR point clouds in driving scenarios, improving generalization for rare vehicle categories.


<details>
  <summary>Details</summary>
Motivation: Addressing the long-tailed distribution of real-world data in autonomous driving, which hinders generalization for rare but safety-critical vehicle categories.

Method: Uses 3D Gaussian Splatting (3DGS) as a prior, with multi-level appearance control and depth-guided deformable cross-modality conditioning.

Result: Achieves high-fidelity reconstruction, editing controllability, and cross-modality consistency, enhancing detection accuracy for rare vehicle categories.

Conclusion: MultiEditor effectively improves multimodal perception data for autonomous driving, particularly for underrepresented classes.

Abstract: Autonomous driving systems rely heavily on multimodal perception data to
understand complex environments. However, the long-tailed distribution of
real-world data hinders generalization, especially for rare but safety-critical
vehicle categories. To address this challenge, we propose MultiEditor, a
dual-branch latent diffusion framework designed to edit images and LiDAR point
clouds in driving scenarios jointly. At the core of our approach is introducing
3D Gaussian Splatting (3DGS) as a structural and appearance prior for target
objects. Leveraging this prior, we design a multi-level appearance control
mechanism--comprising pixel-level pasting, semantic-level guidance, and
multi-branch refinement--to achieve high-fidelity reconstruction across
modalities. We further propose a depth-guided deformable cross-modality
condition module that adaptively enables mutual guidance between modalities
using 3DGS-rendered depth, significantly enhancing cross-modality consistency.
Extensive experiments demonstrate that MultiEditor achieves superior
performance in visual and geometric fidelity, editing controllability, and
cross-modality consistency. Furthermore, generating rare-category vehicle data
with MultiEditor substantially enhances the detection accuracy of perception
models on underrepresented classes.

</details>


### [57] [A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data](https://arxiv.org/abs/2507.21873)
*Raffaele Pojer,Andrea Passerini,Kim G. Larsen,Manfred Jaeger*

Main category: cs.AI

TL;DR: The paper integrates Graph Neural Networks (GNNs) with Relational Bayesian Networks (RBNs) to combine learning and reasoning, presenting two implementation methods and demonstrating improved performance in node classification and environmental planning.


<details>
  <summary>Details</summary>
Motivation: GNNs lack symbolic reasoning, while RBNs lack learning capabilities. The goal is to merge their strengths for enhanced performance and versatility.

Method: Two integration approaches: compiling GNNs into RBNs or keeping GNNs external. Includes a MAP inference method for neuro-symbolic models.

Result: Improved accuracy in node classification and effective decision-making in environmental planning, with new benchmark datasets.

Conclusion: The framework successfully bridges learning and reasoning, enabling novel applications and better performance in diverse tasks.

Abstract: Graph neural networks (GNNs) excel at predictive tasks on graph-structured
data but often lack the ability to incorporate symbolic domain knowledge and
perform general reasoning. Relational Bayesian Networks (RBNs), in contrast,
enable fully generative probabilistic modeling over graph-like structures and
support rich symbolic knowledge and probabilistic inference. This paper
presents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,
combining the learning strength of GNNs with the flexible reasoning
capabilities of RBNs.
  We develop two implementations of this integration: one compiles GNNs
directly into the native RBN language, while the other maintains the GNN as an
external component. Both approaches preserve the semantics and computational
properties of GNNs while fully aligning with the RBN modeling paradigm. We also
propose a maximum a-posteriori (MAP) inference method for these neuro-symbolic
models.
  To demonstrate the framework's versatility, we apply it to two distinct
problems. First, we transform a GNN for node classification into a collective
classification model that explicitly models homo- and heterophilic label
patterns, substantially improving accuracy. Second, we introduce a
multi-objective network optimization problem in environmental planning, where
MAP inference supports complex decision-making. Both applications include new
publicly available benchmark datasets.
  This work introduces a powerful and coherent neuro-symbolic approach to graph
data, bridging learning and reasoning in ways that enable novel applications
and improved performance across diverse tasks.

</details>


### [58] [Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis](https://arxiv.org/abs/2507.21875)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: The paper introduces Tiny-BioMoE, a lightweight pretrained model for biosignal analysis, aimed at improving automatic pain assessment through multimodal physiological signals.


<details>
  <summary>Details</summary>
Motivation: Accurate pain assessment is crucial for patient care and management, and leveraging physiological signals can provide objective insights.

Method: The study proposes Tiny-BioMoE, a pretrained embedding model trained on 4.4 million biosignal image representations with 7.3 million parameters, tested on diverse physiological signals.

Result: The model demonstrates effectiveness in automatic pain recognition across various biosignal modalities.

Conclusion: Tiny-BioMoE offers a lightweight, high-quality solution for biosignal embedding extraction, enhancing pain assessment systems.

Abstract: Pain is a complex and pervasive condition that affects a significant portion
of the population. Accurate and consistent assessment is essential for
individuals suffering from pain, as well as for developing effective management
strategies in a healthcare system. Automatic pain assessment systems enable
continuous monitoring, support clinical decision-making, and help minimize
patient distress while mitigating the risk of functional deterioration.
Leveraging physiological signals offers objective and precise insights into a
person's state, and their integration in a multimodal framework can further
enhance system performance. This study has been submitted to the \textit{Second
Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The
proposed approach introduces \textit{Tiny-BioMoE}, a lightweight pretrained
embedding model for biosignal analysis. Trained on $4.4$ million biosignal
image representations and consisting of only $7.3$ million parameters, it
serves as an effective tool for extracting high-quality embeddings for
downstream tasks. Extensive experiments involving electrodermal activity, blood
volume pulse, respiratory signals, peripheral oxygen saturation, and their
combinations highlight the model's effectiveness across diverse modalities in
automatic pain recognition tasks. \textit{\textcolor{blue}{The model's
architecture (code) and weights are available at
https://github.com/GkikasStefanos/Tiny-BioMoE.

</details>


### [59] [Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image](https://arxiv.org/abs/2507.21881)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: The paper proposes a pipeline using electrodermal activity signals for automatic pain assessment, demonstrating its effectiveness through experiments and comparing it favorably to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Pain affects many people, and reliable assessment is crucial for effective management. Automated systems can provide continuous, objective monitoring to improve clinical decisions and reduce distress.

Method: The method uses electrodermal activity signals, creating and visualizing multiple signal representations in a single diagram. Various processing and filtering techniques are tested.

Result: The approach performs comparably or better than traditional fusion methods, proving robust for integrating signal representations or modalities.

Conclusion: The proposed pipeline is a viable alternative for pain assessment, offering accuracy and reliability in monitoring physiological signals.

Abstract: Pain is a multifaceted phenomenon that affects a substantial portion of the
population. Reliable and consistent evaluation benefits those experiencing pain
and underpins the development of effective and advanced management strategies.
Automatic pain-assessment systems deliver continuous monitoring, inform
clinical decision-making, and aim to reduce distress while preventing
functional decline. By incorporating physiological signals, these systems
provide objective, accurate insights into an individual's condition. This study
has been submitted to the \textit{Second Multimodal Sensing Grand Challenge for
Next-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline
that leverages electrodermal activity signals as input modality. Multiple
representations of the signal are created and visualized as waveforms, and they
are jointly visualized within a single multi-representation diagram. Extensive
experiments incorporating various processing and filtering techniques, along
with multiple representation combinations, demonstrate the effectiveness of the
proposed approach. It consistently yields comparable, and in several cases
superior, results to traditional fusion methods, establishing it as a robust
alternative for integrating different signal representations or modalities.

</details>


### [60] [The Impact of Foundational Models on Patient-Centric e-Health Systems](https://arxiv.org/abs/2507.21882)
*Elmira Onagh,Alireza Davoodi,Maleknaz Nayebi*

Main category: cs.AI

TL;DR: The study evaluates AI maturity in 116 patient-centric healthcare apps, finding most (86.21%) at early stages and few (13.79%) with advanced AI.


<details>
  <summary>Details</summary>
Motivation: Assess AI maturity in healthcare apps to gauge trustworthiness, transparency, and impact.

Method: Used LLMs to extract and categorize features into Gartner AI maturity stages.

Result: 86.21% of apps are in early AI stages; 13.79% show advanced integration.

Conclusion: AI in healthcare apps is mostly immature, highlighting a need for further development.

Abstract: As Artificial Intelligence (AI) becomes increasingly embedded in healthcare
technologies, understanding the maturity of AI in patient-centric applications
is critical for evaluating its trustworthiness, transparency, and real-world
impact. In this study, we investigate the integration and maturity of AI
feature integration in 116 patient-centric healthcare applications. Using Large
Language Models (LLMs), we extracted key functional features, which are then
categorized into different stages of the Gartner AI maturity model. Our results
show that over 86.21\% of applications remain at the early stages of AI
integration, while only 13.79% demonstrate advanced AI integration.

</details>


### [61] [Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline](https://arxiv.org/abs/2507.21886)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: A study proposes a respiration-based pain assessment method using a cross-attention transformer and multi-windowing strategy, showing strong performance with compact models.


<details>
  <summary>Details</summary>
Motivation: Accurate pain evaluation is crucial for effective management, and automated systems can aid continuous monitoring and clinical decisions.

Method: The method uses respiration signals, a cross-attention transformer, and a multi-windowing strategy to capture short-term, long-term, and global features.

Result: Respiration proves valuable for pain assessment, and optimized compact models outperform larger ones. The multi-window approach enhances feature representation.

Conclusion: The proposed pipeline is effective for pain assessment, highlighting the potential of compact models and respiration signals.

Abstract: Pain is a complex condition affecting a large portion of the population.
Accurate and consistent evaluation is essential for individuals experiencing
pain, and it supports the development of effective and advanced management
strategies. Automatic pain assessment systems provide continuous monitoring and
support clinical decision-making, aiming to reduce distress and prevent
functional decline. This study has been submitted to the \textit{Second
Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The
proposed method introduces a pipeline that leverages respiration as the input
signal and incorporates a highly efficient cross-attention transformer
alongside a multi-windowing strategy. Extensive experiments demonstrate that
respiration is a valuable physiological modality for pain assessment. Moreover,
experiments revealed that compact and efficient models, when properly
optimized, can achieve strong performance, often surpassing larger
counterparts. The proposed multi-window approach effectively captures both
short-term and long-term features, as well as global characteristics, thereby
enhancing the model's representational capacity.

</details>


### [62] [LLM-based Content Classification Approach for GitHub Repositories by the README Files](https://arxiv.org/abs/2507.21899)
*Malik Uzair Mehmood,Shahid Hussain,Wen Li Wang,Muhammad Usama Malik*

Main category: cs.AI

TL;DR: The study fine-tunes LLMs (BERT, DistilBERT, RoBERTa) to classify GitHub README sections, achieving high accuracy (F1=0.98) and exploring cost-effective PEFT techniques like LoRA.


<details>
  <summary>Details</summary>
Motivation: GitHub README files often lack detail, hindering repository adoption. Automating classification can improve repository usability and impact.

Method: Fine-tuned three encoder-only LLMs (BERT, DistilBERT, RoBERTa) on 4226 README sections, tested PEFT techniques like LoRA.

Result: Achieved an F1 score of 0.98, outperforming state-of-the-art methods. PEFT techniques like LoRA offered economical alternatives.

Conclusion: LLMs can effectively automate README classification, enhancing GitHub repository usability and adoption.

Abstract: GitHub is the world's most popular platform for storing, sharing, and
managing code. Every GitHub repository has a README file associated with it.
The README files should contain project-related information as per the
recommendations of GitHub to support the usage and improvement of repositories.
However, GitHub repository owners sometimes neglected these recommendations.
This prevents a GitHub repository from reaching its full potential. This
research posits that the comprehensiveness of a GitHub repository's README file
significantly influences its adoption and utilization, with a lack of detail
potentially hindering its full potential for widespread engagement and impact
within the research community. Large Language Models (LLMs) have shown great
performance in many text-based tasks including text classification, text
generation, text summarization and text translation. In this study, an approach
is developed to fine-tune LLMs for automatically classifying different sections
of GitHub README files. Three encoder-only LLMs are utilized, including BERT,
DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a
gold-standard dataset consisting of 4226 README file sections. This approach
outperforms current state-of-the-art methods and has achieved an overall F1
score of 0.98. Moreover, we have also investigated the use of
Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation
(LoRA) and shown an economical alternative to full fine-tuning without
compromising much performance. The results demonstrate the potential of using
LLMs in designing an automatic classifier for categorizing the content of
GitHub README files. Consequently, this study contributes to the development of
automated tools for GitHub repositories to improve their identifications and
potential usages.

</details>


### [63] [Libra: Large Chinese-based Safeguard for AI Content](https://arxiv.org/abs/2507.21929)
*Ziyang Chen,Huimu Yu,Xing Wu,Dongqin Liu,Songlin Hu*

Main category: cs.AI

TL;DR: Libra-Guard is a safeguard system for Chinese LLMs, using a two-stage training pipeline and introducing Libra-Test for safety evaluation, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address safety and ethical concerns in high-stakes applications of Chinese LLMs.

Method: Two-stage curriculum training: guard pretraining on synthetic samples, followed by fine-tuning on real-world data. Introduces Libra-Test benchmark for evaluation.

Result: Libra-Guard achieves 86.79% accuracy, outperforming other models.

Conclusion: Establishes a framework for safer Chinese LLMs and advances AI safety governance.

Abstract: Large language models (LLMs) excel in text understanding and generation but
raise significant safety and ethical concerns in high-stakes applications. To
mitigate these risks, we present Libra-Guard, a cutting-edge safeguard system
designed to enhance the safety of Chinese-based LLMs. Leveraging a two-stage
curriculum training pipeline, Libra-Guard enhances data efficiency by employing
guard pretraining on synthetic samples, followed by fine-tuning on
high-quality, real-world data, thereby significantly reducing reliance on
manual annotations. To enable rigorous safety evaluations, we also introduce
Libra-Test, the first benchmark specifically designed to evaluate the
effectiveness of safeguard systems for Chinese content. It covers seven
critical harm scenarios and includes over 5,700 samples annotated by domain
experts. Experiments show that Libra-Guard achieves 86.79% accuracy,
outperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat
(65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o.
These contributions establish a robust framework for advancing the safety
governance of Chinese LLMs and represent a tentative step toward developing
safer, more reliable Chinese AI systems.

</details>


### [64] [Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data & Activities](https://arxiv.org/abs/2507.21964)
*Sourish Gunesh Dhekane,Thomas Ploetz*

Main category: cs.AI

TL;DR: The paper proposes a zero-shot HAR method using natural language embeddings to avoid LLM prompting, addressing privacy, reliability, and consistency issues.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based HAR methods pose risks like privacy invasion and inconsistency, necessitating alternative approaches.

Method: The method models sensor data and activities with natural language embeddings for zero-shot classification, bypassing LLM prompting.

Result: The solution is validated on six datasets, demonstrating the effectiveness of language modeling in zero-shot HAR.

Conclusion: The work presents a viable alternative to LLM-based HAR, enhancing privacy and reliability without sacrificing performance.

Abstract: Developing zero-shot human activity recognition (HAR) methods is a critical
direction in smart home research -- considering its impact on making HAR
systems work across smart homes having diverse sensing modalities, layouts, and
activities of interest. The state-of-the-art solutions along this direction are
based on generating natural language descriptions of the sensor data and
feeding it via a carefully crafted prompt to the LLM to perform classification.
Despite their performance guarantees, such ``prompt-the-LLM'' approaches carry
several risks, including privacy invasion, reliance on an external service, and
inconsistent predictions due to version changes, making a case for alternative
zero-shot HAR methods that do not require prompting the LLMs. In this paper, we
propose one such solution that models sensor data and activities using natural
language, leveraging its embeddings to perform zero-shot classification and
thereby bypassing the need to prompt the LLMs for activity predictions. The
impact of our work lies in presenting a detailed case study on six datasets,
highlighting how language modeling can bolster HAR systems in zero-shot
recognition.

</details>


### [65] [Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks](https://arxiv.org/abs/2507.21974)
*Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Yibin Kang,Haozhe Zhang,Merouane Debbah,Fadhel Ayed*

Main category: cs.AI

TL;DR: A lightweight framework using LLMs for RCA in mobile networks is proposed, with a two-stage training method improving accuracy and reasoning.


<details>
  <summary>Details</summary>
Motivation: RCA in mobile networks is challenging due to interpretability and domain expertise needs.

Method: Two-stage training (supervised fine-tuning + reinforcement learning) on TeleLogs dataset to adapt LLMs for RCA.

Result: Significant performance gains over state-of-the-art models, with strong generalization.

Conclusion: Domain-adapted, reasoning-enhanced LLMs show promise for practical and explainable RCA.

Abstract: Root Cause Analysis (RCA) in mobile networks remains a challenging task due
to the need for interpretability, domain expertise, and causal reasoning. In
this work, we propose a lightweight framework that leverages Large Language
Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of
annotated troubleshooting problems designed to benchmark RCA capabilities. Our
evaluation reveals that existing open-source reasoning LLMs struggle with these
problems, underscoring the need for domain-specific adaptation. To address this
issue, we propose a two-stage training methodology that combines supervised
fine-tuning with reinforcement learning to improve the accuracy and reasoning
quality of LLMs. The proposed approach fine-tunes a series of RCA models to
integrate domain knowledge and generate structured, multi-step diagnostic
explanations, improving both interpretability and effectiveness. Extensive
experiments across multiple LLM sizes show significant performance gains over
state-of-the-art reasoning and non-reasoning models, including strong
generalization to randomized test variants. These results demonstrate the
promise of domain-adapted, reasoning-enhanced LLMs for practical and
explainable RCA in network operation and management.

</details>


### [66] [The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain](https://arxiv.org/abs/2507.21976)
*Tanvir Ahmed Khan,Aranya Saha,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.AI

TL;DR: The paper evaluates pruning and quantization for compressing MLLMs in medical applications, proposing a novel layer selection method. It achieves 70% memory reduction and 4% higher performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs are computationally expensive, requiring efficient compression for medical use.

Method: Proposes a layer selection method for pruning, analyzes quantization techniques, and tests a prune-SFT-quantize pipeline.

Result: Enables 7B-parameter MLLMs to run in 4GB VRAM, reducing memory by 70% with 4% higher performance.

Conclusion: The method effectively compresses MLLMs for medical applications, balancing performance and efficiency.

Abstract: Multimodal Large Language Models (MLLMs) hold huge potential for usage in the
medical domain, but their computational costs necessitate efficient compression
techniques. This paper evaluates the impact of structural pruning and
activation-aware quantization on a fine-tuned LLAVA model for medical
applications. We propose a novel layer selection method for pruning, analyze
different quantization techniques, and assess the performance trade-offs in a
prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B
parameters to run within 4 GB of VRAM, reducing memory usage by 70% while
achieving 4% higher model performance compared to traditional pruning and
quantization techniques in the same compression ratio.

</details>


### [67] [PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences](https://arxiv.org/abs/2507.22009)
*Bahar İlgen,Akshat Dubey,Georges Hattab*

Main category: cs.AI

TL;DR: PHAX is a framework for generating human-centered AI explanations in public health, using structured argumentation to enhance transparency and trust.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods lack adaptability for diverse health stakeholders, requiring clearer, context-aware explanations.

Method: PHAX combines defeasible reasoning, natural language techniques, and user modeling to create audience-specific justifications.

Result: PHAX improves interpretability and trust in AI outputs, demonstrated through medical term simplification and policy justification use cases.

Conclusion: PHAX advances transparent, human-centered AI in public health by aligning formal reasoning with communicative needs.

Abstract: Ensuring transparency and trust in AI-driven public health and biomedical
sciences systems requires more than accurate predictions-it demands
explanations that are clear, contextual, and socially accountable. While
explainable AI (XAI) has advanced in areas like feature attribution and model
interpretability, most methods still lack the structure and adaptability needed
for diverse health stakeholders, including clinicians, policymakers, and the
general public. We introduce PHAX-a Public Health Argumentation and
eXplainability framework-that leverages structured argumentation to generate
human-centered explanations for AI outputs. PHAX is a multi-layer architecture
combining defeasible reasoning, adaptive natural language techniques, and user
modeling to produce context-aware, audience-specific justifications. More
specifically, we show how argumentation enhances explainability by supporting
AI-driven decision-making, justifying recommendations, and enabling interactive
dialogues across user types. We demonstrate the applicability of PHAX through
use cases such as medical term simplification, patient-clinician communication,
and policy justification. In particular, we show how simplification decisions
can be modeled as argument chains and personalized based on user
expertise-enhancing both interpretability and trust. By aligning formal
reasoning methods with communicative demands, PHAX contributes to a broader
vision of transparent, human-centered AI in public health.

</details>


### [68] [UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding](https://arxiv.org/abs/2507.22025)
*Shuquan Lian,Yuhang Wu,Jia Ma,Zihan Song,Bingqi Chen,Xiawu Zheng,Hui Li*

Main category: cs.AI

TL;DR: UI-AGILE enhances GUI agents with improved training (Continuous Reward, Simple Thinking reward, Cropping-based Resampling) and inference (Decomposed Grounding with Selection), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing issues in GUI agent training (ineffective reward, visual noise) and inference (reasoning designs).

Method: Proposes Continuous Reward, Simple Thinking reward, Cropping-based Resampling for training; Decomposed Grounding with Selection for inference.

Result: 23% grounding accuracy improvement over best baseline on ScreenSpot-Pro.

Conclusion: UI-AGILE significantly advances GUI agent capabilities with its comprehensive framework.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven
significant advances in Graphical User Interface (GUI) agent capabilities.
Nevertheless, existing GUI agent training and inference techniques still suffer
from a dilemma for reasoning designs, ineffective reward, and visual noise. To
address these issues, we introduce UI-AGILE, a comprehensive framework
enhancing GUI agents at both the training and inference stages. For training,
we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:
1) a Continuous Reward function to incentivize high-precision grounding; 2) a
"Simple Thinking" reward to balance planning with speed and grounding accuracy;
and 3) a Cropping-based Resampling strategy to mitigate the sparse reward
problem and improve learning on complex tasks. For inference, we present
Decomposed Grounding with Selection, a novel method that dramatically improves
grounding accuracy on high-resolution displays by breaking the image into
smaller, manageable parts. Experiments show that UI-AGILE achieves the
state-of-the-art performance on two benchmarks ScreenSpot-Pro and
ScreenSpot-v2. For instance, using both our proposed training and inference
enhancement methods brings 23% grounding accuracy improvement over the best
baseline on ScreenSpot-Pro.

</details>


### [69] [UserBench: An Interactive Gym Environment for User-Centric Agents](https://arxiv.org/abs/2507.22034)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Zhiwei Liu,Jianguo Zhang,Haolin Chen,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserBench is a benchmark for evaluating LLM agents in multi-turn, preference-driven interactions, revealing gaps in proactive collaboration with users.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents excel in reasoning and tool use but struggle with proactive collaboration when user goals are vague or evolving.

Method: Introduces UserBench, a user-centric benchmark with simulated users and incremental preference revelation, testing agents' ability to clarify intent and use tools.

Result: Evaluation shows low alignment (20%) with user intents and preference discovery (<30%), highlighting the challenge of true collaboration.

Conclusion: UserBench provides a framework to measure and improve LLM agents' collaborative capabilities.

Abstract: Large Language Models (LLMs)-based agents have made impressive progress in
reasoning and tool use, enabling them to solve complex tasks. However, their
ability to proactively collaborate with users, especially when goals are vague,
evolving, or indirectly expressed, remains underexplored. To address this gap,
we introduce UserBench, a user-centric benchmark designed to evaluate agents in
multi-turn, preference-driven interactions. UserBench features simulated users
who start with underspecified goals and reveal preferences incrementally,
requiring agents to proactively clarify intent and make grounded decisions with
tools. Our evaluation of leading open- and closed-source LLMs reveals a
significant disconnect between task completion and user alignment. For
instance, models provide answers that fully align with all user intents only
20% of the time on average, and even the most advanced models uncover fewer
than 30% of all user preferences through active interaction. These results
highlight the challenges of building agents that are not just capable task
executors, but true collaborative partners. UserBench offers an interactive
environment to measure and advance this critical capability.

</details>


### [70] [The Interspeech 2025 Speech Accessibility Project Challenge](https://arxiv.org/abs/2507.22047)
*Xiuwen Zheng,Bornali Phukon,Jonghwan Na,Ed Cutrell,Kyu Han,Mark Hasegawa-Johnson,Pan-Pan Jiang,Aadhrik Kuila,Colin Lea,Bob MacDonald,Gautam Mantena,Venkatesh Ravichandran,Leda Sari,Katrin Tomanek,Chang D. Yoo,Chris Zwilling*

Main category: cs.AI

TL;DR: The 2025 Interspeech SAP Challenge improved ASR for speech disabilities using 400+ hours of data, with 12/22 teams beating the baseline WER and 17/22 surpassing SemScore. Top team achieved 8.11% WER and 88.44% SemScore.


<details>
  <summary>Details</summary>
Motivation: Address inadequate ASR performance for individuals with speech disabilities due to limited training data.

Method: Utilized 400+ hours of SAP data from 500+ individuals, evaluated submissions via EvalAI using Word Error Rate and Semantic Score.

Result: 12 teams outperformed whisper-large-v2 on WER; 17 on SemScore. Top team achieved 8.11% WER and 88.44% SemScore.

Conclusion: The SAP Challenge set new benchmarks for ASR in recognizing impaired speech, demonstrating significant progress.

Abstract: While the last decade has witnessed significant advancements in Automatic
Speech Recognition (ASR) systems, performance of these systems for individuals
with speech disabilities remains inadequate, partly due to limited public
training data. To bridge this gap, the 2025 Interspeech Speech Accessibility
Project (SAP) Challenge was launched, utilizing over 400 hours of SAP data
collected and transcribed from more than 500 individuals with diverse speech
disabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline,
the SAP Challenge evaluates submissions based on Word Error Rate and Semantic
Score. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2
baseline in terms of WER, while 17 teams surpassed the baseline on SemScore.
Notably, the top team achieved the lowest WER of 8.11\%, and the highest
SemScore of 88.44\% at the same time, setting new benchmarks for future ASR
systems in recognizing impaired speech.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students](https://arxiv.org/abs/2507.21109)
*Prital Bamnodkar*

Main category: cs.LG

TL;DR: TFC-SR, a novel continual learning method inspired by human strategies like Active Recall, outperforms existing methods by stabilizing past knowledge through Active Recall Probes, achieving higher accuracy on benchmarks like Split CIFAR-100.


<details>
  <summary>Details</summary>
Motivation: Addressing Catastrophic Forgetting in Deep Neural Networks by mimicking human learning strategies such as Active Recall, Deliberate Practice, and Spaced Repetition.

Method: Introduces Task Focused Consolidation with Spaced Recall (TFC-SR), enhancing experience replay with Active Recall Probes for periodic, task-aware evaluation.

Result: TFC-SR outperforms baselines, e.g., 13.17% accuracy on Split CIFAR-100 vs. 7.40% for standard replay, due to the stabilizing effect of probes.

Conclusion: TFC-SR is robust and efficient, emphasizing the value of active memory retrieval in continual learning, with optimal performance in memory-constrained settings.

Abstract: Deep Neural Networks often suffer from a critical limitation known as
Catastrophic Forgetting, where performance on past tasks degrades after
learning new ones. This paper introduces a novel continual learning approach
inspired by human learning strategies like Active Recall, Deliberate Practice
and Spaced Repetition, named Task Focused Consolidation with Spaced Recall
(TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we
termed the Active Recall Probe. It is a periodic, task-aware evaluation of the
model's memory that stabilizes the representations of past knowledge. We test
TFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading
regularization-based and replay-based baselines. Our results show that TFC-SR
performs significantly better than these methods. For instance, on the Split
CIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's
7.40%. We demonstrate that this advantage comes from the stabilizing effect of
the probe itself, and not from the difference in replay volume. Additionally,
we analyze the trade-off between memory size and performance and show that
while TFC-SR performs better in memory-constrained environments, higher replay
volume is still more effective when available memory is abundant. We conclude
that TFC-SR is a robust and efficient approach, highlighting the importance of
integrating active memory retrieval mechanisms into continual learning systems.

</details>


### [72] [Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks](https://arxiv.org/abs/2507.21119)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,Nicola Sambo,João Pedro,Mohammad M. Hosseini,Antonio Napoli,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: Comparison of techniques for class imbalance mitigation in optical network failure detection, highlighting Threshold Adjustment for F1 gain and Random Under-sampling for speed.


<details>
  <summary>Details</summary>
Motivation: Address class imbalance in optical network failure detection to improve accuracy and efficiency.

Method: Evaluate pre-, in-, and post-processing techniques, focusing on Threshold Adjustment and Random Under-sampling (RUS).

Result: Threshold Adjustment achieves the highest F1 gain (15.3%), while RUS offers the fastest inference.

Conclusion: Trade-off between performance (F1 gain) and complexity (inference speed) must be considered when choosing mitigation techniques.

Abstract: We compare pre-, in-, and post-processing techniques for class imbalance
mitigation in optical network failure detection. Threshold Adjustment achieves
the highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the
fastest inference, highlighting a key performance-complexity trade-off.

</details>


### [73] [Quantum Geometry of Data](https://arxiv.org/abs/2507.21135)
*Alexander G. Abanov,Luca Candelori,Harold C. Steinacker,Martin T. Wells,Jerome R. Busemeyer,Cameron J. Hogan,Vahagn Kirakosyan,Nicola Marzari,Sunil Pinnamaneni,Dario Villani,Mengjia Xu,Kharen Musaelian*

Main category: cs.LG

TL;DR: QCML encodes data as quantum geometry using Hermitian matrices and Hilbert space states, capturing global properties and avoiding dimensionality issues.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum geometry for understanding cognitive phenomena and overcoming the limitations of local methods in machine learning.

Method: Represent data features as learned Hermitian matrices and map data points to states in Hilbert space, deriving geometric and topological structures.

Result: QCML captures intrinsic dimension, quantum metric, and Berry curvature, demonstrated on synthetic and real-world datasets.

Conclusion: QCML's quantum geometric representation offers a novel framework for advancing quantum cognition and machine learning.

Abstract: We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as
quantum geometry. In QCML, features of the data are represented by learned
Hermitian matrices, and data points are mapped to states in Hilbert space. The
quantum geometry description endows the dataset with rich geometric and
topological structure - including intrinsic dimension, quantum metric, and
Berry curvature - derived directly from the data. QCML captures global
properties of data, while avoiding the curse of dimensionality inherent in
local methods. We illustrate this on a number of synthetic and real-world
examples. Quantum geometric representation of QCML could advance our
understanding of cognitive phenomena within the framework of quantum cognition.

</details>


### [74] [A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning](https://arxiv.org/abs/2507.21136)
*Mojtaba Moattari*

Main category: cs.LG

TL;DR: The paper proposes 3 independence criteria for unsupervised and supervised dimensionality reduction, outperforming baselines like tSNE and PCA, and advancing interpretable ML.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring proposed nonlinearities maximize variability and capture data diversity in unsupervised and supervised learning.

Method: Reviewed independence criteria, proposed 3 new ones, and designed dimensionality reduction methods evaluated in linear and neural nonlinear settings.

Result: Outperformed baselines (tSNE, PCA, LDA, VAE) in contrast, accuracy, and interpretability.

Conclusion: The methods open a new line of interpretable ML research.

Abstract: Unsupervised and supervised learning methods conventionally use kernels to
capture nonlinearities inherent in data structure. However experts have to
ensure their proposed nonlinearity maximizes variability and capture inherent
diversity of data. We reviewed all independence criteria to design unsupervised
learners. Then we proposed 3 independence criteria and used them to design
unsupervised and supervised dimensionality reduction methods. We evaluated
contrast, accuracy and interpretability of these methods in both linear and
neural nonlinear settings. The results show that the methods have outperformed
the baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and
layer sharing) and opened a new line of interpretable machine learning (ML) for
the researchers.

</details>


### [75] [Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning](https://arxiv.org/abs/2507.21147)
*Fabrizio Lo Scudo,Alessio De Rango,Luca Furnari,Alfonso Senatore,Donato D'Ambrosio,Giuseppe Mendicino,Gianluigi Greco*

Main category: cs.LG

TL;DR: The paper proposes a contrastive learning framework to improve wildfire prediction by addressing data imbalance and high-dimensional spatio-temporal challenges, using smaller patch sizes without performance loss.


<details>
  <summary>Details</summary>
Motivation: Wildfires severely impact ecosystems and health, worsened by climate change. Current methods face data imbalance and computational challenges, needing advanced solutions.

Method: Introduces morphology-based curriculum contrastive learning to enhance latent representations of dynamic features, reducing computational costs and improving predictions.

Result: Experimental analysis validates the framework's effectiveness in handling diverse regional characteristics and imbalanced data.

Conclusion: The proposed contrastive learning approach offers a viable solution for better wildfire risk management with reduced computational demands.

Abstract: Wildfires significantly impact natural ecosystems and human health, leading
to biodiversity loss, increased hydrogeological risks, and elevated emissions
of toxic substances. Climate change exacerbates these effects, particularly in
regions with rising temperatures and prolonged dry periods, such as the
Mediterranean. This requires the development of advanced risk management
strategies that utilize state-of-the-art technologies. However, in this
context, the data show a bias toward an imbalanced setting, where the incidence
of wildfire events is significantly lower than typical situations. This
imbalance, coupled with the inherent complexity of high-dimensional
spatio-temporal data, poses significant challenges for training deep learning
architectures. Moreover, since precise wildfire predictions depend mainly on
weather data, finding a way to reduce computational costs to enable more
frequent updates using the latest weather forecasts would be beneficial. This
paper investigates how adopting a contrastive framework can address these
challenges through enhanced latent representations for the patch's dynamic
features. We thus introduce a new morphology-based curriculum contrastive
learning that mitigates issues associated with diverse regional characteristics
and enables the use of smaller patch sizes without compromising performance. An
experimental analysis is performed to validate the effectiveness of the
proposed modeling strategies.

</details>


### [76] [Deep Unfolding for MIMO Signal Detection](https://arxiv.org/abs/2507.21152)
*Hangli Ge,Noboru Koshizuka*

Main category: cs.LG

TL;DR: A deep unfolding neural network-based MIMO detector using Wirtinger calculus, called DPST, offers efficient, interpretable, and low-complexity signal detection in the complex domain.


<details>
  <summary>Details</summary>
Motivation: Prior methods rely on real-valued approximations, which misalign with the complex nature of signal processing tasks.

Method: Dynamic Partially Shrinkage Thresholding (DPST) operates natively in the complex domain with minimal trainable parameters.

Result: Superior detection performance with fewer iterations and lower computational complexity.

Conclusion: DPST is a practical solution for next-generation massive MIMO systems.

Abstract: In this paper, we propose a deep unfolding neural network-based MIMO detector
that incorporates complex-valued computations using Wirtinger calculus. The
method, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables
efficient, interpretable, and low-complexity MIMO signal detection. Unlike
prior approaches that rely on real-valued approximations, our method operates
natively in the complex domain, aligning with the fundamental nature of signal
processing tasks. The proposed algorithm requires only a small number of
trainable parameters, allowing for simplified training. Numerical results
demonstrate that the proposed method achieves superior detection performance
with fewer iterations and lower computational complexity, making it a practical
solution for next-generation massive MIMO systems.

</details>


### [77] [Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers](https://arxiv.org/abs/2507.21153)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.LG

TL;DR: A DRL-optimized energy management system for e-commerce data centers reduces costs by 38%, improves energy efficiency by 82%, and cuts carbon emissions by 45%, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance energy efficiency, cost-effectiveness, and environmental sustainability in e-commerce data centers by dynamically managing renewable energy, storage, and grid power.

Method: Uses Deep Reinforcement Learning (DRL) algorithms for real-time energy management, adapting to fluctuating energy availability.

Result: Achieves 38% cost reduction, 82% energy efficiency improvement, 45% lower carbon emissions, and 1.5% SLA violation rate, outperforming RL and heuristic methods.

Conclusion: DRL offers a robust solution for energy management, advancing sustainability and optimization in data centers.

Abstract: This paper explores the implementation of a Deep Reinforcement Learning
(DRL)-optimized energy management system for e-commerce data centers, aimed at
enhancing energy efficiency, cost-effectiveness, and environmental
sustainability. The proposed system leverages DRL algorithms to dynamically
manage the integration of renewable energy sources, energy storage, and grid
power, adapting to fluctuating energy availability in real time. The study
demonstrates that the DRL-optimized system achieves a 38\% reduction in energy
costs, significantly outperforming traditional Reinforcement Learning (RL)
methods (28\%) and heuristic approaches (22\%). Additionally, it maintains a
low SLA violation rate of 1.5\%, compared to 3.0\% for RL and 4.8\% for
heuristic methods. The DRL-optimized approach also results in an 82\%
improvement in energy efficiency, surpassing other methods, and a 45\%
reduction in carbon emissions, making it the most environmentally friendly
solution. The system's cumulative reward of 950 reflects its superior
performance in balancing multiple objectives. Through rigorous testing and
ablation studies, the paper validates the effectiveness of the DRL model's
architecture and parameters, offering a robust solution for energy management
in data centers. The findings highlight the potential of DRL in advancing
energy optimization strategies and addressing sustainability challenges.

</details>


### [78] [SPADE-S: A Sparsity-Robust Foundational Forecaster](https://arxiv.org/abs/2507.21155)
*Malcolm Wolff,Matthew Li,Ravi Kiran Selvam,Hanjing Zhu,Kin G. Olivares,Ruijun Ma,Abhinav Katoch,Shankar Ramasubramanian,Mengfei Cao,Roberto Bandarra,Rahul Gopalsamy,Stefania La Vattiata,Sitan Yang,Michael M. Mahoney*

Main category: cs.LG

TL;DR: SPADE-S is a forecasting model addressing biases in time series with low magnitude/sparsity, improving accuracy by up to 15% over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing models underperform on low-magnitude and sparse time series due to biases in loss functions, sampling, and encoding methods.

Method: SPADE-S reduces biases in magnitude and sparsity, enhancing prediction accuracy.

Result: SPADE-S outperforms state-of-the-art methods, with accuracy gains up to 15%, notably in demand forecasting.

Conclusion: SPADE-S effectively addresses systematic biases, improving forecast accuracy across diverse datasets.

Abstract: Despite significant advancements in time series forecasting, accurate
modeling of time series with strong heterogeneity in magnitude and/or sparsity
patterns remains challenging for state-of-the-art deep learning architectures.
We identify several factors that lead existing models to systematically
underperform on low-magnitude and sparse time series, including loss functions
with implicit biases toward high-magnitude series, training-time sampling
methods, and limitations of time series encoding methods.
  SPADE-S is a robust forecasting architecture that significantly reduces
magnitude- and sparsity-based systematic biases and improves overall prediction
accuracy. Empirical results demonstrate that SPADE-S outperforms existing
state-of-the-art approaches across a diverse set of use cases in demand
forecasting. In particular, we show that, depending on the quantile forecast
and magnitude of the series, SPADE-S can improve forecast accuracy by up to
15%. This results in P90 overall forecast accuracy gains of 2.21%, 6.58%, and
4.28%, and P50 forecast accuracy gains of 0.92%, 0.77%, and 1.95%,
respectively, for each of three distinct datasets, ranging from 3 million to
700 million series, from a large online retailer.

</details>


### [79] [Handling Out-of-Distribution Data: A Survey](https://arxiv.org/abs/2507.21160)
*Lakpa Tamang,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.LG

TL;DR: The paper addresses distribution shifts in ML, categorizing them into covariate and concept shifts, and reviews methods to handle them.


<details>
  <summary>Details</summary>
Motivation: The challenge of data distribution changes between training and deployment stages in ML applications.

Method: Formalizes distribution shifts, reviews conventional methods, and discusses detection and mitigation techniques.

Result: Identifies gaps in handling shifts and proposes future research directions.

Conclusion: Provides a comprehensive review of distribution shift literature, emphasizing overlooked OOD data.

Abstract: In the field of Machine Learning (ML) and data-driven applications, one of
the significant challenge is the change in data distribution between the
training and deployment stages, commonly known as distribution shift. This
paper outlines different mechanisms for handling two main types of distribution
shifts: (i) Covariate shift: where the value of features or covariates change
between train and test data, and (ii) Concept/Semantic-shift: where model
experiences shift in the concept learned during training due to emergence of
novel classes in the test phase. We sum up our contributions in three folds.
First, we formalize distribution shifts, recite on how the conventional method
fails to handle them adequately and urge for a model that can simultaneously
perform better in all types of distribution shifts. Second, we discuss why
handling distribution shifts is important and provide an extensive review of
the methods and techniques that have been developed to detect, measure, and
mitigate the effects of these shifts. Third, we discuss the current state of
distribution shift handling mechanisms and propose future research directions
in this area. Overall, we provide a retrospective synopsis of the literature in
the distribution shift, focusing on OOD data that had been overlooked in the
existing surveys.

</details>


### [80] [OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection](https://arxiv.org/abs/2507.21164)
*Nicolas Pinon,Carole Lartizien*

Main category: cs.LG

TL;DR: A novel unsupervised anomaly detection method couples representation learning with a one-class SVM, demonstrating robustness in tasks like MNIST-C and brain MRI lesion detection.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing UAD methods, such as poor anomaly reconstruction or suboptimal feature spaces, by tightly integrating feature learning with anomaly detection.

Method: Proposes a custom loss formulation aligning latent features with an analytically solvable one-class SVM decision boundary.

Result: Outperforms in detecting small, non-hyperintense brain lesions and shows robustness to domain shifts in MNIST-C and MRI tasks.

Conclusion: The method is effective for general UAD and medical imaging, with potential for real-world applications.

Abstract: Unsupervised anomaly detection (UAD) aims to detect anomalies without labeled
data, a necessity in many machine learning applications where anomalous samples
are rare or not available. Most state-of-the-art methods fall into two
categories: reconstruction-based approaches, which often reconstruct anomalies
too well, and decoupled representation learning with density estimators, which
can suffer from suboptimal feature spaces. While some recent methods attempt to
couple feature learning and anomaly detection, they often rely on surrogate
objectives, restrict kernel choices, or introduce approximations that limit
their expressiveness and robustness. To address this challenge, we propose a
novel method that tightly couples representation learning with an analytically
solvable one-class SVM (OCSVM), through a custom loss formulation that directly
aligns latent features with the OCSVM decision boundary. The model is evaluated
on two tasks: a new benchmark based on MNIST-C, and a challenging brain MRI
subtle lesion detection task. Unlike most methods that focus on large,
hyperintense lesions at the image level, our approach succeeds to target small,
non-hyperintense lesions, while we evaluate voxel-wise metrics, addressing a
more clinically relevant scenario. Both experiments evaluate a form of
robustness to domain shifts, including corruption types in MNIST-C and
scanner/age variations in MRI. Results demonstrate performance and robustness
of our proposed mode,highlighting its potential for general UAD and real-world
medical imaging applications. The source code is available at
https://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning

</details>


### [81] [AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation](https://arxiv.org/abs/2507.21166)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.LG

TL;DR: AGORA introduces structured interaction as a scaling axis, outperforming monolithic models by 4.45% on math benchmarks through collaborative ensemble reasoning.


<details>
  <summary>Details</summary>
Motivation: Current training datasets are static, limiting complex reasoning progress. The paper explores interaction as a scalable alternative to increasing model parameters.

Method: Proposes AGORA, a self-evolving framework enabling collaborative ensembles to enhance reasoning via group emergent abilities.

Result: Achieves a 4.45 percentage point improvement over state-of-the-art monolithic systems on mathematical benchmarks.

Conclusion: Collaborative ecosystems are a promising frontier for advancing AI capabilities, with interaction as a key driver of intelligence.

Abstract: Progress in complex reasoning is constrained by the static nature of the
current training datasets. We propose structured interaction as a new scaling
axis, moving beyond the prevailing paradigm of increasing model parameters. Our
self-evolving framework, AGORA, enables a collaborative ensemble to achieve
reasoning performance exceeding state-of-the-art monolithic systems by up to
4.45 percentage points on challenging mathematical benchmarks. This gain stems
from group emergent ability-the synthesis of collective capabilities
unattainable by isolated models, validating interaction as a scalable driver of
intelligence. Our results position the engineering of collaborative ecosystems
as a vital frontier for capability emergence.

</details>


### [82] [LLM-Adapted Interpretation Framework for Machine Learning Models](https://arxiv.org/abs/2507.21179)
*Yuqi Jin,Zihan Hu,Weiteng Zhang,Weihao Xie,Jianwei Shuai,Xian Shen,Zhen Feng*

Main category: cs.LG

TL;DR: LAI-ML framework enhances interpretability of XGBoost for sarcopenia risk assessment, improving accuracy and generating transparent diagnostic narratives.


<details>
  <summary>Details</summary>
Motivation: Address the lack of interpretability in high-performance ML models like XGBoost to enable clinical adoption.

Method: Proposes LAI-ML, a knowledge distillation architecture using HAGA and CACS techniques to transform XGBoost feature attributions into probabilistic formats, then uses an LLM with reinforcement learning for narrative generation.

Result: Achieved 83% prediction accuracy (13% higher than baseline XGBoost) and corrected 21.7% of discordant cases.

Conclusion: LAI-ML successfully bridges the gap between accuracy and interpretability, providing a deployable solution for medical AI.

Abstract: Background & Aims: High-performance machine learning models like XGBoost are
often "black boxes," limiting their clinical adoption due to a lack of
interpretability. This study aims to bridge the gap between predictive accuracy
and narrative transparency for sarcopenia risk assessment. Methods: We propose
the LLM-Adapted Interpretation Framework (LAI-ML), a novel knowledge
distillation architecture. LAI-ML transforms feature attributions from a
trained XGBoost model into a probabilistic format using specialized techniques
(HAGA and CACS). A Large Language Model (LLM), guided by a reinforcement
learning loop and case-based retrieval, then generates data-faithful diagnostic
narratives. Results: The LAI-ML framework achieved 83% prediction accuracy,
significantly outperforming the baseline XGBoost model, 13% higher. Notably,
the LLM not only replicated the teacher model's logic but also corrected its
predictions in 21.7% of discordant cases, demonstrating enhanced reasoning.
Conclusion: LAI-ML effectively translates opaque model predictions into
trustworthy and interpretable clinical insights, offering a deployable solution
to the "black-box" problem in medical AI.

</details>


### [83] [MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge](https://arxiv.org/abs/2507.21183)
*Guangchen Lan,Sipeng Zhang,Tianle Wang,Yuwei Zhang,Daoan Zhang,Xinpeng Wei,Xiaoman Pan,Hongming Zhang,Dong-Jun Han,Christopher G. Brinton*

Main category: cs.LG

TL;DR: MaPPO is a new framework for aligning LLMs with human preferences by incorporating prior reward knowledge into optimization, outperforming existing methods like DPO without extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DPO treat preference learning as MLE, oversimplifying response classification. MaPPO aims to integrate prior knowledge for better alignment.

Method: MaPPO extends MLE to a Maximum a Posteriori objective, incorporating prior reward estimates, and works offline/online without new hyperparameters.

Result: MaPPO consistently improves alignment performance on benchmarks like MT-Bench and AlpacaEval 2.0, without sacrificing efficiency.

Conclusion: MaPPO generalizes and enhances existing methods, offering a robust, hyperparameter-free solution for preference optimization in LLMs.

Abstract: As the era of large language models (LLMs) on behalf of users unfolds,
Preference Optimization (PO) methods have become a central approach to aligning
LLMs with human preferences and improving performance. We propose Maximum a
Posteriori Preference Optimization (MaPPO), a framework for learning from
preferences that explicitly incorporates prior reward knowledge into the
optimization objective. While existing methods such as Direct Preference
Optimization (DPO) and its variants treat preference learning as a Maximum
Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating
prior reward estimates into a principled Maximum a Posteriori (MaP) objective.
This not only generalizes DPO and its variants, but also enhances alignment by
mitigating the oversimplified binary classification of responses. More
importantly, MaPPO introduces no additional hyperparameter, and supports
preference optimization in both offline and online settings. In addition, MaPPO
can be used as a plugin with consistent improvement on DPO variants, including
widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different
model sizes and model series on three standard benchmarks, including MT-Bench,
AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in
alignment performance without sacrificing computational efficiency.

</details>


### [84] [EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models](https://arxiv.org/abs/2507.21184)
*Haowei Lin,Xiangyu Wang,Jianzhu Ma,Yitao Liang*

Main category: cs.LG

TL;DR: EvoSLD is an automated framework for discovering scaling laws in neural networks using evolutionary algorithms and LLMs, outperforming manual methods and baselines in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional scaling law discovery requires extensive human expertise and manual effort, prompting the need for an automated solution.

Method: EvoSLD uses evolutionary algorithms guided by LLMs to co-evolve symbolic expressions and optimization routines, handling scaling and control variables across diverse settings.

Result: EvoSLD rediscovers human-derived laws in some cases and surpasses them in others, achieving significant error reductions on test sets.

Conclusion: EvoSLD offers superior accuracy, interpretability, and efficiency, potentially accelerating AI research.

Abstract: Scaling laws are fundamental mathematical relationships that predict how
neural network performance evolves with changes in variables such as model
size, dataset size, and computational resources. Traditionally, discovering
these laws requires extensive human expertise and manual experimentation. We
introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that
leverages evolutionary algorithms guided by Large Language Models (LLMs) to
co-evolve symbolic expressions and their optimization routines. Formulated to
handle scaling variables, control variables, and response metrics across
diverse experimental settings, EvoSLD searches for parsimonious, universal
functional forms that minimize fitting errors on grouped data subsets.
Evaluated on five real-world scenarios from recent literature, EvoSLD
rediscovers exact human-derived laws in two cases and surpasses them in others,
achieving up to orders-of-magnitude reductions in normalized mean squared error
on held-out test sets. Compared to baselines like symbolic regression and
ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and
efficiency, highlighting its potential to accelerate AI research. Code is
available at https://github.com/linhaowei1/SLD.

</details>


### [85] [Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs](https://arxiv.org/abs/2507.21188)
*Raj Krishnan Vijayaraj*

Main category: cs.LG

TL;DR: The paper introduces LAPD, a geometry-aware framework to evaluate clinical LLMs' robustness under adversarial edits, revealing latent fragility despite high static benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Clinical LLMs often fail under small input shifts (e.g., masking symptoms), which standard NLP metrics miss. This gap threatens reliability in safety-critical settings.

Method: Proposes LAPD with Latent Diagnosis Flip Rate (LDFR) to measure representational instability. Uses structured adversarial edits (masking, negation, etc.) on synthetic and real clinical notes.

Result: Latent fragility emerges even with minor input changes. LDFR generalizes to real clinical notes (DiReCT benchmark), exposing gaps in surface robustness.

Conclusion: Geometry-aware auditing (LAPD) is crucial for clinical AI safety, as latent instability persists despite high benchmark performance.

Abstract: LLMs for clinical decision support often fail under small but clinically
meaningful input shifts such as masking a symptom or negating a finding,
despite high performance on static benchmarks. These reasoning failures
frequently go undetected by standard NLP metrics, which are insensitive to
latent representation shifts that drive diagnosis instability. We propose a
geometry-aware evaluation framework, LAPD (Latent Agentic Perturbation
Diagnostics), which systematically probes the latent robustness of clinical
LLMs under structured adversarial edits. Within this framework, we introduce
Latent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that
captures representational instability when embeddings cross decision boundaries
in PCA-reduced latent space. Clinical notes are generated using a structured
prompting pipeline grounded in diagnostic reasoning, then perturbed along four
axes: masking, negation, synonym replacement, and numeric variation to simulate
common ambiguities and omissions. We compute LDFR across both foundation and
clinical LLMs, finding that latent fragility emerges even under minimal
surface-level changes. Finally, we validate our findings on 90 real clinical
notes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of
LDFR beyond synthetic settings. Our results reveal a persistent gap between
surface robustness and semantic stability, underscoring the importance of
geometry-aware auditing in safety-critical clinical AI.

</details>


### [86] [Operator-Based Machine Intelligence: A Hilbert Space Framework for Spectral Learning and Symbolic Reasoning](https://arxiv.org/abs/2507.21189)
*Andrew Kiruluta,Andreas Lemos,Priscilla Burity*

Main category: cs.LG

TL;DR: The paper explores learning tasks in infinite-dimensional Hilbert spaces, using tools from functional analysis and spectral theory, as an alternative to traditional neural networks.


<details>
  <summary>Details</summary>
Motivation: To move beyond finite-dimensional parameter spaces and leverage infinite-dimensional Hilbert spaces for more expressive and interpretable machine learning.

Method: Uses Reproducing Kernel Hilbert Spaces (RKHS), spectral operator learning, and wavelet-domain representations, with models like scattering transforms and Koopman operators.

Result: Provides a rigorous mathematical framework for learning in Hilbert spaces, comparing advantages and limitations against traditional neural networks.

Conclusion: Proposes scalable and interpretable machine learning directions based on Hilbertian signal processing.

Abstract: Traditional machine learning models, particularly neural networks, are rooted
in finite-dimensional parameter spaces and nonlinear function approximations.
This report explores an alternative formulation where learning tasks are
expressed as sampling and computation in infinite dimensional Hilbert spaces,
leveraging tools from functional analysis, signal processing, and spectral
theory. We review foundational concepts such as Reproducing Kernel Hilbert
Spaces (RKHS), spectral operator learning, and wavelet-domain representations.
We present a rigorous mathematical formulation of learning in Hilbert spaces,
highlight recent models based on scattering transforms and Koopman operators,
and discuss advantages and limitations relative to conventional neural
architectures. The report concludes by outlining directions for scalable and
interpretable machine learning grounded in Hilbertian signal processing.

</details>


### [87] [Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals](https://arxiv.org/abs/2507.21190)
*Andrew Kiruluta,Andreas Lemos,Priscilla Burity*

Main category: cs.LG

TL;DR: A non-neural learning framework using Graph Laplacian Wavelet Transforms (GLWT) for graph-based tasks, offering interpretability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To provide a transparent and resource-efficient alternative to neural networks for graph learning.

Method: Uses GLWT for signal decomposition, nonlinear shrinkage, and symbolic logic over wavelet coefficients, with a domain-specific language (DSL) for reasoning.

Result: Competitive performance in denoising and token classification tasks compared to lightweight GNNs, with greater transparency.

Conclusion: Proposes a principled, interpretable, and efficient non-neural approach for graph learning.

Abstract: We present a fully non neural learning framework based on Graph Laplacian
Wavelet Transforms (GLWT). Unlike traditional architectures that rely on
convolutional, recurrent, or attention based neural networks, our model
operates purely in the graph spectral domain using structured multiscale
filtering, nonlinear shrinkage, and symbolic logic over wavelet coefficients.
Signals defined on graph nodes are decomposed via GLWT, modulated with
interpretable nonlinearities, and recombined for downstream tasks such as
denoising and token classification. The system supports compositional reasoning
through a symbolic domain-specific language (DSL) over graph wavelet
activations. Experiments on synthetic graph denoising and linguistic token
graphs demonstrate competitive performance against lightweight GNNs with far
greater transparency and efficiency. This work proposes a principled,
interpretable, and resource-efficient alternative to deep neural architectures
for learning on graphs.

</details>


### [88] [Exploring Adaptive Structure Learning for Heterophilic Graphs](https://arxiv.org/abs/2507.21191)
*Garv Kaushik*

Main category: cs.LG

TL;DR: The paper proposes structure learning to rewire edges in shallow GCNs to capture long-range dependencies in heterophilic graphs, though it lacks generalizability.


<details>
  <summary>Details</summary>
Motivation: Improving GCN performance on heterophilic graphs by addressing limitations in capturing long-range dependencies due to localized feature aggregation.

Method: Parameterizing the adjacency matrix to learn connections between non-local nodes and extending the hop span of shallow GCNs.

Result: The method captures long-range dependencies but performs inconsistently on node classification tasks and lacks generalizability across heterophilic graphs.

Conclusion: While effective for specific cases, the method's inconsistency and lack of generalizability highlight the need for further research.

Abstract: Graph Convolutional Networks (GCNs) gained traction for graph representation
learning, with recent attention on improving performance on heterophilic graphs
for various real-world applications. The localized feature aggregation in a
typical message-passing paradigm hinders the capturing of long-range
dependencies between non-local nodes of the same class. The inherent
connectivity structure in heterophilic graphs often conflicts with information
sharing between distant nodes of same class. We propose structure learning to
rewire edges in shallow GCNs itself to avoid performance degradation in
downstream discriminative tasks due to oversmoothing. Parameterizing the
adjacency matrix to learn connections between non-local nodes and extend the
hop span of shallow GCNs facilitates the capturing of long-range dependencies.
However, our method is not generalizable across heterophilic graphs and
performs inconsistently on node classification task contingent to the graph
structure.

</details>


### [89] [EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks](https://arxiv.org/abs/2507.21196)
*Abir Ray*

Main category: cs.LG

TL;DR: EdgeAgentX-DT enhances edge intelligence in military networks using digital twins and generative AI for robust training and validation.


<details>
  <summary>Details</summary>
Motivation: To improve edge intelligence in contested military environments by integrating digital twins and generative AI for realistic and adversarial training.

Method: Uses network digital twins synchronized with real-world devices and generative AI (diffusion models, transformers) for scenario training. Multi-layer architecture includes edge intelligence, twin synchronization, and generative training.

Result: Faster learning convergence, higher throughput, reduced latency, and improved resilience against jamming and failures. Outperforms baseline in tactical scenarios.

Conclusion: EdgeAgentX-DT demonstrates the effectiveness of digital-twin-enabled generative training for robust edge AI in contested environments.

Abstract: We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework
that integrates digital twin simulations and generative AI-driven scenario
training to significantly enhance edge intelligence in military networks.
EdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized
with real-world edge devices, to provide a secure, realistic environment for
training and validation. Leveraging generative AI methods, such as diffusion
models and transformers, the system creates diverse and adversarial scenarios
for robust simulation-based agent training. Our multi-layer architecture
includes: (1) on-device edge intelligence; (2) digital twin synchronization;
and (3) generative scenario training. Experimental simulations demonstrate
notable improvements over EdgeAgentX, including faster learning convergence,
higher network throughput, reduced latency, and improved resilience against
jamming and node failures. A case study involving a complex tactical scenario
with simultaneous jamming attacks, agent failures, and increased network loads
illustrates how EdgeAgentX-DT sustains operational performance, whereas
baseline methods fail. These results highlight the potential of
digital-twin-enabled generative training to strengthen edge AI deployments in
contested environments.

</details>


### [90] [AdaptHetero: Machine Learning Interpretation-Driven Subgroup Adaptation for EHR-Based Clinical Prediction](https://arxiv.org/abs/2507.21197)
*Ling Liao,Eva Aagaard*

Main category: cs.LG

TL;DR: AdaptHetero is an MLI-driven framework that uses interpretability insights to tailor model training for EHR subpopulations, improving predictive performance.


<details>
  <summary>Details</summary>
Motivation: The complexity and heterogeneity of EHR data limit the effectiveness of machine learning interpretation in guiding subgroup-specific modeling.

Method: The framework integrates SHAP-based interpretation and unsupervised clustering to identify subgroup-specific characteristics.

Result: Evaluated on three datasets, AdaptHetero identifies heterogeneous model behaviors and improves predictive performance for ICU mortality, in-hospital death, and hidden hypoxemia.

Conclusion: AdaptHetero effectively transforms interpretability insights into actionable guidance for subgroup-specific modeling in EHRs.

Abstract: Machine learning interpretation has primarily been leveraged to build
clinician trust and uncover actionable insights in EHRs. However, the intrinsic
complexity and heterogeneity of EHR data limit its effectiveness in guiding
subgroup-specific modeling. We propose AdaptHetero, a novel MLI-driven
framework that transforms interpretability insights into actionable guidance
for tailoring model training and evaluation across subpopulations within
individual hospital systems. Evaluated on three large-scale EHR datasets -
GOSSIS-1-eICU, WiDS, and MIMIC-IV - AdaptHetero consistently identifies
heterogeneous model behaviors in predicting ICU mortality, in-hospital death,
and hidden hypoxemia. By integrating SHAP-based interpretation and unsupervised
clustering, the framework enhances the identification of clinically meaningful
subgroup-specific characteristics, leading to improved predictive performance.

</details>


### [91] [Uncovering Gradient Inversion Risks in Practical Language Model Training](https://arxiv.org/abs/2507.21198)
*Xinguo Feng,Zhongkui Ma,Zihan Wang,Eu Joe Chegne,Mengyao Ma,Alsharif Abuadbba,Guangdong Bai*

Main category: cs.LG

TL;DR: Grab, a gradient inversion attack for language models in FL, recovers up to 92.9% of private data, outperforming prior methods by 28.9%-48.5%.


<details>
  <summary>Details</summary>
Motivation: Privacy threats in FL for language models are underestimated due to discrete token challenges.

Method: Grab uses hybrid optimization: alternating dropout mask optimization and discrete token sequencing.

Result: Achieves up to 92.9% recovery rate, surpassing prior methods significantly.

Conclusion: Grab advances understanding of privacy risks in FL for language models.

Abstract: The gradient inversion attack has been demonstrated as a significant privacy
threat to federated learning (FL), particularly in continuous domains such as
vision models. In contrast, it is often considered less effective or highly
dependent on impractical training settings when applied to language models, due
to the challenges posed by the discrete nature of tokens in text data. As a
result, its potential privacy threats remain largely underestimated, despite FL
being an emerging training method for language models. In this work, we propose
a domain-specific gradient inversion attack named Grab (gradient inversion with
hybrid optimization). Grab features two alternating optimization processes to
address the challenges caused by practical training settings, including a
simultaneous optimization on dropout masks between layers for improved token
recovery and a discrete optimization for effective token sequencing. Grab can
recover a significant portion (up to 92.9% recovery rate) of the private
training data, outperforming the attack strategy of utilizing discrete
optimization with an auxiliary model by notable improvements of up to 28.9%
recovery rate in benchmark settings and 48.5% recovery rate in practical
settings. Grab provides a valuable step forward in understanding this privacy
threat in the emerging FL training mode of language models.

</details>


### [92] [Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications](https://arxiv.org/abs/2507.21199)
*Xinye Cao,Hongcan Guo,Guoshun Nan,Jiaoyang Cui,Haoting Qian,Yihan Lin,Yilin Peng,Diyang Zhang,Yanzhao Hou,Huici Wu,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: The paper introduces a single compositional LLM for diverse IMAs, proposing ContextLoRA for task adaptation and ContextGear for efficiency, validated on benchmarks and a real-world testbed.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of using multiple LLMs for IMAs by developing a unified approach adaptable to diverse tasks in resource-constrained environments.

Method: Proposes ContextLoRA for structured context learning via task dependency graphs and ContextGear for optimized training, with step-by-step fine-tuning.

Result: Superior performance on benchmarks and practical applicability demonstrated on a wireless testbed.

Conclusion: The paradigm offers an efficient, flexible solution for IMAs using a single LLM, with code to be released.

Abstract: Interactive multimodal applications (IMAs), such as route planning in the
Internet of Vehicles, enrich users' personalized experiences by integrating
various forms of data over wireless networks. Recent advances in large language
models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple
IMAs, with each LLM trained individually for a specific task that presents
different business workflows. In contrast to existing approaches that rely on
multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes
various IMAs using a single compositional LLM over wireless networks. The two
primary challenges include 1) guiding a single LLM to adapt to diverse IMA
objectives and 2) ensuring the flexibility and efficiency of the LLM in
resource-constrained mobile environments. To tackle the first challenge, we
propose ContextLoRA, a novel method that guides an LLM to learn the rich
structured context among IMAs by constructing a task dependency graph. We
partition the learnable parameter matrix of neural layers for each IMA to
facilitate LLM composition. Then, we develop a step-by-step fine-tuning
procedure guided by task relations, including training, freezing, and masking
phases. This allows the LLM to learn to reason among tasks for better
adaptation, capturing the latent dependencies between tasks. For the second
challenge, we introduce ContextGear, a scheduling strategy to optimize the
training procedure of ContextLoRA, aiming to minimize computational and
communication costs through a strategic grouping mechanism. Experiments on
three benchmarks show the superiority of the proposed ContextLoRA and
ContextGear. Furthermore, we prototype our proposed paradigm on a real-world
wireless testbed, demonstrating its practical applicability for various IMAs.
We will release our code to the community.

</details>


### [93] [Learning from Limited and Imperfect Data](https://arxiv.org/abs/2507.21205)
*Harsh Rangwani*

Main category: cs.LG

TL;DR: The paper addresses the challenge of training deep models on real-world, imbalanced data by developing robust algorithms for diverse scenarios, including generative modeling, inductive regularization, semi-supervised learning, and domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Real-world data is often imbalanced and imperfect, unlike curated datasets. Existing algorithms perform poorly on such data, necessitating robust methods to reduce reliance on labor-intensive curation.

Method: The thesis proposes four approaches: 1) Learning generative models for long-tail data, 2) Inductive regularization for tail-class generalization, 3) Optimizing metrics for semi-supervised learning, and 4) Efficient domain adaptation with minimal labeled data.

Result: The developed algorithms mitigate mode-collapse in generative models, improve tail-class generalization, optimize learning metrics, and enable domain adaptation with few or no labeled samples.

Conclusion: The work advances deep learning for real-world data by addressing imbalances and distribution shifts, reducing the need for curated datasets.

Abstract: The distribution of data in the world (eg, internet, etc.) significantly
differs from the well-curated datasets and is often over-populated with samples
from common categories. The algorithms designed for well-curated datasets
perform suboptimally when used for learning from imperfect datasets with
long-tailed imbalances and distribution shifts. To expand the use of deep
models, it is essential to overcome the labor-intensive curation process by
developing robust algorithms that can learn from diverse, real-world data
distributions. Toward this goal, we develop practical algorithms for Deep
Neural Networks which can learn from limited and imperfect data present in the
real world. This thesis is divided into four segments, each covering a scenario
of learning from limited or imperfect data. The first part of the thesis
focuses on Learning Generative Models from Long-Tail Data, where we mitigate
the mode-collapse and enable diverse aesthetic image generations for tail
(minority) classes. In the second part, we enable effective generalization on
tail classes through Inductive Regularization schemes, which allow tail classes
to generalize as effectively as the head classes without requiring explicit
generation of images. In the third part, we develop algorithms for Optimizing
Relevant Metrics for learning from long-tailed data with limited annotation
(semi-supervised), followed by the fourth part, which focuses on the Efficient
Domain Adaptation of the model to various domains with very few to zero labeled
samples.

</details>


### [94] [Bubbleformer: Forecasting Boiling with Transformers](https://arxiv.org/abs/2507.21244)
*Sheikh Md Shakeel Hassan,Xianwei Zou,Akash Dhruv,Vishwanath Ganesan,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: Bubbleformer is a transformer-based model for forecasting boiling dynamics without relying on simulation data during inference, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing neural PDE surrogates struggle with learning nucleation from past states and modeling flow boiling velocity fields, limiting autonomous forecasting.

Method: Bubbleformer uses factorized axial attention, frequency-aware scaling, and thermophysical parameter conditioning to generalize across diverse conditions.

Result: Bubbleformer achieves benchmark results in predicting and forecasting two-phase boiling flows, validated by physics-based metrics.

Conclusion: Bubbleformer advances boiling dynamics modeling with improved autonomy and generalization, supported by the high-fidelity BubbleML 2.0 dataset.

Abstract: Modeling boiling (an inherently chaotic, multiphase process central to energy
and thermal systems) remains a significant challenge for neural PDE surrogates.
Existing models require future input (e.g., bubble positions) during inference
because they fail to learn nucleation from past states, limiting their ability
to autonomously forecast boiling dynamics. They also fail to model flow boiling
velocity fields, where sharp interface-momentum coupling demands long-range and
directional inductive biases. We introduce Bubbleformer, a transformer-based
spatiotemporal model that forecasts stable and long-range boiling dynamics
including nucleation, interface evolution, and heat transfer without dependence
on simulation data during inference. Bubbleformer integrates factorized axial
attention, frequency-aware scaling, and conditions on thermophysical parameters
to generalize across fluids, geometries, and operating conditions. To evaluate
physical fidelity in chaotic systems, we propose interpretable physics-based
metrics that evaluate heat-flux consistency, interface geometry, and mass
conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans
diverse working fluids (cryogens, refrigerants, dielectrics), boiling
configurations (pool and flow boiling), flow regimes (bubbly, slug, annular),
and boundary conditions. Bubbleformer sets new benchmark results in both
prediction and forecasting of two-phase boiling flows.

</details>


### [95] [Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors](https://arxiv.org/abs/2507.21260)
*Amartya Banerjee,Xingyu Xu,Caroline Moosmüller,Harlin Lee*

Main category: cs.LG

TL;DR: Adam-PnP is a Plug-and-Play framework for guiding protein diffusion models with heterogeneous experimental data, featuring adaptive noise estimation and dynamic weighting to reduce manual tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of integrating noisy experimental data from multiple sources into deep generative models for protein structure recovery.

Method: Introduces Adam-PnP, a framework with adaptive noise estimation and dynamic modality weighting to guide pre-trained diffusion models.

Result: Demonstrates significantly improved accuracy in complex reconstruction tasks.

Conclusion: Adam-PnP effectively reduces manual hyperparameter tuning and enhances protein structure recovery.

Abstract: In an inverse problem, the goal is to recover an unknown parameter (e.g., an
image) that has typically undergone some lossy or noisy transformation during
measurement. Recently, deep generative models, particularly diffusion models,
have emerged as powerful priors for protein structure generation. However,
integrating noisy experimental data from multiple sources to guide these models
remains a significant challenge. Existing methods often require precise
knowledge of experimental noise levels and manually tuned weights for each data
modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that
guides a pre-trained protein diffusion model using gradients from multiple,
heterogeneous experimental sources. Our framework features an adaptive noise
estimation scheme and a dynamic modality weighting mechanism integrated into
the diffusion process, which reduce the need for manual hyperparameter tuning.
Experiments on complex reconstruction tasks demonstrate significantly improved
accuracy using Adam-PnP.

</details>


### [96] [Deep Polynomial Chaos Expansion](https://arxiv.org/abs/2507.21273)
*Johannes Exenberger,Sascha Ranftl,Robert Peharz*

Main category: cs.LG

TL;DR: DeepPCE combines PCE with probabilistic circuits to scale PCE for high-dimensional problems, matching MLP performance while retaining exact statistical inference.


<details>
  <summary>Details</summary>
Motivation: PCE struggles with high-dimensionality due to exponential growth of basis functions, limiting its scalability.

Method: DeepPCE integrates PCE with probabilistic circuits, enabling deep generalization for high-dimensional inputs.

Result: DeepPCE achieves MLP-like predictive performance and retains PCE's exact statistical inference capabilities.

Conclusion: DeepPCE effectively scales PCE to high dimensions, maintaining performance and inference advantages.

Abstract: Polynomial chaos expansion (PCE) is a classical and widely used surrogate
modeling technique in physical simulation and uncertainty quantification. By
taking a linear combination of a set of basis polynomials - orthonormal with
respect to the distribution of uncertain input parameters - PCE enables
tractable inference of key statistical quantities, such as (conditional) means,
variances, covariances, and Sobol sensitivity indices, which are essential for
understanding the modeled system and identifying influential parameters and
their interactions. As the number of basis functions grows exponentially with
the number of parameters, PCE does not scale well to high-dimensional problems.
We address this challenge by combining PCE with ideas from probabilistic
circuits, resulting in the deep polynomial chaos expansion (DeepPCE) - a deep
generalization of PCE that scales effectively to high-dimensional input spaces.
DeepPCE achieves predictive performance comparable to that of multi-layer
perceptrons (MLPs), while retaining PCE's ability to compute exact statistical
inferences via simple forward passes.

</details>


### [97] [Large Language Model-Enhanced Reinforcement Learning for Diverse and Novel Recommendations](https://arxiv.org/abs/2507.21274)
*Jiin Woo,Alireza Bagheri Garakani,Tianchen Zhou,Zhishen Huang,Yan Gao*

Main category: cs.LG

TL;DR: LAAC uses LLMs to guide novel item suggestions in recommendation systems, refining them with a lightweight policy via bilevel optimization, improving diversity and novelty without costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Diversity and novelty in recommendations are often overshadowed by click relevance, and existing RL methods rely on random exploration. LAAC addresses this by leveraging LLMs for better alignment with user interests.

Method: LAAC combines LLMs as reference policies with a lightweight policy trained via bilevel optimization. Regularization prevents overestimation of unreliable LLM suggestions.

Result: LAAC outperforms baselines in diversity, novelty, and accuracy, and remains robust on imbalanced data.

Conclusion: LAAC effectively integrates LLM knowledge into recommendation systems, enhancing diversity and novelty without expensive fine-tuning.

Abstract: In recommendation systems, diversity and novelty are essential for capturing
varied user preferences and encouraging exploration, yet many systems
prioritize click relevance. While reinforcement learning (RL) has been explored
to improve diversity, it often depends on random exploration that may not align
with user interests. We propose LAAC (LLM-guided Adversarial Actor Critic), a
novel method that leverages large language models (LLMs) as reference policies
to suggest novel items, while training a lightweight policy to refine these
suggestions using system-specific data. The method formulates training as a
bilevel optimization between actor and critic networks, enabling the critic to
selectively favor promising novel actions and the actor to improve its policy
beyond LLM recommendations. To mitigate overestimation of unreliable LLM
suggestions, we apply regularization that anchors critic values for unexplored
items close to well-estimated dataset actions. Experiments on real-world
datasets show that LAAC outperforms existing baselines in diversity, novelty,
and accuracy, while remaining robust on imbalanced data, effectively
integrating LLM knowledge without expensive fine-tuning.

</details>


### [98] [Blending data and physics for reduced-order modeling of systems with spatiotemporal chaotic dynamics](https://arxiv.org/abs/2507.21299)
*Alex Guo,Michael D. Graham*

Main category: cs.LG

TL;DR: A hybrid reduced-order model (ROM) combines data and full-order physics to improve predictions of chaotic dynamics, outperforming data-only methods.


<details>
  <summary>Details</summary>
Motivation: To enhance predictive capability by integrating known physics (full-order model) with data-driven techniques for chaotic systems.

Method: Develops a hybrid ROM using an autoencoder for manifold coordinates, projects FOM vector fields, and corrects with data or updates via Bayesian prior, employing neural ODEs.

Result: Hybrid ROM significantly improves time-series predictions in scenarios of abundant/scarce data or incorrect FOM parameters, tested on Kuramoto-Sivashinsky and Ginzburg-Landau equations.

Conclusion: Combining physics and data in hybrid ROMs offers superior predictive performance for chaotic dynamics, even with imperfect models or limited data.

Abstract: While data-driven techniques are powerful tools for reduced-order modeling of
systems with chaotic dynamics, great potential remains for leveraging known
physics (i.e. a full-order model (FOM)) to improve predictive capability. We
develop a hybrid reduced order model (ROM), informed by both data and FOM, for
evolving spatiotemporal chaotic dynamics on an invariant manifold whose
coordinates are found using an autoencoder. This approach projects the vector
field of the FOM onto the invariant manifold; then, this physics-derived vector
field is either corrected using dynamic data, or used as a Bayesian prior that
is updated with data. In both cases, the neural ordinary differential equation
approach is used. We consider simulated data from the Kuramoto-Sivashinsky and
complex Ginzburg-Landau equations. Relative to the data-only approach, for
scenarios of abundant data, scarce data, and even an incorrect FOM (i.e.
erroneous parameter values), the hybrid approach yields substantially improved
time-series predictions.

</details>


### [99] [DEM-NeRF: A Neuro-Symbolic Method for Scientific Discovery through Physics-Informed Simulation](https://arxiv.org/abs/2507.21350)
*Wenkai Tan,Alvaro Velasquez,Houbing Song*

Main category: cs.LG

TL;DR: A neuro-symbolic framework combines neural networks with symbolic physics to reconstruct and simulate elastic objects from sparse images, integrating NeRF for reconstruction and PINN for physics constraints.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap between purely empirical methods (risking deviation from physics) and traditional solvers (requiring full geometric knowledge and high computational cost).

Method: Uses neural radiance fields (NeRF) for object reconstruction and physics-informed neural networks (PINN) with elasticity equations. An energy-constrained PINN handles complex boundary conditions.

Result: Learns spatiotemporal representations of deforming objects, balancing image supervision and physical constraints for accurate, explainable simulations.

Conclusion: The framework effectively bridges data-driven learning and symbolic physics, enhancing simulation accuracy and explainability without explicit geometric data.

Abstract: Neural networks have emerged as a powerful tool for modeling physical
systems, offering the ability to learn complex representations from limited
data while integrating foundational scientific knowledge. In particular,
neuro-symbolic approaches that combine data-driven learning, the neuro, with
symbolic equations and rules, the symbolic, address the tension between methods
that are purely empirical, which risk straying from established physical
principles, and traditional numerical solvers that demand complete geometric
knowledge and can be prohibitively expensive for high-fidelity simulations. In
this work, we present a novel neuro-symbolic framework for reconstructing and
simulating elastic objects directly from sparse multi-view image sequences,
without requiring explicit geometric information. Specifically, we integrate a
neural radiance field (NeRF) for object reconstruction with physics-informed
neural networks (PINN) that incorporate the governing partial differential
equations of elasticity. In doing so, our method learns a spatiotemporal
representation of deforming objects that leverages both image supervision and
symbolic physical constraints. To handle complex boundary and initial
conditions, which are traditionally confronted using finite element methods,
boundary element methods, or sensor-based measurements, we employ an
energy-constrained Physics-Informed Neural Network architecture. This design
enhances both simulation accuracy and the explainability of results.

</details>


### [100] [A Contrastive Diffusion-based Network (CDNet) for Time Series Classification](https://arxiv.org/abs/2507.21357)
*Yaoyu Zhang,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: CDNet, a Contrastive Diffusion-based Network, improves deep learning classifiers for time series classification by generating informative samples via a learned diffusion process, enhancing performance under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for time series classification struggle with class similarity, multimodal distributions, and noise. CDNet addresses these limitations by leveraging contrastive diffusion.

Method: CDNet uses convolutional approximations of reverse diffusion steps to learn transitions between samples, introduces a CNN-based mechanism for denoising and mode coverage, and employs an uncertainty-weighted composite loss for training.

Result: Experiments on the UCR Archive and simulated datasets show CDNet significantly outperforms state-of-the-art classifiers, especially in noisy, similar, and multimodal scenarios.

Conclusion: CDNet effectively enhances classifier robustness and performance under challenging data conditions, advancing time series classification.

Abstract: Deep learning models are widely used for time series classification (TSC) due
to their scalability and efficiency. However, their performance degrades under
challenging data conditions such as class similarity, multimodal distributions,
and noise. To address these limitations, we propose CDNet, a Contrastive
Diffusion-based Network that enhances existing classifiers by generating
informative positive and negative samples via a learned diffusion process.
Unlike traditional diffusion models that denoise individual samples, CDNet
learns transitions between samples--both within and across classes--through
convolutional approximations of reverse diffusion steps. We introduce a
theoretically grounded CNN-based mechanism to enable both denoising and mode
coverage, and incorporate an uncertainty-weighted composite loss for robust
training. Extensive experiments on the UCR Archive and simulated datasets
demonstrate that CDNet significantly improves state-of-the-art (SOTA) deep
learning classifiers, particularly under noisy, similar, and multimodal
conditions.

</details>


### [101] [Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2507.21386)
*Xuan Wu,Di Wang,Chunguo Wu,Kaifang Qi,Chunyan Miao,Yubin Xiao,Jian Zhang,You Zhou*

Main category: cs.LG

TL;DR: ECHO is a Neural Combinatorial Optimization solver for the min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP), addressing limitations like myopic decisions and overlooked properties by leveraging dual-modality encoding, cross-attention, and tailored data augmentation.


<details>
  <summary>Details</summary>
Motivation: Existing solvers for MMHCVRP often make myopic decisions and ignore key problem properties, leading to suboptimal performance.

Method: ECHO uses a dual-modality node encoder, Parameter-Free Cross-Attention, and tailored data augmentation to improve decision-making and training stability.

Result: ECHO outperforms state-of-the-art solvers in experiments, showing strong generalization across vehicle and node scales.

Conclusion: The proposed methods in ECHO effectively address MMHCVRP challenges, validated by ablation studies.

Abstract: Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed
to address Vehicle Routing Problems (VRPs). However, most of these solvers
focus exclusively on single-vehicle VRP variants, overlooking the more
realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP),
which involves multiple vehicles. Existing MMHCVRP solvers typically select a
vehicle and its next node to visit at each decoding step, but often make myopic
decoding decisions and overlook key properties of MMHCVRP, including local
topological relationships, vehicle permutation invariance, and node symmetry,
resulting in suboptimal performance. To better address these limitations, we
propose ECHO, an efficient NCO solver. First, ECHO exploits the proposed
dual-modality node encoder to capture local topological relationships among
nodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed
Parameter-Free Cross-Attention mechanism to prioritize the vehicle selected in
the preceding decoding step. Finally, leveraging vehicle permutation invariance
and node symmetry, we introduce a tailored data augment strategy for MMHCVRP to
stabilize the Reinforcement Learning training process. To assess the
performance of ECHO, we conduct extensive experiments. The experimental results
demonstrate that ECHO outperforms state-of-the-art NCO solvers across varying
numbers of vehicles and nodes, and exhibits well-performing generalization
across both scales and distribution patterns. Finally, ablation studies
validate the effectiveness of all proposed methods.

</details>


### [102] [Systolic Array-based Accelerator for State-Space Models](https://arxiv.org/abs/2507.21394)
*Shiva Raja,Cansu Demirkiran,Aakash Sarkar,Milos Popovic,Ajay Joshi*

Main category: cs.LG

TL;DR: The paper introduces EpochCore, a hardware accelerator for State-Space Models (SSMs), achieving significant performance and energy efficiency improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing models (RNNs, CNNs, Transformers) struggle with long sequences due to memory limitations. SSMs offer better efficiency but are compute-intensive, requiring specialized hardware.

Method: EpochCore uses systolic arrays and a versatile processing element (LIMA-PE) with a novel dataflow (ProDF) to optimize SSM execution.

Result: EpochCore achieves 250x performance gains, 45x energy efficiency, and ~2,000x latency improvement over GPUs.

Conclusion: EpochCore is a promising solution for accelerating SSMs, balancing performance, efficiency, and area cost.

Abstract: Sequence modeling is crucial for AI to understand temporal data and detect
complex time-dependent patterns. While recurrent neural networks (RNNs),
convolutional neural networks (CNNs), and Transformers have advanced in
capturing long-range dependencies, they struggle with achieving high accuracy
with very long sequences due to limited memory retention (fixed context
window). State-Space Models (SSMs) leverage exponentially decaying memory
enabling lengthy context window and so they process very long data sequences
more efficiently than recurrent and Transformer-based models. Unlike
traditional neural models like CNNs and RNNs, SSM-based models require solving
differential equations through continuous integration, making training and
inference both compute- and memory-intensive on conventional CPUs and GPUs. In
this paper we introduce a specialized hardware accelerator, EpochCore, for
accelerating SSMs. EpochCore is based on systolic arrays (SAs) and is designed
to enhance the energy efficiency and throughput of inference of SSM-based
models for long-range sequence tasks. Within the SA, we propose a versatile
processing element (PE) called LIMA-PE to perform traditional and specialized
MAC operations to support traditional DNNs and SSMs. To complement the
EpochCore microarchitecture, we propose a novel dataflow, ProDF, which enables
highly efficient execution of SSM-based models. By leveraging the LIMA-PE
microarchitecture and ProDF, EpochCore achieves on average 250x gains in
performance and 45x improvement in energy efficiency, at the expense of 2x
increase in area cost over traditional SA-based accelerators, and around
~2,000x improvement in latency/inference on LRA datasets compared to GPU kernel
operations.

</details>


### [103] [Enabling Pareto-Stationarity Exploration in Multi-Objective Reinforcement Learning: A Multi-Objective Weighted-Chebyshev Actor-Critic Approach](https://arxiv.org/abs/2507.21397)
*Fnu Hairi,Jiao Yang,Tianchen Zhou,Haibo Yang,Chaosheng Dong,Fan Yang,Michinari Momma,Yan Gao,Jia Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In many multi-objective reinforcement learning (MORL) applications, being
able to systematically explore the Pareto-stationary solutions under multiple
non-convex reward objectives with theoretical finite-time sample complexity
guarantee is an important and yet under-explored problem. This motivates us to
take the first step and fill the important gap in MORL. Specifically, in this
paper, we propose a \uline{M}ulti-\uline{O}bjective weighted-\uline{CH}ebyshev
\uline{A}ctor-critic (MOCHA) algorithm for MORL, which judiciously integrates
the weighted-Chebychev (WC) and actor-critic framework to enable
Pareto-stationarity exploration systematically with finite-time sample
complexity guarantee. Sample complexity result of MOCHA algorithm reveals an
interesting dependency on $p_{\min}$ in finding an $\epsilon$-Pareto-stationary
solution, where $p_{\min}$ denotes the minimum entry of a given weight vector
$\mathbf{p}$ in WC-scarlarization. By carefully choosing learning rates, the
sample complexity for each exploration can be
$\tilde{\mathcal{O}}(\epsilon^{-2})$. Furthermore, simulation studies on a
large KuaiRand offline dataset, show that the performance of MOCHA algorithm
significantly outperforms other baseline MORL approaches.

</details>


### [104] [Data Leakage and Redundancy in the LIT-PCBA Benchmark](https://arxiv.org/abs/2507.21404)
*Amber Huang,Ian Scott Knight,Slava Naprienko*

Main category: cs.LG

TL;DR: The paper audits LIT-PCBA, a virtual screening benchmark, revealing severe flaws like data leakage, duplication, and analog redundancy, making it unreliable for fair model evaluation. A trivial memorization-based baseline outperforms state-of-the-art models, highlighting the dataset's inadequacy.


<details>
  <summary>Details</summary>
Motivation: To expose critical flaws in the LIT-PCBA benchmark that compromise its validity for evaluating virtual screening models, undermining previous research relying on it.

Method: Conducted an audit identifying data leakage, duplication, and structural redundancy. Implemented a memorization-based baseline to demonstrate the dataset's flaws.

Result: Found 2,491 duplicated inactives, 3 leaked query ligands, and 323 highly similar active pairs in ALDH1. The trivial baseline outperformed advanced models like CHEESE.

Conclusion: LIT-PCBA is unfit for its intended purpose due to fundamental flaws. The findings question prior results and advocate for more rigorous dataset development.

Abstract: LIT-PCBA is a widely used benchmark for virtual screening, but our audit
reveals it is fundamentally compromised. The dataset suffers from egregious
data leakage, rampant duplication, and pervasive analog redundancy -- flaws
that invalidate its use for fair model evaluation. Notably, we identify 2,491
inactives duplicated across training and validation sets, and thousands more
repeated within individual data splits (2,945 in training, 789 in validation).
Critically, three ligands in the query set -- meant to represent unseen test
cases -- are leaked: two appear in the training set, one in validation.
Structural redundancy compounds these issues: for some targets, over 80% of
query ligands are near duplicates, with Tanimoto similarity >= 0.9. In ALDH1
alone, we find 323 highly similar active pairs between training and validation
sets, invalidating claims of chemical diversity. These and other flaws
collectively cause models trained on LIT-PCBA to memorize rather than
generalize. To demonstrate the consequences of these data integrity failures,
we implement a trivial memorization-based baseline -- using no learning, no
physics, and no modeling -- that outperforms state-of-the-art models, including
deep neural networks like CHEESE, on LIT-PCBA simply by exploiting these
artifacts. Our findings render the benchmark unfit for its intended purpose and
call into question previous results based on its use. We share this audit to
raise awareness and provide tooling to help the community develop more rigorous
and reliable datasets going forward. All scripts necessary to reproduce our
audit and the baseline implementation are available at:
https://github.com/sievestack/LIT-PCBA-audit

</details>


### [105] [Torque-based Graph Surgery:Enhancing Graph Neural Networks with Hierarchical Rewiring](https://arxiv.org/abs/2507.21422)
*Sujia Huang,Lele Fu,Zhen Cui,Tong Zhang,Na Song,Bo Huang*

Main category: cs.LG

TL;DR: A torque-driven hierarchical rewiring strategy for GNNs improves representation learning in heterophilous and noisy graphs by dynamically adjusting message passing based on interference-aware torque metrics.


<details>
  <summary>Details</summary>
Motivation: Native graph interactions may hinder effective message passing in GNNs, especially in heterophilous and noisy graphs, necessitating graph rewiring methods.

Method: Proposes a torque-driven rewiring strategy using an interference-aware torque metric to prune high-torque edges and add low-torque links, optimizing node receptive fields.

Result: Outperforms state-of-the-art methods on heterophilous, homophilous, and noisy graphs, enhancing robustness and accuracy.

Conclusion: The torque-driven rewiring strategy effectively improves GNN performance by dynamically modulating message passing, demonstrating broad applicability.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning from
graph-structured data, leveraging message passing to diffuse information and
update node representations. However, most efforts have suggested that native
interactions encoded in the graph may not be friendly for this process,
motivating the development of graph rewiring methods. In this work, we propose
a torque-driven hierarchical rewiring strategy, inspired by the notion of
torque in classical mechanics, dynamically modulating message passing to
improve representation learning in heterophilous graphs and enhance robustness
against noisy graphs. Specifically, we define an interference-aware torque
metric that integrates structural distance and energy scores to quantify the
perturbation induced by edges, thereby encouraging each node to aggregate
information from its nearest low-energy neighbors. We use the metric to
hierarchically reconfigure the receptive field of each layer by judiciously
pruning high-torque edges and adding low-torque links, suppressing propagation
noise and boosting pertinent signals. Extensive evaluations on benchmark
datasets show that our approach surpasses state-of-the-art methods on both
heterophilous and homophilous graphs, and maintains high accuracy on noisy
graph.

</details>


### [106] [MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse](https://arxiv.org/abs/2507.21433)
*Kaiwen Chen,Xin Tan,Minchen Yu,Hong Xu*

Main category: cs.LG

TL;DR: MemShare reduces memory overhead in Large Reasoning Models by reusing similar KV cache blocks, improving throughput by 84.79% without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs generate redundant intermediate reasoning steps, causing memory inefficiency due to similar KV cache states.

Method: MemShare uses collaborative filtering to identify reusable KV cache blocks and enables zero-copy reuse.

Result: MemShare improves throughput by up to 84.79% while maintaining accuracy.

Conclusion: MemShare effectively reduces memory overhead and enhances performance in LRMs.

Abstract: Large Reasoning Models (LRMs) have achieved significant advances in
mathematical reasoning and formal logic tasks. However, their tendency to
generate lengthy chain-of-thought sequences leads to substantial memory
overhead during inference. We observe that LRMs frequently produce highly
similar intermediate reasoning steps, which correspond to similar KV cache
states across layers. Motivated by this observation, we propose MemShare, a
novel KV cache management approach that effectively reduces memory overhead.
MemShare employs a collaborative filtering algorithm to efficiently identify
reusable KV cache blocks and enables zero copy cache reuse to significantly
reduce memory overhead, improve throughput while maintaining accuracy.
Experimental results demonstrate that MemShare delivers up to 84.79\%
improvement in throughput while maintaining better accuracy compared to
existing KV cache management methods.

</details>


### [107] [PVD-ONet: A Multi-scale Neural Operator Method for Singularly Perturbed Boundary Layer Problems](https://arxiv.org/abs/2507.21437)
*Tiantian Sun,Jian Zu*

Main category: cs.LG

TL;DR: Proposes PVD-Net and PVD-ONet frameworks to solve singularly perturbed PDEs without data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the failure of physics-informed neural networks in singularly perturbed problems.

Method: Two versions of PVD-Net (stability-focused and high-accuracy) and PVD-ONet (operator learning) using Prandtl-Van Dyke principles.

Result: Numerical experiments show superior performance over baselines in multi-scale problems.

Conclusion: PVD-Net and PVD-ONet offer effective solutions for singularly perturbed PDEs, enhancing stability and accuracy.

Abstract: Physics-informed neural networks and Physics-informed DeepONet excel in
solving partial differential equations; however, they often fail to converge
for singularly perturbed problems. To address this, we propose two novel
frameworks, Prandtl-Van Dyke neural network (PVD-Net) and its operator learning
extension Prandtl-Van Dyke Deep Operator Network (PVD-ONet), which rely solely
on governing equations without data. To address varying task-specific
requirements, both PVD-Net and PVD-ONet are developed in two distinct versions,
tailored respectively for stability-focused and high-accuracy modeling. The
leading-order PVD-Net adopts a two-network architecture combined with Prandtl's
matching condition, targeting stability-prioritized scenarios. The high-order
PVD-Net employs a five-network design with Van Dyke's matching principle to
capture fine-scale boundary layer structures, making it ideal for high-accuracy
scenarios. PVD-ONet generalizes PVD-Net to the operator learning setting by
assembling multiple DeepONet modules, directly mapping initial conditions to
solution operators and enabling instant predictions for an entire family of
boundary layer problems without retraining. Numerical experiments on various
models show that our proposed methods consistently outperform existing
baselines under various error metrics, thereby offering a powerful new approach
for multi-scale problems.

</details>


### [108] [Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training](https://arxiv.org/abs/2507.21452)
*Sodtavilan Odonchimed,Tatsuya Matsushima,Simon Holk,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: RAGDP is a novel framework that speeds up Diffusion Policies (DPs) without extra training by using a knowledge base of expert demonstrations, improving accuracy and speed trade-offs.


<details>
  <summary>Details</summary>
Motivation: DPs are slow due to multiple noise removal steps, and existing methods like CP require long training times. RAGDP aims to address these issues.

Method: RAGDP encodes observation-action pairs into a vector database. During inference, it retrieves the most similar expert action and combines it with intermediate noise removal to reduce steps.

Result: RAGDP improves accuracy and speed, maintaining a 7% accuracy advantage over CP even when models are accelerated 20 times.

Conclusion: RAGDP offers a training-free solution to enhance DPs, balancing speed and accuracy effectively.

Abstract: Diffusion Policies (DPs) have attracted attention for their ability to
achieve significant accuracy improvements in various imitation learning tasks.
However, DPs depend on Diffusion Models, which require multiple noise removal
steps to generate a single action, resulting in long generation times. To solve
this problem, knowledge distillation-based methods such as Consistency Policy
(CP) have been proposed. However, these methods require a significant amount of
training time, especially for difficult tasks. In this study, we propose RAGDP
(Retrieve-Augmented Generation for Diffusion Policies) as a novel framework
that eliminates the need for additional training using a knowledge base to
expedite the inference of pre-trained DPs. In concrete, RAGDP encodes
observation-action pairs through the DP encoder to construct a vector database
of expert demonstrations. During inference, the current observation is
embedded, and the most similar expert action is extracted. This extracted
action is combined with an intermediate noise removal step to reduce the number
of steps required compared to the original diffusion step. We show that by
using RAGDP with the base model and existing acceleration methods, we improve
the accuracy and speed trade-off with no additional training. Even when
accelerating the models 20 times, RAGDP maintains an advantage in accuracy,
with a 7% increase over distillation models such as CP.

</details>


### [109] [Capacity-Constrained Continual Learning](https://arxiv.org/abs/2507.21479)
*Zheng Wen,Doina Precup,Benjamin Van Roy,Satinder Singh*

Main category: cs.LG

TL;DR: The paper addresses how agents with limited memory and compute resources should allocate capacity for optimal performance, focusing on a continual learning problem called the capacity-constrained LQG sequential prediction problem.


<details>
  <summary>Details</summary>
Motivation: To understand optimal resource allocation for agents with finite capacity, a topic that has received little attention.

Method: Derives a solution for the capacity-constrained LQG sequential prediction problem under technical conditions and demonstrates optimal capacity allocation for decomposable sub-problems in steady state.

Result: Provides a theoretical solution for capacity allocation in constrained learning scenarios.

Conclusion: This work is a foundational step in systematically studying learning under capacity constraints.

Abstract: Any agents we can possibly build are subject to capacity constraints, as
memory and compute resources are inherently finite. However, comparatively
little attention has been dedicated to understanding how agents with limited
capacity should allocate their resources for optimal performance. The goal of
this paper is to shed some light on this question by studying a simple yet
relevant continual learning problem: the capacity-constrained
linear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a
solution to this problem under appropriate technical conditions. Moreover, for
problems that can be decomposed into a set of sub-problems, we also demonstrate
how to optimally allocate capacity across these sub-problems in the steady
state. We view the results of this paper as a first step in the systematic
theoretical study of learning under capacity constraints.

</details>


### [110] [Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning](https://arxiv.org/abs/2507.21494)
*Wenxuan Bao,Ruxi Deng,Ruizhong Qiu,Tianxin Wei,Hanghang Tong,Jingrui He*

Main category: cs.LG

TL;DR: Latte is a novel framework for test-time adaptation in decentralized settings, using local and external memories to enhance model performance while minimizing communication costs.


<details>
  <summary>Details</summary>
Motivation: Existing test-time adaptation methods struggle with limited data in decentralized settings and lack personalization for individual clients.

Method: Latte uses local and external memories to store embeddings and class prototypes, leveraging similarity and uncertainty for adaptation.

Result: Latte outperforms existing methods in decentralized settings with minimal added costs.

Conclusion: Latte effectively addresses distribution shifts in decentralized environments, offering robust and personalized adaptation.

Abstract: Test-time adaptation with pre-trained vision-language models has gained
increasing attention for addressing distribution shifts during testing. Among
these approaches, memory-based algorithms stand out due to their training-free
nature and ability to leverage historical test data. However, existing
test-time adaptation methods are typically designed for a single domain with
abundant data. In decentralized settings such as federated learning, applying
these methods individually to each client suffers from limited test data, while
directly sharing a single global memory via the server prevents proper
personalization to each client's unique distribution. To address this, we
propose Latte, a novel framework where each client maintains a local memory to
store embeddings from its own historical test data and an external memory to
store class prototypes from other relevant clients. During communication, each
client retrieves prototypes from similar clients under the server's
coordination to expand its memory. For local adaptation, Latte utilizes both
embedding similarity and uncertainty to enhance model performance. Our
theoretical analysis shows that Latte effectively leverages in-distribution
clients while remaining robust to out-of-distribution clients. Extensive
experiments on domain adaptation and corruption benchmarks validate that Latte
achieves superior performance in decentralized settings, while introducing only
negligible communication and computation costs. Our code is available at
https://github.com/baowenxuan/Latte .

</details>


### [111] [Evaluation and Benchmarking of LLM Agents: A Survey](https://arxiv.org/abs/2507.21504)
*Mahmoud Mohammadi,Yipeng Li,Jane Lo,Wendy Yip*

Main category: cs.LG

TL;DR: The paper surveys LLM agent evaluation, proposing a taxonomy for objectives and processes, addressing enterprise challenges, and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLM-based agents is complex and underdeveloped, necessitating a structured approach for real-world deployment.

Method: Introduces a two-dimensional taxonomy (evaluation objectives and process) and discusses enterprise-specific challenges.

Result: Provides a framework for systematic evaluation, highlighting gaps like reliability and compliance.

Conclusion: Aims to clarify the fragmented evaluation landscape and guide future research for scalable, realistic assessments.

Abstract: The rise of LLM-based agents has opened new frontiers in AI applications, yet
evaluating these agents remains a complex and underdeveloped area. This survey
provides an in-depth overview of the emerging field of LLM agent evaluation,
introducing a two-dimensional taxonomy that organizes existing work along (1)
evaluation objectives -- what to evaluate, such as agent behavior,
capabilities, reliability, and safety -- and (2) evaluation process -- how to
evaluate, including interaction modes, datasets and benchmarks, metric
computation methods, and tooling. In addition to taxonomy, we highlight
enterprise-specific challenges, such as role-based access to data, the need for
reliability guarantees, dynamic and long-horizon interactions, and compliance,
which are often overlooked in current research. We also identify future
research directions, including holistic, more realistic, and scalable
evaluation. This work aims to bring clarity to the fragmented landscape of
agent evaluation and provide a framework for systematic assessment, enabling
researchers and practitioners to evaluate LLM agents for real-world deployment.

</details>


### [112] [Hierarchical Stochastic Differential Equation Models for Latent Manifold Learning in Neural Time Series](https://arxiv.org/abs/2507.21531)
*Pedram Rajaei,Maryam Ostadsharif Memar,Navid Ziaei,Behzad Nazari,Ali Yousefi*

Main category: cs.LG

TL;DR: A novel hierarchical SDE model is proposed to efficiently and interpretably uncover low-dimensional manifolds in high-dimensional neural time series, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing latent dynamical variable models lack balance between computational efficiency and interpretability for uncovering low-dimensional manifolds in neural time series.

Method: The model uses hierarchical Brownian bridge SDEs to reconstruct manifold trajectories from sparse samples, mapping these to observed data via continuous, differentiable processes.

Result: The model accurately recovers manifold structure, scales linearly with data length, and handles high-dimensional data effectively.

Conclusion: The proposed SDE model offers a scalable, interpretable solution for manifold discovery in neural time series.

Abstract: The manifold hypothesis suggests that high-dimensional neural time series lie
on a low-dimensional manifold shaped by simpler underlying dynamics. To uncover
this structure, latent dynamical variable models such as state-space models,
recurrent neural networks, neural ordinary differential equations, and Gaussian
Process Latent Variable Models are widely used. We propose a novel hierarchical
stochastic differential equation (SDE) model that balances computational
efficiency and interpretability, addressing key limitations of existing
methods. Our model assumes the trajectory of a manifold can be reconstructed
from a sparse set of samples from the manifold trajectory. The latent space is
modeled using Brownian bridge SDEs, with points - specified in both time and
value - sampled from a multivariate marked point process. These Brownian
bridges define the drift of a second set of SDEs, which are then mapped to the
observed data. This yields a continuous, differentiable latent process capable
of modeling arbitrarily complex time series as the number of manifold points
increases. We derive training and inference procedures and show that the
computational cost of inference scales linearly with the length of the
observation data. We then validate our model on both synthetic data and neural
recordings to demonstrate that it accurately recovers the underlying manifold
structure and scales effectively with data dimensionality.

</details>


### [113] [Categorical Distributions are Effective Neural Network Outputs for Event Prediction](https://arxiv.org/abs/2507.21616)
*Kevin Doran,Tom Baden*

Main category: cs.LG

TL;DR: A simple neural network output (categorical probability distribution) is effective for next spike prediction, challenging the underuse of such outputs in neural temporal point process models.


<details>
  <summary>Details</summary>
Motivation: To investigate why simple output structures are uncommon in neural temporal point process models and to explore dataset limitations.

Method: Extend and create datasets to test the effectiveness of categorical distribution outputs in varied scenarios.

Result: Simple categorical distribution outputs are competitive across datasets, revealing limitations in existing datasets and models.

Conclusion: Simple outputs can be effective, and dataset constraints may obscure true model performance.

Abstract: We demonstrate the effectiveness of using a simple neural network output, a
categorical probability distribution, for the task of next spike prediction.
This case study motivates an investigation into why this simple output
structure is not commonly used with neural temporal point process models. We
find evidence that many existing datasets for evaluating temporal point process
models do not reveal much information about the underlying event generating
processes, and many existing models perform well due to regularization effects
of model size and constraints on output structure. We extend existing datasets
and create new ones in order to explore outside of this information limited
regime and find that outputting a simple categorical distribution is
competitive across a wide range of datasets.

</details>


### [114] [Hyperbolic Genome Embeddings](https://arxiv.org/abs/2507.21648)
*Raiyan R. Khan,Philippe Chlenski,Itsik Pe'er*

Main category: cs.LG

TL;DR: Hyperbolic CNNs outperform Euclidean models in genomic sequence representation, achieving better performance with fewer parameters and no pretraining.


<details>
  <summary>Details</summary>
Motivation: Aligning machine learning inductive biases with biological evolutionary structure for better DNA sequence modeling.

Method: Novel application of hyperbolic CNNs to exploit evolutionary structure without explicit phylogenetic mapping.

Result: Outperforms Euclidean models on 37/42 benchmarks and state-of-the-art on 7 GUE datasets, with fewer parameters.

Conclusion: Hyperbolic framework shows robust potential for genome representation learning, validated by empirical assessments.

Abstract: Current approaches to genomic sequence modeling often struggle to align the
inductive biases of machine learning models with the evolutionarily-informed
structure of biological systems. To this end, we formulate a novel application
of hyperbolic CNNs that exploits this structure, enabling more expressive DNA
sequence representations. Our strategy circumvents the need for explicit
phylogenetic mapping while discerning key properties of sequences pertaining to
core functional and regulatory behavior. Across 37 out of 42 genome
interpretation benchmark datasets, our hyperbolic models outperform their
Euclidean equivalents. Notably, our approach even surpasses state-of-the-art
performance on seven GUE benchmark datasets, consistently outperforming many
DNA language models while using orders of magnitude fewer parameters and
avoiding pretraining. Our results include a novel set of benchmark
datasets--the Transposable Elements Benchmark--which explores a major but
understudied component of the genome with deep evolutionary significance. We
further motivate our work by exploring how our hyperbolic models recognize
genomic signal under various data-generating conditions and by constructing an
empirical method for interpreting the hyperbolicity of dataset embeddings.
Throughout these assessments, we find persistent evidence highlighting the
potential of our hyperbolic framework as a robust paradigm for genome
representation learning. Our code and benchmark datasets are available at
https://github.com/rrkhan/HGE.

</details>


### [115] [DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs](https://arxiv.org/abs/2507.21653)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.LG

TL;DR: Dual Granularity Prompting (DGP) improves fraud detection by summarizing neighbor information into concise prompts, addressing the limitations of text-only prompting in heterogeneous graphs.


<details>
  <summary>Details</summary>
Motivation: Text-only prompting struggles with heterogeneous fraud-detection graphs due to exponential growth of multi-hop relations and dense textual information, which overwhelms models and degrades performance.

Method: DGP preserves fine-grained textual details for the target node while summarizing neighbor information into coarse-grained prompts, using tailored summarization strategies for different data modalities.

Result: DGP improves fraud detection performance by up to 6.8% (AUPRC) over state-of-the-art methods while operating within a manageable token budget.

Conclusion: DGP demonstrates the potential of Graph-Enhanced LLMs for fraud detection by effectively mitigating information overload and enhancing performance.

Abstract: Real-world fraud detection applications benefit from graph learning
techniques that jointly exploit node features, often rich in textual data, and
graph structural information. Recently, Graph-Enhanced LLMs emerge as a
promising graph learning approach that converts graph information into prompts,
exploiting LLMs' ability to reason over both textual and structural
information. Among them, text-only prompting, which converts graph information
to prompts consisting solely of text tokens, offers a solution that relies only
on LLM tuning without requiring additional graph-specific encoders. However,
text-only prompting struggles on heterogeneous fraud-detection graphs:
multi-hop relations expand exponentially with each additional hop, leading to
rapidly growing neighborhoods associated with dense textual information. These
neighborhoods may overwhelm the model with long, irrelevant content in the
prompt and suppress key signals from the target node, thereby degrading
performance. To address this challenge, we propose Dual Granularity Prompting
(DGP), which mitigates information overload by preserving fine-grained textual
details for the target node while summarizing neighbor information into
coarse-grained text prompts. DGP introduces tailored summarization strategies
for different data modalities, bi-level semantic abstraction for textual fields
and statistical aggregation for numerical features, enabling effective
compression of verbose neighbor content into concise, informative prompts.
Experiments across public and industrial datasets demonstrate that DGP operates
within a manageable token budget while improving fraud detection performance by
up to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of
Graph-Enhanced LLMs for fraud detection.

</details>


### [116] [Probabilistic Consistency in Machine Learning and Its Connection to Uncertainty Quantification](https://arxiv.org/abs/2507.21670)
*Paul Patrone,Anthony Kearsley*

Main category: cs.LG

TL;DR: The paper explores the connections between machine learning models and uncertainty quantification, focusing on prevalence and deriving a level-set theory of classification to show equivalence between self-consistent ML models and class-conditional probability distributions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of quantifying confidence in ML predictions and understanding how models abstract training data, particularly in diagnostic settings where prevalence is key.

Method: Analyzes binary Bayes optimal classifiers, reinterprets their boundary sets as level-sets of density ratios, and parameterizes them by prevalence to deduce properties like monotonicity and class-switching. Extends to multiclass cases with normalization and self-consistency conditions.

Result: Shows that certain ML models are equivalent to class-conditional probability distributions and derives necessary conditions for valid probabilistic interpretations.

Conclusion: The analysis provides a framework for uncertainty quantification in ML, linking model properties to probabilistic interpretations and uncertainty propagation.

Abstract: Machine learning (ML) is often viewed as a powerful data analysis tool that
is easy to learn because of its black-box nature. Yet this very nature also
makes it difficult to quantify confidence in predictions extracted from ML
models, and more fundamentally, to understand how such models are mathematical
abstractions of training data. The goal of this paper is to unravel these
issues and their connections to uncertainty quantification (UQ) by pursuing a
line of reasoning motivated by diagnostics. In such settings, prevalence - i.e.
the fraction of elements in class - is often of inherent interest. Here we
analyze the many interpretations of prevalence to derive a level-set theory of
classification, which shows that certain types of self-consistent ML models are
equivalent to class-conditional probability distributions. We begin by studying
the properties of binary Bayes optimal classifiers, recognizing that their
boundary sets can be reinterpreted as level-sets of pairwise density ratios. By
parameterizing Bayes classifiers in terms of the prevalence, we then show that
they satisfy important monotonicity and class-switching properties that can be
used to deduce the density ratios without direct access to the boundary sets.
Moreover, this information is sufficient for tasks such as constructing the
multiclass Bayes-optimal classifier and estimating inherent uncertainty in the
class assignments. In the multiclass case, we use these results to deduce
normalization and self-consistency conditions, the latter being equivalent to
the law of total probability for classifiers. We also show that these are
necessary conditions for arbitrary ML models to have valid probabilistic
interpretations. Throughout we demonstrate how this analysis informs the
broader task of UQ for ML via an uncertainty propagation framework.

</details>


### [117] [PREIG: Physics-informed and Reinforcement-driven Interpretable GRU for Commodity Demand Forecasting](https://arxiv.org/abs/2507.21710)
*Hongwei Ma,Junbin Gao,Minh-Ngoc Tran*

Main category: cs.LG

TL;DR: PREIG is a deep learning framework for commodity demand forecasting, combining GRU and PINN with economic constraints, outperforming traditional models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of volatile and nonlinear commodity demand forecasting while ensuring economic consistency.

Method: Integrates GRU with PINN, enforces economic constraints via a custom loss function, and uses hybrid optimization (NAdam, L-BFGS, POP).

Result: PREIG outperforms ARIMA, GARCH, BPNN, and RNN in RMSE and MAPE, maintaining explainability.

Conclusion: PREIG offers a robust, interpretable, and scalable solution for nonlinear time series forecasting in economics.

Abstract: Accurately forecasting commodity demand remains a critical challenge due to
volatile market dynamics, nonlinear dependencies, and the need for economically
consistent predictions. This paper introduces PREIG, a novel deep learning
framework tailored for commodity demand forecasting. The model uniquely
integrates a Gated Recurrent Unit (GRU) architecture with physics-informed
neural network (PINN) principles by embedding a domain-specific economic
constraint: the negative elasticity between price and demand. This constraint
is enforced through a customized loss function that penalizes violations of the
physical rule, ensuring that model predictions remain interpretable and aligned
with economic theory. To further enhance predictive performance and stability,
PREIG incorporates a hybrid optimization strategy that couples NAdam and L-BFGS
with Population-Based Training (POP). Experiments across multiple commodities
datasets demonstrate that PREIG significantly outperforms traditional
econometric models (ARIMA,GARCH) and deep learning baselines (BPNN,RNN) in both
RMSE and MAPE. When compared with GRU,PREIG maintains good explainability while
still performing well in prediction. By bridging domain knowledge, optimization
theory and deep learning, PREIG provides a robust, interpretable, and scalable
solution for high-dimensional nonlinear time series forecasting in economy.

</details>


### [118] [Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins](https://arxiv.org/abs/2507.21720)
*Gang Wang,Peng Hu*

Main category: cs.LG

TL;DR: A neural network extended corresponding state model is proposed to predict hydrofluoroolefin refrigerant properties, combining theoretical and data-driven methods for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: The lack of reliable thermodynamic data for hydrofluoroolefins hinders their discovery and application as next-generation refrigerants.

Method: Integrates graph neural networks and specialized model architecture to predict residual thermodynamic properties, trained on accurate data and validated via leave-one-out cross-validation.

Result: Achieves high accuracy for density and energy properties in liquid and supercritical regions, outperforming conventional models.

Conclusion: The model effectively embeds physics knowledge into machine learning, accelerating the discovery of superior hydrofluoroolefin refrigerants.

Abstract: Hydrofluoroolefins are considered the most promising next-generation
refrigerants due to their extremely low global warming potential values, which
can effectively mitigate the global warming effect. However, the lack of
reliable thermodynamic data hinders the discovery and application of newer and
superior hydrofluoroolefin refrigerants. In this work, integrating the
strengths of theoretical method and data-driven method, we proposed a neural
network extended corresponding state model to predict the residual
thermodynamic properties of hydrofluoroolefin refrigerants. The innovation is
that the fluids are characterized through their microscopic molecular
structures by the inclusion of graph neural network module and the specialized
design of model architecture to enhance its generalization ability. The
proposed model is trained using the highly accurate data of available known
fluids, and evaluated via the leave-one-out cross-validation method. Compared
to conventional extended corresponding state models or cubic equation of state,
the proposed model shows significantly improved accuracy for density and energy
properties in liquid and supercritical regions, with average absolute deviation
of 1.49% (liquid) and 2.42% (supercritical) for density, 3.37% and 2.50% for
residual entropy, 1.85% and 1.34% for residual enthalpy. These results
demonstrate the effectiveness of embedding physics knowledge into the machine
learning model. The proposed neural network extended corresponding state model
is expected to significantly accelerate the discovery of novel
hydrofluoroolefin refrigerants.

</details>


### [119] [Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation](https://arxiv.org/abs/2507.21738)
*Huiqiang Chen,Tianqing Zhu,Xin Yu,Wanlei Zhou*

Main category: cs.LG

TL;DR: ZS-PAG is a novel framework for zero-shot machine unlearning, addressing over-unlearning by generating adversarial samples, pinpointing a subspace for unlearning, and using influence-based pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods rely on remaining data, making them impractical for zero-shot scenarios where only unlearning samples are available.

Method: ZS-PAG approximates remaining data with adversarial samples, identifies a subspace for unlearning, and uses influence-based pseudo-labeling to improve performance.

Result: The method theoretically guarantees performance and outperforms baselines in experiments.

Conclusion: ZS-PAG effectively addresses zero-shot unlearning challenges, preventing over-unlearning and enhancing model performance.

Abstract: Machine unlearning aims to remove the influence of specific samples from a
trained model. A key challenge in this process is over-unlearning, where the
model's performance on the remaining data significantly drops due to the change
in the model's parameters. Existing unlearning algorithms depend on the
remaining data to prevent this issue. As such, these methods are inapplicable
in a more practical scenario, where only the unlearning samples are available
(i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to
fill this gap. Our approach offers three key innovations: (1) we approximate
the inaccessible remaining data by generating adversarial samples; (2)
leveraging the generated samples, we pinpoint a specific subspace to perform
the unlearning process, therefore preventing over-unlearning in the challenging
zero-shot scenario; and (3) we consider the influence of the unlearning process
on the remaining samples and design an influence-based pseudo-labeling
strategy. As a result, our method further improves the model's performance
after unlearning. The proposed method holds a theoretical guarantee, and
experiments on various benchmarks validate the effectiveness and superiority of
our proposed method over several baselines.

</details>


### [120] [evoxels: A differentiable physics framework for voxel-based microstructure simulations](https://arxiv.org/abs/2507.21748)
*Simon Daubner,Alexander E. Cohen,Benjamin Dörich,Samuel J. Cooper*

Main category: cs.LG

TL;DR: The paper introduces evoxels, a Python-based framework integrating microscopy, simulations, and machine learning for inverse material design.


<details>
  <summary>Details</summary>
Motivation: Bridging experimental and computational domains in materials science to enable inverse design by linking processing, structure, and properties.

Method: Uses a differentiable physics framework (evoxels) combining segmented 3D microscopy data, physical simulations, and machine learning.

Result: Accelerates discovery and enhances understanding of process-structure-property relationships.

Conclusion: The evoxels framework effectively integrates diverse tools for advancing inverse material design.

Abstract: Materials science inherently spans disciplines: experimentalists use advanced
microscopy to uncover micro- and nanoscale structure, while theorists and
computational scientists develop models that link processing, structure, and
properties. Bridging these domains is essential for inverse material design
where you start from desired performance and work backwards to optimal
microstructures and manufacturing routes. Integrating high-resolution imaging
with predictive simulations and data-driven optimization accelerates discovery
and deepens understanding of process-structure-property relationships. The
differentiable physics framework evoxels is based on a fully Pythonic, unified
voxel-based approach that integrates segmented 3D microscopy data, physical
simulations, inverse modeling, and machine learning.

</details>


### [121] [TempRe: Template generation for single and direct multi-step retrosynthesis](https://arxiv.org/abs/2507.21762)
*Nguyen Xuan-Vu,Daniel Armstrong,Zlatko Joncev,Philippe Schwaller*

Main category: cs.LG

TL;DR: TempRe is a generative framework for retrosynthesis planning, combining the scalability of template-free methods with the chemical plausibility of template-based approaches, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge in retrosynthesis planning lies in the vast chemical reaction space, with traditional methods being either limited in scalability or prone to invalid reactions.

Method: TempRe reformulates template-based approaches as sequence generation, enabling scalable and flexible retrosynthesis. It is evaluated on single-step and multi-step tasks.

Result: TempRe outperforms template classification and SMILES-based methods, achieving strong accuracy on the PaRoutes benchmark and enabling direct multi-step synthesis.

Conclusion: Template generative modeling, as demonstrated by TempRe, is a promising paradigm for computer-aided synthesis planning.

Abstract: Retrosynthesis planning remains a central challenge in molecular discovery
due to the vast and complex chemical reaction space. While traditional
template-based methods offer tractability, they suffer from poor scalability
and limited generalization, and template-free generative approaches risk
generating invalid reactions. In this work, we propose TempRe, a generative
framework that reformulates template-based approaches as sequence generation,
enabling scalable, flexible, and chemically plausible retrosynthesis. We
evaluated TempRe across single-step and multi-step retrosynthesis tasks,
demonstrating its superiority over both template classification and
SMILES-based generation methods. On the PaRoutes multi-step benchmark, TempRe
achieves strong top-k route accuracy. Furthermore, we extend TempRe to direct
multi-step synthesis route generation, providing a lightweight and efficient
alternative to conventional single-step and search-based approaches. These
results highlight the potential of template generative modeling as a powerful
paradigm in computer-aided synthesis planning.

</details>


### [122] [Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer](https://arxiv.org/abs/2507.21799)
*Xie Zhang,Yina Wang,Chenshu Wu*

Main category: cs.LG

TL;DR: RF-CRATE is the first mathematically interpretable deep network for RF sensing, extending white-box transformers to the complex domain with improved performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing deep wireless sensing models lack interpretability, limiting generalizability and raising security concerns in physical applications.

Method: Extends white-box transformers to the complex domain using CR-Calculus, introduces Subspace Regularization for feature diversity, and evaluates with public and self-collected datasets.

Result: Achieves performance comparable to black-box models, with 19.98% average improvement, 5.08% classification gain, and 10.34% regression error reduction.

Conclusion: RF-CRATE offers full mathematical interpretability and superior performance, advancing RF sensing with open-source availability.

Abstract: The empirical success of deep learning has spurred its application to the
radio-frequency (RF) domain, leading to significant advances in Deep Wireless
Sensing (DWS). However, most existing DWS models function as black boxes with
limited interpretability, which hampers their generalizability and raises
concerns in security-sensitive physical applications. In this work, inspired by
the remarkable advances of white-box transformers, we present RF-CRATE, the
first mathematically interpretable deep network architecture for RF sensing,
grounded in the principles of complex sparse rate reduction. To accommodate the
unique RF signals, we conduct non-trivial theoretical derivations that extend
the original real-valued white-box transformer to the complex domain. By
leveraging the CR-Calculus framework, we successfully construct a fully
complex-valued white-box transformer with theoretically derived self-attention
and residual multi-layer perceptron modules. Furthermore, to improve the
model's ability to extract discriminative features from limited wireless data,
we introduce Subspace Regularization, a novel regularization strategy that
enhances feature diversity, resulting in an average performance improvement of
19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against
seven baselines with multiple public and self-collected datasets involving
different RF signals. The results show that RF-CRATE achieves performance on
par with thoroughly engineered black-box models, while offering full
mathematical interpretability. More importantly, by extending CRATE to the
complex domain, RF-CRATE yields substantial improvements, achieving an average
classification gain of 5.08% and reducing regression error by 10.34% across
diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at:
https://github.com/rfcrate/RF_CRATE.

</details>


### [123] [Bayesian Neural Network Surrogates for Bayesian Optimization of Carbon Capture and Storage Operations](https://arxiv.org/abs/2507.21803)
*Sofianos Panagiotis Fotias,Vassilis Gaganis*

Main category: cs.LG

TL;DR: The paper explores Bayesian Optimization (BO) for optimizing Carbon Capture and Storage (CCS) projects, comparing novel stochastic models to Gaussian Processes (GPs) in scenarios where GPs underperform. It highlights BO's potential to improve economic viability and sustainability in CCS.


<details>
  <summary>Details</summary>
Motivation: To enhance CCS project efficiency and sustainability by addressing limitations of traditional BO methods like GPs in complex scenarios (e.g., many decision variables or multiple objectives).

Method: Uses derivative-free Bayesian Optimization with various stochastic models, including novel ones, and evaluates them against GPs. Incorporates Net Present Value (NPV) as a key objective.

Result: Demonstrates BO's effectiveness in optimizing CCS projects, especially with exotic stochastic models, improving economic viability and sustainability.

Conclusion: BO, particularly with alternative stochastic models, is a promising method for optimizing CCS projects, marking its first application in reservoir engineering for sustainability.

Abstract: Carbon Capture and Storage (CCS) stands as a pivotal technology for fostering
a sustainable future. The process, which involves injecting supercritical
CO$_2$ into underground formations, a method already widely used for Enhanced
Oil Recovery, serves a dual purpose: it not only curbs CO$_2$ emissions and
addresses climate change but also extends the operational lifespan and
sustainability of oil fields and platforms, easing the shift toward greener
practices. This paper delivers a thorough comparative evaluation of strategies
for optimizing decision variables in CCS project development, employing a
derivative-free technique known as Bayesian Optimization. In addition to
Gaussian Processes, which usually serve as the gold standard in BO, various
novel stochastic models were examined and compared within a BO framework. This
research investigates the effectiveness of utilizing more exotic stochastic
models than GPs for BO in environments where GPs have been shown to
underperform, such as in cases with a large number of decision variables or
multiple objective functions that are not similarly scaled. By incorporating
Net Present Value (NPV) as a key objective function, the proposed framework
demonstrates its potential to improve economic viability while ensuring the
sustainable deployment of CCS technologies. Ultimately, this study represents
the first application in the reservoir engineering industry of the growing body
of BO research, specifically in the search for more appropriate stochastic
models, highlighting its potential as a preferred method for enhancing
sustainability in the energy sector.

</details>


### [124] [Analysis of Fourier Neural Operators via Effective Field Theory](https://arxiv.org/abs/2507.21833)
*Taeyoung Kim*

Main category: cs.LG

TL;DR: The paper analyzes Fourier Neural Operators (FNOs) using effective-field-theory to explain their stability, generalization, and frequency behavior, revealing how nonlinear activations and architecture choices impact performance.


<details>
  <summary>Details</summary>
Motivation: To provide a principled explanation for the stability, generalization, and frequency behavior of FNOs, which are widely used for high-dimensional PDEs but lack theoretical understanding.

Method: The study employs effective-field-theory analysis in infinite-dimensional function space, deriving recursion relations for layer kernels and vertices, and examines analytic activations, scale-invariant cases, and residual connections.

Result: Nonlinear activations couple frequency inputs to high-frequency modes, and criticality conditions for weight initialization are derived. Experiments confirm frequency transfer and validate predictions.

Conclusion: The work quantifies how nonlinearity aids FNOs in capturing features, offers hyper-parameter selection criteria, and explains the benefits of scale-invariant activations and residual connections.

Abstract: Fourier Neural Operators (FNOs) have emerged as leading surrogates for
high-dimensional partial-differential equations, yet their stability,
generalization and frequency behavior lack a principled explanation. We present
the first systematic effective-field-theory analysis of FNOs in an
infinite-dimensional function space, deriving closed recursion relations for
the layer kernel and four-point vertex and then examining three practically
important settings-analytic activations, scale-invariant cases and
architectures with residual connections. The theory shows that nonlinear
activations inevitably couple frequency inputs to high-frequency modes that are
otherwise discarded by spectral truncation, and experiments confirm this
frequency transfer. For wide networks we obtain explicit criticality conditions
on the weight-initialization ensemble that keep small input perturbations to
have uniform scale across depth, and empirical tests validate these
predictions. Taken together, our results quantify how nonlinearity enables
neural operators to capture non-trivial features, supply criteria for
hyper-parameter selection via criticality analysis, and explain why
scale-invariant activations and residual connections enhance feature learning
in FNOs.

</details>


### [125] [Discovering Interpretable Ordinary Differential Equations from Noisy Data](https://arxiv.org/abs/2507.21841)
*Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: An unsupervised method for discovering interpretable ODE models from noisy data, using spline transformations and gradient matrices for accurate, sparse solutions.


<details>
  <summary>Details</summary>
Motivation: Current data-driven models often lack physical interpretability and accuracy. The goal is to develop a method that captures true physics while handling noisy data.

Method: Proposes a two-step approach: 1) Find an approximate general solution using a functional form similar to analytical solutions of ODEs, 2) Use spline transformations to estimate ODE coefficients via gradient matrices.

Result: The method accurately discovers ODEs, promotes sparsity without regularization, and is robust to noisy data.

Conclusion: The approach enables high-fidelity, interpretable modeling of physical systems, suitable for real-world experimental data.

Abstract: The data-driven discovery of interpretable models approximating the
underlying dynamics of a physical system has gained attraction in the past
decade. Current approaches employ pre-specified functional forms or basis
functions and often result in models that lack physical meaning and
interpretability, let alone represent the true physics of the system. We
propose an unsupervised parameter estimation methodology that first finds an
approximate general solution, followed by a spline transformation to linearly
estimate the coefficients of the governing ordinary differential equation
(ODE). The approximate general solution is postulated using the same functional
form as the analytical solution of a general homogeneous, linear,
constant-coefficient ODE. An added advantage is its ability to produce a
high-fidelity, smooth functional form even in the presence of noisy data. The
spline approximation obtains gradient information from the functional form
which are linearly independent and creates the basis of the gradient matrix.
This gradient matrix is used in a linear system to find the coefficients of the
ODEs. From the case studies, we observed that our modeling approach discovers
ODEs with high accuracy and also promotes sparsity in the solution without
using any regularization techniques. The methodology is also robust to noisy
data and thus allows the integration of data-driven techniques into real
experimental setting for data-driven learning of physical phenomena.

</details>


### [126] [Cardiovascular Disease Prediction using Machine Learning: A Comparative Analysis](https://arxiv.org/abs/2507.21898)
*Risshab Srinivas Ramesh,Roshani T S Udupa,Monisha J,Kushi K K S*

Main category: cs.LG

TL;DR: The study analyzes CVD risk factors using a dataset of 68,119 records, identifying age, blood pressure, and cholesterol as primary risks, with CatBoost as the best-performing model.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular diseases (CVDs) cause 31% of global deaths, necessitating research into risk factors for better prevention.

Method: Statistical analyses (t-tests, Chi-square, ANOVA) and logistic regression were used to assess CVD associations. Model performance was compared, with CatBoost evaluated for accuracy.

Result: Key risk factors include age, hypertension, and abnormal cholesterol. CatBoost achieved 0.734 accuracy and excelled in probabilistic prediction. Unexpected negative associations for smoking/alcohol suggest data issues.

Conclusion: The study highlights significant CVD risk factors but notes data challenges, recommending improved preprocessing for better predictive reliability.

Abstract: Cardiovascular diseases (CVDs) are a main cause of mortality globally,
accounting for 31% of all deaths. This study involves a cardiovascular disease
(CVD) dataset comprising 68,119 records to explore the influence of numerical
(age, height, weight, blood pressure, BMI) and categorical gender, cholesterol,
glucose, smoking, alcohol, activity) factors on CVD occurrence. We have
performed statistical analyses, including t-tests, Chi-square tests, and ANOVA,
to identify strong associations between CVD and elderly people, hypertension,
higher weight, and abnormal cholesterol levels, while physical activity (a
protective factor). A logistic regression model highlights age, blood pressure,
and cholesterol as primary risk factors, with unexpected negative associations
for smoking and alcohol, suggesting potential data issues. Model performance
comparisons reveal CatBoost as the top performer with an accuracy of 0.734 and
an ECE of 0.0064 and excels in probabilistic prediction (Brier score = 0.1824).
Data challenges, including outliers and skewed distributions, indicate a need
for improved preprocessing to enhance predictive reliability.

</details>


### [127] [Multi-state Protein Design with DynamicMPNN](https://arxiv.org/abs/2507.21938)
*Alex Abrudan,Sebastian Pujalte Ojeda,Chaitanya K. Joshi,Matthew Greenig,Felipe Engelberger,Alena Khmelinskaia,Jens Meiler,Michele Vendruscolo,Tuomas P. J. Knowles*

Main category: cs.LG

TL;DR: DynamicMPNN, a new inverse folding model, outperforms ProteinMPNN by 13% in multi-state protein design by jointly learning across conformational ensembles.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of single-state design for proteins that adopt multiple conformations, critical for biological processes like enzyme catalysis and membrane transport.

Method: Introduces DynamicMPNN, trained on 46,033 conformational pairs covering 75% of CATH superfamilies, using joint learning across ensembles and evaluated with AlphaFold.

Result: DynamicMPNN achieves up to 13% better structure-normalized RMSD than ProteinMPNN on a multi-state protein benchmark.

Conclusion: DynamicMPNN advances multi-state protein design by directly addressing conformational diversity, improving experimental success rates.

Abstract: Structural biology has long been dominated by the one sequence, one
structure, one function paradigm, yet many critical biological processes - from
enzyme catalysis to membrane transport - depend on proteins that adopt multiple
conformational states. Existing multi-state design approaches rely on post-hoc
aggregation of single-state predictions, achieving poor experimental success
rates compared to single-state design. We introduce DynamicMPNN, an inverse
folding model explicitly trained to generate sequences compatible with multiple
conformations through joint learning across conformational ensembles. Trained
on 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated
using AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13%
on structure-normalized RMSD across our challenging multi-state protein
benchmark.

</details>


### [128] [SLA-Centric Automated Algorithm Selection Framework for Cloud Environments](https://arxiv.org/abs/2507.21963)
*Siana Rizwan,Tasnim Ahmed,Salimur Choudhury*

Main category: cs.LG

TL;DR: An SLA-aware automated algorithm-selection framework for combinatorial optimization in cloud environments, using ML to predict performance and rank algorithms, applied to the 0-1 knapsack problem.


<details>
  <summary>Details</summary>
Motivation: SLA violations in cloud computing impact efficiency and profitability, necessitating automated solutions for optimal algorithm selection under constraints.

Method: An ensemble of ML models predicts performance and ranks algorithm-hardware pairs based on SLA constraints, tested on the 0-1 knapsack problem with a curated dataset.

Result: The framework is evaluated on classification and regression tasks, with ablation studies on hyperparameters, learning approaches, LLM effectiveness, and SHAP-based interpretability.

Conclusion: The proposed framework effectively addresses SLA-aware algorithm selection, demonstrating practical utility in resource-constrained cloud environments.

Abstract: Cloud computing offers on-demand resource access, regulated by Service-Level
Agreements (SLAs) between consumers and Cloud Service Providers (CSPs). SLA
violations can impact efficiency and CSP profitability. In this work, we
propose an SLA-aware automated algorithm-selection framework for combinatorial
optimization problems in resource-constrained cloud environments. The framework
uses an ensemble of machine learning models to predict performance and rank
algorithm-hardware pairs based on SLA constraints. We also apply our framework
to the 0-1 knapsack problem. We curate a dataset comprising instance specific
features along with memory usage, runtime, and optimality gap for 6 algorithms.
As an empirical benchmark, we evaluate the framework on both classification and
regression tasks. Our ablation study explores the impact of hyperparameters,
learning approaches, and large language models effectiveness in regression, and
SHAP-based interpretability.

</details>


### [129] [Improving Generative Ad Text on Facebook using Reinforcement Learning](https://arxiv.org/abs/2507.21983)
*Daniel R. Jiang,Alex Nikulkov,Yu-Chia Chen,Yang Bai,Zheqing Zhu*

Main category: cs.LG

TL;DR: The paper explores the economic impact of reinforcement learning (RL) post-training for large language models (LLMs), demonstrating its effectiveness in generative advertising on Facebook through a model called AdLlama.


<details>
  <summary>Details</summary>
Motivation: To quantify the economic impact of RL post-training for LLMs, particularly in real-world applications like generative advertising.

Method: Introduces reinforcement learning with performance feedback (RLPF), using historical ad performance data as rewards, and tests it in a large-scale A/B experiment on Facebook.

Result: AdLlama improves click-through rates by 6.7% compared to a supervised imitation model, enhancing advertiser ROI and satisfaction.

Conclusion: RLPF is a promising, generalizable method for aligning LLMs with tangible outcomes, bridging the gap between model capability and real-world impact.

Abstract: Generative artificial intelligence (AI), in particular large language models
(LLMs), is poised to drive transformative economic change. LLMs are pre-trained
on vast text data to learn general language patterns, but a subsequent
post-training phase is critical to align them for specific real-world tasks.
Reinforcement learning (RL) is the leading post-training technique, yet its
economic impact remains largely underexplored and unquantified. We examine this
question through the lens of the first deployment of an RL-trained LLM for
generative advertising on Facebook. Integrated into Meta's Text Generation
feature, our model, "AdLlama," powers an AI tool that helps advertisers create
new variations of human-written ad text. To train this model, we introduce
reinforcement learning with performance feedback (RLPF), a post-training method
that uses historical ad performance data as a reward signal. In a large-scale
10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad
variations, we find that AdLlama improves click-through rates by 6.7%
(p=0.0296) compared to a supervised imitation model trained on curated ads.
This represents a substantial improvement in advertiser return on investment on
Facebook. We also find that advertisers who used AdLlama generated more ad
variations, indicating higher satisfaction with the model's outputs. To our
knowledge, this is the largest study to date on the use of generative AI in an
ecologically valid setting, offering an important data point quantifying the
tangible impact of RL post-training. Furthermore, the results show that RLPF is
a promising and generalizable approach for metric-driven post-training that
bridges the gap between highly capable language models and tangible outcomes.

</details>


### [130] [Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation](https://arxiv.org/abs/2507.21992)
*Siddhartha Pradhan,Shikshya Shiwakoti,Neha Bathuri*

Main category: cs.LG

TL;DR: Knowledge distillation from multiple heterogeneous teachers improves adversarial example generation, achieving comparable attack success to ensembles while being six times faster.


<details>
  <summary>Details</summary>
Motivation: Explore if knowledge distillation (KD) from multiple teachers can enhance adversarial example transferability, aiming for efficiency and effectiveness in black-box attacks.

Method: Train a lightweight student model using curriculum-based switching and joint optimization KD strategies with ResNet50 and DenseNet-161 teachers. Generate adversarial examples via FG, FGS, and PGD attacks, evaluated against GoogLeNet.

Result: Student models achieve attack success rates similar to ensemble baselines, with six times faster generation. Lower temperature and hard-label supervision boost transferability.

Conclusion: KD is not just for model compression but also enhances black-box adversarial attack efficiency and effectiveness.

Abstract: We investigate whether knowledge distillation (KD) from multiple
heterogeneous teacher models can enhance the generation of transferable
adversarial examples. A lightweight student model is trained using two KD
strategies: curriculum-based switching and joint optimization, with ResNet50
and DenseNet-161 as teachers. The trained student is then used to generate
adversarial examples using FG, FGS, and PGD attacks, which are evaluated
against a black-box target model (GoogLeNet). Our results show that student
models distilled from multiple teachers achieve attack success rates comparable
to ensemble-based baselines, while reducing adversarial example generation time
by up to a factor of six. An ablation study further reveals that lower
temperature settings and the inclusion of hard-label supervision significantly
enhance transferability. These findings suggest that KD can serve not only as a
model compression technique but also as a powerful tool for improving the
efficiency and effectiveness of black-box adversarial attacks.

</details>


### [131] [Classification of Honey Botanical and Geographical Sources using Mineral Profiles and Machine Learning](https://arxiv.org/abs/2507.22032)
*Mokhtar Al-Awadhi,Ratnadeep Deshmukh*

Main category: cs.LG

TL;DR: A machine learning approach using mineral element profiles effectively classifies honey's floral and geographical sources, with Random Forests achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To identify honey's botanical and geographical origins using mineral element profiles for quality control and authenticity verification.

Method: Two-step process: preprocessing (missing-value treatment, normalization) and classification (supervised models tested on a honey mineral dataset).

Result: Mineral elements are discriminative; Random Forests achieve 99.30% accuracy for botanical and 98.01% for geographical classification.

Conclusion: Mineral element profiles combined with machine learning, especially Random Forests, are highly effective for honey origin classification.

Abstract: This paper proposes a machine learning-based approach for identifying honey
floral and geographical sources using mineral element profiles. The proposed
method comprises two steps: preprocessing and classification. The preprocessing
phase involves missing-value treatment and data normalization. In the
classification phase, we employ various supervised classification models for
discriminating between six botanical sources and 13 geographical origins of
honey. We test the classifiers' performance on a publicly available honey
mineral element dataset. The dataset contains mineral element profiles of
honeys from various floral and geographical origins. Results show that mineral
element content in honey provides discriminative information useful for
classifying honey botanical and geographical sources. Results also show that
the Random Forests (RF) classifier obtains the best performance on this
dataset, achieving a cross-validation accuracy of 99.30% for classifying honey
botanical origins and 98.01% for classifying honey geographical origins.

</details>


### [132] [Structure-Informed Deep Reinforcement Learning for Inventory Management](https://arxiv.org/abs/2507.22040)
*Alvaro Maggiar,Sohrab Andaz,Akhil Bagaria,Carson Eisenach,Dean Foster,Omer Gottesman,Dominique Perrault-Joncas*

Main category: cs.LG

TL;DR: The paper explores using Deep Reinforcement Learning (DRL) for inventory management, showing it outperforms benchmarks and heuristics while requiring minimal tuning. It introduces a Structure-Informed Policy Network to enhance interpretability and robustness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between data-driven learning and analytical insights in inventory management, avoiding unrealistic assumptions about demand distributions.

Method: Applies a DRL algorithm (DirectBackprop) to various inventory scenarios, using historical data. Introduces a Structure-Informed Policy Network to incorporate optimal policy characteristics.

Result: DRL performs competitively or better than benchmarks, captures structural properties of optimal policies, and improves interpretability with the proposed technique.

Conclusion: DRL effectively combines data-driven learning with analytical insights, offering practical and robust solutions for inventory management.

Abstract: This paper investigates the application of Deep Reinforcement Learning (DRL)
to classical inventory management problems, with a focus on practical
implementation considerations. We apply a DRL algorithm based on DirectBackprop
to several fundamental inventory management scenarios including multi-period
systems with lost sales (with and without lead times), perishable inventory
management, dual sourcing, and joint inventory procurement and removal. The DRL
approach learns policies across products using only historical information that
would be available in practice, avoiding unrealistic assumptions about demand
distributions or access to distribution parameters. We demonstrate that our
generic DRL implementation performs competitively against or outperforms
established benchmarks and heuristics across these diverse settings, while
requiring minimal parameter tuning. Through examination of the learned
policies, we show that the DRL approach naturally captures many known
structural properties of optimal policies derived from traditional operations
research methods. To further improve policy performance and interpretability,
we propose a Structure-Informed Policy Network technique that explicitly
incorporates analytically-derived characteristics of optimal policies into the
learning process. This approach can help interpretability and add robustness to
the policy in out-of-sample performance, as we demonstrate in an example with
realistic demand data. Finally, we provide an illustrative application of DRL
in a non-stationary setting. Our work bridges the gap between data-driven
learning and analytical insights in inventory management while maintaining
practical applicability.

</details>


### [133] [Weight-Parameterization in Continuous Time Deep Neural Networks for Surrogate Modeling](https://arxiv.org/abs/2507.22045)
*Haley Rosso,Lars Ruthotto,Khachik Sargsyan*

Main category: cs.LG

TL;DR: The paper explores polynomial basis functions (monomial and Legendre) for parameterizing time-varying weights in neural ODEs and ResNets, finding Legendre bases improve stability, efficiency, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Training continuous-time deep learning models like neural ODEs requires expressive yet stable time-varying weights under computational constraints.

Method: Evaluated monomial and Legendre polynomial bases in neural ODEs and ResNets using discretize-then-optimize and optimize-then-discretize paradigms.

Result: Legendre bases provided more stable training, lower computational cost, and comparable or better accuracy than monomial or unconstrained weights.

Conclusion: Orthogonal polynomial bases (e.g., Legendre) offer a favorable balance between expressivity and training efficiency for time-dependent weight parameterization.

Abstract: Continuous-time deep learning models, such as neural ordinary differential
equations (ODEs), offer a promising framework for surrogate modeling of complex
physical systems. A central challenge in training these models lies in learning
expressive yet stable time-varying weights, particularly under computational
constraints. This work investigates weight parameterization strategies that
constrain the temporal evolution of weights to a low-dimensional subspace
spanned by polynomial basis functions. We evaluate both monomial and Legendre
polynomial bases within neural ODE and residual network (ResNet) architectures
under discretize-then-optimize and optimize-then-discretize training paradigms.
Experimental results across three high-dimensional benchmark problems show that
Legendre parameterizations yield more stable training dynamics, reduce
computational cost, and achieve accuracy comparable to or better than both
monomial parameterizations and unconstrained weight models. These findings
elucidate the role of basis choice in time-dependent weight parameterization
and demonstrate that using orthogonal polynomial bases offers a favorable
tradeoff between model expressivity and training efficiency.

</details>


### [134] [Foundation Models for Demand Forecasting via Dual-Strategy Ensembling](https://arxiv.org/abs/2507.22053)
*Wei Yang,Defu Cao,Yan Liu*

Main category: cs.LG

TL;DR: A unified ensemble framework improves sales forecasting by combining hierarchical and architectural strategies, outperforming baselines in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Demand forecasting is challenging due to hierarchical complexity, domain shifts, and external factors. Foundation models lack robustness under distributional change.

Method: Proposes Hierarchical Ensemble (HE) for localized patterns and Architectural Ensemble (AE) for diverse model integration.

Result: Outperforms baselines on M5 benchmark and external datasets, improving accuracy and generalization.

Conclusion: The framework effectively enhances forecasting performance in complex supply chain environments.

Abstract: Accurate demand forecasting is critical for supply chain optimization, yet
remains difficult in practice due to hierarchical complexity, domain shifts,
and evolving external factors. While recent foundation models offer strong
potential for time series forecasting, they often suffer from architectural
rigidity and limited robustness under distributional change. In this paper, we
propose a unified ensemble framework that enhances the performance of
foundation models for sales forecasting in real-world supply chains. Our method
combines two complementary strategies: (1) Hierarchical Ensemble (HE), which
partitions training and inference by semantic levels (e.g., store, category,
department) to capture localized patterns; and (2) Architectural Ensemble (AE),
which integrates predictions from diverse model backbones to mitigate bias and
improve stability. We conduct extensive experiments on the M5 benchmark and
three external sales datasets, covering both in-domain and zero-shot
forecasting. Results show that our approach consistently outperforms strong
baselines, improves accuracy across hierarchical levels, and provides a simple
yet effective mechanism for boosting generalization in complex forecasting
environments.

</details>


### [135] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: R-Stitch accelerates CoT reasoning by hybrid decoding between small and large language models, reducing latency by 85% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Current CoT acceleration methods like speculative decoding have limitations in speedup and fail to leverage small models' concise reasoning.

Method: R-Stitch uses token-level confidence-based switching between SLM and LLM, invoking LLM only when SLM confidence is low.

Result: Achieves up to 85% reduction in inference latency with negligible accuracy drop on math reasoning benchmarks.

Conclusion: R-Stitch is a practical, model-agnostic solution for efficient CoT reasoning without compromising quality.

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [136] [Online hierarchical partitioning of the output space in extreme multi-label data stream](https://arxiv.org/abs/2507.20894)
*Lara Neves,Afonso Lourenço,Alberto Cano,Goreti Marreiros*

Main category: cs.LG

TL;DR: iHOMER is an online multi-label learning framework that dynamically clusters labels and detects drift, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in multi-label data streams include evolving distributions, high-dimensional labels, and concept drift affecting label correlations.

Method: iHOMER uses incremental clustering (Jaccard similarity) and a tree-based learner with drift detection at global/local levels.

Result: iHOMER outperforms 5 global baselines by 23% and 12 local baselines by 32% in experiments.

Conclusion: iHOMER is robust for online multi-label classification, effectively handling dynamic label spaces and concept drift.

Abstract: Mining data streams with multi-label outputs poses significant challenges due
to evolving distributions, high-dimensional label spaces, sparse label
occurrences, and complex label dependencies. Moreover, concept drift affects
not only input distributions but also label correlations and imbalance ratios
over time, complicating model adaptation. To address these challenges,
structured learners are categorized into local and global methods. Local
methods break down the task into simpler components, while global methods adapt
the algorithm to the full output space, potentially yielding better predictions
by exploiting label correlations. This work introduces iHOMER (Incremental
Hierarchy Of Multi-label Classifiers), an online multi-label learning framework
that incrementally partitions the label space into disjoint, correlated
clusters without relying on predefined hierarchies. iHOMER leverages online
divisive-agglomerative clustering based on \textit{Jaccard} similarity and a
global tree-based learner driven by a multivariate \textit{Bernoulli} process
to guide instance partitioning. To address non-stationarity, it integrates
drift detection mechanisms at both global and local levels, enabling dynamic
restructuring of label partitions and subtrees. Experiments across 23
real-world datasets show iHOMER outperforms 5 state-of-the-art global
baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\%, and 12 local
baselines, such as binary relevance transformations of kNN, EFDT, ARF, and
ADWIN bagging/boosting ensembles, by 32\%, establishing its robustness for
online multi-label classification.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [137] [Replicating the behaviour of electric vehicle drivers using an agent-based reinforcement learning model](https://arxiv.org/abs/2507.21341)
*Zixin Feng,Qunshan Zhao,Alison Heppenstall*

Main category: cs.MA

TL;DR: A multi-stage reinforcement learning framework is proposed to simulate EV charging demand, addressing gaps in capturing adaptive behaviors of private drivers and large-scale geographical analysis.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model adaptive behaviors of private EV drivers and large-scale charging needs, especially for long-distance travel.

Method: A multi-stage reinforcement learning framework is developed and validated with real-world data to simulate EV charging demand.

Result: The model identifies critical 'charging deserts' and aligns with policy shifts toward rapid charging hubs for long-distance trips.

Conclusion: The framework effectively captures private driver behaviors and highlights areas needing charging infrastructure improvements.

Abstract: Despite the rapid expansion of electric vehicle (EV) charging networks,
questions remain about their efficiency in meeting the growing needs of EV
drivers. Previous simulation-based approaches, which rely on static behavioural
rules, have struggled to capture the adaptive behaviours of human drivers.
Although reinforcement learning has been introduced in EV simulation studies,
its application has primarily focused on optimising fleet operations rather
than modelling private drivers who make independent charging decisions.
Additionally, long-distance travel remains a primary concern for EV drivers.
However, existing simulation studies rarely explore charging behaviour over
large geographical scales. To address these gaps, we propose a multi-stage
reinforcement learning framework that simulates EV charging demand across large
geographical areas. We validate the model against real-world data, and identify
the training stage that most closely reflects actual driver behaviour, which
captures both the adaptive behaviours and bounded rationality of private
drivers. Based on the simulation results, we also identify critical 'charging
deserts' where EV drivers consistently have low state of charge. Our findings
also highlight recent policy shifts toward expanding rapid charging hubs along
motorway corridors and city boundaries to meet the demand from long-distance
trips.

</details>


### [138] [Agent-Based Exploration of Recommendation Systems in Misinformation Propagation](https://arxiv.org/abs/2507.21724)
*Lise Jakobsen,Anna Johanne Holden,Önder Gürcan,Özlem Özgöbek*

Main category: cs.MA

TL;DR: Agent-based modeling reveals that popularity-driven recommendation algorithms amplify misinformation, while item-based collaborative filtering and content-based methods limit fake content exposure.


<details>
  <summary>Details</summary>
Motivation: To understand how different recommendation algorithms affect misinformation spread on social networks.

Method: Simulated a synthetic environment with heterogeneous agents (users, bots, influencers) and tested four recommendation strategies: popularity-based, collaborative filtering, content-based, and random.

Result: Popularity-driven algorithms worsen misinformation spread, while item-based collaborative filtering and content-based methods reduce it. Item-based collaborative filtering outperformed prior expectations.

Conclusion: Algorithm design critically shapes online information exposure, and agent-based modeling provides realistic insights into misinformation dynamics.

Abstract: This study uses agent-based modeling to examine the impact of various
recommendation algorithms on the propagation of misinformation on online social
networks. We simulate a synthetic environment consisting of heterogeneous
agents, including regular users, bots, and influencers, interacting through a
social network with recommendation systems. We evaluate four recommendation
strategies: popularity-based, collaborative filtering, and content-based
filtering, along with a random baseline. Our results show that
popularity-driven algorithms significantly amplify misinformation, while
item-based collaborative filtering and content-based approaches are more
effective in limiting exposure to fake content. Item-based collaborative
filtering was found to perform better than previously reported in related
literature. These findings highlight the role of algorithm design in shaping
online information exposure and show that agent-based modeling can be used to
gain realistic insight into how misinformation spreads.

</details>


### [139] [Towards Cognitive Synergy in LLM-Based Multi-Agent Systems: Integrating Theory of Mind and Critical Evaluation](https://arxiv.org/abs/2507.21969)
*Adam Kostka,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: The paper explores cognitive mechanisms like adaptive theory of mind (ToM) and structured critique to enhance collaborative reasoning in multi-agent systems (MAS), showing improved coherence and adaptability in agent interactions.


<details>
  <summary>Details</summary>
Motivation: Current MAS lack human-like collaborative reasoning due to missing mechanisms like recursive reasoning and mental state inference, limiting their collective intelligence.

Method: The study investigates adaptive ToM and structured critique through an empirical case study on complex decision-making.

Result: Integration of these mechanisms leads to more coherent, adaptive, and rigorous agent interactions, surpassing individual capabilities.

Conclusion: The framework advances MAS by emulating human-like collaborative reasoning, emphasizing dynamic ToM and critical evaluation for real-world challenges.

Abstract: Recently, the field of Multi-Agent Systems (MAS) has gained popularity as
researchers are trying to develop artificial intelligence capable of efficient
collective reasoning. Agents based on Large Language Models (LLMs) perform well
in isolated tasks, yet struggle with higher-order cognition required for
adaptive collaboration. Human teams achieve synergy not only through knowledge
sharing, but also through recursive reasoning, structured critique, and the
ability to infer others' mental states. Current artificial systems lack these
essential mechanisms, limiting their ability to engage in sophisticated
collective reasoning. This work explores cognitive processes that enable
effective collaboration, focusing on adaptive theory of mind (ToM) and
systematic critical evaluation. We investigate three key questions. First, how
does the ability to model others' perspectives enhance coordination and reduce
redundant reasoning? Second, to what extent does structured critique improve
reasoning quality by identifying logical gaps and mitigating biases? Third, the
interplay of these mechanisms can lead to emergent cognitive synergy, where the
collective intelligence of the system exceeds the sum of its parts. Through an
empirical case study on complex decision making, we show that the integration
of these cognitive mechanisms leads to more coherent, adaptive, and rigorous
agent interactions. This article contributes to the field of cognitive science
and AI research by presenting a structured framework that emulates human-like
collaborative reasoning MAS. It highlights the significance of dynamic ToM and
critical evaluation in advancing multi-agent systems' ability to tackle
complex, real-world challenges.

</details>


### [140] [Validating Generative Agent-Based Models of Social Norm Enforcement: From Replication to Novel Predictions](https://arxiv.org/abs/2507.22049)
*Logan Cross,Nick Haber,Daniel L. K. Yamins*

Main category: cs.MA

TL;DR: The paper presents a two-stage validation approach for using LLMs in generative agent-based modeling (GABM) to simulate human social behavior, focusing on social dilemmas. It identifies key cognitive components and tests novel predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of validating LLM-based simulations of human social behavior, particularly in mixed-motive settings.

Method: A systematic two-stage validation using social dilemma paradigms, comparing cognitive architectures (persona-based differences and theory of mind) and testing novel conditions.

Result: Key findings include the necessity of persona-based differences and theory of mind for replicating behaviors like third-party punishment (TPP) and cooperation in public goods games. Novel predictions were also tested.

Conclusion: The work provides a validation framework for GABM and demonstrates its potential to generate new insights into human social behavior.

Abstract: As large language models (LLMs) advance, there is growing interest in using
them to simulate human social behavior through generative agent-based modeling
(GABM). However, validating these models remains a key challenge. We present a
systematic two-stage validation approach using social dilemma paradigms from
psychological literature, first identifying the cognitive components necessary
for LLM agents to reproduce known human behaviors in mixed-motive settings from
two landmark papers, then using the validated architecture to simulate novel
conditions. Our model comparison of different cognitive architectures shows
that both persona-based individual differences and theory of mind capabilities
are essential for replicating third-party punishment (TPP) as a costly signal
of trustworthiness. For the second study on public goods games, this
architecture is able to replicate an increase in cooperation from the spread of
reputational information through gossip. However, an additional strategic
component is necessary to replicate the additional boost in cooperation rates
in the condition that allows both ostracism and gossip. We then test novel
predictions for each paper with our validated generative agents. We find that
TPP rates significantly drop in settings where punishment is anonymous, yet a
substantial amount of TPP persists, suggesting that both reputational and
intrinsic moral motivations play a role in this behavior. For the second paper,
we introduce a novel intervention and see that open discussion periods before
rounds of the public goods game further increase contributions, allowing groups
to develop social norms for cooperation. This work provides a framework for
validating generative agent models while demonstrating their potential to
generate novel and testable insights into human social behavior.

</details>
