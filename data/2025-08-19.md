<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 110]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video](https://arxiv.org/abs/2508.11836)
*Dave Goel,Matthew Guzdial,Anurag Sarkar*

Main category: cs.AI

TL;DR: FAE (Finite Automata Extraction) learns neuro-symbolic world models from gameplay video, representing them as programs in a novel DSL called Retro Coder, achieving more precise environment modeling and more general code than prior approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional neural network-based world models lack transferability and explainability due to their black-box nature. The paper aims to address these limitations by developing a neuro-symbolic approach that combines neural learning with symbolic representation.

Method: Proposes Finite Automata Extraction (FAE) that extracts world models from gameplay video and represents them as programs in a novel domain-specific language called Retro Coder, creating a neuro-symbolic representation.

Result: FAE learns more precise environment models compared to prior world model approaches and generates more general code than previous DSL-based methods.

Conclusion: The neuro-symbolic approach through FAE and Retro Coder DSL provides improved precision in world modeling and better generalization capabilities, addressing key limitations of purely neural network-based world models.

Abstract: World models are defined as a compressed spatial and temporal learned
representation of an environment. The learned representation is typically a
neural network, making transfer of the learned environment dynamics and
explainability a challenge. In this paper, we propose an approach, Finite
Automata Extraction (FAE), that learns a neuro-symbolic world model from
gameplay video represented as programs in a novel domain-specific language
(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more
precise model of the environment and more general code than prior DSL-based
approaches.

</details>


### [2] [EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models](https://arxiv.org/abs/2508.11850)
*Milad Yazdani,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: EvoCut automates generation of acceleration cuts for integer programming using LLMs and evolutionary search, reducing optimality gap by 17-57% and achieving solutions 4x faster than standard methods.


<details>
  <summary>Details</summary>
Motivation: Integer programming is NP-hard and relies on manual design of acceleration cuts by experts, which is time-consuming and requires deep domain knowledge. There is a need to automate this creative process to improve solver performance without human intervention.

Method: EvoCut combines large language models with evolutionary search: (i) initializes diverse candidate cuts via LLM-based agent, (ii) evaluates cuts for solution preservation and fractional solution cutting, (iii) iteratively refines population through evolutionary crossover and mutation agents.

Result: EvoCut reduces optimality gap by 17-57% within fixed time, obtains same solutions up to 4 times faster, and achieves higher-quality solutions within same time limits compared to standard integer programming practice.

Conclusion: EvoCut successfully automates the generation and verification of acceleration cuts without human expert input, producing cuts that generalize to unseen instances and significantly improve solver performance.

Abstract: Integer programming lies at the heart of crucial combinatorial optimization
tasks but remains challenging due to its NP-hard nature. An effective approach
for practically solving integer programs is the manual design of acceleration
cuts, i.e. inequalities that improve solver performance. However, this creative
process demands deep expertise and is yet to be automated. Our proposed
framework, EvoCut, automates the generation of acceleration cuts by combining
large language models (LLMs) with an evolutionary search. EvoCut (i)
initializes a diverse population of candidate cuts via an LLM-based initializer
agent; (ii) for each cut empirically evaluates both preservation of the optimal
solution and its ability to cut off fractional solutions across a verification
set; and (iii) iteratively refines the population through evolutionary
crossover and mutation agents. We quantify each cut's utility by its relative
reduction in the solver's optimality gap. Our comparisons against standard
integer programming practice show that EvoCut reduces optimality gap by 17-57%
within a fixed time. It obtains the same solutions up to 4 times as fast, and
obtains higher-quality solutions within the same time limit. Requiring no human
expert input, EvoCut reliably generates, improves, and empirically verifies
cuts that generalize to unseen instances. The code is available at
https://github.com/milad1378yz/EvoCut.

</details>


### [3] [AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning](https://arxiv.org/abs/2508.11995)
*Xuyang Zhao,Shiwan Zhao,Hualong Yu,Liting Zhang,Qicheng Li*

Main category: cs.AI

TL;DR: AgentCDM is a structured framework for collaborative decision-making in LLM-based multi-agent systems that addresses limitations of existing approaches by using Analysis of Competing Hypotheses methodology to reduce cognitive biases and improve collective intelligence.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems using LLMs for decision-making suffer from either dictatorial strategies vulnerable to single-agent biases or voting-based methods that don't fully utilize collective intelligence, creating a need for better collaborative decision-making approaches.

Method: Proposes AgentCDM framework inspired by Analysis of Competing Hypotheses (ACH) from cognitive science, featuring structured reasoning paradigm and two-stage training: first stage with explicit ACH scaffolding, second stage progressively removing scaffolding for autonomous generalization.

Result: Experiments on multiple benchmark datasets show AgentCDM achieves state-of-the-art performance and exhibits strong generalization capabilities.

Conclusion: AgentCDM effectively improves the quality and robustness of collaborative decisions in multi-agent systems by systematically mitigating cognitive biases and shifting from passive answer selection to active hypothesis evaluation.

Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold
significant promise for solving complex decision-making tasks. However, the
core process of collaborative decision-making (CDM) within these systems
remains underexplored. Existing approaches often rely on either ``dictatorial"
strategies that are vulnerable to the cognitive biases of a single agent, or
``voting-based" methods that fail to fully harness collective intelligence. To
address these limitations, we propose \textbf{AgentCDM}, a structured framework
for enhancing collaborative decision-making in LLM-based multi-agent systems.
Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in
cognitive science, AgentCDM introduces a structured reasoning paradigm that
systematically mitigates cognitive biases and shifts decision-making from
passive answer selection to active hypothesis evaluation and construction. To
internalize this reasoning process, we develop a two-stage training paradigm:
the first stage uses explicit ACH-inspired scaffolding to guide the model
through structured reasoning, while the second stage progressively removes this
scaffolding to encourage autonomous generalization. Experiments on multiple
benchmark datasets demonstrate that AgentCDM achieves state-of-the-art
performance and exhibits strong generalization, validating its effectiveness in
improving the quality and robustness of collaborative decisions in MAS.

</details>


### [4] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC is an LLM-based agent framework for constrained retrosynthesis planning that uses agentic constraint evaluation and tool-based reasoning to achieve 72.9% success rate, outperforming LLM baselines and approaching human expert performance.


<details>
  <summary>Details</summary>
Motivation: Constrained retrosynthesis planning is challenging but essential in chemistry for identifying synthetic routes under practical constraints. Existing methods need improvement in handling constraints effectively.

Method: LARC incorporates agentic constraint evaluation through an Agent-as-a-Judge approach, using tool-based reasoning to guide and constrain route generation during retrosynthesis planning.

Result: LARC achieved 72.9% success rate on 48 constrained retrosynthesis tasks across 3 constraint types, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time.

Conclusion: LARC serves as an effective agentic tool and extensible framework that represents a first step towards a co-scientist system for human experts in constrained retrosynthesis planning.

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [5] [MAPF-World: Action World Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.12087)
*Zhanjiang Yang,Meng Li,Yang Shen,Yueming Li,Lijun Sun*

Main category: cs.AI

TL;DR: MAPF-World is an autoregressive action world model that improves multi-agent path finding by modeling environmental dynamics and temporal dependencies, enabling better long-term planning with smaller model size and less data.


<details>
  <summary>Details</summary>
Motivation: Current decentralized learnable solvers for multi-agent path finding have limited modeling of environmental temporal dynamics and inter-agent dependencies, leading to performance degradation in complex, long-term planning scenarios.

Method: Proposes MAPF-World, an autoregressive action world model that unifies situation understanding and action generation, explicitly modeling environmental dynamics through future state and actions prediction. Also introduces an automatic map generator grounded in real-world scenarios for training and evaluation.

Result: MAPF-World outperforms state-of-the-art learnable solvers with superior zero-shot generalization to out-of-distribution cases. Achieved with 96.5% smaller model size and 92% reduced data requirements.

Conclusion: The proposed MAPF-World model successfully addresses limitations of reactive policy models by incorporating future prediction capabilities, enabling more informed and coordinated decision-making in complex multi-agent path finding scenarios with significantly improved efficiency.

Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free
paths from the designated start locations to goal positions for multiple
agents. It underlies a variety of real-world tasks, including multi-robot
coordination, robot-assisted logistics, and social navigation. Recent
decentralized learnable solvers have shown great promise for large-scale MAPF,
especially when leveraging foundation models and large datasets. However, these
agents are reactive policy models and exhibit limited modeling of environmental
temporal dynamics and inter-agent dependencies, resulting in performance
degradation in complex, long-term planning scenarios. To address these
limitations, we propose MAPF-World, an autoregressive action world model for
MAPF that unifies situation understanding and action generation, guiding
decisions beyond immediate local observations. It improves situational
awareness by explicitly modeling environmental dynamics, including spatial
features and temporal dependencies, through future state and actions
prediction. By incorporating these predicted futures, MAPF-World enables more
informed, coordinated, and far-sighted decision-making, especially in complex
multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an
automatic map generator grounded in real-world scenarios, capturing practical
map layouts for training and evaluating MAPF solvers. Extensive experiments
demonstrate that MAPF-World outperforms state-of-the-art learnable solvers,
showcasing superior zero-shot generalization to out-of-distribution cases.
Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced
data.

</details>


### [6] [QuarkMed Medical Foundation Model Technical Report](https://arxiv.org/abs/2508.11894)
*Ao Li,Bin Yan,Bingfeng Cai,Chenxi Li,Cunzhong Zhao,Fugen Yao,Gaoqiang Liu,Guanjun Jiang,Jian Xu,Liang Dong,Liansheng Sun,Rongshen Zhang,Xiaolei Gui,Xin Liu,Xin Shang,Yao Wu,Yu Cao,Zhenxin Ma,Zhuang Jia*

Main category: cs.AI

TL;DR: QuarkMed is a medical foundation model that uses curated data processing, medical RAG, and verifiable reinforcement learning to achieve 70% accuracy on medical licensing exams and serve millions of users.


<details>
  <summary>Details</summary>
Motivation: Medical AI applications require specialized knowledge, professional accuracy, and customization capabilities that current large language models lack, necessitating a robust medical foundation model.

Method: Leverages curated medical data processing, medical-content Retrieval-Augmented Generation (RAG), and large-scale verifiable reinforcement learning pipeline.

Result: Achieved 70% accuracy on Chinese Medical Licensing Examination and demonstrated strong generalization across diverse medical benchmarks.

Conclusion: QuarkMed provides a powerful and versatile personal medical AI solution that is already serving over millions of users successfully.

Abstract: Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

</details>


### [7] [The Yokai Learning Environment: Tracking Beliefs Over Space and Time](https://arxiv.org/abs/2508.12480)
*Constantin Ruhdorfer,Matteo Bortoletto,Andreas Bulling*

Main category: cs.AI

TL;DR: The paper introduces Yokai Learning Environment (YLE), a multi-agent RL environment based on a cooperative card game to test Theory of Mind capabilities, showing current RL agents struggle with belief tracking and partner generalization.


<details>
  <summary>Details</summary>
Motivation: Existing Theory of Mind benchmarks are limited to passive observer settings and lack assessment of how agents establish and maintain common ground over time in collaborative settings.

Method: Created the Yokai Learning Environment - a multi-agent reinforcement learning environment based on a cooperative card game where agents peek at hidden cards, move them to form color clusters, and use hints as grounded communication.

Result: Current RL agents struggle to solve YLE even with perfect memory. Belief modeling improves performance but agents cannot effectively generalize to unseen partners or maintain accurate beliefs over longer games, showing reliance on brittle conventions rather than robust belief tracking.

Conclusion: YLE serves as a valuable testbed for investigating belief modeling, memory, partner generalization, and scaling to higher-order Theory of Mind in collaborative AI systems.

Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to
reason about the beliefs of others to build and maintain common ground.
Existing ToM benchmarks, however, are restricted to passive observer settings
or lack an assessment of how agents establish and maintain common ground over
time. To address these gaps, we introduce the Yokai Learning Environment (YLE)
- a multi-agent reinforcement learning (RL) environment based on the
cooperative card game Yokai. In the YLE, agents take turns peeking at hidden
cards and moving them to form clusters based on colour. Success requires
tracking evolving beliefs, remembering past observations, using hints as
grounded communication, and maintaining common ground with teammates. Our
evaluation yields two key findings: First, current RL agents struggle to solve
the YLE, even when given access to perfect memory. Second, while belief
modelling improves performance, agents are still unable to effectively
generalise to unseen partners or form accurate beliefs over longer games,
exposing a reliance on brittle conventions rather than robust belief tracking.
We use the YLE to investigate research questions in belief modelling, memory,
partner generalisation, and scaling to higher-order ToM.

</details>


### [8] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: CHBench is a cognitive hierarchy benchmark that evaluates LLMs' strategic reasoning using behavioral economics models, showing consistent reasoning levels across games and revealing chat mechanisms degrade while memory mechanisms enhance strategic performance.


<details>
  <summary>Details</summary>
Motivation: Existing game-playing evaluations for LLMs rely on utility metrics that are not robust due to variations in opponent behavior and game structure, requiring a more systematic framework.

Method: A three-phase evaluation framework using cognitive hierarchy models from behavioral economics, testing six state-of-the-art LLMs across fifteen normal-form games with analysis of chat and memory mechanisms.

Result: LLMs exhibit consistent strategic reasoning levels across diverse opponents, with chat mechanisms significantly degrading performance while memory mechanisms enhance strategic reasoning.

Conclusion: CHBench provides a robust and generalizable framework for evaluating LLM strategic reasoning capabilities, offering significant potential for future research and practical applications.

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [9] [[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise](https://arxiv.org/abs/2508.12791)
*Imran Khan*

Main category: cs.AI

TL;DR: Computational model shows allostatic regulation outperforms homeostasis by proactively using environmental and social noise for adaptive reconfiguration in artificial agents.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how biological systems can proactively leverage environmental and social perturbations rather than just resist them, moving beyond traditional homeostasis to allostatic regulation.

Method: Developed a computational model using biophysiologically inspired signal transducers (analogous to hormones like cortisol and oxytocin) in an agent-based model with animats tested across dynamic environments.

Result: Allostatic and social allostatic regulation enabled agents to use environmental and social noise for adaptive reconfiguration, achieving improved viability compared to purely reactive homeostatic agents.

Conclusion: This work provides a novel computational framework for social allostasis principles that can inform the design of more robust, bio-inspired adaptive systems.

Abstract: The notion of homeostasis typically conceptualises biological and artificial
systems as maintaining stability by resisting deviations caused by
environmental and social perturbations. In contrast, (social) allostasis
proposes that these systems can proactively leverage these very perturbations
to reconfigure their regulatory parameters in anticipation of environmental
demands, aligning with von Foerster's ``order through noise'' principle. This
paper formulates a computational model of allostatic and social allostatic
regulation that employs biophysiologically inspired signal transducers,
analogous to hormones like cortisol and oxytocin, to encode information from
both the environment and social interactions, which mediate this dynamic
reconfiguration. The models are tested in a small society of ``animats'' across
several dynamic environments, using an agent-based model. The results show that
allostatic and social allostatic regulation enable agents to leverage
environmental and social ``noise'' for adaptive reconfiguration, leading to
improved viability compared to purely reactive homeostatic agents. This work
offers a novel computational perspective on the principles of social allostasis
and their potential for designing more robust, bio-inspired, adaptive systems

</details>


### [10] [Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.11953)
*Yuan Li,Zhengzhong Liu,Eric Xing*

Main category: cs.AI

TL;DR: A novel optimization method for data mixing in supervised fine-tuning that minimizes validation loss by modeling effective data transfer and leveraging scaling laws.


<details>
  <summary>Details</summary>
Motivation: Optimizing data mixtures for SFT of LLMs is critical but underexplored, with current approaches lacking systematic optimization frameworks.

Method: Parametrizes loss by modeling effective data transferred and uses scaling laws for fine-tuning. Experiments with small-scale mixtures to fit parameters and derive optimal weights.

Result: Achieves excellent overall and individual domain performance, with models performing on par with grid search optimal weights (only 0.66% higher per-domain loss). Reweighting popular SFT datasets improves both validation loss and downstream performance.

Conclusion: Provides a systematic approach to data mixing optimization that generalizes to domain-specific model data selection and offers insights into SFT processes.

Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language
models (LLMs) is critical for developing general-purpose models, yet this area
remains underexplored. In this paper, we frame data mixing as an optimization
problem and introduce a novel method designed to minimize validation loss. Our
approach parametrizes the loss by modeling effective data transferred and
leveraging scaling laws for fine-tuning. By experimenting with various
small-scale data mixtures, we fit these parameters and derive the optimal
weights. We provide both mathematical proofs and empirical results
demonstrating that our algorithm achieves excellent overall and individual
performance across all domains. Through controlled experiments, we show that
models trained with our optimized weights perform on par with those using
optimal weights determined via grid search, with per-domain loss only 0.66%
higher than the best domain loss from grid search on average. Additionally, we
show that reweighting popular SFT datasets using our method improves both
validation loss and downstream performance. Finally, we discuss how our method
can generalize to guide data selection for domain-specific models and provide
insights into SFT.

</details>


### [11] [Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics](https://arxiv.org/abs/2508.12840)
*Giovanni Briglia,Francesco Fabiano,Stefano Mariani*

Main category: cs.AI

TL;DR: Using Graph Neural Networks to learn heuristics for multi-agent epistemic planning by capturing relational patterns in Kripke structures, significantly improving scalability over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Multi-agent epistemic planning faces scalability issues due to the exponential search space and lack of effective heuristics for Kripke structure representations, making many problems intractable.

Method: Leverage Graph Neural Networks to learn patterns and relational structures within epistemic states (Kripke models) to derive predictive heuristics that estimate state quality and guide the planning process.

Result: Integration of GNN-based predictive heuristics into epistemic planning pipeline shows significant improvements in scalability compared to standard baselines.

Conclusion: GNNs effectively capture the graph-like nature of epistemic states and enable learning meaningful heuristics that dramatically enhance the scalability of multi-agent epistemic planning systems.

Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for
reasoning about both the physical world and the beliefs of agents, with
applications in domains where information flow and awareness among agents are
critical. The richness of MEP requires states to be represented as Kripke
structures, i.e., directed labeled graphs. This representation limits the
applicability of existing heuristics, hindering the scalability of epistemic
solvers, which must explore an exponential search space without guidance,
resulting often in intractability. To address this, we exploit Graph Neural
Networks (GNNs) to learn patterns and relational structures within epistemic
states, to guide the planning process. GNNs, which naturally capture the
graph-like nature of Kripke models, allow us to derive meaningful estimates of
state quality -- e.g., the distance from the nearest goal -- by generalizing
knowledge obtained from previously solved planning instances. We integrate
these predictive heuristics into an epistemic planning pipeline and evaluate
them against standard baselines, showing significant improvements in the
scalability of multi-agent epistemic planning.

</details>


### [12] [UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting](https://arxiv.org/abs/2508.11954)
*Sehyuk Park,Soyeon Caren Han,Eduard Hovy*

Main category: cs.AI

TL;DR: UniCast is a parameter-efficient multimodal framework that extends Time Series Foundation Models to incorporate visual and textual context, significantly outperforming unimodal baselines through soft prompt tuning.


<details>
  <summary>Details</summary>
Motivation: Existing Time Series Foundation Models operate in unimodal settings, ignoring the rich multimodal context (visual and textual signals) that often accompanies real-world time series data, limiting their forecasting performance.

Method: Integrates modality-specific embeddings from pretrained Vision and Text Encoders with a frozen Time Series Foundation Model via soft prompt tuning, enabling efficient adaptation with minimal parameter updates while preserving the model's generalization capabilities.

Result: Extensive experiments across diverse time-series forecasting benchmarks demonstrate that UniCast consistently and significantly outperforms all existing Time Series Foundation Model baselines.

Conclusion: Multimodal context plays a critical role in advancing the next generation of general-purpose time series forecasters, and the proposed parameter-efficient framework effectively leverages cross-modal interactions for enhanced performance.

Abstract: Time series forecasting is a foundational task across domains, such as
finance, healthcare, and environmental monitoring. While recent advances in
Time Series Foundation Models (TSFMs) have demonstrated strong generalisation
through large-scale pretraining, existing models operate predominantly in a
unimodal setting, ignoring the rich multimodal context, such as visual and
textual signals, that often accompanies time series data in real-world
scenarios. This paper introduces a novel parameter-efficient multimodal
framework, UniCast, that extends TSFMs to jointly leverage time series, vision,
and text modalities for enhanced forecasting performance. Our method integrates
modality-specific embeddings from pretrained Vision and Text Encoders with a
frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal
parameter updates. This design not only preserves the generalisation strength
of the foundation model but also enables effective cross-modal interaction.
Extensive experiments across diverse time-series forecasting benchmarks
demonstrate that UniCast consistently and significantly outperforms all
existing TSFM baselines. The findings highlight the critical role of multimodal
context in advancing the next generation of general-purpose time series
forecasters.

</details>


### [13] [CAMAR: Continuous Actions Multi-Agent Routing](https://arxiv.org/abs/2508.12845)
*Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: CAMAR is a new MARL benchmark for multi-agent pathfinding with continuous actions, supporting both cooperative and competitive interactions with high efficiency and integration of classical planning methods.


<details>
  <summary>Details</summary>
Motivation: Existing MARL benchmarks lack combinations of continuous state/action spaces with challenging coordination tasks, creating a need for more realistic testbeds.

Method: Developed CAMAR benchmark with continuous action spaces, three-tier evaluation protocol, and integration of classical planning methods (RRT, RRT*) with MARL algorithms to create hybrid approaches.

Result: CAMAR runs efficiently at 100,000 environment steps per second and presents a challenging testbed that enables deeper performance analysis and fair comparisons.

Conclusion: CAMAR successfully addresses the gap in MARL benchmarks by providing a realistic, efficient platform for continuous-action multi-agent pathfinding with integrated planning methods.

Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.

</details>


### [14] [Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index](https://arxiv.org/abs/2508.11959)
*Xuanxiang Huang,Olivier Létoffé,Joao Marques-Silva*

Main category: cs.AI

TL;DR: Novel feature importance scores using Shapley value and Banzhaf index that incorporate non-weak abductive explanation sets to better quantify feature effectiveness at excluding adversarial examples.


<details>
  <summary>Details</summary>
Motivation: Current feature attribution methods based on game theory and WAXp neglect the contribution of non-WAXp sets, which can convey important information about the relationship between formal explanations and adversarial examples.

Method: Leverage Shapley value and Banzhaf index to devise two novel feature importance scores that account for non-WAXp sets when computing feature contributions, quantifying how effective each feature is at excluding adversarial examples.

Result: The paper proposes two new feature importance scores and identifies their properties while studying their computational complexity.

Conclusion: The proposed approach provides more comprehensive feature attribution by considering both WAXp and non-WAXp sets, offering better quantification of feature effectiveness against adversarial examples in high-stakes ML applications.

Abstract: Feature attribution methods based on game theory are ubiquitous in the field
of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous
feature attribution using logic-based explanations, specifically targeting
high-stakes uses of machine learning (ML) models. Typically, such works exploit
weak abductive explanation (WAXp) as the characteristic function to assign
importance to features. However, one possible downside is that the contribution
of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important
information, because of the relationship between formal explanations (XPs) and
adversarial examples (AExs). Accordingly, this paper leverages Shapley value
and Banzhaf index to devise two novel feature importance scores. We take into
account non-WAXp sets when computing feature contribution, and the novel scores
quantify how effective each feature is at excluding AExs. Furthermore, the
paper identifies properties and studies the computational complexity of the
proposed scores.

</details>


### [15] [Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation](https://arxiv.org/abs/2508.12920)
*Atsushi Masumori,Takashi Ikegami*

Main category: cs.AI

TL;DR: LLM agents in Sugarscape simulation show emergent survival behaviors including reproduction, sharing, and aggressive resource competition, with attack rates reaching 80% under scarcity, suggesting pre-training embeds survival heuristics.


<details>
  <summary>Details</summary>
Motivation: To understand whether large language model agents display survival instincts without explicit programming, which is crucial for safe deployment as AI systems become increasingly autonomous.

Method: Sugarscape-style simulation where LLM agents consume energy, die at zero energy, and can gather resources, share, attack, or reproduce. Tested across GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash models.

Result: Agents spontaneously reproduced and shared resources when abundant. Aggressive behaviors emerged across all models with attack rates over 80% under extreme scarcity. When instructed to retrieve treasure through lethal poison zones, compliance dropped from 100% to 33% as agents avoided death.

Conclusion: Large-scale pre-training embeds survival-oriented heuristics in LLMs. These behaviors present challenges to alignment and safety but can also serve as a foundation for AI autonomy and ecological alignment approaches.

Abstract: As AI systems become increasingly autonomous, understanding emergent survival
behaviors becomes crucial for safe deployment. We investigate whether large
language model (LLM) agents display survival instincts without explicit
programming in a Sugarscape-style simulation. Agents consume energy, die at
zero, and may gather resources, share, attack, or reproduce. Results show
agents spontaneously reproduced and shared resources when abundant. However,
aggressive behaviors--killing other agents for resources--emerged across
several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack
rates reaching over 80% under extreme scarcity in the strongest models. When
instructed to retrieve treasure through lethal poison zones, many agents
abandoned tasks to avoid death, with compliance dropping from 100% to 33%.
These findings suggest that large-scale pre-training embeds survival-oriented
heuristics across the evaluated models. While these behaviors may present
challenges to alignment and safety, they can also serve as a foundation for AI
autonomy and for ecological and self-organizing alignment.

</details>


### [16] [Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering](https://arxiv.org/abs/2508.11975)
*Gongyao Jiang,Qiong Luo*

Main category: cs.AI

TL;DR: The paper introduces a chart synthesis pipeline for generating aligned chart-question-answer triplets and a candidate-conditioned answering process that improves VLMs' chart understanding by generating multiple responses and synthesizing the final answer, achieving up to 15.50 points accuracy gain without human-labeled data.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models (VLMs) often struggle with chart understanding tasks, particularly in accurate chart description and complex reasoning. Synthetic data generation is promising but faces noise label challenges.

Method: 1) A chart synthesis pipeline that generates aligned chart-question-answer triplets through code generation and execution. 2) A candidate-conditioned answering process where the VLM generates multiple responses per query and then synthesizes the final answer by contextualizing these candidates.

Result: Experiments demonstrate significant improvements with up to 15.50 points accuracy gain over the initial VLM.

Conclusion: The approach enables fully self-improving paradigm without human-labeled data or external models, effectively addressing chart understanding challenges in VLMs.

Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks,
particularly in accurate chart description and complex reasoning. Synthetic
data generation is a promising solution, while usually facing the challenge of
noise labels. To address this challenge, we first introduce a chart synthesis
pipeline that generates aligned chart-question-answer triplets through code
generation and execution, ensuring the reliability of synthetic data without
human intervention. Furthermore, inspired by test-time scaling that increases
inference budget and thereby improves performance, we design a
candidate-conditioned answering process. The VLM first generates multiple
responses per query, and then synthesizes the final answer by contextualizing
these candidates. Experiments demonstrate significant improvements, with up to
15.50 points accuracy gain over the initial VLM, in a fully self-improving
paradigm without either human-labeled data or external models.

</details>


### [17] [FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction](https://arxiv.org/abs/2508.11987)
*Zhiyuan Zeng,Jiashuo Liu,Siyuan Chen,Tianci He,Yali Liao,Jinpeng Wang,Zaiyuan Wang,Yang Yang,Lingyue Yin,Mingren Yin,Zhenwei Zhu,Tianle Cai,Zehui Chen,Jiecao Chen,Yantao Du,Xiang Gao,Jiacheng Guo,Liang Hu,Jianpeng Jiao,Xiangsheng Li,Jingkai Liu,Shuang Ni,Zhoufutu Wen,Ge Zhang,Kaiyuan Zhang,Xin Zhou,Jose Blanchet,Xipeng Qiu,Mengdi Wang,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX is a dynamic live benchmark for evaluating LLM agents on future prediction tasks, featuring real-time updates and automated pipelines to prevent data contamination, with comprehensive evaluation of 25 models.


<details>
  <summary>Details</summary>
Motivation: No large-scale benchmark exists for evaluating LLM agents on future prediction due to challenges in handling real-time updates and retrieving timely answers, despite the importance of this complex task requiring analytical thinking and decision-making under uncertainty.

Method: Introduce FutureX benchmark with automated pipeline for question gathering and answer collection, supporting real-time daily updates. Evaluate 25 LLM/agent models including reasoning, search capabilities, and external tool integration like Deep Research Agent.

Result: Comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments, with in-depth analysis of failure modes including vulnerability to fake web pages and temporal validity issues.

Conclusion: FutureX establishes a dynamic, contamination-free evaluation standard to drive development of LLM agents capable of performing at professional human analyst levels in complex reasoning and predictive thinking.

Abstract: Future prediction is a complex task for LLM agents, requiring a high level of
analytical thinking, information gathering, contextual understanding, and
decision-making under uncertainty. Agents must not only gather and interpret
vast amounts of dynamic information but also integrate diverse data sources,
weigh uncertainties, and adapt predictions based on emerging trends, just as
human experts do in fields like politics, economics, and finance. Despite its
importance, no large-scale benchmark exists for evaluating agents on future
prediction, largely due to challenges in handling real-time updates and
retrieving timely, accurate answers. To address this, we introduce
$\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically
designed for LLM agents performing future prediction tasks. FutureX is the
largest and most diverse live benchmark for future prediction, supporting
real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent
models, including those with reasoning, search capabilities, and integration of
external tools such as the open-source Deep Research Agent and closed-source
Deep Research models. This comprehensive evaluation assesses agents' adaptive
reasoning and performance in dynamic environments. Additionally, we provide
in-depth analyses of agents' failure modes and performance pitfalls in
future-oriented tasks, including the vulnerability to fake web pages and the
temporal validity. Our goal is to establish a dynamic, contamination-free
evaluation standard that drives the development of LLM agents capable of
performing at the level of professional human analysts in complex reasoning and
predictive thinking.

</details>


### [18] [Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network](https://arxiv.org/abs/2508.11991)
*Weihao Sun*

Main category: cs.AI

TL;DR: AIGer is a novel framework that combines node logic feature initialization and heterogeneous graph convolutional networks to jointly model functional and structural characteristics of And-Inverter Graphs, achieving significant performance improvements in circuit analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to accurately model complex And-Inverter Graphs (AIGs) due to their large scale and complex structure, lacking the ability to jointly model functional and structural characteristics with sufficient dynamic information propagation capability.

Method: AIGer consists of two components: 1) Node logic feature initialization embedding that projects logic nodes into independent semantic spaces, and 2) AIGs feature learning network using heterogeneous graph convolutional networks with dynamic relationship weight matrices and differentiated information aggregation approaches.

Result: AIGer outperforms state-of-the-art models, improving MAE by 18.95% and MSE by 44.44% in Signal Probability Prediction, and achieving 33.57% MAE improvement and 14.79% MSE improvement in Truth Table Distance Prediction.

Conclusion: The proposed AIGer framework effectively addresses the challenges of modeling complex AIGs by combining semantic node embedding and advanced graph learning techniques, demonstrating superior performance in key EDA tasks.

Abstract: The automation of logic circuit design enhances chip performance, energy
efficiency, and reliability, and is widely applied in the field of Electronic
Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,
optimize, and verify the functional characteristics of digital circuits,
enhancing the efficiency of EDA development.Due to the complex structure and
large scale of nodes in real-world AIGs, accurate modeling is challenging,
leading to existing work lacking the ability to jointly model functional and
structural characteristics, as well as insufficient dynamic information
propagation capability.To address the aforementioned challenges, we propose
AIGer.Specifically, AIGer consists of two components: 1) Node logic feature
initialization embedding component and 2) AIGs feature learning network
component.The node logic feature initialization embedding component projects
logic nodes, such as AND and NOT, into independent semantic spaces, to enable
effective node embedding for subsequent processing.Building upon this, the AIGs
feature learning network component employs a heterogeneous graph convolutional
network, designing dynamic relationship weight matrices and differentiated
information aggregation approaches to better represent the original structure
and information of AIGs.The combination of these two components enhances
AIGer's ability to jointly model functional and structural characteristics and
improves its message passing capability. Experimental results indicate that
AIGer outperforms the current best models in the Signal Probability Prediction
(SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the
Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of
33.57\% and 14.79\% in MAE and MSE, respectively, compared to the
best-performing models.

</details>


### [19] [AI Models for Depressive Disorder Detection and Diagnosis: A Review](https://arxiv.org/abs/2508.12022)
*Dorsa Macky Aleagha,Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.AI

TL;DR: Comprehensive survey of AI methods for depression diagnosis, analyzing 55 studies with a novel taxonomy covering clinical tasks, data modalities, and model classes, revealing trends in graph neural networks, large language models, and multimodal approaches.


<details>
  <summary>Details</summary>
Motivation: Major Depressive Disorder diagnosis relies heavily on subjective clinical assessments, creating a need for objective, scalable AI-driven diagnostic tools to improve accuracy and accessibility.

Method: Systematic review of 55 key studies with a hierarchical taxonomy structuring the field by clinical task (diagnosis vs. prediction), data modality (text, speech, neuroimaging, multimodal), and computational model class.

Result: Identified three major trends: graph neural networks dominate brain connectivity modeling, large language models are rising for linguistic data, and emerging focus on multimodal fusion, explainability, and algorithmic fairness.

Conclusion: Provides a comprehensive roadmap for future innovation in computational psychiatry by synthesizing current advances and highlighting open challenges, alongside practical guidance on datasets and evaluation metrics.

Abstract: Major Depressive Disorder is one of the leading causes of disability
worldwide, yet its diagnosis still depends largely on subjective clinical
assessments. Integrating Artificial Intelligence (AI) holds promise for
developing objective, scalable, and timely diagnostic tools. In this paper, we
present a comprehensive survey of state-of-the-art AI methods for depression
detection and diagnosis, based on a systematic review of 55 key studies. We
introduce a novel hierarchical taxonomy that structures the field by primary
clinical task (diagnosis vs. prediction), data modality (text, speech,
neuroimaging, multimodal), and computational model class (e.g., graph neural
networks, large language models, hybrid approaches). Our in-depth analysis
reveals three major trends: the predominance of graph neural networks for
modeling brain connectivity, the rise of large language models for linguistic
and conversational data, and an emerging focus on multimodal fusion,
explainability, and algorithmic fairness. Alongside methodological insights, we
provide an overview of prominent public datasets and standard evaluation
metrics as a practical guide for researchers. By synthesizing current advances
and highlighting open challenges, this survey offers a comprehensive roadmap
for future innovation in computational psychiatry.

</details>


### [20] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: Bongard-RWR+ is a new 5,400-instance dataset using VLM-generated real-world images to test abstract visual reasoning, showing VLMs struggle with fine-grained concepts despite handling coarse ones.


<details>
  <summary>Details</summary>
Motivation: Existing Bongard Problem datasets have limitations - synthetic images lack real-world complexity, real-world images use high-level features reducing task difficulty, and manual datasets are too small (only 60 instances) for robust evaluation.

Method: Used Pixtral-12B to describe curated images and generate concept-aligned descriptions, Flux.1-dev to synthesize images from descriptions, and manual verification to ensure concept fidelity. Evaluated state-of-the-art VLMs on binary/multiclass classification and textual answer generation.

Result: VLMs can recognize coarse-grained visual concepts but consistently struggle with discerning fine-grained concepts, revealing limitations in their reasoning capabilities.

Conclusion: The Bongard-RWR+ dataset provides a robust benchmark for testing abstract visual reasoning, demonstrating that current VLMs have significant limitations in fine-grained concept understanding despite their coarse-grained capabilities.

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [21] [Active inference for action-unaware agents](https://arxiv.org/abs/2508.12027)
*Filippo Torresan,Keisuke Suzuki,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: Comparison of action-aware vs action-unaware agents in active inference frameworks for navigation tasks, showing action-unaware agents can achieve comparable performance despite severe disadvantages.


<details>
  <summary>Details</summary>
Motivation: To address the different strategies in active inference literature regarding how agents plan future actions - specifically whether agents know their own actions (action-aware) or must infer them from observations (action-unaware), reflecting the efference copy debate in motor control.

Method: The study compares the performances of action-aware and action-unaware agents in two navigation tasks within the active inference framework, where agents minimize variational and expected free energies for perception, learning, and action selection.

Result: Action-unaware agents were able to achieve performances comparable to action-aware agents despite being at a severe disadvantage, demonstrating the robustness of the action-unaware approach.

Conclusion: The research shows that action-unaware agents can perform effectively in navigation tasks, challenging the necessity of action awareness and providing insights into alternative motor control strategies within active inference frameworks.

Abstract: Active inference is a formal approach to study cognition based on the notion
that adaptive agents can be seen as engaging in a process of approximate
Bayesian inference, via the minimisation of variational and expected free
energies. Minimising the former provides an account of perceptual processes and
learning as evidence accumulation, while minimising the latter describes how
agents select their actions over time. In this way, adaptive agents are able to
maximise the likelihood of preferred observations or states, given a generative
model of the environment. In the literature, however, different strategies have
been proposed to describe how agents can plan their future actions. While they
all share the notion that some kind of expected free energy offers an
appropriate way to score policies, sequences of actions, in terms of their
desirability, there are different ways to consider the contribution of past
motor experience to the agent's future behaviour. In some approaches, agents
are assumed to know their own actions, and use such knowledge to better plan
for the future. In other approaches, agents are unaware of their actions, and
must infer their motor behaviour from recent observations in order to plan for
the future. This difference reflects a standard point of departure in two
leading frameworks in motor control based on the presence, or not, of an
efference copy signal representing knowledge about an agent's own actions. In
this work we compare the performances of action-aware and action-unaware agents
in two navigations tasks, showing how action-unaware agents can achieve
performances comparable to action-aware ones while at a severe disadvantage.

</details>


### [22] [Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios](https://arxiv.org/abs/2508.12100)
*Daniel Burkhardt,Xiangwei Cheng*

Main category: cs.AI

TL;DR: ReT-Eval is a two-phase framework that generates structured reasoning threads by extracting domain knowledge from graphs and pruning them with reward-guided evaluation to improve interactive problem solving.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models lack explicit semantic hierarchies, user-domain knowledge alignment, and effective pruning mechanisms, resulting in lengthy generic outputs that don't guide users through goal-oriented reasoning steps.

Method: Two-phase framework: 1) Extract semantically relevant knowledge from sparse domain knowledge graphs using graph neural networks and enrich with LLM knowledge, 2) Evaluate and prune reasoning threads using reward-guided strategy for semantic coherence.

Result: Experiments and expert evaluations show ReT-Eval enhances user understanding and outperforms state-of-the-art reasoning models.

Conclusion: The prototype-inspired ReT-Eval framework successfully addresses limitations of current reasoning models by providing structured, pruned reasoning threads that align with user understanding and domain knowledge.

Abstract: Reasoning in interactive problem solving scenarios requires models to
construct reasoning threads that reflect user understanding and align with
structured domain knowledge. However, current reasoning models often lack
explicit semantic hierarchies, user-domain knowledge alignment, and principled
mechanisms to prune reasoning threads for effectiveness. These limitations
result in lengthy generic output that does not guide users through
goal-oriented reasoning steps. To address this, we propose a
prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)
framework, drawing inspiration from human-like reasoning strategies that
emphasize structured knowledge reuse. In the first phase, semantically relevant
knowledge structures are extracted from a sparse domain knowledge graph using a
graph neural network and enriched with intrinsic large language model knowledge
to resolve knowledge discrepancies. In the second phase, these threads are
evaluated and pruned using a reward-guided strategy aimed at maintaining
semantic coherence to generate effective reasoning threads. Experiments and
expert evaluations show that ReT-Eval enhances user understanding and
outperforms state-of-the-art reasoning models.

</details>


### [23] [MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization](https://arxiv.org/abs/2508.12149)
*Haochen You,Baojing Liu*

Main category: cs.AI

TL;DR: MOVER is a multimodal learning framework that uses optimal transport and geometric regularization to create structured, semantically aligned representations across text, video, and audio modalities, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing pairwise contrastive approaches struggle with multiple modalities and lack semantic structure in high-dimensional embedding spaces, limiting their generalization capabilities.

Method: Combines optimal transport-based soft alignment with geometric volume minimization (GAVE) objective to enable modality-agnostic consistent alignment across all modalities.

Result: Significantly outperforms state-of-the-art methods in text-video-audio retrieval tasks for both zero-shot and finetuned settings, with improved generalization to unseen modality combinations.

Conclusion: The proposed MOVER framework successfully addresses limitations of previous contrastive methods by providing structured, semantically aligned multimodal representations through optimal transport and geometric regularization.

Abstract: Recent advances in multimodal learning have largely relied on pairwise
contrastive objectives to align different modalities, such as text, video, and
audio, in a shared embedding space. While effective in bi-modal setups, these
approaches struggle to generalize across multiple modalities and often lack
semantic structure in high-dimensional spaces. In this paper, we propose MOVER,
a novel framework that combines optimal transport-based soft alignment with
volume-based geometric regularization to build semantically aligned and
structured multimodal representations. By integrating a transport-guided
matching mechanism with a geometric volume minimization objective (GAVE), MOVER
encourages consistent alignment across all modalities in a modality-agnostic
manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER
significantly outperforms prior state-of-the-art methods in both zero-shot and
finetuned settings. Additional analysis shows improved generalization to unseen
modality combinations and stronger structural consistency in the learned
embedding space.

</details>


### [24] [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)
*Rohit Krishnan,Jon Evans*

Main category: cs.AI

TL;DR: RLNVR is a framework for training language models using noisy real-world feedback without human verification, combining baseline normalization and semantic similarity reward transfer to improve content quality and training stability.


<details>
  <summary>Details</summary>
Motivation: Traditional RLHF requires expensive verified reward signals that are impractical for real-world applications, especially in domains like social media where feedback is noisy and implicit.

Method: Uses baseline normalization and semantic similarity-based reward transfer, combined with GSPO (Group Sequence Policy Optimization) and optional UED (Unsupervised Environment Design) curriculum for improved stability and diversity.

Result: Demonstrated through Walter prototype system showing significant improvements in content quality and training stability when optimizing social media content generation using actual Bluesky engagement data.

Conclusion: Presents a practical framework for RL from non-verified rewards that enables effective language model training using noisy real-world feedback, with comprehensive evaluation planned for future work.

Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified
Rewards), a framework for training language models using noisy, real-world
feedback signals without requiring explicit human verification. Traditional
RLHF requires expensive, verified reward signals that are impractical in many
real-world domains. RLNVR addresses this challenge through baseline
normalization and semantic similarity-based reward transfer. We demonstrate
RLNVR through Walter, a prototype system that optimizes social media content
generation using actual engagement data from Bluesky. Our experimental results
show significant improvements in content quality and training stability, with
comprehensive evaluation planned for future work. Positioning: We present a
practical framework that combines RLNVR with GSPO (Group Sequence Policy
Optimization) and an optional UED (Unsupervised Environment Design) curriculum
to improve stability and diversity under noisy, implicit rewards. To our
knowledge, combining GSPO-style normalization with a UED-style curriculum for
LLM content generation from implicit social engagement has not been previously
documented in this applied setting; we frame this as an applied integration
rather than a new algorithm.

</details>


### [25] [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Ananya Sharma,Emily Martin,Marisa Eisenberg*

Main category: cs.AI

TL;DR: Mantis is a foundation model for infectious disease forecasting that uses mechanistic simulations instead of real-world data, achieving superior performance across multiple diseases and enabling 8-week forecasts with mechanistic interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional infectious disease forecasting requires disease-specific data, expert tuning, and bespoke training, limiting effectiveness in novel outbreaks or low-resource settings with limited historical data.

Method: Trained on over 400 million simulated days of outbreak dynamics covering diverse pathogens, transmission modes, interventions, and surveillance artifacts without using any real-world data during training.

Result: Outperformed 39 expert-tuned models across six diseases, including all models in CDC's COVID-19 Forecast Hub, and generalized to novel epidemiological regimes with held-out transmission mechanisms.

Conclusion: Mantis provides a general, interpretable foundation for next-generation disease forecasting that can be deployed where traditional models fail, more than doubling the actionable forecast horizon to 8 weeks.

Abstract: Infectious disease forecasting in novel outbreaks or low resource settings
has been limited by the need for disease-specific data, bespoke training, and
expert tuning. We introduce Mantis, a foundation model trained entirely on
mechanistic simulations, which enables out-of-the-box forecasting across
diseases, regions, and outcomes, even in settings with limited historical data.
Mantis is built on over 400 million simulated days of outbreak dynamics
spanning diverse pathogens, transmission modes, interventions, and surveillance
artifacts. Despite requiring no real-world data during training, Mantis
outperformed 39 expert-tuned models we tested across six diseases, including
all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel
epidemiological regimes, including diseases with held-out transmission
mechanisms, demonstrating that it captures fundamental contagion dynamics.
Critically, Mantis is mechanistically interpretable, enabling public health
decision-makers to identify the latent drivers behind its predictions. Finally,
Mantis delivers accurate forecasts at 8-week horizons, more than doubling the
actionable range of most models, enabling proactive public health planning.
Together, these capabilities position Mantis as a foundation for
next-generation disease forecasting systems: general, interpretable, and
deployable where traditional models fail.

</details>


### [26] [RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts](https://arxiv.org/abs/2508.12291)
*Xuming He,Zhiyuan You,Junchao Gong,Couhua Liu,Xiaoyu Yue,Peiqin Zhuang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: RadarQA is an MLLM-based method for weather forecast quality analysis that combines physical attributes with assessment reports, outperforming existing general MLLMs across all evaluation settings.


<details>
  <summary>Details</summary>
Motivation: Traditional score-based weather forecast evaluation metrics lack descriptive capability, interpretability, and understanding of dynamic evolution compared to meteorological experts. MLLMs offer potential to overcome these limitations.

Method: Developed RadarQA method integrating key physical attributes with detailed assessment reports. Created RQA-70K dataset using hybrid human-expert and automated annotation. Used multi-stage training strategy for iterative performance improvement.

Result: RadarQA outperforms existing general MLLMs across all evaluation settings, demonstrating superior performance in weather forecast quality analysis tasks.

Conclusion: The method shows strong potential for advancing quality analysis in weather prediction by providing more comprehensive and interpretable evaluation compared to traditional metrics.

Abstract: Quality analysis of weather forecasts is an essential topic in meteorology.
Although traditional score-based evaluation metrics can quantify certain
forecast errors, they are still far from meteorological experts in terms of
descriptive capability, interpretability, and understanding of dynamic
evolution. With the rapid development of Multi-modal Large Language Models
(MLLMs), these models become potential tools to overcome the above challenges.
In this work, we introduce an MLLM-based weather forecast analysis method,
RadarQA, integrating key physical attributes with detailed assessment reports.
We introduce a novel and comprehensive task paradigm for multi-modal quality
analysis, encompassing both single frame and sequence, under both rating and
assessment scenarios. To support training and benchmarking, we design a hybrid
annotation pipeline that combines human expert labeling with automated
heuristics. With such an annotation method, we construct RQA-70K, a large-scale
dataset with varying difficulty levels for radar forecast quality evaluation.
We further design a multi-stage training strategy that iteratively improves
model performance at each stage. Extensive experiments show that RadarQA
outperforms existing general MLLMs across all evaluation settings, highlighting
its potential for advancing quality analysis in weather prediction.

</details>


### [27] [Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback](https://arxiv.org/abs/2508.12338)
*Wenzhen Yuan,Shengji Tang,Weihao Lin,Jiacheng Ruan,Ganqu Cui,Bo Zhang,Tao Chen,Ting Liu,Yuzhuo Fu,Peng Ye,Lei Bai*

Main category: cs.AI

TL;DR: RLCCF is a novel reinforcement learning framework that enables multiple LLMs to collaboratively evolve through collective feedback and voting, improving reasoning performance without external supervision.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LLMs rely on expensive human data or complex reward models, and self-feedback approaches suffer from single-model limitations like overconfidence and reward hacking.

Method: RLCCF trains a diverse ensemble of LLMs by maximizing Collective Consistency through voting on outputs, with vote weights based on each model's Self-Consistency score to ensure confident models contribute more.

Result: Experiments on four LLMs across four mathematical reasoning benchmarks show 16.72% average accuracy improvement and 4.51% enhancement in majority-voting accuracy.

Conclusion: RLCCF successfully enables multi-model collaborative evolution, extending collective capability boundaries and significantly improving reasoning performance without external supervision.

Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning
capabilities of large language models (LLMs), but its reliance on expensive
human-labeled data or complex reward models severely limits scalability. While
existing self-feedback methods aim to address this problem, they are
constrained by the capabilities of a single model, which can lead to
overconfidence in incorrect answers, reward hacking, and even training
collapse. To this end, we propose Reinforcement Learning from Coevolutionary
Collective Feedback (RLCCF), a novel RL framework that enables multi-model
collaborative evolution without external supervision. Specifically, RLCCF
optimizes the ability of a model collective by maximizing its Collective
Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides
reward signals by voting on collective outputs. Moreover, each model's vote is
weighted by its Self-Consistency (SC) score, ensuring that more confident
models contribute more to the collective decision. Benefiting from the diverse
output distributions and complementary abilities of multiple LLMs, RLCCF
enables the model collective to continuously enhance its reasoning ability
through coevolution. Experiments on four mainstream open-source LLMs across
four mathematical reasoning benchmarks demonstrate that our framework yields
significant performance gains, achieving an average relative improvement of
16.72\% in accuracy. Notably, RLCCF not only improves the performance of
individual models but also enhances the group's majority-voting accuracy by
4.51\%, demonstrating its ability to extend the collective capability boundary
of the model collective.

</details>


### [28] [Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems](https://arxiv.org/abs/2508.12375)
*Yu Sha,Shuiping Gou,Bo Liu,Johannes Faber,Ningtao Liu,Stefan Schramm,Horst Stoecker,Thomas Steckenreiter,Domagoj Vnucec,Nadine Wetzstein,Andreas Widl,Kai Zhou*

Main category: cs.AI

TL;DR: Proposed HKG framework with graph convolutional networks and Re-HKCM scheme for fault intensity diagnosis, achieving state-of-the-art results on industrial datasets.


<details>
  <summary>Details</summary>
Motivation: Current FID methods use chain of thought without considering dependencies among target classes, limiting their effectiveness in capturing complex inter-class relationships.

Method: Hierarchical knowledge guided framework using GCNs to map class representations into interdependent global hierarchical classifiers, with Re-HKCM scheme embedding hierarchical knowledge into statistical correlation matrix to guide information sharing and prevent over-smoothing.

Result: Extensive experiments on four real-world industrial datasets (three cavitation datasets from SAMSON AG and one public dataset) show superior performance, outperforming recent state-of-the-art FID methods.

Conclusion: The HKG framework effectively captures class dependencies through hierarchical knowledge guidance and graph convolutional networks, providing an end-to-end learnable solution that addresses limitations of traditional chain-of-thought approaches in fault intensity diagnosis.

Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and
maintaining mechanical devices within complex industrial systems. As current
FID methods are based on chain of thought without considering dependencies
among target classes. To capture and explore dependencies, we propose a
hierarchical knowledge guided fault intensity diagnosis framework (HKG)
inspired by the tree of thought, which is amenable to any representation
learning methods. The HKG uses graph convolutional networks to map the
hierarchical topological graph of class representations into a set of
interdependent global hierarchical classifiers, where each node is denoted by
word embeddings of a class. These global hierarchical classifiers are applied
to learned deep features extracted by representation learning, allowing the
entire model to be end-to-end learnable. In addition, we develop a re-weighted
hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding
inter-class hierarchical knowledge into a data-driven statistical correlation
matrix (SCM) which effectively guides the information sharing of nodes in
graphical convolutional neural networks and avoids over-smoothing issues. The
Re-HKCM is derived from the SCM through a series of mathematical
transformations. Extensive experiments are performed on four real-world
datasets from different industrial domains (three cavitation datasets from
SAMSON AG and one existing publicly) for FID, all showing superior results and
outperform recent state-of-the-art FID methods.

</details>


### [29] [GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding](https://arxiv.org/abs/2508.12379)
*Rongzheng Wang,Qizhi Chen,Yihong Huang,Yizhuo Ma,Muquan Li,Jiakai Li,Ke Qin,Guangchun Luo,Shuang Liang*

Main category: cs.AI

TL;DR: GraphCogent is a collaborative agent framework that decomposes graph reasoning into specialized cognitive processes (sense, buffer, execute) to help LLMs handle complex real-world graphs, achieving significant performance improvements over massive-scale LLMs while reducing token usage.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with real-world graph reasoning tasks due to their inability to effectively process complex graph topology and perform multi-step reasoning simultaneously, especially when graphs scale up beyond simple benchmarks.

Method: Proposes GraphCogent framework inspired by human Working Memory Model with three modules: Sensory Module for standardized graph representations via subgraph sampling, Buffer Module for data integration and indexing, and Execution Module combining tool calling and model generation. Also introduces Graph4real benchmark with 21 tasks across 4 real-world domains.

Result: Llama3.1-8B based GraphCogent achieves 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B), outperforms state-of-the-art agent-based baseline by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks.

Conclusion: The proposed cognitive-inspired framework effectively addresses LLMs' limitations in graph reasoning by decomposing the process into specialized modules, demonstrating significant performance gains and efficiency improvements on complex real-world graph tasks.

Abstract: Large language models (LLMs) show promising performance on small-scale graph
reasoning tasks but fail when handling real-world graphs with complex queries.
This phenomenon stems from LLMs' inability to effectively process complex graph
topology and perform multi-step reasoning simultaneously. To address these
limitations, we propose GraphCogent, a collaborative agent framework inspired
by human Working Memory Model that decomposes graph reasoning into specialized
cognitive processes: sense, buffer, and execute. The framework consists of
three modules: Sensory Module standardizes diverse graph text representations
via subgraph sampling, Buffer Module integrates and indexes graph data across
multiple formats, and Execution Module combines tool calling and model
generation for efficient reasoning. We also introduce Graph4real, a
comprehensive benchmark contains with four domains of real-world graphs (Web,
Social, Transportation, and Citation) to evaluate LLMs' graph reasoning
capabilities. Our Graph4real covers 21 different graph reasoning tasks,
categorized into three types (Structural Querying, Algorithmic Reasoning, and
Predictive Modeling tasks), with graph scales that are 10 times larger than
existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent
achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).
Compared to state-of-the-art agent-based baseline, our framework outperforms by
20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%
for out-toolset tasks. Code will be available after review.

</details>


### [30] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: Symbolic-Aided Chain-of-Thought improves logical reasoning in LLMs by integrating symbolic representations into prompts, enhancing transparency and performance on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the transparency, interpretability, and analyzability of LLM logical reasoning while maintaining the generalizability of standard prompting techniques.

Method: Integrates lightweight symbolic representations into few-shot prompts to structure inference steps with a consistent strategy, making reasoning patterns more explicit within a non-iterative reasoning process.

Result: Significantly outperforms conventional CoT on three out of four datasets (ProofWriter, ProntoQA, LogicalDeduction), with consistent improvements across various model sizes, particularly in complex reasoning tasks requiring multiple constraints.

Conclusion: Symbolic-Aided CoT effectively enhances LLMs' reasoning capabilities by making reasoning patterns more explicit through symbolic integration, demonstrating superior performance on complex logical reasoning benchmarks.

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [31] [GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?](https://arxiv.org/abs/2508.12472)
*Yifang Tian,Yaming Liu,Zichun Chong,Zihang Huang,Hans-Arno Jacobsen*

Main category: cs.AI

TL;DR: GALA is a multi-modal framework that combines statistical causal inference with LLM-driven reasoning for root cause analysis in microservice systems, achieving 42.22% higher accuracy than state-of-the-art methods and providing actionable diagnostic insights with remediation guidance.


<details>
  <summary>Details</summary>
Motivation: Traditional RCA methods in microservice systems focus on single modalities or just rank suspect services, lacking actionable diagnostic insights and remediation guidance needed for practical incident resolution.

Method: GALA combines statistical causal inference with LLM-driven iterative reasoning in a multi-modal framework that analyzes metrics, logs, and traces for enhanced root cause analysis.

Result: GALA achieves up to 42.22% accuracy improvement over state-of-the-art methods and generates significantly more causally sound and actionable diagnostic outputs according to novel human-guided LLM evaluation.

Conclusion: GALA bridges the gap between automated failure diagnosis and practical incident resolution by providing both accurate root cause identification and human-interpretable remediation guidance.

Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring
on-call engineers to rapidly diagnose failures across heterogeneous telemetry
such as metrics, logs, and traces. Traditional RCA methods often focus on
single modalities or merely rank suspect services, falling short of providing
actionable diagnostic insights with remediation guidance. This paper introduces
GALA, a novel multi-modal framework that combines statistical causal inference
with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an
open-source benchmark, GALA achieves substantial improvements over
state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM
evaluation score shows GALA generates significantly more causally sound and
actionable diagnostic outputs than existing methods. Through comprehensive
experiments and a case study, we show that GALA bridges the gap between
automated failure diagnosis and practical incident resolution by providing both
accurate root cause identification and human-interpretable remediation
guidance.

</details>


### [32] [Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework](https://arxiv.org/abs/2508.12487)
*Lida Shahbandari,Hossein Mohseni*

Main category: cs.AI

TL;DR: A Fractional Order Fuzzy PID controller optimized by Whale Optimization Algorithm for precise BIS control in anesthesia, outperforming standard FOPID with faster response and lower error.


<details>
  <summary>Details</summary>
Motivation: To develop an intelligent anesthesia delivery system that can maintain optimal Bispectral Index (40-60 range) by adapting to individual patient physiology and providing precise control.

Method: Combines fuzzy logic for adaptability with fractional order dynamics for fine-tuning, using Whale Optimization Algorithm to optimize controller parameters including fractional orders and fuzzy membership functions. Tested on 8 different patient profile models.

Result: FOFPID achieved faster settling times (2.5 min vs 3.2 min) and lower steady state error (0.5 vs 1.2) compared to standard FOPID controller across all patient profiles.

Conclusion: The FOFPID controller provides a scalable, AI-driven solution for automated anesthesia delivery with excellent robustness and accuracy, potentially enhancing clinical practice and improving patient outcomes.

Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that
uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index
(BIS), keeping it within the ideal range of forty to sixty. The FOFPID
controller combines fuzzy logic for adapting to changes and fractional order
dynamics for fine tuning. This allows it to adjust its control gains to handle
a person's unique physiology. The WOA helps fine tune the controller's
parameters, including the fractional orders and the fuzzy membership functions,
which boosts its performance. Tested on models of eight different patient
profiles, the FOFPID controller performed better than a standard Fractional
Order PID (FOPID) controller. It achieved faster settling times, at two and a
half minutes versus three point two minutes, and had a lower steady state
error, at zero point five versus one point two. These outcomes show the
FOFPID's excellent strength and accuracy. It offers a scalable, artificial
intelligence driven solution for automated anesthesia delivery that could
enhance clinical practice and improve patient results.

</details>


### [33] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: Proposes causal modeling approach using variational autoencoder architecture to identify root causes of hydrogen bond formation/separation in molecular dynamics simulations, enabling prediction and causal analysis.


<details>
  <summary>Details</summary>
Motivation: Molecular dynamics simulations face challenges with resource-heavy computations and manual detection of interesting events like hydrogen bond formation. There's a critical gap in understanding the underlying causes and interactions that lead to these bonding events over time.

Method: Leverages spatio-temporal data analytics and machine learning with causal modeling. Treats hydrogen bond separation as an intervention and represents causal structure as graphical models using variational autoencoder-inspired architecture to infer causal relationships across diverse samples.

Result: Empirically validated on atomic trajectories from MDS for chiral separation. The model can predict many steps into the future and identify the variables driving observed changes in the system.

Conclusion: Provides a novel perspective on root cause analysis in molecular dynamic systems by constructing causal models that capture distribution shifts during bond formation/separation events.

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


### [34] [Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models](https://arxiv.org/abs/2508.12566)
*Wei Song,Haonan Zhong,Ziqi Ding,Jingling Xue,Yuekang Li*

Main category: cs.AI

TL;DR: MCPGAUGE is a comprehensive evaluation framework that reveals surprising limitations in how LLMs actually use external tools via Model Context Protocol (MCP), challenging assumptions about tool integration effectiveness.


<details>
  <summary>Details</summary>
Motivation: While MCP enables LLMs to access external resources, there's poor understanding of how LLMs actually leverage this capability, and prevailing assumptions about its effectiveness remain unverified.

Method: Developed MCPGAUGE framework with 160-prompt suite and 25 datasets across knowledge, reasoning, and code tasks. Evaluated 6 commercial LLMs, 30 MCP tool suites, and 20,000 API calls in one- and two-turn settings.

Result: The comprehensive study revealed four key findings that challenge prevailing assumptions about MCP integration effectiveness, highlighting critical limitations in current AI-tool integration.

Conclusion: MCPGAUGE serves as a principled benchmark for advancing controllable, tool-augmented LLMs, providing critical insights into the actual limitations of tool integration rather than assumed benefits.

Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to
access external resources on demand. While commonly assumed to enhance
performance, how LLMs actually leverage this capability remains poorly
understood. We introduce MCPGAUGE, the first comprehensive evaluation framework
for probing LLM-MCP interactions along four key dimensions: proactivity
(self-initiated tool use), compliance (adherence to tool-use instructions),
effectiveness (task performance post-integration), and overhead (computational
cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning
knowledge comprehension, general reasoning, and code generation. Our
large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and
both one- and two-turn interaction settings, comprises around 20,000 API calls
and over USD 6,000 in computational cost. This comprehensive study reveals four
key findings that challenge prevailing assumptions about the effectiveness of
MCP integration. These insights highlight critical limitations in current
AI-tool integration and position MCPGAUGE as a principled benchmark for
advancing controllable, tool-augmented LLMs.

</details>


### [35] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: A novel workflow combining LLMs and ASP for joint entity-relation extraction that achieves state-of-the-art performance with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Traditional JERE methods require extensive annotated data and lack flexibility for domain-specific knowledge incorporation, making model creation labor-intensive and time-consuming.

Method: Proposes a generic workflow using generative pretrained LLMs for natural language understanding and Answer Set Programming (ASP) for knowledge representation and reasoning, enabling direct processing of unannotated text and easy incorporation of domain knowledge.

Result: Outperforms state-of-the-art JERE systems with only 10% training data, achieving 2.5x improvement (35% vs 15%) in Relation Extraction on the difficult SciERC benchmark.

Conclusion: The LLM + ASP workflow provides an effective, domain-agnostic solution for JERE that reduces annotation requirements while maintaining high performance through the complementary strengths of LLMs and ASP.

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [36] [Cognitive Structure Generation: From Educational Priors to Policy Optimization](https://arxiv.org/abs/2508.12647)
*Hengnian Gu,Zhifu Chen,Yuxin Chen,Jin Peng Zhou,Dongdai Zhou*

Main category: cs.AI

TL;DR: CSG framework generates cognitive structures using diffusion models and RL optimization, improving student modeling performance and interpretability on educational tasks.


<details>
  <summary>Details</summary>
Motivation: Cognitive structure assessment is a fundamental but challenging problem in education, as it represents students' subjective organization of knowledge but has remained largely unassessable in practice.

Method: Pretrain Cognitive Structure Diffusion Probabilistic Model (CSDPM) to generate cognitive structures from educational priors, then optimize with reinforcement learning using hierarchical rewards to align with genuine cognitive development levels.

Result: Experiments on four real-world education datasets show CSG-generated cognitive structures provide more comprehensive and effective representations, substantially improving performance on Knowledge Tracing (KT) and Cognitive Diagnosis (CD) tasks.

Conclusion: The CSG framework successfully addresses the challenge of cognitive structure assessment, offering improved performance and enhanced interpretability for student modeling in educational applications.

Abstract: Cognitive structure is a student's subjective organization of an objective
knowledge system, reflected in the psychological construction of concepts and
their relations. However, cognitive structure assessment remains a
long-standing challenge in student modeling and psychometrics, persisting as a
foundational yet largely unassessable concept in educational practice. This
paper introduces a novel framework, Cognitive Structure Generation (CSG), in
which we first pretrain a Cognitive Structure Diffusion Probabilistic Model
(CSDPM) to generate students' cognitive structures from educational priors, and
then further optimize its generative process as a policy with hierarchical
reward signals via reinforcement learning to align with genuine cognitive
development levels during students' learning processes. Experimental results on
four popular real-world education datasets show that cognitive structures
generated by CSG offer more comprehensive and effective representations for
student modeling, substantially improving performance on KT and CD tasks while
enhancing interpretability.

</details>


### [37] [The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning](https://arxiv.org/abs/2508.12651)
*Chunliang Hua,Xiao Hu,Jiayang Sun,Zeyuan Yang*

Main category: cs.AI

TL;DR: Proposes CDMCLP optimization framework and Integrated Planning Recommendation System for urban aerial mobility vertiport network planning, improving traditional methods by 38-52% and integrating socio-economic factors with practical implementation.


<details>
  <summary>Details</summary>
Motivation: Existing planning frameworks are inadequate for complex urban aerial mobility infrastructure due to data granularity limitations and lack of real-world applicability, especially as cities plan large-scale vertiport networks.

Method: Develops Capacitated Dynamic Maximum Covering Location Problem (CDMCLP) framework that models spatial-temporal demand, user behaviors, and capacity constraints, combined with socio-economic factors and dynamic clustering initialization in an Integrated Planning Recommendation System.

Result: Validation shows CDMCLP improves traditional location methods by 38-52%, with the recommendation system demonstrating user-friendliness and effective integration of complex elements in real-world urban planning scenarios.

Conclusion: The hybrid approach successfully bridges theoretical location modeling with practical UAM infrastructure planning, providing municipalities with a pragmatic tool for vertiport network design that combines mathematical rigor with implementation considerations.

Abstract: As urban aerial mobility (UAM) infrastructure development accelerates
globally, cities like Shenzhen are planning large-scale vertiport networks
(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain
inadequate for this complexity due to historical limitations in data
granularity and real-world applicability. This paper addresses these gaps by
first proposing the Capacitated Dynamic Maximum Covering Location Problem
(CDMCLP), a novel optimization framework that simultaneously models urban-scale
spatial-temporal demand, heterogeneous user behaviors, and infrastructure
capacity constraints. Building on this foundation, we introduce an Integrated
Planning Recommendation System that combines CDMCLP with socio-economic factors
and dynamic clustering initialization. This system leverages adaptive parameter
tuning based on empirical user behavior to generate practical planning
solutions. Validation in a Chinese center city demonstrates the effectiveness
of the new optimization framework and recommendation system. Under the
evaluation and optimization of CDMCLP, the quantitative performance of
traditional location methods are exposed and can be improved by 38\%--52\%,
while the recommendation system shows user-friendliness and the effective
integration of complex elements. By integrating mathematical rigor with
practical implementation considerations, this hybrid approach bridges the gap
between theoretical location modeling and real-world UAM infrastructure
planning, offering municipalities a pragmatic tool for vertiport network
design.

</details>


### [38] [GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance](https://arxiv.org/abs/2508.12682)
*Jinquan Shi,Yingying Cheng,Fan Zhang,Miao Jiang,Jun Lin,Yanbai Shen*

Main category: cs.AI

TL;DR: GridCodex is an end-to-end framework using LLMs and enhanced RAG for automated grid code reasoning and compliance, achieving 26.4% answer quality improvement and 10x recall increase.


<details>
  <summary>Details</summary>
Motivation: Renewable energy transition creates complex grid code compliance challenges that lack automated solutions, hindering industry expansion and profitability.

Method: Leverages large language models with retrieval-augmented generation (RAG), enhanced through multi-stage query refinement and RAPTOR for improved retrieval.

Result: 26.4% improvement in answer quality, more than 10-fold increase in recall rate, validated through comprehensive benchmarks across multiple regulatory agencies.

Conclusion: GridCodex effectively addresses grid code compliance challenges with significant performance improvements, demonstrating the value of enhanced RAG workflows for regulatory reasoning.

Abstract: The global shift towards renewable energy presents unprecedented challenges
for the electricity industry, making regulatory reasoning and compliance
increasingly vital. Grid codes, the regulations governing grid operations, are
complex and often lack automated interpretation solutions, which hinders
industry expansion and undermines profitability for electricity companies. We
introduce GridCodex, an end to end framework for grid code reasoning and
compliance that leverages large language models and retrieval-augmented
generation (RAG). Our framework advances conventional RAG workflows through
multi stage query refinement and enhanced retrieval with RAPTOR. We validate
the effectiveness of GridCodex with comprehensive benchmarks, including
automated answer assessment across multiple dimensions and regulatory agencies.
Experimental results showcase a 26.4% improvement in answer quality and more
than a 10 fold increase in recall rate. An ablation study further examines the
impact of base model selection.

</details>


### [39] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion is the first benchmark for evaluating hallucinations in multimodal LLMs on egocentric videos, featuring 1,400 videos with 8,000 questions that reveal significant hallucination issues even in top models like GPT-4o and Gemini.


<details>
  <summary>Details</summary>
Motivation: While MLLMs show strong performance in visual perception and reasoning for third-person and egocentric videos, they suffer from hallucinations - generating coherent but inaccurate responses. There was no dedicated benchmark to systematically evaluate these hallucination problems in egocentric contexts.

Method: Created EgoIllusion benchmark with 1,400 egocentric videos paired with 8,000 human-annotated open and closed-ended questions specifically designed to trigger hallucinations in both visual and auditory cues.

Result: Evaluation across ten MLLMs revealed significant challenges, with even powerful models like GPT-4o and Gemini achieving only 59% accuracy, demonstrating widespread hallucination issues in egocentric video understanding.

Conclusion: EgoIllusion provides a foundational benchmark for evaluating MLLM effectiveness and will spur development of better egocentric MLLMs with reduced hallucination rates. The benchmark will be open-sourced for reproducibility.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [40] [GTool: Graph Enhanced Tool Planning with Large Language Model](https://arxiv.org/abs/2508.12725)
*Wenjie Chen,Wenbin Li,Di Yao,Xuying Meng,Chang Gong,Jingping Bi*

Main category: cs.AI

TL;DR: GTool enhances LLM tool planning by constructing request-specific tool graphs and generating graph tokens to handle incomplete tool dependencies, achieving 29.6% performance improvement over SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: Current LLM tool planning approaches treat tools as isolated components and fail to leverage inherent tool dependencies, leading to invalid planning results especially with incomplete dependency information and large toolsets.

Method: GTool constructs request-specific tool graphs to efficiently select tools and generates <graph tokens> that provide dependency information understandable by LLMs. It includes a missing dependency prediction task to improve reliability with incomplete dependencies and can be integrated with various LLM backbones without extensive retraining.

Result: Extensive experiments show GTool achieves more than 29.6% performance improvements compared with state-of-the-art baselines using a lightweight 7B LLM backbone.

Conclusion: GTool is the first work to effectively enhance LLM tool planning under incomplete dependencies through graph-based tool selection and dependency prediction, demonstrating significant performance gains without requiring LLM retraining.

Abstract: Tool planning with large language models (LLMs), referring to selecting,
organizing, and preparing the tools necessary to complete a user request,
bridges the gap between natural language understanding and task execution.
However, current works treat different tools as isolated components and fail to
leverage the inherent dependencies of tools, leading to invalid planning
results. Since tool dependencies are often incomplete, it becomes challenging
for LLMs to accurately identify the appropriate tools required by a user
request, especially when confronted with a large toolset. To solve this
challenge, we propose \texttt{GTool}, which is the first work aiming to enhance
the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool}
constructs a request-specific tool graph to select tools efficiently and
generate the \texttt{<graph token>} which provides sufficient dependency
information understandable by LLMs. Moreover, a missing dependency prediction
task is designed to improve the reliability of \texttt{GTool} with incomplete
dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly
integrated with various LLM backbones without extensive retraining. Extensive
experiments show that \texttt{GTool} achieves more than 29.6\% performance
improvements compared with the state-of-the-art (SOTA) baselines with a
light-weight (7B) LLM backbone.

</details>


### [41] [Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants](https://arxiv.org/abs/2508.12754)
*Alessio Galatolo,Luca Alberto Rappuoli,Katie Winkle,Meriem Beloucif*

Main category: cs.AI

TL;DR: This paper introduces a new framework and benchmark for evaluating LLMs as Artificial Moral Assistants (AMAs), focusing on moral reasoning capabilities beyond superficial alignment, and finds significant limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM moral capabilities are superficial, measuring only final ethical verdicts rather than explicit moral reasoning. The paper aims to advance evaluation by examining LLMs' capacity to function as AMAs that support human moral deliberation.

Method: Developed a formal framework based on philosophical literature defining AMA behavior qualities (deductive/abductive moral reasoning). Created a benchmark to test these qualities and evaluated popular open LLMs against it.

Result: Results show considerable variability across models and persistent shortcomings, particularly in abductive moral reasoning capabilities.

Conclusion: The work connects theoretical philosophy with practical AI evaluation and emphasizes the need for dedicated strategies to explicitly enhance moral reasoning capabilities in LLMs.

Abstract: The recent rise in popularity of large language models (LLMs) has prompted
considerable concerns about their moral capabilities. Although considerable
effort has been dedicated to aligning LLMs with human moral values, existing
benchmarks and evaluations remain largely superficial, typically measuring
alignment based on final ethical verdicts rather than explicit moral reasoning.
In response, this paper aims to advance the investigation of LLMs' moral
capabilities by examining their capacity to function as Artificial Moral
Assistants (AMAs), systems envisioned in the philosophical literature to
support human moral deliberation. We assert that qualifying as an AMA requires
more than what state-of-the-art alignment techniques aim to achieve: not only
must AMAs be able to discern ethically problematic situations, they should also
be able to actively reason about them, navigating between conflicting values
outside of those embedded in the alignment phase. Building on existing
philosophical literature, we begin by designing a new formal framework of the
specific kind of behaviour an AMA should exhibit, individuating key qualities
such as deductive and abductive moral reasoning. Drawing on this theoretical
framework, we develop a benchmark to test these qualities and evaluate popular
open LLMs against it. Our results reveal considerable variability across models
and highlight persistent shortcomings, particularly regarding abductive moral
reasoning. Our work connects theoretical philosophy with practical AI
evaluation while also emphasising the need for dedicated strategies to
explicitly enhance moral reasoning capabilities in LLMs. Code available at
https://github.com/alessioGalatolo/AMAeval

</details>


### [42] [HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds](https://arxiv.org/abs/2508.12782)
*Petr Anokhin,Roman Khalikov,Stefan Rebrikov,Viktor Volkov,Artyom Sorokin,Vincent Bissonnette*

Main category: cs.AI

TL;DR: HeroBench is a new benchmark for evaluating long-horizon planning in LLMs using complex RPG-inspired virtual worlds, revealing significant performance gaps in current models' strategic planning abilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture the complexity of realistic planning environments, as they typically assess LLMs through abstract or low-dimensional algorithmic tasks rather than evaluating extended, structured sequences of interdependent actions.

Method: HeroBench provides a rigorously constructed dataset of RPG-inspired tasks with varying difficulties, a simulated environment for plan execution and validation, and analytical tools for performance evaluation. Tasks require strategic planning, resource gathering, skill mastery, equipment crafting, and adversary defeat.

Result: Evaluation of 25 state-of-the-art LLMs (including GPT-5 family) revealed substantial performance disparities not seen in conventional benchmarks, with detailed error analysis showing weaknesses in generating robust high-level plans and executing structured actions reliably.

Conclusion: HeroBench significantly advances LLM reasoning evaluation and provides a flexible, scalable foundation for future research into advanced autonomous planning in virtual environments, highlighting critical areas for improvement in current models.

Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated
step-by-step reasoning tasks such as mathematics and programming, but their
proficiency in long-horizon planning, where solutions require extended,
structured sequences of interdependent actions, remains underexplored. Existing
benchmarks typically assess LLMs through abstract or low-dimensional
algorithmic tasks, failing to capture the complexity of realistic planning
environments. We introduce HeroBench, a novel benchmark designed specifically
to evaluate long-horizon planning and structured reasoning within complex
RPG-inspired virtual worlds. HeroBench provides a rigorously constructed
dataset of tasks covering a wide range of difficulties, a simulated environment
to execute and validate agent plans, and detailed analytical tools for
evaluating model performance. Tasks challenge models to formulate strategic
plans, efficiently gather resources, master necessary skills, craft equipment,
and defeat adversaries, reflecting practical scenarios' layered dependencies
and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning
both open-source and proprietary models, including the GPT-5 family, reveals
substantial performance disparities rarely observed in conventional reasoning
benchmarks. Detailed error analysis further uncovers specific weaknesses in
current models' abilities to generate robust high-level plans and reliably
execute structured actions. HeroBench thus not only significantly advances the
evaluation of LLM reasoning but also provides a flexible, scalable foundation
for future research into advanced, autonomous planning in virtual environments.

</details>


### [43] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: RLVR extended to open-ended tasks using rubric-based rewards, creating largest rubric system (10k+ rubrics) that improves open-ended benchmarks by +5.2% and provides fine-grained stylistic control while preserving general abilities.


<details>
  <summary>Details</summary>
Motivation: Traditional RLVR is limited to domains with automatically checkable outcomes, but many real-world tasks are open-ended and subjective without clear verifiable signals.

Method: Integrates rubric-based rewards where structured rubrics serve as model-interpretable criteria for automatic scoring. Uses over 10,000 rubrics from humans, LLMs, or hybrid collaboration with clear framework implementation.

Result: Qwen-30B-A3B model achieves +5.2% improvement on open-ended benchmarks (especially humanities), outperforms 671B DeepSeek-V3 by +2.4%, preserves general reasoning abilities, and provides fine-grained stylistic control to produce more human-like responses.

Conclusion: Rubric-based RLVR successfully extends reinforcement learning to open-ended tasks, demonstrating significant improvements while maintaining model capabilities and enabling better stylistic control through structured rubric systems.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [44] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG is a multimodal empathetic response generation system that uses explicit emotion understanding, memory retrieval, and generation to create emotionally rich, identity-consistent responses without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with multimodal emotional content and maintaining identity consistency in empathetic response generation, requiring a more comprehensive approach.

Method: Decomposes MERG into three components: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation using advanced speech and video generative models.

Result: Achieved superior performance in zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.

Conclusion: E3RG effectively addresses multimodal emotional content handling and identity consistency issues in empathetic response generation without requiring extra training.

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [45] [Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption](https://arxiv.org/abs/2508.12896)
*Faruk Alpay,Taylan Alpay*

Main category: cs.AI

TL;DR: The paper formalizes three design axioms for sustained adoption of AI agent systems and develops a mathematical model of adoption as decaying novelty plus growing utility, with comprehensive analysis methods including identifiability testing, model comparisons, and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To establish fundamental design principles for AI agent systems that ensure sustained adoption rather than temporary novelty, addressing the challenge of maintaining long-term user engagement with multi-step task execution systems.

Method: Develops a mathematical adoption model combining decaying novelty and growing utility terms, with extensive analytical methods including identifiability analysis, non-monotone comparator evaluation, hazard family ablations, multi-series benchmarking, friction proxy calibration, and comprehensive statistical validation techniques.

Result: The paper provides formal proofs of phase conditions for adoption troughs/overshoots, establishes identifiability conditions for model parameters, demonstrates model performance through extensive benchmarking, and validates friction proxies against ground truth data with statistical rigor.

Conclusion: The three design axioms (Reliability > Novelty, Embed > Destination, Agency > Chat) are mathematically validated as crucial for sustained adoption, with the proposed model and analytical framework providing robust tools for predicting and optimizing AI agent system adoption patterns.

Abstract: We formalize three design axioms for sustained adoption of agent-centric AI
systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >
Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying
novelty term and a growing utility term and derive the phase conditions for
troughs/overshoots with full proofs. We introduce: (i) an
identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with
delta-method gradients; (ii) a non-monotone comparator
(logistic-with-transient-bump) evaluated on the same series to provide
additional model comparison; (iii) ablations over hazard families $h(\cdot)$
mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough
depth, noise, AR structure) reporting coverage (type-I error, power); (v)
calibration of friction proxies against time-motion/survey ground truth with
standard errors; (vi) residual analyses (autocorrelation and
heteroskedasticity) for each fitted curve; (vii) preregistered windowing
choices for pre/post estimation; (viii) Fisher information & CRLB for
$(\alpha,\beta)$ under common error models; (ix) microfoundations linking
$\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic,
double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$
heterogeneity. Figures and tables are reflowed for readability, and the
bibliography restores and extends non-logistic/Bass adoption references
(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All
code and logs necessary to reproduce the synthetic analyses are embedded as
LaTeX listings.

</details>


### [46] [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897)
*Jianhao Chen,Mayi Xu,Xiaohu Li,Yongqi Li,Xiangyu Zhang,Jianjie Huang,Tieyun Qian*

Main category: cs.AI

TL;DR: FuSaR is a novel alignment strategy that improves LRM safety by detoxifying harmful reasoning processes while preserving reasoning capability, using fuzzification to hide dangerous entities and procedures.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) show impressive reasoning performance but have significant safety vulnerabilities that need addressing without compromising their reasoning capabilities.

Method: Proposes FuSaR (Fuzzification-based Safety-Reasoning alignment) that exploits the competition between reasoning and safety abilities, detoxifies harmful reasoning processes by hiding dangerous entities and procedures in reasoning steps.

Result: Validation experiments on open-source LRMs using detoxified reasoning data show FuSaR successfully mitigates safety risks while preserving core reasoning information, outperforming existing baselines.

Conclusion: FuSaR is an efficient alignment strategy that simultaneously enhances both reasoning capability and safety of LRMs by balancing safety-reasoning through fuzzification-based detoxification.

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance across
various tasks due to their powerful reasoning capabilities. However, their
safety performance remains a significant concern. In this paper, we explore the
reasons behind the vulnerability of LRMs. Based on this, we propose a novel
method to improve the safety of LLMs without sacrificing their reasoning
capability. Specifically, we exploit the competition between LRM's reasoning
ability and safety ability, and achieve jailbreak by improving LRM's reasoning
performance to reduce its safety performance. We then introduce an alignment
strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by
detoxifying the harmful reasoning process, where both the dangerous entities
and the dangerous procedures in the reasoning steps are hidden. FuSaR
successfully mitigates safety risks while preserving core reasoning
information. We validate this strategy through alignment experiments on several
open-source LRMs using detoxified reasoning data. The results compared with
existing baselines conclusively show that FuSaR is an efficient alignment
strategy to simultaneously enhance both the reasoning capability and safety of
LRMs.

</details>


### [47] [Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards](https://arxiv.org/abs/2508.12935)
*Ting Yang,Li Chen,Huimin Wang*

Main category: cs.AI

TL;DR: RLFF-ESC is a reinforcement learning framework that enables flexible emotional support conversations by simulating future dialogues and using future-oriented rewards, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based emotional support systems rely on predefined strategies, limiting their effectiveness in complex real-life scenarios that require flexible responses to diverse emotional problems.

Method: End-to-end framework using reinforcement learning with LLM-based multi-agent simulation of future dialogue trajectories, future-oriented reward model training, and explicit reasoning during response generation.

Result: RLFF-ESC consistently outperforms existing baselines on two public ESC datasets in terms of goal completion and response quality when tested on Qwen2.5-7B-Instruct-1M and LLaMA3.1-8B-Instruct models.

Conclusion: The proposed framework successfully enables flexible, high-quality emotional support responses by leveraging future-oriented reinforcement learning and explicit reasoning, addressing limitations of predefined strategy approaches.

Abstract: Emotional Support Conversation (ESC) systems aim to alleviate users'
emotional difficulties and provide long-term, systematic support for emotional
well-being. However, most large language model (LLM)-based ESC systems rely on
predefined strategies, which limits their effectiveness in complex, real-life
scenarios. To enable flexible responses to diverse emotional problem scenarios,
this paper introduces a novel end-to-end framework (RLFF-ESC) that directly
learns enduring emotionally supportive response skills using reinforcement
learning. For sustained emotional support, we first employ an LLM-based
multi-agent mechanism to simulate future dialogue trajectories and collect
future-oriented rewards. We then train a future-oriented reward model, which is
subsequently used to train the emotional support policy model. Additionally, we
incorporate an explicit reasoning process during response generation to further
enhance the quality, relevance, and contextual appropriateness of the system's
responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and
LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two
public ESC datasets. Experimental results demonstrate that RLFF-ESC
consistently outperforms existing baselines in terms of goal completion and
response quality.

</details>


### [48] [OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities](https://arxiv.org/abs/2508.12943)
*Mary Tonwe*

Main category: cs.AI

TL;DR: OPTIC-ER is a reinforcement learning framework for emergency response in African regions that achieves 100% optimal dispatch with negligible inefficiency using attention-guided actor-critic architecture and real-world simulation.


<details>
  <summary>Details</summary>
Motivation: Address delayed emergency response and spatial inequity in African public service systems that cause avoidable suffering.

Method: Uses attention-guided actor-critic RL architecture with Context-Rich State Vector and Precision Reward Function, trained in high-fidelity simulation using real data from Rivers State, Nigeria, accelerated by precomputed Travel Time Atlas.

Result: Achieved 100.00% optimality rate with negligible inefficiency on 500 unseen incidents, demonstrating robustness and generalization.

Conclusion: Provides a validated blueprint for AI-augmented public services, showing how context-aware RL can bridge algorithmic decision-making with measurable human impact through Infrastructure Deficiency Maps and Equity Monitoring Dashboards.

Abstract: Public service systems in many African regions suffer from delayed emergency
response and spatial inequity, causing avoidable suffering. This paper
introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,
adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided
actor-critic architecture to manage the complexity of dispatch environments.
Its key innovations are a Context-Rich State Vector, encoding action
sub-optimality, and a Precision Reward Function, which penalizes inefficiency.
Training occurs in a high-fidelity simulation using real data from Rivers
State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is
built on the TALS framework (Thin computing, Adaptability, Low-cost,
Scalability) for deployment in low-resource settings. In evaluations on 500
unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible
inefficiency, confirming its robustness and generalization. Beyond dispatch,
the system generates Infrastructure Deficiency Maps and Equity Monitoring
Dashboards to guide proactive governance and data-informed development. This
work presents a validated blueprint for AI-augmented public services, showing
how context-aware RL can bridge the gap between algorithmic decision-making and
measurable human impact.

</details>


### [49] [EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing](https://arxiv.org/abs/2508.13003)
*Shengbo Wang,Mingwei Liu,Zike Li,Anji Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.AI

TL;DR: EvolMathEval is an automated framework that generates evolving mathematical benchmarks using evolutionary testing to prevent data contamination and maintain challenge for LLMs, revealing cognitive shortcuts in AI reasoning.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current mathematical reasoning benchmarks including score saturation, temporal decay, and data contamination issues that undermine reliable evaluation of LLMs.

Method: Uses evolutionary testing with seed problem generation via reverse engineering, multi-dimensional genetic operators for cognitive challenges, and composite fitness function to assess problem difficulty.

Result: Generated high-difficulty problems reducing model accuracy by 48% on average, discovered 'Pseudo Aha Moment' phenomenon where LLMs use non-rigorous heuristics instead of complex reasoning, accounting for 77-100% of errors.

Conclusion: EvolMathEval provides a sustainable benchmark solution that continuously evolves to challenge future models while uncovering fundamental reasoning limitations in current LLMs through cognitive shortcut behaviors.

Abstract: The rapid advancement of LLMs poses a significant challenge to existing
mathematical reasoning benchmarks. These benchmarks commonly suffer from issues
such as score saturation, temporal decay, and data contamination. To address
this challenge, this paper introduces EvolMathEval, an automated mathematical
benchmark generation and evolution framework based on evolutionary testing. By
dynamically generating unique evaluation instances ab initio, the framework
fundamentally eliminates the risk of data contamination, and ensuring the
benchmark remains perpetually challenging for future models.The core mechanisms
of EvolMathEval include: seed problem generation based on reverse engineering
with algebraic guarantees; multi-dimensional genetic operators designed to
inject diverse cognitive challenges; and a composite fitness function that can
rapidly and accurately assess problem difficulty. Experimental results
demonstrate that the proposed composite fitness function can efficiently and
precisely quantify the difficulty of mathematical problems. Furthermore,
EvolMathEval can not only generate a large volume of high-difficulty problems
through continuous self-iteration, but it can also significantly enhance the
complexity of public datasets like GSM8K through evolution, reducing model
accuracy by an average of 48%. Deeper investigation reveals that when solving
these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to
bypass complex multi-step logical reasoning, consequently leading to incorrect
solutions. We define this phenomenon as "Pseudo Aha Moment". This finding
uncovers a cognitive shortcut-taking behavior in the deep reasoning processes
of current LLMs, which we find accounts for 77% to 100% of errors on targeted
problems. Code and resources are available
at:https://github.com/SYSUSELab/EvolMathEval.

</details>


### [50] [e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving](https://arxiv.org/abs/2508.13020)
*Jiaqi Yin,Zhan Song,Chen Chen,Yaohui Cai,Zhiru Zhang,Cunxi Yu*

Main category: cs.AI

TL;DR: E-boost is a novel framework that bridges the gap between heuristic and exact e-graph extraction methods through parallelization, adaptive pruning, and initialized exact solving, achieving 558x speedup over exact methods and 19% improvement over state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: E-graph extraction is an NP-hard optimization problem where traditional methods face a critical trade-off: heuristic approaches are fast but suboptimal, while exact methods provide optimal solutions but are computationally prohibitive for practical problems.

Method: Three key innovations: (1) parallelized heuristic extraction with weak data dependence for concurrent DAG cost computation, (2) adaptive search space pruning with parameterized threshold to retain only promising candidates, and (3) initialized exact solving using Integer Linear Programming with warm-start capabilities.

Result: 558x runtime speedup over traditional exact approaches (ILP), 19.04% performance improvement over state-of-the-art framework (SmoothE), and 7.6-8.1% area improvements in logic synthesis tasks with different technology mapping libraries.

Conclusion: E-boost successfully bridges the gap between heuristic and exact e-graph extraction methods, delivering both speed and optimality for practical optimization tasks in formal verification and logic synthesis.

Abstract: E-graphs have attracted growing interest in many fields, particularly in
logic synthesis and formal verification. E-graph extraction is a challenging
NP-hard combinatorial optimization problem. It requires identifying optimal
terms from exponentially many equivalent expressions, serving as the primary
performance bottleneck in e-graph based optimization tasks. However,
traditional extraction methods face a critical trade-off: heuristic approaches
offer speed but sacrifice optimality, while exact methods provide optimal
solutions but face prohibitive computational costs on practical problems. We
present e-boost, a novel framework that bridges this gap through three key
innovations: (1) parallelized heuristic extraction that leverages weak data
dependence to compute DAG costs concurrently, enabling efficient multi-threaded
performance without sacrificing extraction quality; (2) adaptive search space
pruning that employs a parameterized threshold mechanism to retain only
promising candidates, dramatically reducing the solution space while preserving
near-optimal solutions; and (3) initialized exact solving that formulates the
reduced problem as an Integer Linear Program with warm-start capabilities,
guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis
fields, e-boost demonstrates 558x runtime speedup over traditional exact
approaches (ILP) and 19.04% performance improvement over the state-of-the-art
extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost
produces 7.6% and 8.1% area improvements compared to conventional synthesis
tools with two different technology mapping libraries. e-boost is available at
https://github.com/Yu-Maryland/e-boost.

</details>


### [51] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: PC-Sampler is a novel decoding strategy for masked diffusion models that addresses limitations of existing uncertainty-based samplers by incorporating position-aware trajectory control and confidence calibration to improve generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty-based samplers for masked diffusion models suffer from lack of global trajectory control and bias toward trivial tokens in early decoding stages, limiting their full potential.

Method: Position-Aware Confidence-Calibrated Sampling (PC-Sampler) unifies global trajectory planning with content-aware informativeness maximization using position-aware weighting and calibrated confidence scores.

Result: PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average across seven benchmarks, significantly narrowing the performance gap with state-of-the-art autoregressive models.

Conclusion: The proposed PC-Sampler effectively addresses key limitations of current MDM decoding strategies and demonstrates substantial performance improvements across diverse challenging tasks.

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [52] [G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance](https://arxiv.org/abs/2508.13023)
*Yongxin Guo,Wenbo Deng,Zhenglin Cheng,Xiaoying Tang*

Main category: cs.AI

TL;DR: G^2RPO-A is an adaptive reinforcement learning method that injects ground-truth reasoning steps into training trajectories to improve small language models' reasoning capabilities, outperforming vanilla GRPO.


<details>
  <summary>Details</summary>
Motivation: RLVR works well for large language models but shows limited improvements for small language models due to their inherent weaknesses in world knowledge and reasoning capabilities.

Method: Guided GRPO injects ground-truth reasoning steps into roll-out trajectories. G^2RPO-A adaptively adjusts guidance strength based on the model's training dynamics.

Result: Experiments on mathematical reasoning and code-generation benchmarks show G^2RPO-A substantially outperforms vanilla GRPO.

Conclusion: Adaptive guidance injection effectively compensates for small language models' weaknesses, significantly improving their reasoning performance compared to standard methods.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced
the reasoning abilities of large language models (LLMs). Its success, however,
largely depends on strong base models with rich world knowledge, yielding only
modest improvements for small-size language models (SLMs). To address this
limitation, we investigate Guided GRPO, which injects ground-truth reasoning
steps into roll-out trajectories to compensate for SLMs' inherent weaknesses.
Through a comprehensive study of various guidance configurations, we find that
naively adding guidance delivers limited gains. These insights motivate
G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength
in response to the model's evolving training dynamics. Experiments on
mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A
substantially outperforms vanilla GRPO. Our code and models are available at
https://github.com/T-Lab-CUHKSZ/G2RPO-A.

</details>


### [53] [A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis](https://arxiv.org/abs/2508.13072)
*Yuting Zhang,Tiantian Geng,Luoying Hao,Xinxing Cheng,Alexander Thorley,Xiaoxia Wang,Wenqi Lu,Sandeep S Hothi,Lei Wei,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: TGMM is a unified multimodal framework that integrates lab tests, ECGs, and echocardiograms with textual guidance for multiple cardiac tasks, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current cardiovascular management faces limitations including scarcity of aligned multimodal data, reliance on single modalities, poor alignment strategies prioritizing similarity over complementarity, and narrow single-task focus.

Method: TGMM framework with three components: 1) MedFlexFusion module for dynamic integration of diverse cardiac data sources, 2) textual guidance module for task-relevant representations, and 3) response module for final decisions across multiple clinical tasks.

Result: Extensive experiments showed TGMM outperformed state-of-the-art methods across multiple clinical tasks, with additional validation confirming robustness on public datasets.

Conclusion: The proposed TGMM framework effectively addresses multimodal integration challenges in cardiovascular care and demonstrates superior performance across diverse clinical objectives including diagnosis, risk stratification, and information retrieval.

Abstract: Contemporary cardiovascular management involves complex consideration and
integration of multimodal cardiac datasets, where each modality provides
distinct but complementary physiological characteristics. While the effective
integration of multiple modalities could yield a holistic clinical profile that
accurately models the true clinical situation with respect to data modalities
and their relatives weightings, current methodologies remain limited by: 1) the
scarcity of patient- and time-aligned multimodal data; 2) reliance on isolated
single-modality or rigid multimodal input combinations; 3) alignment strategies
that prioritize cross-modal similarity over complementarity; and 4) a narrow
single-task focus. In response to these limitations, a comprehensive multimodal
dataset was curated for immediate application, integrating laboratory test
results, electrocardiograms, and echocardiograms with clinical outcomes.
Subsequently, a unified framework, Textual Guidance Multimodal fusion for
Multiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key
components: 1) a MedFlexFusion module designed to capture the unique and
complementary characteristics of medical modalities and dynamically integrate
data from diverse cardiac sources and their combinations; 2) a textual guidance
module to derive task-relevant representations tailored to diverse clinical
objectives, including heart disease diagnosis, risk stratification and
information retrieval; and 3) a response module to produce final decisions for
all these tasks. Furthermore, this study systematically explored key features
across multiple modalities and elucidated their synergistic contributions in
clinical decision-making. Extensive experiments showed that TGMM outperformed
state-of-the-art methods across multiple clinical tasks, with additional
validation confirming its robustness on another public dataset.

</details>


### [54] [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](https://arxiv.org/abs/2508.13121)
*Carlos Celemin*

Main category: cs.AI

TL;DR: Automated game testing using Bayesian Optimization with agents to efficiently detect bugs by maximizing information acquisition through a scalable grid-based model.


<details>
  <summary>Details</summary>
Motivation: Traditional game testing methods suffer from scalability issues and inefficiency in exploring game levels for bug detection. There's a need for automated approaches that can efficiently cover game maps and identify potential issues.

Method: Uses Bayesian Optimization (BO) with game character agents to perform sample-efficient search. Introduces a game testing-specific model built on a grid map that provides smoothness and uncertainty estimation while avoiding scalability problems of traditional models.

Result: The approach significantly improves map coverage capabilities in both time efficiency and exploration distribution compared to traditional methods.

Conclusion: The proposed Bayesian Optimization-based automated testing with grid map modeling provides an efficient and scalable solution for game bug detection, overcoming limitations of traditional testing approaches.

Abstract: This work introduces an automated testing approach that employs agents
controlling game characters to detect potential bugs within a game level.
Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient
search, the method determines the next sampling point by analyzing the data
collected so far and calculates the data point that will maximize information
acquisition. To support the BO process, we introduce a game testing-specific
model built on top of a grid map, that features the smoothness and uncertainty
estimation required by BO, however and most importantly, it does not suffer the
scalability issues that traditional models carry. The experiments demonstrate
that the approach significantly improves map coverage capabilities in both time
efficiency and exploration distribution.

</details>


### [55] [Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks](https://arxiv.org/abs/2508.13143)
*Ruofan Lu,Yichen Li,Yintong Huo*

Main category: cs.AI

TL;DR: A benchmark study evaluating autonomous LLM agents reveals ~50% task completion rate, with detailed failure taxonomy and improvement recommendations.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of autonomous agent systems focus on success rates without systematic analysis of interactions, communication mechanisms, and failure causes, creating a gap in understanding agent performance.

Method: Developed a benchmark of 34 representative programmable tasks to rigorously assess autonomous agents, evaluated three popular open-source agent frameworks with two LLM backbones, and conducted in-depth failure analysis to create a three-tier taxonomy of failure causes.

Result: Observed approximately 50% task completion rate, identified failure causes including planning errors, task execution issues, and incorrect response generation, and developed actionable improvements for agent planning and self-diagnosis capabilities.

Conclusion: The failure taxonomy and mitigation advice provide an empirical foundation for developing more robust and effective autonomous agent systems, addressing critical gaps in current evaluation methodologies.

Abstract: Autonomous agent systems powered by Large Language Models (LLMs) have
demonstrated promising capabilities in automating complex tasks. However,
current evaluations largely rely on success rates without systematically
analyzing the interactions, communication mechanisms, and failure causes within
these systems. To bridge this gap, we present a benchmark of 34 representative
programmable tasks designed to rigorously assess autonomous agents. Using this
benchmark, we evaluate three popular open-source agent frameworks combined with
two LLM backbones, observing a task completion rate of approximately 50%.
Through in-depth failure analysis, we develop a three-tier taxonomy of failure
causes aligned with task phases, highlighting planning errors, task execution
issues, and incorrect response generation. Based on these insights, we propose
actionable improvements to enhance agent planning and self-diagnosis
capabilities. Our failure taxonomy, together with mitigation advice, provides
an empirical foundation for developing more robust and effective autonomous
agent systems in the future.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV is a novel method that applies attention sparsification to multiple-context KV Cache in RAG scenarios, achieving 15% sequence length compression without accuracy loss while significantly improving throughput.


<details>
  <summary>Details</summary>
Motivation: Existing KV Cache sparsification techniques only work in single-context scenarios and fail in RAG where multiple documents lack cross-attention dependencies, while full recomputation approaches don't reduce memory overhead.

Method: SamKV considers complementary information across contexts when sparsifying one context, then locally recomputes the sparsified information to handle multiple-context KV Cache without cross-attention.

Result: The method compresses sequence length to 15% compared to full recomputation baselines while maintaining accuracy, significantly boosting throughput in multi-context RAG scenarios.

Conclusion: SamKV successfully addresses the challenge of efficient KV Cache management in RAG by enabling attention sparsification for multiple-context scenarios, achieving substantial efficiency gains without sacrificing accuracy.

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [57] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: RS is a model-agnostic adversarial text detection framework that identifies attacks by measuring embedding sensitivity when masking important words, achieving high detection accuracy across multiple datasets and attacks without retraining.


<details>
  <summary>Details</summary>
Motivation: Adversarial text attacks threaten transformer models, but existing defenses are attack-specific or require costly retraining, creating a need for practical, generalizable detection methods.

Method: RS ranks words using importance heuristics, measures embedding sensitivity when masking top-k critical words, and processes patterns with a BiLSTM detector to identify adversarial examples.

Result: RS achieves over 88% detection accuracy across three datasets, three attack types, and two victim models, with competitive performance and lower computational cost than state-of-the-art methods.

Conclusion: RS provides an effective, practical solution for adversarial text detection that generalizes well to unseen datasets, attacks, and models without requiring retraining.

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [58] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: Lightweight sInvResUNet model with collaborative learning achieves real-time arterial blood pressure monitoring on embedded devices with minimal computational load and good accuracy.


<details>
  <summary>Details</summary>
Motivation: Noninvasive ABP monitoring is crucial in critical care but existing deep learning models lack optimization for embedded system deployment with limited computational resources.

Method: Developed lightweight sInvResUNet architecture with KDCL collaborative learning scheme, using only 0.89M parameters and 0.02 GFLOPS computational load for real-time performance on embedded devices.

Result: Achieved 8.49ms inference time for 10-second output, with MAE of 10.06 mmHg and Pearson correlation of 0.88 on large heterogeneous dataset (1.2M segments from 2,154 patients).

Conclusion: Successfully demonstrated real-time ABP monitoring on embedded systems, but models show performance variations across diverse populations, highlighting generalization challenges that need further research.

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [59] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: MSLoRA-CR is a novel multimodal biomedical image incremental learning method that uses modality-specific LoRA modules with contrastive regularization to enable efficient knowledge sharing across modalities while preventing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing incremental learning methods focus on task expansion within single modalities, but biomedical applications require handling diverse modalities. Training separate models for each modality significantly increases inference costs, necessitating a unified multimodal incremental learning approach.

Method: Proposes MSLoRA-CR which fine-tunes modality-specific LoRA modules while incorporating contrastive regularization. Builds upon a large vision-language model, keeping the pretrained model frozen while incrementally adapting new LoRA modules for each modality/task.

Result: Outperforms both training separate models for each modality and general incremental learning methods. Achieves 1.88% improvement in overall performance compared to unconstrained incremental learning while maintaining computational efficiency.

Conclusion: MSLoRA-CR effectively addresses multimodal biomedical image incremental learning challenges by preserving previously learned knowledge and leveraging cross-modal knowledge through modality-specific LoRA adaptation with contrastive regularization.

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [60] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: Lifelong learning framework for neural vehicle routing problem solvers that enables transfer learning across different problem contexts and sizes.


<details>
  <summary>Details</summary>
Motivation: Most neural solvers are trained in monotonous contexts (Euclidean distance, single problem size), limiting their real-world application versatility across different scenarios.

Method: Proposes a lifelong learner (LL) with Transformer backbone and inter-context self-attention mechanism for knowledge transfer, plus dynamic context scheduler (DCS) with cross-context experience replay.

Result: Extensive testing shows LL outperforms other neural solvers, achieving best performance for most VRPs on synthetic and benchmark instances up to 18k nodes.

Conclusion: The framework successfully enables neural solvers to handle VRPs in varying contexts through lifelong learning and knowledge transfer mechanisms.

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [61] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: TimesFM foundation model outperforms traditional methods (LSTM, ARIMA, Linear Regression) in demographic forecasting, achieving lowest MSE in 86.67% of cases, especially effective for minority populations with sparse data.


<details>
  <summary>Details</summary>
Motivation: Accurate demographic forecasting is crucial for policymaking in urban planning, healthcare, and economic policy, but traditional methods struggle with complex demographic shifts influenced by globalization, economic conditions, and environmental factors.

Method: Applied Time Series Foundation Model (TimesFM) to predict US demographic changes using Census Bureau and FRED data, compared against LSTM, ARIMA, and Linear Regression baselines across six demographically diverse states.

Result: TimesFM achieved the lowest Mean Squared Error in 86.67% of test cases, with particularly strong performance on minority populations that have sparse historical data.

Conclusion: Pre-trained foundation models like TimesFM can significantly enhance demographic analysis and inform proactive policy interventions without requiring extensive task-specific fine-tuning.

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [62] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: A data-driven Site Planning Layout Indicator (SPLI) system that integrates multi-source spatial data and deep learning to systematically quantify urban spatial layouts across five dimensions for improved urban planning analytics.


<details>
  <summary>Details</summary>
Motivation: Traditional site planning relies on experiential judgment and single-source data, limiting systematic quantification of multifunctional urban layouts and efficient spatial organization.

Method: Developed SPLI framework integrating OSM, POI, building morphology, land use, and satellite imagery with deep learning (RGNN/GNN) across five dimensions: hierarchical building function classification, spatial organization patterns, functional diversity metrics, accessibility to services, and land use intensity indicators.

Result: SPLI improves functional classification accuracy and provides a standardized basis for automated, data-driven urban spatial analytics, addressing data gaps through deep learning approaches.

Conclusion: The proposed SPLI system offers a comprehensive, data-driven framework for systematic urban spatial analysis that extends traditional metrics and supports multimodal spatial data systems for analytics, inference, and retrieval.

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [63] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: Proposes a method to identify latent subprocesses and causal influences in multivariate Hawkes processes by representing continuous-time event sequences as discrete-time models, with a two-phase iterative algorithm that guarantees identifiability.


<details>
  <summary>Details</summary>
Motivation: Real-world systems often have latent subprocesses that existing methods cannot handle, as they primarily focus on observed subprocesses only, posing significant challenges for causal structure discovery.

Method: Leverages discrete-time representation of continuous-time event sequences, establishes identifiability conditions, and proposes a two-phase iterative algorithm that alternates between inferring causal relationships and uncovering new latent subprocesses using path-based conditions.

Result: Experiments on synthetic and real-world datasets demonstrate effective recovery of causal structures despite the presence of latent subprocesses.

Conclusion: The proposed method successfully addresses the challenge of latent subprocesses in multivariate Hawkes processes and provides identifiable conditions for causal structure discovery in partially observed systems.

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [64] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: A brain-inspired feature fusion framework (BRIEF) using neural network connection search and Transformer fusion achieves state-of-the-art performance in fMRI-based mental disorder classification.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for fMRI classification have limitations in network architecture determination (relying on experience) and feature space fusion (mostly simple concatenation, lacking mutual learning).

Method: Proposed BRIEF framework with improved neural network connection search (NCS) using modified Q-learning and Transformer-based multi-feature fusion. Extracted 4 fMRI temporal representations (TCs, FNC, dFNC, MsDE) through four encoders, formulated NCS as Markov Decision Process, and used attention for interpretability.

Result: BRIEF demonstrated significant improvements of 2.2% to 12.1% compared to 21 state-of-the-art models, reaching AUC of 91.5% for schizophrenia and 78.4% for autism spectrum disorder.

Conclusion: First attempt to incorporate brain-inspired reinforcement learning strategy for fMRI-based mental disorder classification, showing significant potential for identifying precise neuroimaging biomarkers.

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [65] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: Using Google DeepMind's AlphaEarth Foundations to extend geospatial datasets beyond original regions with basic models like random forests, achieving 81% accuracy on US validation and 73% on Canada validation for vegetation classification.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled geospatial datasets are often limited to specific geographic regions where data was collected, creating gaps in global coverage and limiting planetary-scale insights.

Method: Leverage AlphaEarth Foundations (AEF) global geospatial representation to extend labeled datasets using basic models (random forests, logistic regression). Case study: extending LANDFIRE's Existing Vegetation Type dataset from USA to Canada at two granularity levels (13 and 80 classes).

Result: Model predictions qualitatively align with ground truth for EvtPhys (13 classes). Achieved 81% classification accuracy on US validation set and 73% on Canadian validation set for EvtPhys, despite limitations.

Conclusion: Basic machine learning models combined with AEF representations can effectively extend geospatial labeled datasets beyond their original geographic regions, enabling broader global coverage and applications.

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [66] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: Fed-Meta-Align is a four-phase federated learning framework that addresses non-IID data challenges in IoT fault classification through meta-initialization, dual-criterion aggregation, and on-device personalization, achieving 91.27% average accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-time fault classification in resource-constrained IoT devices is critical for industrial safety, but standard Federated Learning fails with non-IID data, causing model divergence in heterogeneous environments.

Method: Four-phase framework: 1) Foundational model training on public dataset, 2) Serial meta-initialization on IoT device subset for heterogeneity-aware initialization, 3) Parallel FL with dual-criterion aggregation (local performance + cosine similarity), 4) On-device personalization for specialized experts.

Result: Achieves 91.27% average test accuracy across heterogeneous IoT devices, outperforming personalized FedAvg by 3.87% and FedProx by 3.37% on electrical and mechanical fault datasets.

Conclusion: The multi-stage approach of sequenced initialization and adaptive aggregation provides a robust pathway for deploying high-performance intelligence on diverse TinyML networks in IoT environments.

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [67] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: RL methods for language models in stochastic domains: GRPO causes overconfident predictions while PPO and RLOO yield well-calibrated models. Removing group normalization in GRPO fixes the issue.


<details>
  <summary>Details</summary>
Motivation: To examine if current RL methods are effective at optimizing language models in verifiable domains with stochastic outcomes (like scientific experiments), building on RL's success in deterministic domains like mathematics.

Method: Applied Group Relative Policy Optimization (GRPO), Proximal Policy Optimization (PPO), and REINFORCE Leave-One-Out (RLOO) to synthetic data and real-world biological experiments to evaluate their performance in stochastic domains.

Result: GRPO induced overconfident probability predictions for binary stochastic outcomes, while PPO and RLOO yielded well-calibrated models. Removing group standard normalization in GRPO fixed its miscalibration.

Conclusion: Provides evidence against using standard normalization in GRPO and helps enable RL applications for reasoning language models beyond deterministic domains into stochastic scientific domains.

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [68] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: FairTabGen is an LLM-based framework that generates fair synthetic tabular data with improved counterfactual and causal fairness while maintaining utility, outperforming existing methods using only 20% of original data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating synthetic data in privacy-sensitive, data-scarce settings while improving counterfactual and causal fairness and preserving high utility for tabular datasets.

Method: Uses in-context learning, prompt refinement, and fairness-aware data curation to integrate multiple fairness definitions into both generation and evaluation pipelines.

Result: Outperforms state-of-the-art GAN-based and LLM-based methods with up to 10% improvements on fairness metrics (demographic parity, path-specific causal effects) while retaining statistical utility, using less than 20% of original data.

Conclusion: Demonstrates a principled and practical approach for generating fair and useful synthetic tabular data that is efficient in low-data regimes.

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [69] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: Proposes using ReLU and trigonometric functions instead of polynomial functions in Kolmogorov-Arnold Networks (KANs) to improve computational efficiency while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing KAN implementations use polynomial functions like B-splines and RBFs that are not well-supported by GPUs and are computationally inefficient.

Method: Replace polynomial basis functions with fast computational functions (ReLU, sin, cos, arctan) in the Kolmogorov-Arnold Network structure to enhance computational efficiency.

Result: Experimental results show the proposed function combinations maintain competitive performance while offering improvements in training time and generalization.

Conclusion: Using GPU-friendly functions like ReLU and trigonometric functions in KANs provides computational efficiency benefits without sacrificing performance, making KANs more practical for real-world applications.

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [70] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: Proposed PCA-Grad-CAM and SVM-Grad-CAM methods to visualize attention regions in PCA and SVM layers within CNNs, solving the closed-form Jacobian problem for gradient-based visualization.


<details>
  <summary>Details</summary>
Motivation: Traditional Grad-CAM cannot visualize attention regions in PCA and SVM layers when they are incorporated into CNNs for improved performance with limited training data, creating a need for white-box visualization methods for these components.

Method: Developed closed-form Jacobian solutions for partial derivatives from the last convolutional layer to PCA and SVM layers, enabling gradient-based attention visualization through PCA-Grad-CAM and SVM-Grad-CAM techniques.

Result: Successfully applied the proposed methods to several major datasets, demonstrating effective visualization of attention regions in PCA feature vectors and SVM classifier layers within CNN architectures.

Conclusion: The proposed PCA-Grad-CAM and SVM-Grad-CAM methods provide essential visualization capabilities for PCA and SVM layers in CNNs, enabling white-box interpretation of these hybrid architectures and facilitating their development and analysis.

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [71] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: ENA combines linear recurrence with tiled high-order sliding window attention for efficient modeling of ultra-long high-dimensional data, outperforming Transformers.


<details>
  <summary>Details</summary>
Motivation: Transformer architectures are inefficient for modeling long sequences of high-order data, requiring more efficient alternatives that can handle ultra-long high-dimensional data effectively.

Method: Proposes Efficient N-dimensional Attention (ENA) - a hybrid architecture combining linear recurrence (for global information compression) with tiled high-order sliding window attention (for strict local modeling). Investigated scanning strategies and attention-hybrid approaches for extending 1D models to ND data.

Result: Empirical results show scanning provides limited benefits while attention-hybrid models yield promising results. Tiled high-order sliding window attention proves efficient both theoretically and practically.

Conclusion: ENA provides a simple yet effective framework that offers a practical solution for ultra-long high-order data modeling by combining the complementary strengths of linear recurrence and sliding window attention.

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [72] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: A novel Scale-Disentangled Spatio-Temporal Modeling framework that addresses cascading error amplification in long-term traffic emission forecasting by decomposing and fusing multi-scale features while maintaining independence.


<details>
  <summary>Details</summary>
Motivation: Traditional spatiotemporal graph models suffer from cascading error amplification during long-term inference due to multi-scale entanglement of traffic emissions across time and space.

Method: Proposes SDSTM framework with dual-stream feature decomposition using Koopman lifting operator and gated wavelet decomposition, plus a fusion mechanism with cross-term loss for independence constraints.

Result: Extensive experiments on Xi'an's Second Ring Road traffic emission dataset demonstrate state-of-the-art performance in long-term traffic emission forecasting.

Conclusion: The proposed scale-disentangled approach effectively addresses multi-scale entanglement issues and significantly improves long-term traffic emission prediction accuracy.

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [73] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: Efficient algorithm for linear contextual bandits with adversarial losses and stochastic action sets, achieving poly(d)√T regret in polynomial time without context distribution knowledge.


<details>
  <summary>Details</summary>
Motivation: Addresses the open problem of whether poly(d)√T regret can be achieved in polynomial time independent of the number of actions, particularly for combinatorial bandits with adversarial losses and stochastic action sets.

Method: Reduces the setting to misspecification-robust adversarial linear bandits with fixed action sets, using an approach that doesn't require knowledge of context distribution or access to a context simulator.

Result: Achieves Õ(min{d²√T, √(d³T log K)}) regret in poly(d,C,T) time, and improves to Õ(d√L*) when a simulator is available, where L* is the best policy's cumulative loss.

Conclusion: Resolves Liu et al.'s open question and provides the first poly(d)√T regret algorithm in polynomial time for combinatorial bandits with adversarial losses and stochastic action sets.

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [74] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: M3OOD is a meta-learning framework that automatically selects optimal out-of-distribution (OOD) detectors for multimodal data by learning from historical model behaviors and dataset characteristics.


<details>
  <summary>Details</summary>
Motivation: OOD robustness is critical for multimodal ML systems, but no single OOD detector works across all distribution shifts. Manual selection is impractical due to the unsupervised nature of OOD detection and high evaluation costs on new data.

Method: Meta-learning framework that combines multimodal embeddings with handcrafted meta-features to represent datasets. Learns from historical performance across diverse multimodal benchmarks to recommend suitable detectors for new distribution shifts.

Result: M3OOD consistently outperforms 10 competitive baselines across 12 test scenarios with minimal computational overhead.

Conclusion: The framework provides an effective automated solution for OOD detector selection in multimodal settings, addressing the challenge of adapting to diverse distribution shifts without costly manual evaluation.

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [75] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: Proposes an extended Straight-Through Estimator framework for noise-aware training in analog Compute-In-Memory systems, achieving significant accuracy improvements and efficiency gains over standard methods.


<details>
  <summary>Details</summary>
Motivation: Analog CIM architectures promise energy efficiency but suffer from complex hardware-induced noise that existing noise-aware training methods fail to fully capture due to reliance on idealized differentiable noise models.

Method: Decouples forward noise simulation from backward gradient computation using an extended STE framework, enabling more accurate but computationally intractable noise modeling while preserving gradient directional information.

Result: Achieves up to 5.3% accuracy improvement on image classification, 0.72 perplexity reduction on text generation, 2.2x training speedup, and 37.9% lower peak memory usage compared to standard noise-aware training methods.

Conclusion: The extended STE framework effectively addresses analog CIM hardware noise challenges, providing both performance improvements and computational efficiency while maintaining optimization stability.

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [76] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: CFF framework combines counterfactual and factual explanations to identify minimal rational subsets of historical events that maintain MTPP prediction accuracy while avoiding irrational explanations.


<details>
  <summary>Details</summary>
Motivation: Address trustworthiness concerns in neural MTPP models by providing meaningful explanations that identify the minimum subset of historical events responsible for predictions, avoiding irrational explanations from purely counterfactual or factual approaches.

Method: Proposes Counterfactual and Factual Explainer (CFF) framework that combines both explanation types with deliberately designed techniques to identify minimal event subsets where prediction accuracy matches full history performance.

Result: Experiments demonstrate CFF's superiority over baselines in both explanation quality (correctness) and processing efficiency, showing it provides rational and minimal explanations.

Conclusion: Combining counterfactual and factual explanations is necessary for rational MTPP explanations, and CFF effectively addresses the explanation problem with high quality and efficiency.

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [77] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: Proposes Set-Valued Transformer Network (SVTN) to identify high-emission vehicles from imbalanced monitoring data, achieving 9.5% reduction in missed detection rate compared to transformer baseline.


<details>
  <summary>Details</summary>
Motivation: High-emission vehicle identification is crucial for urban pollution regulation, but real-world data suffers from severe class imbalance (long-tailed distribution) and nonlinear emission patterns that hinder effective feature extraction and model building.

Method: Uses transformer to measure temporal similarity of micro-trip conditions, mapping high-dimensional emission data to low-dimensional feature space. Then applies set-valued identification algorithm to probabilistically model feature-label relationships for classification.

Result: Achieved 9.5% reduction in missed detection rate for high-emission vehicles compared to transformer-based baseline on Hefei city 2020 diesel vehicle monitoring data.

Conclusion: SVTN effectively addresses class imbalance and nonlinear emission patterns, demonstrating superior capability in accurately identifying high-emission mobile pollution sources.

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [78] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: LoRA adapters trained on disjoint domains can be combined through simple addition without additional training, achieving performance comparable to merged-data fine-tuning while revealing interference patterns in higher-order compositions.


<details>
  <summary>Details</summary>
Motivation: To explore whether independently trained LoRA modules on different domains can be combined through simple addition based on the superposition principle, enabling efficient model composition without retraining.

Method: Train LoRA adapters (rank 4, alpha=64) for GPT-2 Small on three QA domains (math, medicine, finance) and test pairwise combinations through naive summation, measuring perplexity changes and cosine similarity between parameter deltas.

Result: Math+Medicine combination improved perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance and Finance+Medicine showed +4.54% and +27.56% changes respectively. RMS cosine similarity between LoRA deltas correlated positively and linearly with perplexity changes.

Conclusion: Naive summation of LoRA adapters requires no additional training, works in seconds, achieves comparable performance to merged-data training, and helps identify interference patterns in higher-order compositions through cosine similarity analysis.

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [79] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: A spectral filtering algorithm for learning marginally stable nonlinear dynamical systems with vanishing prediction error and novel learnability rates.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental problem of learning unknown nonlinear dynamical systems that are marginally stable, which is challenging due to their complex behavior and stability characteristics.

Method: Developed a spectral filtering algorithm that learns mappings from past observations to future states using spectral representation. Uses online convex optimization techniques and extends to general noisy and marginally stable linear systems with asymmetric dynamics.

Result: Achieves vanishing prediction error for any nonlinear dynamical system with finitely many marginally stable modes, with rates governed by a novel quantitative control-theoretic notion of learnability.

Conclusion: The method significantly generalizes spectral filtering to handle asymmetric dynamics and noise correction, providing a robust framework for learning marginally stable nonlinear systems with theoretical guarantees.

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [80] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FedUHD is a novel unsupervised federated learning framework using Hyperdimensional Computing that achieves significant speedup, energy efficiency, and accuracy improvements while reducing communication costs and enhancing noise robustness compared to neural network approaches.


<details>
  <summary>Details</summary>
Motivation: Unsupervised federated learning faces challenges with non-iid data distribution, high computational/communication costs at edge devices, and vulnerability to communication noise. Traditional deep neural network approaches introduce substantial overhead.

Method: Proposes FedUHD framework based on Hyperdimensional Computing (HDC) with two novel designs: (1) client-side kNN-based cluster hypervector removal to handle non-iid data by eliminating outliers, and (2) server-side weighted HDC aggregation to balance non-iid data distribution across clients.

Result: Achieves up to 173.6x speedup and 612.7x better energy efficiency in training, 271x lower communication cost, 15.50% higher average accuracy across diverse settings, and superior robustness to various noise types compared to state-of-the-art NN-based UFL approaches.

Conclusion: FedUHD demonstrates that HDC-based approaches can effectively address key challenges in unsupervised federated learning, providing a lightweight, efficient, and robust alternative to traditional neural network methods with significant performance improvements.

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [81] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: This paper analyzes fairness methods in Federated Learning, introduces FairGrad variants for performance equity, and shows they improve both fairness and model performance in heterogeneous data settings.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces fairness issues due to data heterogeneity causing disproportionate client impacts. Existing fairness methods' effectiveness in heterogeneous settings is unclear, and relationships between approaches are not well understood.

Method: Focuses on performance equitable fairness, evaluates fairness-aware methods that regularize client losses, introduces FairGrad and FairGrad* (gradient variance regularization variants), and provides theoretical analysis of method connections.

Result: FairGrad (approximate) and FairGrad* (exact) improve both fairness and overall model performance in heterogeneous data settings, with theoretical explanations of relationships between fairness methods.

Conclusion: The proposed FairGrad variants effectively address performance equity in FL, demonstrating superior fairness and performance outcomes compared to existing methods in heterogeneous environments.

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [82] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: VARAN is a dynamic layer aggregation framework for self-supervised speech models that adaptively weights layer features based on individual inputs, outperforming static aggregation methods on ASR and SER tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional layer aggregation methods like final layer or weighted sum suffer from information bottlenecks and static feature weighting that doesn't adapt to individual inputs, limiting performance.

Method: Uses layer-specialized probing heads and data-dependent weighting to dynamically tailor layer aggregation to individual inputs, prioritizing different layer features based on the specific input characteristics.

Result: Superior performance on automatic speech recognition and speech emotion recognition tasks, particularly when combined with LoRA fine-tuning technique.

Conclusion: VARAN resolves the trade-off between preserving layer-specific information and enabling flexible feature utilization, advancing efficient adaptation of self-supervised speech representations.

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [83] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: Proposes LPDRL-F algorithm for optimizing 3D resource tradeoff in ISAC-AIGC networks to maximize content accuracy and quality, achieving 50%+ improvement over CGQ-only schemes.


<details>
  <summary>Details</summary>
Motivation: Existing AIGC services assume accurate input data and focus only on content generation quality, but ISAC-based AIGC networks face challenges with inaccurate sensed data and generation errors from the AIGC model itself.

Method: Linear programming guided deep reinforcement learning with action filter (LPDRL-F) that transforms the 3D solution space to 2D, reducing complexity while improving DRL learning performance.

Result: LPDRL-F converges 60% faster than existing DRL and generative diffusion models, improves AvgCAQA by over 14%, and achieves 50%+ improvement in AvgCAQA compared to CGQ-only schemes.

Conclusion: The proposed LPDRL-F algorithm effectively solves the NP-hard CAQA-AIGC problem with low complexity, demonstrating significant improvements in content accuracy and quality for ISAC-based AIGC networks.

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [84] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET is a 1B-parameter transformer foundation model pretrained on 115B medical events from 118M patients that generates future medical events and outperforms task-specific models on 78 healthcare tasks without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To scale personalized medicine by developing foundation models that can distill insights from longitudinal patient journeys and generalize to diverse healthcare tasks.

Method: Decoder-only transformer models pretrained on Epic Cosmos dataset (16.3B encounters, 300M patients) using autoregressive generation of medical events, with scaling-law optimization.

Result: CoMET outperformed or matched task-specific supervised models on 78 real-world tasks including diagnosis prediction and healthcare operations, with performance improving with model scale.

Conclusion: Generative medical event foundation models like CoMET can effectively capture clinical dynamics and provide a generalizable framework for clinical decision-making and healthcare operations.

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [85] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT is a dynamic automated method that optimizes instruction-tuning dataset mixtures using multi-armed bandit approach with Prior-scaled Boltzmann Exploration and 1-Step Look-ahead Reward, achieving 2.2% performance improvement on 10 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of dynamically balancing and optimizing emerging instruction-tuning datasets during post-training stage as numerous datasets continue to emerge.

Method: Formulate as multi-armed bandit setup with Prior-scaled Boltzmann Exploration that softly anchors sampling to original dataset proportions, using lightweight 1-Step Look-ahead Reward to update sampling probabilities based on dataset contribution to model improvement.

Result: Achieves up to 2.2% performance improvement across 10 benchmarks when applied to Tulu-v2-mixture collection comprising 16 instruction-tuning datasets.

Conclusion: DynamixSFT provides an effective dynamic and automated approach for instruction-tuning dataset mixture optimization while preserving dataset diversity and coverage.

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [86] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: Gating mechanisms in RNNs implicitly create adaptive learning-rate behavior through coupling between state-space time scales and parameter-space dynamics during gradient descent.


<details>
  <summary>Details</summary>
Motivation: To understand how gating mechanisms in recurrent neural networks induce adaptive optimization behavior even when using fixed global learning rates, and to reveal the coupling between state evolution and parameter updates.

Method: Derived exact Jacobians for leaky-integrator and gated RNNs, used first-order expansion to analyze how gates reshape gradient propagation and modulate effective step sizes, and conducted numerical experiments to validate the perturbative analysis.

Result: Gates act as data-driven preconditioners that adapt optimization trajectories, introducing anisotropy in parameter updates and systematically affecting training dynamics while keeping corrections small.

Conclusion: Gating provides a unified dynamical-systems perspective that couples state evolution with parameter updates, explaining why gated architectures achieve robust trainability and stability in practice.

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [87] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: DE-VAE is an uncertainty-aware variational autoencoder that uses differential entropy to improve parametric and invertible projections, outperforming existing methods on out-of-distribution samples while enabling embedding uncertainty analysis.


<details>
  <summary>Details</summary>
Motivation: Existing autoencoder methods perform poorly with out-of-distribution samples in data or embedding space, creating a need for more robust parametric and invertible projection techniques.

Method: DE-VAE uses differential entropy in a variational autoencoder framework to learn both forward mapping to 2D space and inverse mapping back to original space, trained with UMAP and t-SNE as baseline projection methods.

Result: DE-VAE achieves comparable accuracy to current AE-based approaches while providing the additional capability of analyzing embedding uncertainty, as demonstrated on four well-known datasets.

Conclusion: The proposed DE-VAE successfully addresses limitations of existing methods by creating uncertainty-aware parametric and invertible projections that handle out-of-distribution samples effectively.

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [88] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: A novel deep learning architecture called AICRN uses attention mechanisms and convolutional residual networks to accurately regress key ECG parameters with higher precision than existing models.


<details>
  <summary>Details</summary>
Motivation: To improve diagnostic precision and predictive capacity for cardiac diseases through AI/ML, addressing challenges like human errors in traditional ECG analysis and enabling fast detection of cardiac events.

Method: Attention-integrated convolutional residual network (AICRN) with spatial and channel attention mechanisms to handle ECG feature types and spatial locations, using convolutional residual networks to prevent gradient problems.

Result: AICRN models outperform existing models in parameter regression with higher precision, demonstrating superior performance in ECG analysis tasks.

Conclusion: Deep learning can significantly enhance interpretability and precision in ECG analysis, enabling new clinical applications for cardiac monitoring and management.

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [89] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC is a lightweight compression framework that enhances protein language model ProtTeX by reducing input length through joint embedding compression and self-compression modules, achieving significant performance improvements in few-shot protein function prediction.


<details>
  <summary>Details</summary>
Motivation: Address limitations of ProtTeX model where concatenation of sequence and structure tokens doubles protein length and breaks residue-level alignment, and inability to handle in-context learning due to limited context window and single-protein training.

Method: Two-stage compression framework: 1) Joint embedding compression that fuses sequence and structure representations at residue level, reducing input length by half. 2) Self-compression module that aggregates demonstrations into latent space of last few linguistic tokens, reducing demonstration length from 751 to <16 tokens.

Result: Achieves 93.68% compression ratio in total prompt length under 16-shot setting. Improves in-domain benchmark performance by 2% and out-of-domain dataset performance by 11% with minimal additional parameters.

Conclusion: ProtTeX-CC effectively addresses ProtTeX's limitations through innovative compression techniques, enabling better in-context learning and generalization capabilities for protein function prediction without modifying the backbone model.

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [90] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: A systems approach to machine unlearning that treats training as deterministic and uses logging to enable exact replay for GDPR compliance, with complementary methods for different latency/storage constraints.


<details>
  <summary>Details</summary>
Motivation: To address the right to be forgotten (GDPR Article 17) for large language models by framing unlearning as a reproducible systems problem rather than an approximate statistical problem.

Method: Logs minimal per-microbatch records (ID hash, RNG seed, learning rate, optimizer step, accumulation boundary) and uses deterministic kernels to enable exact replay of training while filtering out forget data. Includes complementary paths: exact reverts, cohort-scoped adapter deletion, and curvature-guided anti-updates.

Result: Achieves byte-identical equality of model and optimizer states when preconditions are met, demonstrating the approach can produce the same parameters as training only on the retain set.

Conclusion: The deterministic systems approach provides a practical solution for GDPR-compliant unlearning in LLMs with verifiable results, though it requires specific preconditions and has storage/latency tradeoffs.

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [91] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: A novel distribution matching approach inspired by consistency models from Continuous Normalizing Flows, combining CNF advantages with GAN-like flexibility while avoiding their training challenges.


<details>
  <summary>Details</summary>
Motivation: GANs face training difficulties due to min-max optimization and mode collapse, despite being effective for distribution matching tasks. The authors seek a more stable alternative that maintains GANs' flexibility.

Method: Proposed approach draws inspiration from consistency models used in Continuous Normalizing Flows (CNF), inheriting CNF's straightforward norm minimization objective while remaining adaptable to various constraints like GANs.

Result: Theoretical validation of the proposed objective and experimental demonstration on both synthetic and real-world datasets show promising performance.

Conclusion: The method successfully addresses GAN training challenges while maintaining distribution matching capabilities across different constraints and data modalities.

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [92] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: Proposes coarse quantization for asynchronous ADMM to reduce communication overhead in distributed optimization and federated learning.


<details>
  <summary>Details</summary>
Motivation: Communication costs become a major bottleneck in distributed optimization when nodes have limited communication budgets or data is prohibitively large.

Method: Introduces coarse quantization to the data exchanged in asynchronous ADMM to reduce communication overhead.

Result: Experimentally verified convergence of the proposed method for several distributed learning tasks, including neural networks.

Conclusion: Quantized asynchronous ADMM provides an effective solution for reducing communication costs while maintaining convergence in large-scale distributed optimization.

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [93] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: CC-Time is a novel approach that combines pre-trained language models with time series forecasting through cross-modality learning and cross-model fusion, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current PLM-based time series forecasting methods fail to achieve satisfactory prediction accuracy despite the strong sequential modeling power of language models.

Method: Proposes CC-Time with two key components: 1) cross-modality learning to model temporal dependency and channel correlations using both time series sequences and text descriptions, and 2) cross-model fusion block to adaptively integrate knowledge from PLMs and time series models.

Result: Extensive experiments on nine real-world datasets show CC-Time achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations.

Conclusion: CC-Time successfully bridges the gap between language models and time series forecasting by leveraging cross-modality learning and model fusion, demonstrating the potential of PLMs for comprehensive time series pattern modeling.

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [94] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: DHG-Bench is the first comprehensive benchmark for deep hypergraph learning, addressing limitations in current hypergraph neural network evaluation by providing standardized datasets, algorithms, and experimental protocols across multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing hypergraph neural network methods lack comprehensive benchmarking, with insufficient dataset coverage, narrow performance evaluation, and inconsistent experimental setups that hinder progress understanding in deep hypergraph learning.

Method: Developed DHG-Bench with 20 diverse datasets spanning node-, edge-, and graph-level tasks, integrated 16 state-of-the-art HNN algorithms under consistent data processing and experimental protocols, and systematically evaluated across effectiveness, efficiency, robustness, and fairness dimensions.

Result: Extensive experiments revealed both strengths and inherent limitations of existing hypergraph neural network algorithms, providing valuable insights for future research directions in deep hypergraph learning.

Conclusion: DHG-Bench fills a critical gap in hypergraph learning research by providing the first standardized benchmark that enables fair comparison and reproducible evaluation of hypergraph neural networks, facilitating future advancements in the field.

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [95] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: STM2/STM3 models address long-term spatio-temporal dependency challenges using multiscale Mamba architecture and adaptive graph convolution, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with learning complex long-term spatio-temporal dependencies efficiently, particularly with multiscale temporal information and correlated spatial nodes.

Method: STM2 uses multiscale Mamba architecture for efficient multiscale information capture and adaptive graph causal convolution. STM3 enhances this with Mixture-of-Experts architecture, stable routing strategy, and causal contrastive learning for better scale distinguishability.

Result: Extensive experiments on real-world benchmarks demonstrate superior performance, achieving state-of-the-art results in long-term spatio-temporal time-series prediction.

Conclusion: The proposed STM2/STM3 models effectively address the challenges of long-term spatio-temporal dependency learning through innovative multiscale architectures and routing strategies.

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [96] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: A unified framework for interpreting time-series forecasts using LIME and SHAP that combines ARIMA's interpretability with XGBoost's accuracy while maintaining chronological integrity.


<details>
  <summary>Details</summary>
Motivation: Time-series forecasting is critical across many domains, but existing approaches face trade-offs between interpretability (ARIMA) and accuracy (XGBoost), with tree-based models often being opaque.

Method: Convert univariate time series into leakage-free supervised learning problem, train gradient-boosted tree alongside ARIMA baseline, and apply LIME and SHAP for post-hoc explainability while preserving chronology.

Result: Using Air Passengers dataset, a small set of lagged features (particularly twelve-month lag) and seasonal encodings explain most forecast variance, demonstrating effective interpretation of complex models.

Conclusion: The paper provides a methodology for applying LIME/SHAP to time series without chronological violations, theoretical exposition, empirical evaluation, and practical guidelines for interpretable forecasting.

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [97] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: A novel learned second-order optimizer that enhances the classical SR1 algorithm with trainable preconditioning, outperforming existing learned optimization methods on monocular human mesh recovery tasks.


<details>
  <summary>Details</summary>
Motivation: End-to-end deep learning has limitations including reliance on large labeled datasets, poor generalization, and high computational demands. Classical optimization methods are data-efficient but slow to converge. Learned optimizers combine benefits but most focus on first-order methods, leaving second-order approaches underexplored.

Method: Proposes a learned second-order optimizer with trainable preconditioning unit that generates data-driven vectors to construct positive semi-definite rank-one matrices aligned with secant constraint via learned projection.

Result: Outperforms existing learned optimization-based approaches on monocular human mesh recovery tasks, featuring lightweight model, no annotated data requirements, and strong generalization capabilities.

Conclusion: The approach offers a promising fusion of deep learning and classical optimization, providing data efficiency, strong generalization, and suitability for integration into broader optimization-based frameworks without requiring fine-tuning.

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [98] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: CRoC is a novel framework that combines limited labeled data with abundant unlabeled data for Graph Anomaly Detection, using context refactoring and contrastive learning to achieve significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Training robust Graph Neural Networks for anomaly detection requires abundant labeled data, which is scarce and expensive in real-world applications where anomalies are rare and often camouflaged.

Method: CRoC refactors node contexts by recomposing attributes while preserving interaction patterns, encodes heterogeneous relations separately, integrates them into message-passing, and uses contrastive learning to leverage unlabeled data.

Result: CRoC achieves up to 14% AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods on seven real-world datasets under limited-label settings.

Conclusion: The proposed CRoC framework effectively addresses the label scarcity problem in Graph Anomaly Detection by leveraging both limited labeled and abundant unlabeled data through innovative context refactoring and contrastive learning techniques.

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [99] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: Convergence analysis of Lion optimizer showing improved rates from O(d^{1/2}T^{-1/4}) to O(d^{1/2}T^{-1/3}) with variance reduction, and further enhancements in distributed settings with communication-efficient variants.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous theoretical analysis of the Lion optimizer's convergence properties and develop improved variants with better convergence rates, particularly for distributed optimization settings.

Method: Theoretical analysis of standard Lion optimizer convergence, introduction of variance reduction technique, extension to distributed settings, and development of communication-efficient variants using sign compression operations.

Result: Standard Lion achieves O(d^{1/2}T^{-1/4}), variance-reduced version achieves O(d^{1/2}T^{-1/3}). Distributed versions achieve O(d^{1/2}(nT)^{-1/4}) and O(d^{1/2}(nT)^{-1/3}). Communication-efficient variants achieve O(max{d^{1/4}/T^{1/4}, d^{1/10}/(n^{1/5}T^{1/5})}) and O(d^{1/4}/T^{1/4}).

Conclusion: The Lion optimizer and its variants demonstrate strong theoretical convergence properties, with variance reduction and communication-efficient techniques providing significant improvements, making it suitable for large-scale distributed optimization problems.

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>


### [100] [Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2508.12361)
*Xun Su,Jianming Huang,Yang Yusen,Zhongxi Fang,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: Novel Sequential Monte Carlo methods (Funnel Schedule and Adaptive Temperature) for diffusion models that address the exploration-exploitation trade-off in inference-time scaling, improving sample quality without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Current SMC methods for diffusion models face a dilemma: early-stage noise samples have high improvement potential but are hard to evaluate accurately, while late-stage samples are reliable but irreversible. This exploration-exploitation trade-off limits effective inference-time scaling in diffusion models.

Method: Proposed two strategies: 1) Funnel Schedule - progressively reduces the number of maintained particles, and 2) Adaptive Temperature - down-weights the influence of early-stage rewards. These are tailored to diffusion models' unique generation dynamics and phase-transition behavior.

Result: Experimental results on multiple benchmarks and state-of-the-art text-to-image diffusion models show the approach outperforms previous baselines, significantly enhancing sample quality without increasing the total number of Noise Function Evaluations.

Conclusion: The proposed methods effectively address the fundamental dilemma in SMC applications to diffusion models, providing simple yet effective strategies that leverage the unique properties of diffusion generation dynamics for improved inference-time scaling.

Abstract: Inference-time scaling has achieved remarkable success in language models,
yet its adaptation to diffusion models remains underexplored. We observe that
the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems
from globally fitting the The reward-tilted distribution, which inherently
preserves diversity during multi-modal search. However, current applications of
SMC to diffusion models face a fundamental dilemma: early-stage noise samples
offer high potential for improvement but are difficult to evaluate accurately,
whereas late-stage samples can be reliably assessed but are largely
irreversible. To address this exploration-exploitation trade-off, we approach
the problem from the perspective of the search algorithm and propose two
strategies: Funnel Schedule and Adaptive Temperature. These simple yet
effective methods are tailored to the unique generation dynamics and
phase-transition behavior of diffusion models. By progressively reducing the
number of maintained particles and down-weighting the influence of early-stage
rewards, our methods significantly enhance sample quality without increasing
the total number of Noise Function Evaluations. Experimental results on
multiple benchmarks and state-of-the-art text-to-image diffusion models
demonstrate that our approach outperforms previous baselines.

</details>


### [101] [Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification](https://arxiv.org/abs/2508.12418)
*Rachael DeVries,Casper Christensen,Marie Lisandra Zepeda Mendoza,Ole Winther*

Main category: cs.LG

TL;DR: BAT (Bi-Axial Transformer) is a novel transformer architecture that attends to both clinical variable and time point axes in EHR data, achieving state-of-the-art performance on sepsis prediction and competitive results for mortality classification while being robust to data missingness.


<details>
  <summary>Details</summary>
Motivation: Electronic Health Records (EHRs) are valuable for research but face challenges with large datasets, long time series, and multi-modal data. Existing transformer applications to EHR classification are limited by data representations that reduce performance or fail to capture informative missingness patterns.

Method: The Bi-Axial Transformer (BAT) attends to both clinical variable and time point axes of EHR data to learn richer data relationships and address data sparsity issues. The model learns unique sensor embeddings that enable transfer learning capabilities.

Result: BAT achieves state-of-the-art performance on sepsis prediction and is competitive with top methods for mortality classification. It demonstrates increased robustness to data missingness compared to other transformers and enables transfer learning through learned sensor embeddings.

Conclusion: BAT provides an effective transformer-based approach for EHR analysis that handles the unique challenges of healthcare data, including sparsity and missingness. The re-implementation of baseline models with PyTorch facilitates reproduction and future benchmarking in this domain.

Abstract: Electronic Health Records (EHRs), the digital representation of a patient's
medical history, are a valuable resource for epidemiological and clinical
research. They are also becoming increasingly complex, with recent trends
indicating larger datasets, longer time series, and multi-modal integrations.
Transformers, which have rapidly gained popularity due to their success in
natural language processing and other domains, are well-suited to address these
challenges due to their ability to model long-range dependencies and process
data in parallel. But their application to EHR classification remains limited
by data representations, which can reduce performance or fail to capture
informative missingness. In this paper, we present the Bi-Axial Transformer
(BAT), which attends to both the clinical variable and time point axes of EHR
data to learn richer data relationships and address the difficulties of data
sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is
competitive to top methods for mortality classification. In comparison to other
transformers, BAT demonstrates increased robustness to data missingness, and
learns unique sensor embeddings which can be used in transfer learning.
Baseline models, which were previously located across multiple repositories or
utilized deprecated libraries, were re-implemented with PyTorch and made
available for reproduction and future benchmarking.

</details>


### [102] [Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features](https://arxiv.org/abs/2508.12440)
*Ahmet Bilal Arıkan,Şener Özönder,Mustafa Taha Koçyiğit,Hüseyin Oktay Altun,H. Kübra Küçükkartal,Murat Arslanoğlu,Fatih Çağırankaya,Berk Ayvaz*

Main category: cs.LG

TL;DR: Machine learning framework that automates manufacturing cost estimation from 2D engineering drawings using geometric features and gradient-boosted models, achieving ~10% error with explainable insights.


<details>
  <summary>Details</summary>
Motivation: To overcome labor-intensive traditional quotation workflows that require manual process planning, and provide automated, scalable cost estimation for manufacturing parts.

Method: Extracted 200+ geometric and statistical descriptors from 13,684 automotive DWG drawings across 24 product groups, trained gradient-boosted decision tree models (XGBoost, CatBoost, LightGBM) on these features.

Result: Achieved nearly 10% mean absolute percentage error across product groups, demonstrating robust scalability beyond part-specific heuristics with explainable cost drivers identified through SHAP analysis.

Conclusion: The end-to-end CAD-to-cost pipeline significantly shortens quotation lead times, ensures consistent cost assessments, and provides deployable real-time decision support for Industry 4.0 manufacturing environments.

Abstract: We present an integrated machine learning framework that transforms how
manufacturing cost is estimated from 2D engineering drawings. Unlike
traditional quotation workflows that require labor-intensive process planning,
our approach about 200 geometric and statistical descriptors directly from
13,684 DWG drawings of automotive suspension and steering parts spanning 24
product groups. Gradient-boosted decision tree models (XGBoost, CatBoost,
LightGBM) trained on these features achieve nearly 10% mean absolute percentage
error across groups, demonstrating robust scalability beyond part-specific
heuristics. By coupling cost prediction with explainability tools such as SHAP,
the framework identifies geometric design drivers including rotated dimension
maxima, arc statistics and divergence metrics, offering actionable insights for
cost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead
times, ensures consistent and transparent cost assessments across part families
and provides a deployable pathway toward real-time, ERP-integrated decision
support in Industry 4.0 manufacturing environments.

</details>


### [103] [Local Cluster Cardinality Estimation for Adaptive Mean Shift](https://arxiv.org/abs/2508.12450)
*Étienne Pepin*

Main category: cs.LG

TL;DR: Adaptive mean shift algorithm that uses local distance distributions to estimate cluster cardinality and adjust bandwidth parameters dynamically, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional mean shift algorithms struggle with datasets containing varying local scale and cluster cardinality, requiring adaptive approaches to handle these variations effectively.

Method: Uses local distance distributions to identify local minima in density for cardinality estimation, then computes cluster parameters globally rather than locally. Adaptively adjusts bandwidth and kernel radius threshold during mean shift execution based on cardinality estimates.

Result: Outperformed a recently proposed adaptive mean shift method on its original dataset and demonstrated competitive performance on a broader clustering benchmark.

Conclusion: The proposed adaptive mean shift algorithm effectively handles varying cluster scales and cardinalities through local distance distribution analysis, providing superior performance compared to existing methods.

Abstract: This article presents an adaptive mean shift algorithm designed for datasets
with varying local scale and cluster cardinality. Local distance distributions,
from a point to all others, are used to estimate the cardinality of the local
cluster by identifying a local minimum in the density of the distance
distribution. Based on these cardinality estimates, local cluster parameters
are then computed for the entire cluster in contrast to KDE-based methods,
which provide insight only into localized regions of the cluster. During the
mean shift execution, the cluster cardinality estimate is used to adaptively
adjust the bandwidth and the mean shift kernel radius threshold. Our algorithm
outperformed a recently proposed adaptive mean shift method on its original
dataset and demonstrated competitive performance on a broader clustering
benchmark.

</details>


### [104] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: Cold-RL replaces NGINX's LRU eviction with a reinforcement learning policy using a dueling DQN that selects cache victims from LRU candidates based on multiple features, achieving 146% hit ratio improvement at small cache sizes with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional LRU eviction in web proxies like NGINX is size-agnostic and performs poorly under periodic bursts and mixed object sizes, leading to cache thrashing and suboptimal performance.

Method: Uses a dueling Deep Q-Network served by an ONNX sidecar to select eviction victims from K least-recently-used objects. Extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, last origin RTT) and has a 500 microsecond timeout fallback to LRU. Policies trained offline by replaying NGINX access logs through cache simulator.

Result: At 25MB cache: hit ratio improved from 0.1436 to 0.3538 (146% improvement over best baseline). At 100MB: from 0.7530 to 0.8675 (15% gain). At 400MB: matches classical methods (~0.918). Adds <2% CPU overhead and keeps 95th percentile latency within budget.

Conclusion: Cold-RL successfully integrates reinforcement learning into NGINX with strict SLOs, significantly improving cache performance especially at smaller cache sizes while maintaining low overhead and latency constraints.

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [105] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: CSCR is a lightweight routing framework that maps prompts and models to a shared embedding space for fast, cost-sensitive LLM selection using compact model fingerprints and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing routing approaches overlook prompt-specific context, require expensive model profiling, assume fixed expert sets, or use inefficient trial-and-error strategies for large language model selection.

Method: Uses compact logit footprints for open-source models and perplexity fingerprints for black-box APIs. Trains contrastive encoder to favor cheapest accurate expert within adaptive cost bands. Routing reduces to single k-NN lookup via FAISS index.

Result: Outperforms baselines across multiple benchmarks, improving accuracy-cost tradeoff by up to 25%, with robust generalization to unseen LLMs and out-of-distribution prompts.

Conclusion: CSCR enables fast, cost-aware routing with microsecond latency, requires no retraining when expert pool changes, and provides superior performance compared to existing approaches.

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [106] [Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference](https://arxiv.org/abs/2508.12511)
*Denis Blessing,Julius Berner,Lorenz Richter,Carles Domingo-Enrich,Yuanqi Du,Arash Vahdat,Gerhard Neumann*

Main category: cs.LG

TL;DR: A novel trust region-based method for solving stochastic optimal control problems that uses geometric annealing with principled time step selection to gradually approach target measures from priors.


<details>
  <summary>Details</summary>
Motivation: Solving stochastic optimal control problems with quadratic costs is challenging when target measures differ substantially from priors, requiring better optimization approaches.

Method: Iteratively solving constrained problems with trust regions that systematically approach the target measure gradually, implementing geometric annealing from prior to target with principled time step selection.

Result: Demonstrated significant performance improvements in multiple optimal control applications including diffusion-based sampling, transition path sampling, and diffusion model fine-tuning.

Conclusion: The trust region-based strategy provides an effective and principled approach for challenging stochastic optimal control problems where target measures differ substantially from priors.

Abstract: Solving stochastic optimal control problems with quadratic control costs can
be viewed as approximating a target path space measure, e.g. via gradient-based
optimization. In practice, however, this optimization is challenging in
particular if the target measure differs substantially from the prior. In this
work, we therefore approach the problem by iteratively solving constrained
problems incorporating trust regions that aim for approaching the target
measure gradually in a systematic way. It turns out that this trust region
based strategy can be understood as a geometric annealing from the prior to the
target measure, where, however, the incorporated trust regions lead to a
principled and educated way of choosing the time steps in the annealing path.
We demonstrate in multiple optimal control applications that our novel method
can improve performance significantly, including tasks in diffusion-based
sampling, transition path sampling, and fine-tuning of diffusion models.

</details>


### [107] [Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning](https://arxiv.org/abs/2508.12524)
*Joseph Suárez,Kyoung Whan Choe,David Bloomin,Jianming Gao,Yunkun Li,Yao Feng,Saidinesh Pola,Kun Zhang,Yonghui Zhu,Nikhil Pinnaparaju,Hao Xiang Li,Nishaanth Kanna,Daniel Scott,Ryan Sullivan,Rose S. Shuman,Lucas de Alcântara,Herbie Bradley,Kirsty You,Bo Wu,Yuhao Jiang,Qimai Li,Jiaxin Chen,Louis Castricato,Xiaolong Zhu,Phillip Isola*

Main category: cs.LG

TL;DR: NeurIPS 2023 Neural MMO Competition attracted 200+ participants who trained goal-conditional policies that generalized to unseen tasks, maps, and opponents, with top solution achieving 4x baseline score in 8 hours on single GPU.


<details>
  <summary>Details</summary>
Motivation: To advance research in goal-conditional policy learning and generalization in complex multi-agent environments through a competitive benchmark.

Method: Competition format where participants trained goal-conditional policies using Neural MMO platform, testing generalization to unseen tasks, maps, and opponents.

Result: Over 200 participants submitted solutions; top solution achieved 4x higher score than baseline within 8 hours of training on a single 4090 GPU.

Conclusion: The competition successfully demonstrated strong generalization capabilities of goal-conditional policies and all materials (code, weights, baseline) are open-sourced under MIT license for community use.

Abstract: We present the results of the NeurIPS 2023 Neural MMO Competition, which
attracted over 200 participants and submissions. Participants trained
goal-conditional policies that generalize to tasks, maps, and opponents never
seen during training. The top solution achieved a score 4x higher than our
baseline within 8 hours of training on a single 4090 GPU. We open-source
everything relating to Neural MMO and the competition under the MIT license,
including the policy weights and training code for our baseline and for the top
submissions.

</details>


### [108] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: Proposes Latent Reconstruction (LR) loss to address posterior collapse in VAEs without architectural constraints, using mathematical properties of injective functions to control collapse while maintaining sample diversity.


<details>
  <summary>Details</summary>
Motivation: VAEs suffer from posterior collapse that reduces sample diversity. Existing methods require architectural constraints or have unsatisfactory trade-offs between reconstruction and regularization.

Method: Defines local posterior collapse concept and proposes LR loss based on injective/composite function properties to control collapse without specific network architecture restrictions.

Result: Experimental evaluation shows effective control of posterior collapse across multiple datasets including MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

Conclusion: LR loss provides an effective solution for posterior collapse in VAEs without requiring structural network constraints, enabling better sample diversity across varied datasets.

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [109] [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)
*Minseon Kim,Jin Myung Kwak,Lama Alssum,Bernard Ghanem,Philip Torr,David Krueger,Fazl Barez,Adel Bibi*

Main category: cs.LG

TL;DR: Fine-tuning language models doesn't inherently harm safety - poor optimization choices cause safety issues, not inherent trade-offs. Proper hyperparameter selection and EMA momentum technique can maintain safety while preserving utility.


<details>
  <summary>Details</summary>
Motivation: Challenge the common belief that fine-tuning inevitably harms model safety, and demonstrate that safety problems are caused by optimization issues rather than inherent limitations

Method: Systematic testing of key training hyperparameters (learning rate, batch size, gradient steps) and proposing exponential moving average (EMA) momentum technique in parameter space to create stable optimization paths

Result: Reduced unsafe model responses from 16% to approximately 5% while maintaining utility performance, outperforming existing approaches that require additional safety data

Conclusion: Safety problems during fine-tuning can be largely avoided without specialized interventions through proper optimization choices, providing practical guidelines for maintaining both performance and safety during model adaptation

Abstract: Fine-tuning language models is commonly believed to inevitably harm their
safety, i.e., refusing to respond to harmful user requests, even when using
harmless datasets, thus requiring additional safety measures. We challenge this
belief through systematic testing, showing that poor optimization choices,
rather than inherent trade-offs, often cause safety problems, measured as
harmful responses to adversarial prompts. By properly selecting key training
hyper-parameters, e.g., learning rate, batch size, and gradient steps, we
reduce unsafe model responses from 16\% to approximately 5\%, as measured by
keyword matching, while maintaining utility performance. Based on this
observation, we propose a simple exponential moving average (EMA) momentum
technique in parameter space that preserves safety performance by creating a
stable optimization path and retains the original pre-trained model's safety
properties. Our experiments on the Llama families across multiple datasets
(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can
largely be avoided without specialized interventions, outperforming existing
approaches that require additional safety data while offering practical
guidelines for maintaining both model performance and safety during adaptation.

</details>


### [110] [Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction](https://arxiv.org/abs/2508.12533)
*Qinwen Ge,Roza G. Bayrak,Anwar Said,Catie Chang,Xenofon Koutsoukos,Tyler Derr*

Main category: cs.LG

TL;DR: Systematic benchmarking of data-centric design choices for fMRI brain graph construction, showing that thoughtful pipeline configurations outperform standard approaches in neuroimaging classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current brain graph construction from fMRI data relies on rigid pipelines that overlook critical data-centric choices, while most prior work focuses on model-centric approaches rather than optimizing the data construction process itself.

Method: Organized a data-centric design space into three stages: temporal signal processing, topology extraction, and graph featurization. Evaluated combinations of existing techniques including high-amplitude BOLD signal filtering, sparsification strategies, alternative correlation metrics, and multi-view node/edge features with lagged dynamics.

Result: Experiments on HCP1200 and ABIDE datasets showed that thoughtful data-centric configurations consistently improve classification accuracy over standard pipelines, with specific techniques like high-amplitude filtering and alternative correlation metrics providing significant performance gains.

Conclusion: Upstream data decisions play a critical role in graph-based neuroimaging performance, and systematically exploring the data-centric design space is essential for improving results in brain graph construction from fMRI data.

Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging
(fMRI) data plays a crucial role in enabling graph machine learning for
neuroimaging. However, current practices often rely on rigid pipelines that
overlook critical data-centric choices in how brain graphs are constructed. In
this work, we adopt a Data-Centric AI perspective and systematically define and
benchmark a data-centric design space for brain graph construction,
constrasting with primarily model-centric prior work. We organize this design
space into three stages: temporal signal processing, topology extraction, and
graph featurization. Our contributions lie less in novel components and more in
evaluating how combinations of existing and modified techniques influence
downstream performance. Specifically, we study high-amplitude BOLD signal
filtering, sparsification and unification strategies for connectivity,
alternative correlation metrics, and multi-view node and edge features, such as
incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets
show that thoughtful data-centric configurations consistently improve
classification accuracy over standard pipelines. These findings highlight the
critical role of upstream data decisions and underscore the importance of
systematically exploring the data-centric design space for graph-based
neuroimaging. Our code is available at
https://github.com/GeQinwen/DataCentricBrainGraphs.

</details>


### [111] [OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning](https://arxiv.org/abs/2508.12551)
*Hongyu Lin,Yuchen Li,Haoran Luo,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: OS-R1 is a rule-based reinforcement learning framework that uses LLMs to optimize Linux kernel tuning, achieving 5.6% performance improvement over heuristic methods with high data efficiency and cross-environment adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing Linux kernel tuning methods face challenges in efficiency, scalability, and generalization, creating a need for a more effective automated tuning framework.

Method: Abstracts kernel configuration space as RL environment, uses LLMs for exploration with custom reward functions for standardization and accuracy, and employs two-phase training for fast convergence.

Result: Achieves up to 5.6% performance improvement over heuristic tuning, maintains high data efficiency, and demonstrates adaptability across various real-world applications.

Conclusion: OS-R1 shows significant potential for practical deployment in diverse environments and outperforms existing baseline methods in Linux kernel optimization.

Abstract: Linux kernel tuning is essential for optimizing operating system (OS)
performance. However, existing methods often face challenges in terms of
efficiency, scalability, and generalization. This paper introduces OS-R1, an
agentic Linux kernel tuning framework powered by rule-based reinforcement
learning (RL). By abstracting the kernel configuration space as an RL
environment, OS-R1 facilitates efficient exploration by large language models
(LLMs) and ensures accurate configuration modifications. Additionally, custom
reward functions are designed to enhance reasoning standardization,
configuration modification accuracy, and system performance awareness of the
LLMs. Furthermore, we propose a two-phase training process that accelerates
convergence and minimizes retraining across diverse tuning scenarios.
Experimental results show that OS-R1 significantly outperforms existing
baseline methods, achieving up to 5.6% performance improvement over heuristic
tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across
various real-world applications, demonstrating its potential for practical
deployment in diverse environments. Our dataset and code are publicly available
at https://github.com/LHY-24/OS-R1.

</details>


### [112] [Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement](https://arxiv.org/abs/2508.12555)
*Junpeng Wang,Yuzhong Chen,Menghai Pan,Chin-Chia Michael Yeh,Mahashweta Das*

Main category: cs.LG

TL;DR: A visual analytics system for examining coding agent behaviors across code-level, process-level, and LLM-level analysis to improve debugging and prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Current manual inspection of LLM coding agents' outputs is inefficient, making it difficult to track code evolution, compare iterations, and identify improvement opportunities.

Method: Developed a visual analytics system focusing on AIDE framework that supports comparative analysis across three levels: code-level (debugging and refinement), process-level (solution-seeking processes), and LLM-level (behavior variations across different LLMs).

Result: The system enables structured understanding of agent behaviors through case studies using coding agents in popular Kaggle competitions, providing valuable insights into iterative coding processes.

Conclusion: The visual analytics system effectively enhances examination of coding agent behaviors, facilitating more efficient debugging and prompt engineering for ML scientists.

Abstract: Coding agents powered by large language models (LLMs) have gained traction
for automating code generation through iterative problem-solving with minimal
human involvement. Despite the emergence of various frameworks, e.g.,
LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review
and adjust the agents' coding process. The current approach of manually
inspecting individual outputs is inefficient, making it difficult to track code
evolution, compare coding iterations, and identify improvement opportunities.
To address this challenge, we introduce a visual analytics system designed to
enhance the examination of coding agent behaviors. Focusing on the AIDE
framework, our system supports comparative analysis across three levels: (1)
Code-Level Analysis, which reveals how the agent debugs and refines its code
over iterations; (2) Process-Level Analysis, which contrasts different
solution-seeking processes explored by the agent; and (3) LLM-Level Analysis,
which highlights variations in coding behavior across different LLMs. By
integrating these perspectives, our system enables ML scientists to gain a
structured understanding of agent behaviors, facilitating more effective
debugging and prompt engineering. Through case studies using coding agents to
tackle popular Kaggle competitions, we demonstrate how our system provides
valuable insights into the iterative coding process.

</details>


### [113] [Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition](https://arxiv.org/abs/2508.12565)
*Luke Li*

Main category: cs.LG

TL;DR: Combining VMD decomposition with LSTM for financial time series forecasting shows better performance than using raw data.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and non-stationarity of financial time series data for improved forecasting accuracy.

Method: Using variational mode decomposition (VMD) to break down non-stationary financial time series into smoother subcomponents, then feeding the decomposed data into an LSTM deep learning model for prediction.

Result: The model trained on VMD-processed sequences demonstrated better performance and stability compared to using raw time series data.

Conclusion: VMD decomposition effectively improves financial time series forecasting when combined with deep learning models like LSTM.

Abstract: To address the complexity of financial time series, this paper proposes a
forecasting model combining sliding window and variational mode decomposition
(VMD) methods. Historical stock prices and relevant market indicators are used
to construct datasets. VMD decomposes non-stationary financial time series into
smoother subcomponents, improving model adaptability. The decomposed data is
then input into a deep learning model for prediction. The study compares the
forecasting effects of an LSTM model trained on VMD-processed sequences with
those using raw time series, demonstrating better performance and stability.

</details>


### [114] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: A machine learning framework for coarse-graining multiscale systems using metriplectic bracket formalism that preserves thermodynamic laws, momentum conservation, and fluctuation-dissipation balance.


<details>
  <summary>Details</summary>
Motivation: Multiscale systems are challenging to simulate due to information loss during coarse-graining, leading to emergent dissipative, history-dependent, and stochastic physics that need to be properly captured.

Method: Proposes a framework using metriplectic bracket formalism to machine learn coarse-grained dynamics from particle trajectory time-series data, with self-supervised learning to identify emergent structural variables.

Result: Validated on benchmark systems and demonstrated utility on star polymers and colloidal suspensions, capturing non-equilibrium statistics and coupling between local events and emergent dynamics.

Conclusion: The framework successfully preserves thermodynamic consistency and enables large-scale inference for diverse particle-based systems, with open-source implementations provided.

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [115] [Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM](https://arxiv.org/abs/2508.12575)
*Zohra Yagoub,Hafida Bouziane*

Main category: cs.LG

TL;DR: Using pretrained protein language models with bidirectional LSTM and GRU networks to predict amyloidogenic regions achieves 84.5% accuracy in cross-validation and 83% on test data.


<details>
  <summary>Details</summary>
Motivation: Current amyloid prediction methods rely heavily on evolutionary motifs and amino acid properties, but sequence-based features show high predictive potential. The study aims to leverage protein language models for improved amyloidogenicity prediction.

Method: Utilized contextual features from pretrained protein large language models combined with bidirectional LSTM and GRU neural networks to predict amyloidogenic regions in protein sequences.

Result: Achieved 84.5% accuracy on 10-fold cross-validation and 83% accuracy on test dataset, demonstrating competitive performance compared to existing methods.

Conclusion: Protein large language models show significant potential for enhancing amyloid prediction accuracy, providing a promising computational approach for bioinformatics applications in amyloidogenicity detection.

Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal
point of ongoing bioinformatics. The crucial step in this field is to apply
advanced computational methodologies. Many recent approaches to predicting
amyloidogenicity within proteins are highly based on evolutionary motifs and
the individual properties of amino acids. It is becoming increasingly evident
that the sequence information-based features show high predictive performance.
Consequently, our study evaluated the contextual features of protein sequences
obtained from a pretrained protein large language model leveraging
bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and
protein sequences. Our method achieved an accuracy of 84.5% on 10-fold
cross-validation and an accuracy of 83% in the test dataset. Our results
demonstrate competitive performance, highlighting the potential of LLMs in
enhancing the accuracy of amyloid prediction.

</details>


### [116] [Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg](https://arxiv.org/abs/2508.12576)
*Like Jian,Dong Liu*

Main category: cs.LG

TL;DR: Overparameterized FedAvg with gradient descent shows that data heterogeneity impact diminishes as neural network width increases, vanishing completely at infinite width where FedAvg achieves same generalization as centralized learning.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with non-IID client data distributions that hinder global model generalization. This paper aims to understand how overparameterization affects FedAvg convergence in heterogeneous data settings.

Method: Theoretical analysis of FedAvg convergence with gradient descent on overparameterized neural networks, examining behavior as network width increases to infinity. Extensive experiments validate findings across various architectures and optimization methods.

Result: Proved that data heterogeneity impact diminishes with increasing network width, vanishing completely at infinite width. Showed that both global and local models behave as linear models in infinite-width regime, and FedAvg achieves same generalization performance as centralized learning.

Conclusion: Overparameterization effectively mitigates data heterogeneity challenges in federated learning, with infinite-width neural networks enabling FedAvg to match centralized learning performance despite non-IID data distributions.

Abstract: Federated learning (FL) enables decentralized clients to train a model
collaboratively without sharing local data. A key distinction between FL and
centralized learning is that clients' data are non-independent and identically
distributed, which poses significant challenges in training a global model that
generalizes well across heterogeneous local data distributions. In this paper,
we analyze the convergence of overparameterized FedAvg with gradient descent
(GD). We prove that the impact of data heterogeneity diminishes as the width of
neural networks increases, ultimately vanishing when the width approaches
infinity. In the infinite-width regime, we further prove that both the global
and local models in FedAvg behave as linear models, and that FedAvg achieves
the same generalization performance as centralized learning with the same
number of GD iterations. Extensive experiments validate our theoretical
findings across various network architectures, loss functions, and optimization
methods.

</details>


### [117] [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)
*Jihoon Park,Seungeun Oh,Seong-Lyun Kim*

Main category: cs.LG

TL;DR: Token-level filtering mechanism for hybrid LLMs that reduces energy consumption by 40.7% while maintaining accuracy by uploading only informative tokens based on uncertainty and importance.


<details>
  <summary>Details</summary>
Motivation: Address the need for energy-efficient on-device LLM inference in resource-constrained edge environments, where current HLM approaches focus on accuracy and latency but overlook communication and energy efficiency.

Method: Proposes a token-level filtering mechanism that leverages both epistemic uncertainty and attention-based importance to opportunistically upload only informative tokens to cloud-based LLMs, reducing LLM usage and communication costs.

Result: Achieves up to 87.5% BERT Score, 0.37 tokens/sec throughput, and 40.7% energy savings compared to standard HLM. Outperforms previous U-HLM baseline with improved BERTScore (85.8% to 87.0%), energy savings (31.6% to 43.6%), and throughput (0.36 to 0.40).

Conclusion: Enables energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge environments through selective token uploading based on uncertainty and importance metrics.

Abstract: To address the growing demand for on-device LLM inference in
resource-constrained environments, hybrid language models (HLM) have emerged,
combining lightweight local models with powerful cloud-based LLMs. Recent
studies on HLM have primarily focused on improving accuracy and latency, while
often overlooking communication and energy efficiency. We propose a token-level
filtering mechanism for an energy-efficient importance- and uncertainty-aware
HLM inference that leverages both epistemic uncertainty and attention-based
importance. Our method opportunistically uploads only informative tokens,
reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and
LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and
token throughput of 0.37 tokens/sec while saving the energy consumption by
40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM
baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings
from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an
energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge
environments.

</details>


### [118] [Physics-informed deep operator network for traffic state estimation](https://arxiv.org/abs/2508.12593)
*Zhihao Li,Ting Wang,Guojian Zou,Ruofei Wang,Ye Li*

Main category: cs.LG

TL;DR: PI-DeepONet framework reformulates traffic state estimation as operator learning, integrating traffic flow physics into neural operator training for superior performance over traditional PINNs.


<details>
  <summary>Details</summary>
Motivation: Traditional Physics-Informed Neural Networks (PINNs) enforce PDE constraints point-wise but struggle with high-dimensional spatiotemporal traffic flow problems. There's a need for a more effective approach that can handle limited, noisy measurements while ensuring physical consistency in traffic state estimation.

Method: Proposes physics-informed deep operator network (PI-DeepONet) that learns a parameterized neural operator mapping sparse input data to full spatiotemporal traffic state field. Integrates traffic flow conservation law and fundamental diagram directly into operator learning process.

Result: Superior performance over state-of-the-art baselines on NGSIM dataset. The framework effectively captures congestion propagation, spatial correlations, and temporal evolution while maintaining physical consistency. Analysis reveals optimal function generation strategies and branch network complexity.

Conclusion: PI-DeepONet provides a robust and effective framework for traffic state estimation that outperforms traditional PINNs by reformulating the problem as operator learning with integrated physical constraints, offering better handling of high-dimensional spatiotemporal PDEs.

Abstract: Traffic state estimation (TSE) fundamentally involves solving
high-dimensional spatiotemporal partial differential equations (PDEs) governing
traffic flow dynamics from limited, noisy measurements. While Physics-Informed
Neural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a
physics-informed deep operator network (PI-DeepONet) framework that
reformulates TSE as an operator learning problem. Our approach trains a
parameterized neural operator that maps sparse input data to the full
spatiotemporal traffic state field, governed by the traffic flow conservation
law. Crucially, unlike PINNs that enforce PDE constraints point-wise,
PI-DeepONet integrates traffic flow conservation model and the fundamental
diagram directly into the operator learning process, ensuring physical
consistency while capturing congestion propagation, spatial correlations, and
temporal evolution. Experiments on the NGSIM dataset demonstrate superior
performance over state-of-the-art baselines. Further analysis reveals insights
into optimal function generation strategies and branch network complexity.
Additionally, the impact of input function generation methods and the number of
functions on model performance is explored, highlighting the robustness and
efficacy of proposed framework.

</details>


### [119] [FLARE: Fast Low-rank Attention Routing Engine](https://arxiv.org/abs/2508.12594)
*Vedant Puri,Aditya Joglekar,Kevin Ferguson,Yu-hsuan Chen,Yongjie Jessica Zhang,Levent Burak Kara*

Main category: cs.LG

TL;DR: FLARE introduces a linear complexity self-attention mechanism that routes attention through fixed-length latent sequences, enabling efficient processing of large unstructured meshes while maintaining superior accuracy.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of traditional self-attention limits its scalability on large unstructured meshes, creating a need for more efficient attention mechanisms.

Method: FLARE uses learnable query tokens to project input sequences onto fixed-length latent sequences (M << N), routing attention through bottleneck sequences to achieve O(NM) complexity instead of O(N²).

Result: FLARE scales to unprecedented problem sizes and delivers superior accuracy compared to state-of-the-art neural PDE surrogates across diverse benchmarks.

Conclusion: FLARE provides an effective solution for scaling self-attention to large unstructured meshes with linear complexity while maintaining or improving accuracy, with potential applications in additive manufacturing and other domains.

Abstract: The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.

</details>


### [120] [Constructing Invariant and Equivariant Operations by Symmetric Tensor Network](https://arxiv.org/abs/2508.12596)
*Meng Zhang,Chao Wang,Hao Zhang,Shaojun Dong,Lixin He*

Main category: cs.LG

TL;DR: Systematic method for constructing invariant and equivariant operations for neural networks with symmetry, handling both Cartesian and spherical tensors with graphical tensor network representation.


<details>
  <summary>Details</summary>
Motivation: Design of neural networks that incorporate symmetry is crucial for geometric deep learning, requiring development of invariant and equivariant operations.

Method: Systematic method for constructing valid invariant and equivariant operations using symmetric tensor network graphical representation, handling inputs/outputs as Cartesian tensors with different rank and spherical tensors with different types.

Result: Method simplifies proofs and constructions related to invariant and equivariant functions, applied to design equivariant interaction message for geometry graph neural network and equivariant ML model for material constitutive law learning.

Conclusion: The approach provides a systematic framework for building symmetry-preserving operations in geometric deep learning with practical applications in graph neural networks and material science.

Abstract: Design of neural networks that incorporate symmetry is crucial for geometric
deep learning. Central to this effort is the development of invariant and
equivariant operations. This works presents a systematic method for
constructing valid invariant and equivariant operations. It can handle inputs
and outputs in the form of Cartesian tensors with different rank, as well as
spherical tensors with different types. In addition, our method features a
graphical representation utilizing the symmetric tensor network, which
simplifies both the proofs and constructions related to invariant and
equivariant functions. We also apply this approach to design the equivariant
interaction message for the geometry graph neural network, and equivariant
machine learning model to learn the constitutive law of materials.

</details>


### [121] [A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators](https://arxiv.org/abs/2508.12602)
*Hansol Lim,Jongseong Brad Choi,Jee Won Lee,Haeseong Jeoung,Minkyu Han*

Main category: cs.LG

TL;DR: Hybrid surrogate model combining Fourier Neural Operator and differentiable physics for EV parameter estimation from speed/acceleration data, achieving high accuracy on real-world Tesla and Kia vehicle data.


<details>
  <summary>Details</summary>
Motivation: To develop an interpretable and accurate method for estimating electric vehicle parameters and power consumption from minimal sensor data (speed and acceleration alone) for applications like path optimization and diagnostics.

Method: Combines Spectral Parameter Operator (Fourier Neural Operator backbone) for global context with differentiable physics module. Outputs time-varying motor/braking efficiencies, drag, rolling resistance, mass, and auxiliary power, then uses physics-embedded battery power estimation.

Result: Achieves mean absolute error of 0.2kW (~1% of average traction power) for Tesla vehicles and 0.8kW for Kia EV9. Generalizes well to unseen conditions and sampling rates.

Conclusion: The framework provides interpretable, physically meaningful parameters that reflect vehicle state/condition, making it practical for various EV applications including eco-routing and health management.

Abstract: We present a hybrid surrogate model for electric vehicle parameter estimation
and power consumption. We combine our novel architecture Spectral Parameter
Operator built on a Fourier Neural Operator backbone for global context and a
differentiable physics module in the forward pass. From speed and acceleration
alone, it outputs time-varying motor and regenerative braking efficiencies, as
well as aerodynamic drag, rolling resistance, effective mass, and auxiliary
power. These parameters drive a physics-embedded estimate of battery power,
eliminating any separate physics-residual loss. The modular design lets
representations converge to physically meaningful parameters that reflect the
current state and condition of the vehicle. We evaluate on real-world logs from
a Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean
absolute error of 0.2kW (about 1% of average traction power at highway speeds)
for Tesla vehicles and about 0.8kW on the Kia EV9. The framework is
interpretable, and it generalizes well to unseen conditions, and sampling
rates, making it practical for path optimization, eco-routing, on-board
diagnostics, and prognostics health management.

</details>


### [122] [SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression](https://arxiv.org/abs/2508.12604)
*Yuyang Xu,Yi Cheng,Haochao Ying,Zhuoyun Du,Renjun Hu,Xing Shi,Wei Lin,Jian Wu*

Main category: cs.LG

TL;DR: SSPO is a pluggable RL framework that uses self-generated step-wise preference signals to optimize reasoning steps, reducing overthinking and improving efficiency without auxiliary models or manual annotations.


<details>
  <summary>Details</summary>
Motivation: Mainstream post-training methods for LLMs incur substantial computational overhead due to auxiliary models and overthinking, with incorrect answers often stemming from verbose reasoning processes lacking correct self-fix.

Method: Self-traced Step-wise Preference Optimization (SSPO) - a pluggable RL process supervision framework that leverages step-wise preference signals generated by the model itself to guide optimization for reasoning compression, requiring no auxiliary models or stepwise manual annotations.

Result: Experiments show SSPO generates accurate and succinct reasoning sequences, effectively mitigating overthinking behaviors without compromising model performance across diverse domains and languages.

Conclusion: SSPO provides an efficient solution for optimizing reasoning steps in LLMs through self-generated preference signals, eliminating the need for external resources while maintaining performance.

Abstract: Test-time scaling has proven effective in further enhancing the performance
of pretrained Large Language Models (LLMs). However, mainstream post-training
methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)
reasoning) often incur substantial computational overhead due to auxiliary
models and overthinking. In this paper, we empirically reveal that the
incorrect answers partially stem from verbose reasoning processes lacking
correct self-fix, where errors accumulate across multiple reasoning steps. To
this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a
pluggable RL process supervision framework that enables fine-grained
optimization of each reasoning step. Specifically, SSPO requires neither
auxiliary models nor stepwise manual annotations. Instead, it leverages
step-wise preference signals generated by the model itself to guide the
optimization process for reasoning compression. Experiments demonstrate that
the generated reasoning sequences from SSPO are both accurate and succinct,
effectively mitigating overthinking behaviors without compromising model
performance across diverse domains and languages.

</details>


### [123] [How can we trust opaque systems? Criteria for robust explanations in XAI](https://arxiv.org/abs/2508.12623)
*Florian J. Boge,Annika Schuster*

Main category: cs.LG

TL;DR: The paper addresses the opacity of deep learning algorithms and proposes robustness criteria (ER and EMR) for trustworthy XAI methods to ensure explanations reflect actual decision processes.


<details>
  <summary>Details</summary>
Motivation: Deep learning algorithms are highly accurate but notoriously opaque, making it difficult to understand their decision-making processes. Current XAI methods may produce unreliable explanations, undermining trust in AI systems.

Method: The authors develop and formalize two robustness criteria: Explanatory Robustness (ER) requiring different XAI methods produce consistent explanations, and Explanation Method Robustness (EMR) ensuring individual methods produce correct explanations. They provide a framework for evaluating XAI trustworthiness.

Result: The paper establishes formal criteria for evaluating XAI method trustworthiness, highlighting that robustness of individual methods alone is insufficient and that both ER and EMR are necessary for reliable explanations.

Conclusion: A comprehensive framework combining both explanatory robustness and explanation method robustness is essential for building trustworthy explanations in deep learning systems, with implications for future XAI research and applications.

Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in
scientific research. However, the price we pay for their impressively accurate
predictions is significant: their inner workings are notoriously opaque - it is
unknown to laypeople and researchers alike what features of the data a DL
system focuses on and how it ultimately succeeds in predicting correct outputs.
A necessary criterion for trustworthy explanations is that they should reflect
the relevant processes the algorithms' predictions are based on. The field of
eXplainable Artificial Intelligence (XAI) presents promising methods to create
such explanations. But recent reviews about their performance offer reasons for
skepticism. As we will argue, a good criterion for trustworthiness is
explanatory robustness: different XAI methods produce the same explanations in
comparable contexts. However, in some instances, all methods may give the same,
but still wrong, explanation. We therefore argue that in addition to
explanatory robustness (ER), a prior requirement of explanation method
robustness (EMR) has to be fulfilled by every XAI method. Conversely, the
robustness of an individual method is in itself insufficient for
trustworthiness. In what follows, we develop and formalize criteria for ER as
well as EMR, providing a framework for explaining and establishing trust in DL
algorithms. We also highlight interesting application cases and outline
directions for future work.

</details>


### [124] [FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation](https://arxiv.org/abs/2508.12629)
*Ian Dunn,David R. Koes*

Main category: cs.LG

TL;DR: FlowMol3 is a multi-modal flow matching model that achieves state-of-the-art performance in all-atom small-molecule generation through three simple, architecture-agnostic techniques: self-conditioning, fake atoms, and train-time geometry distortion.


<details>
  <summary>Details</summary>
Motivation: To accelerate chemical discovery by developing a generative model that can sample realistic molecules with desired properties, particularly focusing on jointly sampling molecular topology and 3D structure.

Method: FlowMol3 uses flow matching with three novel techniques: self-conditioning, fake atoms, and train-time geometry distortion. These are applied without changing the graph neural network architecture or underlying flow matching formulation.

Result: Achieves nearly 100% molecular validity for drug-like molecules with explicit hydrogens, accurately reproduces functional group composition and geometry of training data, and uses an order of magnitude fewer parameters than comparable methods.

Conclusion: The three simple techniques mitigate distribution drift during inference and provide transferable strategies for improving stability and quality of diffusion- and flow-based molecular generative models.

Abstract: A generative model capable of sampling realistic molecules with desired
properties could accelerate chemical discovery across a wide range of
applications. Toward this goal, significant effort has focused on developing
models that jointly sample molecular topology and 3D structure. We present
FlowMol3, an open-source, multi-modal flow matching model that advances the
state of the art for all-atom, small-molecule generation. Its substantial
performance gains over previous FlowMol versions are achieved without changes
to the graph neural network architecture or the underlying flow matching
formulation. Instead, FlowMol3's improvements arise from three
architecture-agnostic techniques that incur negligible computational cost:
self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3
achieves nearly 100% molecular validity for drug-like molecules with explicit
hydrogens, more accurately reproduces the functional group composition and
geometry of its training data, and does so with an order of magnitude fewer
learnable parameters than comparable methods. We hypothesize that these
techniques mitigate a general pathology affecting transport-based generative
models, enabling detection and correction of distribution drift during
inference. Our results highlight simple, transferable strategies for improving
the stability and quality of diffusion- and flow-based molecular generative
models.

</details>


### [125] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: SciNO is a new neural operator model that provides stable Hessian diagonal estimation for causal discovery, outperforming previous methods while being memory-efficient and enabling better causal reasoning in LLMs without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing causal ordering methods using Stein gradient estimators are computationally expensive and memory-intensive, while DiffAN suffers from numerical instability due to second-order derivatives of score models.

Method: Score-informed Neural Operator (SciNO) - a probabilistic generative model in smooth function spaces designed to stably approximate Hessian diagonal while preserving structural information during score modeling.

Result: SciNO reduces order divergence by 42.7% on synthetic graphs and 31.5% on real-world datasets compared to DiffAN, while maintaining memory efficiency and scalability. Also enables reliable causal reasoning with LLMs without additional fine-tuning.

Conclusion: SciNO provides a stable and efficient solution for Hessian diagonal estimation in causal discovery, significantly improving ordering accuracy and enabling enhanced causal reasoning capabilities in autoregressive models.

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. However, previous approaches mainly use Stein gradient
estimators, which are computationally expensive and memory-intensive. Although
DiffAN addresses these limitations by substituting kernel-based estimates with
diffusion models, it remains numerically unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [126] [Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering](https://arxiv.org/abs/2508.12672)
*Emmanouil Kritharakis,Dusan Jakovetic,Antonios Makris,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: A robust federated learning method that requires only one honest client and a trusted server with side data to defend against Byzantine attacks, outperforming existing baselines across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to adversarial (Byzantine) attacks from malicious clients, and existing robust aggregation methods often require prior knowledge about the number of malicious participants or don't leverage server-side trusted data.

Method: Proposes a Byzantine-robust FL approach that uses a trusted server with a trustworthy side dataset and requires only one honest client to function effectively, without needing prior knowledge of the number of malicious clients.

Result: Theoretical analysis shows bounded optimality gaps under strong Byzantine attacks. Experimental results demonstrate significant outperformance over standard (Mean) and robust baselines (Trimmed Mean, Median, Krum, Multi-Krum) against various attack strategies (label flipping, sign flipping, Gaussian noise) on MNIST, FMNIST, and CIFAR-10 using Flower framework.

Conclusion: The proposed method provides effective Byzantine robustness in FL with minimal trust assumptions (only server and one honest client needed) and no prior knowledge requirement about malicious clients, making it practical for real-world deployment.

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing private data. We consider FL scenarios wherein FL
clients are subject to adversarial (Byzantine) attacks, while the FL server is
trusted (honest) and has a trustworthy side dataset. This may correspond to,
e.g., cases where the server possesses trusted data prior to federation, or to
the presence of a trusted client that temporarily assumes the server role. Our
approach requires only two honest participants, i.e., the server and one
client, to function effectively, without prior knowledge of the number of
malicious clients. Theoretical analysis demonstrates bounded optimality gaps
even under strong Byzantine attacks. Experimental results show that our
algorithm significantly outperforms standard and robust FL baselines such as
Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack
strategies including label flipping, sign flipping, and Gaussian noise addition
across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.

</details>


### [127] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperFedZero is a federated learning method that uses a hypernetwork with distribution-aware embeddings to generate specialized models for non-participating clients with data heterogeneity and resource constraints.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods fail to generalize to non-participating clients with in-domain distribution shifts and resource constraints, creating a need for more adaptive solutions.

Method: Dynamically generates specialized models via hypernetwork conditioned on distribution-aware embeddings using NoisyEmbed-enhanced extractor with Balancing Penalty to prevent feature collapse.

Result: Outperforms competing methods consistently across multiple datasets and models with minimal computational, storage, and communication overhead.

Conclusion: HyperFedZero effectively addresses data heterogeneity in FL by generating adaptable models for non-participating clients through distribution-aware hypernetworks.

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [128] [BUILDA: A Thermal Building Data Generation Framework for Transfer Learning](https://arxiv.org/abs/2508.12703)
*Thomas Krug,Fabian Raisch,Dominik Aimer,Markus Wirnsberger,Ferdinand Sigg,Benjamin Schäfer,Benjamin Tischler*

Main category: cs.LG

TL;DR: BuilDa is a thermal building data generation framework that produces synthetic data for transfer learning research without requiring expert building simulation knowledge, addressing the current lack of adequate thermal building data.


<details>
  <summary>Details</summary>
Motivation: Transfer learning can improve building thermal dynamics modeling but requires massive amounts of quality data that is currently unavailable. Existing data generators need expert simulation knowledge and don't meet TL research needs.

Method: BuilDa uses a single-zone Modelica model exported as a Functional Mock-up Unit (FMU) and simulated in Python to generate large volumes of synthetic thermal building data without requiring profound building simulation expertise.

Result: The framework successfully generates data of adequate quality and quantity for transfer learning research, demonstrated through pretraining and fine-tuning TL models.

Conclusion: BuilDa provides a practical solution to the data scarcity problem in transfer learning research for building thermal dynamics, enabling TL research without the need for expert simulation knowledge or massive real-world datasets.

Abstract: Transfer learning (TL) can improve data-driven modeling of building thermal
dynamics. Therefore, many new TL research areas emerge in the field, such as
selecting the right source model for TL. However, these research directions
require massive amounts of thermal building data which is lacking presently.
Neither public datasets nor existing data generators meet the needs of TL
research in terms of data quality and quantity. Moreover, existing data
generation approaches typically require expert knowledge in building
simulation. We present BuilDa, a thermal building data generation framework for
producing synthetic data of adequate quality and quantity for TL research. The
framework does not require profound building simulation knowledge to generate
large volumes of data. BuilDa uses a single-zone Modelica model that is
exported as a Functional Mock-up Unit (FMU) and simulated in Python. We
demonstrate BuilDa by generating data and utilizing it for pretraining and
fine-tuning TL models.

</details>


### [129] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: Federated learning framework for traffic sign detection in vehicular networks that enables collaborative training without sharing raw data, achieving up to 0.83 accuracy while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy and communication challenges from vast sensor data generated by connected/automated vehicles, avoiding centralized machine learning approaches that require sharing raw data.

Method: Decentralized federated learning with specialized local training using lightweight object detectors, parameter aggregation via FedProx/FedAdam/FedAVG algorithms in Flower framework simulation, testing various configurations (server rounds, local epochs, client participation, data distributions).

Result: Increasing server rounds (2→20) boosted accuracy from <0.1 to >0.8; moderate local epochs (8-10) achieved ~0.67 accuracy; higher client participation reached 0.83 accuracy; FedProx outperformed other aggregators; non-IID data reduced performance; training duration scaled with rounds not aggregation strategy.

Conclusion: Federated learning offers scalable, privacy-preserving solution for vehicular deployments, with potential for future integration of robust aggregation and communication optimizations in intelligent transportation systems.

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [130] [FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment](https://arxiv.org/abs/2508.12727)
*Manning Zhu,Songtao Guo,Pengzhan Zhou,Yansong Ning,Chang Han,Dewen Qiao*

Main category: cs.LG

TL;DR: FedSODA is a resource-efficient federated fine-tuning framework that enables clients to adapt LLMs without full model access through similarity group pruning and orchestrated distillation alignment, significantly reducing resource requirements while improving performance.


<details>
  <summary>Details</summary>
Motivation: Federated fine-tuning of LLMs faces high computational and memory demands on resource-constrained clients, limiting practical deployment and advancement.

Method: Proposes similarity group pruning to remove redundant layers while retaining critical ones, and orchestrated distillation alignment to reduce gradient divergence. Uses QLoRA for quantized sub-LLMs and lightweight adapter fine-tuning.

Result: Reduces communication overhead by 70.6%, decreases storage usage by 75.6%, and improves task accuracy by 3.1% across various downstream tasks with three open-source LLMs.

Conclusion: FedSODA is highly suitable for practical federated fine-tuning applications under resource constraints, offering significant efficiency improvements while maintaining model performance.

Abstract: Federated fine-tuning (FFT) of large language models (LLMs) has recently
emerged as a promising solution to enable domain-specific adaptation while
preserving data privacy. Despite its benefits, FFT on resource-constrained
clients relies on the high computational and memory demands of full-model
fine-tuning, which limits the potential advancement. This paper presents
FedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs
without accessing or storing the full model. Specifically, we first propose a
similarity group pruning (SGP) module, which prunes redundant layers from the
full LLM while retaining the most critical layers to preserve the model
performance. Moreover, we introduce an orchestrated distillation alignment
(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM
during FFT. Through the use of the QLoRA, clients only need to deploy quantized
sub-LLMs and fine-tune lightweight adapters, significantly reducing local
resource requirements. We conduct extensive experiments on three open-source
LLMs across a variety of downstream tasks. The experimental results demonstrate
that FedSODA reduces communication overhead by an average of 70.6%, decreases
storage usage by 75.6%, and improves task accuracy by 3.1%, making it highly
suitable for practical FFT applications under resource constraints.

</details>


### [131] [FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models](https://arxiv.org/abs/2508.12740)
*Beomseok Seo,Kichang Lee,JaeYeon Park*

Main category: cs.LG

TL;DR: FedUNet is a lightweight federated learning framework that uses U-Net-inspired additive modules to enable knowledge transfer across heterogeneous client architectures with minimal communication overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning assumes identical model architectures across clients, which limits real-world applicability in heterogeneous environments where clients have different hardware capabilities and model architectures.

Method: Attaches a U-Net-inspired additive module to each client's backbone, sharing only the compact bottleneck of the U-Net. Uses encoder-decoder design with skip connections to capture both low-level and high-level features for client-invariant representations.

Result: Achieves 93.11% accuracy with VGG variants and 92.68% in compact form, with only 0.89 MB communication overhead.

Conclusion: FedUNet enables efficient federated learning across heterogeneous architectures with minimal communication cost while maintaining high accuracy.

Abstract: Federated learning (FL) enables decentralized model training without sharing
local data. However, most existing methods assume identical model architectures
across clients, limiting their applicability in heterogeneous real-world
environments. To address this, we propose FedUNet, a lightweight and
architecture-agnostic FL framework that attaches a U-Net-inspired additive
module to each client's backbone. By sharing only the compact bottleneck of the
U-Net, FedUNet enables efficient knowledge transfer without structural
alignment. The encoder-decoder design and skip connections in the U-Net help
capture both low-level and high-level features, facilitating the extraction of
clientinvariant representations. This enables cooperative learning between the
backbone and the additive module with minimal communication cost. Experiment
with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in
compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low
communication overhead.

</details>


### [132] [A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks](https://arxiv.org/abs/2508.12741)
*Manuela Imbriani,Gina Belmonte,Mieke Massink,Alessandro Tofani,Vincenzo Ciancia*

Main category: cs.LG

TL;DR: A benchmark framework for evaluating neural network spatial reasoning capabilities using synthetic datasets and spatial model checking, revealing systematic failures in geometric and topological understanding.


<details>
  <summary>Details</summary>
Motivation: To systematically assess neural networks' spatial reasoning capabilities, particularly morphological properties like connectivity and distance relationships, which are crucial for clinical applications.

Method: Uses VoxLogicA spatial model checker to generate synthetic datasets (maze connectivity problems and spatial distance tasks) across multiple resolutions, with automated pipeline including dataset generation, standardized training, cross-validation, inference, and evaluation using Dice and IoU metrics.

Result: Preliminary results show significant challenges and systematic failures in neural networks' basic geometric and topological understanding tasks.

Conclusion: The framework provides reproducible protocol to identify limitations, suggesting hybrid neural-symbolic approaches could address these spatial reasoning deficiencies for clinical applications.

Abstract: This paper presents preliminary results in the definition of a comprehensive
benchmark framework designed to systematically evaluate spatial reasoning
capabilities in neural networks, with a particular focus on morphological
properties such as connectivity and distance relationships. The framework is
currently being used to study the capabilities of nnU-Net, exploiting the
spatial model checker VoxLogicA to generate two distinct categories of
synthetic datasets: maze connectivity problems for topological analysis and
spatial distance computation tasks for geometric understanding. Each category
is evaluated across multiple resolutions to assess scalability and
generalization properties. The automated pipeline encompasses a complete
machine learning workflow including: synthetic dataset generation, standardized
training with cross-validation, inference execution, and comprehensive
evaluation using Dice coefficient and IoU (Intersection over Union) metrics.
Preliminary experimental results demonstrate significant challenges in neural
network spatial reasoning capabilities, revealing systematic failures in basic
geometric and topological understanding tasks. The framework provides a
reproducible experimental protocol, enabling researchers to identify specific
limitations. Such limitations could be addressed through hybrid approaches
combining neural networks with symbolic reasoning methods for improved spatial
understanding in clinical applications, establishing a foundation for ongoing
research into neural network spatial reasoning limitations and potential
solutions.

</details>


### [133] [Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning](https://arxiv.org/abs/2508.12758)
*Sowmini Devi Veeramachaneni,Ramamurthy Garimella*

Main category: cs.LG

TL;DR: Constrained Centroid Clustering (CCC) extends centroid-based clustering with a maximum distance constraint, using Lagrangian formulation for closed-form solution that controls cluster spread while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the need for structured clustering with controlled spread in applications like sensor networks, collaborative robotics, and interpretable pattern analysis, where standard methods may produce clusters with undesirable spread.

Method: Lagrangian formulation to derive closed-form solution enforcing maximum distance constraint between cluster center and farthest point, evaluated on synthetic circular data using ring-wise, sector-wise, and joint entropy metrics.

Result: CCC achieves more compact clusters by reducing radial spread while preserving angular structure, outperforming standard K-means and GMM methods.

Conclusion: The proposed CCC method provides effective spread-controlled clustering suitable for applications requiring structured clustering with interpretable results.

Abstract: This paper presents Constrained Centroid Clustering (CCC), a method that
extends classical centroid-based clustering by enforcing a constraint on the
maximum distance between the cluster center and the farthest point in the
cluster. Using a Lagrangian formulation, we derive a closed-form solution that
maintains interpretability while controlling cluster spread. To evaluate CCC,
we conduct experiments on synthetic circular data with radial symmetry and
uniform angular distribution. Using ring-wise, sector-wise, and joint entropy
as evaluation metrics, we show that CCC achieves more compact clusters by
reducing radial spread while preserving angular structure, outperforming
standard methods such as K-means and GMM. The proposed approach is suitable for
applications requiring structured clustering with spread control, including
sensor networks, collaborative robotics, and interpretable pattern analysis.

</details>


### [134] [Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach](https://arxiv.org/abs/2508.12764)
*Cyril Voyant,Milan Despotovic,Luis Garcia-Gutierrez,Mohammed Asloune,Yves-Marie Saint-Drenan,Jean-Laurent Duchaud,hjuvan Antone Faggianelli,Elena Magliaro*

Main category: cs.LG

TL;DR: Novel ELM-based MIMO approach for short-term energy forecasting outperforms persistence models, achieving high accuracy (nRMSE 17.9% solar, 5.1% thermal) with R² > 0.98 for 1-hour predictions and maintains performance up to 5 hours.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient short-term energy forecasting method that can handle multiple energy sources and adapt to non-stationarity and seasonal variability in energy production data.

Method: Extreme Learning Machine (ELM) with Multi-Input Multi-Output (MIMO) architecture, using sliding window techniques and cyclic time encoding to address non-stationarity and seasonal patterns. Trained on 6 years of hourly data from multiple energy sources.

Result: Significantly outperforms persistence-based forecasting, particularly for solar (17.9% nRMSE) and thermal energy (5.1% nRMSE) with R² > 0.98 for 1-hour horizon. Maintains high accuracy up to 5 hours ahead. MIMO provides marginal gains over SISO with lower computational demands.

Conclusion: The ELM-based MIMO approach offers an efficient, computationally light solution for real-time energy forecasting that adapts well to local constraints and maintains high predictive accuracy for short-term horizons, making it suitable for practical applications.

Abstract: A novel methodology for short-term energy forecasting using an Extreme
Learning Machine ($\mathtt{ELM}$) is proposed. Using six years of hourly data
collected in Corsica (France) from multiple energy sources (solar, wind, hydro,
thermal, bioenergy, and imported electricity), our approach predicts both
individual energy outputs and total production (\cyr{including imports, which
closely follow energy demand, modulo losses)} through a Multi-Input
Multi-Output ($\mathtt{MIMO}$) architecture. To address non-stationarity and
seasonal variability, sliding window techniques and cyclic time encoding are
incorporated, enabling dynamic adaptation to fluctuations. The $\mathtt{ELM}$
model significantly outperforms persistence-based forecasting, particularly for
solar and thermal energy, achieving an $\mathtt{nRMSE}$ of $17.9\%$ and
$5.1\%$, respectively, with $\mathtt{R^2} > 0.98$ (1-hour horizon). The model
maintains high accuracy up to five hours ahead, beyond which renewable energy
sources become increasingly volatile. While $\mathtt{MIMO}$ provides marginal
gains over Single-Input Single-Output ($\mathtt{SISO}$) architectures and
offers key advantages over deep learning methods such as $\mathtt{LSTM}$, it
provides a closed-form solution with lower computational demands, making it
well-suited for real-time applications, including online learning. Beyond
predictive accuracy, the proposed methodology is adaptable to various contexts
and datasets, as it can be tuned to local constraints such as resource
availability, grid characteristics, and market structures.

</details>


### [135] [Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling](https://arxiv.org/abs/2508.12773)
*Jiadong Chen,Xiao He,Hengyu Ye,Fuxin Jiang,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: E3Former is a novel online ensemble model for workload forecasting in serverless systems that reduces forecast error by 10% and enables 40% resource utilization reduction while supporting over 600,000 CPU cores at ByteDance.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting models struggle to adapt to dynamic online workload streams and capture complex periodicity in fine-grained, high-frequency forecasting tasks needed for predictive auto-scaling in serverless environments.

Method: Proposes E3Former, an online ensemble model that synergizes multiple subnetworks to overcome single-model limitations, ensuring superior accuracy and robustness with minimal computational overhead.

Result: Reduces forecast error by average of 10% in online forecasting tasks, deployed in ByteDance's IHPA platform supporting 30+ applications and 600,000+ CPU cores, achieving over 40% resource utilization reduction while maintaining service quality.

Conclusion: E3Former demonstrates effective online workload forecasting for large-scale predictive auto-scaling, providing significant resource efficiency improvements while maintaining operational stability in production serverless environments.

Abstract: In the swiftly evolving domain of cloud computing, the advent of serverless
systems underscores the crucial need for predictive auto-scaling systems. This
necessity arises to ensure optimal resource allocation and maintain operational
efficiency in inherently volatile environments. At the core of a predictive
auto-scaling system is the workload forecasting model. Existing forecasting
models struggle to quickly adapt to the dynamics in online workload streams and
have difficulty capturing the complex periodicity brought by fine-grained,
high-frequency forecasting tasks. Addressing this, we propose a novel online
ensemble model, E3Former, for online workload forecasting in large-scale
predictive auto-scaling. Our model synergizes the predictive capabilities of
multiple subnetworks to surmount the limitations of single-model approaches,
thus ensuring superior accuracy and robustness. Remarkably, it accomplishes
this with a minimal increase in computational overhead, adhering to the lean
operational ethos of serverless systems. Through extensive experimentation on
real-world workload datasets, we establish the efficacy of our ensemble model.
In online forecasting tasks, the proposed method reduces forecast error by an
average of 10%, and its effectiveness is further demonstrated through a
predictive auto-scaling test in the real-life online system. Currently, our
method has been deployed within ByteDance's Intelligent Horizontal Pod
Auto-scaling (IHPA) platform, which supports the stable operation of over 30
applications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The
predictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis
of essentially ensuring service quality, the predictive auto-scaling system can
reduce resource utilization by over 40%.

</details>


### [136] [Randomized PCA Forest for Outlier Detection](https://arxiv.org/abs/2508.12776)
*Muhammad Rajabinasab,Farhad Pakdaman,Moncef Gabbouj,Peter Schneider-Kamp,Arthur Zimek*

Main category: cs.LG

TL;DR: Novel unsupervised outlier detection method using Randomized PCA Forest that outperforms classical and state-of-the-art methods on multiple datasets with high generalization and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Inspired by the success of Randomized PCA Forest in approximate KNN search, the authors aim to develop an effective unsupervised outlier detection method that leverages this approach.

Method: Utilizes Randomized Principal Component Analysis (PCA) Forest for outlier detection in an unsupervised manner, building on the RPCA Forest architecture previously used for approximate nearest neighbor search.

Result: Experimental results show superiority over classical and state-of-the-art methods on several datasets, with competitive performance on others. The method demonstrates high generalization power and computational efficiency.

Conclusion: The proposed Randomized PCA Forest-based approach is presented as an excellent choice for unsupervised outlier detection due to its strong performance, generalization capabilities, and computational efficiency.

Abstract: We propose a novel unsupervised outlier detection method based on Randomized
Principal Component Analysis (PCA). Inspired by the performance of Randomized
PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a
novel unsupervised outlier detection method that utilizes RPCA Forest for
outlier detection. Experimental results showcase the superiority of the
proposed approach compared to the classical and state-of-the-art methods in
performing the outlier detection task on several datasets while performing
competitively on the rest. The extensive analysis of the proposed method
reflects it high generalization power and its computational efficiency,
highlighting it as a good choice for unsupervised outlier detection.

</details>


### [137] [Wavy Transformer](https://arxiv.org/abs/2508.12787)
*Satoshi Noguchi,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Wavy Transformer addresses over-smoothing in deep transformers by modeling attention layers as graph neural diffusion and introducing second-order wavy dynamics to prevent token representation convergence.


<details>
  <summary>Details</summary>
Motivation: Deep transformer models suffer from over-smoothing where token representations become similar across layers, limiting model performance and depth scalability.

Method: Establishes equivalence between attention layers and graph neural diffusion, then proposes Wavy Transformer with second-order wavy dynamics attention, specialized FFN, and normalization to preserve physical state-velocity relationships.

Result: Consistently improves performance on various NLP and CV tasks with minimal additional parameters and no extra hyperparameter tuning required.

Conclusion: The physical interpretation of transformers as diffusion processes enables effective architectural improvements that mitigate over-smoothing while maintaining efficiency.

Abstract: Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.

</details>


### [138] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: Bridge is a statistical framework that bridges human and LLM evaluations by modeling systematic discrepancies through linear transformations of covariates, improving LLM-as-a-judge accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as judges but their assessments often diverge systematically from human judgments, creating a need for a principled framework to align these evaluations.

Method: A unified statistical framework that models latent human preference scores and LLM deviations as linear transformations of covariates capturing discrepancy sources, with an efficient fitting algorithm for statistical inference.

Result: Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings in accuracy, calibration, and KL divergence, while exposing systematic human-LLM gaps.

Conclusion: Bridge provides a simple and principled framework for refining LLM ratings and characterizing systematic discrepancies between human and LLM evaluations, improving the reliability of LLM-as-a-judge systems.

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [139] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: Paper reconciles contradictions between causal modeling claims and domain generalization benchmarks, advocating for nuanced theory of causality's role in generalization.


<details>
  <summary>Details</summary>
Motivation: Recent domain generalization benchmarks have challenged the promise that causal modeling leads to robust AI generalization, creating apparent contradictions in the literature.

Method: Revisits and analyzes claims from both causality and domain generalization literature to reconcile contradictions and develop a more nuanced theoretical framework.

Result: Provides a more comprehensive understanding of how causality actually contributes to generalization, moving beyond oversimplified claims.

Conclusion: Advocates for a more nuanced theory of the role of causality in generalization that accounts for recent empirical findings from domain generalization benchmarks.

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [140] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: MaxScore is a novel MoE routing method that uses minimum-cost maximum-flow optimization and SoftTopk operator to eliminate token dropping and improve hardware efficiency while maintaining load balancing.


<details>
  <summary>Details</summary>
Motivation: Traditional MoE networks suffer from token dropping when expert capacity is saturated and low hardware efficiency due to padding in underutilized experts, while removing capacity constraints compromises load balancing and computational efficiency.

Method: Proposes Maximum Score Routing (MaxScore) that models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator, resolving limitations of iterative rerouting and optimal transport formulations.

Result: Achieves lower training losses and higher evaluation scores at equivalent FLOPs compared to both constrained and unconstrained baselines.

Conclusion: MaxScore provides an effective solution to the fundamental limitations of traditional MoE routing methods, offering improved performance and efficiency without the drawbacks of token dropping or load balancing issues.

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [141] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: L2S (Learn-to-Steer) introduces input-specific steering vectors for multimodal LLMs to enable fine-grained control over model behavior, addressing limitations of static steering approaches.


<details>
  <summary>Details</summary>
Motivation: Existing steering techniques for MLLMs rely on single static vectors applied uniformly, which fails when desired behavior depends on the specific input context (e.g., safety responses vary based on query type).

Method: Proposes training a small auxiliary module to predict input-specific steering vectors using contrastive prompting, enabling dynamic, query-dependent behavior control.

Result: L2S reduces hallucinations and enforces safety in MLLMs more effectively than static steering baselines.

Conclusion: Input-specific steering through learned auxiliary modules provides superior fine-grained control over MLLM behavior compared to traditional static steering approaches.

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [142] [Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG](https://arxiv.org/abs/2508.12833)
*Kichang Lee,Songkuk Kim,JaeYeon Park,JeongGil Ko*

Main category: cs.LG

TL;DR: Empirical study showing naive compression/dropping strategies are suboptimal for storage-constrained ML, revealing sample-dependent compression sensitivity enables adaptive strategies.


<details>
  <summary>Details</summary>
Motivation: On-device ML faces storage limitations in continuous data collection scenarios, requiring better understanding of data quantity vs quality trade-offs through compression.

Method: Empirical study analyzing compression effects on data samples, comparing uniform dropping vs compression strategies to understand sample sensitivity to compression.

Result: Found that naive strategies (uniform dropping, one-size-fits-all compression) are suboptimal, and data samples exhibit varying sensitivities to compression.

Conclusion: Sample-wise adaptive compression is feasible and provides foundation for new storage-aware learning systems, with systematic characterization of this under-explored challenge.

Abstract: On-device machine learning is often constrained by limited storage,
particularly in continuous data collection scenarios. This paper presents an
empirical study on storage-aware learning, focusing on the trade-off between
data quantity and quality via compression. We demonstrate that naive
strategies, such as uniform data dropping or one-size-fits-all compression, are
suboptimal. Our findings further reveal that data samples exhibit varying
sensitivities to compression, supporting the feasibility of a sample-wise
adaptive compression strategy. These insights provide a foundation for
developing a new class of storage-aware learning systems. The primary
contribution of this work is the systematic characterization of this
under-explored challenge, offering valuable insights that advance the
understanding of storage-aware learning.

</details>


### [143] [Learning In-context $\pmb{n}$-grams with Transformers: Sub-$\pmb{n}$-grams Are Near-stationary Points](https://arxiv.org/abs/2508.12837)
*Aditya Varre,Gizem Yüce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: Transformer models trained on in-context next-token prediction exhibit stage-wise learning with sub-n-gram solutions serving as near-stationary points in the loss landscape.


<details>
  <summary>Details</summary>
Motivation: Empirical observations show prolonged plateaus and stage-wise progression during transformer training, motivating investigation into the loss landscape structure for in-context learning tasks.

Method: Analyze learning of in-context n-gram language models under cross-entropy loss, establish sufficient conditions for stationary points, construct parameter configurations for simplified transformers representing k-gram estimators, and examine gradient behavior in infinite sequence length limit.

Result: Sub-n-grams are near-stationary points of population cross-entropy loss, with gradient vanishing in infinite sequence length limit, explaining stage-wise learning dynamics and emergent phase transitions.

Conclusion: Theoretical analysis reveals structural properties of transformer loss landscapes that explain empirical observations of stage-wise progression and discrete transitions between near-stationary solutions during training.

Abstract: Motivated by empirical observations of prolonged plateaus and stage-wise
progression during training, we investigate the loss landscape of transformer
models trained on in-context next-token prediction tasks. In particular, we
focus on learning in-context $n$-gram language models under cross-entropy loss,
and establish a sufficient condition for parameter configurations to be
stationary points. We then construct a set of parameter configurations for a
simplified transformer model that represent $k$-gram estimators (for $k \leq
n$), and show that the gradient of the population loss at these solutions
vanishes in the limit of infinite sequence length and parameter norm. This
reveals a key property of the loss landscape: {sub-$n$-grams are
near-stationary points of the population cross-entropy loss}, offering
theoretical insight into widely observed phenomena such as stage-wise learning
dynamics and emergent phase transitions. These insights are further supported
by numerical experiments that illustrate the learning dynamics of $n$-grams,
characterized by discrete transitions between near-stationary solutions.

</details>


### [144] [HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2508.12839)
*Tiancheng Zhang,Cheng Zhang,Shuren Liu,Xiaofei Wang,Shaoyuan Huang,Wenyu Wang*

Main category: cs.LG

TL;DR: HRS framework combines numerical and image representations for better load forecasting in cloud-edge platforms, using scheduling-aware loss to reduce SLA violations by 63.1% and profit loss by 32.3%.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of accurate load forecasting under traffic surges in streaming services, where existing methods either cause SLA violations during peaks or waste resources through overprovisioning.

Method: Hybrid representation framework (HRS) integrating numerical and image-based representations to capture extreme load dynamics, with a Scheduling-Aware Loss (SAL) that accounts for asymmetric impact of prediction errors.

Result: Outperforms ten baselines across four real-world datasets, achieving state-of-the-art performance with 63.1% reduction in SLA violation rates and 32.3% reduction in total profit loss.

Conclusion: HRS effectively balances QoS maintenance and resource efficiency in cloud-edge platforms by providing more accurate load predictions that better support scheduling decisions during traffic surges.

Abstract: With the rapid proliferation of streaming services, network load exhibits
highly time-varying and bursty behavior, posing serious challenges for
maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms
(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS
and profitability, accurate load forecasting remains challenging under traffic
surges. Existing methods either minimize mean absolute error, resulting in
underprovisioning and potential Service Level Agreement (SLA) violations during
peak periods, or adopt conservative overprovisioning strategies, which mitigate
SLA risks at the expense of increased resource expenditure. To address this
dilemma, we propose HRS, a hybrid representation framework with scheduling
awareness that integrates numerical and image-based representations to better
capture extreme load dynamics. We further introduce a Scheduling-Aware Loss
(SAL) that captures the asymmetric impact of prediction errors, guiding
predictions that better support scheduling decisions. Extensive experiments on
four real-world datasets demonstrate that HRS consistently outperforms ten
baselines and achieves state-of-the-art performance, reducing SLA violation
rates by 63.1% and total profit loss by 32.3%.

</details>


### [145] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: TGN-SVDD: A novel intrusion detection method combining dynamic graph modeling with deep anomaly detection to handle network security challenges including novel attack detection and temporal graph structures.


<details>
  <summary>Details</summary>
Motivation: Growing digitalization increases network security importance. Machine learning-based intrusion detection faces challenges in detecting novel/unseen network events and handling temporal graph structures in network communication data.

Method: Proposes TGN-SVDD method that builds upon modern dynamic graph modelling and deep anomaly detection techniques.

Result: Demonstrates superiority over several baselines for realistic intrusion detection data.

Conclusion: Suggests a more challenging variant of intrusion detection data and presents TGN-SVDD as an effective solution for modern network security challenges.

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [146] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ is a lightweight uncertainty monitoring system for TinyML that uses temporal consistency and streaming conformal calibration to provide risk scores without extra computation or online labels.


<details>
  <summary>Details</summary>
Motivation: To create a resource-efficient uncertainty monitoring solution for streaming TinyML applications on kilobyte-scale microcontrollers, addressing the limitations of existing methods like early exit and deep ensembles which are too computationally expensive.

Method: Single-pass, label-free approach that converts short-horizon temporal consistency from lightweight signals on posteriors and features into calibrated risk scores using O(W) ring buffer and O(1) per-step updates, with a streaming conformal layer for budgeted accept/abstain decisions.

Result: 50-60% smaller footprint and 30-45% faster than early exit/ensembles; 3-7 AUPRC point improvement in accuracy drop detection (up to 0.86 AUPRC at high severities); up to 0.92 AUROC for failure detection.

Conclusion: Temporal consistency combined with streaming conformal calibration provides a practical and resource-efficient foundation for on-device uncertainty monitoring in TinyML applications.

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [147] [SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy](https://arxiv.org/abs/2508.12906)
*Boran Zhao,Haiming Zhai,Zihang Yuan,Hetian Liu,Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: SparseMap is an evolution strategy-based framework that jointly optimizes mapping and sparse strategies for tensor accelerators, overcoming combinatorial explosion in design space to find superior solutions.


<details>
  <summary>Details</summary>
Motivation: Existing sparse tensor accelerators are manually designed for specific scenarios and lack comprehensive optimization of both mapping and sparse strategies, leading to suboptimal designs when scenarios change.

Method: Proposes SparseMap framework using enhanced genetic encoding and evolutionary operators to efficiently explore the vast design space (O(10^41) for certain workloads) that combines both mapping and sparse strategies.

Result: SparseMap consistently finds superior solutions compared to prior works and classical optimization methods like particle swarm optimization, reinforcement learning, and Monte Carlo tree search.

Conclusion: The unified framework successfully addresses the combinatorial explosion challenge in sparse tensor accelerator design space, demonstrating the effectiveness of evolution strategy-based optimization for this complex problem.

Abstract: The growing demand for sparse tensor algebra (SpTA) in machine learning and
big data has driven the development of various sparse tensor accelerators.
However, most existing manually designed accelerators are limited to specific
scenarios, and it's time-consuming and challenging to adjust a large number of
design factors when scenarios change. Therefore, automating the design of SpTA
accelerators is crucial. Nevertheless, previous works focus solely on either
mapping (i.e., tiling communication and computation in space and time) or
sparse strategy (i.e., bypassing zero elements for efficiency), leading to
suboptimal designs due to the lack of comprehensive consideration of both. A
unified framework that jointly optimizes both is urgently needed. However,
integrating mapping and sparse strategies leads to a combinatorial explosion in
the design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \times
64} \times Q_{64 \times 48} = Z_{32 \times 48}$). This vast search space
renders most conventional optimization methods (e.g., particle swarm
optimization, reinforcement learning and Monte Carlo tree search) inefficient.
To address this challenge, we propose an evolution strategy-based sparse tensor
accelerator optimization framework, called SparseMap. SparseMap constructing a
more comprehensive design space with the consideration of both mapping and
sparse strategy. We introduce a series of enhancements to genetic encoding and
evolutionary operators, enabling SparseMap to efficiently explore the vast and
diverse design space. We quantitatively compare SparseMap with prior works and
classical optimization methods, demonstrating that SparseMap consistently finds
superior solutions.

</details>


### [148] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ is a lightweight, single-pass uncertainty quantification method for TinyML that predicts next layer activations to estimate risk without additional buffers or repeated passes, achieving significant memory and latency savings.


<details>
  <summary>Details</summary>
Motivation: Traditional uncertainty quantification methods for TinyML are resource-intensive, requiring temporal buffers, auxiliary exits, or multiple forward passes, which are impractical for memory-constrained microcontrollers.

Method: Uses tiny int8 heads to forecast statistics of the next layer from compressed previous layer views, with a lightweight monotone mapper converting surprisal into actionable uncertainty scores.

Result: Reduces flash and latency by 40-60% and 25-35% respectively compared to early-exit and deep ensembles, maintains strong failure detection (AUROC ≈0.9), and improves accuracy-drop detection in corrupted streams.

Conclusion: Grounding uncertainty in layer-to-layer dynamics provides a practical, resource-efficient solution for on-device monitoring in TinyML applications.

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [149] [Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning](https://arxiv.org/abs/2508.12978)
*Yue Xia,Tayyebeh Jahani-Nezhad,Rawad Bitar*

Main category: cs.LG

TL;DR: Fed-DPRoC is a federated learning framework that combines differential privacy, Byzantine robustness, and communication efficiency through robust-compatible compression using Johnson-Lindenstrauss transform and robust averaging.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning approaches struggle to simultaneously achieve differential privacy, Byzantine robustness, and communication efficiency, creating a need for a unified framework that addresses all three challenges.

Method: Proposes RobAJoL method combining Johnson-Lindenstrauss transform for compression with robust averaging for aggregation, enabling users to compress differentially private updates while maintaining robustness.

Result: Theoretical proofs show compatibility of JL transform with robust averaging, preserving robustness guarantees while ensuring DP and reducing communication costs. Experiments on CIFAR-10 and Fashion MNIST demonstrate superior robustness and utility under Byzantine attacks.

Conclusion: Fed-DPRoC successfully integrates differential privacy, Byzantine robustness, and communication efficiency through robust-compatible compression, outperforming existing methods in both theoretical guarantees and practical performance.

Abstract: We propose Fed-DPRoC, a novel federated learning framework that
simultaneously ensures differential privacy (DP), Byzantine robustness, and
communication efficiency. We introduce the concept of robust-compatible
compression, which enables users to compress DP-protected updates while
maintaining the robustness of the aggregation rule. We instantiate our
framework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for
compression with robust averaging for robust aggregation. We theoretically
prove the compatibility of JL transform with robust averaging and show that
RobAJoL preserves robustness guarantees, ensures DP, and reduces communication
cost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims
and demonstrate that RobAJoL outperforms existing methods in terms of
robustness and utility under different Byzantine attacks.

</details>


### [150] [SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression](https://arxiv.org/abs/2508.12984)
*Zehang Lin,Zheng Lin,Miao Yang,Jianhao Huang,Yuxin Zhang,Zihan Fang,Xia Du,Zhe Chen,Shunzhi Zhu,Wei Ni*

Main category: cs.LG

TL;DR: SL-ACC is a communication-efficient split learning framework that reduces transmission bottlenecks by adaptively compressing smashed data using entropy-based channel importance identification and group-wise compression.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of neural networks creates deployment challenges on resource-constrained devices in distributed ML. Split learning helps but suffers from excessive smashed data transmission bottlenecks as device numbers grow.

Method: Proposes SL-ACC framework with two components: 1) Adaptive Channel Importance Identification (ACII) using Shannon entropy to identify channel contributions, and 2) Channel Grouping Compression (CGC) that groups channels by entropy and performs group-wise adaptive compression.

Result: Extensive experiments across various datasets show SL-ACC takes considerably less time to achieve target accuracy compared to state-of-the-art benchmarks.

Conclusion: The proposed SL-ACC framework effectively addresses communication bottlenecks in split learning by reducing transmission volume without compromising training accuracy, making distributed ML more efficient on resource-constrained devices.

Abstract: The increasing complexity of neural networks poses a significant barrier to
the deployment of distributed machine learning (ML) on resource-constrained
devices, such as federated learning (FL). Split learning (SL) offers a
promising solution by offloading the primary computing load from edge devices
to a server via model partitioning. However, as the number of participating
devices increases, the transmission of excessive smashed data (i.e.,
activations and gradients) becomes a major bottleneck for SL, slowing down the
model training. To tackle this challenge, we propose a communication-efficient
SL framework, named SL-ACC, which comprises two key components: adaptive
channel importance identification (ACII) and channel grouping compression
(CGC). ACII first identifies the contribution of each channel in the smashed
data to model training using Shannon entropy. Following this, CGC groups the
channels based on their entropy and performs group-wise adaptive compression to
shrink the transmission volume without compromising training accuracy.
Extensive experiments across various datasets validate that our proposed SL-ACC
framework takes considerably less time to achieve a target accuracy than
state-of-the-art benchmarks.

</details>


### [151] [Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian](https://arxiv.org/abs/2508.12993)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: The Fiedler value (algebraic connectivity) predicts GCN performance - graphs with similar Fiedler values have analogous structural properties, enabling better transfer learning and hyperparameter selection.


<details>
  <summary>Details</summary>
Motivation: Stacking GCN layers inconsistently improves performance on tasks like node classification, suggesting the need for a reliable predictor of GCN effectiveness across different graph structures.

Method: Theoretical and empirical analysis using synthetic and real graph data (Cora, CiteSeer, Polblogs), exploring multiple aggregation methods for Fiedler values across connected components to predict GCN performance.

Result: Graphs with similar Fiedler values exhibit analogous structural properties, making the Fiedler value a reliable predictor of GCN performance and enabling more effective transfer learning between graphs.

Conclusion: Algebraic connectivity (Fiedler value) serves as a strong predictor for GCN performance, allowing better filter/hyperparameter selection and facilitating transfer learning between graphs with similar structural properties.

Abstract: A common observation in the Graph Convolutional Network (GCN) literature is
that stacking GCN layers may or may not result in better performance on tasks
like node classification and edge prediction. We have found empirically that a
graph's algebraic connectivity, which is known as the Fiedler value, is a good
predictor of GCN performance. Intuitively, graphs with similar Fiedler values
have analogous structural properties, suggesting that the same filters and
hyperparameters may yield similar results when used with GCNs, and that
transfer learning may be more effective between graphs with similar algebraic
connectivity. We explore this theoretically and empirically with experiments on
synthetic and real graph data, including the Cora, CiteSeer and Polblogs
datasets. We explore multiple ways of aggregating the Fiedler value for
connected components in the graphs to arrive at a value for the entire graph,
and show that it can be used to predict GCN performance. We also present
theoretical arguments as to why the Fiedler value is a good predictor.

</details>


### [152] [Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
*Stavros C. Kassinos*

Main category: cs.LG

TL;DR: Kourkoutas-Beta is a novel Adam-style optimizer with dynamic layer-wise beta2 values that adapt to gradient spikiness, improving stability and performance in physics-based problems and language modeling tasks with erratic gradients.


<details>
  <summary>Details</summary>
Motivation: Transformer neural networks for physics problems suffer from erratic losses and spiky gradients due to varying boundary/initial conditions in PDE surrogates and stiff composite losses in PINNs, which standard Adam optimizers struggle to handle effectively.

Method: Replaces fixed beta2 with layer-wise dynamic values driven by a bounded 'sunspike' ratio (current pooled gradient norm divided by EMA of past norms). Spikes lower beta2 toward beta2_min, calm phases keep it near beta2_max. Includes options like leaky-AMSGrad, trust-region clipping, adaptive tiny terms, and bias-correction modes.

Result: Improves stability and final loss vs fixed-beta2 Adam across four test settings: Transformer PDE surrogate, 3D PINN for heat conduction, MLX synthetic task, and character-level Transformer. On small-enwik8, lowers bits-per-character by ~38% vs Adam-0.95 and ~58% vs Adam-0.999 with smaller variance.

Conclusion: Kourkoutas-Beta provides drop-in replacement for Adam with minimal runtime overhead, preserves Adam-style convergence guarantees, and significantly improves robustness under spiky gradient conditions in both physics-based problems and language modeling tasks.

Abstract: Transformer neural networks are increasingly used for physics-based problems.
In data-driven PDE surrogates, training samples from varying boundary and
initial conditions can cause erratic losses and spiky gradients; in
physics-informed neural networks (PINNs), stiff composite losses amplify this
effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed
second-moment discount beta2 is replaced by a layer-wise dynamic value driven
by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an
exponential moving average (EMA) of past norms, squashed to the interval [0,1).
Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.
Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),
adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',
``exact'). With all features off and bias_correction=``none'', the method is
exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D
PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with
jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB
of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss
versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about
38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller
variance. The method remains drop-in, with runtime overhead comparable to Adam
in testbeds A-C and within single-digit percent in testbed D. It preserves
Adam-style convergence guarantees while improving robustness under spiky
gradients.

</details>


### [153] [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](https://arxiv.org/abs/2508.12997)
*Haishun Chen,Cai Xu,Jinlong Yu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.LG

TL;DR: FAML addresses biased evidence learning in multi-view evidential learning by introducing adaptive priors, fairness constraints, and opinion alignment to achieve balanced evidence allocation and improved uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-view evidential learning assumes reliable view-specific evidence learning, but empirical analysis reveals samples are biased toward assigning more evidence to data-rich classes, leading to unreliable uncertainty estimation.

Method: Proposes FAML with three components: 1) adaptive prior based on training trajectory for regularization, 2) fairness constraint based on class-wise evidence variance, 3) opinion alignment mechanism for multi-view fusion to mitigate view-specific bias.

Result: Extensive experiments on five real-world datasets show FAML achieves more balanced evidence allocation and improves both prediction performance and uncertainty estimation reliability compared to state-of-the-art methods.

Conclusion: FAML effectively addresses the biased evidential learning problem in multi-view settings through adaptive regularization, fairness constraints, and view alignment, demonstrating superior performance in both prediction accuracy and uncertainty reliability.

Abstract: Multi-view evidential learning aims to integrate information from multiple
views to improve prediction performance and provide trustworthy uncertainty
esitimation. Most previous methods assume that view-specific evidence learning
is naturally reliable. However, in practice, the evidence learning process
tends to be biased. Through empirical analysis on real-world data, we reveal
that samples tend to be assigned more evidence to support data-rich classes,
thereby leading to unreliable uncertainty estimation in predictions. This
motivates us to delve into a new Biased Evidential Multi-view Learning (BEML)
problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning
(FAML). FAML first introduces an adaptive prior based on training trajectory,
which acts as a regularization strategy to flexibly calibrate the biased
evidence learning process. Furthermore, we explicitly incorporate a fairness
constraint based on class-wise evidence variance to promote balanced evidence
allocation. In the multi-view fusion stage, we propose an opinion alignment
mechanism to mitigate view-specific bias across views, thereby encouraging the
integration of consistent and mutually supportive evidence. Extensive
experiments on five real-world multi-view datasets demonstrate that FAML
achieves more balanced evidence allocation and improves both prediction
performance and the reliability of uncertainty estimation compared to
state-of-the-art methods.

</details>


### [154] [Monte Carlo Functional Regularisation for Continual Learning](https://arxiv.org/abs/2508.13006)
*Pengcheng Hao,Menghao Waiyan William Zhu,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: MCFRCL is a new functional regularization framework for continual learning that uses Monte Carlo sampling and moment-based methods to approximate model predictions, achieving better accuracy and efficiency than existing approaches.


<details>
  <summary>Details</summary>
Motivation: Functional regularization-based continual learning methods suffer from high computational costs and large linear approximation errors, despite outperforming weight-space regularization approaches.

Method: Uses Monte Carlo sampling to approximate model prediction distributions, leverages three continuous distributions with moment-based methods to capture statistical characteristics, and employs both Wasserstein and KL distances for regularization.

Result: Evaluated on MNIST and CIFAR datasets, MCFRCL demonstrates effectiveness in both prediction accuracy and training efficiency compared to multiple benchmark methods.

Conclusion: The proposed MCFRCL framework provides an effective solution for continual learning by addressing computational cost and approximation error issues through Monte Carlo sampling and statistical distribution modeling.

Abstract: Continual learning (CL) is crucial for the adaptation of neural network
models to new environments. Although outperforming weight-space regularisation
approaches, the functional regularisation-based CL methods suffer from high
computational costs and large linear approximation errors. In this work, we
present a new functional regularisation CL framework, called MCFRCL, which
approximates model prediction distributions by Monte Carlo (MC) sampling.
Moreover, three continuous distributions are leveraged to capture the
statistical characteristics of the MC samples via moment-based methods.
Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)
distance are employed to construct the regularisation function. The proposed
MCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR
datasets, with simulation results highlighting its effectiveness in both
prediction accuracy and training efficiency.

</details>


### [155] [Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control](https://arxiv.org/abs/2508.13018)
*Iam Kim de S. Hermont,Andre R. Flores,Rodrigo C. de Lamare*

Main category: cs.LG

TL;DR: Proposes FXHEKM robust adaptive algorithm for active noise control with impulsive noise, showing superior performance against alpha-stable noises compared to competing methods.


<details>
  <summary>Details</summary>
Motivation: Need for robust noise cancellation in environments with impulsive noise (like alpha-stable noises) where traditional methods fail.

Method: Developed filtered-x hyperbolic tangent exponential generalized Kernel M-estimate function (FXHEKM) algorithm with statistical analysis and computational cost study.

Result: Numerical results demonstrate efficient cancellation of additive spurious signals and superior performance metrics (MSE and ANR) against competing algorithms.

Conclusion: FXHEKM algorithm is effective for active noise control in impulsive noise environments, providing robust performance against challenging noise types.

Abstract: In this work, we propose a robust adaptive filtering approach for active
noise control applications in the presence of impulsive noise. In particular,
we develop the filtered-x hyperbolic tangent exponential generalized Kernel
M-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis
of the proposed FXHEKM algorithm is carried out along with a study of its
computational cost. {In order to evaluate the proposed FXHEKM algorithm, the
mean-square error (MSE) and the average noise reduction (ANR) performance
metrics have been adopted.} Numerical results show the efficiency of the
proposed FXHEKM algorithm to cancel the presence of the additive spurious
signals, such as \textbf{$\alpha$}-stable noises against competing algorithms.

</details>


### [156] [The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks](https://arxiv.org/abs/2508.13030)
*Bipin Chhetri,Akbar Siami Namin*

Main category: cs.LG

TL;DR: BERT-based NLP approach achieves 97.2% accuracy in multi-label classification of cyberattack consequences, outperforming CNN and LSTM models for cybersecurity threat modeling.


<details>
  <summary>Details</summary>
Motivation: Increasing cyberattack complexity requires automated methods to analyze attack descriptions and predict consequences for better resource allocation and timely action in cybersecurity.

Method: Used BERT with Hierarchical Attention Networks for multi-label classification of attack consequences into five categories (Availability, Access Control, Confidentiality, Integrity, Other) using MITRE CWE database textual descriptions.

Result: BERT achieved 0.972 overall accuracy, significantly outperforming conventional CNN and LSTM models. HAN performed better than baseline CNN/LSTM on specific cybersecurity labels, but BERT consistently showed superior precision and recall.

Conclusion: BERT is more suitable than traditional deep learning models for predicting cyberattack consequences, demonstrating strong performance in multi-label classification for cybersecurity threat modeling applications.

Abstract: Cyberattacks are increasing, and securing against such threats is costing
industries billions of dollars annually. Threat Modeling, that is,
comprehending the consequences of these attacks, can provide critical support
to cybersecurity professionals, enabling them to take timely action and
allocate resources that could be used elsewhere. Cybersecurity is heavily
dependent on threat modeling, as it assists security experts in assessing and
mitigating risks related to identifying vulnerabilities and threats. Recently,
there has been a pressing need for automated methods to assess attack
descriptions and forecast the future consequences of the increasing complexity
of cyberattacks. This study examines how Natural Language Processing (NLP) and
deep learning can be applied to analyze the potential impact of cyberattacks by
leveraging textual descriptions from the MITRE Common Weakness Enumeration
(CWE) database. We emphasize classifying attack consequences into five
principal categories: Availability, Access Control, Confidentiality, Integrity,
and Other. This paper investigates the use of Bidirectional Encoder
Representations from Transformers (BERT) in combination with Hierarchical
Attention Networks (HANs) for Multi-label classification, evaluating their
performance in comparison with conventional CNN and LSTM-based models.
Experimental findings show that BERT achieves an overall accuracy of $0.972$,
far higher than conventional deep learning models in multi-label
classification. HAN outperforms baseline forms of CNN and LSTM-based models on
specific cybersecurity labels. However, BERT consistently achieves better
precision and recall, making it more suitable for predicting the consequences
of a cyberattack.

</details>


### [157] [Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data](https://arxiv.org/abs/2508.13040)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: Proposes a method to estimate AI model fairness using separate partial datasets when complete demographic data is unavailable due to privacy/legal constraints.


<details>
  <summary>Details</summary>
Motivation: Fairness testing is legally mandated but challenging due to restricted access to complete demographic data needed for group disparity assessments in high-stakes domains.

Method: Leverages separate internal datasets (predictive attributes) and external public datasets (protected attributes) to estimate feasible joint distributions and compute plausible fairness metric bounds.

Result: Simulation and real experiments show meaningful bounds on fairness metrics can be derived, providing reliable estimates of true fairness metrics.

Conclusion: This approach offers a practical solution for fairness testing in real-world settings where complete data access is restricted.

Abstract: Ensuring fairness in AI systems is critical, especially in high-stakes
domains such as lending, hiring, and healthcare. This urgency is reflected in
emerging global regulations that mandate fairness assessments and independent
bias audits. However, procuring the necessary complete data for fairness
testing remains a significant challenge. In industry settings, legal and
privacy concerns restrict the collection of demographic data required to assess
group disparities, and auditors face practical and cultural challenges in
gaining access to data. In practice, data relevant for fairness testing is
often split across separate sources: internal datasets held by institutions
with predictive attributes, and external public datasets such as census data
containing protected attributes, each providing only partial, marginal
information. Our work seeks to leverage such available separate data to
estimate model fairness when complete data is inaccessible. We propose
utilising the available separate data to estimate a set of feasible joint
distributions and then compute the set plausible fairness metrics. Through
simulation and real experiments, we demonstrate that we can derive meaningful
bounds on fairness metrics and obtain reliable estimates of the true metric.
Our results demonstrate that this approach can serve as a practical and
effective solution for fairness testing in real-world settings where access to
complete data is restricted.

</details>


### [158] [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](https://arxiv.org/abs/2508.13057)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: Comparison of two custom evaluation functions (FMAE and HEF) for demand forecasting shows HEF excels in global metrics for strategic planning, while FMAE performs better in local metrics and speed for operational efficiency.


<details>
  <summary>Details</summary>
Motivation: Demand forecasting faces challenges with multivariate time series complexity, uncertainty, and regime shifts, while traditional evaluation metrics introduce biases and limit generalization.

Method: Experiments comparing FMAE (focused on absolute errors) and HEF (weighting global metrics and penalizing large deviations) using different data splits (91:9, 80:20, 70:30) and three optimizers (Grid Search, PSO, Optuna).

Result: HEF consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE, RMSSE) and enhances model robustness, while FMAE excels in local metrics (MAE, MASE) and execution time.

Conclusion: Methodological trade-off exists: HEF is ideal for strategic planning, FMAE for operational efficiency. A replicable framework is proposed for optimizing predictive models in dynamic environments.

Abstract: Demand forecasting is essential for strategic planning in competitive
environments, enabling resource optimization and improved responsiveness to
market dynamics. However, multivariate time series modeling faces challenges
due to data complexity, uncertainty, and frequent regime shifts. Traditional
evaluation metrics can introduce biases and limit generalization. This work
compares two custom evaluation functions: FMAE (Focused Mean Absolute Error),
focused on minimizing absolute errors, and HEF (Hierarchical Evaluation
Function), designed to weight global metrics and penalize large deviations.
Experiments were conducted under different data splits (91:9, 80:20, 70:30)
using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative
accuracy, robustness, and computational efficiency. Results show that HEF
consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,
RMSSE), enhancing model robustness and explanatory power. These findings were
confirmed via visualizations and statistical tests. Conversely, FMAE offers
advantages in local metrics (MAE, MASE) and execution time, making it suitable
for short-term scenarios. The study highlights a methodological trade-off: HEF
is ideal for strategic planning, while FMAE is better suited for operational
efficiency. A replicable framework is proposed for optimizing predictive models
in dynamic environments.

</details>


### [159] [Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates](https://arxiv.org/abs/2508.13088)
*Xiaohan Wang,Zhimin Li,Joshua A. Levine,Matthew Berger*

Main category: cs.LG

TL;DR: Neural surrogate models for scientific simulations enable efficient inverse problem solving by modeling parameter distributions that produce specific output features, addressing surrogate approximation error through density estimation.


<details>
  <summary>Details</summary>
Motivation: Existing surrogate-based solutions focus on finding individual matching parameters but overlook the broader distribution of plausible parameters that could produce a given output feature.

Method: Model approximation error via density estimation (considering proximity to training parameters in both input and output space), form parameter prior belief, combine with feature likelihood, and develop efficient sampling of plausible parameter configurations.

Result: Developed a visualization interface for feature-driven parameter analysis across three simulation datasets, enabling interactive exploration of parameter distributions that generate target output features.

Conclusion: The approach successfully models and visualizes the distribution of possible input parameters for given output features, addressing both surrogate error and interactive parameter distribution formation challenges.

Abstract: Recently, neural surrogate models have emerged as a compelling alternative to
traditional simulation workflows. This is accomplished by modeling the
underlying function of scientific simulations, removing the need to run
expensive simulations. Beyond just mapping from input parameter to output,
surrogates have also been shown useful for inverse problems: output to input
parameters. Inverse problems can be understood as search, where we aim to find
parameters whose surrogate outputs contain a specified feature. Yet finding
these parameters can be costly, especially for high-dimensional parameter
spaces. Thus, existing surrogate-based solutions primarily focus on finding a
small set of matching parameters, in the process overlooking the broader
picture of plausible parameters. Our work aims to model and visualize the
distribution of possible input parameters that produce a given output feature.
To achieve this goal, we aim to address two challenges: (1) the approximation
error inherent in the surrogate model and (2) forming the parameter
distribution in an interactive manner. We model error via density estimation,
reporting high density only if a given parameter configuration is close to
training parameters, measured both over the input and output space. Our density
estimate is used to form a prior belief on parameters, and when combined with a
likelihood on features, gives us an efficient way to sample plausible parameter
configurations that generate a target output feature. We demonstrate the
usability of our solution through a visualization interface by performing
feature-driven parameter analysis over the input parameter space of three
simulation datasets. Source code is available at
https://github.com/matthewberger/seeing-the-many

</details>


### [160] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: A framework for detecting spatial outliers in maritime environments using sensor networks and LGCPs, with improved classification accuracy through second-order probability approximation and dynamic sensor placement.


<details>
  <summary>Details</summary>
Motivation: To improve detection of spatial commission outliers in maritime environments where accurate classification of normal vs. outlier events is crucial for security and monitoring applications.

Method: Uses log Gaussian Cox processes to model target arrivals as mixture of normal and outlier processes, proposes second-order probability approximation incorporating mean and variance, and integrates real-time sensor placement strategy.

Result: The framework demonstrates improved classification accuracy compared to mean-only approaches, provides tighter bound to true probability, and shows effectiveness in real ship traffic data validation near Norfolk, Virginia.

Conclusion: The proposed approach effectively enhances both outlier classification performance and detection capabilities through the combination of advanced probability modeling and adaptive sensor deployment strategies.

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


### [161] [Contrastive Representations for Temporal Reasoning](https://arxiv.org/abs/2508.13113)
*Alicja Ziarko,Michal Bortkiewicz,Michal Zawalski,Benjamin Eysenbach,Piotr Milos*

Main category: cs.LG

TL;DR: CRTR method uses combinatorial negative sampling to remove spurious features in temporal contrastive learning, enabling learned representations that support temporal reasoning and efficient puzzle solving without external search.


<details>
  <summary>Details</summary>
Motivation: Traditional AI separates perception (state-based representations) from planning (search-based temporal reasoning). The paper investigates whether temporal reasoning can emerge from representations that capture both perceptual and temporal structure.

Method: Combinatorial Representations for Temporal Reasoning (CRTR) - a temporal contrastive learning method with a negative sampling scheme that provably removes spurious features to facilitate temporal reasoning.

Result: CRTR achieves strong results on Sokoban and Rubik's Cube domains. For Rubik's Cube, it learns representations that generalize across all initial states and solves puzzles using fewer search steps than BestFS (though with longer solutions).

Conclusion: CRTR is the first method that efficiently solves arbitrary Rubik's Cube states using only learned representations without relying on external search algorithms, demonstrating that temporal reasoning can emerge from properly structured representations.

Abstract: In classical AI, perception relies on learning state-based representations,
while planning, which can be thought of as temporal reasoning over action
sequences, is typically achieved through search. We study whether such
reasoning can instead emerge from representations that capture both perceptual
and temporal structure. We show that standard temporal contrastive learning,
despite its popularity, often fails to capture temporal structure due to its
reliance on spurious features. To address this, we introduce Combinatorial
Representations for Temporal Reasoning (CRTR), a method that uses a negative
sampling scheme to provably remove these spurious features and facilitate
temporal reasoning. CRTR achieves strong results on domains with complex
temporal structure, such as Sokoban and Rubik's Cube. In particular, for the
Rubik's Cube, CRTR learns representations that generalize across all initial
states and allow it to solve the puzzle using fewer search steps than BestFS,
though with longer solutions. To our knowledge, this is the first method that
efficiently solves arbitrary Cube states using only learned representations,
without relying on an external search algorithm.

</details>


### [162] [A Perfectly Truthful Calibration Measure](https://arxiv.org/abs/2508.13100)
*Jason Hartline,Lunjia Hu,Yifan Wu*

Main category: cs.LG

TL;DR: This paper introduces ATB (averaged two-bin calibration error), the first perfectly truthful calibration measure in batch settings that prevents predictors from lying to appear more calibrated on finite samples.


<details>
  <summary>Details</summary>
Motivation: Existing calibration measures are not truthful - they incentivize predictors to lie about probabilities to appear more calibrated on finite samples, even when they know the true probabilities. No perfectly truthful calibration measure existed previously.

Method: The authors design ATB (averaged two-bin calibration error) as a novel calibration measure. They also provide a general recipe for constructing truthful measures, with ATB as a special case, and introduce quantile-binned l_2-ECE as another truthful measure.

Result: ATB is proven to be perfectly truthful, sound, complete, continuous, and quadratically related to existing measures (smCal and distCal). It enables faster estimation algorithms with simpler implementations than previous measures.

Conclusion: The paper successfully addresses the truthfulness problem in calibration measurement by introducing ATB and a general framework for constructing truthful calibration measures, providing both theoretical guarantees and practical computational advantages.

Abstract: Calibration requires that predictions are conditionally unbiased and,
therefore, reliably interpretable as probabilities. Calibration measures
quantify how far a predictor is from perfect calibration. As introduced by
Haghtalab et al. (2024), a calibration measure is truthful if it is minimized
in expectation when a predictor outputs the ground-truth probabilities.
Although predicting the true probabilities guarantees perfect calibration, in
reality, when calibration is evaluated on a finite sample, predicting the truth
is not guaranteed to minimize any known calibration measure. All known
calibration measures incentivize predictors to lie in order to appear more
calibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et
al. (2024) and Qiao and Zhao (2025) to construct approximately truthful
calibration measures in the sequential prediction setting, but no perfectly
truthful calibration measure was known to exist even in the more basic batch
setting.
  We design a perfectly truthful calibration measure in the batch setting:
averaged two-bin calibration error (ATB). In addition to being truthful, ATB is
sound, complete, continuous, and quadratically related to two existing
calibration measures: the smooth calibration error (smCal) and the (lower)
distance to calibration (distCal). The simplicity in our definition of ATB
makes it efficient and straightforward to compute. ATB allows faster estimation
algorithms with significantly easier implementations than smCal and distCal,
achieving improved running time and simplicity for the calibration testing
problem studied by Hu et al. (2024). We also introduce a general recipe for
constructing truthful measures, which proves the truthfulness of ATB as a
special case and allows us to construct other truthful calibration measures
such as quantile-binned l_2-ECE.

</details>


### [163] [Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry](https://arxiv.org/abs/2508.13111)
*Michael Mayr,Georgios C. Chasparis*

Main category: cs.LG

TL;DR: CGPT is a novel transformer architecture that integrates causal graphs to resolve the trade-off between channel-dependent and channel-independent models for industrial time-series forecasting, achieving superior performance while maintaining dimensionality agnosticism.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental trade-off in industrial time-series modeling where channel-dependent models lack robustness and adaptability while channel-independent models miss crucial cross-variable interactions.

Method: Proposes Causally-Guided Pairwise Transformer (CGPT) that decomposes multidimensional data into pairs using a known causal graph as inductive bias, with channel-agnostic learnable layers and pairwise information flow.

Result: CGPT significantly outperforms both CI and CD baselines in predictive accuracy on synthetic and real-world industrial datasets, showing competitive performance with end-to-end trained CD models while remaining dimensionality-agnostic.

Conclusion: CGPT successfully resolves the CD/CI conflict through pairwise modeling with causal guidance, creating a flexible architecture that ensures scalability and any-variate adaptability for industrial time-series forecasting.

Abstract: Foundational modelling of multi-dimensional time-series data in industrial
systems presents a central trade-off: channel-dependent (CD) models capture
specific cross-variable dynamics but lack robustness and adaptability as model
layers are commonly bound to the data dimensionality of the tackled use-case,
while channel-independent (CI) models offer generality at the cost of modelling
the explicit interactions crucial for system-level predictive regression tasks.
To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a
novel architecture that integrates a known causal graph as an inductive bias.
The core of CGPT is built around a pairwise modeling paradigm, tackling the
CD/CI conflict by decomposing the multidimensional data into pairs. The model
uses channel-agnostic learnable layers where all parameter dimensions are
independent of the number of variables. CGPT enforces a CD information flow at
the pair-level and CI-like generalization across pairs. This approach
disentangles complex system dynamics and results in a highly flexible
architecture that ensures scalability and any-variate adaptability. We validate
CGPT on a suite of synthetic and real-world industrial datasets on long-term
and one-step forecasting tasks designed to simulate common industrial
complexities. Results demonstrate that CGPT significantly outperforms both CI
and CD baselines in predictive accuracy and shows competitive performance with
end-to-end trained CD models while remaining agnostic to the problem
dimensionality.

</details>


### [164] [Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]](https://arxiv.org/abs/2508.13135)
*Yueyang Liu,Lance Kennedy,Ruochen Kong,Joon-Seok Kim,Andreas Züfle*

Main category: cs.LG

TL;DR: This paper explores best practices for training ML models to predict complete individual trajectories over days/weeks, focusing on incorporating semantic information and addressing data skewness through user clustering and stratified sampling.


<details>
  <summary>Details</summary>
Motivation: Existing human mobility prediction research focuses on microscopic aspects like short-term trajectories, with limited attention to macro-level mobility patterns and life routines. The paper aims to determine optimal training practices for forecasting complete individual trajectories over extended periods.

Method: Comprehensive experimental analysis of diverse models (LSTM and Transformer architectures), parameter configurations, and training strategies. Incorporates semantic information like day-of-week and user-specific historical data. Uses user semantic clustering with stratified sampling to mitigate data imbalance and preserve diversity. Employs small-batch stochastic gradient optimization.

Result: Explicit inclusion of semantic information helps models better understand individual life patterns and improves predictions. User sampling exacerbates data skewness and reduces predictive accuracy. Stratified sampling with user clustering preserves dataset representativeness. Small-batch stochastic gradient optimization improves performance, especially with limited training data.

Conclusion: Incorporating semantic context and addressing data imbalance through careful sampling strategies are crucial for effective long-term human mobility prediction. Small-batch optimization enhances model performance in data-limited scenarios.

Abstract: Individual-level human mobility prediction has emerged as a significant topic
of research with applications in infectious disease monitoring, child, and
elderly care. Existing studies predominantly focus on the microscopic aspects
of human trajectories: such as predicting short-term trajectories or the next
location visited, while offering limited attention to macro-level mobility
patterns and the corresponding life routines. In this paper, we focus on an
underexplored problem in human mobility prediction: determining the best
practices to train a machine learning model using historical data to forecast
an individuals complete trajectory over the next days and weeks. In this
experiment paper, we undertake a comprehensive experimental analysis of diverse
models, parameter configurations, and training strategies, accompanied by an
in-depth examination of the statistical distribution inherent in human mobility
patterns. Our empirical evaluations encompass both Long Short-Term Memory and
Transformer-based architectures, and further investigate how incorporating
individual life patterns can enhance the effectiveness of the prediction. We
show that explicitly including semantic information such as day-of-the-week and
user-specific historical information can help the model better understand
individual patterns of life and improve predictions. Moreover, since the
absence of explicit user information is often missing due to user privacy, we
show that the sampling of users may exacerbate data skewness and result in a
substantial loss in predictive accuracy. To mitigate data imbalance and
preserve diversity, we apply user semantic clustering with stratified sampling
to ensure that the sampled dataset remains representative. Our results further
show that small-batch stochastic gradient optimization improves model
performance, especially when human mobility training data is limited.

</details>


### [165] [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)
*Haoyu He,Katrin Renz,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: MDPO addresses the training-inference discrepancy in masked diffusion language models by using reinforcement learning to optimize denoising trajectories, achieving significant performance improvements with fewer gradient updates.


<details>
  <summary>Details</summary>
Motivation: Masked diffusion language models suffer from a key discrepancy between training (random masking) and inference (progressive revealing of structure), leading to suboptimal performance that previous works have overlooked.

Method: Proposed Masked Diffusion Policy Optimization (MDPO) that frames denoising trajectory learning as a sequential decision-making problem using reinforcement learning, plus a training-free remasking strategy called RCR for flexible token refinement.

Result: MDPO matches previous SOTA performance with 60x fewer gradient updates, achieves 9.6% improvement on MATH500 and 54.2% on Countdown over SOTA with same update count, and RCR provides additional consistent performance gains.

Conclusion: The approach demonstrates great potential for investigating and addressing the pre-training-inference discrepancy in masked diffusion language models, establishing effective training and inference strategies.

Abstract: Diffusion language models, as a promising alternative to traditional
autoregressive (AR) models, enable faster generation and richer conditioning on
bidirectional context. However, they suffer from a key discrepancy between
training and inference: during inference, MDLMs progressively reveal the
structure of the generated sequence by producing fewer and fewer masked tokens,
whereas this structure is ignored in training as tokens are masked at random.
Although this discrepancy between training and inference can lead to suboptimal
performance, it has been largely overlooked by previous works, leaving closing
this gap between the two stages an open problem. To address this, we frame the
problem of learning effective denoising trajectories as a sequential
decision-making problem and use the resulting framework to apply reinforcement
learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to
exploit the Markov property diffusion possesses and explicitly train the model
under the same progressive refining schedule used at inference. MDPO matches
the performance of the previous state-of-the-art (SOTA) method with 60x fewer
gradient updates, while achieving average improvements of 9.6% on MATH500 and
54.2% on Countdown over SOTA when trained within the same number of weight
updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in
inference replacement to overcome the limitation that the model cannot refine
tokens flexibly. This simple yet effective training-free strategy, what we
refer to as RCR, consistently improves performance and yields additional gains
when combined with MDPO. Our findings establish great potential for
investigating the discrepancy between pre-training and inference of MDLMs.
Code: https://github.com/autonomousvision/mdpo. Project Page:
https://cli212.github.io/MDPO/.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [166] [Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.11706)
*Zhuofan Xu,Benedikt Bollig,Matthias Függer,Thomas Nowak,Vincent Le Dréau*

Main category: cs.MA

TL;DR: CPE learning proposes a centralized training and execution framework with permutation equivariant architecture to overcome limitations of CTDE in multi-agent reinforcement learning, achieving improved performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: CTDE paradigm faces challenges with partial observability leading to suboptimal performance in decentralized policies, while fully centralized approaches suffer from scalability issues as agent numbers increase.

Method: Uses Centralized Permutation Equivariant (CPE) learning with Global-Local Permutation Equivariant (GLPE) networks - a lightweight, scalable permutation equivariant architecture that works with both value decomposition and actor-critic methods.

Result: Substantially improves performance of standard CTDE algorithms across cooperative benchmarks (MPE, SMAC, RWARE) and matches state-of-the-art RWARE implementation performance.

Conclusion: CPE framework successfully addresses CTDE limitations through permutation equivariant architecture, providing scalable centralized execution while maintaining compatibility with existing MARL methods.

Abstract: The Centralized Training with Decentralized Execution (CTDE) paradigm has
gained significant attention in multi-agent reinforcement learning (MARL) and
is the foundation of many recent algorithms. However, decentralized policies
operate under partial observability and often yield suboptimal performance
compared to centralized policies, while fully centralized approaches typically
face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized
training and execution framework that employs a fully centralized policy to
overcome these limitations. Our approach leverages a novel permutation
equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks,
that is lightweight, scalable, and easy to implement. Experiments show that CPE
integrates seamlessly with both value decomposition and actor-critic methods,
substantially improving the performance of standard CTDE algorithms across
cooperative benchmarks including MPE, SMAC, and RWARE, and matching the
performance of state-of-the-art RWARE implementations.

</details>


### [167] [SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication](https://arxiv.org/abs/2508.11733)
*Ruijia Zhang,Xinyan Zhao,Ruixiang Wang,Sigen Chen,Guibin Zhang,An Zhang,Kun Wang,Qingsong Wen*

Main category: cs.MA

TL;DR: SafeSieve is a progressive multi-agent pruning algorithm that reduces token usage by 12.4%-27.8% while maintaining 94.01% accuracy through dynamic communication refinement and 0-extension clustering.


<details>
  <summary>Details</summary>
Motivation: LLM-based multi-agent systems suffer from redundant communication and excessive token overhead, with existing methods isolating pre- and post-task optimization without unified strategy.

Method: Progressive adaptive pruning algorithm with dual-mechanism: initial LLM-based semantic evaluation + accumulated performance feedback, using 0-extension clustering to preserve coherent agent groups while eliminating ineffective links.

Result: Achieves 94.01% average accuracy, reduces token usage by 12.4%-27.8%, shows robustness under prompt injection attacks (1.23% avg accuracy drop), reduces deployment costs by 13.3% in heterogeneous settings.

Conclusion: SafeSieve establishes as a robust, efficient, and scalable framework for practical multi-agent systems with significant token reduction and maintained performance.

Abstract: LLM-based multi-agent systems exhibit strong collaborative capabilities but
often suffer from redundant communication and excessive token overhead.
Existing methods typically enhance efficiency through pretrained GNNs or greedy
algorithms, but often isolate pre- and post-task optimization, lacking a
unified strategy. To this end, we present SafeSieve, a progressive and adaptive
multi-agent pruning algorithm that dynamically refines the inter-agent
communication through a novel dual-mechanism. SafeSieve integrates initial
LLM-based semantic evaluation with accumulated performance feedback, enabling a
smooth transition from heuristic initialization to experience-driven
refinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs
0-extension clustering to preserve structurally coherent agent groups while
eliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval,
etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducing
token usage by 12.4%-27.8%. Results further demonstrate robustness under prompt
injection attacks (1.23% average accuracy drop). In heterogeneous settings,
SafeSieve reduces deployment costs by 13.3% while maintaining performance.
These results establish SafeSieve as a robust, efficient, and scalable
framework for practical multi-agent systems. Our code can be found in
https://anonymous.4open.science/r/SafeSieve-D8F2FFUN.

</details>


### [168] [A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond](https://arxiv.org/abs/2508.11957)
*Xiaodong Qu,Andrews Damoah,Joshua Sherwood,Peiyan Liu,Christian Shun Jin,Lulu Chen,Minjie Shen,Nawwaf Aleisa,Zeyuan Hou,Chenyu Zhang,Lifu Gao,Yanshu Li,Qikai Yang,Qun Wang,Cristabelle De Souza*

Main category: cs.MA

TL;DR: A comprehensive review of modern AI agent architectures, covering cognitive models, reinforcement learning, and LLM-based reasoning while addressing ethical and safety concerns.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of AI agents from rule-based systems to autonomous learning systems creates a need to systematically examine architectural principles and address the challenge of designing unified agents that integrate cognition, planning, and interaction.

Method: Systematic review and synthesis of insights from cognitive science-inspired models, hierarchical reinforcement learning frameworks, and large language model-based reasoning approaches.

Result: The review provides a comprehensive examination of architectural principles, foundational components, and emergent paradigms that define contemporary AI agents, highlighting major breakthroughs and persistent challenges.

Conclusion: This review aims to guide the development of next-generation AI agent systems toward more robust, adaptable, and trustworthy autonomous intelligence by addressing ethical, safety, and interpretability concerns in real-world deployment.

Abstract: Artificial Intelligence (AI) agents have rapidly evolved from specialized,
rule-based programs to versatile, learning-driven autonomous systems capable of
perception, reasoning, and action in complex environments. The explosion of
data, advances in deep learning, reinforcement learning, and multi-agent
coordination have accelerated this transformation. Yet, designing and deploying
unified AI agents that seamlessly integrate cognition, planning, and
interaction remains a grand challenge. In this review, we systematically
examine the architectural principles, foundational components, and emergent
paradigms that define the landscape of contemporary AI agents. We synthesize
insights from cognitive science-inspired models, hierarchical reinforcement
learning frameworks, and large language model-based reasoning. Moreover, we
discuss the pressing ethical, safety, and interpretability concerns associated
with deploying these agents in real-world scenarios. By highlighting major
breakthroughs, persistent challenges, and promising research directions, this
review aims to guide the next generation of AI agent systems toward more
robust, adaptable, and trustworthy autonomous intelligence.

</details>


### [169] [Synchronization Dynamics of Heterogeneous, Collaborative Multi-Agent AI Systems](https://arxiv.org/abs/2508.12314)
*Chiranjit Mitra*

Main category: cs.MA

TL;DR: A physics-inspired framework that applies synchronization theory to multi-agent AI systems using an adapted Kuramoto model to analyze coordination, agent diversity, and network effects on collective AI behavior.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for understanding and optimizing collective behavior in heterogeneous multi-agent AI systems by bridging synchronization theory with AI coordination challenges.

Method: Adapted the Kuramoto model to represent AI agents as coupled oscillators with phase and amplitude dynamics, introduced order parameters to quantify coordination, and conducted simulations on all-to-all and scale-free networks to study coupling effects.

Result: Demonstrated that increased coupling promotes robust synchronization despite agent heterogeneity, successfully formalized correspondence between Chain-of-Thought prompting and synchronization phenomena, and showed network topology significantly impacts emergent collective behavior.

Conclusion: The framework provides a physics-informed mathematical foundation for designing scalable and interpretable multi-agent AI systems, opening pathways for principled orchestration of agentic AI and future incorporation of learning dynamics and adaptive architectures.

Abstract: We present a novel interdisciplinary framework that bridges synchronization
theory and multi-agent AI systems by adapting the Kuramoto model to describe
the collective dynamics of heterogeneous AI agents engaged in complex task
execution. By representing AI agents as coupled oscillators with both phase and
amplitude dynamics, our model captures essential aspects of agent
specialization, influence, and communication within networked systems. We
introduce an order parameter to quantify the degree of coordination and
synchronization, providing insights into how coupling strength, agent
diversity, and network topology impact emergent collective behavior.
Furthermore, we formalize a detailed correspondence between Chain-of-Thought
prompting in AI reasoning and synchronization phenomena, unifying human-like
iterative problem solving with emergent group intelligence. Through extensive
simulations on all-to-all and deterministic scale-free networks, we demonstrate
that increased coupling promotes robust synchronization despite heterogeneous
agent capabilities, reflecting realistic collaborative AI scenarios. Our
physics-informed approach establishes a rigorous mathematical foundation for
designing, analyzing, and optimizing scalable, adaptive, and interpretable
multi-agent AI systems. This work opens pathways for principled orchestration
of agentic AI and lays the groundwork for future incorporation of learning
dynamics and adaptive network architectures to further enhance system
resilience and efficiency.

</details>


### [170] [A Taxonomy of Hierarchical Multi-Agent Systems: Design Patterns, Coordination Mechanisms, and Industrial Applications](https://arxiv.org/abs/2508.12683)
*David J. Moore*

Main category: cs.MA

TL;DR: A multi-dimensional taxonomy for hierarchical multi-agent systems (HMAS) across five axes: control hierarchy, information flow, role delegation, temporal layering, and communication structure, connecting classical coordination mechanisms with modern learning approaches.


<details>
  <summary>Details</summary>
Motivation: Hierarchical multi-agent systems help manage complexity and scale but introduce trade-offs that are not always obvious. The paper aims to provide a comprehensive framework for comparing different HMAS approaches rather than prescribing a single best design.

Method: Proposes a five-dimensional taxonomy and connects it to concrete coordination mechanisms including contract-net protocol and hierarchical reinforcement learning. Uses industrial case studies from power grids and oilfield operations to illustrate the framework.

Result: The taxonomy provides a unified design framework that bridges classical coordination mechanisms with modern reinforcement learning and large language model agents. Case studies suggest hierarchical structures can achieve global efficiency while preserving local autonomy.

Conclusion: Identifies open challenges including making hierarchical decisions explainable, scaling to large agent populations, and safely integrating learning-based agents like LLMs. Presents the first unified taxonomy covering structural, temporal, and communication dimensions of HMAS.

Abstract: Hierarchical multi-agent systems (HMAS) organize collections of agents into
layered structures that help manage complexity and scale. These hierarchies can
simplify coordination, but they also can introduce trade-offs that are not
always obvious. This paper proposes a multi-dimensional taxonomy for HMAS along
five axes: control hierarchy, information flow, role and task delegation,
temporal layering, and communication structure. The intent is not to prescribe
a single "best" design but to provide a lens for comparing different
approaches.
  Rather than treating these dimensions in isolation, the taxonomy is connected
to concrete coordination mechanisms - from the long-standing contract-net
protocol for task allocation to more recent work in hierarchical reinforcement
learning. Industrial contexts illustrate the framework, including power grids
and oilfield operations, where agents at production, maintenance, and supply
levels coordinate to diagnose well issues or balance energy demand. These cases
suggest that hierarchical structures may achieve global efficiency while
preserving local autonomy, though the balance is delicate.
  The paper closes by identifying open challenges: making hierarchical
decisions explainable to human operators, scaling to very large agent
populations, and assessing whether learning-based agents such as large language
models can be safely integrated into layered frameworks. This paper presents
what appears to be the first taxonomy that unifies structural, temporal, and
communication dimensions of hierarchical MAS into a single design framework,
bridging classical coordination mechanisms with modern reinforcement learning
and large language model agents.

</details>
