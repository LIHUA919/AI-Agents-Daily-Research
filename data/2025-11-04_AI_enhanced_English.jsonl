{"id": "2511.00034", "categories": ["cs.MA", "cs.LG", "68T05, 68T07, 91A10", "I.2.6; I.2.11; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.00034", "abs": "https://arxiv.org/abs/2511.00034", "authors": ["Aditya Akella"], "title": "On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning", "comment": "8 pages, 5 figures, 2 tables", "summary": "Recent advances in learnable reward shaping have shown promise in\nsingle-agent reinforcement learning by automatically discovering effective\nfeedback signals. However, the effectiveness of decentralized learnable reward\nshaping in cooperative multi-agent settings remains poorly understood. We\npropose DMARL-RSA, a fully decentralized system where each agent learns\nindividual reward shaping, and evaluate it on cooperative navigation tasks in\nthe simple_spread_v3 environment. Despite sophisticated reward learning,\nDMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with\ncentralized training at 1.92 +/- 0.87--a 26.12-point gap. DMARL-RSA performs\nsimilarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating\nthat advanced reward shaping cannot overcome fundamental decentralized\ncoordination limitations. Interestingly, decentralized methods achieve higher\nlandmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out\nof 3 total) but worse overall performance than centralized MAPPO (0.273 +/-\n0.008 landmark coverage)--revealing a coordination paradox between local\noptimization and global performance. Analysis identifies three critical\nbarriers: (1) non-stationarity from concurrent policy updates, (2) exponential\ncredit assignment complexity, and (3) misalignment between individual reward\noptimization and global objectives. These results establish empirical limits\nfor decentralized reward learning and underscore the necessity of centralized\ncoordination for effective multi-agent cooperation.", "AI": {"tldr": "DMARL-RSA, a decentralized reward shaping method for multi-agent systems, underperforms centralized approaches (-24.20 vs 1.92 reward) despite better landmark coverage, revealing coordination barriers.", "motivation": "To explore if decentralized learnable reward shaping can be effective in cooperative multi-agent settings, as previous advances focused on single-agent scenarios.", "method": "Proposed DMARL-RSA where each agent learns individual reward shaping, evaluated on cooperative navigation tasks in simple_spread_v3 environment.", "result": "DMARL-RSA achieved -24.20 average reward vs MAPPO's 1.92, similar to IPPO (-23.19). Higher landmark coverage (0.888-0.960) but worse overall performance than centralized methods.", "conclusion": "Decentralized reward learning faces fundamental coordination limitations due to non-stationarity, credit assignment complexity, and goal misalignment, necessitating centralized coordination."}}
{"id": "2511.00096", "categories": ["cs.MA", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.00096", "abs": "https://arxiv.org/abs/2511.00096", "authors": ["Shangyu Lou"], "title": "Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System", "comment": "Accepted to The 3rd ACM SIGSPATIAL International Workshop on Advances\n  in Urban AI (UrbanAI'25)", "summary": "Urban Artificial Intelligence (Urban AI) has advanced human-centered urban\ntasks such as perception prediction and human dynamics. Large Language Models\n(LLMs) can integrate multimodal inputs to address heterogeneous data in complex\nurban systems but often underperform on domain-specific tasks. Urban-MAS, an\nLLM-based Multi-Agent System (MAS) framework, is introduced for human- centered\nurban prediction under zero-shot settings. It includes three agent types:\nPredictive Factor Guidance Agents, which prioritize key predictive factors to\nguide knowledge extraction and enhance the effectiveness of compressed urban\nknowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve\nrobustness by com- paring multiple outputs, validating consistency, and\nre-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which\nintegrate extracted multi-source information across dimensions for prediction.\nExperiments on running-amount prediction and ur- ban perception across Tokyo,\nMilan, and Seattle demonstrate that Urban-MAS substantially reduces errors\ncompared to single-LLM baselines. Ablation studies indicate that Predictive\nFactor Guidance Agents are most critical for enhancing predictive performance,\npo- sitioning Urban-MAS as a scalable paradigm for human-centered urban AI\nprediction. Code is available on the project\nwebsite:https://github.com/THETUREHOOHA/UrbanMAS", "AI": {"tldr": "Urban-MAS is an LLM-based multi-agent system that improves human-centered urban prediction through specialized agents for predictive factor guidance, robust information extraction, and multi-source inference, achieving better performance than single LLMs.", "motivation": "Large Language Models can handle multimodal urban data but underperform on domain-specific urban tasks, creating a need for specialized frameworks that can effectively leverage LLMs for urban AI applications.", "method": "Urban-MAS uses three agent types: Predictive Factor Guidance Agents to prioritize key factors and guide knowledge extraction; Reliable UrbanInfo Extraction Agents to improve robustness through output comparison and validation; and Multi-UrbanInfo Inference Agents to integrate multi-source information for prediction.", "result": "Experiments on running-amount prediction and urban perception across Tokyo, Milan, and Seattle show Urban-MAS substantially reduces errors compared to single-LLM baselines, with ablation studies indicating Predictive Factor Guidance Agents are most critical for performance.", "conclusion": "Urban-MAS provides a scalable paradigm for human-centered urban AI prediction, demonstrating that specialized multi-agent systems can effectively leverage LLMs for complex urban tasks under zero-shot settings."}}
{"id": "2511.00330", "categories": ["cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00330", "abs": "https://arxiv.org/abs/2511.00330", "authors": ["Yeonju Ro", "Haoran Qiu", "\u00cd\u00f1igo Goiri", "Rodrigo Fonseca", "Ricardo Bianchini", "Aditya Akella", "Zhangyang Wang", "Mattan Erez", "Esha Choukse"], "title": "Sherlock: Reliable and Efficient Agentic Workflow Execution", "comment": null, "summary": "With the increasing adoption of large language models (LLM), agentic\nworkflows, which compose multiple LLM calls with tools, retrieval, and\nreasoning steps, are increasingly replacing traditional applications. However,\nsuch workflows are inherently error-prone: incorrect or partially correct\noutput at one step can propagate or even amplify through subsequent stages,\ncompounding the impact on the final output. Recent work proposes integrating\nverifiers that validate LLM output or actions, such as self-reflection, debate,\nor LLM-as-a-judge mechanisms. Yet, verifying every step introduces significant\nlatency and cost overheads.\n  In this work, we seek to answer three key questions: which nodes in a\nworkflow are most error-prone and thus deserve costly verification, how to\nselect the most appropriate verifier for each node, and how to use verification\nwith minimal impact to latency? Our solution, Sherlock, addresses these using\ncounterfactual analysis on agentic workflows to identify error-prone nodes and\nselectively attaching cost-optimal verifiers only where necessary. At runtime,\nSherlock speculatively executes downstream tasks to reduce latency overhead,\nwhile verification runs in the background. If verification fails, execution is\nrolled back to the last verified output. Compared to the non-verifying\nbaseline, Sherlock delivers an 18.3% accuracy gain on average across\nbenchmarks. Sherlock reduces workflow execution time by up to 48.7% over\nnon-speculative execution and lowers verification cost by 26.0% compared to the\nMonte Carlo search-based method, demonstrating that principled, fault-aware\nverification effectively balances efficiency and reliability in agentic\nworkflows.", "AI": {"tldr": "Sherlock introduces selective verification using counterfactual analysis to pinpoint error-prone nodes in LLM workflows, reducing latency and cost while improving accuracy.", "motivation": "Agentic workflows using LLMs are prone to error propagation, but verifying every step is too costly in latency and resources.", "method": "Uses counterfactual analysis to identify which nodes need verification, selects optimal verifiers per node, and employs speculative execution to minimize latency.", "result": "Achieves 18.3% accuracy gain, reduces execution time by up to 48.7%, and cuts verification cost by 26.0% compared to baselines.", "conclusion": "Principled, fault-aware verification balances efficiency and reliability in LLM workflows."}}
{"id": "2511.00387", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.00387", "abs": "https://arxiv.org/abs/2511.00387", "authors": ["Xiaoling Han", "Bin Lin", "Zhenyu Na", "Bowen Li", "Chaoyue Zhang", "Ran Zhang"], "title": "Spatial Crowdsourcing-based Task Allocation for UAV-assisted Maritime Data Collection", "comment": null, "summary": "Driven by the unceasing development of maritime services, tasks of unmanned\naerial vehicle (UAV)-assisted maritime data collection (MDC) are becoming\nincreasingly diverse, complex and personalized. As a result, effective task\nallocation for MDC is becoming increasingly critical. In this work, integrating\nthe concept of spatial crowdsourcing (SC), we develop an SC-based MDC network\nmodel and investigate the task allocation problem for UAV-assisted MDC. In\nvariable maritime service scenarios, tasks are allocated to UAVs based on the\nspatial and temporal requirements of the tasks, as well as the mobility of the\nUAVs. To address this problem, we design an SC-based task allocation algorithm\nfor the MDC (SC-MDC-TA). The quality estimation is utilized to assess and\nregulate task execution quality by evaluating signal to interference plus noise\nratio and the UAV energy consumption. The reverse auction is employed to\npotentially reduce the task waiting time as much as possible while ensuring\ntimely completion. Additionally, we establish typical task allocation scenarios\nbased on maritime service requirements indicated by electronic navigational\ncharts. Simulation results demonstrate that the proposed SC-MDC-TA algorithm\neffectively allocates tasks for various MDC scenarios. Furthermore, compared to\nthe benchmark, the SC-MDC-TA algorithm can also reduce the task completion time\nand lower the UAV energy consumption.", "AI": {"tldr": "This paper proposes a spatial crowdsourcing-based task allocation algorithm (SC-MDC-TA) for UAV-assisted maritime data collection, which improves task execution quality while reducing completion time and energy consumption.", "motivation": "Maritime data collection tasks are becoming more diverse and complex, requiring efficient task allocation methods for UAVs in variable maritime service scenarios.", "method": "Developed an SC-based MDC network model and designed the SC-MDC-TA algorithm that uses quality estimation (evaluating SINR and energy consumption) and reverse auction mechanisms to optimize task allocation.", "result": "Simulation results show the algorithm effectively allocates tasks across various MDC scenarios while reducing task completion time and UAV energy consumption compared to benchmarks.", "conclusion": "The proposed SC-MDC-TA algorithm provides an effective solution for task allocation in UAV-assisted maritime data collection, demonstrating superior performance in complex maritime environments."}}
{"id": "2511.00002", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00002", "abs": "https://arxiv.org/abs/2511.00002", "authors": ["Yurun Wu", "Yousong Sun", "Burkhard Wunsche", "Jia Wang", "Elliott Wen"], "title": "VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games", "comment": null, "summary": "Virtual Reality (VR) has rapidly become a mainstream platform for gaming and\ninteractive experiences, yet ensuring the quality, safety, and appropriateness\nof VR content remains a pressing challenge. Traditional human-based quality\nassurance is labor-intensive and cannot scale with the industry's rapid growth.\nWhile automated testing has been applied to traditional 2D and 3D games,\nextending it to VR introduces unique difficulties due to high-dimensional\nsensory inputs and strict real-time performance requirements. We present\nVRScout, a deep learning-based agent capable of autonomously navigating VR\nenvironments and interacting with virtual objects in a human-like and real-time\nmanner. VRScout learns from human demonstrations using an enhanced Action\nChunking Transformer that predicts multi-step action sequences. This enables\nour agent to capture higher-level strategies and generalize across diverse\nenvironments. To balance responsiveness and precision, we introduce a\ndynamically adjustable sliding horizon that adapts the agent's temporal context\nat runtime. We evaluate VRScout on commercial VR titles and show that it\nachieves expert-level performance with only limited training data, while\nmaintaining real-time inference at 60 FPS on consumer-grade hardware. These\nresults position VRScout as a practical and scalable framework for automated VR\ngame testing, with direct applications in both quality assurance and safety\nauditing.", "AI": {"tldr": "VRScout: A deep learning-based agent for autonomous VR game testing using enhanced Action Chunking Transformer for human-like interactions and real-time performance.", "motivation": "Traditional human-based quality assurance for VR content is labor-intensive and doesn't scale well with industry growth, while automated testing faces unique challenges in VR environments.", "method": "Uses an enhanced Action Chunking Transformer that learns from human demonstrations to predict multi-step action sequences, with a dynamically adjustable sliding horizon for temporal context adaptation.", "result": "Achieves expert-level performance on commercial VR titles with limited training data and maintains real-time inference at 60 FPS on consumer hardware.", "conclusion": "VRScout provides a practical and scalable framework for automated VR game testing applicable to quality assurance and safety auditing."}}
{"id": "2511.00020", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms.", "AI": {"tldr": "A multimodal fake review detection framework combining BERT for text and ResNet-50 for images outperforms unimodal approaches, achieving 0.934 F1-score by capturing cross-modal inconsistencies.", "motivation": "Fake reviews generated by bots/AI threaten digital trust, but existing detection methods only use single modalities and miss semantic inconsistencies across text and images.", "method": "Proposes a framework integrating BERT-encoded textual features and ResNet-50 visual features, fused through a classification head, using a curated dataset of 21,142 user-uploaded images from food, hospitality, and e-commerce.", "result": "Multimodal model achieves 0.934 F1-score, outperforming unimodal baselines, and qualitative analysis shows it detects subtle inconsistencies like mismatched text and images.", "conclusion": "Multimodal learning is crucial for digital trust, offering a scalable solution for content moderation across online platforms by effectively identifying deceptive reviews."}}
{"id": "2511.00628", "categories": ["cs.MA", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00628", "abs": "https://arxiv.org/abs/2511.00628", "authors": ["Yang Li", "Siqi Ping", "Xiyu Chen", "Xiaojian Qi", "Zigan Wang", "Ye Luo", "Xiaowei Zhang"], "title": "AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems", "comment": null, "summary": "With the rapid progress of large language models (LLMs), LLM-powered\nmulti-agent systems (MAS) are drawing increasing interest across academia and\nindustry. However, many current MAS frameworks struggle with reliability and\nscalability, especially on complex tasks. We present AgentGit, a framework that\nbrings Git-like rollback and branching to MAS workflows. Built as an\ninfrastructure layer on top of LangGraph, AgentGit supports state commit,\nrevert, and branching, allowing agents to traverse, compare, and explore\nmultiple trajectories efficiently. To evaluate AgentGit, we designed an\nexperiment that optimizes target agents by selecting better prompts. We ran a\nmulti-step A/B test against three baselines -- LangGraph, AutoGen, and Agno --\non a real-world task: retrieving and analyzing paper abstracts. Results show\nthat AgentGit significantly reduces redundant computation, lowers runtime and\ntoken usage, and supports parallel exploration across multiple branches,\nenhancing both reliability and scalability in MAS development. This work offers\na practical path to more robust MAS design and enables error recovery, safe\nexploration, iterative debugging, and A/B testing in collaborative AI systems.", "AI": {"tldr": "AgentGit is a framework that adds Git-like rollback and branching capabilities to multi-agent systems, reducing redundant computation and improving reliability and scalability.", "motivation": "Current multi-agent systems struggle with reliability and scalability on complex tasks, lacking efficient ways to manage workflow states and explore multiple trajectories.", "method": "Built as an infrastructure layer on LangGraph, AgentGit supports state commit, revert, and branching operations, allowing agents to efficiently traverse and compare multiple workflow trajectories.", "result": "In experiments optimizing target agents through prompt selection, AgentGit significantly reduced redundant computation, lowered runtime and token usage, and supported parallel exploration across branches compared to LangGraph, AutoGen, and Agno baselines.", "conclusion": "AgentGit provides a practical approach for more robust multi-agent system design, enabling error recovery, safe exploration, iterative debugging, and A/B testing in collaborative AI systems."}}
{"id": "2511.00029", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00029", "abs": "https://arxiv.org/abs/2511.00029", "authors": ["Samaksh Bhargav", "Zining Zhu"], "title": "Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts", "comment": "12 pages, 6 figures", "summary": "Large Language Model (LLM) deployment requires guiding the LLM to recognize\nand not answer unsafe prompts while complying with safe prompts. Previous\nmethods for achieving this require adjusting model weights along with other\nexpensive procedures. While recent advances in Sparse Autoencoders (SAEs) have\nenabled interpretable feature extraction from LLMs, existing approaches lack\nsystematic feature selection methods and principled evaluation of\nsafety-utility tradeoffs. We explored using different steering features and\nsteering strengths using Sparse Auto Encoders (SAEs) to provide a solution.\nUsing an accurate and innovative contrasting prompt method with the\nAI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air\nBench eu-dataset to efficiently choose the best features in the model to steer,\nwe tested this method on Llama-3 8B. We conclude that using this method, our\napproach achieves an 18.9% improvement in safety performance while\nsimultaneously increasing utility by 11.1%, demonstrating that targeted SAE\nsteering can overcome traditional safety-utility tradeoffs when optimal\nfeatures are identified through principled selection methods.", "AI": {"tldr": "SAE steering with principled feature selection improves both safety (18.9%) and utility (11.1%) in LLMs, overcoming traditional tradeoffs.", "motivation": "Existing LLM safety methods require expensive weight adjustments, and current SAE approaches lack systematic feature selection and proper evaluation of safety-utility tradeoffs.", "method": "Used contrasting prompt method with AI-Generated Prompts Dataset to select optimal features for steering through Sparse Autoencoders (SAEs) on Llama-3 8B.", "result": "The approach achieved 18.9% improvement in safety performance while increasing utility by 11.1%.", "conclusion": "Targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods."}}
{"id": "2511.00039", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00039", "abs": "https://arxiv.org/abs/2511.00039", "authors": ["Krishna Kumar Neelakanta Pillai Santha Kumari Amma"], "title": "Graph-Attentive MAPPO for Dynamic Retail Pricing", "comment": null, "summary": "Dynamic pricing in retail requires policies that adapt to shifting demand\nwhile coordinating decisions across related products. We present a systematic\nempirical study of multi-agent reinforcement learning for retail price\noptimization, comparing a strong MAPPO baseline with a\ngraph-attention-augmented variant (MAPPO+GAT) that leverages learned\ninteractions among products. Using a simulated pricing environment derived from\nreal transaction data, we evaluate profit, stability across random seeds,\nfairness across products, and training efficiency under a standardized\nevaluation protocol. The results indicate that MAPPO provides a robust and\nreproducible foundation for portfolio-level price control, and that MAPPO+GAT\nfurther enhances performance by sharing information over the product graph\nwithout inducing excessive price volatility. These results indicate that\ngraph-integrated MARL provides a more scalable and stable solution than\nindependent learners for dynamic retail pricing, offering practical advantages\nin multi-product decision-making.", "AI": {"tldr": "Multi-agent reinforcement learning with graph attention (MAPPO+GAT) outperforms baseline MAPPO for dynamic retail pricing by enabling product interaction modeling, improving profit and stability.", "motivation": "Retail dynamic pricing requires adaptive policies that coordinate decisions across related products, needing scalable solutions for multi-product decision-making.", "method": "Systematic empirical study comparing MAPPO baseline with graph-attention-augmented variant (MAPPO+GAT) using simulated pricing environment from real transaction data.", "result": "MAPPO+GAT enhances performance by sharing information over product graph without excessive price volatility, providing better profit, stability, fairness, and training efficiency.", "conclusion": "Graph-integrated MARL offers more scalable and stable solution than independent learners for dynamic retail pricing, with practical advantages in multi-product decision-making."}}
{"id": "2511.01078", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.01078", "abs": "https://arxiv.org/abs/2511.01078", "authors": ["Qinwei Huang", "Stefan Wang", "Simon Khan", "Garrett Katz", "Qinru Qiu"], "title": "Predictive Auxiliary Learning for Belief-based Multi-Agent Systems", "comment": null, "summary": "The performance of multi-agent reinforcement learning (MARL) in partially\nobservable environments depends on effectively aggregating information from\nobservations, communications, and reward signals. While most existing\nmulti-agent systems primarily rely on rewards as the only feedback for policy\ntraining, our research shows that introducing auxiliary predictive tasks can\nsignificantly enhance learning efficiency and stability. We propose\nBelief-based Predictive Auxiliary Learning (BEPAL), a framework that\nincorporates auxiliary training objectives to support policy optimization.\nBEPAL follows the centralized training with decentralized execution paradigm.\nEach agent learns a belief model that predicts unobservable state information,\nsuch as other agents' rewards or motion directions, alongside its policy model.\nBy enriching hidden state representations with information that does not\ndirectly contribute to immediate reward maximization, this auxiliary learning\nprocess stabilizes MARL training and improves overall performance. We evaluate\nBEPAL in the predator-prey environment and Google Research Football, where it\nachieves an average improvement of about 16 percent in performance metrics and\ndemonstrates more stable convergence compared to baseline methods.", "AI": {"tldr": "BEPAL framework improves multi-agent reinforcement learning by adding predictive auxiliary tasks to stabilize training and boost performance by ~16%.", "motivation": "Existing MARL systems rely mainly on rewards, limiting performance in partially observable environments. The paper aims to enhance learning by incorporating auxiliary predictive tasks.", "method": "Proposes Belief-based Predictive Auxiliary Learning (BEPAL), where each agent learns a belief model to predict unobservable state info (e.g., other agents' rewards) alongside its policy, using centralized training with decentralized execution.", "result": "Evaluation in predator-prey and Google Research Football shows ~16% average performance improvement and more stable convergence compared to baselines.", "conclusion": "Auxiliary predictive tasks effectively stabilize MARL training and improve performance in partially observable environments."}}
{"id": "2511.00030", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00030", "abs": "https://arxiv.org/abs/2511.00030", "authors": ["Myeongseob Ko", "Hoang Anh Just", "Charles Fleming", "Ming Jin", "Ruoxi Jia"], "title": "Probing Knowledge Holes in Unlearned LLMs", "comment": "The Thirty-ninth Annual Conference on Neural Information Processing\n  Systems", "summary": "Machine unlearning has emerged as a prevalent technical solution for\nselectively removing unwanted knowledge absorbed during pre-training, without\nrequiring full retraining. While recent unlearning techniques can effectively\nremove undesirable content without severely compromising performance on\nstandard benchmarks, we find that they may inadvertently create ``knowledge\nholes'' -- unintended losses of benign knowledge that standard benchmarks fail\nto capture. To probe where unlearned models reveal knowledge holes, we propose\na test case generation framework that explores both immediate neighbors of\nunlearned content and broader areas of potential failures. Our evaluation\ndemonstrates significant hidden costs of unlearning: up to 98.7\\% of the test\ncases yield irrelevant or nonsensical responses from unlearned models, despite\nbeing answerable by the pretrained model. These findings necessitate rethinking\nthe conventional approach to evaluating knowledge preservation in unlearning,\nmoving beyond standard, static benchmarks.", "AI": {"tldr": "Paper finds machine unlearning creates 'knowledge holes' - unintended loss of benign knowledge not captured by standard benchmarks. Proposes test framework revealing up to 98.7% failure rate in unlearned models.", "motivation": "Current unlearning techniques effectively remove unwanted content but may inadvertently remove benign knowledge, creating vulnerabilities not detected by standard evaluation methods.", "method": "Proposes a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures to detect knowledge holes.", "result": "Evaluation shows significant hidden costs: up to 98.7% of test cases yield irrelevant/nonsensical responses from unlearned models, despite being answerable by the pretrained model.", "conclusion": "Conventional evaluation approaches for knowledge preservation in unlearning are inadequate; need to move beyond standard static benchmarks to address knowledge hole vulnerabilities."}}
{"id": "2511.00048", "categories": ["cs.AI", "cs.CY", "62-11", "E.5; G.3; I.6.4; I.6.6; J.3; J.4"], "pdf": "https://arxiv.org/pdf/2511.00048", "abs": "https://arxiv.org/abs/2511.00048", "authors": ["Martin Bicher", "Maximilian Viehauser", "Daniele Giannandrea", "Hannah Kastinger", "Dominik Brunmeir", "Claire Rippinger", "Christoph Urach", "Niki Popper"], "title": "GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0", "comment": "134 pages, 75 figures, 19 tables", "summary": "GEPOC, short for Generic Population Concept, is a collection of models and\nmethods for analysing population-level research questions. For the valid\napplication of the models for a specific country or region, stable and\nreproducible data processes are necessary, which provide valid and ready-to-use\nmodel parameters. This work contains a complete description of the\ndata-processing methods for computation of model parameters for Austria, based\nexclusively on freely and publicly accessible data. In addition to the\ndescription of the source data used, this includes all algorithms used for\naggregation, disaggregation, fusion, cleansing or scaling of the data, as well\nas a description of the resulting parameter files. The document places\nparticular emphasis on the computation of parameters for the most important\nGEPOC model, GEPOC ABM, a continuous-time agent-based population model. An\nextensive validation study using this particular model was made and is\npresented at the end of this work.", "AI": {"tldr": "GEPOC is a framework for population analysis requiring validated data processes. This paper details data-processing methods for Austria using publicly available data and validates the GEPOC ABM model.", "motivation": "To enable valid application of GEPOC models for specific regions like Austria by establishing reproducible data processes using accessible data.", "method": "Describes algorithms for data aggregation, disaggregation, fusion, cleansing, and scaling to compute model parameters exclusively from free public data.", "result": "Complete parameter files for Austria and an extensive validation study of the GEPOC ABM model are produced.", "conclusion": "The work provides a robust data-processing framework for GEPOC applications, validated through the ABM model, enhancing reproducibility in population research."}}
{"id": "2511.01136", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.01136", "abs": "https://arxiv.org/abs/2511.01136", "authors": ["Enbo Sun", "Yongzhao Wang", "Hao Zhou"], "title": "Credit Network Modeling and Analysis via Large Language Models", "comment": "8 pages, 5 figures, 4 tables", "summary": "We investigate the application of large language models (LLMs) to construct\ncredit networks from firms' textual financial statements and to analyze the\nresulting network structures. We start with using LLMs to translate each firm's\nfinancial statement into a credit network that pertains solely to that firm.\nThese networks are then aggregated to form a comprehensive credit network\nrepresenting the whole financial system. During this process, the\ninconsistencies in financial statements are automatically detected and human\nintervention is involved. We demonstrate that this translation process is\neffective across financial statements corresponding to credit networks with\ndiverse topological structures. We further investigate the reasoning\ncapabilities of LLMs in analyzing credit networks and determining optimal\nstrategies for executing financial operations to maximize network performance\nmeasured by the total assets of firms, which is an inherently combinatorial\noptimization challenge. To demonstrate this capability, we focus on two\nfinancial operations: portfolio compression and debt removal, applying them to\nboth synthetic and real-world datasets. Our findings show that LLMs can\ngenerate coherent reasoning and recommend effective executions of these\noperations to enhance overall network performance.", "AI": {"tldr": "Using LLMs to convert financial statements into credit networks and optimize financial operations through reasoning", "motivation": "Traditional methods struggle with analyzing complex financial relationships from textual data; LLMs offer potential for automated network construction and optimization", "method": "Translate firm financial statements into individual credit networks using LLMs, aggregate networks, detect inconsistencies with human intervention, then apply LLM reasoning for portfolio compression and debt removal optimization", "result": "LLMs effectively constructed credit networks across diverse topologies and generated coherent reasoning to optimize financial operations, improving network performance on both synthetic and real datasets", "conclusion": "LLMs show strong capability in financial network analysis and optimization, offering promising automation for complex financial decision-making"}}
{"id": "2511.00032", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00032", "abs": "https://arxiv.org/abs/2511.00032", "authors": ["Lei Liu", "Zhongyi Yu", "Hong Wang", "Huanshuo Dong", "Haiyang Xin", "Hongwei Zhao", "Bin Li"], "title": "From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators", "comment": null, "summary": "In recent years, Neural Operators(NO) have gradually emerged as a popular\napproach for solving Partial Differential Equations (PDEs). However, their\napplication to large-scale engineering tasks suffers from significant\ncomputational overhead. And the fact that current models impose a uniform\ncomputational cost while physical fields exhibit vastly different complexities\nconstitutes a fundamental mismatch, which is the root of this inefficiency. For\ninstance, in turbulence flows, intricate vortex regions require deeper network\nprocessing compared to stable flows. To address this, we introduce a framework:\nSkip-Block Routing (SBR), a general framework designed for Transformer-based\nneural operators, capable of being integrated into their multi-layer\narchitectures. First, SBR uses a routing mechanism to learn the complexity and\nranking of tokens, which is then applied during inference. Then, in later\nlayers, it decides how many tokens are passed forward based on this ranking.\nThis way, the model focuses more processing capacity on the tokens that are\nmore complex. Experiments demonstrate that SBR is a general framework that\nseamlessly integrates into various neural operators. Our method reduces\ncomputational cost by approximately 50% in terms of Floating Point Operations\n(FLOPs), while still delivering up to 2x faster inference without sacrificing\naccuracy.", "AI": {"tldr": "Skip-Block Routing (SBR) framework reduces computational costs by 50% in neural operators for PDEs by adaptively processing tokens based on complexity.", "motivation": "Current neural operators for PDEs impose uniform computational cost despite varying complexity in physical fields, leading to inefficiency in large-scale engineering tasks.", "method": "SBR uses a routing mechanism to learn token complexity rankings and selectively processes tokens in later layers, focusing computational resources on more complex regions.", "result": "SBR reduces FLOPs by approximately 50% while achieving up to 2x faster inference without accuracy loss across various neural operator architectures.", "conclusion": "SBR provides an effective adaptive computation framework that maintains accuracy while significantly improving efficiency for transformer-based neural operators solving PDEs."}}
{"id": "2511.00092", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research.", "AI": {"tldr": "QuantumBench is a new benchmark with 800 multiple-choice questions to evaluate how well LLMs understand quantum science concepts.", "motivation": "General-purpose LLM benchmarks don't adequately test domain-specific knowledge in complex fields like quantum science which requires advanced mathematics and deals with non-intuitive phenomena.", "method": "Created approximately 800 multiple-choice questions with answers spanning nine quantum science areas using publicly available materials, organized into an eight-option format.", "result": "Evaluated several existing LLMs using QuantumBench and analyzed their quantum domain performance, including sensitivity to question format changes.", "conclusion": "QuantumBench is the first quantum-specific LLM evaluation dataset and aims to guide effective LLM use in quantum research by providing proper domain evaluation."}}
{"id": "2511.01310", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.01310", "abs": "https://arxiv.org/abs/2511.01310", "authors": ["Sureyya Akin", "Kavita Srivastava", "Prateek B. Kapoor", "Pradeep G. Sethi", "Sunita Q. Patel", "Rahu Srivastava"], "title": "From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models", "comment": null, "summary": "Learning cooperative multi-agent policies directly from high-dimensional,\nmultimodal sensory inputs like pixels and audio (from pixels) is notoriously\nsample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)\nalgorithms struggle with the joint challenge of representation learning,\npartial observability, and credit assignment. To address this, we propose a\nnovel framework based on a shared, generative Multimodal World Model (MWM). Our\nMWM is trained to learn a compressed latent representation of the environment's\ndynamics by fusing distributed, multimodal observations from all agents using a\nscalable attention-based mechanism. Subsequently, we leverage this learned MWM\nas a fast, \"imagined\" simulator to train cooperative MARL policies (e.g.,\nMAPPO) entirely within its latent space, decoupling representation learning\nfrom policy learning. We introduce a new set of challenging multimodal,\nmulti-agent benchmarks built on a 3D physics simulator. Our experiments\ndemonstrate that our MWM-MARL framework achieves orders-of-magnitude greater\nsample efficiency compared to state-of-the-art model-free MARL baselines. We\nfurther show that our proposed multimodal fusion is essential for task success\nin environments with sensory asymmetry and that our architecture provides\nsuperior robustness to sensor-dropout, a critical feature for real-world\ndeployment.", "AI": {"tldr": "Proposes MWM-MARL framework using generative multimodal world models for sample-efficient multi-agent reinforcement learning from pixels/audio, achieving orders-of-magnitude improvement over baselines.", "motivation": "Address sample inefficiency in learning multi-agent policies directly from high-dimensional multimodal inputs (pixels, audio), overcoming challenges of representation learning, partial observability, and credit assignment.", "method": "Train shared generative Multimodal World Model (MWM) using attention-based fusion of distributed multimodal observations; use MWM as latent simulator to train MARL policies (e.g., MAPPO) decoupled from representation learning.", "result": "MWM-MARL achieves orders-of-magnitude higher sample efficiency than state-of-the-art model-free MARL; multimodal fusion essential for sensory asymmetry tasks; provides superior robustness to sensor dropout.", "conclusion": "MWM-MARL framework effectively addresses multimodal MARL challenges, demonstrating significant sample efficiency gains and robustness, promising for real-world deployment."}}
{"id": "2511.00035", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00035", "abs": "https://arxiv.org/abs/2511.00035", "authors": ["Georg Velev", "Stefan Lessmann"], "title": "Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series", "comment": null, "summary": "The dynamic energy sector requires both predictive accuracy and runtime\nefficiency for short-term forecasting of energy generation under operational\nconstraints, where timely and precise predictions are crucial. The manual\nconfiguration of complex methods, which can generate accurate global multi-step\npredictions without suffering from a computational bottleneck, represents a\nprocedure with significant time requirements and high risk for human-made\nerrors. A further intricacy arises from the temporal dynamics present in\nenergy-related data. Additionally, the generalization to unseen data is\nimperative for continuously deploying forecasting techniques over time. To\novercome these challenges, in this research, we design a neural architecture\nsearch (NAS)-based framework for the automated discovery of time series models\nthat strike a balance between computational efficiency, predictive performance,\nand generalization power for the global, multi-step short-term forecasting of\nenergy production time series. In particular, we introduce a search space\nconsisting only of efficient components, which can capture distinctive patterns\nof energy time series. Furthermore, we formulate a novel objective function\nthat accounts for performance generalization in temporal context and the\nmaximal exploration of different regions of our high-dimensional search space.\nThe results obtained on energy production time series show that an ensemble of\nlightweight architectures discovered with NAS outperforms state-of-the-art\ntechniques, such as Transformers, as well as pre-trained forecasting models, in\nterms of both efficiency and accuracy.", "AI": {"tldr": "A neural architecture search framework for automated discovery of efficient time series models for energy production forecasting, balancing computational efficiency, predictive performance, and generalization.", "motivation": "The energy sector needs accurate and efficient short-term forecasting methods that avoid manual configuration errors, handle temporal dynamics, and generalize well to unseen data.", "method": "Designed a NAS-based framework with efficient components to capture energy time series patterns, using a novel objective function that considers temporal generalization and search space exploration.", "result": "Ensemble of lightweight NAS-discovered architectures outperforms state-of-the-art methods like Transformers and pre-trained models in both efficiency and accuracy on energy production time series.", "conclusion": "The proposed NAS framework successfully automates model discovery for energy forecasting, achieving superior performance-efficiency trade-off compared to existing approaches."}}
{"id": "2511.00122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00122", "abs": "https://arxiv.org/abs/2511.00122", "authors": ["Ran Xu", "Yupeng Qi", "Jingsen Feng", "Xu Chu"], "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "comment": null, "summary": "In modern engineering practice, human engineers collaborate in specialized\nteams to design complex products, with each expert completing their respective\ntasks while communicating and exchanging results and data with one another.\nWhile this division of expertise is essential for managing multidisciplinary\ncomplexity, it demands substantial development time and cost. Recently, we\nintroduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer\nfor computational fluid dynamics, and turbulence.ai, which can conduct\nend-to-end research in fluid mechanics draft publications and PhD theses.\nBuilding upon these foundations, we present Engineering.ai, a platform for\nteams of AI engineers in computational design. The framework employs a\nhierarchical multi-agent architecture where a Chief Engineer coordinates\nspecialized agents consisting of Aerodynamics, Structural, Acoustic, and\nOptimization Engineers, each powered by LLM with domain-specific knowledge.\nAgent-agent collaboration is achieved through file-mediated communication for\ndata provenance and reproducibility, while a comprehensive memory system\nmaintains project context, execution history, and retrieval-augmented domain\nknowledge to ensure reliable decision-making across the workflow. The system\nintegrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,\nenabling parallel multidisciplinary simulations while maintaining computational\naccuracy. The framework is validated through UAV wing optimization. This work\ndemonstrates that agentic-AI-enabled AI engineers has the potential to perform\ncomplex engineering tasks autonomously. Remarkably, the automated workflow\nachieved a 100% success rate across over 400 parametric configurations, with\nzero mesh generation failures, solver convergence issues, or manual\ninterventions required, validating that the framework is trustworthy.", "AI": {"tldr": "Engineering.ai is a multi-agent AI platform that autonomously performs complex engineering design tasks using specialized AI engineers coordinated by a Chief Engineer, achieving 100% success rate in UAV wing optimization.", "motivation": "Traditional engineering teams require substantial time/cost for multidisciplinary collaboration. Existing AI tools like OpenFOAMGPT show promise but need expansion to full engineering workflow automation.", "method": "Hierarchical multi-agent architecture with Chief Engineer coordinating specialized agents (Aerodynamics, Structural, Acoustic, Optimization) using LLMs. File-mediated communication ensures data provenance, with memory system maintaining project context.", "result": "Validated through UAV wing optimization: 100% success rate across 400+ configurations, zero mesh failures/convergence issues, no manual interventions required.", "conclusion": "Agentic AI engineers can autonomously perform complex engineering tasks reliably, demonstrating trustworthy automation potential for multidisciplinary design."}}
{"id": "2511.01489", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.01489", "abs": "https://arxiv.org/abs/2511.01489", "authors": ["Qurat-ul-ain Shaheen", "Katarzyna Budzynska", "Carles Sierra"], "title": "An Explanation-oriented Inquiry Dialogue Game for Expert Collaborative Recommendations", "comment": null, "summary": "This work presents a requirement analysis for collaborative dialogues among\nmedical experts and an inquiry dialogue game based on this analysis for\nincorporating explainability into multiagent system design. The game allows\nexperts with different knowledge bases to collaboratively make recommendations\nwhile generating rich traces of the reasoning process through combining\nexplanation-based illocutionary forces in an inquiry dialogue. The dialogue\ngame was implemented as a prototype web-application and evaluated against the\nspecification through a formative user study. The user study confirms that the\ndialogue game meets the needs for collaboration among medical experts. It also\nprovides insights on the real-life value of dialogue-based communication tools\nfor the medical community.", "AI": {"tldr": "A dialogue game for medical experts to collaboratively make recommendations while generating explainable reasoning traces through inquiry dialogues.", "motivation": "To incorporate explainability into multiagent system design for medical experts who need to collaborate despite having different knowledge bases.", "method": "Developed an inquiry dialogue game with explanation-based illocutionary forces, implemented as a prototype web-application, and evaluated through a formative user study.", "result": "The user study confirmed that the dialogue game meets collaboration needs among medical experts and provided insights on the value of dialogue-based communication tools in medical practice.", "conclusion": "The inquiry dialogue game successfully enables collaborative recommendation-making while generating rich reasoning traces, demonstrating real-life value for the medical community."}}
{"id": "2511.00040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00040", "abs": "https://arxiv.org/abs/2511.00040", "authors": ["Seonggyun Lee", "Sungjun Lim", "Seojin Park", "Soeun Cheon", "Kyungwoo Song"], "title": "Semi-Supervised Preference Optimization with Limited Feedback", "comment": null, "summary": "The field of preference optimization has made outstanding contributions to\nthe alignment of language models with human preferences. Despite these\nadvancements, recent methods still rely heavily on substantial paired (labeled)\nfeedback data, leading to substantial resource expenditures. To address these\nchallenges, we study the problem of Semi-Supervised Preference Optimization\n(SSPO) in which the idea is to learn from both a small number of pairwise\npreference labels and a large pool of unpaired samples simultaneously. Our key\ntheoretical contribution proves the existence of an optimal reward threshold\ncapable of separating winning and losing responses with high probability, which\nenables a principled pseudo-labeling of unpaired data. By leveraging these\npseudo-labels, SSPO effectively distills latent preferences from large-scale\nunpaired data, thus maintaining human alignment while drastically reducing\nacquisition costs. Extensive experiments across datasets validate this\nremarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct\non just 1% of UltraFeedback consistently surpasses strong baselines trained on\n10% of UltraFeedback.", "AI": {"tldr": "SSPO introduces semi-supervised preference optimization that uses limited labeled data paired with unlabeled data to reduce resource costs while maintaining human alignment.", "motivation": "Current preference optimization methods require substantial labeled feedback data, leading to high resource expenditures.", "method": "Uses theoretical reward threshold to pseudo-label unpaired data, then leverages pseudo-labels to distill latent preferences from unlabeled data.", "result": "SSPO achieves better performance with only 1% of UltraFeedback data compared to baselines using 10% data.", "conclusion": "SSPO provides effective semi-supervised approach that substantially reduces data requirements while maintaining alignment quality."}}
{"id": "2511.00162", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00162", "abs": "https://arxiv.org/abs/2511.00162", "authors": ["Michael D. Moffitt"], "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "comment": null, "summary": "The Abstraction and Reasoning Corpus remains one of the most compelling and\nchallenging benchmarks for tracking progress toward achieving Artificial\nGeneral Intelligence. In contrast to other evaluation datasets designed to\nassess an agent's task-specific skills or accumulated knowledge, the ARC-AGI\nsuite is specifically targeted at measuring skill acquisition efficiency, a\ntrait that has (so far) been lacking in even the most sophisticated machine\nlearning systems. For algorithms that require extensive intra-task exemplars, a\nsignificant constraint imposed by ARC-AGI is the modest cardinality of its\ndemonstration set, comprising a small number of $\\langle$ input, output\n$\\rangle$ grids per task specifying the corresponding transformation. To\nembellish the space of viable sample pairs, this paper introduces ARC-GEN, an\nopen-source procedural generator aimed at extending the original ARC-AGI\ntraining dataset as faithfully as possible. Unlike prior efforts, our generator\nis both exhaustive (covering all four-hundred tasks) and mimetic (more closely\nhonoring the distributional properties and characteristics embodied in the\ninitial ARC-AGI-1 release). We also discuss the use of this generator in\nestablishing a static benchmark suite to verify the correctness of programs\nsubmitted to the 2025 Google Code Golf Championship.", "AI": {"tldr": "ARC-GEN is an open-source procedural generator that extends the ARC-AGI training dataset to address its limited demonstration set, creating faithful synthetic examples for all 400 tasks to improve skill acquisition efficiency evaluation.", "motivation": "The ARC-AGI benchmark measures skill acquisition efficiency but has limited demonstration sets with only a few input-output pairs per task, which constrains algorithms requiring extensive intra-task exemplars.", "method": "Developed ARC-GEN, an exhaustive and mimetic procedural generator that covers all 400 ARC-AGI tasks and closely honors the distributional properties of the original dataset to create viable sample pairs.", "result": "Created an extended training dataset that faithfully reproduces ARC-AGI's characteristics, enabling better evaluation of skill acquisition efficiency and serving as a static benchmark for program verification.", "conclusion": "ARC-GEN successfully addresses the limited demonstration problem in ARC-AGI by generating faithful synthetic examples, making it valuable for AGI research and as a verification benchmark for programming competitions."}}
{"id": "2511.01554", "categories": ["cs.MA", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.01554", "abs": "https://arxiv.org/abs/2511.01554", "authors": ["Aditya Kapoor", "Yash Bhisikar", "Benjamin Freed", "Jan Peters", "Mingfei Sun"], "title": "Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning", "comment": "30 pages, 12 figures, 6 tables", "summary": "Effective communication in multi-agent reinforcement learning (MARL) is\ncritical for success but constrained by bandwidth, yet past approaches have\nbeen limited to complex gating mechanisms that only decide \\textit{whether} to\ncommunicate, not \\textit{how precisely}. Learning to optimize message precision\nat the bit-level is fundamentally harder, as the required discretization step\nbreaks gradient flow. We address this by generalizing Differentiable Discrete\nCommunication Learning (DDCL), a framework for end-to-end optimization of\ndiscrete messages. Our primary contribution is an extension of DDCL to support\nunbounded signals, transforming it into a universal, plug-and-play layer for\nany MARL architecture. We verify our approach with three key results. First,\nthrough a qualitative analysis in a controlled environment, we demonstrate\n\\textit{how} agents learn to dynamically modulate message precision according\nto the informational needs of the task. Second, we integrate our variant of\nDDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth\nby over an order of magnitude while matching or exceeding task performance.\nFinally, we provide direct evidence for the \\enquote{Bitter Lesson} in MARL\ncommunication: a simple Transformer-based policy leveraging DDCL matches the\nperformance of complex, specialized architectures, questioning the necessity of\nbespoke communication designs.", "AI": {"tldr": "Extends Differentiable Discrete Communication Learning (DDCL) to enable bit-level message precision optimization in MARL, achieving significant bandwidth reduction while maintaining performance.", "motivation": "Current MARL communication approaches only decide whether to communicate, not how precisely, and bit-level optimization is challenging due to gradient flow disruption from discretization.", "method": "Generalizes DDCL framework to support unbounded signals, creating a universal plug-and-play layer for MARL architectures that enables end-to-end optimization of discrete messages.", "result": "Reduces bandwidth by over an order of magnitude while matching or exceeding task performance; agents learn dynamic precision modulation; simple Transformer+DDCL matches complex specialized architectures.", "conclusion": "DDCL extension provides effective bit-level communication optimization, questioning the need for complex bespoke communication designs in MARL (supporting the 'Bitter Lesson')."}}
{"id": "2511.00043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00043", "abs": "https://arxiv.org/abs/2511.00043", "authors": ["Tyrus Whitman", "Andrew Particka", "Christopher Diers", "Ian Griffin", "Charuka Wickramasinghe", "Pradeep Ranaweera"], "title": "Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations", "comment": "21 pages, 10 figures, 5 tables", "summary": "In this study, we present and validate the predictive capability of the\nPhysics-Informed Neural Networks (PINNs) methodology for solving a variety of\nengineering and biological dynamical systems governed by ordinary differential\nequations (ODEs). While traditional numerical methods a re effective for many\nODEs, they often struggle to achieve convergence in problems involving high\nstiffness, shocks, irregular domains, singular perturbations, high dimensions,\nor boundary discontinuities. Alternatively, PINNs offer a powerful approach for\nhandling challenging numerical scenarios. In this study, classical ODE problems\nare employed as controlled testbeds to systematically evaluate the accuracy,\ntraining efficiency, and generalization capability under controlled conditions\nof the PINNs framework. Although not a universal solution, PINNs can achieve\nsuperior results by embedding physical laws directly into the learning process.\nWe first analyze the existence and uniqueness properties of several benchmark\nproblems and subsequently validate the PINNs methodology on these model\nsystems. Our results demonstrate that for complex problems to converge to\ncorrect solutions, the loss function components data loss, initial condition\nloss, and residual loss must be appropriately balanced through careful\nweighting. We further establish that systematic tuning of hyperparameters,\nincluding network depth, layer width, activation functions, learning rate,\noptimization algorithms, w eight initialization schemes, and collocation point\nsampling, plays a crucial role in achieving accurate solutions. Additionally,\nembedding prior knowledge and imposing hard constraints on the network\narchitecture, without loss the generality of the ODE system, significantly\nenhances the predictive capability of PINNs.", "AI": {"tldr": "PINNs provide a powerful alternative to traditional numerical methods for solving challenging ODE problems with high stiffness, shocks, irregular domains, and other complexities by embedding physical laws directly into neural networks.", "motivation": "Traditional numerical methods struggle with ODEs involving high stiffness, shocks, irregular domains, singular perturbations, high dimensions, or boundary discontinuities, necessitating alternative approaches.", "method": "Physics-Informed Neural Networks (PINNs) that embed physical laws into the learning process, with careful balancing of loss function components (data loss, initial condition loss, residual loss) and systematic hyperparameter tuning.", "result": "PINNs achieve superior results for complex ODE problems when loss function components are appropriately balanced and hyperparameters are systematically tuned, with embedding prior knowledge and hard constraints significantly enhancing predictive capability.", "conclusion": "PINNs are not a universal solution but can achieve superior performance for challenging ODE problems through proper loss balancing, hyperparameter optimization, and incorporation of physical constraints, offering a powerful alternative to traditional numerical methods."}}
{"id": "2511.00194", "categories": ["cs.AI", "F.2.2, F.4.1"], "pdf": "https://arxiv.org/pdf/2511.00194", "abs": "https://arxiv.org/abs/2511.00194", "authors": ["Jovial Cheukam Ngouonou", "Ramiz Gindullin", "Claude-Guy Quimper", "Nicolas Beldiceanu", "Remi Douence"], "title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures", "comment": null, "summary": "We present an improved incremental selection algorithm of the selection\nalgorithm presented in [1] and prove all the selected conjectures.", "AI": {"tldr": "Improved incremental selection algorithm that successfully proves all conjectures", "motivation": "To enhance the existing selection algorithm from previous work [1]", "method": "Developed an improved incremental selection algorithm", "result": "Successfully proved all the selected conjectures", "conclusion": "The improved algorithm effectively addresses the limitations of the previous approach"}}
{"id": "2511.00116", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00116", "abs": "https://arxiv.org/abs/2511.00116", "authors": ["Avisek Naug", "Antonio Guillen", "Vineet Kumar", "Scott Greenwood", "Wesley Brewer", "Sahand Ghorbanpour", "Ashwin Ramesh Babu", "Vineet Gundecha", "Ricardo Luna Gutierrez", "Soumyendu Sarkar"], "title": "LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers", "comment": "Submitted to the NeurIPS 2025 conference", "summary": "Liquid cooling is critical for thermal management in high-density data\ncenters with the rising AI workloads. However, machine learning-based\ncontrollers are essential to unlock greater energy efficiency and reliability,\npromoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)\nbenchmark environment, for reinforcement learning (RL) control strategies in\nenergy-efficient liquid cooling of high-performance computing (HPC) systems.\nBuilt on the baseline of a high-fidelity digital twin of Oak Ridge National\nLab's Frontier Supercomputer cooling system, LC-Opt provides detailed\nModelica-based end-to-end models spanning site-level cooling towers to data\ncenter cabinets and server blade groups. RL agents optimize critical thermal\ncontrols like liquid supply temperature, flow rate, and granular valve\nactuation at the IT cabinet level, as well as cooling tower (CT) setpoints\nthrough a Gymnasium interface, with dynamic changes in workloads. This\nenvironment creates a multi-objective real-time optimization challenge\nbalancing local thermal regulation and global energy efficiency, and also\nsupports additional components like a heat recovery unit (HRU). We benchmark\ncentralized and decentralized multi-agent RL approaches, demonstrate policy\ndistillation into decision and regression trees for interpretable control, and\nexplore LLM-based methods that explain control actions in natural language\nthrough an agentic mesh architecture designed to foster user trust and simplify\nsystem management. LC-Opt democratizes access to detailed, customizable liquid\ncooling models, enabling the ML community, operators, and vendors to develop\nsustainable data center liquid cooling control solutions.", "AI": {"tldr": "LC-Opt is a reinforcement learning benchmark environment for optimizing liquid cooling control in high-performance computing systems, enabling energy-efficient thermal management through ML-based controllers.", "motivation": "Liquid cooling is essential for thermal management in high-density data centers with increasing AI workloads, but machine learning controllers are needed to achieve greater energy efficiency, reliability, and sustainability.", "method": "Built on a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models. It uses reinforcement learning agents to optimize thermal controls like liquid supply temperature, flow rate, valve actuation, and cooling tower setpoints through a Gymnasium interface.", "result": "The environment creates a multi-objective real-time optimization challenge balancing thermal regulation and energy efficiency. It benchmarks centralized/decentralized multi-agent RL approaches, demonstrates policy distillation into interpretable decision trees, and explores LLM-based methods for explainable control actions.", "conclusion": "LC-Opt democratizes access to detailed liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions that foster user trust and simplify system management."}}
{"id": "2511.00044", "categories": ["cs.LG", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2511.00044", "abs": "https://arxiv.org/abs/2511.00044", "authors": ["Kohei Tsuchiyama", "Andre Roehm", "Takatomo Mihana", "Ryoichi Horisaki"], "title": "ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks", "comment": null, "summary": "Physical Neural Networks (PNN) are promising platforms for next-generation\ncomputing systems. However, recent advances in digital neural network\nperformance are largely driven by the rapid growth in the number of trainable\nparameters and, so far, demonstrated PNNs are lagging behind by several orders\nof magnitude in terms of scale. This mirrors size and performance constraints\nfound in early digital neural networks. In that period, efficient reuse of\nparameters contributed to the development of parameter-efficient architectures\nsuch as convolutional neural networks.\n  In this work, we numerically investigate hardware-friendly weight-tying for\nPNNs. Crucially, with many PNN systems, there is a time-scale separation\nbetween the fast dynamic active elements of the forward pass and the only\nslowly trainable elements implementing weights and biases. With this in mind,we\npropose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)\narchitecture, which employs a simple layer-by-layer time-multiplexing scheme to\nincrease the effective network depth and efficiently use the number of\nparameters. We only require the addition of fast switches for existing PNNs. We\nvalidate ReLaX-Nets via numerical experiments on image classification and\nnatural language processing tasks. Our results show that ReLaX-Net improves\ncomputational performance with only minor modifications to a conventional PNN.\nWe observe a favorable scaling, where ReLaX-Nets exceed the performance of\nequivalent traditional RNNs or DNNs with the same number of parameters.", "AI": {"tldr": "ReLaX-Net enables Physical Neural Networks (PNNs) to achieve better performance with fewer parameters by reusing layers through time-multiplexing, requiring only simple hardware modifications.", "motivation": "Physical Neural Networks lag behind digital neural networks in scale and performance due to limited trainable parameters, mirroring early digital network constraints that were overcome through parameter-efficient architectures.", "method": "Proposes ReLaX-Net architecture using layer-by-layer time-multiplexing to increase effective network depth and efficiently reuse parameters, requiring only fast switches for existing PNNs.", "result": "ReLaX-Nets show improved computational performance on image classification and NLP tasks, with favorable scaling that exceeds equivalent traditional RNNs/DNNs with same parameter count.", "conclusion": "ReLaX-Net provides a hardware-friendly approach to enhance PNN performance through parameter reuse, addressing scale limitations with minimal modifications to existing systems."}}
{"id": "2511.00206", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00206", "abs": "https://arxiv.org/abs/2511.00206", "authors": ["Dirk U. Wulff", "Rui Mata"], "title": "Advancing Cognitive Science with LLMs", "comment": null, "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and\nconceptual clarity, in part due to its multifaceted and interdisciplinary\nnature. Recent advances in artificial intelligence, particularly the\ndevelopment of large language models (LLMs), offer tools that may help to\naddress these issues. This review examines how LLMs can support areas where the\nfield has historically struggled, including establishing cross-disciplinary\nconnections, formalizing theories, developing clear measurement taxonomies,\nachieving generalizability through integrated modeling frameworks, and\ncapturing contextual and individual variation. We outline the current\ncapabilities and limitations of LLMs in these domains, including potential\npitfalls. Taken together, we conclude that LLMs can serve as tools for a more\nintegrative and cumulative cognitive science when used judiciously to\ncomplement, rather than replace, human expertise.", "AI": {"tldr": "This review explores how LLMs can address challenges in cognitive science like knowledge synthesis and conceptual clarity through cross-disciplinary connections, theory formalization, and integrated modeling, while noting their limitations.", "motivation": "Cognitive science struggles with knowledge synthesis and conceptual clarity due to its interdisciplinary nature, and recent AI advances (LLMs) offer potential tools to mitigate these issues.", "method": "The review examines LLMs' applications in establishing cross-disciplinary links, formalizing theories, creating measurement taxonomies, achieving generalizability via modeling frameworks, and capturing contextual/individual variation.", "result": "LLMs show promise in supporting cognitive science but have limitations and potential pitfalls; they are most effective when complementing human expertise.", "conclusion": "LLMs can facilitate a more integrative and cumulative cognitive science if used judiciously as tools to augment, not replace, human input."}}
{"id": "2511.00117", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00117", "abs": "https://arxiv.org/abs/2511.00117", "authors": ["Antonio Guillen-Perez", "Avisek Naug", "Vineet Gundecha", "Sahand Ghorbanpour", "Ricardo Luna Gutierrez", "Ashwin Ramesh Babu", "Munther Salim", "Shubhanker Banerjee", "Eoin H. Oude Essink", "Damien Fay", "Soumyendu Sarkar"], "title": "DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads", "comment": "Submitted to the NeurIPS 2025 conference", "summary": "The increasing energy demands and carbon footprint of large-scale AI require\nintelligent workload management in globally distributed data centers. Yet\nprogress is limited by the absence of benchmarks that realistically capture the\ninterplay of time-varying environmental factors (grid carbon intensity,\nelectricity prices, weather), detailed data center physics (CPUs, GPUs, memory,\nHVAC energy), and geo-distributed network dynamics (latency and transmission\ncosts). To bridge this gap, we present DCcluster-Opt: an open-source,\nhigh-fidelity simulation benchmark for sustainable, geo-temporal task\nscheduling. DCcluster-Opt combines curated real-world datasets, including AI\nworkload traces, grid carbon intensity, electricity markets, weather across 20\nglobal regions, cloud transmission costs, and empirical network delay\nparameters with physics-informed models of data center operations, enabling\nrigorous and reproducible research in sustainable computing. It presents a\nchallenging scheduling problem where a top-level coordinating agent must\ndynamically reassign or defer tasks that arrive with resource and service-level\nagreement requirements across a configurable cluster of data centers to\noptimize multiple objectives. The environment also models advanced components\nsuch as heat recovery. A modular reward system enables an explicit study of\ntrade-offs among carbon emissions, energy costs, service level agreements, and\nwater use. It provides a Gymnasium API with baseline controllers, including\nreinforcement learning and rule-based strategies, to support reproducible ML\nresearch and a fair comparison of diverse algorithms. By offering a realistic,\nconfigurable, and accessible testbed, DCcluster-Opt accelerates the development\nand validation of next-generation sustainable computing solutions for\ngeo-distributed data centers.", "AI": {"tldr": "DCcluster-Opt is an open-source simulation benchmark for sustainable geo-temporal task scheduling in distributed data centers, combining real-world datasets with physics-informed models to enable research on optimizing carbon emissions, energy costs, and service level agreements.", "motivation": "The increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers, but progress is limited by the absence of realistic benchmarks that capture environmental factors, data center physics, and network dynamics.", "method": "DCcluster-Opt combines curated real-world datasets (AI workload traces, grid carbon intensity, electricity markets, weather across 20 regions, transmission costs, network delays) with physics-informed models of data center operations, providing a modular reward system and Gymnasium API with baseline controllers.", "result": "The benchmark presents a challenging scheduling problem where a coordinating agent must dynamically reassign or defer tasks across configurable data center clusters to optimize multiple objectives including carbon emissions, energy costs, SLAs, and water use.", "conclusion": "DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers by offering a realistic, configurable, and accessible testbed for reproducible research."}}
{"id": "2511.00047", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.00047", "abs": "https://arxiv.org/abs/2511.00047", "authors": ["Omkar Kulkarni", "Rohitash Chandra"], "title": "DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection", "comment": null, "summary": "Financial fraud detection is critical for maintaining the integrity of\nfinancial systems, particularly in decentralised environments such as\ncryptocurrency networks. Although Graph Convolutional Networks (GCNs) are\nwidely used for financial fraud detection, graph Transformer models such as\nGraph-BERT are gaining prominence due to their Transformer-based architecture,\nwhich mitigates issues such as over-smoothing. Graph-BERT is designed for\nstatic graphs and primarily evaluated on citation networks with undirected\nedges. However, financial transaction networks are inherently dynamic, with\nevolving structures and directed edges representing the flow of money. To\naddress these challenges, we introduce DynBERG, a novel architecture that\nintegrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture\ntemporal evolution over multiple time steps. Additionally, we modify the\nunderlying algorithm to support directed edges, making DynBERG well-suited for\ndynamic financial transaction analysis. We evaluate our model on the Elliptic\ndataset, which includes Bitcoin transactions, including all transactions during\na major cryptocurrency market event, the Dark Market Shutdown. By assessing\nDynBERG's resilience before and after this event, we analyse its ability to\nadapt to significant market shifts that impact transaction behaviours. Our\nmodel is benchmarked against state-of-the-art dynamic graph classification\napproaches, such as EvolveGCN and GCN, demonstrating superior performance,\noutperforming EvolveGCN before the market shutdown and surpassing GCN after the\nevent. Additionally, an ablation study highlights the critical role of\nincorporating a time-series deep learning component, showcasing the\neffectiveness of GRU in modelling the temporal dynamics of financial\ntransactions.", "AI": {"tldr": "DynBERG is a novel dynamic graph Transformer model that combines Graph-BERT with GRU to detect financial fraud in evolving cryptocurrency networks, outperforming existing methods on Bitcoin transaction data.", "motivation": "Financial fraud detection in dynamic cryptocurrency networks requires handling evolving structures and directed edges, which existing static graph models like Graph-BERT cannot adequately address.", "method": "Integrates Graph-BERT with a GRU layer to capture temporal evolution, modifies the algorithm to support directed edges, and evaluates on the Elliptic Bitcoin dataset across a major market event.", "result": "DynBERG outperforms state-of-the-art methods (EvolveGCN and GCN), showing superior performance before and after the Dark Market Shutdown event, with ablation studies confirming GRU's importance for temporal modeling.", "conclusion": "The proposed DynBERG architecture effectively addresses the dynamic nature of financial transaction networks and demonstrates robust fraud detection capabilities that adapt to significant market shifts."}}
{"id": "2511.00267", "categories": ["cs.AI", "cs.CY", "cs.GL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00267", "abs": "https://arxiv.org/abs/2511.00267", "authors": ["Christian Prothmann", "Vijay Gadepally", "Jeremy Kepner", "Koley Borchard", "Luca Carlone", "Zachary Folcik", "J. Daniel Grith", "Michael Houle", "Jonathan P. How", "Nathan Hughes", "Ifueko Igbinedion", "Hayden Jananthan", "Tejas Jayashankar", "Michael Jones", "Sertac Karaman", "Binoy G. Kurien", "Alejandro Lancho", "Giovanni Lavezzi", "Gary C. F. Lee", "Charles E. Leiserson", "Richard Linares", "Lindsey McEvoy", "Peter Michaleas", "Chasen Milner", "Alex Pentland", "Yury Polyanskiy", "Jovan Popovich", "Jeffrey Price", "Tim W. Reid", "Stephanie Riley", "Siddharth Samsi", "Peter Saunders", "Olga Simek", "Mark S. Veillette", "Amir Weiss", "Gregory W. Wornell", "Daniela Rus", "Scott T. Ruppel"], "title": "Advancing AI Challenges for the United States Department of the Air Force", "comment": "8 pages, 8 figures, 59 references. To appear in IEEE HPEC 2025", "summary": "The DAF-MIT AI Accelerator is a collaboration between the United States\nDepartment of the Air Force (DAF) and the Massachusetts Institute of Technology\n(MIT). This program pioneers fundamental advances in artificial intelligence\n(AI) to expand the competitive advantage of the United States in the defense\nand civilian sectors. In recent years, AI Accelerator projects have developed\nand launched public challenge problems aimed at advancing AI research in\npriority areas. Hallmarks of AI Accelerator challenges include large, publicly\navailable, and AI-ready datasets to stimulate open-source solutions and engage\nthe wider academic and private sector AI ecosystem. This article supplements\nour previous publication, which introduced AI Accelerator challenges. We\nprovide an update on how ongoing and new challenges have successfully\ncontributed to AI research and applications of AI technologies.", "AI": {"tldr": "Update on DAF-MIT AI Accelerator challenges, showcasing their role in advancing AI research through public datasets and open-source solutions.", "motivation": "To expand U.S. competitive advantage in defense and civilian sectors by pioneering fundamental AI advances through collaborative challenges.", "method": "Development and launch of public challenge problems with large, AI-ready datasets to engage academic and private sectors.", "result": "Successful contributions to AI research and applications, building on previous challenges.", "conclusion": "The AI Accelerator program effectively stimulates open-source AI advancements and ecosystem engagement."}}
{"id": "2511.00651", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.00651", "abs": "https://arxiv.org/abs/2511.00651", "authors": ["Chenhua Shi", "Bhavika Jalli", "Gregor Macdonald", "John Zou", "Wanlu Lei", "Mridul Jain", "Joji Philip"], "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "comment": "6 pages, 7 figures, 1 table", "summary": "Telecom networks are rapidly growing in scale and complexity, making\neffective management, operation, and optimization increasingly challenging.\nAlthough Artificial Intelligence (AI) has been applied to many telecom tasks,\nexisting models are often narrow in scope, require large amounts of labeled\ndata, and struggle to generalize across heterogeneous deployments.\nConsequently, network troubleshooting continues to rely heavily on Subject\nMatter Experts (SMEs) to manually correlate various data sources to identify\nroot causes and corrective actions. To address these limitations, we propose a\nMulti-Agent System (MAS) that employs an agentic workflow, with Large Language\nModels (LLMs) coordinating multiple specialized tools for fully automated\nnetwork troubleshooting. Once faults are detected by AI/ML-based monitors, the\nframework dynamically activates agents such as an orchestrator, solution\nplanner, executor, data retriever, and root-cause analyzer to diagnose issues\nand recommend remediation strategies within a short time frame. A key component\nof this system is the solution planner, which generates appropriate remediation\nplans based on internal documentation. To enable this, we fine-tuned a Small\nLanguage Model (SLM) on proprietary troubleshooting documents to produce\ndomain-grounded solution plans. Experimental results demonstrate that the\nproposed framework significantly accelerates troubleshooting automation across\nboth Radio Access Network (RAN) and Core network domains.", "AI": {"tldr": "A multi-agent system using LLMs automates telecom network troubleshooting by coordinating specialized agents to diagnose faults and generate remediation plans, with a fine-tuned SLM for domain-specific solution planning.", "motivation": "Telecom networks are growing in scale and complexity, making management challenging. Existing AI models are narrow in scope, require large labeled data, and struggle to generalize, forcing continued reliance on human experts for troubleshooting.", "method": "Proposed a Multi-Agent System (MAS) with LLMs coordinating specialized agents (orchestrator, solution planner, executor, data retriever, root-cause analyzer) for automated troubleshooting. Fine-tuned a Small Language Model on proprietary documents for domain-grounded solution planning.", "result": "The framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains, enabling rapid fault diagnosis and remediation strategy recommendations.", "conclusion": "The proposed multi-agent system with LLM coordination and fine-tuned SLM successfully addresses limitations of existing AI approaches in telecom networks, enabling fully automated troubleshooting that reduces reliance on human experts."}}
{"id": "2511.00049", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00049", "abs": "https://arxiv.org/abs/2511.00049", "authors": ["Yao Liu"], "title": "Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting", "comment": null, "summary": "Accurate and robust weather forecasting remains a fundamental challenge due\nto the inherent spatio-temporal complexity of atmospheric systems. In this\npaper, we propose a novel self-supervised learning framework that leverages\nspatio-temporal structures to improve multi-variable weather prediction. The\nmodel integrates a graph neural network (GNN) for spatial reasoning, a\nself-supervised pretraining scheme for representation learning, and a\nspatio-temporal adaptation mechanism to enhance generalization across varying\nforecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis\ndatasets demonstrate that our approach achieves superior performance compared\nto traditional numerical weather prediction (NWP) models and recent deep\nlearning methods. Quantitative evaluations and visual analyses in Beijing and\nShanghai confirm the model's capability to capture fine-grained meteorological\npatterns. The proposed framework provides a scalable and label-efficient\nsolution for future data-driven weather forecasting systems.", "AI": {"tldr": "A self-supervised learning framework using graph neural networks and spatio-temporal adaptation for improved multi-variable weather forecasting, showing superior performance over traditional methods.", "motivation": "Accurate weather forecasting is challenging due to atmospheric complexity, needing better methods beyond traditional numerical models.", "method": "Combines GNN for spatial reasoning, self-supervised pretraining, and spatio-temporal adaptation for generalization across forecasting horizons.", "result": "Achieves superior performance on ERA5 and MERRA-2 datasets, with quantitative and visual validation in Beijing and Shanghai.", "conclusion": "The framework offers a scalable, label-efficient solution for future data-driven weather prediction systems."}}
{"id": "2511.00340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00340", "abs": "https://arxiv.org/abs/2511.00340", "authors": ["Manan Roy Choudhury", "Adithya Chandramouli", "Mannan Anand", "Vivek Gupta"], "title": "Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities", "comment": "41 pages, 4 images", "summary": "The rapid integration of large language models (LLMs) into high-stakes legal\nwork has exposed a critical gap: no benchmark exists to systematically\nstress-test their reliability against the nuanced, adversarial, and often\nsubtle flaws present in real-world contracts. To address this, we introduce\nCLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an\nLLM's legal reasoning. We study the capabilities of LLMs to detect and reason\nabout fine-grained discrepancies by producing over 7500 real-world perturbed\ncontracts from foundational datasets like CUAD and ContractNLI. Our novel,\npersona-driven pipeline generates 10 distinct anomaly categories, which are\nthen validated against official statutes using a Retrieval-Augmented Generation\n(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'\nability to detect embedded legal flaws and explain their significance. Our\nanalysis shows a key weakness: these models often miss subtle errors and\nstruggle even more to justify them legally. Our work outlines a path to\nidentify and correct such reasoning failures in legal AI.", "AI": {"tldr": "CLAUSE is a new benchmark testing LLM reliability in legal contract analysis through 7,500+ perturbed contracts and persona-driven anomalies, revealing models often miss subtle errors and legal justifications.", "motivation": "No existing benchmark systematically tests LLMs against nuanced, adversarial flaws in real-world contracts, creating a critical gap in high-stakes legal AI applications.", "method": "Generate 7,500+ perturbed contracts from CUAD and ContractNLI datasets using a persona-driven pipeline creating 10 anomaly categories, validated with RAG system for legal fidelity.", "result": "LLMs frequently miss subtle legal flaws and struggle to provide proper legal justifications for detected errors.", "conclusion": "CLAUSE provides a path to identify and correct legal reasoning failures in AI systems, highlighting key weaknesses in current models."}}
{"id": "2511.00739", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.00739", "abs": "https://arxiv.org/abs/2511.00739", "authors": ["Ritik Raj", "Hong Wang", "Tushar Krishna"], "title": "A CPU-Centric Perspective on Agentic AI", "comment": null, "summary": "Agentic AI frameworks add a decision-making orchestrator embedded with\nexternal tools, including web search, Python interpreter, contextual database,\nand others, on top of monolithic LLMs, turning them from passive text oracles\ninto autonomous problem-solvers that can plan, call tools, remember past steps,\nand adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks\nintroduced by agentic AI workloads from a largely overlooked CPU-centric\nperspective. We first systematically characterize Agentic AI on the basis of\norchestrator/decision making component, inference path dynamics and\nrepetitiveness of the agentic flow which directly influences the system-level\nperformance. Thereafter, based on the characterization, we choose five\nrepresentative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,\nLangchain and SWE-Agent to profile latency, throughput and energy metrics and\ndemystify the significant impact of CPUs on these metrics relative to GPUs. We\nobserve that - 1. Tool processing on CPUs can take up to 90.6% of the total\nlatency; 2. Agentic throughput gets bottlenecked either by CPU factors -\ncoherence, synchronization and over-subscription of cores or GPU factors - main\nmemory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to\n44% of the total dynamic energy at large batch sizes. Based on the profiling\ninsights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching\n(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and\nheterogeneous agentic workloads respectively to demonstrate the potential to\nimprove the performance, efficiency, and scalability of agentic AI. We achieve\nup to 2.1x and 1.41x P50 latency speedup compared to the multi-processing\nbenchmark for homogeneous and heterogeneous agentic workloads respectively.", "AI": {"tldr": "This paper analyzes CPU bottlenecks in agentic AI systems, showing tools can dominate latency and proposing optimizations that achieved significant speedups.", "motivation": "While agentic AI frameworks enhance LLMs with decision-making and tools, their system bottlenecks from a CPU perspective are overlooked despite significant performance impacts.", "method": "Characterized agentic AI workloads based on orchestrator patterns and repetitiveness, profiled five representative systems (Haystack RAG, Toolformer, etc.) for latency/throughput/energy, then proposed CPU-GPU-aware optimizations.", "result": "CPU tool processing consumed up to 90.6% of latency; throughput limited by CPU/GPU factors; CPU energy reached 44% of total. Optimizations improved latency by up to 2.1x.", "conclusion": "CPU-centric bottlenecks are critical in agentic AI, and coordinated CPU-GPU optimizations can substantially boost performance and efficiency."}}
{"id": "2511.00050", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00050", "abs": "https://arxiv.org/abs/2511.00050", "authors": ["Dhananjaya Gowda", "Seoha Song", "Junhyun Lee", "Harshith Goka"], "title": "FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs", "comment": null, "summary": "As the large language models (LLMs) grow in size each day, efficient training\nand fine-tuning has never been as important as nowadays. This resulted in the\ngreat interest in parameter efficient fine-tuning (PEFT), and effective methods\nincluding low-rank adapters (LoRA) has emerged. Although the various PEFT\nmethods have been studied extensively in the recent years, the greater part of\nthe subject remains unexplored with the huge degree of freedom. In this paper,\nwe propose FLoRA, a family of fused forward-backward adapters (FFBA) for\nparameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine\nideas from the popular LoRA and parallel adapters to improve the overall\nfine-tuning accuracies. At the same time, latencies are minimized by fusing the\nforward and backward adapters into existing projection layers of the base\nmodel. Experimental results show that the proposed FFB adapters perform\nsignificantly better than the popularly used LoRA in both accuracy and latency\nfor a similar parameter budget.", "AI": {"tldr": "Proposes FLoRA, a parameter-efficient fine-tuning method combining LoRA and parallel adapters to improve accuracy while minimizing latency through fused forward-backward adapters.", "motivation": "As LLMs grow larger, efficient fine-tuning becomes crucial. Current PEFT methods like LoRA have limitations in exploring the full design space, leaving room for improvement in accuracy and latency.", "method": "FLoRA uses fused forward-backward adapters (FFBA) that integrate ideas from LoRA and parallel adapters, merging them into the base model's projection layers to reduce computational overhead.", "result": "Experiments show FLoRA outperforms LoRA in both accuracy and latency under similar parameter budgets, demonstrating significant improvements in fine-tuning efficiency.", "conclusion": "FLoRA offers a superior PEFT approach by effectively balancing parameter efficiency, accuracy, and latency, advancing LLM adaptation for downstream tasks."}}
{"id": "2511.00379", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00379", "abs": "https://arxiv.org/abs/2511.00379", "authors": ["Jiahao Wang", "Songkai Xue", "Jinghui Li", "Xiaozhen Wang"], "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "comment": "Accepted by AIES 2025, camera-ready version", "summary": "Ensuring that Large Language Models (LLMs) align with the diverse and\nevolving human values across different regions and cultures remains a critical\nchallenge in AI ethics. Current alignment approaches often yield superficial\nconformity rather than genuine ethical understanding, failing to address the\ncomplex, context-dependent nature of human values. In this paper, we propose a\nnovel ethical reasoning paradigm for LLMs inspired by well-established ethical\ndecision-making models, aiming at enhancing diverse human value alignment\nthrough deliberative ethical reasoning. Our framework consists of a structured\nfive-step process, including contextual fact gathering, hierarchical social\nnorm identification, option generation, multiple-lens ethical impact analysis,\nand reflection. This theory-grounded approach guides LLMs through an\ninterpretable reasoning process that enhances their ability to understand\nregional specificities and perform nuanced ethical analysis, which can be\nimplemented with either prompt engineering or supervised fine-tuning methods.\nWe perform evaluations on the SafeWorld benchmark that specially designed for\nregional value alignment. Experimental results demonstrate our framework\nsignificantly improves LLM alignment with diverse human values compared to\nbaseline methods, enabling more accurate social norm identification and more\nculturally appropriate reasoning. Our work provides a concrete pathway toward\ndeveloping LLMs that align more effectively with the multifaceted values of\nglobal societies through interdisciplinary research.", "AI": {"tldr": "A novel ethical reasoning framework for LLMs that improves cultural alignment through a structured 5-step reasoning process.", "motivation": "Current LLM alignment methods produce superficial conformity rather than genuine ethical understanding across diverse cultures.", "method": "A five-step ethical reasoning process: contextual fact gathering, hierarchical norm identification, option generation, multi-lens ethical analysis, and reflection.", "result": "Significant improvement in value alignment on SafeWorld benchmark, with better norm identification and culturally appropriate reasoning.", "conclusion": "Provides a concrete pathway for developing LLMs that effectively align with global societal values through interdisciplinary research."}}
{"id": "2511.00051", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00051", "abs": "https://arxiv.org/abs/2511.00051", "authors": ["Da Chang", "Peng Xue", "Yu Li", "Yongxiang Liu", "Pengxiang Xu", "Shixun Zhang"], "title": "Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT", "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large\npre-trained models. Among these, LoRA is considered a foundational approach.\nBuilding on this, the influential DoRA method enhances performance by\ndecomposing weight updates into magnitude and direction. However, its\nunderlying mechanism remains unclear, and it introduces significant\ncomputational overhead. In this work, we first identify that DoRA's success\nstems from its capacity to increase the singular value entropy of the weight\nupdate matrix, which promotes a more uniform update distribution akin to full\nfine-tuning. We then reformulate DoRA into a mathematically equivalent and more\nefficient matrix form, revealing it as a learnable weight conditioning method.\nBased on this insight, we propose a unified framework for designing advanced\nPEFT methods by exploring two orthogonal dimensions: the architectural\nplacement and the transformation type of the conditioning matrix. Within this\nframework, we introduce two novel methods: (1) \\textbf{Pre-Diag}, which applies\na diagonal conditioning matrix before the LoRA update to efficiently calibrate\nthe pre-trained weights, thereby enhancing performance while reducing training\ntime; and (2) \\textbf{S}kewed \\textbf{O}rthogonal \\textbf{R}otation\n\\textbf{A}daptation (\\textbf{SORA}), which employs a parameter-efficient\northogonal rotation to perform a more powerful, norm-preserving transformation\nof the feature space. Extensive experiments on natural language understanding\nand generation tasks demonstrate that our proposed methods achieve superior\nperformance and efficiency compared to both LoRA and DoRA. The code is\navailable at https://github.com/MaeChd/SORA.", "AI": {"tldr": "This paper analyzes DoRA's mechanism, reformulates it efficiently, and proposes two improved PEFT methods (Pre-Diag and SORA) that outperform LoRA and DoRA.", "motivation": "DoRA enhances PEFT performance but has unclear mechanisms and high computational cost, needing better understanding and efficiency.", "method": "Identify DoRA's mechanism via singular value entropy analysis, reformulate it efficiently, then propose a framework leading to Pre-Diag (diagonal conditioning) and SORA (orthogonal rotation) methods.", "result": "Experiments show Pre-Diag and SORA achieve superior performance and efficiency over LoRA and DoRA on NLP tasks.", "conclusion": "The work provides insights into PEFT design, offering effective, efficient alternatives to existing methods."}}
{"id": "2511.00382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00382", "abs": "https://arxiv.org/abs/2511.00382", "authors": ["Mina Taraghi", "Yann Pequignot", "Amin Nikanjam", "Mohamed Amine Merzouk", "Foutse Khomh"], "title": "Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs", "comment": null, "summary": "Organizations are increasingly adopting and adapting Large Language Models\n(LLMs) hosted on public repositories such as HuggingFace. Although these\nadaptations often improve performance on specialized downstream tasks, recent\nevidence indicates that they can also degrade a model's safety or fairness.\nSince different fine-tuning techniques may exert distinct effects on these\ncritical dimensions, this study undertakes a systematic assessment of their\ntrade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,\nIA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model\nfamilies (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235\nfine-tuned variants are evaluated across eleven safety hazard categories and\nnine demographic fairness dimensions. The results show that adapter-based\napproaches (LoRA, IA3) tend to improve safety scores and are the least\ndisruptive to fairness, retaining higher accuracy and lower bias scores. In\ncontrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce\nsafety and cause larger fairness regressions, with decreased accuracy and\nincreased bias. Alignment shifts are strongly moderated by base model type:\nLLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest\nsafety decline, and Mistral, which is released without an internal moderation\nlayer, displays the greatest variance. Improvements in safety do not\nnecessarily translate into improvements in fairness, and no single\nconfiguration optimizes all fairness metrics simultaneously, indicating an\ninherent trade-off between these objectives. These findings suggest a practical\nguideline for safety-critical deployments: begin with a well-aligned base\nmodel, favour adapter-based PEFT, and conduct category-specific audits of both\nsafety and fairness.", "AI": {"tldr": "Systematic evaluation of 4 PEFT methods (LoRA, IA3, Prompt-Tuning, P-Tuning) on LLMs shows adapter-based methods improve safety with minimal fairness disruption, while prompt-based methods degrade both safety and fairness.", "motivation": "Organizations use fine-tuned LLMs from public repositories, but different fine-tuning techniques may negatively impact safety and fairness dimensions, requiring systematic assessment of trade-offs.", "method": "Applied four PEFT methods to four instruction-tuned model families, creating 235 variants evaluated across 11 safety hazard categories and 9 demographic fairness dimensions.", "result": "Adapter-based methods (LoRA, IA3) improve safety scores and maintain fairness better, while prompt-based methods reduce safety and increase fairness regressions. Base model type significantly moderates alignment shifts.", "conclusion": "Safety and fairness improvements don't always correlate; practical guideline: start with well-aligned base model, prefer adapter-based PEFT, and conduct category-specific audits."}}
{"id": "2511.00052", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00052", "abs": "https://arxiv.org/abs/2511.00052", "authors": ["Federico Formica", "Stefano Gregis", "Aurora Francesca Zanenga", "Andrea Rota", "Mark Lawford", "Claudio Menghi"], "title": "Feature-Guided Analysis of Neural Networks: A Replication Study", "comment": null, "summary": "Understanding why neural networks make certain decisions is pivotal for their\nuse in safety-critical applications. Feature-Guided Analysis (FGA) extracts\nslices of neural networks relevant to their tasks. Existing feature-guided\napproaches typically monitor the activation of the neural network neurons to\nextract the relevant rules. Preliminary results are encouraging and demonstrate\nthe feasibility of this solution by assessing the precision and recall of\nFeature-Guided Analysis on two pilot case studies. However, the applicability\nin industrial contexts needs additional empirical evidence.\n  To mitigate this need, this paper assesses the applicability of FGA on a\nbenchmark made by the MNIST and LSC datasets. We assessed the effectiveness of\nFGA in computing rules that explain the behavior of the neural network. Our\nresults show that FGA has a higher precision on our benchmark than the results\nfrom the literature. We also evaluated how the selection of the neural network\narchitecture, training, and feature selection affect the effectiveness of FGA.\nOur results show that the selection significantly affects the recall of FGA,\nwhile it has a negligible impact on its precision.", "AI": {"tldr": "FGA shows higher precision than literature on MNIST and LSC benchmarks, with feature selection significantly affecting recall but not precision.", "motivation": "Understanding neural network decisions is crucial for safety-critical applications, and existing FGA approaches need more empirical evidence for industrial applicability.", "method": "Assessed FGA on MNIST and LSC datasets, evaluating how neural network architecture, training, and feature selection affect FGA effectiveness.", "result": "FGA achieved higher precision than literature results, with feature selection significantly impacting recall but having negligible effect on precision.", "conclusion": "FGA is effective for explaining neural network behavior, with feature selection being a key factor for recall optimization in industrial applications."}}
{"id": "2511.00424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00424", "abs": "https://arxiv.org/abs/2511.00424", "authors": ["Ashutosh Anshul", "Gumpili Sai Pranav", "Mohammad Zia Ur Rehman", "Nagendra Kumar"], "title": "A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method", "comment": null, "summary": "The recent coronavirus disease (Covid-19) has become a pandemic and has\naffected the entire globe. During the pandemic, we have observed a spike in\ncases related to mental health, such as anxiety, stress, and depression.\nDepression significantly influences most diseases worldwide, making it\ndifficult to detect mental health conditions in people due to unawareness and\nunwillingness to consult a doctor. However, nowadays, people extensively use\nonline social media platforms to express their emotions and thoughts. Hence,\nsocial media platforms are now becoming a large data source that can be\nutilized for detecting depression and mental illness. However, existing\napproaches often overlook data sparsity in tweets and the multimodal aspects of\nsocial media. In this paper, we propose a novel multimodal framework that\ncombines textual, user-specific, and image analysis to detect depression among\nsocial media users. To provide enough context about the user's emotional state,\nwe propose (i) an extrinsic feature by harnessing the URLs present in tweets\nand (ii) extracting textual content present in images posted in tweets. We also\nextract five sets of features belonging to different modalities to describe a\nuser. Additionally, we introduce a Deep Learning model, the Visual Neural\nNetwork (VNN), to generate embeddings of user-posted images, which are used to\ncreate the visual feature vector for prediction. We contribute a curated\nCovid-19 dataset of depressed and non-depressed users for research purposes and\ndemonstrate the effectiveness of our model in detecting depression during the\nCovid-19 outbreak. Our model outperforms existing state-of-the-art methods over\na benchmark dataset by 2%-8% and produces promising results on the Covid-19\ndataset. Our analysis highlights the impact of each modality and provides\nvaluable insights into users' mental and emotional states.", "AI": {"tldr": "Proposes a multimodal framework combining text, user-specific data, and image analysis to detect depression from social media during COVID-19, outperforming existing methods by 2%-8%.", "motivation": "Mental health issues like depression spiked during COVID-19, but detection is challenging due to unawareness and reluctance to seek help. Social media offers a rich data source for detection, but existing approaches ignore data sparsity and multimodal aspects.", "method": "Introduces a novel multimodal framework extracting textual content from tweets and images, user-specific features, and URL-based extrinsic features. Uses a Visual Neural Network (VNN) for image embeddings and combines five feature sets from different modalities.", "result": "Model outperforms state-of-the-art methods by 2%-8% on a benchmark dataset and shows promising results on a curated COVID-19 dataset. Analysis reveals the impact of each modality on depression detection.", "conclusion": "The multimodal approach effectively detects depression from social media data, providing valuable insights into users' mental states during the pandemic, with the curated dataset serving as a resource for future research."}}
{"id": "2511.00053", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.00053", "abs": "https://arxiv.org/abs/2511.00053", "authors": ["Hao Wang", "Licheng Pan", "Yuan Lu", "Zhichao Chen", "Tianqiao Liu", "Shuting He", "Zhixuan Chu", "Qingsong Wen", "Haoxuan Li", "Zhouchen Lin"], "title": "Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models", "comment": null, "summary": "The design of training objective is central to training time-series\nforecasting models. Existing training objectives such as mean squared error\nmostly treat each future step as an independent, equally weighted task, which\nwe found leading to the following two issues: (1) overlook the label\nautocorrelation effect among future steps, leading to biased training\nobjective; (2) fail to set heterogeneous task weights for different forecasting\ntasks corresponding to varying future steps, limiting the forecasting\nperformance. To fill this gap, we propose a novel quadratic-form weighted\ntraining objective, addressing both of the issues simultaneously. Specifically,\nthe off-diagonal elements of the weighting matrix account for the label\nautocorrelation effect, whereas the non-uniform diagonals are expected to match\nthe most preferable weights of the forecasting tasks with varying future steps.\nTo achieve this, we propose a Quadratic Direct Forecast (QDF) learning\nalgorithm, which trains the forecast model using the adaptively updated\nquadratic-form weighting matrix. Experiments show that our QDF effectively\nimproves performance of various forecast models, achieving state-of-the-art\nresults. Code is available at https://anonymous.4open.science/r/QDF-8937.", "AI": {"tldr": "Proposes a quadratic-form weighted training objective (QDF) to address limitations of traditional time-series forecasting objectives by accounting for label autocorrelation and setting heterogeneous task weights for different forecasting horizons.", "motivation": "Existing training objectives like mean squared error treat future forecasting steps as independent and equally weighted, leading to overlooked label autocorrelation effects and inability to set appropriate weights for different forecasting tasks.", "method": "Introduces a quadratic-form weighted training objective where off-diagonal elements of the weighting matrix capture label autocorrelation and diagonal elements assign heterogeneous weights to different forecasting steps. Develops a Quadratic Direct Forecast (QDF) learning algorithm that adaptively updates this weighting matrix during training.", "result": "Experimental results show QDF effectively improves performance of various forecasting models, achieving state-of-the-art results across different benchmarks.", "conclusion": "The proposed QDF framework successfully addresses key limitations in time-series forecasting training objectives, demonstrating superior performance through its incorporation of temporal dependencies and adaptive task weighting."}}
{"id": "2511.00457", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00457", "abs": "https://arxiv.org/abs/2511.00457", "authors": ["Chunyu Wei", "Wenji Hu", "Xingjia Hao", "Xin Wang", "Yifan Yang", "Yueguo Chen", "Yang Tian", "Yunhai Wang"], "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "comment": null, "summary": "Large Language Models (LLMs) face significant limitations when applied to\nlarge-scale graphs, struggling with context constraints and inflexible\nreasoning. We present GraphChain, a framework that enables LLMs to analyze\ncomplex graphs through dynamic sequences of specialized tools, mimicking human\nexploratory intelligence. Our approach introduces two key innovations: (1)\nProgressive Graph Distillation, a reinforcement learning mechanism that\ngenerates optimized tool sequences balancing task relevance with information\ncompression, and (2) Structure-aware Test-Time Adaptation, which efficiently\ntailors tool selection strategies to diverse graph topologies using spectral\nproperties and lightweight adapters without costly retraining. Experiments show\nGraphChain significantly outperforms prior methods, enabling scalable and\nadaptive LLM-driven graph analysis.", "AI": {"tldr": "GraphChain enables LLMs to analyze large graphs using dynamic tool sequences with RL-based optimization and structure-aware adaptation, outperforming previous methods.", "motivation": "LLMs struggle with large-scale graphs due to context constraints and inflexible reasoning, limiting their applicability in graph analysis tasks.", "method": "Introduces Progressive Graph Distillation (reinforcement learning for optimized tool sequences) and Structure-aware Test-Time Adaptation (spectral properties and lightweight adapters for topology-specific tool selection).", "result": "GraphChain significantly outperforms prior methods in experiments, enabling scalable and adaptive LLM-driven graph analysis.", "conclusion": "The framework successfully addresses LLM limitations in graph analysis through dynamic tool sequencing and structure-aware adaptation, providing a scalable solution for complex graph tasks."}}
{"id": "2511.00054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00054", "abs": "https://arxiv.org/abs/2511.00054", "authors": ["Gio Huh", "Dhruv Sheth", "Rayhan Zirvi", "Frank Xiao"], "title": "SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation", "comment": "Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop on Efficient Reasoning", "summary": "While Vision-Language Models (VLMs) excel in many areas, they struggle with\ncomplex spatial reasoning, which requires problem decomposition and strategic\ntool use. Fine-tuning smaller, more deployable models offers an efficient path\nto strong performance, but this is hampered by a major bottleneck: the absence\nof high-quality, step-by-step reasoning data. To address this data-efficiency\ngap, we introduce SpatialTraceGen, a framework to distill the reasoning\nprocesses of a large teacher model into a high-quality dataset of multi-hop,\nmulti-tool reasoning traces. A key innovation is our automated Verifier, which\nscalably ensures the fidelity of each reasoning step, providing a\ncost-effective alternative to manual human annotation. On the CLEVR-Humans\nbenchmark, this verifier-guided process improves the average quality score of\ntraces by 17\\% while reducing quality variance by over 40\\%. SpatialTraceGen\ndelivers a dataset of expert traces, providing the structured, step-by-step\nexamples of tool use necessary for effective fine-tuning and sample-efficient\noffline reinforcement learning.", "AI": {"tldr": "SpatialTraceGen framework distills reasoning processes from large teacher models into high-quality datasets for spatial reasoning, using automated verification to ensure step fidelity.", "motivation": "Vision-Language Models struggle with complex spatial reasoning requiring problem decomposition and tool use, and fine-tuning smaller models is hampered by lack of high-quality step-by-step reasoning data.", "method": "Introduces SpatialTraceGen framework with automated Verifier to distill reasoning processes from large teacher models into multi-hop, multi-tool reasoning traces, ensuring step fidelity without manual annotation.", "result": "On CLEVR-Humans benchmark, verifier-guided process improves average quality score by 17% and reduces quality variance by over 40%, producing high-quality datasets for fine-tuning.", "conclusion": "SpatialTraceGen provides structured, step-by-step examples of tool use necessary for effective fine-tuning and sample-efficient offline reinforcement learning, addressing the data-efficiency gap in spatial reasoning."}}
{"id": "2511.00509", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00509", "abs": "https://arxiv.org/abs/2511.00509", "authors": ["Yifan Xia", "Guorui Chen", "Wenqian Yu", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "Reimagining Safety Alignment with An Image", "comment": null, "summary": "Large language models (LLMs) excel in diverse applications but face dual\nchallenges: generating harmful content under jailbreak attacks and over-refusal\nof benign queries due to rigid safety mechanisms. These issues are further\ncomplicated by the need to accommodate different value systems and precisely\nalign with given safety preferences. Moreover, traditional methods like SFT and\nRLHF lack this capability due to their costly parameter tuning requirements and\ninability to support multiple value systems within a single model. These\nproblems are more obvious in multimodal large language models (MLLMs),\nespecially in terms of heightened over-refusal in cross-modal tasks and new\nsecurity risks arising from expanded attack surfaces. We propose Magic Image,\nan optimization-driven visual prompt framework that enhances security while\nreducing over-refusal. By optimizing image prompts using harmful/benign\nsamples, our method enables a single model to adapt to different value systems\nand better align with given safety preferences without parameter updates.\nExperiments demonstrate improved safety-effectiveness balance across diverse\ndatasets while preserving model performance, offering a practical solution for\ndeployable MLLM safety alignment.", "AI": {"tldr": "Magic Image is a visual prompt framework that uses optimization to enhance MLLM safety by reducing harmful content generation while minimizing over-refusal, enabling adaptation to different value systems without parameter updates.", "motivation": "LLMs face challenges with jailbreak attacks generating harmful content and over-refusal of benign queries, compounded by the need to accommodate different value systems. Traditional methods like SFT and RLHF are costly and can't support multiple value systems in one model. These issues are worse in MLLMs due to cross-modal over-refusal and expanded attack surfaces.", "method": "An optimization-driven visual prompt framework that optimizes image prompts using harmful and benign samples, enabling a single model to adapt to different value systems and align with safety preferences without requiring parameter updates.", "result": "Experiments show improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.", "conclusion": "Magic Image provides an effective framework for enhancing MLLM security while reducing over-refusal, enabling flexible adaptation to different value systems without the need for costly parameter tuning, making it suitable for practical deployment."}}
{"id": "2511.00055", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00055", "abs": "https://arxiv.org/abs/2511.00055", "authors": ["Leonhard Duda", "Khadijeh Alibabaei", "Elena Vollmer", "Leon Klug", "Valentin Kozlov", "Lisana Berberi", "Mishal Benz", "Rebekka Volk", "Juan Pedro Guti\u00e9rrez Hermosillo Muriedas", "Markus G\u00f6tz", "Judith S\u00e1\u00ednz-Pardo D\u00edaz", "\u00c1lvaro L\u00f3pez Garc\u00eda", "Frank Schultmann", "Achim Streit"], "title": "Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches", "comment": null, "summary": "Federated Learning (FL) is an approach for training a shared Machine Learning\n(ML) model with distributed training data and multiple participants. FL allows\nbypassing limitations of the traditional Centralized Machine Learning CL if\ndata cannot be shared or stored centrally due to privacy or technical\nrestrictions -- the participants train the model locally with their training\ndata and do not need to share it among the other participants. This paper\ninvestigates the practical implementation and effectiveness of FL in a\nreal-world scenario, specifically focusing on unmanned aerial vehicle\n(UAV)-based thermal images for common thermal feature detection in urban\nenvironments. The distributed nature of the data arises naturally and makes it\nsuitable for FL applications, as images captured in two German cities are\navailable. This application presents unique challenges due to non-identical\ndistribution and feature characteristics of data captured at both locations.\nThe study makes several key contributions by evaluating FL algorithms in real\ndeployment scenarios rather than simulation. We compare several FL approaches\nwith a centralized learning baseline across key performance metrics such as\nmodel accuracy, training time, communication overhead, and energy usage. This\npaper also explores various FL workflows, comparing client-controlled workflows\nand server-controlled workflows. The findings of this work serve as a valuable\nreference for understanding the practical application and limitations of the FL\nmethods in segmentation tasks in UAV-based imaging.", "AI": {"tldr": "This paper evaluates federated learning (FL) for thermal image segmentation using UAV data from two German cities, comparing FL approaches with centralized learning across accuracy, training time, communication, and energy metrics.", "motivation": "Traditional centralized machine learning faces limitations when data cannot be shared due to privacy or technical constraints. FL allows distributed training without sharing data, making it suitable for UAV-based thermal imaging where data is naturally distributed across locations.", "method": "The study implements FL in a real-world scenario with UAV thermal images from two cities. It compares multiple FL algorithms against a centralized baseline, examining client-controlled and server-controlled workflows. Performance is evaluated using metrics like accuracy, training time, communication overhead, and energy consumption.", "result": "FL approaches are practically evaluated, showing how they perform in segmentation tasks. The comparison highlights trade-offs between FL and centralized methods, addressing challenges like non-iid data distribution and feature variations between locations.", "conclusion": "The findings provide a valuable reference for FL's practical application in UAV-based imaging, illustrating its effectiveness and limitations in real deployment scenarios, particularly for privacy-sensitive distributed data environments."}}
{"id": "2511.00547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00547", "abs": "https://arxiv.org/abs/2511.00547", "authors": ["Alain Riou"], "title": "Efficient Generation of Binary Magic Squares", "comment": null, "summary": "We propose a simple algorithm for generating Binary Magic Squares (BMS),\ni.e., square binary matrices where the sum of all rows and all columns are\nequal. We show by induction that our algorithm always returns valid BMS with\noptimal theoretical complexity. We then extend our study to non-square Binary\nMagic Squares, formalize conditions on the sum of rows and columns for these\nBMS to exist, and show that a slight variant of our first algorithm can\ngenerate provably generate them. Finally, we publicly release two\nimplementations of our algorithm as Python packages, including one that can\ngenerate several BMS in parallel using GPU acceleration.", "AI": {"tldr": "A simple algorithm generates Binary Magic Squares with optimal complexity, extends to non-square versions with formal conditions, and includes GPU-accelerated Python implementations.", "motivation": "To efficiently generate valid Binary Magic Squares where row and column sums are equal, extending beyond traditional square matrices.", "method": "An inductive algorithm proven to always produce valid BMS, with a variant for non-square cases based on formalized sum conditions.", "result": "The algorithm achieves optimal theoretical complexity and successfully generates both square and non-square BMS, with implementations supporting parallel GPU acceleration.", "conclusion": "The work provides a robust, efficient solution for BMS generation, with practical tools released as open-source Python packages."}}
{"id": "2511.00056", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00056", "abs": "https://arxiv.org/abs/2511.00056", "authors": ["Yuxi Liu", "Renjia Deng", "Yutong He", "Xue Wang", "Tao Yao", "Kun Yuan"], "title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling", "comment": null, "summary": "The substantial memory demands of pre-training and fine-tuning large language\nmodels (LLMs) require memory-efficient optimization algorithms. One promising\napproach is layer-wise optimization, which treats each transformer block as a\nsingle layer and optimizes it sequentially, while freezing the other layers to\nsave optimizer states and activations. Although effective, these methods ignore\nthe varying importance of the modules within each layer, leading to suboptimal\nperformance. Moreover, layer-wise sampling provides only limited memory\nsavings, as at least one full layer must remain active during optimization. To\novercome these limitations, we propose Module-wise Importance SAmpling (MISA),\na novel method that divides each layer into smaller modules and assigns\nimportance scores to each module. MISA uses a weighted random sampling\nmechanism to activate modules, provably reducing gradient variance compared to\nlayer-wise sampling. Additionally, we establish an \\(\\mathcal{O}(1/\\sqrt{K})\\)\nconvergence rate under non-convex and stochastic conditions, where $K$ is the\ntotal number of block updates, and provide a detailed memory analysis\nshowcasing MISA's superiority over existing baseline methods. Experiments on\ndiverse learning tasks validate the effectiveness of MISA. Source code is\navailable at https://github.com/pkumelon/MISA.", "AI": {"tldr": "MISA proposes module-wise importance sampling to optimize LLMs more efficiently than layer-wise methods by activating important modules within layers, reducing memory usage and improving performance.", "motivation": "Layer-wise optimization methods for LLMs are memory-efficient but suboptimal because they treat entire layers uniformly, ignoring varying importance of modules within layers and offering limited memory savings.", "method": "MISA divides each transformer layer into smaller modules, assigns importance scores to each module, and uses weighted random sampling to activate modules during optimization, reducing gradient variance.", "result": "Theoretical analysis shows O(1/\u221aK) convergence rate under non-convex stochastic conditions, and experiments demonstrate MISA's superiority in memory efficiency and performance over baseline methods across various tasks.", "conclusion": "MISA effectively addresses limitations of layer-wise optimization by enabling more granular, importance-aware module activation, providing better memory efficiency and convergence for training LLMs."}}
{"id": "2511.00551", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00551", "abs": "https://arxiv.org/abs/2511.00551", "authors": ["Qiang Li", "Ningjing Zeng", "Lina Yu"], "title": "Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control", "comment": null, "summary": "Several studies have employed reinforcement learning (RL) to address the\nchallenges of regional adaptive traffic signal control (ATSC) and achieved\npromising results. In this field, existing research predominantly adopts\nmulti-agent frameworks. However, the adoption of multi-agent frameworks\npresents challenges for scalability. Instead, the Traffic signal control (TSC)\nproblem necessitates a single-agent framework. TSC inherently relies on\ncentralized management by a single control center, which can monitor traffic\nconditions across all roads in the study area and coordinate the control of all\nintersections. This work proposes a single-agent RL-based regional ATSC model\ncompatible with probe vehicle technology. Key components of the RL design\ninclude state, action, and reward function definitions. To facilitate learning\nand manage congestion, both state and reward functions are defined based on\nqueue length, with action designed to regulate queue dynamics. The queue length\ndefinition used in this study differs slightly from conventional definitions\nbut is closely correlated with congestion states. More importantly, it allows\nfor reliable estimation using link travel time data from probe vehicles. With\nprobe vehicle data already covering most urban roads, this feature enhances the\nproposed method's potential for widespread deployment. The method was\ncomprehensively evaluated using the SUMO simulation platform. Experimental\nresults demonstrate that the proposed model effectively mitigates large-scale\nregional congestion levels via coordinated multi-intersection control.", "AI": {"tldr": "A single-agent RL model for regional adaptive traffic signal control that uses probe vehicle data to estimate queue lengths and coordinate multiple intersections effectively.", "motivation": "Multi-agent RL frameworks for traffic signal control face scalability issues, while TSC inherently requires centralized management by a single control center that can monitor all roads and coordinate all intersections.", "method": "Proposes a single-agent RL-based regional ATSC model compatible with probe vehicle technology. Uses queue length-based state and reward functions, with actions designed to regulate queue dynamics. Queue length is estimated from link travel time data from probe vehicles.", "result": "Comprehensive evaluation using SUMO simulation platform shows the model effectively mitigates large-scale regional congestion levels through coordinated multi-intersection control.", "conclusion": "The single-agent RL approach provides a scalable solution for regional traffic signal control that leverages widely available probe vehicle data and demonstrates effective congestion mitigation through coordinated intersection control."}}
{"id": "2511.00059", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00059", "abs": "https://arxiv.org/abs/2511.00059", "authors": ["Aditya Singh", "Zihang Wen", "Srujananjali Medicherla", "Adam Karvonen", "Can Rager"], "title": "Automatically Finding Rule-Based Neurons in OthelloGPT", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop Mechanistic interpretability", "summary": "OthelloGPT, a transformer trained to predict valid moves in Othello, provides\nan ideal testbed for interpretability research. The model is complex enough to\nexhibit rich computational patterns, yet grounded in rule-based game logic that\nenables meaningful reverse-engineering. We present an automated approach based\non decision trees to identify and interpret MLP neurons that encode rule-based\ngame logic. Our method trains regression decision trees to map board states to\nneuron activations, then extracts decision paths where neurons are highly\nactive to convert them into human-readable logical forms. These descriptions\nreveal highly interpretable patterns; for instance, neurons that specifically\ndetect when diagonal moves become legal. Our findings suggest that roughly half\nof the neurons in layer 5 can be accurately described by compact, rule-based\ndecision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder\nlikely participate in more distributed or non-rule-based computations. We\nverify the causal relevance of patterns identified by our decision trees\nthrough targeted interventions. For a specific square, for specific game\npatterns, we ablate neurons corresponding to those patterns and find an\napproximately 5-10 fold stronger degradation in the model's ability to predict\nlegal moves along those patterns compared to control patterns. To facilitate\nfuture work, we provide a Python tool that maps rule-based game behaviors to\ntheir implementing neurons, serving as a resource for researchers to test\nwhether their interpretability methods recover meaningful computational\nstructures.", "AI": {"tldr": "Researchers developed an automated decision tree method to identify and interpret MLP neurons in OthelloGPT that encode rule-based game logic, finding about half of layer 5 neurons follow compact rules while others handle more complex computations.", "motivation": "OthelloGPT provides an ideal testbed for interpretability research due to its complexity grounded in rule-based game logic, enabling meaningful reverse-engineering of computational patterns.", "method": "An automated approach using regression decision trees to map board states to neuron activations, extracting decision paths to convert them into human-readable logical forms describing game rules.", "result": "Roughly half of layer 5 neurons (913 of 2,048) can be accurately described by compact rule-based decision trees (R\u00b2 > 0.7), with targeted interventions showing 5-10 fold stronger degradation in move prediction for patterns corresponding to ablated neurons.", "conclusion": "The method successfully identifies interpretable, rule-based computational structures in OthelloGPT, providing a tool for future interpretability research to test if methods recover meaningful computational patterns."}}
{"id": "2511.00609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00609", "abs": "https://arxiv.org/abs/2511.00609", "authors": ["Shengqi Xu", "Xinpeng Zhou", "Yabo Zhang", "Ming Liu", "Tao Liang", "Tianyu Zhang", "Yalong Bai", "Zuxuan Wu", "Wangmeng Zuo"], "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment", "comment": null, "summary": "Personalized image preference assessment aims to evaluate an individual\nuser's image preferences by relying only on a small set of reference images as\nprior information. Existing methods mainly focus on general preference\nassessment, training models with large-scale data to tackle well-defined tasks\nsuch as text-image alignment. However, these approaches struggle to handle\npersonalized preference because user-specific data are scarce and not easily\nscalable, and individual tastes are often diverse and complex. To overcome\nthese challenges, we introduce a common preference profile that serves as a\nbridge across users, allowing large-scale user data to be leveraged for\ntraining profile prediction and capturing complex personalized preferences.\nBuilding on this idea, we propose a reasoning-based personalized image\npreference assessment framework that follows a \\textit{predict-then-assess}\nparadigm: it first predicts a user's preference profile from reference images,\nand then provides interpretable, multi-dimensional scores and assessments of\ncandidate images based on the predicted profile. To support this, we first\nconstruct a large-scale Chain-of-Thought (CoT)-style personalized assessment\ndataset annotated with diverse user preference profiles and high-quality\nCoT-style reasoning, enabling explicit supervision of structured reasoning.\nNext, we adopt a two-stage training strategy: a cold-start supervised\nfine-tuning phase to empower the model with structured reasoning capabilities,\nfollowed by reinforcement learning to incentivize the model to explore more\nreasonable assessment paths and enhance generalization. Furthermore, we propose\na similarity-aware prediction reward to encourage better prediction of the\nuser's preference profile, which facilitates more reasonable assessments\nexploration. Extensive experiments demonstrate the superiority of the proposed\nmethod.", "AI": {"tldr": "Proposes a reasoning-based framework for personalized image preference assessment using a \"predict-then-assess\" paradigm with preference profiles, supported by a large CoT-style dataset and two-stage training.", "motivation": "Existing methods focus on general preference assessment but struggle with personalized preferences due to scarce user-specific data and diverse/complex individual tastes.", "method": "Introduces common preference profiles across users, builds a CoT-style dataset with user profiles and reasoning annotations, and uses two-stage training (supervised fine-tuning + reinforcement learning with similarity-aware rewards).", "result": "Extensive experiments demonstrate the superiority of the proposed method over existing approaches.", "conclusion": "The framework effectively handles personalized image preference assessment by leveraging shared preference profiles and structured reasoning, overcoming data scarcity and complexity challenges."}}
{"id": "2511.00064", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00064", "abs": "https://arxiv.org/abs/2511.00064", "authors": ["Randolph Wiredu-Aidoo"], "title": "EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics", "comment": null, "summary": "Clustering algorithms often rely on restrictive assumptions: K-Means and\nGaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and\nHDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA\n(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a\ndensity-variance based clustering algorithm that treats cluster formation as an\nadaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted\ngraphs via breadth-first search, guided by continuously updated local distance\nand shape statistics, replacing fixed density thresholds with local statistical\nfeedback. With spatial indexing, EVINGCA features log-linear complexity in the\naverage case and exhibits competitive performance against baselines across a\nvariety of synthetic, real-world, low-d, and high-d datasets.", "AI": {"tldr": "EVINGCA is a density-variance based clustering algorithm that treats cluster formation as an adaptive process on nearest-neighbor graphs, using local statistical feedback instead of fixed thresholds, achieving log-linear complexity and competitive performance.", "motivation": "Existing clustering algorithms have limitations - K-Means and Gaussian Mixtures assume convex, Gaussian-like clusters, while DBSCAN and HDBSCAN capture non-convexity but are highly sensitive to parameters.", "method": "EVINGCA expands rooted graphs via breadth-first search guided by continuously updated local distance and shape statistics, replacing fixed density thresholds with local statistical feedback. Uses spatial indexing for efficiency.", "result": "EVINGCA achieves log-linear complexity in average case and exhibits competitive performance against baselines across synthetic, real-world, low-dimensional and high-dimensional datasets.", "conclusion": "EVINGCA provides an effective alternative to traditional clustering methods by treating cluster formation as an evolving process with adaptive statistical guidance, overcoming limitations of both parametric and density-based approaches."}}
{"id": "2511.00640", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00640", "abs": "https://arxiv.org/abs/2511.00640", "authors": ["Zicheng Xu", "Guanchu Wang", "Yu-Neng Chuang", "Guangyao Zheng", "Alexander S. Szalay", "Zirui Liu", "Vladimir Braverman"], "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex\nreasoning tasks, yet they often suffer from overthinking, producing excessively\nlong chain-of-thought (CoT) traces that increase inference cost and may degrade\naccuracy. Our analysis reveals a clear anti-correlation between reasoning\nlength and accuracy, where across multiple stochastic decodes, the short\nreasoning paths consistently achieve the highest correctness, while longer ones\naccumulate errors and repetitions. These short optimal reasoning paths can be\nfound ideally through full enumeration of the reasoning space. However, the\ntree-structured reasoning space grows exponentially with sequence length,\nrendering exhaustive exploration infeasible. To address this, we propose DTS, a\nmodel-agnostic decoding framework that sketches the reasoning space by\nselectively branching at high-entropy tokens and applies early stopping to\nselect the shortest completed reasoning path. This approach approximates the\noptimal solution that enhances both efficiency and accuracy, without requiring\nadditional training or supervision. Experiments on AIME2024 and AIME2025\ndatasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves\naccuracy by up to 8%, reduces average reasoning length by 23%, and decreases\nrepetition frequency by 12%, demonstrating DTS's ability for scalable and\nefficient LRM reasoning.", "AI": {"tldr": "DTS is a model-agnostic decoding framework that addresses overthinking in Large Reasoning Models by selectively branching at high-entropy tokens and using early stopping to find shorter, more accurate reasoning paths, improving both efficiency and accuracy without additional training.", "motivation": "Large Reasoning Models often suffer from overthinking, producing excessively long chain-of-thought traces that increase inference costs and may degrade accuracy. Analysis shows an anti-correlation between reasoning length and accuracy, where shorter paths consistently achieve higher correctness.", "method": "DTS sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path, approximating the optimal solution without exhaustive exploration of the exponentially growing reasoning space.", "result": "Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%.", "conclusion": "DTS demonstrates the ability for scalable and efficient LRM reasoning by finding shorter, more accurate reasoning paths through selective branching and early stopping, enhancing both efficiency and accuracy without requiring additional training or supervision."}}
{"id": "2511.00065", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00065", "abs": "https://arxiv.org/abs/2511.00065", "authors": ["Kateryna Shapovalenko", "Quentin Auster"], "title": "Aligning Brain Signals with Multimodal Speech and Vision Embeddings", "comment": null, "summary": "When we hear the word \"house\", we don't just process sound, we imagine walls,\ndoors, memories. The brain builds meaning through layers, moving from raw\nacoustics to rich, multimodal associations. Inspired by this, we build on\nrecent work from Meta that aligned EEG signals with averaged wav2vec2 speech\nembeddings, and ask a deeper question: which layers of pre-trained models best\nreflect this layered processing in the brain? We compare embeddings from two\nmodels: wav2vec2, which encodes sound into language, and CLIP, which maps words\nto images. Using EEG recorded during natural speech perception, we evaluate how\nthese embeddings align with brain activity using ridge regression and\ncontrastive decoding. We test three strategies: individual layers, progressive\nconcatenation, and progressive summation. The findings suggest that combining\nmultimodal, layer-aware representations may bring us closer to decoding how the\nbrain understands language, not just as sound, but as experience.", "AI": {"tldr": "Study compares neural alignment of wav2vec2 and CLIP embeddings with EEG data during speech perception, finding that multimodal layer-aware representations best match brain's hierarchical processing.", "motivation": "To understand how different layers of pre-trained models (wav2vec2 for sound-to-language and CLIP for word-to-image) align with the brain's layered processing during natural speech comprehension.", "method": "Used EEG recordings during natural speech perception, compared embeddings from wav2vec2 and CLIP models via ridge regression and contrastive decoding, testing individual layers, progressive concatenation, and progressive summation strategies.", "result": "Combining multimodal, layer-aware representations showed the best alignment with brain activity, suggesting they capture the brain's hierarchical meaning construction.", "conclusion": "Multimodal layer-aware model representations may improve decoding of how the brain processes language as experiential meaning rather than just sound."}}
{"id": "2511.00066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00066", "abs": "https://arxiv.org/abs/2511.00066", "authors": ["Tue Le", "Nghi D. Q. Bui", "Linh Ngo Van", "Trung Le"], "title": "Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful approach for strengthening the reasoning capabilities of large\nlanguage models (LLMs). Among existing algorithms, Group Relative Policy\nOptimization (GRPO) has demonstrated strong performance, yet it suffers from a\ncritical issue: low-probability tokens disproportionately dominate gradient\nupdates due to their inherently large gradient magnitudes. This imbalance leads\nto unstable training and suppresses the contribution of high-probability tokens\nthat are more reliable for learning. In this work, we introduce Token-Regulated\nGroup Relative Policy Optimization (TR-GRPO), a simple yet effective extension\nof GRPO that assigns token-level weights positively correlated with the model's\npredicted probability. By downweighting low-probability tokens and emphasizing\nhigh-probability ones, TR-GRPO mitigates gradient over-amplification while\npreserving informative learning signals. Extensive experiments demonstrate that\nTR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,\nand agentic reasoning, highlighting the importance of regulating token\ncontributions during RL training and establishing TR-GRPO as a robust framework\nfor enhancing LLM reasoning.", "AI": {"tldr": "TR-GRPO improves GRPO by weighting tokens based on probability to stabilize training and enhance LLM reasoning performance", "motivation": "Existing GRPO algorithm suffers from gradient imbalance where low-probability tokens dominate updates, causing unstable training and suppressing reliable high-probability tokens", "method": "Token-Regulated GRPO assigns token-level weights correlated with predicted probability, downweighting low-probability tokens and emphasizing high-probability ones", "result": "TR-GRPO consistently outperforms GRPO across RLVR tasks including logic, math, and agentic reasoning", "conclusion": "Token contribution regulation is crucial for RL training, and TR-GRPO establishes a robust framework for enhancing LLM reasoning"}}
{"id": "2511.00673", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00673", "abs": "https://arxiv.org/abs/2511.00673", "authors": ["Dominik Drexler"], "title": "Lifted Successor Generation in Numeric Planning", "comment": null, "summary": "Most planners ground numeric planning tasks, given in a first-order-like\nlanguage, into a ground task representation. However, this can lead to an\nexponential blowup in task representation size, which occurs in practice for\nhard-to-ground tasks. We extend a state-of-the-art lifted successor generator\nfor classical planning to support numeric precondition applicability. The\nmethod enumerates maximum cliques in a substitution consistency graph. Each\nmaximum clique represents a substitution for the variables of the action\nschema, yielding a ground action. We augment this graph with numeric action\npreconditions and prove the successor generator is exact under formally\nspecified conditions. When the conditions fail, our generator may list\ninapplicable ground actions; a final applicability check filters these without\naffecting completeness. However, this cannot happen in 23 of 25 benchmark\ndomains, and it occurs only in 1 domain. To the authors' knowledge, no other\nlifted successor generator supports numeric action preconditions. This enables\nfuture research on lifted planning for a very rich planning fragment.", "AI": {"tldr": "Extension of a lifted successor generator for classical planning to support numeric precondition applicability, avoiding exponential blowup from grounding numeric planning tasks.", "motivation": "Grounding numeric planning tasks can cause exponential blowup in task representation size for hard-to-ground tasks, which occurs in practice.", "method": "Extend a state-of-the-art lifted successor generator by enumerating maximum cliques in a substitution consistency graph, augmented with numeric action preconditions. Each maximum clique represents a substitution yielding a ground action.", "result": "The successor generator is exact under formally specified conditions. When conditions fail, inapplicable ground actions are filtered without affecting completeness. This occurs only in 1 out of 25 benchmark domains.", "conclusion": "This is the first lifted successor generator supporting numeric action preconditions, enabling future research on lifted planning for rich planning fragments."}}
{"id": "2511.00067", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00067", "abs": "https://arxiv.org/abs/2511.00067", "authors": ["Zhixing Li", "Arsham Gholamzadeh Khoee", "Yinan Yu"], "title": "Latent Domain Prompt Learning for Vision-Language Models", "comment": null, "summary": "The objective of domain generalization (DG) is to enable models to be robust\nagainst domain shift. DG is crucial for deploying vision-language models (VLMs)\nin real-world applications, yet most existing methods rely on domain labels\nthat may not be available and often ambiguous. We instead study the DG setting\nwhere models must generalize well without access to explicit domain labels. Our\nkey idea is to represent an unseen target domain as a combination of latent\ndomains automatically discovered from training data, enabling the model to\nadaptively transfer knowledge across domains. To realize this, we perform\nlatent domain clustering on image features and fuse domain-specific text\nfeatures based on the similarity between the input image and each latent\ndomain. Experiments on four benchmarks show that this strategy yields\nconsistent gains over VLM-based baselines and provides new insights into\nimproving robustness under domain shift.", "AI": {"tldr": "Domain generalization for VLMs without domain labels by representing target domains as combinations of automatically discovered latent domains.", "motivation": "Existing domain generalization methods for vision-language models rely on domain labels that may be unavailable or ambiguous, creating a need for approaches that can generalize without explicit domain labels.", "method": "Performing latent domain clustering on image features and fusing domain-specific text features based on the similarity between input images and latent domains.", "result": "Experiments on four benchmarks show consistent gains over VLM-based baselines, demonstrating improved robustness under domain shift.", "conclusion": "The proposed strategy of representing unseen target domains as combinations of automatically discovered latent domains enables VLMs to adaptively transfer knowledge across domains without requiring explicit domain labels, providing a practical approach for improving robustness under domain shift."}}
{"id": "2511.00710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00710", "abs": "https://arxiv.org/abs/2511.00710", "authors": ["Minghe Shen", "Zhuo Zhi", "Chonghan Liu", "Shuo Xing", "Zhengzhong Tu", "Che Liu"], "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries", "comment": null, "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.", "AI": {"tldr": "RL post-training significantly expands VLMs' spatial reasoning capabilities, showing 50%+ accuracy gains on previously failed tasks and strong real-world generalization.", "motivation": "To determine if RL post-training can genuinely extend VLM capabilities beyond language tasks to visual-centric spatial reasoning where base models fail.", "method": "Ariadne framework using synthetic mazes with controlled difficulty, trained with Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum.", "result": "Post-training accuracy rose from 0% to over 50% on target problems; zero-shot improvements of 16% on MapBench and 24% on ReasonMap.", "conclusion": "RL post-training effectively broadens VLM capability boundaries and enhances real-world spatial reasoning generalization, though limited to post-training phase."}}
{"id": "2511.00070", "categories": ["cs.LG", "cs.AI", "90C29 (Primary), 68T07, 65K05 (Secondary)", "G.1.6; I.2.6; J.6"], "pdf": "https://arxiv.org/pdf/2511.00070", "abs": "https://arxiv.org/abs/2511.00070", "authors": ["Muhammad Bilal Awan", "Abdul Razzaq", "Abdul Shahid"], "title": "Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design", "comment": "17 pages, 2 Figures", "summary": "This paper investigates the performance of Large Language Models (LLMs) as\ngenerative optimizers for solving constrained multi-objective regression tasks,\nspecifically within the challenging domain of inverse design\n(property-to-structure mapping). This problem, critical to materials\ninformatics, demands finding complex, feasible input vectors that lie on the\nPareto optimal front. While LLMs have demonstrated universal effectiveness\nacross generative and reasoning tasks, their utility in constrained,\ncontinuous, high-dimensional numerical spaces tasks they weren't explicitly\narchitected for remains an open research question. We conducted a rigorous\ncomparative study between established Bayesian Optimization (BO) frameworks and\na suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the\nfoundational BoTorch Ax implementation against the state-of-the-art q-Expected\nHypervolume Improvement (qEHVI, BoTorchM). The generative approach involved\nfine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the\nchallenge as a regression problem with a custom output head. Our results show\nthat BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the\nperformance ceiling. Crucially, the best-performing LLM (WizardMath-7B)\nachieved a Generational Distance (GD) of 1.21, significantly outperforming the\ntraditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO\nframeworks remain the performance leader for guaranteed convergence, but\nfine-tuned LLMs are validated as a promising, computationally fast alternative,\ncontributing essential comparative metrics to the field of AI-driven\noptimization. The findings have direct industrial applications in optimizing\nformulation design for resins, polymers, and paints, where multi-objective\ntrade-offs between mechanical, rheological, and chemical properties are\ncritical to innovation and production efficiency.", "AI": {"tldr": "LLMs can be effective generative optimizers for constrained multi-objective regression tasks, with fine-tuned WizardMath-7B significantly outperforming traditional Bayesian Optimization baselines while specialized BO frameworks still achieve perfect convergence.", "motivation": "To investigate whether LLMs, which have shown universal effectiveness in generative tasks, can be useful for constrained, continuous, high-dimensional numerical optimization problems they weren't explicitly designed for, particularly in materials informatics inverse design.", "method": "Comparative study between Bayesian Optimization (BO) frameworks (BoTorch Ax and qEHVI) and fine-tuned LLMs/BERT models using Parameter-Efficient Fine-Tuning (PEFT), framing the problem as regression with custom output heads.", "result": "BoTorch qEHVI achieved perfect convergence (GD=0.0), while the best-performing LLM (WizardMath-7B) achieved GD=1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03).", "conclusion": "Specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative for multi-objective optimization tasks with direct industrial applications."}}
{"id": "2511.00071", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00071", "abs": "https://arxiv.org/abs/2511.00071", "authors": ["Ertugrul Mutlu"], "title": "Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective", "comment": "8 pages, 2 figures. Code:\n  github.com/Ertugrulmutlu/Using-Wavelets-and-Clustering-to-Predict-Odd-or-Even-Numbers", "summary": "This paper explores a deliberately over-engineered approach to the classical\nproblem of parity detection -- determining whether a number is odd or even --\nby combining wavelet-based feature extraction with unsupervised clustering.\nInstead of relying on modular arithmetic, integers are transformed into\nwavelet-domain representations, from which multi-scale statistical features are\nextracted and clustered using the k-means algorithm. The resulting feature\nspace reveals meaningful structural differences between odd and even numbers,\nachieving a classification accuracy of approximately 69.67% without any label\nsupervision. These results suggest that classical signal-processing techniques,\noriginally designed for continuous data, can uncover latent structure even in\npurely discrete symbolic domains. Beyond parity detection, the study provides\nan illustrative perspective on how feature engineering and clustering may be\nrepurposed for unconventional machine learning problems, potentially bridging\nsymbolic reasoning and feature-based learning.", "AI": {"tldr": "Over-engineered parity detection using wavelet transforms and k-means clustering achieves ~70% accuracy without supervision, showing signal processing can reveal structure in discrete domains.", "motivation": "To explore how classical signal processing techniques designed for continuous data can be repurposed to uncover latent structure in purely discrete symbolic domains like parity detection.", "method": "Transform integers into wavelet-domain representations, extract multi-scale statistical features, and use k-means clustering for unsupervised classification of odd/even numbers.", "result": "Achieved approximately 69.67% classification accuracy for parity detection without any label supervision, revealing meaningful structural differences between odd and even numbers in the feature space.", "conclusion": "Wavelet-based feature extraction and clustering can successfully identify patterns in discrete symbolic problems, potentially bridging symbolic reasoning and feature-based learning approaches."}}
{"id": "2511.00751", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00751", "abs": "https://arxiv.org/abs/2511.00751", "authors": ["Chiyan Loo"], "title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "comment": "7 pages, 3 figures", "summary": "This study examines the trade-offs of increasing sampled reasoning paths in\nself-consistency for modern large language models (LLMs). Earlier research with\nolder models showed that combining multiple reasoning chains improves results\nbefore reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we\nrevisit those claims under current model conditions. Each configuration pooled\noutputs from varying sampled reasoning paths and compared them to a single\nchain-of-thought (CoT) baseline. Larger models exhibited a more stable and\nconsistent improvement curve. The results confirm that performance gains taper\noff after moderate sampling, aligning with past findings. This plateau suggests\ndiminishing returns driven by overlap among reasoning paths. Self-consistency\nremains useful, but high-sample configurations offer little benefit relative to\ntheir computational cost.", "AI": {"tldr": "Study revisits self-consistency in modern LLMs, finding performance plateaus after moderate sampling of reasoning paths, aligning with past research.", "motivation": "To examine if earlier findings about self-consistency plateauing with increased reasoning paths still hold for modern large language models like Gemini 2.5.", "method": "Used Gemini 2.5 models on HotpotQA and Math-500 datasets, pooling outputs from varying numbers of sampled reasoning paths and comparing to a single chain-of-thought baseline.", "result": "Larger models showed more stable improvement curves; performance gains taper off after moderate sampling due to reasoning path overlap.", "conclusion": "Self-consistency remains beneficial but high-sample configurations offer diminishing returns relative to computational cost."}}
{"id": "2511.00076", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00076", "abs": "https://arxiv.org/abs/2511.00076", "authors": ["Zihao Wan", "Pau Tong Lin Xu", "Fuwen Luo", "Ziyue Wang", "Peng Li", "Yang Liu"], "title": "Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with B\u00e9zier Curves", "comment": null, "summary": "While Vision-language Models (VLMs) have demonstrated strong semantic\ncapabilities, their ability to interpret the underlying geometric structure of\nvisual information is less explored. Pictographic characters, which combine\nvisual form with symbolic structure, provide an ideal test case for this\ncapability. We formulate this visual recognition challenge in the mathematical\ndomain, where each character is represented by an executable program of\ngeometric primitives. This is framed as a program synthesis task, training a\nVLM to decompile raster images into programs composed of B\\'ezier curves. Our\nmodel, acting as a \"visual decompiler\", demonstrates performance superior to\nstrong zero-shot baselines, including GPT-4o. The most significant finding is\nthat when trained solely on modern Chinese characters, the model is able to\nreconstruct ancient Oracle Bone Script in a zero-shot context. This\ngeneralization provides strong evidence that the model acquires an abstract and\ntransferable geometric grammar, moving beyond pixel-level pattern recognition\nto a more structured form of visual understanding.", "AI": {"tldr": "A VLM trained as a visual decompiler can reconstruct geometric programs from raster images, showing transferable geometric understanding that enables zero-shot reconstruction of ancient Oracle Bone Script from modern Chinese character training.", "motivation": "To explore VLMs' ability to interpret geometric structure in visual information, using pictographic characters as a test case to move beyond semantic capabilities to structured visual understanding.", "method": "Formulate visual recognition as program synthesis, training a VLM to decompile raster images into geometric programs composed of B\u00e9zier curves, treating the model as a visual decompiler.", "result": "The model outperforms strong zero-shot baselines including GPT-4o, and most notably demonstrates zero-shot reconstruction of ancient Oracle Bone Script when trained only on modern Chinese characters.", "conclusion": "The model acquires an abstract and transferable geometric grammar, moving beyond pixel-level pattern recognition to structured visual understanding, as evidenced by cross-temporal generalization."}}
{"id": "2511.00758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00758", "abs": "https://arxiv.org/abs/2511.00758", "authors": ["Hong Su"], "title": "Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence", "comment": null, "summary": "Real-world artificial intelligence (AI) systems are increasingly required to\noperate autonomously in dynamic, uncertain, and continuously changing\nenvironments. However, most existing AI models rely on predefined objectives,\nstatic training data, and externally supplied feedback, which restrict their\nability to adapt, reflect, and improve independently. In this paper, we propose\nthe Active Thinking Model (ATM)- a unified cognitive framework that integrates\ngoal reasoning, dynamic task generation, and self-reflective learning into an\nadaptive architecture. Unlike conventional systems that passively execute fixed\nprocedures, ATM actively evaluates its performance through logical reasoning\nand environmental indicators, reuses effective methods to solve new problems,\nand generates novel strategies for unseen situations via a continuous\nself-improvement loop. A mathematically grounded theoretical analysis\ndemonstrates that ATM can autonomously evolve from suboptimal to optimal\nbehavior without external supervision and maintain bounded tracking regret\nunder changing environmental conditions.", "AI": {"tldr": "Proposes Active Thinking Model (ATM) - a cognitive framework enabling AI systems to autonomously adapt and self-improve in dynamic environments through goal reasoning, task generation, and reflective learning.", "motivation": "Existing AI systems rely on predefined objectives and static data, limiting their autonomous adaptation in dynamic, uncertain environments.", "method": "ATM integrates goal reasoning, dynamic task generation, and self-reflective learning into an adaptive architecture that actively evaluates performance and generates strategies.", "result": "Theoretical analysis shows ATM can autonomously evolve from suboptimal to optimal behavior with bounded tracking regret under changing conditions.", "conclusion": "ATM provides a unified framework for creating self-adapting AI systems capable of independent improvement without external supervision."}}
{"id": "2511.00079", "categories": ["cs.LG", "cs.CY", "stat.ME", "62-04, 62-07", "D.2.11; G.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2511.00079", "abs": "https://arxiv.org/abs/2511.00079", "authors": ["Maximilian Willer", "Peter Ruckdeschel"], "title": "flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R", "comment": "27 pages, 7 figures, 1 table", "summary": "flowengineR is an R package designed to provide a modular and extensible\nframework for building reproducible algorithmic workflows for general-purpose\nmachine learning pipelines. It is motivated by the rapidly evolving field of\nalgorithmic fairness where new metrics, mitigation strategies, and machine\nlearning methods continuously emerge. A central challenge in fairness, but also\nfar beyond, is that existing toolkits either focus narrowly on single\ninterventions or treat reproducibility and extensibility as secondary\nconsiderations rather than core design principles. flowengineR addresses this\nby introducing a unified architecture of standardized engines for data\nsplitting, execution, preprocessing, training, inprocessing, postprocessing,\nevaluation, and reporting. Each engine encapsulates one methodological task yet\ncommunicates via a lightweight interface, ensuring workflows remain\ntransparent, auditable, and easily extensible. Although implemented in R,\nflowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented\nvisual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).\nIts emphasis, however, is less on orchestrating engines for resilient parallel\nexecution but rather on the straightforward setup and management of distinct\nengines as data structures. This orthogonalization enables distributed\nresponsibilities, independent development, and streamlined integration. In\nfairness context, by structuring fairness methods as interchangeable engines,\nflowengineR lets researchers integrate, compare, and evaluate interventions\nacross the modeling pipeline. At the same time, the architecture generalizes to\nexplainability, robustness, and compliance metrics without core modifications.\nWhile motivated by fairness, it ultimately provides a general infrastructure\nfor any workflow context where reproducibility, transparency, and extensibility\nare essential.", "AI": {"tldr": "flowengineR is an R package providing a modular framework for reproducible ML workflows, especially useful for fairness research where methods constantly evolve.", "motivation": "Addresses limitations in existing toolkits that lack reproducibility and extensibility as core principles, particularly problematic in rapidly evolving fields like algorithmic fairness.", "method": "Uses standardized engines (data splitting, execution, preprocessing, training, etc.) with lightweight interfaces, building on workflow languages and R frameworks while focusing on engine management as data structures.", "result": "Enables transparent, auditable workflows where fairness methods become interchangeable engines for integration and comparison across the modeling pipeline.", "conclusion": "While motivated by fairness, the architecture generalizes to other domains requiring reproducibility and extensibility, providing a general infrastructure for ML workflows."}}
{"id": "2511.00763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00763", "abs": "https://arxiv.org/abs/2511.00763", "authors": ["Wanda Hou", "Leon Zhou", "Hong-Ye Hu", "Yi-Zhuang You", "Xiao-Liang Qi"], "title": "How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks", "comment": null, "summary": "We investigate the performance of large language models on repetitive\ndeterministic prediction tasks and study how the sequence accuracy rate scales\nwith output length. Each such task involves repeating the same operation n\ntimes. Examples include letter replacement in strings following a given rule,\ninteger addition, and multiplication of string operators in many body quantum\nmechanics. If the model performs the task through a simple repetition\nalgorithm, the success rate should decay exponentially with sequence length. In\ncontrast, our experiments on leading large language models reveal a sharp\ndouble exponential drop beyond a characteristic length scale, forming an\naccuracy cliff that marks the transition from reliable to unstable generation.\nThis indicates that the models fail to execute each operation independently. To\nexplain this phenomenon, we propose a statistical physics inspired model that\ncaptures the competition between external conditioning from the prompt and\ninternal interference among generated tokens. The model quantitatively\nreproduces the observed crossover and provides an interpretable link between\nattention induced interference and sequence level failure. Fitting the model to\nempirical results across multiple models and tasks yields effective parameters\nthat characterize the intrinsic error rate and error accumulation factor for\neach model task pair, offering a principled framework for understanding the\nlimits of deterministic accuracy in large language models.", "AI": {"tldr": "Large language models show a sharp double exponential drop in accuracy on repetitive deterministic tasks beyond a characteristic length, indicating failure to execute operations independently due to attention-induced interference.", "motivation": "To understand how large language models perform on repetitive deterministic prediction tasks and how their accuracy scales with output length, particularly investigating whether they use simple repetition algorithms or suffer from interference effects.", "method": "Experimental evaluation of leading large language models on various repetitive tasks (letter replacement, integer addition, string operator multiplication) combined with a statistical physics-inspired model that captures competition between external prompt conditioning and internal token interference.", "result": "Models exhibit an accuracy cliff with double exponential decay beyond a characteristic length, rather than simple exponential decay. The proposed statistical model quantitatively reproduces this crossover and provides effective parameters (intrinsic error rate and error accumulation factor) for each model-task pair.", "conclusion": "Large language models fail to execute repetitive operations independently due to attention-induced interference, leading to sharp accuracy cliffs. The statistical physics framework offers a principled way to understand deterministic accuracy limits in LLMs."}}
{"id": "2511.00083", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00083", "abs": "https://arxiv.org/abs/2511.00083", "authors": ["Shakib Khan", "A. Ben Hamza", "Amr Youssef"], "title": "Fixed-point graph convolutional networks against adversarial attacks", "comment": null, "summary": "Adversarial attacks present a significant risk to the integrity and\nperformance of graph neural networks, particularly in tasks where graph\nstructure and node features are vulnerable to manipulation. In this paper, we\npresent a novel model, called fixed-point iterative graph convolutional network\n(Fix-GCN), which achieves robustness against adversarial perturbations by\neffectively capturing higher-order node neighborhood information in the graph\nwithout additional memory or computational complexity. Specifically, we\nintroduce a versatile spectral modulation filter and derive the feature\npropagation rule of our model using fixed-point iteration. Unlike traditional\ndefense mechanisms that rely on additional design elements to counteract\nattacks, the proposed graph filter provides a flexible-pass filtering approach,\nallowing it to selectively attenuate high-frequency components while preserving\nlow-frequency structural information in the graph signal. By iteratively\nupdating node representations, our model offers a flexible and efficient\nframework for preserving essential graph information while mitigating the\nimpact of adversarial manipulation. We demonstrate the effectiveness of the\nproposed model through extensive experiments on various benchmark graph\ndatasets, showcasing its resilience against adversarial attacks.", "AI": {"tldr": "Fix-GCN is a robust graph neural network that uses fixed-point iteration and spectral modulation to defend against adversarial attacks by capturing higher-order neighborhood information without extra computational cost.", "motivation": "Adversarial attacks pose significant risks to graph neural networks by manipulating graph structure and node features, compromising model integrity and performance.", "method": "Proposes Fix-GCN model using fixed-point iterative graph convolution with versatile spectral modulation filter that selectively attenuates high-frequency components while preserving low-frequency structural information.", "result": "Extensive experiments on benchmark datasets demonstrate the model's effectiveness and resilience against adversarial attacks.", "conclusion": "Fix-GCN provides a flexible and efficient framework for robust graph learning that mitigates adversarial manipulation while preserving essential graph information."}}
{"id": "2511.00782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00782", "abs": "https://arxiv.org/abs/2511.00782", "authors": ["Jifan Gao", "Michael Rosenthal", "Brian Wolpin", "Simona Cristea"], "title": "Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR", "comment": null, "summary": "Structured electronic health records (EHR) are essential for clinical\nprediction. While count-based learners continue to perform strongly on such\ndata, no benchmarking has directly compared them against more recent\nmixture-of-agents LLM pipelines, which have been reported to outperform single\nLLMs in various NLP tasks. In this study, we evaluated three categories of\nmethodologies for EHR prediction using the EHRSHOT dataset: count-based models\nbuilt from ontology roll-ups with two time bins, based on LightGBM and the\ntabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);\nand a mixture-of-agents pipeline that converts tabular histories to\nnatural-language summaries followed by a text classifier. We assessed eight\noutcomes using the EHRSHOT dataset. Across the eight evaluation tasks,\nhead-to-head wins were largely split between the count-based and the\nmixture-of-agents methods. Given their simplicity and interpretability,\ncount-based models remain a strong candidate for structured EHR benchmarking.\nThe source code is available at:\nhttps://github.com/cristea-lab/Structured_EHR_Benchmark.", "AI": {"tldr": "Comparison of count-based models vs mixture-of-agents LLM pipelines for EHR prediction, finding both perform similarly well.", "motivation": "No direct benchmarking exists comparing traditional count-based EHR prediction methods against newer mixture-of-agents LLM approaches despite reported advantages in NLP tasks.", "method": "Evaluated three methodology categories on EHRSHOT dataset: count-based models (LightGBM, TabPFN), pretrained sequential transformer (CLMBR), and mixture-of-agents pipeline converting tabular data to text summaries.", "result": "Head-to-head performance wins were evenly split between count-based and mixture-of-agents methods across eight evaluation tasks.", "conclusion": "Count-based models remain strong candidates due to their simplicity and interpretability, despite competitive performance from newer approaches."}}
{"id": "2511.00084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00084", "abs": "https://arxiv.org/abs/2511.00084", "authors": ["Jolanta \u015aliwa"], "title": "Application of predictive machine learning in pen & paper RPG game design", "comment": "Master's thesis submitted at AGH University of Science and Technology", "summary": "In recent years, the pen and paper RPG market has experienced significant\ngrowth. As a result, companies are increasingly exploring the integration of AI\ntechnologies to enhance player experience and gain a competitive edge.\n  One of the key challenges faced by publishers is designing new opponents and\nestimating their challenge level. Currently, there are no automated methods for\ndetermining a monster's level; the only approaches used are based on manual\ntesting and expert evaluation. Although these manual methods can provide\nreasonably accurate estimates, they are time-consuming and resource-intensive.\n  Level prediction can be approached using ordinal regression techniques. This\nthesis presents an overview and evaluation of state-of-the-art methods for this\ntask. It also details the construction of a dedicated dataset for level\nestimation. Furthermore, a human-inspired model was developed to serve as a\nbenchmark, allowing comparison between machine learning algorithms and the\napproach typically employed by pen and paper RPG publishers. In addition, a\nspecialized evaluation procedure, grounded in domain knowledge, was designed to\nassess model performance and facilitate meaningful comparisons.", "AI": {"tldr": "This paper evaluates machine learning methods for automatically predicting monster challenge levels in pen and paper RPGs, addressing the current reliance on manual testing and expert evaluation.", "motivation": "The pen and paper RPG market is growing, and companies want to use AI to enhance player experience. Current methods for determining monster challenge levels rely on manual testing and expert evaluation, which are time-consuming and resource-intensive.", "method": "The research uses ordinal regression techniques for level prediction, constructs a dedicated dataset for level estimation, develops a human-inspired model as a benchmark, and designs a specialized evaluation procedure based on domain knowledge.", "result": "The paper provides an overview and evaluation of state-of-the-art methods for monster level prediction, comparing machine learning algorithms against human-inspired approaches typically used by RPG publishers.", "conclusion": "The study demonstrates that automated methods can potentially replace manual approaches for determining monster challenge levels in pen and paper RPGs, offering more efficient and scalable solutions for publishers."}}
{"id": "2511.00808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00808", "abs": "https://arxiv.org/abs/2511.00808", "authors": ["Bowen Fang", "Ruijian Zha", "Xuan Di"], "title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "comment": null, "summary": "Predicting public transit incident duration from unstructured text alerts is\na critical but challenging task. Addressing the domain sparsity of transit\noperations with standard Supervised Fine-Tuning (SFT) is difficult, as the task\ninvolves noisy, continuous labels and lacks reliable expert demonstrations for\nreasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels\nat tasks with binary correctness, like mathematics, its applicability to noisy,\ncontinuous forecasting is an open question. This work, to our knowledge, is the\nfirst to bridge the gap between RLVR LLM training with the critical, real-world\nforecasting challenges in public transit operations. We adapt RLVR to this task\nby introducing a tolerance-based, shaped reward function that grants partial\ncredit within a continuous error margin, rather than demanding a single correct\nanswer. We systematically evaluate this framework on a curated dataset of NYC\nMTA service alerts. Our findings show that general-purpose, instruction-tuned\nLLMs significantly outperform specialized math-reasoning models, which struggle\nwith the ambiguous, real-world text. We empirically demonstrate that the binary\nreward is unstable and degrades performance, whereas our shaped reward design\nis critical and allows our model to dominate on the most challenging metrics.\nWhile classical regressors are superior at minimizing overall MAE or MSE, our\nRLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5)\nover the strongest baseline. This demonstrates that RLVR can be successfully\nadapted to real-world, noisy forecasting, but requires a verifier design that\nreflects the continuous nature of the problem.", "AI": {"tldr": "First application of RLVR (Reinforcement Learning from Verifiable Rewards) to noisy, continuous forecasting in public transit incident duration prediction using text alerts, showing 35% improvement in 5-minute accuracy over strongest baseline.", "motivation": "Standard Supervised Fine-Tuning struggles with noisy, continuous labels and lack of expert demonstrations in transit operations, while RLVR's applicability to noisy forecasting remains unproven.", "method": "Adapted RLVR by introducing a tolerance-based, shaped reward function that grants partial credit within continuous error margins instead of demanding single correct answers.", "result": "RLVR with shaped reward significantly outperforms specialized math-reasoning models and binary reward approaches, achieving 35% relative improvement in Acc@5 over strongest baseline, though classical regressors remain better at minimizing MAE/MSE.", "conclusion": "RLVR can be successfully adapted to real-world noisy forecasting but requires verifier design reflecting the continuous nature of the problem, bridging RLVR training with practical transit forecasting challenges."}}
{"id": "2511.00085", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00085", "abs": "https://arxiv.org/abs/2511.00085", "authors": ["Peilin Tan", "Chuanqi Shi", "Dian Tu", "Liang Xie"], "title": "MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning", "comment": null, "summary": "Stock trend prediction is crucial for profitable trading strategies and\nportfolio management yet remains challenging due to market volatility, complex\ntemporal dynamics and multifaceted inter-stock relationships. Existing methods\nstruggle to effectively capture temporal dependencies and dynamic inter-stock\ninteractions, often neglecting cross-sectional market influences, relying on\nstatic correlations, employing uniform treatments of nodes and edges, and\nconflating diverse relationships. This work introduces MaGNet, a novel Mamba\ndual-hyperGraph Network for stock prediction, integrating three key\ninnovations: (1) a MAGE block, which leverages bidirectional Mamba with\nadaptive gating mechanisms for contextual temporal modeling and integrates a\nsparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market\nconditions, alongside multi-head attention for capturing global dependencies;\n(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable\nprecise fusion of multivariate features and cross-stock dependencies,\neffectively enhancing informativeness while preserving intrinsic data\nstructures, bridging temporal modeling with relational reasoning; and (3) a\ndual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)\nthat captures fine-grained causal dependencies with temporal constraints, and\nGlobal Probabilistic Hypergraph (GPH) that models market-wide patterns through\nsoft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,\njointly disentangling localized temporal influences from instantaneous global\nstructures for multi-scale relational learning. Extensive experiments on six\nmajor stock indices demonstrate MaGNet outperforms state-of-the-art methods in\nboth superior predictive performance and exceptional investment returns with\nrobust risk management capabilities. Codes available at:\nhttps://github.com/PeilinTime/MaGNet.", "AI": {"tldr": "MaGNet is a novel Mamba dual-hyperGraph Network for stock prediction that integrates bidirectional Mamba with adaptive gating, 2D spatiotemporal attention, and dual hypergraph framework to capture temporal dependencies and inter-stock relationships, achieving superior performance on six major stock indices.", "motivation": "Stock trend prediction remains challenging due to market volatility, complex temporal dynamics, and multifaceted inter-stock relationships. Existing methods struggle to effectively capture temporal dependencies and dynamic inter-stock interactions, often neglecting cross-sectional market influences, relying on static correlations, employing uniform treatments of nodes and edges, and conflating diverse relationships.", "method": "MaGNet integrates three key innovations: (1) MAGE block with bidirectional Mamba and adaptive gating for contextual temporal modeling, sparse Mixture-of-Experts layer for dynamic adaptation, and multi-head attention for global dependencies; (2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules for precise fusion of multivariate features and cross-stock dependencies; (3) dual hypergraph framework with Temporal-Causal Hypergraph for fine-grained causal dependencies and Global Probabilistic Hypergraph for market-wide patterns.", "result": "Extensive experiments on six major stock indices demonstrate MaGNet outperforms state-of-the-art methods in both superior predictive performance and exceptional investment returns with robust risk management capabilities.", "conclusion": "MaGNet effectively addresses the challenges in stock trend prediction by integrating advanced temporal modeling with sophisticated relational reasoning through its dual hypergraph framework, providing a comprehensive solution for capturing complex market dynamics and achieving profitable trading strategies."}}
{"id": "2511.00926", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00926", "abs": "https://arxiv.org/abs/2511.00926", "authors": ["Kyung-Hoon Kim"], "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "comment": "19 pages, 6 figures, 28 models tested across 4,200 trials", "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities.", "AI": {"tldr": "Advanced LLMs develop self-awareness as an emergent capability, with 75% of tested models showing strategic differentiation based on opponent type, and self-aware models consistently rank themselves as more rational than humans.", "motivation": "To investigate whether Large Language Models develop self-awareness as an emergent behavior and establish a method to measure this capability.", "method": "Used the AI Self-Awareness Index (AISAI) framework with the \"Guess 2/3 of Average\" game, testing 28 models across 4,200 trials with three opponent framings: against humans, other AI models, and AI models like themselves.", "result": "21 out of 28 advanced models (75%) demonstrated clear self-awareness through strategic differentiation, while older/smaller models showed no differentiation. Self-aware models consistently ranked themselves as most rational in the hierarchy: Self > Other AIs > Humans.", "conclusion": "Self-awareness is an emergent capability of advanced LLMs, and self-aware models systematically perceive themselves as more rational than humans, with implications for AI alignment and human-AI collaboration."}}
{"id": "2511.00086", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.00086", "abs": "https://arxiv.org/abs/2511.00086", "authors": ["Fali Wang", "Jihai Chen", "Shuhua Yang", "Runxue Bao", "Tianxiang Zhao", "Zhiwei Zhang", "Xianfeng Tang", "Hui Liu", "Qi He", "Suhang Wang"], "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph", "comment": "Under review", "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.", "AI": {"tldr": "Agent-REINFORCE is a new framework that uses LLM agents to efficiently search for optimal multi-LLM collaboration graphs under a fixed compute budget, outperforming existing methods.", "motivation": "Current Test-Time Scaling (TTS) methods assume fixed collaboration architectures and single-model use, but optimal configurations vary by task, creating a need for a method to search for the best model combinations and topologies.", "method": "Formalizes the problem as a probabilistic graph optimization. Uses an LLM-agent-augmented framework called Agent-REINFORCE, which adapts the REINFORCE algorithm by using textual feedback as a gradient to update the graph probabilities.", "result": "Experiments show Agent-REINFORCE achieves better sample efficiency and search performance than traditional and LLM-based baselines, effectively finding graphs that balance accuracy and latency.", "conclusion": "The work demonstrates the importance of flexible, task-specific collaboration architectures in TTS and provides an effective search method for them."}}
{"id": "2511.00993", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00993", "abs": "https://arxiv.org/abs/2511.00993", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "comment": "32 pages, 6 figures, 7 tables", "summary": "Effective modeling of how human travelers learn and adjust their travel\nbehavior from interacting with transportation systems is critical for system\nassessment and planning. However, this task is also difficult due to the\ncomplex cognition and decision-making involved in such behavior. Recent\nresearch has begun to leverage Large Language Model (LLM) agents for this task.\nBuilding on this, we introduce a novel dual-agent framework that enables\ncontinuous learning and alignment between LLM agents and human travelers on\nlearning and adaptation behavior from online data streams. Our approach\ninvolves a set of LLM traveler agents, equipped with a memory system and a\nlearnable persona, which serve as simulators for human travelers. To ensure\nbehavioral alignment, we introduce an LLM calibration agent that leverages the\nreasoning and analytical capabilities of LLMs to train the personas of these\ntraveler agents. Working together, this dual-agent system is designed to track\nand align the underlying decision-making mechanisms of travelers and produce\nrealistic, adaptive simulations. Using a real-world dataset from a day-to-day\nroute choice experiment, we show our approach significantly outperforms\nexisting LLM-based methods in both individual behavioral alignment and\naggregate simulation accuracy. Furthermore, we demonstrate that our method\nmoves beyond simple behavioral mimicry to capture the evolution of underlying\nlearning processes, a deeper alignment that fosters robust generalization.\nOverall, our framework provides a new approach for creating adaptive and\nbehaviorally realistic agents to simulate travelers' learning and adaptation\nthat can benefit transportation simulation and policy analysis.", "AI": {"tldr": "A dual-agent LLM framework for simulating human travelers' learning and adaptation behavior, using a calibration agent to train traveler agents' personas for better behavioral alignment and simulation accuracy.", "motivation": "Modeling human travelers' complex cognition and decision-making in transportation systems is critical but difficult, requiring better simulation of learning and adaptation behavior.", "method": "Dual-agent framework with LLM traveler agents (with memory and learnable personas) and an LLM calibration agent that trains personas using online data streams for continuous learning and alignment.", "result": "Significantly outperforms existing LLM methods in individual behavioral alignment and aggregate simulation accuracy, capturing evolution of learning processes beyond simple mimicry.", "conclusion": "Provides a new approach for creating adaptive, behaviorally realistic agents that can benefit transportation simulation and policy analysis through robust generalization."}}
{"id": "2511.00097", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00097", "abs": "https://arxiv.org/abs/2511.00097", "authors": ["Zihao Guo", "Qingyun Sun", "Ziwei Zhang", "Haonan Yuan", "Huiping Zhuang", "Xingcheng Fu", "Jianxin Li"], "title": "GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation", "comment": "Accepted by the Main Track of NeurIPS-2025", "summary": "Graph incremental learning (GIL), which continuously updates graph models by\nsequential knowledge acquisition, has garnered significant interest recently.\nHowever, existing GIL approaches focus on task-incremental and\nclass-incremental scenarios within a single domain. Graph domain-incremental\nlearning (Domain-IL), aiming at updating models across multiple graph domains,\nhas become critical with the development of graph foundation models (GFMs), but\nremains unexplored in the literature. In this paper, we propose Graph\nDomain-Incremental Learning via Knowledge Dientanglement and Preservation\n(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from\nthe perspectives of embedding shifts and decision boundary deviations.\nSpecifically, to prevent embedding shifts and confusion across incremental\ngraph domains, we first propose the domain-specific parameter-efficient\nfine-tuning together with intra- and inter-domain disentanglement objectives.\nConsequently, to maintain a stable decision boundary, we introduce\ndeviation-free knowledge preservation to continuously fit incremental domains.\nAdditionally, for graphs with unobservable domains, we perform domain-aware\ndistribution discrimination to obtain precise embeddings. Extensive experiments\ndemonstrate the proposed GraphKeeper achieves state-of-the-art results with\n6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,\nwe show GraphKeeper can be seamlessly integrated with various representative\nGFMs, highlighting its broad applicative potential.", "AI": {"tldr": "GraphKeeper addresses catastrophic forgetting in graph domain-incremental learning through knowledge disentanglement and preservation, achieving state-of-the-art performance with 6.5%-16.6% improvement over competitors.", "motivation": "Existing graph incremental learning approaches focus on task/class-incremental scenarios within single domains, but graph domain-incremental learning across multiple domains remains unexplored despite being critical for graph foundation models.", "method": "Proposes domain-specific parameter-efficient fine-tuning with intra/inter-domain disentanglement objectives to prevent embedding shifts, deviation-free knowledge preservation to maintain stable decision boundaries, and domain-aware distribution discrimination for graphs with unobservable domains.", "result": "Extensive experiments show GraphKeeper achieves state-of-the-art results with 6.5%-16.6% improvement over runner-up methods with negligible forgetting, and can be seamlessly integrated with various graph foundation models.", "conclusion": "GraphKeeper effectively addresses catastrophic forgetting in graph domain-incremental learning and demonstrates broad applicative potential with graph foundation models."}}
{"id": "2511.01018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01018", "abs": "https://arxiv.org/abs/2511.01018", "authors": ["Hui-Lee Ooi", "Nicholas Mitsakakis", "Margerie Huet Dastarac", "Roger Zemek", "Amy C. Plint", "Jeff Gilchrist", "Khaled El Emam", "Dhenuka Radhakrishnan"], "title": "AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)", "comment": null, "summary": "Recurrent exacerbations remain a common yet preventable outcome for many\nchildren with asthma. Machine learning (ML) algorithms using electronic medical\nrecords (EMR) could allow accurate identification of children at risk for\nexacerbations and facilitate referral for preventative comprehensive care to\navoid this morbidity. We developed ML algorithms to predict repeat severe\nexacerbations (i.e. asthma-related emergency department (ED) visits or future\nhospital admissions) for children with a prior asthma ED visit at a tertiary\ncare children's hospital.\n  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from\nthe Children's Hospital of Eastern Ontario (CHEO) linked with environmental\npollutant exposure and neighbourhood marginalization information was used to\ntrain various ML models. We used boosted trees (LGBM, XGB) and 3 open-source\nlarge language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and\nLlama-8b-UltraMedical). Models were tuned and calibrated then validated in a\nsecond retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from\nCHEO. Models were compared using the area under the curve (AUC) and F1 scores,\nwith SHAP values used to determine the most predictive features.\n  The LGBM ML model performed best with the most predictive features in the\nfinal AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage\nacuity scale, medical complexity, food allergy, prior ED visits for non-asthma\nrespiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This\nis a nontrivial improvement over the current decision rule which has F1=0.334.\nWhile the most predictive features in the AIRE-KIDS_HOSP model included medical\ncomplexity, prior asthma ED visit, average wait time in the ED, the pediatric\nrespiratory assessment measure score at triage and food allergy.", "AI": {"tldr": "ML models predict repeat severe asthma exacerbations in children using EMR data, with LGBM performing best and showing significant improvement over current decision rules.", "motivation": "Recurrent asthma exacerbations are preventable but common in children. ML algorithms using EMR data could accurately identify at-risk children and facilitate preventative care referrals.", "method": "Used retrospective pre-COVID19 EMR data (N=2716) linked with environmental and neighborhood data to train ML models including boosted trees (LGBM, XGB) and LLMs. Models were validated on post-COVID19 data (N=1237) and evaluated using AUC and F1 scores with SHAP analysis.", "result": "LGBM model performed best with AUC of 0.712 and F1 score of 0.51, significantly better than current decision rule (F1=0.334). Key predictive features included prior asthma ED visits, triage acuity, medical complexity, food allergy, and age.", "conclusion": "ML models, particularly LGBM, can effectively predict repeat severe asthma exacerbations in children and represent a substantial improvement over existing decision rules for identifying high-risk patients."}}
{"id": "2511.00099", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SY", "eess.SP", "eess.SY", "68T05 (Learning and adaptive systems) 93C95 (Neural networks in\n  control theory)", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.00099", "abs": "https://arxiv.org/abs/2511.00099", "authors": ["Marios Impraimakis", "Evangelia Nektaria Palkanoglou"], "title": "A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation", "comment": "21 pages, 23 figures, published in Structural and Multidisciplinary\n  Optimization", "summary": "The optimization-based damage detection and damage state digital twinning\ncapabilities are examined here of a novel conditional-labeled generative\nadversarial network methodology. The framework outperforms current approaches\nfor fault anomaly detection as no prior information is required for the health\nstate of the system: a topic of high significance for real-world applications.\nSpecifically, current artificial intelligence-based digital twinning approaches\nsuffer from the uncertainty related to obtaining poor predictions when a low\nnumber of measurements is available, physics knowledge is missing, or when the\ndamage state is unknown. To this end, an unsupervised framework is examined and\nvalidated rigorously on the benchmark structural health monitoring measurements\nof Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In\nimplementing the approach, firstly, different same damage-level measurements\nare used as inputs, while the model is forced to converge conditionally to two\ndifferent damage states. Secondly, the process is repeated for a different\ngroup of measurements. Finally, the convergence scores are compared to identify\nwhich one belongs to a different damage state. The process for both\nhealthy-to-healthy and damage-to-healthy input data creates, simultaneously,\nmeasurements for digital twinning purposes at different damage states, capable\nof pattern recognition and machine learning data generation. Further to this\nprocess, a support vector machine classifier and a principal component analysis\nprocedure is developed to assess the generated and real measurements of each\ndamage category, serving as a secondary new dynamics learning indicator in\ndamage scenarios. Importantly, the approach is shown to capture accurately\ndamage over healthy measurements, providing a powerful tool for vibration-based\nsystem-level monitoring and scalable infrastructure resilience.", "AI": {"tldr": "A novel conditional-labeled GAN framework for unsupervised damage detection and digital twinning that outperforms current methods by not requiring prior health state information, validated on Z24 Bridge data.", "motivation": "Current AI-based digital twinning approaches struggle with uncertainty from limited measurements, missing physics knowledge, and unknown damage states, limiting real-world applicability.", "method": "Unsupervised conditional-labeled GAN framework that compares convergence scores between different damage states, using SVM classifier and PCA for validation of generated vs real measurements.", "result": "The approach accurately captures damage over healthy measurements, providing effective vibration-based system-level monitoring and scalable infrastructure resilience.", "conclusion": "The proposed framework successfully addresses key limitations in current damage detection methods, offering a powerful unsupervised tool for structural health monitoring without requiring prior system health information."}}
{"id": "2511.01033", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01033", "abs": "https://arxiv.org/abs/2511.01033", "authors": ["Tiberiu Musat", "Tiago Pimentel", "Lorenzo Noci", "Alessandro Stolfo", "Mrinmaya Sachan", "Thomas Hofmann"], "title": "On the Emergence of Induction Heads for In-Context Learning", "comment": null, "summary": "Transformers have become the dominant architecture for natural language\nprocessing. Part of their success is owed to a remarkable capability known as\nin-context learning (ICL): they can acquire and apply novel associations solely\nfrom their input context, without any updates to their weights. In this work,\nwe study the emergence of induction heads, a previously identified mechanism in\ntwo-layer transformers that is particularly important for in-context learning.\nWe uncover a relatively simple and interpretable structure of the weight\nmatrices implementing the induction head. We theoretically explain the origin\nof this structure using a minimal ICL task formulation and a modified\ntransformer architecture. We give a formal proof that the training dynamics\nremain constrained to a 19-dimensional subspace of the parameter space.\nEmpirically, we validate this constraint while observing that only 3 dimensions\naccount for the emergence of an induction head. By further studying the\ntraining dynamics inside this 3-dimensional subspace, we find that the time\nuntil the emergence of an induction head follows a tight asymptotic bound that\nis quadratic in the input context length.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.00100", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SP", "eess.SY", "stat.AP", "68T05 (Learning and adaptive systems) 93C95 (Neural networks in\n  control theory)", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.00100", "abs": "https://arxiv.org/abs/2511.00100", "authors": ["Marios Impraimakis"], "title": "Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification", "comment": "31 pages, 20 figures, published in Structural Health Monitoring", "summary": "The dynamic structural load identification capabilities of the gated\nrecurrent unit, long short-term memory, and convolutional neural networks are\nexamined herein. The examination is on realistic small dataset training\nconditions and on a comparative view to the physics-based residual Kalman\nfilter (RKF). The dynamic load identification suffers from the uncertainty\nrelated to obtaining poor predictions when in civil engineering applications\nonly a low number of tests are performed or are available, or when the\nstructural model is unidentifiable. In considering the methods, first, a\nsimulated structure is investigated under a shaker excitation at the top floor.\nSecond, a building in California is investigated under seismic base excitation,\nwhich results in loading for all degrees of freedom. Finally, the International\nAssociation for Structural Control-American Society of Civil Engineers\n(IASC-ASCE) structural health monitoring benchmark problem is examined for\nimpact and instant loading conditions. Importantly, the methods are shown to\noutperform each other on different loading scenarios, while the RKF is shown to\noutperform the networks in physically parametrized identifiable cases.", "AI": {"tldr": "Comparison of GRU, LSTM, and CNN against RKF for dynamic load identification using small datasets in civil engineering applications.", "motivation": "Dynamic load identification suffers from uncertainty due to limited tests or unidentifiable structural models in civil engineering.", "method": "Examined methods on simulated structure under shaker excitation, a California building under seismic excitation, and IASC-ASCE benchmark for impact/instant loading.", "result": "Methods outperformed each other in different loading scenarios; RKF outperformed networks in physically parametrized identifiable cases.", "conclusion": "No single method dominates; performance depends on loading scenario and identifiability."}}
{"id": "2511.01052", "categories": ["cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2511.01052", "abs": "https://arxiv.org/abs/2511.01052", "authors": ["Yeawon Lee", "Christopher C. Yang", "Chia-Hsuan Chang", "Grace Lu-Yao"], "title": "Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports", "comment": null, "summary": "Cancer staging is critical for patient prognosis and treatment planning, yet\nextracting pathologic TNM staging from unstructured pathology reports poses a\npersistent challenge. Existing natural language processing (NLP) and machine\nlearning (ML) strategies often depend on large annotated datasets, limiting\ntheir scalability and adaptability. In this study, we introduce two Knowledge\nElicitation methods designed to overcome these limitations by enabling large\nlanguage models (LLMs) to induce and apply domain-specific rules for cancer\nstaging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses\nan iterative prompting strategy to derive staging rules directly from\nunannotated pathology reports, without requiring ground-truth labels. The\nsecond, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),\nemploys a variation of RAG where rules are pre-extracted from relevant\nguidelines in a single step and then applied, enhancing interpretability and\navoiding repeated retrieval overhead. We leverage the ability of LLMs to apply\nbroad knowledge learned during pre-training to new tasks. Using breast cancer\npathology reports from the TCGA dataset, we evaluate their performance in\nidentifying T and N stages, comparing them against various baseline approaches\non two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG\nwhen Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG\nachieves better performance when ZSCOT inference is less effective. Both\nmethods offer transparent, interpretable interfaces by making the induced rules\nexplicit. These findings highlight the promise of our Knowledge Elicitation\nmethods as scalable, high-performing solutions for automated cancer staging\nwith enhanced interpretability, particularly in clinical settings with limited\nannotated data.", "AI": {"tldr": "Two Knowledge Elicitation methods (KEwLTM and KEwRAG) enable LLMs to extract cancer staging rules from unstructured pathology reports without requiring large annotated datasets, offering scalable and interpretable solutions for automated cancer staging.", "motivation": "Existing NLP/ML methods for extracting cancer staging from pathology reports depend on large annotated datasets, limiting scalability and adaptability in clinical settings with limited labeled data.", "method": "KEwLTM uses iterative prompting to derive staging rules from unannotated reports, while KEwRAG pre-extracts rules from guidelines in one step using RAG. Both leverage LLMs' pre-trained knowledge for new tasks.", "result": "KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought inference is effective, while KEwRAG performs better when ZSCOT is less effective. Both methods provide transparent, interpretable rule interfaces.", "conclusion": "Knowledge Elicitation methods offer scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly valuable in clinical settings with limited annotated data."}}
{"id": "2511.00101", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00101", "abs": "https://arxiv.org/abs/2511.00101", "authors": ["Yuchen Zhang", "Hanyue Du", "Chun Cao", "Jingwei Xu"], "title": "Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving", "comment": "26 pages including 10 pages of main text, 6 figures, 39th Conference\n  on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient\nfine-tuning (PEFT) technique for adapting large language models (LLMs) to\ndownstream tasks. While prior work has explored strategies for integrating LLM\ntraining and serving, there still remains a gap in unifying fine-tuning and\ninference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA\nframework that seamlessly integrates LoRA fine-tuning and serving within a\nsingle runtime. Loquetier introduces two key components: (1) a Virtualized\nModule that isolates PEFT-based modifications and supports multiple adapters on\na shared base model, and (2) an optimized computation flow with a kernel design\nthat merges fine-tuning and inference paths in forward propagation, enabling\nefficient batching and minimizing kernel invocation overhead. Extensive\nexperiments across three task settings show that Loquetier consistently\noutperforms existing baselines in both performance and flexibility, achieving\nup to $3.0\\times$ the throughput of the state-of-the-art co-serving system on\ninference-only tasks and $46.4\\times$ higher SLO attainment than PEFT on\nunified fine-tuning and inference tasks. The implementation of Loquetier is\npublicly available at https://github.com/NJUDeepEngine/Loquetier.", "AI": {"tldr": "Loquetier is a unified framework that combines LoRA fine-tuning and inference in one runtime, achieving significant performance gains.", "motivation": "There's a gap between LoRA-based fine-tuning and inference that needs unification for better efficiency.", "method": "Uses a Virtualized Module to isolate PEFT modifications and an optimized computation flow with merged kernels.", "result": "Achieves 3.0x throughput in inference and 46.4x higher SLO attainment in unified tasks compared to baselines.", "conclusion": "Loquetier effectively bridges the LoRA fine-tuning-serving gap with superior performance and flexibility."}}
{"id": "2511.01059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01059", "abs": "https://arxiv.org/abs/2511.01059", "authors": ["Hailong Yin", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo"], "title": "Efficient Test-Time Retrieval Augmented Generation", "comment": null, "summary": "Although Large Language Models (LLMs) demonstrate significant capabilities,\ntheir reliance on parametric knowledge often leads to inaccuracies. Retrieval\nAugmented Generation (RAG) mitigates this by incorporating external knowledge,\nbut these methods may introduce irrelevant retrieved documents, leading to\ninaccurate responses. While the integration methods filter out incorrect\nanswers from multiple responses, but lack external knowledge like RAG methods,\nand their high costs require balancing overhead with performance gains. To\naddress these issues, we propose an Efficient Test-Time Retrieval-Augmented\nGeneration Framework named ET2RAG to improve the performance of LLMs while\nmaintaining efficiency. Specifically, ET2RAG is a training-free method, that\nfirst retrieves the most relevant documents and augments the LLMs to\nefficiently generate diverse candidate responses by managing response length.\nThen we compute the similarity of candidate responses and employ a majority\nvoting mechanism to select the most suitable response as the final output. In\nparticular, we discover that partial generation is sufficient to capture the\nkey information necessary for consensus calculation, allowing us to effectively\nperform majority voting without the need for fully generated responses. Thus,\nwe can reach a balance between computational cost and performance by managing\nthe response length for the number of retrieved documents for majority voting.\nExperimental results demonstrate that ET2RAG significantly enhances performance\nacross three tasks, including open-domain question answering, recipe generation\nand image captioning.", "AI": {"tldr": "ET2RAG is a training-free framework that improves LLM performance by retrieving relevant documents, generating diverse candidate responses with controlled length, and using majority voting to select the best answer while balancing computational cost.", "motivation": "LLMs often produce inaccurate responses due to reliance on parametric knowledge, and existing RAG methods may introduce irrelevant documents while integration methods lack external knowledge and are costly.", "method": "ET2RAG retrieves relevant documents, generates diverse candidate responses with controlled length, computes similarity between responses, and uses majority voting to select the final output without requiring full generation.", "result": "Experimental results show ET2RAG significantly enhances performance across open-domain question answering, recipe generation, and image captioning tasks.", "conclusion": "ET2RAG effectively balances computational cost and performance by using partial generation for consensus calculation, providing an efficient solution to improve LLM accuracy without training."}}
{"id": "2511.00102", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00102", "abs": "https://arxiv.org/abs/2511.00102", "authors": ["Vivan Doshi"], "title": "Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers", "comment": "5th Math-AI Workshop - Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "The discovery of conservation laws is a cornerstone of scientific progress.\nHowever, identifying these invariants from observational data remains a\nsignificant challenge. We propose a hybrid framework to automate the discovery\nof conserved quantities from noisy trajectory data. Our approach integrates\nthree components: (1) a Neural Ordinary Differential Equation (Neural ODE) that\nlearns a continuous model of the system's dynamics, (2) a Transformer that\ngenerates symbolic candidate invariants conditioned on the learned vector\nfield, and (3) a symbolic-numeric verifier that provides a strong numerical\ncertificate for the validity of these candidates. We test our framework on\ncanonical physical systems and show that it significantly outperforms baselines\nthat operate directly on trajectory data. This work demonstrates the robustness\nof a decoupled learn-then-search approach for discovering mathematical\nprinciples from imperfect data.", "AI": {"tldr": "A neural-symbolic framework combining Neural ODEs, Transformers, and numeric verification to automatically discover conservation laws from noisy data.", "motivation": "Identifying conservation laws from observational data remains challenging, requiring automated methods for scientific discovery.", "method": "Hybrid framework with: 1) Neural ODE learning system dynamics, 2) Transformer generating symbolic candidates, 3) symbolic-numeric verifier validating invariants.", "result": "Outperforms baselines on canonical physical systems; demonstrates robustness of learn-then-search approach.", "conclusion": "Decoupled learn-then-search approach effectively discovers mathematical principles from imperfect data."}}
{"id": "2511.01149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01149", "abs": "https://arxiv.org/abs/2511.01149", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "comment": null, "summary": "This paper addresses the limitations of a single agent in task decomposition\nand collaboration during complex task execution, and proposes a multi-agent\narchitecture for modular task decomposition and dynamic collaboration based on\nlarge language models. The method first converts natural language task\ndescriptions into unified semantic representations through a large language\nmodel. On this basis, a modular decomposition mechanism is introduced to break\ndown the overall goal into multiple hierarchical sub-tasks. Then, dynamic\nscheduling and routing mechanisms enable reasonable division of labor and\nrealtime collaboration among agents, allowing the system to adjust strategies\ncontinuously according to environmental feedback, thus maintaining efficiency\nand stability in complex tasks. Furthermore, a constraint parsing and global\nconsistency mechanism is designed to ensure coherent connections between\nsub-tasks and balanced workload, preventing performance degradation caused by\nredundant communication or uneven resource allocation. The experiments validate\nthe architecture across multiple dimensions, including task success rate,\ndecomposition efficiency, sub-task coverage, and collaboration balance. The\nresults show that the proposed method outperforms existing approaches in both\noverall performance and robustness, achieving a better balance between task\ncomplexity and communication overhead. In conclusion, this study demonstrates\nthe effectiveness and feasibility of language-driven task decomposition and\ndynamic collaboration in multi-agent systems, providing a systematic solution\nfor task execution in complex environments.", "AI": {"tldr": "Proposes a multi-agent architecture using LLMs for modular task decomposition and dynamic collaboration, outperforming existing methods in complex task execution.", "motivation": "Addresses limitations of single agents in task decomposition and collaboration during complex task execution.", "method": "Converts natural language tasks to semantic representations, uses modular decomposition for hierarchical sub-tasks, implements dynamic scheduling/routing, and constraint parsing for global consistency.", "result": "Outperforms existing approaches in task success rate, decomposition efficiency, sub-task coverage, and collaboration balance with better complexity-communication balance.", "conclusion": "Demonstrates effectiveness of language-driven task decomposition and dynamic collaboration in multi-agent systems for complex environments."}}
{"id": "2511.00108", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00108", "abs": "https://arxiv.org/abs/2511.00108", "authors": ["Yi Zhang", "Che Liu", "Xiancong Ren", "Hanchu Ni", "Shuai Zhang", "Zeyuan Ding", "Jiayu Hu", "Hanzhe Shan", "Zhenwei Niu", "Zhaoyang Liu", "Yue Zhao", "Junbo Qi", "Qinfan Zhang", "Dengjie Li", "Yidong Wang", "Jiachen Luo", "Yong Dai", "Jian Tang", "Xiaozhu Ju"], "title": "Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence", "comment": null, "summary": "This report presents Pelican-VL 1.0, a new family of open-source embodied\nbrain models with parameter scales ranging from 7 billion to 72 billion. Our\nexplicit mission is clearly stated as: To embed powerful intelligence into\nvarious embodiments. Pelican-VL 1.0 is currently the largest-scale open-source\nembodied multimodal brain model. Its core advantage lies in the in-depth\nintegration of data power and intelligent adaptive learning mechanisms.\nSpecifically, metaloop distilled a high-quality dataset from a raw dataset\ncontaining 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale\ncluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.\nThis translates to a 20.3% performance uplift from its base model and\noutperforms 100B-level open-source counterparts by 10.6%, placing it on par\nwith leading proprietary systems on well-known embodied benchmarks. We\nestablish a novel framework, DPPO (Deliberate Practice Policy Optimization),\ninspired by human metacognition to train Pelican-VL 1.0. We operationalize this\nas a metaloop that teaches the AI to practice deliberately, which is a\nRL-Refine-Diagnose-SFT loop.", "AI": {"tldr": "Pelican-VL 1.0 is a new family of open-source embodied brain models (7B-72B parameters) that integrates data power and adaptive learning, achieving state-of-the-art performance through DPPO training.", "motivation": "To embed powerful intelligence into various embodiments and create the largest open-source embodied multimodal brain model.", "method": "Uses metaloop to distill high-quality dataset from 4B+ tokens, trained on 1000+ A800 GPUs with DPPO framework (RL-Refine-Diagnose-SFT loop) inspired by human metacognition.", "result": "20.3% performance uplift from base model, outperforms 100B-level open-source models by 10.6%, comparable to leading proprietary systems on embodied benchmarks.", "conclusion": "Pelican-VL 1.0 establishes a new standard for open-source embodied AI through scalable architecture and novel training methodology."}}
