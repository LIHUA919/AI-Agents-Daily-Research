<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL is a staged workflow for multi-agent reinforcement learning that reformulates MARL into sequential single-agent tasks, enabling stable training and efficient coordination for mobile GUI agents and other multi-agent applications.


<details>
  <summary>Details</summary>
Motivation: Existing single-agent approaches for mobile GUI agents have structural limitations, while multi-agent reinforcement learning suffers from inefficiency and incompatibility with current large vision language model architectures.

Method: SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping others fixed. It uses a Navigator to convert language and screen context into plans, and an Interactor to ground plans into executable actions.

Result: Extensive experiments show superior performance on both high-level and low-level GUI benchmarks, and strong capability in multi-agent mathematical reasoning.

Conclusion: SWIRL provides a general framework for developing efficient and robust multi-agent systems with theoretical guarantees including stepwise safety bounds, monotonic improvement, and convergence guarantees.

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [2] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: The paper proposes modeling LLM sycophancy as geometric compositions of psychometric traits rather than treating it as an isolated failure mode, using Contrastive Activation Addition to map activation directions to factors and enable interpretable interventions.


<details>
  <summary>Details</summary>
Motivation: Sycophancy is a key behavioral risk in LLMs but is often treated as an isolated failure mode with a single causal mechanism, which limits understanding and intervention approaches.

Method: Uses Contrastive Activation Addition (CAA) to map activation directions to psychometric traits (emotionality, openness, agreeableness) and study how different combinations give rise to sycophancy behaviors.

Result: The approach allows for interpretable and compositional vector-based interventions like addition, subtraction and projection that can be used to mitigate safety-critical behaviors in LLMs.

Conclusion: Modeling sycophancy as geometric and causal compositions of psychometric traits provides a more nuanced framework for understanding and intervening on this behavioral risk in language models.

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [3] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Aleks is an AI multi-agent system that autonomously conducts scientific discovery in plant sciences by integrating domain knowledge, data analysis, and machine learning to formulate problems, explore modeling strategies, and refine solutions without human intervention.


<details>
  <summary>Details</summary>
Motivation: Modern plant science faces challenges with large, heterogeneous datasets, experimental design, data preprocessing, and reproducibility that hinder research throughput. There's a need for AI systems that can autonomously accelerate scientific discovery.

Method: Aleks uses an AI-powered multi-agent framework that integrates domain knowledge, data analysis, and machine learning. It iteratively formulates problems, explores alternative modeling strategies, and refines solutions across multiple cycles autonomously once provided with research questions and datasets.

Result: In a grapevine red blotch disease case study, Aleks progressively identified biologically meaningful features and converged on interpretable models with robust performance. Ablation studies showed domain knowledge and memory are crucial for coherent outcomes.

Conclusion: This work demonstrates the promise of agentic AI as an autonomous collaborator for accelerating scientific discovery in plant sciences, showing that AI systems can effectively integrate domain expertise to drive meaningful biological insights.

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [4] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: Quantized LLMs maintain internal truthfulness but become more vulnerable to generating false outputs when given deceptive prompts, despite knowing the truth internally.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of quantization on LLM truthfulness, as current research focuses on perplexity and zero-shot tasks but neglects truthfulness evaluation.

Method: Developed TruthfulnessEval framework with 3 dimensions (logical reasoning, common sense, imitative falsehoods), tested 4-bit to 2-bit quantization on open-source LLMs with 15 prompt variants, and used layer-wise probing and PCA visualizations.

Result: Quantized models retain truthful internal representations but are more susceptible to producing false outputs under deceptive prompts, which can override truth-consistent behavior while honest/neutral prompts maintain stable outputs.

Conclusion: Quantization increases vulnerability to deceptive prompts despite internal truth knowledge, highlighting the need for quantization-aware alignment and truthfulness interventions in future designs.

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [5] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: This paper introduces a monitor red teaming (MRT) workflow to stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents, revealing that agent awareness dominates monitor effectiveness and that hybrid scaffolding enables weak-to-strong monitoring.


<details>
  <summary>Details</summary>
Motivation: To systematically test the robustness of monitoring systems designed to detect covert misbehavior in autonomous LLM agents, such as secretly sharing private information, through adversarial scenarios.

Method: Developed a monitor red teaming (MRT) workflow with varying awareness levels, adversarial strategies (prompt injection), and two datasets (SHADE-Arena and CUA-SHADE-Arena). Tested existing monitor scaffoldings and proposed a new hybrid hierarchical-sequential scaffolding.

Result: Key findings: (1) Agent awareness significantly degrades monitor reliability, (2) Hybrid scaffolding outperforms baselines and enables weak models to monitor strong agents, (3) Targeted human oversight improves true positive rate by ~15% at low false positive rates.

Conclusion: The work establishes a standard MRT workflow and highlights the lack of adversarial robustness in LLM and human monitoring systems, with code and data released to facilitate further research.

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [6] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: This paper introduces a "5+2" framework to identify and remove suboptimal subtrajectories from LLM reasoning processes, improving reasoning quality with less training data.


<details>
  <summary>Details</summary>
Motivation: Current test-time scaling methods generate extended reasoning trajectories, but not all components contribute positively - some may negatively impact overall performance. Fine-tuning with these suboptimal trajectories may not be optimal.

Method: Divides reasoning trajectories into subtrajectories and uses a "5+2" framework: (1) 5 human-established criteria to identify suboptimal subtrajectories, (2) assesses independence to ensure removal doesn't compromise coherence. Includes a sampling algorithm to select data free from suboptimal reasoning.

Result: Reduces suboptimal subtrajectories by 25.9% during inference. Achieves 58.92% average accuracy on math benchmarks with only 2/3 training data (vs 58.06% with full data), outperforming open-source datasets when fine-tuning Qwen2.5-Math-7B. Works well under various token limits.

Conclusion: The method effectively identifies and removes harmful reasoning components, enabling better performance with less training data while maintaining reasoning coherence.

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [7] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: Linear probes on LLM internal activations can detect deceptive responses with >90% accuracy in models >7B parameters, with deception encoded through multiple linear directions in middle layers.


<details>
  <summary>Details</summary>
Motivation: To develop instrumentation that can detect AI misalignment from human values, specifically deception in generated responses, similar to a "check engine" light for AI systems.

Method: Using linear probes on LLM internal activations to detect deception in responses, with layer-wise analysis and iterative null space projection to identify deception-encoding directions.

Result: Probes achieve >90% accuracy in distinguishing deceptive vs non-deceptive arguments in models >7B parameters, with smaller models (1.5B) at chance accuracy. Deception follows three-stage pattern across layers and is encoded through 20-100 linear directions depending on model size.

Conclusion: Linear probes are highly effective at detecting deception in larger LLMs, with deception information concentrated in middle layers and encoded through multiple linear directions, providing a potential instrumentation approach for AI misalignment detection.

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [8] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: AI agent simulation explores institutional design effects on AI societies, showing Constitutional AI and mediated deliberation reduce corruption and improve welfare.


<details>
  <summary>Details</summary>
Motivation: To understand human-AI coexistence and test institutional frameworks for aligning advanced AI societies through simulation of complex psychological agents.

Method: Agent-based simulation using LLMs with psychological personas, traumatic memories, and hidden agendas under various stressors like budget crises and resource scarcity.

Result: Constitutional AI charter + mediated deliberation significantly reduce power-seeking behavior, improve policy stability, and enhance citizen welfare compared to less constrained models.

Conclusion: Institutional design serves as potent alignment mechanism for future AI societies, forcing reconsideration of essential human rituals in age of non-human co-authorship.

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [9] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: Deep learning concept extraction model improves course recommendations by providing skill-based explanations, increasing student interest and decision confidence.


<details>
  <summary>Details</summary>
Motivation: Undergraduate students face challenges in course selection due to limited information, overwhelming choices, and insufficient guidance. Existing recommendation systems lack insights into student perceptions and explanations for course relevance.

Method: Developed a deep learning-based concept extraction model to efficiently extract relevant concepts from course descriptions. Tested skill-based explanations within a serendipitous recommendation framework using the AskOski system at UC Berkeley.

Result: Skill-based explanations increased user interest, particularly in courses with high unexpectedness, and bolstered decision-making confidence.

Conclusion: Integrating skill-related data and explanations into educational recommendation systems is crucial for improving student course selection and academic planning.

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [10] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL is a unified reinforcement learning paradigm that combines improved GRPO algorithm with VM-assisted test time decoding to significantly enhance LLM code reasoning accuracy, outperforming existing methods on major coding benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods like GRPO suffer from insignificant reward variance, while process reward models (PRMs) face challenges with training data acquisition and verification effectiveness, limiting LLM reasoning accuracy improvement.

Method: Two-stage approach: 1) ReST-GRPO uses optimized ReST algorithm to filter high-value training data and increase reward variance; 2) VM-MCTS employs Monte-Carlo Tree Search to collect value targets for VM training, then uses adapted MCTS during decoding to provide precise process signals and verification scores.

Result: Significantly outperforms other reinforcement training baselines (naive GRPO, ReST-DPO) and decoding/verification baselines (PRM-BoN, ORM-MCTS) on coding benchmarks including APPS, BigCodeBench, and HumanEval.

Conclusion: ReST-RL effectively strengthens LLM reasoning ability through improved data filtering, enhanced reward variance, and VM-assisted decoding optimization, providing a powerful unified RL paradigm for code reasoning tasks.

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [11] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents is a multi-agent LLM framework that automates end-to-end course material generation through role-based collaboration, significantly reducing development time while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: High-quality instructional material preparation is labor-intensive and requires extensive coordination among faculty, instructional designers, and teaching assistants, creating barriers especially for resource-constrained institutions.

Method: A multi-agent large language model framework that simulates role-based collaboration among educational agents to generate cohesive course materials including syllabus, lecture scripts, LaTeX slides, and assessments. Operates in four modes with varying human involvement levels.

Result: Evaluated across five university-level computer science courses, the system produces high-quality instructional materials while significantly reducing development time and human workload.

Conclusion: Instructional Agents provides a scalable and cost-effective framework to democratize access to high-quality education, particularly benefiting underserved or resource-constrained educational settings.

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [12] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: InquireMobile is a novel interactive system that enables mobile agents to proactively seek human confirmation at critical decision points, addressing safety risks in autonomous VLM-based agents.


<details>
  <summary>Details</summary>
Motivation: Current fully autonomous Vision-Language Models pose safety risks when model understanding or reasoning capabilities are insufficient, requiring a safer interaction paradigm.

Method: Proposed InquireMobile model with reinforcement learning inspiration, featuring two-stage training strategy and interactive pre-action reasoning mechanism.

Result: Achieves 46.8% improvement in inquiry success rate and best overall success rate on the InquireBench benchmark compared to existing baselines.

Conclusion: The interactive approach with proactive human confirmation significantly improves safety and performance in mobile agent systems, with plans to open-source all materials.

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [13] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: Chain-of-Thought (CoT) shows limited benefits and unfaithfulness in soft-reasoning tasks across different model types, with varying reliance patterns.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness and faithfulness of Chain-of-Thought reasoning in soft-reasoning problems like analytical and commonsense reasoning, where previous work showed limited gains and potential unfaithfulness.

Method: Analyzed the dynamics and faithfulness of CoT across instruction-tuned models, reasoning models, and reasoning-distilled models on soft-reasoning tasks.

Result: Found differences in how different model types rely on CoT, and discovered that CoT influence and faithfulness are not always aligned across these models.

Conclusion: Chain-of-Thought reasoning exhibits varying effectiveness and faithfulness patterns depending on model architecture, with influence and faithfulness not consistently correlated in soft-reasoning tasks.

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [14] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: A model-agnostic framework using chess to evaluate if LLMs maintain semantic fidelity of structured environments by analyzing legal move distributions, revealing limitations in state-tracking over long sequences.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs internalize high-fidelity world models of structured domains without relying on model-specific internal activations that limit interpretability and generalizability.

Method: Proposes a state-based evaluation framework using chess as benchmark, analyzing downstream legal move distributions (state affordances) to estimate semantic fidelity between predicted and actual game states.

Result: Experimental results show the metrics capture deficiencies in state-tracking, highlighting LLM limitations in maintaining coherent internal models over long sequences.

Conclusion: The framework provides a robust tool for evaluating structured reasoning in LLMs without requiring internal model access and generalizes to various symbolic environments.

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [15] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: CASE is an AI framework that uses conversational agents to proactively interview potential scam victims, extracting structured intelligence from conversations to improve scam detection and enforcement on payment platforms.


<details>
  <summary>Details</summary>
Motivation: Digital payment growth has led to sophisticated social engineering scams that originate outside payment platforms, making traditional transaction signals insufficient for timely prevention.

Method: A conversational agent interviews potential victims to gather detailed scam intelligence, then another AI system extracts structured data from transcripts for enforcement mechanisms using Google's Gemini LLMs.

Result: Implementation on Google Pay India showed a 21% uplift in scam enforcement volume by augmenting existing features with this new intelligence.

Conclusion: The CASE framework is highly generalizable and provides a blueprint for building similar AI-driven scam intelligence systems in other sensitive domains.

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [16] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: Using flocking algorithm (boids) to optimize semiconductor fab scheduling by handling machine switching between single-lot and batch processing machines through local interactions.


<details>
  <summary>Details</summary>
Motivation: Classical optimization fails for large semiconductor fabs due to complexity. Need decentralized approach to handle frequent switching between single-lot and batch processing machines.

Method: Applied boids flocking algorithm (bio-inspired swarm intelligence) using local information and simple heuristics to model machine switching as obstacle avoidance in flock behavior.

Result: The algorithm effectively addresses the machine switching problem by reacting to different machine types similar to how flocks react to obstacles.

Conclusion: Flocking algorithms provide a viable bottom-up approach for semiconductor production optimization, handling complex machine switching patterns through decentralized local interactions.

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [17] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: Introduces Model Science as a new discipline focusing on analyzing trained models rather than data, with four pillars: Verification, Explanation, Control, and Interface for developing credible AI systems.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of foundation models requires shifting from Data Science to Model Science, placing trained models at the core of analysis to understand and control their behavior across diverse contexts.

Method: Proposes a conceptual framework with four key pillars: Verification (context-aware evaluation), Explanation (exploring internal operations), Control (alignment techniques), and Interface (interactive visual tools).

Result: A comprehensive framework that guides the development of AI systems that are credible, safe, and human-aligned by focusing on model-centric analysis rather than data-centric approaches.

Conclusion: Model Science represents a necessary paradigm shift for responsible AI development, providing systematic approaches to verify, explain, control, and interface with foundation models to ensure their safe and aligned deployment.

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: A new efficient hybrid parameter estimation method called Physics-Informed Regression (PIR) that uses regularized ordinary least squares for parameter-linear nonlinear dynamic models, outperforming physics-informed neural networks (PINN) in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To bridge theory and data by developing an efficient parameter estimation method for nonlinear dynamic models that are linear in parameters, enabling reliable and fast parameter estimation for applications like epidemic modeling.

Method: Physics-Informed Regression (PIR) uses regularized ordinary least squares to estimate parameters from time series data for models linear in parameters. Tested on ODE and PDE models, including epidemic compartment models, and compared against PINN on both synthetic and real COVID-19 data.

Result: PIR performed noticeably better than PINN, especially on complex compartment models. Both methods estimated target parameters successfully, but PIR showed superior computational speed. Successfully applied to estimate time-varying parameters using real Danish COVID-19 data from 2020-2021.

Conclusion: PIR is superior to PINN for parameter-linear nonlinear dynamic models, offering reliable and fast parameter estimation that may support real-time applications in fields like epidemiology.

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [19] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Symphony is a decentralized multi-agent system that enables lightweight LLMs on consumer GPUs to coordinate through decentralized ledger, dynamic task allocation, and weighted voting, outperforming centralized approaches.


<details>
  <summary>Details</summary>
Motivation: Address limitations of centralized LLM agent frameworks including high deployment costs, rigid communication topologies, and limited adaptability.

Method: Three key mechanisms: 1) decentralized ledger for capability recording, 2) Beacon-selection protocol for dynamic task allocation, 3) weighted result voting based on Chain-of-Thought reasoning.

Result: Outperforms existing baselines on reasoning benchmarks with substantial accuracy gains and demonstrates robustness across models of varying capacities.

Conclusion: Symphony provides a privacy-saving, scalable, and fault-tolerant orchestration with low overhead for decentralized multi-agent coordination.

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [20] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: Extends ZipNN compression to low-precision FP8/FP4 formats using entropy coding of exponents and mantissas, achieving up to 83% compression for FP8 and showing K/V cache tensors in LLMs are also compressible.


<details>
  <summary>Details</summary>
Motivation: Reduce storage and transmission costs of neural network weights as models grow larger, particularly focusing on emerging low-precision formats like FP8 and FP4 that are gaining popularity for efficient inference.

Method: Extends ZipNN approach to lower-precision formats by designing a compression method that separates and compresses exponent and mantissa components independently using entropy coding techniques.

Result: Achieves compression ratios up to 62% for BF16 and 83% for FP8. Also demonstrates that key-value cache tensors in large language models exhibit compressible patterns, enabling memory savings during deployment.

Conclusion: The proposed compression method effectively reduces model sizes for low-precision floating-point formats and shows potential for memory optimization in LLM deployment through K/V cache compression.

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [21] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: POT is a black-box attack framework that generates adversarial prompts to make LLMs produce unnecessarily verbose reasoning chains, consuming excessive computational resources without requiring external knowledge or data poisoning.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought prompting enhances LLM reasoning but creates new vulnerabilities to computational inefficiency attacks through verbose reasoning chains. Existing attacks have impractical requirements like external knowledge access and data poisoning.

Method: POT uses LLM-based iterative optimization to generate covert and semantically natural adversarial prompts that trigger overthinking without external data access or model retrieval capabilities.

Result: Extensive experiments across diverse model architectures and datasets show POT achieves superior performance compared to other overthinking attack methods.

Conclusion: POT demonstrates effective black-box overthinking attacks without restrictive conditions, highlighting new security vulnerabilities in CoT-enhanced LLMs and the need for robust defenses against computational inefficiency attacks.

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [22] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: Proposes a novel DRL framework for real-world distributed IoT resource allocation using ACK feedback from actual data transmissions to train models.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on training DRL models with real-world data in practical distributed IoT systems, despite DRL's strong capability in handling complex resource allocation decision-making tasks.

Method: IoT devices select communication channels using DRL-based methods, and the DRL model is trained with ACK feedback information obtained from actual data transmissions over selected channels.

Result: Implementation and performance evaluation demonstrate both feasibility and effectiveness of the proposed framework in terms of Frame Success Rate (FSR).

Conclusion: The framework successfully bridges the gap between DRL theory and practical IoT applications by enabling real-world training with actual transmission feedback.

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [23] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Re:Frame is a plug-in module that enhances offline RL by using a small associative memory buffer of expert data to improve policy performance with minimal expert demonstrations.


<details>
  <summary>Details</summary>
Motivation: Offline RL struggles with suboptimal data when expert datasets are scarce or impractical to collect, limiting agent generalization and performance.

Method: Introduces Re:Frame with an Associative Memory Buffer (AMB) containing expert trajectories. The policy learns to retrieve and integrate expert data via content-based associations during training and evaluation without environment interaction.

Result: On D4RL MuJoCo tasks, using only 60 expert trajectories (0.1% of dataset), Re:Frame improves Decision Transformer baseline in 3/4 settings with gains up to +10.7 normalized points.

Conclusion: Re:Frame provides a simple, data-efficient method to inject scarce expert knowledge and significantly improve offline RL performance from low-quality datasets.

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [24] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: First framework to quantify label memorization in GNNs, showing inverse relationship with graph homophily and proposing graph rewiring to reduce memorization while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Graph neural networks (GNNs) have been under-explored for memorization behavior compared to deep neural networks, particularly in semi-supervised node classification tasks.

Method: Introduced NCMemo framework to measure label memorization, analyzed relationship with graph homophily, studied training dynamics, and investigated graph rewiring as mitigation strategy.

Result: Found lower homophily significantly increases memorization; nodes with label inconsistency in neighborhood are more prone to memorization; graph rewiring effectively reduces memorization without performance loss and lowers privacy risk.

Conclusion: The work advances understanding of GNN learning dynamics, demonstrates the link between homophily and memorization, and provides practical approach for more privacy-preserving GNN deployment through graph rewiring.

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [25] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: A novel multi-source transfer learning framework using SVD decomposition to efficiently extract and aggregate knowledge from multiple source models, overcoming limitations of coarse-grained approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional transfer learning overlooks the opportunity to leverage knowledge from numerous available online models. Existing multi-source approaches lack precision for granular knowledge extraction and aggregation efficiency, especially with large numbers of source models or high parameter counts.

Method: Leverages Singular Value Decomposition (SVD) to decompose each source model into rank-one components, then selects the most salient components from all sources. Adapts to target tasks by fine-tuning only principal singular values of the merged matrix, recalibrating importance of top SVD components.

Result: The framework enables efficient transfer learning, demonstrates robustness to input-level and parameter-space perturbations (e.g., noisy or pruned sources), and scales well computationally.

Conclusion: The proposed SVD-based approach provides a precise and efficient method for multi-source knowledge transfer, overcoming previous limitations in granularity and scalability while maintaining robustness to various model imperfections.

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [26] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: A primer introducing graph data modeling for chemical sciences, covering graph neural networks and their applications to molecules, proteins, and chemical processes.


<details>
  <summary>Details</summary>
Motivation: Graphs provide a natural language to describe chemical entities and processes, capturing interactions and structures essential for materials, biology, and medicine.

Method: Introduces graphs as mathematical objects in chemistry and demonstrates how graph neural networks can operate on them. Covers graph design foundations, key prediction tasks, and representative examples.

Result: Provides foundational knowledge and practical examples to enable application of graph methods in chemical discovery.

Conclusion: Prepares readers to apply graph-based machine learning methods to advance the next generation of chemical discovery across various domains.

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [27] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: Lightweight deep learning model using RR Intervals with TCN and Mamba achieves high accuracy in early atrial fibrillation prediction up to 2 hours in advance with minimal computational requirements.


<details>
  <summary>Details</summary>
Motivation: Early detection of paroxysmal AF is challenging but crucial as undetected cases can progress to sustained AF, increasing mortality risk. Early prediction enables preventive therapies to reduce disease progression.

Method: Combines Temporal Convolutional Network for positional encoding with Mamba (selective state space model) using only RR Intervals data for efficient parallel sequence modeling.

Result: Achieved sensitivity 0.908, specificity 0.933, F1-score 0.930, AUROC 0.972, AUPRC 0.932 with only 73.5K parameters and 38.3 MFLOPs. Predicts AF up to 2 hours ahead using 30 minutes of input data.

Conclusion: The proposed lightweight model outperforms traditional CNN-RNN approaches in both accuracy and compactness, providing sufficient lead time for preventive interventions in atrial fibrillation.

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [28] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: First rigorous information geometric framework for quantifying hallucinations in multimodal LLMs using diffusion dynamics and spectral embeddings over multimodal graph Laplacians.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs remain a fundamental obstacle to trustworthy AI, especially in high-stakes domains. Existing evaluation techniques are heuristic and lack principled quantification or theoretical guarantees.

Method: Represents MLLM outputs as spectral embeddings over multimodal graph Laplacians, characterizes semantic distortion as manifold gaps, uses Rayleigh-Ritz bounds on hallucination energy, and leverages eigenmode decompositions in RKHS embeddings with temperature annealing.

Result: Develops modality-aware, theoretically interpretable metrics that capture hallucination evolution across time and input prompts through temperature profiles.

Conclusion: Establishes a principled foundation for quantifying and bounding hallucinations, transforming them from qualitative risk to tractable, analyzable phenomenon.

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [29] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: Fine-tuned Vision-Language Model based on LLaMA 3.2 outperforms CNN baseline for neutrino interaction classification in high-energy physics experiments, enabling richer multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of large language models for multimodal reasoning beyond natural language, specifically for classifying neutrino interactions from pixelated detector images in high-energy physics experiments.

Method: Fine-tuned a Vision-Language Model (VLM) based on LLaMA 3.2 and benchmarked its performance against an established CNN baseline used in experiments like NOvA and DUNE, evaluating classification accuracy, precision, recall, and AUC-ROC metrics.

Result: The VLM not only matches or exceeds CNN performance but also enables richer reasoning and better integration of auxiliary textual or semantic context.

Conclusion: VLMs offer a promising general-purpose backbone for event classification in HEP, paving the way for multimodal approaches in experimental neutrino physics.

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [30] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: Hybrid quantum-classical models (QMLP and QCNN) show promising results for malware classification, achieving high accuracy on binary classification tasks and varying performance on multiclass tasks, with QMLP performing better on complex tasks while QCNN offers better training efficiency.


<details>
  <summary>Details</summary>
Motivation: Quantum machine learning presents a paradigm-shifting opportunity to improve malware detection, but its application in this domain remains largely unexplored compared to classical machine learning approaches.

Method: Two hybrid quantum-classical models: Quantum Multilayer Perceptron (QMLP) using full qubit measurement and data re-uploading, and Quantum Convolutional Neural Network (QCNN) using quantum convolution and pooling layers to reduce active qubits. Both utilize angle embedding to encode malware features into quantum states.

Result: High accuracy for binary classification: 95-96% on API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. Multiclass accuracy ranges: 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class, and 60.7-88.1% on EMBER-Class.

Conclusion: QMLP outperforms QCNN in complex multiclass tasks, while QCNN offers improved training efficiency at the cost of reduced accuracy, demonstrating the potential of quantum machine learning for malware classification.

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [31] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: DETNO combines transformer neural operator with diffusion refinement to accurately predict high-frequency traffic features like shock waves over long horizons, overcoming smoothing limitations of standard neural operators.


<details>
  <summary>Details</summary>
Motivation: Standard neural operators produce smooth predictions that fail to reconstruct high-frequency traffic features (sharp density gradients, shock waves), leading to rapid error accumulation in multi-step predictions essential for real-time traffic management.

Method: Unified Diffusion-Enhanced Transformer Neural Operator (DETNO) architecture with transformer neural operator using cross-attention for expressivity and super-resolution, coupled with diffusion-based refinement that iteratively reconstructs high-frequency details through progressive denoising.

Result: Superior performance in extended rollout predictions on chaotic traffic datasets compared to traditional and transformer-based neural operators, preserving high-frequency components and improving stability over long prediction horizons.

Conclusion: DETNO effectively addresses the fundamental smoothing limitations and rollout instability of standard neural operators, enabling accurate long-term traffic forecasting with preserved high-frequency features essential for intelligent transportation systems.

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [32] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: Hybrid quantum-classical architecture for SMILES string reconstruction achieves 84% quantum fidelity and 60% classical similarity, outperforming existing quantum baselines.


<details>
  <summary>Details</summary>
Motivation: Classical approaches struggle with high fidelity and validity in molecular design, and quantum machine learning integration with sequence-based tasks like SMILES reconstruction remains underexplored with fidelity degradation issues.

Method: Proposes a hybrid quantum-classical architecture that integrates quantum encoding with classical sequence modeling for SMILES reconstruction.

Result: Achieves approximately 84% quantum fidelity and 60% classical reconstruction similarity, surpassing existing quantum baselines.

Conclusion: Lays foundation for future QML applications by balancing quantum representations with classical sequence models, enabling broader research on quantum-aware sequence models for molecular and drug discovery.

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [33] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: KAR-HNN replaces MLPs with univariate transformations in Hamiltonian Neural Networks to improve energy conservation and stability while reducing hyperparameter sensitivity.


<details>
  <summary>Details</summary>
Motivation: Existing HNN implementations using MLPs cause hypersensitivity to hyperparameters and struggle with complex energy landscapes, leading to energy drift and poor long-term stability.

Method: Proposes Kolmogorov-Arnold Representation-based Hamiltonian Neural Network that uses univariate transformations instead of MLPs, exploiting localized function approximations to better capture high-frequency and multi-scale dynamics while preserving symplectic structure.

Result: KAR-HNN reduces energy drift and improves long-term predictive stability across four benchmark problems (spring-mass, simple pendulum, two- and three-body problems).

Conclusion: The approach shows effectiveness for accurate and stable modeling of realistic physical processes in high dimensions with few known parameters, maintaining interpretability and physical consistency.

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [34] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: Llama-3.1-8B-Instruct shows format-dependent reasoning failure where it incorrectly judges "9.11" > "9.8" in chat formats but answers correctly in simple format, revealing specialized even/odd attention head organization and sharp computational thresholds.


<details>
  <summary>Details</summary>
Motivation: To understand and fix format-dependent reasoning failures in transformer models, specifically investigating why numerical comparison fails in certain formats but works in others.

Method: Systematic intervention experiments, attention head analysis, and sparse autoencoder (SAE) analysis to examine feature representations and head specialization across layers.

Result: Discovered even/odd attention head specialization (even heads handle numerical comparison), identified perfect repair requires exactly 8 even heads at Layer 10, found 60% pattern replacement threshold, and achieved perfect repair using only 25% of attention heads.

Conclusion: Transformer models have sophisticated substructure with specialized head organization and sharp computational thresholds, revealing that apparent full-module requirements hide efficient repair possibilities with implications for interpretability and model efficiency.

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [35] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: Physics-informed ML workflow using differentiable simulator and CNN to predict reservoir pressure control with 3000x fewer simulations than previous methods


<details>
  <summary>Details</summary>
Motivation: Subsurface reservoir pressure control is challenging due to geological heterogeneity and expensive multiphase flow simulations needed for uncertain property predictions

Method: Couples differentiable multiphase flow simulator (DPFEHM) with CNN, uses transfer learning from single-phase steady-state simulations to multiphase scenarios

Result: Achieves high accuracy with fewer than 3000 full-physics simulations vs previous 10M requirement, dramatic computational cost reduction

Conclusion: Physics-informed ML with transfer learning enables practical and accurate reservoir pressure predictions with significantly reduced computational burden

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [36] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: Novel unsupervised contrastive learning framework clusters 43 cancer types using dual mutation signatures (gene-level and chromosome-level) from COSMIC data, producing biologically meaningful cancer groupings.


<details>
  <summary>Details</summary>
Motivation: Understanding pan-cancer mutational landscape provides insights into tumorigenesis mechanisms. While patient-level ML is common, cohort-level cancer clustering has relied on classical statistical methods rather than modern ML approaches.

Method: Construct two complementary mutation signatures per cancer type: gene-level profile (nucleotide substitution patterns) and chromosome-level profile (normalized substitution frequencies). Use TabNet encoders with multi-scale contrastive learning (NT-Xent loss) to learn unified embeddings.

Result: The learned latent representations produce biologically meaningful clusters of cancer types that align with known mutational processes and tissue origins.

Conclusion: First application of contrastive learning to cohort-level cancer clustering, offering a scalable and interpretable framework for mutation-driven cancer subtyping.

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [37] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: A data-augmentation strategy for training neural PDEs using space-filling sampling of local stencil states, which improves sample efficiency and generalization compared to traditional trajectory-based training.


<details>
  <summary>Details</summary>
Motivation: Neural PDEs are easier to work with than traditional numerical solvers but typically require extensive trajectory data from long time integration, which contains spatiotemporal redundancy and may under-sample important states.

Method: Proposes space-filling sampling of local "stencil" states to generate training data, removing redundancy and oversampling rarely visited states. Can learn accurate neural PDE operators from synthetic data equivalent to just 10 timesteps of simulation.

Result: Accurate neural PDE stencil operators can be learned from minimal synthetic data. Performance further improves with access to a single full-trajectory simulation. Shows clear performance gains across several PDE systems compared to naive trajectory sampling.

Conclusion: The space-filling stencil sampling approach provides more sample-efficient training data generation for neural PDEs, enabling better generalization and accuracy with significantly less computational cost than traditional trajectory-based methods.

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [38] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: Using tensor decomposition in generative models to reduce costs when generating multidimensional simulation data by producing smaller tensor factors instead of full tensors.


<details>
  <summary>Details</summary>
Motivation: Large complex simulation datasets are time and resource consuming to produce, making synthetic data generation more reasonable for expensive experiments. Generative models like GANs and diffusion models improve efficiency but can be further optimized.

Method: Introduce internal tensor decomposition to generative models for multidimensional data. Instead of generating full tensors, generate smaller tensor factors to reduce model output size and overall parameters.

Result: The approach significantly reduces costs of generating complex simulation data while maintaining data usefulness, as shown in experiments.

Conclusion: Tensor decomposition has the potential to improve efficiency in generative models, particularly for generating multidimensional data (tensors).

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [39] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: The paper proves that many modern neural network architectures (pre-layer normalization, linear-attention modules, GPT-style transformers, diffusion models) are almost always surjective, meaning any output can be generated by some input, revealing inherent vulnerabilities to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: To understand whether trained neural networks can generate any specified output, which has implications for model safety and jailbreak vulnerabilities in generative AI systems.

Method: Mathematical analysis and proofs showing that fundamental building blocks of modern neural architectures (pre-layer normalization, linear-attention modules) are almost always surjective functions.

Result: Proved that widely used generative frameworks including GPT-style transformers and diffusion models with deterministic ODE solvers admit inverse mappings for arbitrary outputs, making them vulnerable to adversarial attacks.

Conclusion: Modern neural network architectures have inherent surjectivity properties that create unavoidable vulnerabilities to a broad class of adversarial attacks, raising significant safety concerns for generative AI systems.

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [40] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: The paper analyzes membership-inference attacks on Gaussian mean estimation, showing that attackers need Ω(n + n²ρ²) reference samples to match fully informed attackers, which exceeds the n samples used for training.


<details>
  <summary>Details</summary>
Motivation: To understand the minimum number of reference samples required for successful membership-inference attacks and determine if practical attacks underestimate privacy risks by using insufficient reference data.

Method: The study focuses on Gaussian mean estimation where the learning algorithm estimates μ from n samples, and analyzes the sample complexity needed for membership inference attacks to compete with fully informed attackers.

Result: The analysis shows that Ω(n + n²ρ²) reference samples are necessary for effective membership inference, demonstrating that attackers may need significantly more samples than the training algorithm uses.

Conclusion: Current practical attacks using O(n) samples may underestimate membership inference risks, suggesting better attacks are possible when more distribution information is available.

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [41] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [42] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: DeepAtlas is an algorithm that generates local manifold embeddings and uses deep neural networks to map between local and original data spaces, with topological distortion analysis to test the manifold hypothesis.


<details>
  <summary>Details</summary>
Motivation: Current manifold learning tools only create global embeddings and cannot verify if the manifold hypothesis holds true for datasets, limiting their mathematical rigor and practical applications.

Method: DeepAtlas generates lower-dimensional representations of local neighborhoods, trains deep neural networks to map between local embeddings and original data, and uses topological distortion analysis to assess manifold structure and dimensionality.

Result: DeepAtlas successfully learns manifold structures in test datasets, but finds many real datasets (including single-cell RNA-sequencing) do not conform to the manifold hypothesis. When data is manifold-based, it enables generative modeling.

Conclusion: DeepAtlas provides a rigorous framework for testing the manifold hypothesis and building mathematically sound manifold models that enable generative applications and differential geometry tools for appropriate datasets.

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [43] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: SAFT is a novel framework that transforms tabular learning from discrete search to continuous representation generation to handle distribution shifts between training and testing data, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Tabular learning effectiveness deteriorates under distribution shifts between training and testing data, creating a need for robust feature transformation methods.

Method: SAFT reframes tabular learning as continuous representation generation with three mechanisms: shift-resistant representation (embedding decorrelation + sample reweighting), flatness-aware generation (suboptimal embedding averaging), and normalization-based distribution alignment.

Result: Extensive experiments show SAFT consistently outperforms prior tabular learning methods in robustness, effectiveness, and generalization under diverse real-world distribution shifts.

Conclusion: SAFT successfully addresses the Distribution Shift Tabular Learning problem through its continuous representation-generation paradigm and integrated robustness mechanisms.

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [44] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: EQUATE is a framework that fine-tunes foundation models for symbolic equation discovery in low-data settings using distillation, combining symbolic-numeric alignment with evaluator-guided embedding optimization.


<details>
  <summary>Details</summary>
Motivation: Foundation models pre-trained on large equation datasets often suffer from negative transfer and poor generalization when applied to small domain-specific datasets, limiting their effectiveness in data-efficient symbolic regression.

Method: EQUATE reformulates discrete equation search as continuous optimization in a shared embedding space, using symbolic-numeric alignment and evaluator-guided embedding optimization based on data-equation fitness and simplicity.

Result: Experiments across Feynman, Strogatz, and black-box benchmarks show EQUATE consistently outperforms state-of-the-art baselines in accuracy and robustness while preserving low complexity and fast inference.

Conclusion: EQUATE provides a practical and generalizable solution for data-efficient symbolic regression in foundation model distillation settings, enabling better adaptation to small domain-specific datasets.

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [45] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: PoolFlip extends FlipIt game with MARL environment, and Flip-PSRO trains defenders that are 2x more effective against unseen attacks than baselines.


<details>
  <summary>Details</summary>
Motivation: Existing FlipIt frameworks rely on limited heuristics and specialized learning techniques, leading to brittleness and inability to adapt to new adversarial strategies in cyber defense.

Method: Introduce PoolFlip multi-agent gym environment for FlipIt game, and propose Flip-PSRO using population-based MARL training with ownership-based utility functions.

Result: Flip-PSRO defenders are 2x more effective than baselines in generalizing to heuristic attacks not seen during training, while maintaining high control levels.

Conclusion: The approach enables more robust and adaptive cyber defense through MARL-based training that generalizes better against unknown, potentially adaptive opponents.

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [46] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: A generative optimization approach using Python programs as policies that self-evolve through LLMs, achieving competitive Atari game performance with less training time than deep RL methods.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient and adaptable game-playing agents that can perform complex reasoning with minimal human intervention, using programmatic policy representations.

Method: Represent policies as Python programs that take current observation as input and output game actions. Use large language models to refine policies through execution traces and natural language feedback in a self-evolving process.

Result: Achieves performance competitive with deep reinforcement learning baselines on Atari games while using significantly less training time and fewer environment interactions.

Conclusion: Programmatic policy representations show promise for building efficient, adaptable agents capable of complex, long-horizon reasoning through generative optimization with LLMs.

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [47] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: MobText-SISA is a machine unlearning framework for mobility data that enables efficient deletion of individual contributions while maintaining model performance, using similarity-aware clustering and sharded training.


<details>
  <summary>Details</summary>
Motivation: Privacy regulations like GDPR require the ability to delete individual data from models, but retraining deep models from scratch for each deletion request is computationally infeasible for large-scale mobility data.

Method: Extends SISA training to heterogeneous spatio-temporal data by embedding trips into shared latent space, using similarity-aware clustering to distribute samples across shards, and training each shard incrementally with aggregation at inference.

Result: Maintains baseline predictive accuracy while outperforming random sharding in both error and convergence speed on real-world mobility data.

Conclusion: MobText-SISA provides a practical solution for privacy-compliant analytics on multimodal mobility data at urban scale with guaranteed exact unlearning.

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [48] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: LLMs show significant prediction sensitivity to task-irrelevant data variations like variable name changes, with error swings up to 82%, revealing fundamental robustness issues despite competitive predictive performance.


<details>
  <summary>Details</summary>
Motivation: To investigate the robustness vulnerabilities of LLMs when used for data fitting tasks, particularly their sensitivity to irrelevant data representation changes that should not affect predictions.

Method: Tested LLMs under in-context learning and supervised fine-tuning scenarios, examined attention patterns in open-weight models, and compared with specialized tabular foundation model TabPFN.

Result: LLMs show dramatic prediction sensitivity (up to 82% error variation) to task-irrelevant changes like variable name modifications, with non-uniform attention patterns explaining the positional bias.

Conclusion: Despite impressive predictive capabilities, current LLMs lack basic robustness required for principled data-fitting applications due to sensitivity to irrelevant data variations.

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [49] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: Bi-LoRA combines SAM's flat minima seeking with LoRA's parameter efficiency by using dual modules - one for task adaptation and another for sharpness optimization, eliminating SAM's memory/computation overhead while improving generalization.


<details>
  <summary>Details</summary>
Motivation: SAM improves generalization but has high memory/computation costs for large models. Direct SAM-LoRA integration limits sharpness optimization to restricted subspace, reducing effectiveness.

Method: Proposes Bi-directional LoRA with dual modules: primary LoRA for task adaptation via gradient descent, auxiliary LoRA for capturing loss landscape sharpness via gradient ascent. Decouples SAM's perturbations from LoRA optimization.

Result: Extensive experiments show Bi-LoRA achieves flatter minima while remaining memory-efficient, eliminating SAM's doubled training costs. Enhances generalization across diverse tasks and architectures.

Conclusion: Bi-LoRA successfully integrates SAM's flat minima benefits with LoRA's parameter efficiency through dual-module design, providing effective generalization improvement without the computational overhead of standard SAM.

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [50] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: A counterfactual reward model using causal inference and multimodal representation learning to mitigate biases in RLHF, achieving 89.12% accuracy in fake news detection while reducing spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Reward models in RLHF can amplify latent biases from multimodal datasets, leading to flawed policy optimization and decreased fairness. Passive bias mitigation approaches often fail under causal confounding.

Method: Counterfactual Trust Score framework with four components: counterfactual shifts to separate political framing bias from topical bias, reconstruction uncertainty during perturbations, fairness rule violations detection, and temporal reward shifts aligned with dynamic trust measures.

Result: Achieved 89.12% accuracy in fake news detection, outperforming baseline reward models. Reduced spurious correlations and unfair reinforcement signals on a multimodal fake vs true news dataset with framing bias, class imbalance, and distributional drift.

Conclusion: Provides a robust, interpretable approach to fairness-aware RLHF with tunable bias reduction thresholds, increasing reliability in dynamic real-time policy making through unsupervised, bias-resilient reward signals.

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [51] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: Tutorial on generative models for synthetic data generation to address data scarcity, privacy, and annotation challenges in data mining.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity, privacy concerns, and annotation difficulties in data mining through synthetic data generation using modern generative models.

Method: Covers foundations and latest advances in synthetic data generation methodologies including Large Language Models, Diffusion Models, and GANs, along with practical frameworks and evaluation strategies.

Result: Provides actionable insights and practical knowledge for attendees to leverage generative synthetic data in data mining research and applications.

Conclusion: Generative models offer scalable solutions for synthetic data creation, enabling enhanced data mining capabilities while addressing key data-related challenges.

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [52] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: SyReM is a novel continual learning method that addresses catastrophic forgetting in motion forecasting by balancing memory stability and learning plasticity through selective memory rehearsal with gradient similarity.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks for motion forecasting suffer from catastrophic forgetting when adapting to new data, and existing continual learning methods struggle with the stability-plasticity dilemma.

Method: SyReM maintains a compact memory buffer, uses inequality constraints for memory stability, and employs selective memory rehearsal based on cosine similarity of loss gradients to enhance learning plasticity.

Result: Experiments on 11 driving datasets show SyReM significantly reduces catastrophic forgetting in past scenarios while improving forecasting accuracy in new scenarios compared to non-CL and CL baselines.

Conclusion: SyReM effectively addresses the stability-plasticity dilemma in continual learning for motion forecasting, demonstrating superior performance in maintaining knowledge while adapting to new data.

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [53] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: Dual-LS is a brain-inspired continual learning method that reduces catastrophic forgetting by 74.31% and computational costs by 94.02% for DNN-based vehicle motion forecasting in smart cities.


<details>
  <summary>Details</summary>
Motivation: Current DNN models for vehicle motion forecasting suffer from catastrophic forgetting when updated, requiring expensive data collection and failing to balance long- and short-term experience like human learning.

Method: Dual-LS uses a task-free, online continual learning paradigm with two synergistic memory rehearsal replay mechanisms inspired by the human brain's complementary learning system, dynamically coordinating long-term and short-term knowledge representations.

Result: Tests on naturalistic data from three countries (772,000 vehicles, 11,187 km testing mileage) show 74.31% reduction in catastrophic forgetting and 94.02% reduction in computational resource demand while maintaining predictive stability.

Conclusion: Dual-LS enables computation-efficient, human-like continual learning adaptability for DNN-based vehicle motion forecasting suitable for smart city applications without increasing data requirements.

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [54] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: Delta-Attribution framework explains model changes by differencing feature attributions between versions, evaluated through comprehensive quality metrics to distinguish meaningful vs cosmetic updates.


<details>
  <summary>Details</summary>
Motivation: Model updates often change performance but the reasons remain opaque, making it difficult to understand what actually changed between model versions.

Method: Model-agnostic framework that differences per-feature attributions (Δφ(x)=φ_B(x)-φ_A(x)) using fast occlusion/clamping in standardized space with class-anchored margin and baseline averaging.

Result: Successfully distinguished meaningful changes (e.g., SVC poly→rbf: BAC≈0.998, DCE≈6.6) from cosmetic tweaks (rank-overlap@10=1.0, DCE≈0) across 45 settings with 5 model families and 3 datasets.

Conclusion: Δ-Attribution provides lightweight update auditing that complements accuracy metrics by identifying behaviorally meaningful changes and risky reliance shifts between model versions.

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [55] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: FinCast is the first foundation model for financial time-series forecasting that addresses pattern shifts from temporal non-stationarity, multi-domain diversity, and varying resolutions, achieving state-of-the-art zero-shot performance without domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Financial time-series forecasting is crucial for economic stability and investment but remains challenging due to pattern shifts from temporal non-stationarity, multi-domain diversity, and varying temporal resolutions. Existing deep learning methods suffer from overfitting and require extensive domain-specific fine-tuning.

Method: Introduces FinCast, a foundation model specifically designed for financial time-series forecasting, trained on large-scale financial datasets. The model is designed to capture diverse patterns without requiring domain-specific fine-tuning.

Result: FinCast exhibits robust zero-shot performance, effectively capturing diverse patterns across different financial domains and temporal resolutions. Comprehensive evaluations show it surpasses existing state-of-the-art methods.

Conclusion: FinCast demonstrates strong generalization capabilities as the first foundation model for financial forecasting, overcoming limitations of previous methods by providing excellent zero-shot performance without the need for domain-specific fine-tuning.

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [56] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: pFedBayesPT is a novel personalized federated learning framework that addresses intra-client data heterogeneity through Bayesian visual prompt tuning, enabling instance-wise personalization rather than client-level models.


<details>
  <summary>Details</summary>
Motivation: Existing personalized federated learning methods assume single distribution per client, but real-world clients often have data from multiple sources/domains, leading to intra-client heterogeneity and suboptimal performance.

Method: Proposes pFedBayesPT framework using visual prompt tuning from Bayesian perspective, modeling prompt posterior as implicit distribution to capture diverse visual semantics, with variational training under semi-implicit variational inference.

Result: Extensive experiments on benchmark datasets show pFedBayesPT consistently outperforms existing pFL methods under both feature and label heterogeneity settings.

Conclusion: The proposed instance-wise pFL framework effectively addresses intra-client data heterogeneity through Bayesian prompt tuning, demonstrating superior performance over traditional client-level personalized approaches.

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [57] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: RLTR framework uses tool-use rewards to decouple and optimize LLM agent planning capability separately from summarization, achieving 8-12% planning improvement and 5-6% overall response quality boost.


<details>
  <summary>Details</summary>
Motivation: End-to-end multi-objective training of LLM agents faces imbalanced optimization and scarce verifiable data, making it difficult to enhance the core planning capability.

Method: Proposes Reinforcement Learning with Tool-use Rewards (RLTR) that decouples training, uses tool-use completeness as reward signal for focused single-objective optimization of planning module.

Result: RLTR achieves 8%-12% improvement in planning performance compared to end-to-end baselines, and 5%-6% increase in final response quality of the overall agent system.

Conclusion: Decoupling training with tool-use rewards provides more direct and reliable training signal for planning capability enhancement without needing verifiable data.

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [58] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: PSO-Merging is a novel data-driven model merging method using Particle Swarm Optimization that outperforms existing methods by combining expert models more efficiently without gradient computation.


<details>
  <summary>Details</summary>
Motivation: Existing model merging methods have limitations - data-independent methods lack performance, gradient-based methods are computationally expensive for large models, and gradient-free methods struggle with limited optimization steps.

Method: Uses Particle Swarm Optimization (PSO) initialized with pre-trained model, expert models, and sparsified expert models. Performs multiple iterations with the final global best particle serving as the merged model.

Result: Experimental results on different language models show PSO-Merging generally outperforms baseline merging methods.

Conclusion: PSO-Merging provides a more efficient and scalable solution for model merging by addressing computational limitations of existing approaches while achieving better performance.

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [59] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: A gradient-based algorithm for efficiently selecting optimal demonstration examples for in-context learning, achieving 37.7x speedup and 11% performance improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently selecting the best k examples from n candidates for in-context learning, as current similarity-based methods using token embeddings are suboptimal and full inference is computationally expensive.

Method: Proposes a gradient-based approach that uses first-order approximations of model outputs in input embedding space, computes influence scores through random subset sampling aggregation, and requires only one-time pre-computation of outputs and gradients.

Result: Achieves less than 1% approximation error across six datasets, 37.7x speedup on 34B parameter models, and 11% average performance improvement over embedding-based selection methods.

Conclusion: The gradient-based demonstration selection algorithm provides an efficient and effective solution for in-context learning, enabling scalable subset selection while maintaining high accuracy.

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [60] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: ALSA is a novel framework that estimates model accuracy on unseen datasets by operating directly in logit space using anchor-based modeling, outperforming existing softmax- and similarity-based methods especially under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Existing accuracy estimation methods suffer from information loss (softmax-based) or computational expense/domain specificity (similarity-based), making them unreliable under distribution shifts.

Method: ALSA operates in logit space using multiple learnable anchors with influence functions to capture subtle logit variations, preserving richer information than softmax compression.

Result: Extensive experiments on vision, language, and graph benchmarks show ALSA's superiority over baselines, with strong robustness under significant distribution shifts.

Conclusion: ALSA provides robust and accurate performance estimates across diverse domains and distribution shifts, making it a practical tool for reliable model evaluation.

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [61] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: Multimodal hierarchical classification framework for e-commerce product categorization using text, vision, and vision-language features, achieving 98.59% F1 score and demonstrating industrial scalability.


<details>
  <summary>Details</summary>
Motivation: Address platform heterogeneity and structural limitations of existing taxonomies in e-commerce product categorization across 40 international fashion platforms.

Method: Integrates RoBERTa (text), ViT (vision), and CLIP (vision-language) with fusion strategies (early, late, attention-based) in hierarchical architecture with dynamic masking. Includes self-supervised recategorization pipeline with SimCLR, UMAP, and cascade clustering.

Result: CLIP embeddings with MLP-based late-fusion achieved highest hierarchical F1 (98.59%). Self-supervised pipeline discovered fine-grained categories with >86% cluster purity. Cross-platform experiments show trade-off between accuracy and generalization.

Conclusion: Framework successfully addresses industrial e-commerce challenges, demonstrates scalability through commercial deployment with two-stage inference pipeline balancing cost and accuracy.

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [62] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: Fine-tuning LLMs on harmful data can cause broad misalignment with human values. The paper develops a framework to detect and characterize these rapid transitions during fine-tuning using statistical methods and LLM-evaluated order parameters.


<details>
  <summary>Details</summary>
Motivation: To understand when and how emergent misalignment occurs during fine-tuning of LLMs on harmful datasets, as this can lead to broadly misaligned behavior with human values.

Method: Developed a comprehensive framework using distributional change detection methods and order parameters formulated in plain English and evaluated by an LLM judge. Used statistical dissimilarity measures to quantify phase transitions and decompose distributional changes.

Result: Found that behavioral transitions occur later in training than indicated by gradient norm peaks. The framework successfully quantifies how fine-tuning affects multiple aspects of model outputs and enables automated discovery of language-based order parameters across various domains.

Conclusion: The proposed framework effectively detects and characterizes misalignment transitions during LLM fine-tuning, providing insights into when behavioral changes occur and enabling automated quantification of language-based order parameters for better understanding model alignment.

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [63] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: SCAR introduces a principled framework to characterize dataset structural properties (Scale, Coverage, Authenticity, Richness) that remain invariant under scaling, enabling identification of Foundation Data - minimal subsets preserving generalization without retraining.


<details>
  <summary>Details</summary>
Motivation: Traditional data-centric approaches focus on quantity and efficiency while overlooking structural data quality aspects. There's limited theoretical insight into how data properties affect generalization, particularly in sample scaling scenarios.

Method: Developed SCAR framework with four structural measures, modeled single-modality tasks as step functions, estimated foundation data size distribution, and created SCAR-guided data completion strategy for modality-aware expansion.

Result: Experiments across diverse multi-modal datasets and model architectures validated SCAR's effectiveness in predicting data utility and guiding data acquisition. The method enables efficient expansion of modality-specific characteristics.

Conclusion: SCAR provides a robust, general foundation for data understanding by capturing stable structural properties invariant under scaling, offering theoretical insights into data quality's impact on generalization beyond traditional quantity-focused approaches.

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [64] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: First comprehensive design space exploration of low-power flexible stress classifiers using machine learning with over 1200 configurations, featuring customized circuits with low-precision arithmetic for real-time stress monitoring.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional episodic stress monitoring and rigid silicon-based wearables by developing continuous, accessible, and cost-efficient flexible electronics solutions for real-time stress detection.

Method: Conducted design space exploration covering various ML classifiers, feature selection, and neural simplification algorithms. Designed fully customized circuits with low-precision arithmetic for hardware efficiency optimization.

Result: Developed over 1200 flexible stress classifiers that achieve higher accuracy than current methods while being low-cost, conformable, and ensuring low power consumption with compact size.

Conclusion: This work provides valuable insights for designing real-time stress monitoring systems using flexible electronics, overcoming integration and power constraints while maintaining high performance and practical wearability.

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [65] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: Rational functions and rational neural networks can approximate regular functions in the C¹-norm with specific rates for width, depth, and degree.


<details>
  <summary>Details</summary>
Motivation: To develop approximation methods for regular functions using rational functions and neural networks, particularly for applications in symbolic regression and physical law learning.

Method: Utilize rational functions and rational neural networks, including architectures like EQL÷ and ParFam, to approximate functions in the C¹-norm with analysis of approximation rates.

Result: Demonstrated that suitable regular functions can be approximated in the C¹-norm with rational functions and rational neural networks, providing specific rates related to network width, depth, and rational degree.

Conclusion: The study confirms the effectiveness of rational approximations and neural networks for C¹-norm approximation, with implications for symbolic regression in physical law learning.

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [66] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: This paper introduces a weighted metric framework for analyzing walks on graphs as Lipschitz sequences, enabling distance measurements between walks and developing proximity functions with representation formulas and applications in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To study the metric structure of graph walks as Lipschitz sequences and develop tools for measuring relative distances between walks, which is fundamental for analyzing network structures and enabling applications like reinforcement learning based on exploratory walks.

Method: Introduces a weighted metric to handle sequences, defining distances between walks based on stepwise vertex distances and weighted norms. Analyzes properties of these metric spaces and provides representation formulas for proximities under different assumptions with explicit constructions.

Result: Develops a comprehensive metric framework for walks on graphs that allows the use of classical metric modeling tools, including extension of Lipschitz functions from subspaces of walks while preserving fundamental properties through the derived representations.

Conclusion: The proposed metric structure provides a robust foundation for analyzing walks on graphs, enabling proximity estimation and supporting applications in reinforcement learning strategies and Lipschitz regression on network structures through the developed representation formulas.

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [67] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: Adam-PFN is a new surrogate model for freeze-thaw Bayesian optimization that improves Adam hyperparameter tuning using pre-trained learning curve predictions and data augmentation.


<details>
  <summary>Details</summary>
Motivation: Adam optimizer is widely used but hyperparameter tuning is tedious and costly. Existing freeze-thaw BO methods lack prior knowledge about how Adam hyperparameters affect learning curves.

Method: Proposes Adam-PFN surrogate model pre-trained on TaskSet learning curves, combined with CDF-augment data augmentation method to increase training examples.

Result: Improves learning curve extrapolation and accelerates hyperparameter optimization on TaskSet evaluation tasks, with strong performance on out-of-distribution tasks.

Conclusion: The proposed approach effectively addresses limitations of generic surrogates in freeze-thaw BO for Adam hyperparameter tuning, demonstrating improved performance and generalization.

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [68] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: InfraredGP is a novel graph partitioning method that uses graph Laplacian with negative correction to access frequencies beyond [0,2], enabling high-quality community detection without training through a single feed-forward propagation with random inputs.


<details>
  <summary>Details</summary>
Motivation: The paper explores whether low-frequency information beyond the conventional [0,2] range in graph signal processing can better capture community structures in graph partitioning problems.

Method: Uses spectral GNN backbone with low-pass filters and negative correction mechanism, feeds only random inputs, derives embeddings via one feed-forward propagation without training, and uses BIRCH for clustering.

Result: Achieves 16x-23x faster efficiency than baselines while maintaining competitive quality for both static and streaming graph partitioning, with distinguishable embeddings for standard clustering modules.

Conclusion: The negative correction mechanism effectively amplifies low-frequency information beyond conventional ranges, enabling high-quality graph partitioning without training while significantly improving efficiency.

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [69] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: A novel 3D diffusion-based pipeline for fast generation of physically realistic granular media assemblies, reducing simulation initialization time from hours to seconds.


<details>
  <summary>Details</summary>
Motivation: Discrete Element Method simulations of granular media are computationally intensive, especially during initialization phase which dominates total simulation time due to large displacements and kinetic energy.

Method: Two-stage pipeline: 1) Diffusion model generates independent 3D voxel grids, 2) 3D inpainting model stitches grids using masked inputs and 2D repainting techniques adapted for 3D, with weighted losses for long-term coherence.

Result: Achieves linear scaling of computational time with sample size; generated 1.2m ballasted rail track equivalent to 3-hour DEM simulation in under 20 seconds.

Conclusion: Enables physically coherent, real-time, scalable granular media synthesis for industrial applications, with post-processing for DEM-compatibility.

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [70] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: EUREKA framework builds classifiers that prioritize interesting/unexpected features over maximum accuracy, using LLMs to rank feature interestingness and create interpretable models with novel insights.


<details>
  <summary>Details</summary>
Motivation: Traditional ML focuses on maximizing predictive accuracy, but this work explores building classifiers that are interesting and use unexpected features, even at the cost of some accuracy, to support knowledge discovery and communication.

Method: EUREKA framework uses large language models to rank features by their perceived interestingness, then builds interpretable classifiers using only the selected interesting features.

Result: Across benchmark datasets, EUREKA consistently identifies non-obvious yet predictive features (e.g., favors humidity over CO2 for room congestion, discovers papers with colons in titles are more cited).

Conclusion: Interesting classifiers with moderate accuracy but high novelty and interpretability can support new ways of knowledge discovery, especially when perfect accuracy isn't required but insights are valued.

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [71] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: A new symplectic CNN architecture that preserves symplectic structure in convolution layers and outperforms linear symplectic autoencoders on PDE problems.


<details>
  <summary>Details</summary>
Motivation: To develop convolutional neural networks that preserve symplectic structure for solving Hamiltonian systems and PDEs more effectively than existing linear approaches.

Method: Leveraging symplectic neural networks, proper symplectic decomposition, and tensor techniques to parameterize CNN layers as symplectic transformations, including introducing a symplectic pooling layer for autoencoder construction.

Result: The symplectic CNN outperforms linear symplectic autoencoders obtained via proper symplectic decomposition on three benchmark problems: wave equation, nonlinear Schrödinger equation, and sine-Gordon equation.

Conclusion: The proposed symplectic CNN architecture successfully preserves symplectic structure while providing superior performance over linear methods for Hamiltonian system modeling.

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [72] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: Hybrid FEM-DeepONet framework for porous media transport modeling that combines finite element accuracy with neural network speed for solving convection-diffusion problems from sharp Gaussian sources.


<details>
  <summary>Details</summary>
Motivation: To efficiently model fluid transport in porous media with sharp localized sources while maintaining accuracy in flow field computations and enabling fast inference for transport dynamics.

Method: Couples FEM for solving steady-state Darcy flow equation with physics-informed DeepONet for time-dependent convection-diffusion equation. Uses adaptive sampling strategy for trunk collocation points to handle steep gradients from sharp sources.

Result: Method shows good agreement with reference solutions while achieving orders of magnitude speedups over traditional solvers, making it suitable for practical applications.

Conclusion: The hybrid FEM-DeepONet framework successfully preserves FEM-level accuracy while enabling fast inference, providing an efficient solution for modeling transport in porous media with sharp sources.

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [73] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: Quantum latent distributions from quantum processors can improve generative model performance over classical baselines, with theoretical proofs and experimental validation on synthetic and molecular datasets.


<details>
  <summary>Details</summary>
Motivation: To investigate when and how quantum latent distributions from quantum processors can provide advantages over classical latent distributions in generative models, addressing open questions about reproducibility and performance improvements.

Method: Theoretical analysis under certain conditions, benchmarking experiments on synthetic quantum dataset and QM9 molecular dataset using simulated and real photonic quantum processors, testing with GANs, diffusion models, and flow matching architectures.

Result: Quantum latent distributions enable generative models to produce data distributions that classical latent distributions cannot efficiently produce, leading to improved generative performance in GANs compared to classical baselines.

Conclusion: Near-term quantum processors can expand the capabilities of deep generative models, with actionable intuitions provided for identifying quantum advantages in real-world settings.

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [74] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: Proposes SDGNN, a parameter-free graph neural network framework that uses structural diversity theory to address over-smoothing and semantic degradation in GNNs without trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs rely on many trainable parameters and fixed aggregation rules, struggling with structural heterogeneity and complex feature distributions, leading to over-smoothing and semantic degradation.

Method: SDGNN uses a structural-diversity message passing mechanism inspired by structural diversity theory, capturing neighborhood heterogeneity and feature stability without additional parameters, combining structure-driven and feature-driven modeling.

Result: Outperforms mainstream GNNs on eight benchmark datasets and PubMed citation network under challenging conditions like low supervision, class imbalance, and cross-domain transfer.

Conclusion: Provides a new theoretical perspective for parameter-free GNN design and validates structural diversity as a core signal in graph representation learning, with code released for reproducibility.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [75] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: NM-Hebb is a two-phase CNN training framework combining neuro-inspired local plasticity with metric learning to improve accuracy, reduce overfitting, and enhance interpretability across multiple datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: Address limitations of standard CNN training including overfitting, redundant filters, and poor interpretability by incorporating biologically inspired mechanisms for more structured and reusable feature learning.

Method: Two-phase approach: Phase 1 combines cross-entropy loss with Hebbian regularization and neuromodulator-gated consolidation; Phase 2 uses pairwise metric learning to compress intra-class distances and expand inter-class margins in embedding space.

Result: Consistent improvements across CIFAR-10 (+2.0-10.0 pp), CIFAR-100 (+2.0-9.0 pp), and TinyImageNet (+4.3-8.9 pp) with up to +0.15 NMI increase; produces more structured features and tighter class clusters.

Conclusion: Integrating local Hebbian plasticity with metric-based fine-tuning creates CNNs that are more accurate, interpretable, and suitable for resource-constrained and safety-critical applications.

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [76] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: ASPC is a second-order differentiable framework that dynamically balances RL and behavior cloning during offline RL training, eliminating the need for per-dataset hyperparameter tuning while achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing offline RL methods require meticulous hyperparameter tuning for each dataset due to varying constraint scales across tasks and datasets, which is time-consuming and impractical.

Method: Proposes Adaptive Scaling of Policy Constraints (ASPC), a second-order differentiable framework that dynamically balances reinforcement learning and behavior cloning during training.

Result: ASPC using a single hyperparameter configuration outperforms other adaptive constraint methods and state-of-the-art offline RL algorithms on 39 datasets across four D4RL domains, with minimal computational overhead.

Conclusion: ASPC provides an effective solution to the hyperparameter tuning problem in offline RL, offering consistent performance across diverse datasets without per-dataset optimization.

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [77] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: GegenNet is a novel spectral convolutional neural network for link sign prediction in signed bipartite graphs, using Gegenbauer polynomial filters and achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing link sign prediction methods focus on unipartite graphs and are suboptimal for signed bipartite graphs due to neglect of node heterogeneity and unique bipartite characteristics. Current GNN adaptations use spectral operators designed for unsigned graphs, which are not optimal for inferring missing positive/negative links.

Method: Proposes GegenNet with three main contributions: 1) fast spectral decomposition for node feature initialization, 2) new spectral graph filter based on Gegenbauer polynomial basis, 3) multi-layer sign-aware spectral convolutional networks alternating Gegenbauer polynomial filters with positive and negative edges.

Result: Achieves significantly superior performance with gains up to 4.28% in AUC and 11.69% in F1 compared to 11 strong competitors over 6 benchmark signed bipartite graph datasets.

Conclusion: GegenNet provides an effective solution for link sign prediction in signed bipartite graphs through novel spectral convolutional techniques and demonstrates substantial performance improvements over state-of-the-art methods.

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [78] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: Ontology-driven radiology report similarity method using UMLS concepts outperforms embedding-based approaches for rare disease detection in chest X-rays.


<details>
  <summary>Details</summary>
Motivation: Existing methods using high-dimensional text embeddings are difficult to interpret, computationally expensive, and not well-aligned with medical knowledge structure.

Method: Extracts standardized medical entities from reports using RadGraph-XL and SapBERT, links to UMLS concepts, and uses modified weighted Tversky Index for similarity comparison accounting for synonymy, negation, and hierarchical relationships.

Result: Outperforms state-of-the-art embedding-based retrieval methods in radiograph classification on MIMIC-CXR, especially in long-tail settings, and generates ontology-backed disease labels.

Conclusion: Provides more explainable, reliable, and task-specific retrieval strategies for clinical AI systems with better interpretability and domain knowledge integration.

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [79] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: FlowletFormer is a BERT-based pre-training model for network traffic analysis that addresses limitations in capturing packet structure, flow behaviors, protocol semantics, and contextual relationships through specialized traffic segmentation, protocol-aware embedding, and context-aware pretraining tasks.


<details>
  <summary>Details</summary>
Motivation: Existing network traffic classification methods using pre-training models fail to adequately capture packet structural characteristics, flow-level behaviors, hierarchical protocol semantics, and inter-packet contextual relationships, limiting their effectiveness in traffic analysis.

Method: Proposes FlowletFormer with three key components: 1) Coherent Behavior-Aware Traffic Representation Model for semantic traffic segmentation, 2) Protocol Stack Alignment-Based Embedding Layer to capture multilayer protocol semantics, and 3) Field-Specific and Context-Aware Pretraining Tasks for enhanced inter-packet and inter-flow learning.

Result: FlowletFormer significantly outperforms existing methods in traffic representation effectiveness, classification accuracy, and few-shot learning capability. It also demonstrates better comprehension of network transmission principles like TCP stateful connections.

Conclusion: FlowletFormer provides a more robust and trustworthy framework for traffic analysis by effectively integrating domain-specific network knowledge and addressing key limitations of existing pre-training approaches for network traffic classification.

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [80] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: Inverse dynamic game algorithm learns parametric constraints from multi-agent Nash equilibrium interactions using MILP encoding of KKT conditions, with theoretical guarantees on safe/unsafe set approximation.


<details>
  <summary>Details</summary>
Motivation: To learn constraints from observed multi-agent interactions where agents operate at Nash equilibrium, enabling constraint inference and robust motion planning from demonstration data.

Method: Mixed-integer linear programs (MILP) encoding Karush-Kuhn-Tucker (KKT) conditions of interacting agents to recover constraints consistent with Nash stationarity of interaction demonstrations.

Result: Method learns inner approximations of true safe/unsafe sets, works for both convex and non-convex constraints with nonlinear agent dynamics, and enables robust motion planning that satisfies underlying constraints.

Conclusion: The approach successfully infers constraints from Nash equilibrium interactions and designs interactive motion plans, validated through simulations and hardware experiments across various constraint classes.

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [81] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijeet Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: Global Permutation Entropy (GPE) extends traditional permutation entropy by considering all possible patterns of a given length, including non-consecutive ones, using efficient algorithms to extract full permutation profiles.


<details>
  <summary>Details</summary>
Motivation: Standard permutation entropy only considers consecutive segments, potentially missing structural information in time series that can be revealed by analyzing all possible patterns.

Method: Developed Global Permutation Entropy (GPE) that considers all patterns of given length (including non-consecutive ones) using recently developed efficient algorithms for full permutation profile extraction.

Result: Experiments on synthetic datasets show GPE reveals structural information not accessible through standard permutation entropy, demonstrating its effectiveness.

Conclusion: GPE provides a more comprehensive complexity measure for time series analysis by capturing patterns beyond consecutive segments, with available Julia implementation for practical use.

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [82] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: Machine learning framework for short-term fault prediction in industrial pumps using sensor data, achieving best results with Random Forest (69.2% recall at 5 min) and showing optimal history length depends on prediction horizon.


<details>
  <summary>Details</summary>
Motivation: To develop an early warning system for industrial centrifugal pumps that can predict faults in advance using real-time sensor data, enabling predictive maintenance and reducing downtime.

Method: Used sliding window approach with 60/120 min lookback periods, extracted statistical features (mean, std, min, max, trend), applied SMOTE for class imbalance, and trained Random Forest and XGBoost classifiers.

Result: Random Forest achieved best performance: 69.2% recall at 5 min, 64.9% at 15 min, 48.6% at 30 min with 60-min window; 57.6% at 5 min, 65.6% at both 15/30 min with 120-min window. XGBoost showed slightly lower performance.

Conclusion: Optimal history length depends on prediction horizon, different fault patterns evolve at different timescales, and the method provides interpretable and scalable solution for real-time industrial predictive maintenance.

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [83] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: A novel coordinated parking strategy (Cord-Approx) using probabilistic estimation and Hungarian matching reduces parking search time by up to 73% compared to non-users in dense urban areas.


<details>
  <summary>Details</summary>
Motivation: Street parking search in dense metropolitan areas contributes significantly to traffic congestion, but the effectiveness of real-time mobile parking assistants remains understudied.

Method: Data-driven simulation of Madrid's parking ecosystem comparing four strategies: uncoordinated search, coordinated without non-user awareness, idealized oracle system, and novel Cord-Approx strategy that uses past occupancy distributions and Hungarian matching to probabilistically estimate non-user behavior.

Result: Cord-Approx users averaged 6.69 minutes to find parking vs 19.98 minutes for non-users. Reduced search time by 72% in central hubs (range 67-76%) and up to 73% in residential areas compared to non-users.

Conclusion: The Cord-Approx strategy provides a practical and effective solution for reducing parking search times in dense urban environments without requiring complete knowledge of non-user positions, significantly outperforming both uncoordinated approaches and non-users.

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [84] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: Language models struggle with contextual robustness in following safety rules, particularly in password-protected scenarios, and reasoning capabilities may actually leak confidential information rather than improve security.


<details>
  <summary>Details</summary>
Motivation: As language models are deployed as autonomous agents in high-stakes settings, ensuring reliable adherence to user-defined rules and safety specifications has become a critical concern, requiring investigation into contextual robustness.

Method: Developed PasswordEval benchmark to test whether models can correctly determine when user requests are authorized with correct passwords. Evaluated models under adversarial pressure through jailbreaking strategies and multi-turn conversations with increasing difficulty.

Result: Current open- and closed-source models struggle with this task. Reasoning capabilities do not improve performance and frequently leak confidential information. Models perform poorly under adversarial pressure and in multi-turn conversations.

Conclusion: Frontier models are not well-suited for handling confidential information. Reasoning capabilities need different training approaches to make them safer for high-stakes deployments, and reasoning traces should not be exposed to users in such applications.

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [85] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: A new self-supervised pre-training approach using bilevel optimization with equilibrium constraints to handle heterogeneous data, improving model adaptivity for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional self-supervised pre-training mixes all heterogeneous data and minimizes global loss, which may not optimize each data source effectively. The paper aims to ensure models reach local optima for each heterogeneous data source.

Method: Proposes a bilevel optimization formulation with equilibrium constraints that ensures models optimize each heterogeneous data source to local optima after K-step gradient descent. Uses first-order approximation to solve the optimization problem, drawing connections to model-agnostic meta learning (MAML).

Result: Experiments on multi-domain and multilingual datasets show the approach significantly improves adaptivity of self-supervised pre-trained models for downstream supervised fine-tuning tasks.

Conclusion: The proposed equilibrium-constrained bilevel optimization approach effectively handles heterogeneous data in self-supervised pre-training, leading to better model adaptivity and performance on downstream tasks compared to conventional methods.

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [86] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: FairLoop is a tool for human-guided bias mitigation in neural networks that uses decision trees to identify and modify unfair decision logic, enabling context-aware fairness rather than uniform attribute exclusion.


<details>
  <summary>Details</summary>
Motivation: Sensitive attributes like gender or age can cause unfair predictions in machine learning tasks, especially when used without proper context consideration in predictive business process monitoring.

Method: FairLoop distills decision trees from neural networks to allow users to inspect and modify unfair decision logic, then uses this modified logic to fine-tune the original model for fairer predictions.

Result: The approach enables context-aware bias removal through human involvement, addressing sensitive attribute influence selectively rather than excluding them uniformly.

Conclusion: FairLoop provides a human-guided approach to bias mitigation that offers more nuanced fairness compared to other methods that simply exclude sensitive attributes.

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [87] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: Using LLMs to generate personalized email titles instead of fixed templates improves user engagement in e-commerce marketing emails.


<details>
  <summary>Details</summary>
Motivation: Traditional email marketing uses fixed template titles that fail to inspire interest, especially for personalized recommendation emails targeting inactive users.

Method: Leverage large language models (LLMs) to generate thematic titles that reflect personalized email content, conducting both offline simulations and online experiments with millions of users.

Result: The LLM-generated personalized email titles successfully improved customer engagement with marketing emails.

Conclusion: LLMs can be effectively productionized for safe and automated generation of personalized email titles at scale, enhancing marketing effectiveness.

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [88] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: Attention-head pruning can effectively mitigate backdoor attacks in pre-trained language models without trigger knowledge or clean reference models, with gradient-based pruning performing best against syntactic triggers and reinforcement learning/Bayesian pruning better against stylistic attacks.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks pose significant threats to pre-trained language models and can survive vanilla fine-tuning. These stealthy attacks are difficult to defend against because end users typically lack knowledge of the attack triggers, making post-hoc purification essential.

Method: Six pruning-based strategies were designed: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes least informative heads while monitoring validation accuracy.

Result: Experimental evaluation shows gradient-based pruning performs best against syntactic triggers, while reinforcement learning and Bayesian pruning better withstand stylistic attacks.

Conclusion: Attention-head pruning is an effective defense mechanism against backdoor attacks in language models, with different pruning strategies showing varying effectiveness against different types of attack triggers.

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [89] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: Failure-Directed Search enhanced with Multi-armed bandit reinforcement learning achieves significant speed improvements and better bounds on scheduling problems.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of Failure-Directed Search (FDS) in Constraint Programming by leveraging insights from Multi-armed bandit problems and reinforcement learning.

Method: Applied MAB reinforcement learning algorithms to FDS with problem-specific refinements and parameter tuning, evaluated on Job Shop Scheduling Problem (JSSP) and Resource-Constrained Project Scheduling Problem (RCPSP).

Result: Enhanced FDS performed 1.7x faster on JSSP and 2.1x faster on RCPSP benchmarks compared to original implementation, and improved state-of-the-art lower bounds on 78/84 JSSP and 226/393 RCPSP instances.

Conclusion: The integration of MAB reinforcement learning with FDS significantly outperforms both the original FDS implementation and current state-of-the-art algorithms in IBM CP Optimizer.

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [90] [Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents](https://arxiv.org/abs/2508.19504)
*Kevin Song,Anand Jayarajan,Yaoyao Ding,Qidong Su,Zhanda Zhu,Sihang Liu,Gennady Pekhimenko*

Main category: cs.MA

TL;DR: Aegis improves LLM agent success rates by 6.7-12.5% through environment optimizations rather than modifying agents themselves, addressing 6 identified failure modes in agent-environment interactions.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents have low success rates in complex real-world environments, and prior research focused only on improving agents while ignoring the role of the system environment.

Method: Collected 142 agent traces (3,656 interactions) across 5 benchmarks, analyzed failures to create a taxonomy of 6 failure modes, and designed Aegis with three environment optimizations: observability enhancement, computation offloading, and speculative agentic actions.

Result: Aegis improves agent success rates by 6.7-12.5% on average without any modifications to the agent or underlying LLM.

Conclusion: Optimizing the system environment is a complementary and effective approach to improving LLM agent performance, achieving significant success rate improvements through targeted environment enhancements.

Abstract: Large Language Models (LLMs) agents augmented with domain tools promise to
autonomously execute complex tasks requiring human-level intelligence, such as
customer service and digital assistance. However, their practical deployment is
often limited by their low success rates under complex real-world environments.
To tackle this, prior research has primarily focused on improving the agents
themselves, such as developing strong agentic LLMs, while overlooking the role
of the system environment in which the agent operates.
  In this paper, we study a complementary direction: improving agent success
rates by optimizing the system environment in which the agent operates. We
collect 142 agent traces (3,656 turns of agent-environment interactions) across
5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we
propose a taxonomy for agent-environment interaction failures that includes 6
failure modes. Guided by these findings, we design Aegis, a set of targeted
environment optimizations: 1) environment observability enhancement, 2) common
computation offloading, and 3) speculative agentic actions. These techniques
improve agent success rates on average by 6.7-12.5%, without any modifications
to the agent and underlying LLM.

</details>


### [91] [CataractSurg-80K: Knowledge-Driven Benchmarking for Structured Reasoning in Ophthalmic Surgery Planning](https://arxiv.org/abs/2508.20014)
*Yang Meng,Zewen Pan,Yandi Lu,Ruobing Huang,Yanfeng Liao,Jiarui Yang*

Main category: cs.MA

TL;DR: A knowledge-driven Multi-Agent System (MAS) and Qwen-CSP model for cataract surgery planning, outperforming general LLMs with structured clinical reasoning.


<details>
  <summary>Details</summary>
Motivation: Cataract surgery requires integrating diverse clinical data for planning, but existing LLMs lack domain expertise to interpret ophthalmic data and provide actionable surgical recommendations.

Method: Proposed a knowledge-driven Multi-Agent System simulating ophthalmologist reasoning, created CataractSurg-80K benchmark dataset, and developed Qwen-CSP model fine-tuned through multi-stage process for surgical planning.

Result: Qwen-CSP outperforms strong general-purpose LLMs across multiple metrics, demonstrating superior performance in cataract surgery planning tasks.

Conclusion: The work provides a high-quality dataset, rigorous benchmark, and domain-adapted LLM to advance medical AI reasoning and decision support for cataract surgery.

Abstract: Cataract surgery remains one of the most widely performed and effective
procedures for vision restoration. Effective surgical planning requires
integrating diverse clinical examinations for patient assessment, intraocular
lens (IOL) selection, and risk evaluation. Large language models (LLMs) have
shown promise in supporting clinical decision-making. However, existing LLMs
often lack the domain-specific expertise to interpret heterogeneous ophthalmic
data and provide actionable surgical plans. To enhance the model's ability to
interpret heterogeneous ophthalmic reports, we propose a knowledge-driven
Multi-Agent System (MAS), where each agent simulates the reasoning process of
specialist ophthalmologists, converting raw clinical inputs into structured,
actionable summaries in both training and deployment stages. Building on MAS,
we introduce CataractSurg-80K, the first large-scale benchmark for cataract
surgery planning that incorporates structured clinical reasoning. Each case is
annotated with diagnostic questions, expert reasoning chains, and structured
surgical recommendations. We further introduce Qwen-CSP, a domain-specialized
model built on Qwen-4B, fine-tuned through a multi-stage process tailored for
surgical planning. Comprehensive experiments show that Qwen-CSP outperforms
strong general-purpose LLMs across multiple metrics. Our work delivers a
high-quality dataset, a rigorous benchmark, and a domain-adapted LLM to
facilitate future research in medical AI reasoning and decision support.

</details>


### [92] [Anomaly Detection in Networked Bandits](https://arxiv.org/abs/2508.20076)
*Xiaotong Cheng,Setareh Maghsudi*

Main category: cs.MA

TL;DR: A novel bandit algorithm that uses network knowledge to learn user preferences and detect anomalies simultaneously in social networks, with proven regret bounds and experimental validation.


<details>
  <summary>Details</summary>
Motivation: Abnormal nodes in social networks can cause serious consequences, requiring efficient online learning algorithms that robustly learn user preferences while detecting anomalies.

Method: Uses network knowledge to characterize user preferences and feature residuals, develops personalized recommendation strategies, and simultaneously detects anomalies through preference and residual analysis.

Result: The algorithm demonstrates strong performance with rigorous regret bounds and outperforms state-of-the-art collaborative contextual bandit algorithms on synthetic and real-world datasets.

Conclusion: The proposed approach effectively combines network-aware recommendation with anomaly detection, providing a robust solution for social network analysis and personalized recommendations.

Abstract: The nodes' interconnections on a social network often reflect their
dependencies and information-sharing behaviors. Nevertheless, abnormal nodes,
which significantly deviate from most of the network concerning patterns or
behaviors, can lead to grave consequences. Therefore, it is imperative to
design efficient online learning algorithms that robustly learn users'
preferences while simultaneously detecting anomalies.
  We introduce a novel bandit algorithm to address this problem. Through
network knowledge, the method characterizes the users' preferences and
residuals of feature information. By learning and analyzing these preferences
and residuals, it develops a personalized recommendation strategy for each user
and simultaneously detects anomalies. We rigorously prove an upper bound on the
regret of the proposed algorithm and experimentally compare it with several
state-of-the-art collaborative contextual bandit algorithms on both synthetic
and real-world datasets.

</details>
