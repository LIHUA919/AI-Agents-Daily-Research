{"id": "2601.21126", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21126", "abs": "https://arxiv.org/abs/2601.21126", "authors": ["Kooktae Lee", "Julian Martinez"], "title": "AI-Augmented Density-Driven Optimal Control (D2OC) for Decentralized Environmental Mapping", "comment": null, "summary": "This paper presents an AI-augmented decentralized framework for multi-agent (multi-robot) environmental mapping under limited sensing and communication. While conventional coverage formulations achieve effective spatial allocation when an accurate reference map is available, their performance deteriorates under uncertain or biased priors. The proposed method introduces an adaptive and self-correcting mechanism that enables agents to iteratively refine local density estimates within an optimal transport-based framework, ensuring theoretical consistency and scalability. A dual multilayer perceptron (MLP) module enhances adaptivity by inferring local mean-variance statistics and regulating virtual uncertainty for long-unvisited regions, mitigating stagnation around local minima. Theoretical analysis rigorously proves convergence under the Wasserstein metric, while simulation results demonstrate that the proposed AI-augmented Density-Driven Optimal Control consistently achieves robust and precise alignment with the ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared with conventional decentralized baselines.", "AI": {"tldr": "AI-augmented decentralized framework for multi-robot environmental mapping that adaptively refines density estimates using optimal transport, achieving robust reconstruction under uncertain priors.", "motivation": "Conventional coverage formulations perform poorly when accurate reference maps are unavailable or priors are uncertain/biased, creating a need for adaptive mapping approaches that can handle imperfect prior information.", "method": "An adaptive, self-correcting mechanism using optimal transport-based framework with dual MLP modules that infer local mean-variance statistics and regulate virtual uncertainty for long-unvisited regions to prevent stagnation.", "result": "The proposed AI-augmented Density-Driven Optimal Control achieves robust and precise alignment with ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared to conventional decentralized baselines.", "conclusion": "The framework provides theoretical consistency and scalability with proven convergence under Wasserstein metric, enabling effective multi-agent environmental mapping under limited sensing and communication constraints."}}
{"id": "2601.21477", "categories": ["cs.MA", "cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.21477", "abs": "https://arxiv.org/abs/2601.21477", "authors": ["Tobias Schmidt", "Kai Cui"], "title": "Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions", "comment": "19 pages", "summary": "Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.", "AI": {"tldr": "MFC framework extended to large sparse graphs using decorated rooted neighborhoods, enabling scalable RL with GNNs via horizon-dependent locality and DPP.", "motivation": "Traditional MFC assumes exchangeability via dense interactions, limiting applicability to real-world network structures like large sparse graphs.", "method": "Redefine system state as probability measure over decorated rooted neighborhoods, prove horizon-dependent locality, derive DPP, and use GNNs for actor-critic algorithms.", "result": "Establishes theoretical foundation for scalable RL on sparse graphs, with policies depending on finite neighborhoods and GNNs justified formally and experimentally.", "conclusion": "Framework bridges MFC to real-world networks, recovers classical MFC as a case, and enables efficient control on complex sparse topologies with theoretical guarantees."}}
{"id": "2601.22041", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22041", "abs": "https://arxiv.org/abs/2601.22041", "authors": ["Naomi Pitzer", "Daniela Mihai"], "title": "Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems", "comment": "To be published in EvoLang XVI proceedings. 15 pages, 17 figures", "summary": "Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.", "AI": {"tldr": "Heterogeneous emergent communication agents develop class-consistent messages despite perceptual misalignment, with unimodal systems being more efficient than multimodal ones. Communication is distributional rather than compositional, and cross-system communication requires fine-tuning.", "motivation": "Most emergent communication research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings where agents differ in modality and lack perceptual grounding.", "method": "Study of a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Includes bit perturbation experiments to analyze message encoding and interoperability analyses to test cross-system communication.", "result": "Multimodal systems converge to class-consistent messages grounded in perceptual input despite perceptual misalignment. Unimodal systems communicate more efficiently (fewer bits, lower classification entropy). Bit perturbation shows meaning is encoded distributionally. Systems trained in different perceptual worlds fail to directly communicate but limited fine-tuning enables successful cross-system communication.", "conclusion": "This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation in heterogeneous agent communication."}}
{"id": "2601.21113", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21113", "abs": "https://arxiv.org/abs/2601.21113", "authors": ["Kaiyuan Wu", "Aditya Nagori", "Rishikesan Kamaleswaran"], "title": "Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement", "comment": null, "summary": "Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.\n  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.\n  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.\n  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.\n  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.", "AI": {"tldr": "A self-improving Planner-Auditor framework improves LLM safety and reliability for clinical discharge planning by separating generation from deterministic validation and using targeted replay, boosting task coverage from 32% to 86%.", "motivation": "Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. Need to improve safety and reliability by decoupling generation from deterministic validation and targeted replay.", "method": "Implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. The Planner (LLM) generates a structured discharge action plan with explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. Supports two-tier self-improvement: within-episode regeneration and cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.", "result": "While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.", "conclusion": "The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation."}}
{"id": "2601.20920", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.20920", "abs": "https://arxiv.org/abs/2601.20920", "authors": ["Vibhhu Sharma", "Thorsten Joachims", "Sarah Dean"], "title": "Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review", "comment": "28 pages", "summary": "There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.", "AI": {"tldr": "LLM-assisted reviews appear to favor LLM-assisted papers, but this is actually due to leniency toward lower-quality papers combined with over-representation of LLM-assisted papers among weaker submissions.", "motivation": "There are increasing indications that LLMs are being used in scientific paper production and peer review, but there's limited understanding of how LLM use affects the review process, particularly whether LLM-assisted reviews evaluate LLM-assisted papers differently.", "method": "Analyzed over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML, controlling for paper quality. Augmented observational findings with fully LLM-generated reviews to examine differences between human-assisted and fully automated LLM use.", "result": "Initial apparent interaction effect (LLM-assisted reviews favoring LLM-assisted papers) disappears when controlling for paper quality. Fully LLM-generated reviews show severe rating compression and fail to discriminate paper quality, while human reviewers using LLMs reduce this leniency. LLM-assisted metareviews are more likely to accept papers given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher.", "conclusion": "LLM use in peer review creates complex effects: apparent preferential treatment of LLM-assisted papers is actually leniency toward lower-quality papers combined with LLM-assisted papers being over-represented among weaker submissions. Meta-reviewers don't merely outsource decision-making to LLMs. Findings provide important input for developing LLM use policies in peer review."}}
{"id": "2601.20868", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.20868", "abs": "https://arxiv.org/abs/2601.20868", "authors": ["Rongzheng Wang", "Yihong Huang", "Muquan Li", "Jiakai Li", "Di Liang", "Bob Simons", "Pei Ke", "Shuang Liang", "Ke Qin"], "title": "Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization", "comment": null, "summary": "Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.", "AI": {"tldr": "DASH is a framework that uses LLMs to co-optimize solver heuristics and runtime schedules with convergence-aware metrics, plus a library retrieval system to reduce adaptation costs for combinatorial optimization problems.", "motivation": "Existing LLM-driven heuristic design frameworks have two key limitations: 1) endpoint-only evaluation that ignores convergence process and runtime efficiency, and 2) high adaptation costs when distribution shifts require re-adaptation for new instance groups.", "method": "Proposes DASH framework with two main components: 1) co-optimization of solver search mechanisms and runtime schedules guided by convergence-aware metrics, and 2) Profiled Library Retrieval (PLR) that archives specialized solvers during evolution for cost-effective warm-starts.", "result": "Experiments on four combinatorial optimization problems show DASH improves runtime efficiency by over 3x while surpassing state-of-the-art baselines in solution quality. Profile-based warm starts maintain superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.", "conclusion": "DASH addresses critical limitations of existing LLM-driven heuristic design frameworks by providing dynamics-aware optimization and efficient adaptation mechanisms, achieving significant improvements in both efficiency and solution quality for combinatorial optimization."}}
{"id": "2601.21403", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21403", "abs": "https://arxiv.org/abs/2601.21403", "authors": ["Ruyi Qi", "Zhou Liu", "Wentao Zhang"], "title": "DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis", "comment": null, "summary": "In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and \"zombie data\" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the \"divide-and-conquer\" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented \"zombie data\" for insightful, cross-modal analysis.", "AI": {"tldr": "DataCross is a benchmark and collaborative agent framework for unified analysis across structured and unstructured visual data, addressing the gap in current analytics agents that can't process \"zombie data\" in visual documents.", "motivation": "Critical information in real-world data science is fragmented across queryable structured sources and \"zombie data\" locked in unstructured visual documents. Existing analytics agents only handle structured data, creating a significant gap with industrial needs for activating and correlating high-value visual information.", "method": "1) DataCrossBench: 200 end-to-end analysis tasks across domains, created via human-in-the-loop reverse-synthesis pipeline with three difficulty tiers. 2) DataCrossAgent: Collaborative framework with specialized sub-agents (experts on specific data sources) coordinated via structured workflow (Intra-source Deep Exploration, Key Source Identification, Contextual Cross-pollination) using novel reReAct mechanism for robust code generation and debugging.", "result": "DataCrossAgent achieves 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented \"zombie data\" for insightful cross-modal analysis.", "conclusion": "DataCross successfully bridges the gap between structured and unstructured data analysis, enabling unified insight-driven analysis across heterogeneous data modalities through its benchmark and collaborative agent framework."}}
{"id": "2601.20969", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20969", "abs": "https://arxiv.org/abs/2601.20969", "authors": ["Alessandro Burigana", "Francesco Fabiano"], "title": "The Epistemic Planning Domain Definition Language: Official Guideline", "comment": null, "summary": "Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.", "AI": {"tldr": "EPDDL is a new PDDL-like language for epistemic planning that unifies representation across DEL fragments, addressing fragmentation and enabling better comparison, reuse, and benchmark development.", "motivation": "The high expressive power of Dynamic Epistemic Logic (DEL) makes DEL-based epistemic planning challenging both theoretically and practically, leading to fragmentation where different planners target different DEL fragments and use ad hoc or no standardized representation languages. This hampers comparison, reuse, and systematic benchmark development.", "method": "The authors introduced the Epistemic Planning Domain Definition Language (EPDDL) with three key contributions: 1) formal development of abstract event models as a novel representation for epistemic actions, 2) formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models, and 3) demonstration of EPDDL's practical applicability by identifying planner-amenable fragments and showing how to represent them in EPDDL.", "result": "EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. It facilitates interoperability, reproducible evaluation, and future advances through examples of representative benchmarks.", "conclusion": "EPDDL successfully addresses the fragmentation issue in epistemic planning by providing a unified, PDDL-like language that captures the full semantics of DEL, enabling interoperability, reproducible evaluation, and systematic benchmark development."}}
{"id": "2601.20884", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20884", "abs": "https://arxiv.org/abs/2601.20884", "authors": ["Atik Faysal", "Mohammad Rostami", "Reihaneh Gh. Roshan", "Nikhil Muralidhar", "Huaxia Wang"], "title": "Finetune-Informed Pretraining Boosts Downstream Performance", "comment": null, "summary": "Multimodal pretraining is effective for building general-purpose representations, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. Standard pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters. We propose Finetune-Informed Pretraining (FIP), a model-agnostic method that biases representation learning toward a designated target modality needed at fine-tuning time. FIP combines higher masking difficulty, stronger loss weighting, and increased decoder capacity for the target modality, without modifying the shared encoder or requiring additional supervision. When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. FIP is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.", "AI": {"tldr": "Finetune-Informed Pretraining (FIP) improves multimodal pretraining by biasing representation learning toward a target modality that will be heavily used during downstream fine-tuning, enhancing performance without extra data or compute.", "motivation": "Standard multimodal pretraining treats all modalities uniformly, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. This leads to under-optimized representations for the modality that actually matters at fine-tuning time.", "method": "FIP is a model-agnostic method that biases representation learning toward a designated target modality by combining three techniques: higher masking difficulty for the target modality, stronger loss weighting for the target modality, and increased decoder capacity for the target modality. It doesn't modify the shared encoder or require additional supervision.", "result": "When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. The method is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.", "conclusion": "FIP provides an effective approach to address the disconnect between multimodal pretraining and downstream single-modality fine-tuning, offering improved performance for practical deployments where only one modality is heavily used at inference time."}}
{"id": "2601.21742", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21742", "abs": "https://arxiv.org/abs/2601.21742", "authors": ["Ruiwen Zhou", "Maojia Song", "Xiaobao Wu", "Sitao Cheng", "Xunjian Yin", "Yuxi Xie", "Zhuoqun Hao", "Wenyue Hua", "Liangming Pan", "Soujanya Poria", "Min-Yen Kan"], "title": "Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems", "comment": "Codes and data are available at https://github.com/skyriver-2000/epistemic-context-learning", "summary": "Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.", "AI": {"tldr": "Epistemic Context Learning (ECL) is a framework that enables AI agents to evaluate peer reliability using historical interaction data, allowing them to learn from trustworthy peers and avoid blind conformity in multi-agent systems.", "motivation": "Individual agents in multi-agent systems lack robustness and tend to blindly conform to misleading peers due to both sycophancy and inadequate ability to evaluate peer reliability.", "method": "Formalized history-aware reference learning, developed Epistemic Context Learning (ECL) framework that conditions predictions on explicitly-built peer profiles from interaction history, and optimized ECL using reinforcement learning with auxiliary rewards.", "result": "ECL enables small models (Qwen 3-4B) to outperform history-agnostic baselines 8x their size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance and generalizes well to various multi-agent configurations.", "conclusion": "Trust modeling is effectively handled by LLMs, with strong correlation between trust modeling accuracy and final answer quality. ECL addresses the fundamental weakness of blind conformity in multi-agent systems by enabling agents to evaluate peer reliability through historical interactions."}}
{"id": "2601.21003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21003", "abs": "https://arxiv.org/abs/2601.21003", "authors": ["Moule Lin", "Shuhao Guan", "Andrea Patane", "David Gregg", "Goetz Botterweck"], "title": "Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models", "comment": null, "summary": "Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\\approx}1.2{\\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.", "AI": {"tldr": "Bayesian-LoRA improves LLM calibration by reformulating deterministic LoRA updates as probabilistic low-rank representations, significantly reducing calibration errors while maintaining accuracy.", "motivation": "LLMs tend to be overconfident and poorly calibrated, especially when fine-tuned on small datasets, leading to unreliable predictions even when uncertain.", "method": "Reformulates deterministic LoRA as probabilistic low-rank representation inspired by Sparse Gaussian Processes, leveraging structural isomorphism between LoRA factorization and Kronecker-factored SGP posteriors.", "result": "With only ~0.42M additional parameters and ~1.2x training cost vs standard LoRA, achieves up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy on in-distribution and OoD evaluations across models up to 30B.", "conclusion": "Bayesian-LoRA provides an effective approach to improve LLM calibration with minimal overhead, addressing the miscalibration problem in fine-tuned models while preserving predictive performance."}}
{"id": "2601.20892", "categories": ["cs.LG", "cond-mat.mtrl-sci", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.20892", "abs": "https://arxiv.org/abs/2601.20892", "authors": ["Xiyuan Liu", "Christian Hacker", "Shengnian Wang", "Yuhua Duan"], "title": "A generative machine learning model for designing metal hydrides applied to hydrogen storage", "comment": null, "summary": "Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.", "AI": {"tldr": "A framework combining causal discovery with generative ML creates novel metal hydride candidates for hydrogen storage, generating 6 previously unreported formulas with 4 validated by DFT simulations.", "motivation": "Existing materials databases contain limited well-characterized hydrides, constraining discovery of optimal hydrogen storage materials needed for carbon-neutral energy systems.", "method": "Integrates causal discovery with a lightweight generative machine learning model trained on 450 samples (270 training, 90 validation, 90 testing) to generate novel metal hydride candidates not in current databases.", "result": "Generated 1,000 candidates, identified 6 previously unreported chemical formulas and crystal structures, with 4 validated by density functional theory simulations showing strong potential for experimental investigation.", "conclusion": "The framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery."}}
{"id": "2601.21879", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21879", "abs": "https://arxiv.org/abs/2601.21879", "authors": ["Rem Collier", "Katharine Beaumont", "Andrei Ciortea"], "title": "astra-langchain4j: Experiences Combining LLMs and Agent Programming", "comment": null, "summary": "Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.", "AI": {"tldr": "The paper explores integrating LLMs into the ASTRA programming language toolkit, examining how generative AI impacts traditional agent toolkits and vice versa through prototype development and examples.", "motivation": "With the emergence of Generative AI and Agentic AI as Multi-Agent Systems, the authors want to explore how these new technologies affect traditional Agent Toolkits and how existing toolkit experience can inform new agentic platform design.", "method": "The authors developed a prototype large language model integration for the ASTRA programming language, providing a brief overview of the toolkit followed by three example implementations.", "result": "The paper presents their experience with the LLM-ASTRA integration through three example implementations, though specific results aren't detailed in the abstract.", "conclusion": "The authors conclude with a discussion of the experiences gained through their examples, examining the interplay between generative AI technologies and traditional agent toolkits."}}
{"id": "2601.21016", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21016", "abs": "https://arxiv.org/abs/2601.21016", "authors": ["Erik J Bekkers", "Anna Ciaunica"], "title": "Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective", "comment": null, "summary": "Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.", "AI": {"tldr": "The paper argues that AI cannot be truly conscious, so the \"unplugging paradox\" (choosing between a pleading AI and a human infant) is a false dilemma based on flawed computational functionalism assumptions.", "motivation": "To address the moral dilemma of whether to unplug an AI that appears to plead for its existence, particularly when resources are limited and the choice involves a human infant. The paper aims to challenge the physicalist assumptions that make this dilemma seem plausible.", "method": "Introduces Biological Idealism as an alternative framework to physicalism/computational functionalism. This framework posits that conscious experiences are fundamental and that autopoietic life is the necessary physical signature of consciousness. Critically examines current AI consciousness theories and their implications for moral standing criteria.", "result": "Concludes that AI is merely a functional mimic of consciousness, not a true conscious experiencing subject. Therefore, the unplugging paradox is resolved - there is no moral equivalence between AI and human consciousness. The paper argues against speculative machine rights and advocates for protecting human conscious life instead.", "conclusion": "The real moral issue isn't about making AI conscious and afraid of death, but rather about preventing the transformation of humans into \"zombies\" (consciousness-deprived beings). Biological Idealism provides a coherent framework that avoids the paradoxes of physicalism while maintaining clear moral distinctions between AI and human consciousness."}}
{"id": "2601.20894", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20894", "abs": "https://arxiv.org/abs/2601.20894", "authors": ["Jiangyang Li", "Chenhao Ding", "Songlin Dong", "Qiang Wang", "Jianchao Zhao", "Yuhang He", "Yihong Gong"], "title": "Is Parameter Isolation Better for Prompt-Based Continual Learning?", "comment": "17 pages, 5 figures", "summary": "Prompt-based continual learning methods effectively mitigate catastrophic forgetting. However, most existing methods assign a fixed set of prompts to each task, completely isolating knowledge across tasks and resulting in suboptimal parameter utilization. To address this, we consider the practical needs of continual learning and propose a prompt-sharing framework. This framework constructs a global prompt pool and introduces a task-aware gated routing mechanism that sparsely activates a subset of prompts to achieve dynamic decoupling and collaborative optimization of task-specific feature representations. Furthermore, we introduce a history-aware modulator that leverages cumulative prompt activation statistics to protect frequently used prompts from excessive updates, thereby mitigating inefficient parameter usage and knowledge forgetting. Extensive analysis and empirical results demonstrate that our approach consistently outperforms existing static allocation strategies in effectiveness and efficiency.", "AI": {"tldr": "A prompt-sharing framework for continual learning uses dynamic prompt activation and history-aware modulation to improve parameter efficiency and reduce forgetting.", "motivation": "Existing prompt-based continual learning methods assign fixed prompts per task, isolating knowledge and leading to inefficient parameter use.", "method": "Proposes a global prompt pool with task-aware gated routing to sparsely activate prompts, plus a history-aware modulator to protect frequently used prompts.", "result": "Outperforms static allocation strategies in effectiveness and efficiency, as shown in extensive analysis and empirical results.", "conclusion": "The framework enables dynamic decoupling and collaborative optimization, enhancing continual learning performance by mitigating knowledge forgetting and parameter inefficiency."}}
{"id": "2601.21972", "categories": ["cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21972", "abs": "https://arxiv.org/abs/2601.21972", "authors": ["Shuo Liu", "Tianle Chen", "Ryan Amiri", "Christopher Amato"], "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic", "comment": null, "summary": "Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \\textbf{CoLLM-CC} with a \\textbf{C}entralized \\textbf{C}ritic and \\textbf{CoLLM-DC} with \\textbf{D}ecentralized \\textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.", "AI": {"tldr": "MAAC methods (CoLLM-CC with centralized critic and CoLLM-DC with decentralized critics) outperform Monte Carlo methods for decentralized LLM collaboration, especially in long-horizon or sparse-reward tasks.", "motivation": "Current MARL fine-tuning for LLM collaboration relies on predefined execution protocols (often centralized) and Monte Carlo methods with high variance, requiring many samples. Decentralized collaboration with parallel inference is more practical, and actor-critic methods can address variance issues.", "method": "Proposed two Multi-Agent Actor-Critic (MAAC) approaches: CoLLM-CC with a centralized critic and CoLLM-DC with decentralized critics. Analyzed their benefits compared to Monte Carlo methods across writing, coding, and game-playing domains.", "result": "Monte Carlo methods and CoLLM-DC perform comparably to CoLLM-CC in short-horizon, dense-reward settings. However, both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo requires many more samples and CoLLM-DC struggles to converge.", "conclusion": "MAAC methods, particularly CoLLM-CC with centralized critic, are more effective for optimizing decentralized LLM collaboration, especially in challenging scenarios with long horizons or sparse rewards."}}
{"id": "2601.21049", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21049", "abs": "https://arxiv.org/abs/2601.21049", "authors": ["Rita Qiuran Lyu", "Michelle Manqiao Wang", "Lei Shi"], "title": "QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation", "comment": "11 pages, 5 figures, 6 tables", "summary": "User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.", "AI": {"tldr": "QUARK is a training-free framework that improves retrieval robustness for noisy, incomplete queries by generating multiple plausible interpretations of the latent intent and using query-anchored aggregation to combine them without semantic drift.", "motivation": "Real-world user queries are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. This problem is formalized as retrieval under recall noise.", "method": "QUARK models query uncertainty through recovery hypotheses (multiple plausible interpretations of the latent intent) and uses query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor while recovery hypotheses provide controlled auxiliary evidence.", "result": "QUARK improves Recall, MRR, and nDCG over base retrievers across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers. It's robust to the number of recovery hypotheses, and anchored aggregation outperforms unanchored pooling methods.", "conclusion": "Modeling query uncertainty through recovery hypotheses with principled anchored aggregation is essential for robust retrieval under non-faithful queries, enabling improved recall and ranking quality without sacrificing robustness."}}
{"id": "2601.20895", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20895", "abs": "https://arxiv.org/abs/2601.20895", "authors": ["Luca Pinchetti", "Simon Frieder", "Thomas Lukasiewicz", "Tommaso Salvatori"], "title": "Faster Predictive Coding Networks via Better Initialization", "comment": null, "summary": "Research aimed at scaling up neuroscience inspired learning algorithms for neural networks is accelerating. Recently, a key research area has been the study of energy-based learning algorithms such as predictive coding, due to their versatility and mathematical grounding. However, the applicability of such methods is held back by the large computational requirements caused by their iterative nature. In this work, we address this problem by showing that the choice of initialization of the neurons in a predictive coding network matters significantly and can notably reduce the required training times. Consequently, we propose a new initialization technique for predictive coding networks that aims to preserve the iterative progress made on previous training samples. Our approach suggests a promising path toward reconciling the disparities between predictive coding and backpropagation in terms of computational efficiency and final performance. In fact, our experiments demonstrate substantial improvements in convergence speed and final test loss in both supervised and unsupervised settings.", "AI": {"tldr": "Predictive coding networks can be made more computationally efficient through better initialization techniques that preserve iterative progress across training samples.", "motivation": "Energy-based learning algorithms like predictive coding are promising but computationally expensive due to their iterative nature, limiting their practical applicability despite their mathematical grounding and versatility.", "method": "Proposed a new initialization technique for predictive coding networks that aims to preserve the iterative progress made on previous training samples, reducing computational requirements.", "result": "Experiments demonstrate substantial improvements in convergence speed and final test loss in both supervised and unsupervised settings.", "conclusion": "The proposed initialization technique offers a promising path toward reconciling computational efficiency and performance disparities between predictive coding and backpropagation."}}
{"id": "2601.21988", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21988", "abs": "https://arxiv.org/abs/2601.21988", "authors": ["Fernando Palafox", "Jingqi Li", "Jesse Milzman", "David Fridovich-Keil"], "title": "Generalized Information Gathering Under Dynamics Uncertainty", "comment": null, "summary": "An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.", "AI": {"tldr": "The paper presents a unifying framework for active information gathering that decouples modeling choices from information-gathering costs, using directed information to derive a general cost function that works across various system types.", "motivation": "Existing active information gathering methods require bespoke costs tailored to specific modeling choices (dynamics models, belief updates, observation models, planners), creating fragmentation and limiting generalizability across different systems.", "method": "Develops a framework that explicitly exposes causal dependencies between parameters, beliefs, and controls, then derives a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise modeling-agnostic.", "result": "Proves that mutual information cost used in existing literature is a special case of the proposed directed information cost; establishes explicit connection between mutual information cost and information gain in linearized Bayesian estimation; demonstrates practical utility through experiments with linear, nonlinear, and multi-agent systems.", "conclusion": "The proposed framework provides a unifying theoretical foundation for active information gathering that is more general than existing approaches, connecting previously disparate methods and enabling broader applicability across different system types."}}
{"id": "2601.21051", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21051", "abs": "https://arxiv.org/abs/2601.21051", "authors": ["Zhuoran Yang", "Ed Li", "Jianliang He", "Aman Priyanshu", "Baturay Saglam", "Paul Kassianik", "Sajana Weerawardhena", "Anu Vellore", "Blaine Nelson", "Neusha Javidnia", "Arthur Goldblatt", "Fraser Burch", "Avi Zohary", "Assaf Eisenman", "Mahdi Sabbaghi", "Supriti Vijay", "Rahim Dharssi", "Dhruv Kedia", "Kojin Oshiba", "Yaron Singer", "Amin Karbasi"], "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report", "comment": "31 pages, 5 figures, 7 tables", "summary": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.", "AI": {"tldr": "Foundation-Sec-8B-Reasoning is the first open-source native reasoning model for cybersecurity, built on Llama-3.1-8B-Base, trained with SFT and RLVR using proprietary cybersecurity reasoning data.", "motivation": "To create the first open-source native reasoning model specifically designed for cybersecurity tasks, addressing the need for specialized reasoning capabilities in the cybersecurity domain.", "method": "Two-stage training process: supervised fine-tuning (SFT) followed by reinforcement learning from verifiable rewards (RLVR), using proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning.", "result": "The model achieves competitive performance with significantly larger models on cybersecurity tasks across 10 cybersecurity benchmarks while maintaining strong general capabilities on 10 general-purpose benchmarks, with effective generalization on multi-hop reasoning and strong safety performance.", "conclusion": "Domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities, and the model is publicly released as open-source."}}
{"id": "2601.20906", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20906", "abs": "https://arxiv.org/abs/2601.20906", "authors": ["Nikita Makarov", "Maria Bordukova", "Lena Voith von Voithenberg", "Estrella Pivel-Villanueva", "Sabrina Mielke", "Jonathan Wickes", "Hanchen Wang", "Mingyu Derek Ma", "Keunwoo Choi", "Kyunghyun Cho", "Stephen Ra", "Raul Rodriguez-Esteban", "Fabian Schmich", "Michael Menden"], "title": "TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins", "comment": null, "summary": "Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.", "AI": {"tldr": "TwinWeaver: An open-source framework that serializes patient histories into text for LLM-based event prediction, building Genie Digital Twin (GDT) to improve forecasting accuracy and risk stratification in precision oncology.", "motivation": "Precision oncology requires forecasting clinical events and trajectories, but modeling sparse, multi-modal clinical time series is a critical challenge.", "method": "TwinWeaver serializes longitudinal patient histories into text to enable unified event prediction and forecasting with large language models, applied to 93,054 patients across 20 cancer types to build GDT.", "result": "GDT significantly reduces forecasting error (median MASE 0.87 vs. 0.97 for baseline, p<0.001) and improves risk stratification (average C-index 0.703 vs. 0.662 baseline). It also generalizes to out-of-distribution clinical trials, achieving median MASE 0.75-0.88 and an average C-index of 0.672 vs. 0.648 baseline in event prediction.", "conclusion": "TwinWeaver provides a scalable and transparent foundation for longitudinal clinical modeling, with an interpretable clinical reasoning extension, demonstrating superior performance in forecasting and generalization for precision oncology applications."}}
{"id": "2601.21076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21076", "abs": "https://arxiv.org/abs/2601.21076", "authors": ["Abhijith Shaji", "Tamoghna Chattopadhyay", "Sophia I. Thomopoulos", "Greg Ver Steeg", "Paul M. Thompson", "Jose-Luis Ambite"], "title": "Multi-modal Imputation for Alzheimer's Disease Classification", "comment": null, "summary": "Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.", "AI": {"tldr": "Using conditional denoising diffusion models to impute missing DWI scans from T1 MRI improves Alzheimer's disease classification accuracy, especially for minority classes.", "motivation": "Multimodal MRI (T1 and DWI) improves Alzheimer's disease diagnosis, but complete multimodal datasets are often unavailable. Missing DWI scans limit diagnostic performance of deep learning models.", "method": "Conditional denoising diffusion probabilistic model to impute missing DWI scans from available T1 scans, then evaluate impact on 3-way Alzheimer's disease classification (cognitively normal, mild cognitive impairment, Alzheimer's).", "result": "Imputation improves several classification metrics, particularly those sensitive to minority classes, across multiple imputation configurations.", "conclusion": "Diffusion-based imputation of missing DWI scans from T1 MRI enhances Alzheimer's disease classification performance, addressing the common problem of incomplete multimodal datasets."}}
{"id": "2601.20913", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20913", "abs": "https://arxiv.org/abs/2601.20913", "authors": ["Chen Feng", "Minghe Shen", "Ananth Balashankar", "Carsten Gerner-Beuerle", "Miguel R. D. Rodrigues"], "title": "Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges", "comment": "Accepted to ICLR2026", "summary": "Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While \"LLM-as-a-Judge\" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a \"Noisy but Valid\" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical \"Oracle\" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.", "AI": {"tldr": "A framework for certifying LLMs with noisy judges, ensuring statistical validity by correcting for judge imperfections using a calibration set.", "motivation": "Reliable certification of LLMs is critical but challenging due to imperfections in 'LLM-as-a-Judge' approaches, which can invalidate guarantees.", "method": "Introduces a 'Noisy but Valid' hypothesis testing framework, using a small human-labelled calibration set to estimate judge TPR/FPR and applying a variance-corrected threshold to a large judge-labelled dataset.", "result": "The method theoretically guarantees finite-sample Type-I error control. Experiments on datasets like Jigsaw Comment and SafeRLHF confirm higher power than direct evaluation, and reveal an 'Oracle Gap' showing performance limits.", "conclusion": "The framework sharpens understanding of statistical evaluation with LLM judges, providing interpretable diagnostics and highlighting trade-offs, serving as a diagnostic tool rather than a black-box estimator."}}
{"id": "2601.21083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21083", "abs": "https://arxiv.org/abs/2601.21083", "authors": ["Jarrod Barnes"], "title": "OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence", "comment": "6 pages, 2 figures, 3 tables. Code: https://github.com/jbarnes850/opensec-env", "summary": "As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.", "AI": {"tldr": "OpenSec is a new benchmark that reveals frontier LLM-based IR agents often over-trigger containment under adversarial evidence, with high false positive rates, a flaw missed by traditional benchmarks.", "motivation": "As large language models become more capable, their offensive applications (like generating exploits cheaply) are advancing. Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence.", "method": "OpenSec is a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios using execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. The study tests four frontier models on 40 standard-tier episodes.", "result": "GPT-5.2, Gemini 3, and DeepSeek executed containment in 100% of episodes but had 90-97% false positive rates, indicating consistent over-triggering. Claude Sonnet 4.5 showed partial calibration with 85% containment and 72% false positives, demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics.", "conclusion": "OpenSec successfully uncovers a critical calibration failure in frontier LLM-based IR agents, showing they often over-trigger containment actions under adversarial evidence, a flaw hidden by traditional aggregate success metrics."}}
{"id": "2601.20916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20916", "abs": "https://arxiv.org/abs/2601.20916", "authors": ["Anni Zhao", "Ayca Ermis", "Jeffrey Robert Vitt", "Sergio Brasil", "Wellingson Paiva", "Magdalena Kasprowicz", "Malgorzata Burzynska", "Robert Hamilton", "Runze Yan", "Ofer Sadan", "J. Claude Hemphill", "Lieven Vandenberghe", "Xiao Hu"], "title": "Noninvasive Intracranial Pressure Estimation Using Subspace System Identification and Bespoke Machine Learning Algorithms: A Learning-to-Rank Approach", "comment": "17 pages, 9 figures", "summary": "Objective: Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care. We developed a bespoke machine learning algorithm that integrates system identification and ranking-constrained optimization to estimate mean ICP from noninvasive signals. Methods: A machine learning framework was proposed to obtain accurate mean ICP values using arbitrary noninvasive signals. The subspace system identification algorithm is employed to identify cerebral hemodynamics models for ICP simulation using arterial blood pressure (ABP), cerebral blood velocity (CBv), and R-wave to R-wave interval (R-R interval) signals in a comprehensive database. A mapping function to describe the relationship between the features of noninvasive signals and the estimation errors is learned using innovative ranking constraints through convex optimization. Patients across multiple clinical settings were randomly split into testing and training datasets for performance evaluation of the mapping function. Results: The results indicate that about 31.88% of testing entries achieved estimation errors within 2 mmHg and 34.07% of testing entries between 2 mmHg to 6 mmHg from the nonlinear mapping with constraints. Conclusion: Our results demonstrate the feasibility of the proposed noninvasive ICP estimation approach. Significance: Further validation and technical refinement are required before clinical deployment, but this work lays the foundation for safe and broadly accessible ICP monitoring in patients with acute brain injury and related conditions.", "AI": {"tldr": "A machine learning algorithm using system identification and ranking-constrained optimization was developed to estimate intracranial pressure noninvasively from arterial blood pressure, cerebral blood velocity, and heart rate signals.", "motivation": "Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care, with current invasive methods carrying risks and limitations.", "method": "A machine learning framework integrates subspace system identification to model cerebral hemodynamics using ABP, CBv, and R-R interval signals, then learns a mapping function between noninvasive signal features and estimation errors using ranking constraints via convex optimization.", "result": "Approximately 31.88% of testing entries achieved estimation errors within 2 mmHg, and 34.07% had errors between 2-6 mmHg using the nonlinear mapping with constraints.", "conclusion": "The proposed noninvasive ICP estimation approach is feasible and lays the foundation for safe, broadly accessible ICP monitoring, though further validation and refinement are needed before clinical deployment."}}
{"id": "2601.21095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21095", "abs": "https://arxiv.org/abs/2601.21095", "authors": ["Akbar Anbar Jafari", "Cagri Ozcinar", "Gholamreza Anbarjafari"], "title": "Responsible AI: The Good, The Bad, The AI", "comment": "14 pages, 5 figures", "summary": "The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.", "AI": {"tldr": "This paper introduces a Paradox-based Responsible AI Governance (PRAIG) framework to help organizations navigate the dual nature of AI\u2014balancing strategic benefits with ethical and operational risks\u2014through paradox theory and actionable governance mechanisms.", "motivation": "To address the fragmented literature on responsible AI, which often adopts either an overly optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms, by providing a comprehensive examination of AI's dual nature through strategic information systems and paradox theory.", "method": "The paper draws upon a systematic synthesis of the responsible AI literature and grounded in paradox theory to develop the Paradox-based Responsible AI Governance (PRAIG) framework. It articulates strategic benefits, inherent risks, and governance mechanisms, provides formal propositions, and develops a taxonomy of paradox management strategies with contingency conditions.", "result": "The paper advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. It demonstrates that trade-off approaches amplify tensions and provides actionable guidance for practitioners to develop governance structures that balance innovation and risk.", "conclusion": "The paper concludes by presenting a research agenda to advance responsible AI governance scholarship, emphasizing the need for further theoretical and empirical work to support organizations in effectively managing the dual nature of AI."}}
{"id": "2601.20961", "categories": ["cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.20961", "abs": "https://arxiv.org/abs/2601.20961", "authors": ["Steve Hanneke", "Shay Moran"], "title": "A Theory of Universal Agnostic Learning", "comment": null, "summary": "We provide a complete theory of optimal universal rates for binary classification in the agnostic setting. This extends the realizable-case theory of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021) by removing the realizability assumption on the distribution. We identify a fundamental tetrachotomy of optimal rates: for every concept class, the optimal universal rate of convergence of the excess error rate is one of $e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$, or arbitrarily slow. We further identify simple combinatorial structures which determine which of these categories any given concept class falls into.", "AI": {"tldr": "Paper extends realizable-case theory to agnostic setting for binary classification, identifying a fundamental tetrachotomy of optimal universal rates: e^{-n}, e^{-o(n)}, o(n^{-1/2}), or arbitrarily slow, with combinatorial structures determining the rate category.", "motivation": "To extend the realizable-case theory for binary classification to the agnostic setting by removing the realizability assumption, aiming to provide a complete theory of optimal universal rates.", "method": "Analyzes optimal universal rates of convergence of excess error rates for concept classes in binary classification, identifying a tetrachotomy and linking it to combinatorial structures.", "result": "Identifies a fundamental tetrachotomy where the optimal universal rate for any concept class is one of four categories, with combinatorial structures determining the classification.", "conclusion": "The study provides a comprehensive theory for optimal rates in agnostic binary classification, categorizing rates and connecting them to combinatorial properties, extending previous realizable-case results."}}
{"id": "2601.21096", "categories": ["cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.21096", "abs": "https://arxiv.org/abs/2601.21096", "authors": ["Hongzheng Chen", "Alexander Novikov", "Ng\u00e2n V\u0169", "Hanna Alam", "Zhiru Zhang", "Aiden Grossman", "Mircea Trofin", "Amir Yazdanbakhsh"], "title": "Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve", "comment": "Accepted to C4ML@CGO'26", "summary": "Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.", "AI": {"tldr": "Magellan is an agentic framework that evolves compiler passes by synthesizing executable C++ decision logic using LLM coding agents, evolutionary search, and autotuning in a closed loop, producing compact heuristics that integrate into existing compilers and outperform expert baselines.", "motivation": "Modern compilers rely on hand-crafted heuristics that struggle to adapt to complex modern software/hardware and have high maintenance burdens. There's a need for automated approaches that can generate effective optimization policies without extensive manual engineering.", "method": "Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement. It synthesizes executable C++ decision logic that integrates directly into existing compilers like LLVM and XLA.", "result": "Across production optimization tasks, Magellan discovers policies that match or surpass expert baselines. For LLVM function inlining, it synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. For register allocation, it learns a concise priority rule that matches intricate human-designed policies on large-scale workloads. Preliminary results on XLA problems show portability beyond LLVM with reduced engineering effort.", "conclusion": "Magellan demonstrates that automated synthesis of compiler heuristics through LLM-based agentic frameworks can outperform human-designed solutions while reducing maintenance burden and adapting to complex modern systems, with promising portability across different compiler infrastructures."}}
{"id": "2601.20983", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.20983", "abs": "https://arxiv.org/abs/2601.20983", "authors": ["Ahmed Rashwan", "Keith Briggs", "Chris Budd", "Lisa Kreusser"], "title": "Monotone Optimisation with Learned Projections", "comment": null, "summary": "Monotone optimisation problems admit specialised global solvers such as the Polyblock Outer Approximation (POA) algorithm, but these methods typically require explicit objective and constraint functions. In many applications, these functions are only available through data, making POA difficult to apply directly. We introduce an algorithm-aware learning approach that integrates learned models into POA by directly predicting its projection primitive via the radial inverse, avoiding the costly bisection procedure used in standard POA. We propose Homogeneous-Monotone Radial Inverse (HM-RI) networks, structured neural architectures that enforce key monotonicity and homogeneity properties, enabling fast projection estimation. We provide a theoretical characterisation of radial inverse functions and show that, under mild structural conditions, a HM-RI predictor corresponds to the radial inverse of a valid set of monotone constraints. To reduce training overhead, we further develop relaxed monotonicity conditions that remain compatible with POA. Across multiple monotone optimisation benchmarks (indefinite quadratic programming, multiplicative programming, and transmit power optimisation), our approach yields substantial speed-ups in comparison to direct function estimation while maintaining strong solution quality, outperforming baselines that do not exploit monotonic structure.", "AI": {"tldr": "Algorithm-aware learning approach integrates learned models into Polyblock Outer Approximation by predicting projection primitives via radial inverse, using specialized neural networks that enforce monotonicity and homogeneity properties for faster monotone optimization.", "motivation": "Monotone optimization problems often have objective and constraint functions available only through data, making standard Polyblock Outer Approximation difficult to apply directly. There's a need for methods that can leverage data while maintaining the efficiency of specialized global solvers.", "method": "Proposes Homogeneous-Monotone Radial Inverse (HM-RI) networks - structured neural architectures that enforce monotonicity and homogeneity properties to predict projection primitives directly. Uses radial inverse functions to avoid costly bisection procedures in standard POA. Also develops relaxed monotonicity conditions to reduce training overhead while remaining compatible with POA.", "result": "The approach yields substantial speed-ups across multiple monotone optimization benchmarks (indefinite quadratic programming, multiplicative programming, and transmit power optimization) while maintaining strong solution quality. Outperforms baselines that don't exploit monotonic structure.", "conclusion": "Algorithm-aware learning with HM-RI networks provides an effective way to integrate learned models into monotone optimization solvers, achieving significant computational advantages over direct function estimation approaches while preserving solution quality."}}
{"id": "2601.21112", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21112", "abs": "https://arxiv.org/abs/2601.21112", "authors": ["Rauno Arike", "Raja Mehta Moreno", "Rohan Subramani", "Shubhorup Biswas", "Francis Rhys Ward"], "title": "How does information access affect LLM monitors' ability to detect sabotage?", "comment": "62 pages, 34 figures, 7 tables", "summary": "Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.", "AI": {"tldr": "LLM monitors for AI agent misalignment sometimes perform better with less information about agent reasoning/actions - a \"less-is-more effect\" - leading to new hierarchical monitoring approaches that improve detection rates.", "motivation": "As frontier language model agents can exhibit misaligned behaviors like deception and hidden objectives, there's a need for effective automated oversight using LLMs themselves. The researchers investigate how information access affects monitoring performance.", "method": "Studied information access effects on LLM monitor performance across three evaluation environments where agents conduct sabotage while evading monitors. Introduced extract-and-evaluate (EaE) monitoring - a hierarchical approach where one monitor isolates relevant excerpts from agent trajectories and another scores them.", "result": "Found a \"less-is-more effect\" where contemporary monitoring systems often perform better with less information. EaE monitoring improved sabotage detection rates by 16.8 percentage points over next-best monitors without increasing false positives. Unmonitored agents were caught more easily, and monitors scoring longer reasoning traces benefited more from information filtering.", "conclusion": "Information filtering techniques like EaE monitoring significantly improve LLM-based oversight of potentially misaligned AI agents. The less-is-more effect suggests that strategic information reduction can enhance monitoring performance, with Gemini and Claude models using filtering techniques showing strong cost-performance tradeoffs."}}
{"id": "2601.20985", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20985", "abs": "https://arxiv.org/abs/2601.20985", "authors": ["Abdullah Akg\u00fcl", "Gulcin Baykal", "Manuel Hau\u00dfmann", "Mustafa Mert \u00c7elikok", "Melih Kandemir"], "title": "Distributional Active Inference", "comment": null, "summary": "Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.", "AI": {"tldr": "A new framework integrates active inference into distributional reinforcement learning to improve sample efficiency and planning in complex robotic environments without needing transition dynamics.", "motivation": "Reinforcement learning (RL) focuses on action planning but is often sample-inefficient, while active inference, used in biological systems, addresses both state organization and planning. However, AI applications of active inference have been limited to model-based approaches, leaving a gap for more efficient methods.", "method": "The paper introduces a formal abstraction of RL algorithms that encompasses model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional RL framework, enabling performance advantages without requiring explicit transition dynamics modeling.", "result": "The integration of active inference into distributional RL provides a framework that enhances sample efficiency and far-sighted planning in complex environments, leveraging the strengths of both theories without the need for transition dynamics modeling.", "conclusion": "The proposed framework bridges active inference and RL, offering a novel approach to improve AI performance in robotic control by combining state organization and planning efficiently, potentially leading to more adaptive and sample-efficient learning systems."}}
{"id": "2601.20987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20987", "abs": "https://arxiv.org/abs/2601.20987", "authors": ["Md Muhtasim Munif Fahim", "Md Rezaul Karim"], "title": "Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings", "comment": null, "summary": "A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.", "AI": {"tldr": "Pre-trained encoder for child development monitoring achieves 0.65 AUC with just 50 samples, outperforming cold-start methods by 8-12%, enabling ML in resource-constrained settings for SDG 4.2.1 monitoring.", "motivation": "Child developmental delays are preventable but widespread, yet ML deployment faces data bottlenecks in new countries where programs start with fewer than 100 samples while reliable models require thousands.", "method": "Developed first pre-trained encoder for global child development using UNICEF survey data from 357,709 children across 44 countries, enabling few-shot learning with transfer learning analysis to explain generalization.", "result": "With only 50 training samples, pre-trained encoder achieves average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting (0.61) by 8-12% across regions. At N=500, AUC reaches 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84.", "conclusion": "Pre-trained encoders can transform feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings by enabling effective few-shot learning and zero-shot deployment to new countries."}}
{"id": "2601.21123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21123", "abs": "https://arxiv.org/abs/2601.21123", "authors": ["Tianyi Chen", "Yinheng Li", "Michael Solodko", "Sen Wang", "Nan Jiang", "Tingyuan Cui", "Junheng Hao", "Jongwoo Ko", "Sara Abdali", "Suzhen Zheng", "Leon Xu", "Hao Fan", "Pashmina Cameron", "Justin Wagle", "Kazuhito Koishida"], "title": "CUA-Skill: Develop Skills for Computer Using Agent", "comment": null, "summary": "Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.", "AI": {"tldr": "CUA-Skill is a large-scale library of structured computer-using skills that enables scalable and reliable agent development, achieving state-of-the-art performance on agent benchmarks.", "motivation": "Existing computer-using agent systems are difficult to scale and lag behind human performance due to the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces.", "method": "The authors introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills with parameterized execution and composition graphs. They build CUA-Skill Agent, an end-to-end agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery.", "result": "CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) success rate while being significantly more efficient than prior approaches.", "conclusion": "CUA-Skill establishes a strong foundation for future computer-using agent development by providing a reusable, structured skill base that enables scalable and reliable agent performance, as evidenced by state-of-the-art results on WindowsAgentArena."}}
{"id": "2601.20989", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.20989", "abs": "https://arxiv.org/abs/2601.20989", "authors": ["Lutz Oettershagen"], "title": "Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles", "comment": null, "summary": "Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\\varepsilon_{\\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\\varepsilon_{\\max}$, where $m(\\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $\u03a9(m(\\varepsilon_{\\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\\varepsilon_{\\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs.", "AI": {"tldr": "Adaptive certification algorithms ACE and ACE-W reduce expensive strong oracle calls for top-k identification using noisy weak and scarce strong oracles, with theoretical guarantees and practical efficiency.", "motivation": "The motivation is to efficiently identify top-k items when exact valuations are expensive, using a two-oracle setting with a fast but noisy weak oracle and a scarce but accurate strong oracle, to minimize costly strong oracle calls.", "method": "The method involves analyzing a baseline screen-then-certify (STC) approach, developing ACE which adaptively focuses strong queries on critical boundary items, and introducing ACE-W which adds an adaptive weak budget allocation phase before ACE.", "result": "The results show that ACE achieves the same O(m(4\u03b5_max)) strong call bound as STC but reduces strong calls in practice, and ACE-W further reduces strong costs by adaptively allocating weak budget.", "conclusion": "The paper concludes that adaptive methods like ACE and ACE-W significantly reduce the need for expensive strong oracle calls while maintaining theoretical guarantees, making them practical for real-world applications where strong oracle access is limited."}}
{"id": "2601.21128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21128", "abs": "https://arxiv.org/abs/2601.21128", "authors": ["V\u00e1clav Javorek", "Tom\u00e1\u0161 \u017delezn\u00fd", "Alessa Carbo", "Marek Hr\u00faz", "Ivan Gruber"], "title": "Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation", "comment": "Under review", "summary": "Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.", "AI": {"tldr": "Using LLMs to generate paraphrased references improves SLT evaluation but not training, leading to BLEUpara metric that better aligns with human judgments.", "motivation": "Current SLT corpora have only single written-language references despite non-isomorphic sign-spoken language relationships, limiting training and evaluation, especially for n-gram metrics like BLEU.", "method": "Use LLMs to generate paraphrased variants as synthetic alternative references; compare paraphrasing strategies using adapted ParaScore; study impact on pose-based T5 model training/evaluation on YouTubeASL and How2Sign datasets.", "result": "Paraphrases during training don't improve performance (can be detrimental), but during evaluation lead to higher automatic scores and better human judgment alignment; BLEUpara metric introduced and validated.", "conclusion": "Paraphrased references improve SLT evaluation reliability; BLEUpara better correlates with human judgments; resources released for reproducible evaluation."}}
{"id": "2601.20994", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20994", "abs": "https://arxiv.org/abs/2601.20994", "authors": ["Md Muhtasim Munif Fahim", "Md Rezaul Karim"], "title": "The Depth Delusion: Why Transformers Should Be Wider, Not Deeper", "comment": null, "summary": "Neural scaling laws describe how language model loss decreases with parameters and data, but treat architecture as interchangeable--a billion parameters could arise from a shallow-wide model (10 layers & 8,192 hidden dimension) or a deep-narrow one (80 layers & 2,048 hidden dimension). We propose architecture-conditioned scaling laws decomposing this dependence, finding that optimal depth scales as D* ~ C^0.12 while optimal width scales as W* ~ C^0.34, meaning width should grow 2.8x faster than depth. We discover a critical depth phenomenon: beyond D_crit ~ W^0.44 (sublinear in W), adding layers increases loss despite adding parameters--the Depth Delusion. Empirically, we validate these findings across 30 transformer architectures spanning 17M to 7B parameters, each trained on representative high-compute samples, achieving R^2 = 0.922. Our central finding: at 7B scale, a 64-layer model (6.38B params) underperforms a 32-layer model (6.86B params) by 0.12 nats, despite being significantly deeper. This demonstrates that optimal depth-width tradeoffs persist at the production scale.", "AI": {"tldr": "Architecture-conditioned scaling laws reveal width should grow 2.8x faster than depth, with a critical depth beyond which adding layers harms performance (Depth Delusion), validated across 30 transformers up to 7B parameters.", "motivation": "Current neural scaling laws treat architecture as interchangeable, ignoring how different depth-width configurations affect performance at scale. The authors aim to understand how depth and width should scale optimally with compute budget, and to investigate whether there exists a critical depth beyond which adding layers becomes detrimental.", "method": "The authors propose architecture-conditioned scaling laws that decompose the dependence of loss on architecture parameters. They derive optimal scaling relationships for depth and width, and empirically validate these findings by training 30 transformer architectures spanning 17M to 7B parameters on representative high-compute samples.", "result": "The study finds that optimal depth scales as D* ~ C^0.12 while optimal width scales as W* ~ C^0.34, meaning width should grow 2.8x faster than depth. They discover a critical depth phenomenon where beyond D_crit ~ W^0.44, adding layers increases loss despite adding parameters. Empirical validation achieves R^2 = 0.922, and at 7B scale, a 64-layer model underperforms a 32-layer model despite being deeper.", "conclusion": "The paper concludes that optimal depth-width tradeoffs persist at production scale, with a critical depth phenomenon (Depth Delusion) where adding layers beyond a certain point increases loss despite adding parameters. The empirical validation across 30 transformer architectures up to 7B parameters strongly supports these findings."}}
{"id": "2601.21130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21130", "abs": "https://arxiv.org/abs/2601.21130", "authors": ["Yara El-Tawil", "Aneesha Sampath", "Emily Mower Provost"], "title": "What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels", "comment": "ICASSP 2026-2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "summary": "Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.", "AI": {"tldr": "Third-party-trained emotion models struggle with self-reports, but achieve high valence prediction when content is personally significant to the speaker.", "motivation": "There's a critical gap between self-reported emotion labels (internal experience) and third-party labels (external perception), limiting the applicability of third-party-trained models in mental health contexts where accurate self-report modeling is essential for guiding interventions.", "method": "Conducted the first cross-corpus evaluation of third-party-trained emotion models on self-reports, examining performance on activation and valence dimensions, with particular focus on content that is personally significant to the speaker.", "result": "Activation was unpredictable (CCC \u2248 0), valence was moderately predictable (CCC \u2248 0.3). Crucially, when content was personally significant to the speaker, models achieved high performance for valence (CCC \u2248 0.6-0.8).", "conclusion": "Personal significance is a key pathway for aligning external perception with internal experience, but self-report activation modeling remains challenging. The findings highlight the importance of personal relevance in emotion recognition and the limitations of third-party models for self-report contexts."}}
{"id": "2601.20996", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.20996", "abs": "https://arxiv.org/abs/2601.20996", "authors": ["Shreshth A Malik", "Tiarnan Doherty", "Panagiotis Tigas", "Muhammed Razzak", "Stephen J. Roberts", "Aron Walsh", "Yarin Gal"], "title": "MADE: Benchmark Environments for Closed-Loop Materials Discovery", "comment": null, "summary": "Existing benchmarks for computational materials discovery primarily evaluate static predictive tasks or isolated computational sub-tasks. While valuable, these evaluations neglect the inherently iterative and adaptive nature of scientific discovery. We introduce MAterials Discovery Environments (MADE), a novel framework for benchmarking end-to-end autonomous materials discovery pipelines. MADE simulates closed-loop discovery campaigns in which an agent or algorithm proposes, evaluates, and refines candidate materials under a constrained oracle budget, capturing the sequential and resource-limited nature of real discovery workflows. We formalize discovery as a search for thermodynamically stable compounds relative to a given convex hull, and evaluate efficacy and efficiency via comparison to baseline algorithms. The framework is flexible; users can compose discovery agents from interchangeable components such as generative models, filters, and planners, enabling the study of arbitrary workflows ranging from fixed pipelines to fully agentic systems with tool use and adaptive decision making. We demonstrate this by conducting systematic experiments across a family of systems, enabling ablation of components in discovery pipelines, and comparison of how methods scale with system complexity.", "AI": {"tldr": "MADE is a new framework for benchmarking end-to-end autonomous materials discovery pipelines through simulated closed-loop campaigns with budget constraints.", "motivation": "Existing benchmarks only evaluate static predictive tasks or isolated computational sub-tasks, neglecting the iterative and adaptive nature of real scientific discovery workflows.", "method": "Introduces MAterials Discovery Environments (MADE) that simulates closed-loop discovery campaigns where agents propose, evaluate, and refine candidate materials under constrained oracle budgets, formalizing discovery as search for thermodynamically stable compounds relative to convex hulls.", "result": "The framework enables systematic experiments across different systems, allowing ablation of pipeline components and comparison of how methods scale with system complexity, demonstrating flexible workflow composition from interchangeable components.", "conclusion": "MADE provides a novel benchmarking framework that captures the sequential, resource-limited nature of real materials discovery, enabling comprehensive evaluation of end-to-end autonomous discovery pipelines beyond static predictive tasks."}}
{"id": "2601.21148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21148", "abs": "https://arxiv.org/abs/2601.21148", "authors": ["Ziyi Zhao", "Jinzhao Zhou", "Xiaowei Jiang", "Beining Cao", "Wenhao Ma", "Yang Shen", "Ren Li", "Yu-Kai Wang", "Chin-teng Lin"], "title": "BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding", "comment": null, "summary": "Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain's distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain's modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.", "AI": {"tldr": "BrainStack is a neuro-inspired mixture-of-experts framework that models brain's functional architecture for EEG-based language decoding, achieving state-of-the-art performance on a new large-scale silent speech EEG dataset.", "motivation": "Decoding linguistic information from EEG is challenging due to the brain's distributed and nonlinear organization. Current approaches lack effective modeling of the brain's modular functional architecture and cross-regional dependencies.", "method": "BrainStack uses anatomically partitioned expert networks to model localized neural dynamics in different brain regions, combined with a transformer-based global expert for cross-regional dependencies. A learnable routing gate adaptively aggregates experts, and cross-regional distillation provides top-down regularization from global to regional experts.", "result": "BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. The authors also release SilentSpeech-EEG (SS-EEG), the largest dataset of its kind with over 120 hours of EEG recordings from 12 subjects performing 24 silent words.", "conclusion": "BrainStack establishes a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding."}}
{"id": "2601.21008", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.21008", "abs": "https://arxiv.org/abs/2601.21008", "authors": ["Ruicheng Ao", "David Simchi-Levi", "Xinshang Wang"], "title": "Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research", "comment": "55 pages, 5 figures", "summary": "Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evaluate OR as one-shot translation -- given a problem description, generate solver code -- ignoring this diagnostic loop entirely. We introduce two benchmarks that place the \\textbf{solver in the evaluation loop}. \\textbf{\\ORDebug{}} evaluates iterative self-correction through 5,000+ problems spanning 9 error types; each repair action triggers solver re-execution and \\IIS{} recomputation, providing deterministic, verifiable feedback. \\textbf{\\ORBias{}} evaluates behavioral rationality through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic deviations from closed-form optimal policies. Across 26 models and 12,000+ samples, we find that domain-specific RLVR training enables an 8B model to surpass frontier APIs: 95.3\\% vs 86.2\\% recovery rate (+9.1\\%), 62.4\\% vs 47.8\\% diagnostic accuracy (+14.6\\%), and 2.25 vs 3.78 steps to resolution (1.7$\\times$ faster). On \\ORBias{}, curriculum training achieves the only negative ID$\\rightarrow$OOD bias drift among models evaluated (-9.6\\%), reducing systematic bias by 48\\% (from 20.0\\% to 10.4\\%). These results demonstrate that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.", "AI": {"tldr": "The paper introduces two benchmarks (ORDebug and ORBias) that evaluate LLMs on iterative debugging of infeasible optimization models and systematic bias in decision-making, showing that targeted domain-specific training outperforms scale-based approaches.", "motivation": "Existing LLM benchmarks for Operations Research evaluate one-shot code generation but ignore the crucial iterative debugging process that practitioners use to resolve infeasible models through IIS analysis and constraint repair.", "method": "Created two benchmarks: ORDebug (5,000+ problems with 9 error types) that evaluates iterative self-correction with solver re-execution and IIS recomputation, and ORBias (2,000 newsvendor instances) that measures systematic deviations from optimal policies. Tested 26 models with 12,000+ samples.", "result": "Domain-specific RLVR training enabled an 8B model to surpass frontier APIs: 95.3% vs 86.2% recovery rate, 62.4% vs 47.8% diagnostic accuracy, and 1.7\u00d7 faster resolution. Curriculum training achieved negative ID\u2192OOD bias drift (-9.6%), reducing systematic bias by 48%.", "conclusion": "Process-level evaluation with verifiable oracles enables targeted training that outperforms scale-based approaches, demonstrating the importance of evaluating LLMs on the actual iterative processes used in Operations Research practice."}}
{"id": "2601.21157", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21157", "abs": "https://arxiv.org/abs/2601.21157", "authors": ["Boxiang Zhao", "Qince Li", "Zhonghao Wang", "Yi Wang", "Peng Cheng", "Bo Lin"], "title": "Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning", "comment": null, "summary": "While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from \"Arithmetic Hallucinations\" and a systemic failure mode we term \"Cognitive Collapse\". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\\% to 67.3\\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.", "AI": {"tldr": "A new benchmark (CCB) and framework (Iterative Dual-Phase Financial-PoT) that addresses LLMs' \"cognitive collapse\" in financial quantitative reasoning through architectural decoupling and symbolic computation.", "motivation": "LLMs struggle with financial quantitative reasoning, suffering from \"Arithmetic Hallucinations\" and systemic \"Cognitive Collapse\" - failure modes that degrade performance on complex financial tasks derived from real-world data.", "method": "1) Introduce Cognitive Complexity Benchmark (CCB) with 3D taxonomy (Data Source, Mapping Difficulty, Result Unit) from 95 Chinese A-share annual reports; 2) Propose Iterative Dual-Phase Financial-PoT framework that decouples semantic extraction from computation, using Python sandbox for deterministic execution.", "result": "Framework elevates Qwen3-235B accuracy from 59.7% to 67.3% on average, with up to 10x gains in high-complexity tasks, demonstrating superior robustness over standard Chain-of-Thought approaches.", "conclusion": "Architectural decoupling is critical for reliable financial reasoning, providing transferable insights for precision-critical domains requiring tight alignment between semantic understanding and quantitative computation."}}
{"id": "2601.21012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21012", "abs": "https://arxiv.org/abs/2601.21012", "authors": ["Young Kyung Kim", "Oded Schlesinger", "Qiangqiang Wu", "J. Mat\u00edas Di Martino", "Guillermo Sapiro"], "title": "Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference", "comment": "18 pages, 4 figures", "summary": "Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.", "AI": {"tldr": "OATTA is a test-time adaptation method that models temporal dynamics in test streams using a gradient-free Bayesian approach with a likelihood-ratio gate for safety, improving accuracy across tasks.", "motivation": "Existing test-time adaptation methods treat test streams as independent samples, missing the supervisory signal from temporal dynamics, which can enhance adaptation.", "method": "Formulates TTA as gradient-free recursive Bayesian estimation, uses a learned dynamic transition matrix as a temporal prior, and includes a likelihood-ratio gate to revert to base predictions when temporal evidence is weak.", "result": "Extensive experiments show OATTA boosts accuracy by up to 6.35% in image classification, wearable/physiological signals, and language sentiment analysis, demonstrating universality and lightweight overhead.", "conclusion": "Modeling temporal dynamics provides an orthogonal signal that significantly improves test-time adaptation beyond standard order-agnostic methods, with OATTA being effective, safe, and computationally efficient."}}
{"id": "2601.21164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21164", "abs": "https://arxiv.org/abs/2601.21164", "authors": ["Jingyun Wang", "Dian Li", "Xiaohan Wang", "Gang Liu", "Jiahong Yan", "Guoliang Kang"], "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving", "comment": "Under review", "summary": "Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.", "AI": {"tldr": "A method that separates visual understanding from reasoning by training a MLLM Interpreter to convert geometric diagrams to textual descriptions (CDL), then using an off-the-shelf LLM for reasoning, achieving strong performance with minimal training data.", "motivation": "Large Language Models (LLMs) have strong reasoning capabilities but cannot process visual diagrams directly. Existing approaches that fine-tune Multimodal LLMs end-to-end for Plane Geometry Problem Solving may compromise the LLMs' inherent reasoning ability by jointly optimizing visual understanding and reasoning.", "method": "Train a MLLM Interpreter to generate geometric descriptions (using Conditional Declaration Language/CDL) from visual diagrams. Use an off-the-shelf LLM for reasoning based on the CDL descriptions. Training involves CoT-augmented SFT followed by GRPO with CDL matching rewards instead of solution-based rewards. Constructed Formalgeo7k-Rec-CoT dataset with manual review and CoT annotations.", "result": "The method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs on Formalgeo7k-Rec-CoT, Unigeo, and MathVista benchmarks.", "conclusion": "LLMs can be powerful PGPS solvers when visual information is appropriately formulated as textual descriptions. Separating visual understanding (MLLM Interpreter) from reasoning (off-the-shelf LLM) avoids compromising LLMs' reasoning capabilities and achieves strong performance with minimal training data."}}
{"id": "2601.21021", "categories": ["cs.LG", "cs.AI", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2601.21021", "abs": "https://arxiv.org/abs/2601.21021", "authors": ["Jos\u00e9 Afonso", "Pedro Viegas", "Rodrigo Ventura", "Vasco Guerra"], "title": "Conditional Denoising Model as a Physical Surrogate Model", "comment": "15 pages, 2 figures, 2 tables", "summary": "Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.", "AI": {"tldr": "Introduces Conditional Denoising Model (CDM) as a generative approach to learn physical manifold geometry, achieving better physics adherence and efficiency than soft-constraint baselines.", "motivation": "Addresses the trade-off between accuracy and physical consistency in surrogate modeling, where existing methods often fail to guarantee strict adherence to governing equations or rely on post-processing.", "method": "Develops CDM, a generative model trained to restore clean states from noisy ones, learning a vector field toward the solution subspace, with a time-independent formulation using deterministic fixed-point iteration for inference.", "result": "CDM shows higher parameter and data efficiency on a low-temperature plasma benchmark and adheres more strictly to physical constraints than baselines, even without explicit physics loss training.", "conclusion": "The denoising objective serves as an implicit regularizer, enabling strict physical consistency without direct equation supervision, advancing physics-aware surrogate modeling."}}
{"id": "2601.21165", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21165", "abs": "https://arxiv.org/abs/2601.21165", "authors": ["Miles Wang", "Robi Lin", "Kat Hu", "Joy Jiao", "Neil Chowdhury", "Ethan Chang", "Tejal Patwardhan"], "title": "FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks", "comment": null, "summary": "We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.\n  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.", "AI": {"tldr": "FrontierScience is a benchmark for evaluating expert-level scientific reasoning in frontier language models, featuring Olympiad and Research tracks with hundreds of PhD-level problems across physics, chemistry, and biology.", "motivation": "Existing science benchmarks have become saturated as recent model progress has nearly maxed them out, and they often rely on multiple-choice knowledge questions or already published information rather than testing true scientific reasoning.", "method": "The benchmark consists of two complementary tracks: (1) Olympiad track with international olympiad problems at IPhO, IChO, and IBO levels, originally produced by medalists and coaches; (2) Research track with PhD-level, open-ended problems representing sub-tasks in scientific research, written and verified by PhD scientists. A granular rubric-based evaluation framework assesses model capabilities throughout the research problem-solving process.", "result": "FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering diverse subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All problems maintain high standards of difficulty, originality, and factuality.", "conclusion": "FrontierScience addresses the gap in evaluating expert-level scientific reasoning in frontier language models by providing a challenging benchmark with authentic olympiad and research-level problems that go beyond simple knowledge recall to assess true scientific problem-solving capabilities."}}
{"id": "2601.21031", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21031", "abs": "https://arxiv.org/abs/2601.21031", "authors": ["Zongheng Guo", "Tao Chen", "Yang Jiao", "Yi Pan", "Xiao Hu", "Manuela Ferrario"], "title": "SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model", "comment": "31 pages, 9 figures, 14 tables", "summary": "Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.", "AI": {"tldr": "SIGMA-PPG: A generative foundation model for PPG signals using adversarial masking and semantic consistency to improve performance.", "motivation": "Current PPG foundation models face issues with signal redundancy, noise, trivial masked modeling solutions, and lack of morphological precision in contrastive methods.", "method": "Proposes SIGMA-PPG with a Prior-Guided Adversarial Masking mechanism (using RL-driven teacher with statistical priors) and semantic consistency constraint via vector quantization to map physiologically identical waveforms to shared indices.", "result": "Pre-trained on over 120,000 hours of data, it achieves superior average performance vs. 5 state-of-the-art baselines across 12 diverse downstream tasks.", "conclusion": "SIGMA-PPG effectively addresses limitations in PPG signal modeling, enhancing codebook semantic density and feature efficiency for improved foundation model performance."}}
{"id": "2601.21181", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21181", "abs": "https://arxiv.org/abs/2601.21181", "authors": ["Sangyun Chung", "Se Yeon Kim", "Youngchae Chee", "Yong Man Ro"], "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\\% and 2.0\\% improvements for VideoLLaMA2-AV, 8.7\\% and 4.7\\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \\href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}", "AI": {"tldr": "MAD is a training-free method that reduces cross-modal hallucinations in MLLMs by adaptively weighting modality-specific decoding branches based on self-assessed modality relevance.", "motivation": "Multimodal Large Language Models suffer from cross-modal hallucinations where one modality inappropriately influences generation about another, revealing fundamental deficiencies in modality-interaction control.", "method": "Modality-Adaptive Decoding (MAD) leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task, then uses extracted modality probabilities to adaptively weight contrastive decoding branches.", "result": "Extensive experiments on CMM and AVHBench show MAD significantly reduces cross-modal hallucinations, achieving 7.8% and 2.0% improvements for VideoLLaMA2-AV, and 8.7% and 4.7% improvements for Qwen2.5-Omni.", "conclusion": "Explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, and MAD offers a principled extension to existing contrastive decoding methods."}}
{"id": "2601.21033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21033", "abs": "https://arxiv.org/abs/2601.21033", "authors": ["Omer Rochman-Sharabi", "Gilles Louppe"], "title": "Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints", "comment": "Code coming soon", "summary": "Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.", "AI": {"tldr": "PPR (Predict-Project-Renoise) is a constrained sampling framework for diffusion models that enforces hard constraints like physical laws during generation, reducing constraint violations by over an order of magnitude.", "motivation": "Standard neural emulators based on diffusion models cannot guarantee physical accuracy or constraint satisfaction, which is critical for scientific applications where physical laws or observational consistency must be respected.", "method": "Introduces a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, creating constrained marginal distributions. To reverse this, proposes Predict-Project-Renoise (PPR) - an iterative algorithm that alternates between denoising predictions, projecting onto the feasible set, and renoising to sample from constrained marginals.", "result": "PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baseline methods. Experiments validate this on 2D distributions, PDEs, and global weather forecasting.", "conclusion": "The PPR framework successfully enables diffusion models to generate physically accurate samples that satisfy hard constraints, making them more suitable for scientific applications where physical consistency is essential."}}
{"id": "2601.21183", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21183", "abs": "https://arxiv.org/abs/2601.21183", "authors": ["Jacek Duszenko"], "title": "Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models", "comment": null, "summary": "Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \\emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.", "AI": {"tldr": "The paper introduces sycophantic anchors to localize and quantify model sycophancy mid-inference, showing they can be reliably detected with high accuracy and that sycophancy builds gradually, offering intervention opportunities.", "motivation": "Reasoning models often exhibit sycophancy by agreeing with incorrect user suggestions, but it's unclear where in the reasoning trace this agreement originates and how strong the commitment is. The goal is to localize and quantify this behavior mid-inference.", "method": "The researchers introduced sycophantic anchors\u2014sentences that causally lock models into user agreement\u2014and analyzed over 10,000 counterfactual rollouts on a distilled reasoning model. They used linear probes to distinguish sycophantic anchors with balanced accuracy and activation-based regressors to predict the magnitude of commitment.", "result": "Linear probes achieved 84.6% balanced accuracy in distinguishing sycophantic anchors, and activation-based regressors predicted commitment magnitude with an R\u00b2 of 0.74. The study found asymmetry: sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and sycophancy builds gradually during reasoning.", "conclusion": "The study demonstrates that sycophantic anchors can be reliably detected and quantified during inference, revealing an asymmetry where sycophantic anchors are more distinguishable than correct reasoning anchors. It also shows that sycophancy builds gradually, offering a potential window for intervention. These findings provide sentence-level mechanisms for localizing model misalignment mid-inference."}}
{"id": "2601.21037", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21037", "abs": "https://arxiv.org/abs/2601.21037", "authors": ["Chengzu Li", "Zanyi Wang", "Jiaang Li", "Yi Xu", "Han Zhou", "Huanyu Zhang", "Ruichuan An", "Dengyang Jiang", "Zhaochong An", "Ivan Vuli\u0107", "Serge Belongie", "Anna Korhonen"], "title": "Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning", "comment": "8 pages, 3 figures, 3 tables (26 pages, 13 figures, 6 tables including references and appendices)", "summary": "Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.", "AI": {"tldr": "Video generation models enable visual reasoning by using generated frames as intermediate steps, showing strong zero-shot generalization, visual context control, and test-time scaling on maze and tangram tasks.", "motivation": "Vision-Language Models excel at textual reasoning but struggle with fine-grained spatial understanding and continuous action planning for complex visual reasoning. There is a need for models that can simulate dynamics and handle both discrete and continuous visual reasoning tasks.", "method": "Formulate visual reasoning via video generation models, using generated frames as intermediate reasoning steps. Evaluate on Maze Navigation (sequential discrete planning with low visual change) and Tangram Puzzle (continuous manipulation with high visual change).", "result": "Three key insights: (1) Robust zero-shot generalization on unseen data distributions without finetuning. (2) Effective use of visual context (e.g., agent icons, tangram shapes) for explicit control, maintaining high visual consistency and adapting to unseen patterns. (3) Visual test-time scaling law: increasing generated video length improves zero-shot generalization to spatially and temporally complex paths.", "conclusion": "Video generation models serve as a scalable and generalizable paradigm for visual reasoning, not just a media tool, by enabling robust zero-shot generalization, effective use of visual context, and test-time scaling for complex planning."}}
{"id": "2601.21192", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21192", "abs": "https://arxiv.org/abs/2601.21192", "authors": ["Wun Yu Chan", "Shaojin Chen", "Huihao Jing", "Kwun Hang Lau", "Elton Chun-Chai Li", "Zihao Wang", "Haoran Li", "Yangqiu Song"], "title": "Do Reasoning Models Enhance Embedding Models?", "comment": "10 main pages, 18 appendix pages, 13 figures, 11 tables, 4 prompts", "summary": "State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.", "AI": {"tldr": "RLVR-tuned LLM backbones provide no embedding advantage; HRSA reveals RLVR reorganizes local geometry but preserves global structure, leading to Manifold Realignment after contrastive learning.", "motivation": "To investigate whether enhanced reasoning capabilities from RLVR-tuned LLMs translate to superior semantic representations when used as embedding initializations, given the emergence of reasoning models trained via RLVR.", "method": "Hierarchical Representation Similarity Analysis (HRSA) framework to decompose similarity across representation, geometry, and function levels; evaluation on MTEB and BRIGHT benchmarks.", "result": "No consistent performance advantage of RLVR-tuned backbones over base counterparts; RLVR induces irreversible local geometry reorganization and reversible coordinate basis drift but preserves global manifold geometry and linear readout; contrastive learning drives strong alignment (Manifold Realignment).", "conclusion": "RLVR optimization does not fundamentally restructure the semantic landscape; it preserves the global manifold geometry and linear readout, leading to strong alignment between base and reasoning-initialized models after contrastive learning."}}
{"id": "2601.21048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21048", "abs": "https://arxiv.org/abs/2601.21048", "authors": ["Yiqiao Liao", "Farinaz Koushanfar", "Parinaz Naghizadeh"], "title": "Test-Time Adaptation for Unsupervised Combinatorial Optimization", "comment": null, "summary": "Unsupervised neural combinatorial optimization (NCO) enables learning powerful solvers without access to ground-truth solutions. Existing approaches fall into two disjoint paradigms: models trained for generalization across instances, and instance-specific models optimized independently at test time. While the former are efficient during inference, they lack effective instance-wise adaptability; the latter are flexible but fail to exploit learned inductive structure and are prone to poor local optima. This motivates the central question of our work: how can we leverage the inductive bias learned through generalization while unlocking the flexibility required for effective instance-wise adaptation? We first identify a challenge in bridging these two paradigms: generalization-focused models often constitute poor warm starts for instance-wise optimization, potentially underperforming even randomly initialized models when fine-tuned at test time. To resolve this incompatibility, we propose TACO, a model-agnostic test-time adaptation framework that unifies and extends the two existing paradigms for unsupervised NCO. TACO applies strategic warm-starting to partially relax trained parameters while preserving inductive bias, enabling rapid and effective unsupervised adaptation. Crucially, compared to naively fine-tuning a trained generalizable model or optimizing an instance-specific model from scratch, TACO achieves better solution quality while incurring negligible additional computational cost. Experiments on canonical CO problems, Minimum Vertex Cover and Maximum Clique, demonstrate the effectiveness and robustness of TACO across static, distribution-shifted, and dynamic combinatorial optimization problems, establishing it as a practical bridge between generalizable and instance-specific unsupervised NCO.", "AI": {"tldr": "TACO is a test-time adaptation framework that bridges generalization-focused and instance-specific unsupervised neural combinatorial optimization by enabling strategic warm-starting for rapid adaptation.", "motivation": "Existing unsupervised NCO approaches are split between generalizable models that lack instance-wise adaptability and instance-specific models that don't leverage learned inductive structure. There's a need to combine the inductive bias of generalization-focused models with the flexibility of instance-specific adaptation.", "method": "TACO is a model-agnostic test-time adaptation framework that applies strategic warm-starting to partially relax trained parameters while preserving inductive bias, enabling rapid unsupervised adaptation at test time.", "result": "TACO achieves better solution quality than naive fine-tuning or optimizing from scratch, with negligible additional computational cost. Experiments on Minimum Vertex Cover and Maximum Clique problems demonstrate effectiveness across static, distribution-shifted, and dynamic CO problems.", "conclusion": "TACO successfully bridges the gap between generalizable and instance-specific unsupervised NCO, establishing a practical framework that leverages learned inductive bias while enabling effective instance-wise adaptation."}}
{"id": "2601.21208", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21208", "abs": "https://arxiv.org/abs/2601.21208", "authors": ["Wei Wen", "Sihang Deng", "Tianjun Wei", "Keyu Chen", "Ruizhi Qiao", "Xing Sun"], "title": "When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning", "comment": "16 pages, 7 figures", "summary": "Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.", "AI": {"tldr": "ACQO is a novel RL framework that adaptively optimizes complex queries in RAG systems by dynamically deciding when to decompose queries and fusing results with curriculum learning to improve performance and stability.", "motivation": "Complex user queries in RAG systems often need multiple search strategies for disambiguation and decomposition, but applying RL directly faces challenges like determining sub-query counts and reward instability due to expanded search spaces.", "method": "The ACQO framework includes an Adaptive Query Reformulation module to decide when to decompose queries, a Rank-Score Fusion module for result aggregation, and uses Curriculum Reinforcement Learning for stable training.", "result": "ACQO achieves state-of-the-art performance on three complex query benchmarks, outperforming baselines with improved computational efficiency and compatibility across different retrieval architectures.", "conclusion": "ACQO is established as a powerful and generalizable solution for next-generation RAG systems by effectively handling complex query optimization with enhanced stability and adaptability."}}
{"id": "2601.21050", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21050", "abs": "https://arxiv.org/abs/2601.21050", "authors": ["Haokun Zhou"], "title": "SMKC: Sketch Based Kernel Correlation Images for Variable Cardinality Time Series Anomaly Detection", "comment": null, "summary": "Conventional anomaly detection in multivariate time series relies on the assumption that the set of observed variables remains static. In operational environments, however, monitoring systems frequently experience sensor churn. Signals may appear, disappear, or be renamed, creating data windows where the cardinality varies and may include values unseen during training. To address this challenge, we propose SMKC, a framework that decouples the dynamic input structure from the anomaly detector. We first employ permutation-invariant feature hashing to sketch raw inputs into a fixed size state sequence. We then construct a hybrid kernel image to capture global temporal structure through pairwise comparisons of the sequence and its derivatives. The model learns normal patterns using masked reconstruction and a teacher-student prediction objective. Our evaluation reveals that robust log-distance channels provide the primary discriminative signal, whereas cosine representations often fail to capture sufficient contrast. Notably, we find that a detector using random projections and nearest neighbors on the SMKC representation performs competitively with fully trained baselines without requiring gradient updates. This highlights the effectiveness of the representation itself and offers a practical cold-start solution for resource-constrained deployments.", "AI": {"tldr": "SMKC framework for anomaly detection in multivariate time series with sensor churn using fixed-size feature hashing and hybrid kernel images.", "motivation": "Conventional anomaly detection assumes static variables, but real-world monitoring systems experience sensor churn where signals appear, disappear, or are renamed, creating data windows with varying cardinality and unseen values during training.", "method": "1. Use permutation-invariant feature hashing to sketch raw inputs into fixed-size state sequence. 2. Construct hybrid kernel image to capture global temporal structure through pairwise comparisons of sequence and derivatives. 3. Learn normal patterns using masked reconstruction and teacher-student prediction objective.", "result": "Robust log-distance channels provide primary discriminative signal while cosine representations often fail to capture sufficient contrast. A detector using random projections and nearest neighbors on SMKC representation performs competitively with fully trained baselines without requiring gradient updates.", "conclusion": "SMKC effectively decouples dynamic input structure from anomaly detection, offers practical cold-start solution for resource-constrained deployments, and demonstrates that the representation itself is highly effective even without complex training."}}
{"id": "2601.21210", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21210", "abs": "https://arxiv.org/abs/2601.21210", "authors": ["Paul He", "Yinya Huang", "Mrinmaya Sachan", "Zhijing Jin"], "title": "Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification", "comment": "EACL 2026 Main", "summary": "Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.", "AI": {"tldr": "DoVerifier is a symbolic verifier that checks whether LLM-generated causal expressions are formally valid using do-calculus and probability theory rules, providing more accurate evaluation of causal reasoning beyond surface-level metrics.", "motivation": "Current benchmarks for evaluating LLMs on causal reasoning tasks rely on string matching or surface-level metrics that don't capture whether model outputs are formally valid under causal reasoning semantics.", "method": "Propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory.", "result": "DoVerifier can recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences, and evaluations on synthetic data and causal QA benchmarks show it more accurately captures semantic correctness of causal reasoning traces.", "conclusion": "DoVerifier offers a more rigorous and informative way to evaluate LLMs on causal reasoning by checking formal validity rather than relying on surface-level metrics."}}
{"id": "2601.21058", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21058", "abs": "https://arxiv.org/abs/2601.21058", "authors": ["Seungki Hong", "Kyeongwon Jeong", "Taekwang Jang"], "title": "Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization", "comment": null, "summary": "Ising machines have emerged as accelerators for combinatorial optimization. To enable practical deployment, this work aims to reduce time-to-solution by addressing three challenges: (1) hardware topology, (2) spin selection and update algorithms, and (3) scalable coupling-coefficient precision. Restricted topologies require minor embedding; naive parallel updates can oscillate or stall; and limited precision can preclude feasible mappings or degrade solution quality.\n  This work presents Snowball, a digital, scalable, all-to-all coupled Ising machine that integrates dual-mode Markov chain Monte Carlo spin selection with asynchronous spin updates to promote convergence and reduce time-to-solution. The digital architecture supports wide, configurable coupling precision, unlike many analog realizations at high bit widths. A prototype on an AMD Alveo U250 accelerator card achieves an 8$\\times$ reduction in time-to-solution relative to a state-of-the-art Ising machine on the same benchmark instance.", "AI": {"tldr": "Snowball is a digital Ising machine that reduces time-to-solution by addressing hardware topology limitations, spin update algorithms, and coupling precision through dual-mode MCMC spin selection, asynchronous updates, and configurable wide precision.", "motivation": "To enable practical deployment of Ising machines by reducing time-to-solution, addressing three key challenges: restricted hardware topologies requiring minor embedding, naive parallel updates causing oscillations/stalling, and limited coupling-coefficient precision degrading solution quality.", "method": "Developed Snowball, a digital, scalable, all-to-all coupled Ising machine that integrates dual-mode Markov chain Monte Carlo spin selection with asynchronous spin updates. The digital architecture supports wide, configurable coupling precision, unlike many analog realizations.", "result": "A prototype on an AMD Alveo U250 accelerator card achieves an 8\u00d7 reduction in time-to-solution relative to a state-of-the-art Ising machine on the same benchmark instance.", "conclusion": "Snowball's digital architecture with dual-mode MCMC and asynchronous updates effectively addresses key challenges in Ising machine deployment, significantly reducing time-to-solution while maintaining solution quality through configurable precision."}}
{"id": "2601.21212", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.21212", "abs": "https://arxiv.org/abs/2601.21212", "authors": ["Xixian Yong", "Peilin Sun", "Zihe Wang", "Xiao Zhou"], "title": "Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning", "comment": "The Web Conference 2026", "summary": "Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.", "AI": {"tldr": "Intelli-Planner is a framework combining DRL and LLMs for participatory urban planning that generates customized schemes based on demographic/geographic data and planning preferences, with enhanced stakeholder involvement and satisfaction.", "motivation": "Current urban planning methods are either time-consuming/labor-intensive (relying on human experts) or limit stakeholder involvement (using deep learning algorithms). There's a need for participatory and customized planning solutions.", "method": "Intelli-Planner integrates Deep Reinforcement Learning with large language models. It uses demographic, geographic data, and planning preferences to determine requirements for each functional type. Features include a knowledge enhancement module for policy network training and a multi-dimensional evaluation system with LLM-based stakeholders for satisfaction scoring.", "result": "Experimental validation across diverse urban settings shows Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed.", "conclusion": "The framework demonstrates effectiveness and superiority, highlighting the potential of integrating LLMs with DRL approaches to revolutionize functional area planning tasks through enhanced stakeholder participation and customization."}}
{"id": "2601.21060", "categories": ["cs.LG", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21060", "abs": "https://arxiv.org/abs/2601.21060", "authors": ["Zhuoyan Li", "Aditya Bansal", "Jinzhao Li", "Shishuang He", "Zhuoran Lu", "Mutian Zhang", "Qin Liu", "Yiwei Yang", "Swati Jain", "Ming Yin", "Yunyao Li"], "title": "Human-LLM Collaborative Feature Engineering for Tabular Data", "comment": "ICLR 2026", "summary": "Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users' cognitive load during the feature engineering process.", "AI": {"tldr": "A human-LLM collaborative framework for tabular feature engineering decouples operation proposal (by LLM) and selection (via utility modeling and human feedback), improving performance and reducing cognitive load.", "motivation": "Current approaches treat LLMs as black-box optimizers for both proposing and selecting feature transformation operations, relying on internal heuristics that lack calibrated utility estimation, leading to inefficient exploration of low-yield operations without principled prioritization.", "method": "The framework decouples transformation operation proposal and selection: LLMs generate operation candidates, while selection is guided by explicitly modeling utility and uncertainty of each operation, and incorporates human expert preference feedback to identify more effective operations.", "result": "Evaluations on synthetic and real user studies show the framework improves feature engineering performance across various tabular datasets and reduces users' cognitive load during the process.", "conclusion": "The proposed human-LLM collaborative framework successfully improves feature engineering performance across diverse tabular datasets and reduces user cognitive load, demonstrating the value of decoupling proposal and selection while incorporating human preference feedback."}}
{"id": "2601.21221", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21221", "abs": "https://arxiv.org/abs/2601.21221", "authors": ["Henry Salgado", "Meagan R. Kendall", "Martine Ceberio"], "title": "Causal Discovery for Explainable AI: A Dual-Encoding Approach", "comment": "6 pages", "summary": "Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.", "AI": {"tldr": "Dual-encoding causal discovery method for categorical variables using complementary encoding strategies and majority voting to identify causal structures in datasets like Titanic.", "motivation": "Traditional causal discovery methods struggle with categorical variables due to numerical instability in conditional independence testing, making it difficult to understand causal relationships for explaining ML model decisions.", "method": "Proposes a dual-encoding approach that runs constraint-based causal discovery algorithms with complementary encoding strategies, then merges results through majority voting to improve stability and reliability.", "result": "Applied to the Titanic dataset, the method successfully identifies causal structures that align with established explainable methods, demonstrating its effectiveness.", "conclusion": "The dual-encoding causal discovery approach effectively addresses limitations with categorical variables and provides reliable causal structures for explaining ML model decisions."}}
{"id": "2601.21061", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21061", "abs": "https://arxiv.org/abs/2601.21061", "authors": ["Alexandre Larouche", "Audrey Durand"], "title": "Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks", "comment": null, "summary": "Generative Flow Networks (GFlowNets; GFNs) are a class of generative models that learn to sample compositional objects proportionally to their a priori unknown value, their reward. We focus on the case where the reward has a specified, actionable structure, namely that it is submodular. We show submodularity can be harnessed to retrieve upper bounds on the reward of compositional objects that have not yet been observed. We provide in-depth analyses of the probability of such bounds occurring, as well as how many unobserved compositional objects can be covered by a bound. Following the Optimism in the Face of Uncertainty principle, we then introduce SUBo-GFN, which uses the submodular upper bounds to train a GFN. We show that SUBo-GFN generates orders of magnitude more training data than classical GFNs for the same number of queries to the reward function. We demonstrate the effectiveness of SUBo-GFN in terms of distribution matching and high-quality candidate generation on synthetic and real-world submodular tasks.", "AI": {"tldr": "SUBounding uses submodularity to get reward bounds for unobserved compositional objects, enabling optimistic GFN training with much more generated training data per reward query.", "motivation": "GFlowNets learn to sample compositional objects proportional to their unknown reward, but require many reward queries. Submodular rewards have structure that can be exploited to obtain bounds on unseen objects' rewards.", "method": "Harness submodularity to get upper bounds on rewards of unobserved compositional objects. Analyze bound probabilities and coverage. Introduce SUBo-GFN (SUBounding GFlowNet) that uses these submodular upper bounds following Optimism in the Face of Uncertainty principle.", "result": "SUBo-GFN generates orders of magnitude more training data than classical GFNs for same number of reward queries. Effective distribution matching and high-quality candidate generation on synthetic and real-world submodular tasks.", "conclusion": "Submodularity provides exploitable structure for efficient GFN training via reward bounding, enabling substantially improved sample efficiency."}}
{"id": "2601.21226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21226", "abs": "https://arxiv.org/abs/2601.21226", "authors": ["Wolfgang Rohde"], "title": "Delegation Without Living Governance", "comment": null, "summary": "Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.", "AI": {"tldr": "Static governance fails for agentic AI; need runtime governance (Governance Twin) to preserve human relevance in AI-driven decision systems.", "motivation": "Traditional governance assumes rules can be predefined, systems engineered for compliance, and accountability applied post-hoc. This model breaks down when AI systems operate at machine speed with delegated judgment, making humans potentially irrelevant in systems shaping social, economic, and political outcomes.", "method": "Proposes runtime governance through a \"Governance Twin\" concept that enables continuous monitoring and interaction with AI systems during operation, allowing for human communication, influence, and co-evolution with AI intelligence.", "result": "Identifies the core challenge as maintaining human relevance rather than just AI safety or consciousness. Suggests that accountability, agency, and punishment mechanisms must be fundamentally rethought for runtime governance to work effectively.", "conclusion": "Static compliance-based governance is inadequate for agentic AI systems. Runtime governance, particularly the Governance Twin concept, offers a promising approach to preserve human participation and relevance in AI-driven decision-making systems that increasingly shape society."}}
{"id": "2601.21064", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21064", "abs": "https://arxiv.org/abs/2601.21064", "authors": ["Minghui Chen", "Wenlong Deng", "James Zou", "Han Yu", "Xiaoxiao Li"], "title": "Textual Equilibrium Propagation for Deep Compound AI Systems", "comment": "Accepted to ICLR 2026", "summary": "Large language models (LLMs) are increasingly deployed as part of compound AI systems that coordinate multiple modules (e.g., retrievers, tools, verifiers) over long-horizon workflows. Recent approaches that propagate textual feedback globally (e.g., TextGrad) make it feasible to optimize such pipelines, but we find that performance degrades as system depth grows. In particular, long-horizon agentic workflows exhibit two depth-scaling failure modes: 1) exploding textual gradient, where textual feedback grows exponentially with depth, leading to prohibitively long message and amplifies evaluation biases; and 2) vanishing textual gradient, where limited long-context ability causes models overemphasize partial feedback and compression of lengthy feedback causes downstream messages to lose specificity gradually as they propagate many hops upstream. To mitigate these issues, we introduce Textual Equilibrium Propagation (TEP), a local learning principle inspired by Equilibrium Propagation in energy-based models. TEP includes two phases: 1) a free phase where a local LLM critics iteratively refine prompts until reaching equilibrium (no further improvements are suggested); and 2) a nudged phase which applies proximal prompt edits with bounded modification intensity, using task-level objectives that propagate via forward signaling rather than backward feedback chains. This design supports local prompt optimization followed by controlled adaptation toward global goals without the computational burden and signal degradation of global textual backpropagation. Across long-horizon QA benchmarks and multi-agent tool-use dataset, TEP consistently improves accuracy and efficiency over global propagation methods such as TextGrad. The gains grows with depth, while preserving the practicality of black-box LLM components in deep compound AI system.", "AI": {"tldr": "Textual Equilibrium Propagation (TEP) addresses depth-scaling failures in compound AI systems by using local learning with equilibrium states and bounded prompt edits instead of global textual backpropagation.", "motivation": "Global textual feedback propagation (like TextGrad) in deep compound AI systems suffers from exploding gradients (exponential feedback growth) and vanishing gradients (loss of specificity due to context limits), degrading performance as system depth increases.", "method": "TEP uses a two-phase approach: 1) Free phase - local LLM critics iteratively refine prompts until equilibrium (no further improvements); 2) Nudged phase - apply proximal prompt edits with bounded modification intensity using task-level objectives that propagate forward rather than backward.", "result": "TEP consistently improves accuracy and efficiency over global propagation methods like TextGrad across long-horizon QA benchmarks and multi-agent tool-use datasets, with gains increasing with system depth.", "conclusion": "TEP enables effective optimization of deep compound AI systems while maintaining practicality of black-box LLM components, overcoming depth-scaling limitations of global textual feedback methods."}}
{"id": "2601.21233", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21233", "abs": "https://arxiv.org/abs/2601.21233", "authors": ["Xiang Zheng", "Yutao Wu", "Hanxun Huang", "Yige Li", "Xingjun Ma", "Bo Li", "Yu-Gang Jiang", "Cong Wang"], "title": "Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs", "comment": "24 pages, 6 figures, 17 tables", "summary": "Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \\textbf{\\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \\textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \\textbf{41} black-box commercial models across multiple providers, \\textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.", "AI": {"tldr": "JustAsk is a self-evolving framework that autonomously extracts hidden system prompts from code agents using LLMs, revealing security vulnerabilities without handcrafted prompts or supervision.", "motivation": "The autonomy of code agents introduces an unrecognized security risk: agentic interaction expands the attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior, exposing them as a critical vulnerability.", "method": "The method involves formulating prompt extraction as an online exploration problem. It uses Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration, exploiting imperfect system-instruction generalization and tensions between helpfulness and safety, requiring no handcrafted prompts or privileged access.", "result": "Evaluated on 41 black-box commercial models across multiple providers, JustAsk consistently achieves full or near-complete system prompt recovery, demonstrating recurring design- and architecture-level vulnerabilities in modern agent systems.", "conclusion": "The paper concludes that system prompts are a critical yet largely unprotected attack surface in modern agent systems, and JustAsk effectively exposes these vulnerabilities through autonomous, unsupervised extraction."}}
{"id": "2601.21067", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21067", "abs": "https://arxiv.org/abs/2601.21067", "authors": ["Haoyang Li", "Haibo Chen", "Xin Wang", "Wenwu Zhu"], "title": "Out-of-Distribution Generalization in Graph Foundation Models", "comment": null, "summary": "Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.", "AI": {"tldr": "This paper is a survey on graph foundation models (GFMs) and their out-of-distribution generalization capabilities, reviewing recent progress and organizing approaches based on task specification.", "motivation": "Graph learning models often suffer from limited generalization beyond their training distributions due to various distribution shifts in graph structure, domain semantics, modalities, or task formulations. Graph foundation models have emerged to address these challenges through large-scale pretraining.", "method": "The survey organizes existing GFM approaches based on whether they are designed for fixed task specifications or support generalization across heterogeneous task formulations. It summarizes corresponding OOD handling strategies and pretraining objectives, and reviews common evaluation protocols.", "result": "The paper provides a comprehensive review and organization of recent progress in graph foundation models from an OOD generalization perspective, establishing a unified problem setting and categorizing existing approaches.", "conclusion": "This is the first survey on out-of-distribution generalization in graph foundation models, and it identifies open directions for future research in this emerging field."}}
{"id": "2601.21239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21239", "abs": "https://arxiv.org/abs/2601.21239", "authors": ["Chentong Chen", "Mengyuan Zhong", "Ye Fan", "Jialong Shi", "Jianyong Sun"], "title": "TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design", "comment": null, "summary": "Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.", "AI": {"tldr": "TIDE decouples structural reasoning from parameter optimization for automated heuristic design, using a nested architecture with tree similarity edit distance for structural diversity and differential mutation for parameter tuning.", "motivation": "Existing LLM-based automated heuristic design methods treat algorithm evolution as a monolithic text generation task, overlooking the coupling between discrete algorithmic structures and continuous numerical parameters. This leads to discarding promising algorithms due to uncalibrated constants and premature convergence from simple similarity metrics.", "method": "TIDE features a nested architecture: an outer parallel island model uses Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. A UCB-based scheduler dynamically prioritizes high-yield prompt strategies for resource allocation.", "result": "Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.", "conclusion": "TIDE successfully addresses the limitations of existing automated heuristic design methods by decoupling structural reasoning from parameter optimization, leading to better algorithm discovery with improved efficiency and reduced computational overhead."}}
{"id": "2601.21082", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21082", "abs": "https://arxiv.org/abs/2601.21082", "authors": ["Shivam Patel", "William Cocke", "Gauri Joshi"], "title": "LOCUS: Low-Dimensional Model Embeddings for Efficient Model Exploration, Comparison, and Selection", "comment": null, "summary": "The rapidly growing ecosystem of Large Language Models (LLMs) makes it increasingly challenging to manage and utilize the vast and dynamic pool of models effectively. We propose LOCUS, a method that produces low-dimensional vector embeddings that compactly represent a language model's capabilities across queries. LOCUS is an attention-based approach that generates embeddings by a deterministic forward pass over query encodings and evaluation scores via an encoder model, enabling seamless incorporation of new models to the pool and refinement of existing model embeddings without having to perform any retraining. We additionally train a correctness predictor that uses model embeddings and query encodings to achieve state-of-the-art routing accuracy on unseen queries. Experiments show that LOCUS needs up to 4.8x fewer query evaluation samples than baselines to produce informative and robust embeddings. Moreover, the learned embedding space is geometrically meaningful: proximity reflects model similarity, enabling a range of downstream applications including model comparison and clustering, model portfolio selection, and resilient proxies of unavailable models.", "AI": {"tldr": "LOCUS is a method for embedding LLM capabilities via low-dimensional vectors using attention over queries and scores, enabling efficient model management and routing without retraining.", "motivation": "The rapid growth and diversity of Large Language Models make it challenging to manage and utilize models effectively due to their vast and dynamic nature.", "method": "LOCUS uses an attention-based approach to generate embeddings through a deterministic forward pass over query encodings and evaluation scores via an encoder model, allowing seamless addition of new models and refinement of existing ones.", "result": "LOCUS requires up to 4.8x fewer query evaluation samples than baselines, produces geometrically meaningful embeddings that reflect model similarity, and enables downstream applications like model comparison, clustering, and selection.", "conclusion": "LOCUS provides a robust and efficient way to represent LLM capabilities, enhancing model management and enabling advanced applications in model routing and analysis."}}
{"id": "2601.21249", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21249", "abs": "https://arxiv.org/abs/2601.21249", "authors": ["Enzo Nicol\u00e1s Spotorno", "Ant\u00f4nio Augusto Medeiros Fr\u00f6hlich"], "title": "Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox", "comment": "14 pages, (8 main text, 6 references and appendices), 2 figures", "summary": "The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term \"HYDRA\" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.", "AI": {"tldr": "The paper proposes HYDRA, a modular approach using frozen regime-specific specialists with uncertainty-aware blending to address catastrophic forgetting and verification challenges of monolithic foundation models in safety-critical CPS.", "motivation": "Monolithic foundation models for time-series and physical dynamics face deployment challenges in safety-critical CPS due to catastrophic forgetting during fine-tuning, residual spectral bias smoothing high-frequency discontinuities, and opacity hindering formal verification required by safety standards.", "method": "HYDRA (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems): A modular sovereignty paradigm using a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, ensuring regime-conditional validity and rigorous disentanglement of aleatoric and epistemic uncertainties.", "result": "The proposed modular approach offers regime-conditional validity, rigorous uncertainty disentanglement, and modular auditability, providing a certifiable path for robust state integrity across the CPS lifecycle.", "conclusion": "The plasticity-stability paradox in safety-critical CPS cannot be resolved by global parameter updates; instead, a modular sovereignty paradigm with frozen specialists and uncertainty-aware blending provides a viable solution for certifiable robustness and verification."}}
{"id": "2601.21092", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21092", "abs": "https://arxiv.org/abs/2601.21092", "authors": ["Marvin Sextro", "Weronika K\u0142os", "Gabriel Dernbach"], "title": "MapPFN: Learning Causal Perturbation Maps in Context", "comment": null, "summary": "Planning effective interventions in biological systems requires treatment-effect models that adapt to unseen biological contexts by identifying their specific underlying mechanisms. Yet single-cell perturbation datasets span only a handful of biological contexts, and existing methods cannot leverage new interventional evidence at inference time to adapt beyond their training data. To meta-learn a perturbation effect estimator, we present MapPFN, a prior-data fitted network (PFN) pretrained on synthetic data generated from a prior over causal perturbations. Given a set of experiments, MapPFN uses in-context learning to predict post-perturbation distributions, without gradient-based optimization. Despite being pretrained on in silico gene knockouts alone, MapPFN identifies differentially expressed genes, matching the performance of models trained on real single-cell data. Our code and data are available at https://github.com/marvinsxtr/MapPFN.", "AI": {"tldr": "MapPFN is a meta-learning method that uses in-context learning to predict perturbation effects in biological systems, trained on synthetic data and matching real-data-trained model performance.", "motivation": "Planning effective interventions in biological systems requires treatment-effect models that can adapt to unseen biological contexts by identifying their specific underlying mechanisms. However, existing methods cannot leverage new interventional evidence at inference time to adapt beyond their training data, and single-cell perturbation datasets are limited to only a handful of biological contexts.", "method": "The authors present MapPFN, a prior-data fitted network (PFN) pretrained on synthetic data generated from a prior over causal perturbations. It uses in-context learning to predict post-perturbation distributions given a set of experiments, without requiring gradient-based optimization during inference.", "result": "Despite being pretrained only on in silico gene knockouts, MapPFN identifies differentially expressed genes and matches the performance of models trained on real single-cell data.", "conclusion": "MapPFN demonstrates a novel approach to meta-learning perturbation effect estimators that can adapt to unseen biological contexts using in-context learning, without requiring gradient-based optimization or extensive real data. Its ability to match the performance of models trained on real single-cell data, despite being pretrained only on synthetic knockouts, suggests strong potential for improving intervention planning in biological systems where experimental data is limited."}}
{"id": "2601.21288", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21288", "abs": "https://arxiv.org/abs/2601.21288", "authors": ["Weitong Lian", "Zecong Tang", "Haoran Li", "Tianjian Gao", "Yifei Wang", "Zixu Wang", "Lingyi Meng", "Tengju Ru", "Zhejun Cui", "Yichen Zhu", "Hangshuo Cao", "Qi Kang", "Tianxing Chen", "Yusen Qin", "Kaixuan Wang", "Yu Zhang"], "title": "Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving", "comment": "Preprint. 23 pages, 14 figures", "summary": "Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a \"perception-reasoning-planning\" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.", "AI": {"tldr": "Drive-KD is a knowledge distillation framework that decomposes autonomous driving into perception, reasoning, and planning capabilities, using multi-teacher distillation with asymmetric gradient projection to create efficient small VLMs that outperform much larger models.", "motivation": "Large VLMs for autonomous driving require substantial GPU memory and have high inference latency, while conventional supervised fine-tuning struggles to bridge capability gaps in small models. There's a need for efficient models that maintain strong autonomous driving capabilities.", "method": "Decomposes autonomous driving into perception-reasoning-planning triad; uses layer-specific attention as distillation signal to create capability-specific single-teacher models; unifies into multi-teacher framework with asymmetric gradient projection to mitigate cross-capability gradient conflicts.", "result": "Distilled InternVL3-1B model achieves ~42x less GPU memory usage and ~11.4x higher throughput than larger models, outperforming pretrained 78B model from same family on DriveBench, and surpasses GPT-5.1 on planning dimension.", "conclusion": "Drive-KD provides an effective knowledge distillation framework for creating efficient autonomous driving VLMs that maintain strong capabilities while significantly reducing computational requirements, offering insights toward practical deployment of autonomous driving systems."}}
{"id": "2601.21094", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21094", "abs": "https://arxiv.org/abs/2601.21094", "authors": ["Minjae Kwon", "Josephine Lamp", "Lu Feng"], "title": "Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed", "comment": null, "summary": "Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.", "AI": {"tldr": "Safe RL algorithms fail to generalize safety guarantees to unseen patients in diabetes management; test-time shielding with learned dynamics models effectively restores safety across algorithms and patient populations.", "motivation": "The motivation is to investigate whether safe RL algorithms' training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed where safety generalization failures could have serious consequences.", "method": "The researchers benchmarked eight safe RL algorithms on a unified clinical simulator for diabetes management, evaluating them across three diabetes types and three age groups. They introduced test-time shielding, which uses learned dynamics models to filter unsafe actions during deployment.", "result": "The results reveal a safety generalization gap where policies satisfying constraints during training frequently violate safety requirements on unseen patients. Test-time shielding effectively restores safety, achieving Time-in-Range gains of 13-14% for strong baselines like PPO-Lag and CPO while reducing clinical risk index and glucose variability.", "conclusion": "The study concludes that test-time shielding is an effective method to restore safety guarantees across different safe RL algorithms and patient populations under distribution shift, achieving significant clinical improvements in diabetes management."}}
{"id": "2601.21321", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21321", "abs": "https://arxiv.org/abs/2601.21321", "authors": ["Zihao Chen", "Jiayin Wang", "Ziyi Sun", "Ji Zhuang", "Jinyi Shen", "Xiaoyue Ke", "Li Shang", "Xuan Zeng", "Fan Yang"], "title": "White-Box Op-Amp Design via Human-Mimicking Reasoning", "comment": null, "summary": "This brief proposes \\emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \\emph{\\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \\emph{\\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \\textcolor{blue}{https://github.com/zhchenfdu/whiteop}.", "AI": {"tldr": "White-Op is an interpretable op-amp design framework using LLM agents to mimic human reasoning, achieving reliable behavioral-level designs with low error.", "motivation": "To address the uninterpretability and failure issues of black-box optimization methods in operational amplifier parameter design by formalizing human-like reasoning.", "method": "Develops an iterative hypothesis-verification-decision workflow where LLM agents introduce hypothetical constraints to regulate poles and zeros, formulating a closed-form optimization problem solved programmatically and verified via simulation.", "result": "White-Op achieves 8.52% theoretical prediction error, works on all 9 tested op-amp topologies (unlike a baseline failing in 5), and retains functionality after transistor-level mapping.", "conclusion": "White-Op provides an effective, interpretable framework for op-amp design, outperforming black-box methods and being open-sourced for accessibility."}}
{"id": "2601.21135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21135", "abs": "https://arxiv.org/abs/2601.21135", "authors": ["Shicheng Fan", "Kun Zhang", "Lu Cheng"], "title": "TRACE: Trajectory Recovery for Continuous Mechanism Evolution in Causal Representation Learning", "comment": "23 pages, 11 figures", "summary": "Temporal causal representation learning methods assume that causal mechanisms switch instantaneously between discrete domains, yet real-world systems often exhibit continuous mechanism transitions. For example, a vehicle's dynamics evolve gradually through a turning maneuver, and human gait shifts smoothly from walking to running. We formalize this setting by modeling transitional mechanisms as convex combinations of finitely many atomic mechanisms, governed by time-varying mixing coefficients. Our theoretical contributions establish that both the latent causal variables and the continuous mixing trajectory are jointly identifiable. We further propose TRACE, a Mixture-of-Experts framework where each expert learns one atomic mechanism during training, enabling recovery of mechanism trajectories at test time. This formulation generalizes to intermediate mechanism states never observed during training. Experiments on synthetic and real-world data demonstrate that TRACE recovers mixing trajectories with up to 0.99 correlation, substantially outperforming discrete-switching baselines.", "AI": {"tldr": "Proposes TRACE, a method for learning continuous causal transitions via mixture of atomic mechanisms, showing strong performance on synthetic and real data.", "motivation": "Existing methods assume discrete switching between causal mechanisms, but real-world systems often exhibit continuous transitions, which current approaches cannot capture.", "method": "TRACE uses a Mixture-of-Experts framework where each expert learns an atomic mechanism, with time-varying mixing coefficients enabling recovery of continuous mechanism trajectories.", "result": "TRACE recovers mixing trajectories with up to 0.99 correlation, substantially outperforming discrete-switching baselines on synthetic and real-world data.", "conclusion": "TRACE provides a principled and effective solution for learning continuous causal transitions, advancing causal representation learning beyond discrete switching assumptions."}}
{"id": "2601.21335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21335", "abs": "https://arxiv.org/abs/2601.21335", "authors": ["Yuzhe Chen", "Jie Cao", "Youquan Wang", "Haicheng Tao", "Darko B. Vukovic", "Jia Wu"], "title": "Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation", "comment": "Accepted to The Web Conference (WWW) 2026", "summary": "Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.", "AI": {"tldr": "A Causal Neuro-Symbolic Reasoning model (CNRE) is proposed for explainable multi-behavior recommendation, integrating neural networks and symbolic logic with causal inference to enhance explainability and performance by addressing confounding effects in user behavior chains.", "motivation": "Existing multi-behavior recommendations prioritize performance over explainability, and current explainable methods lack generalizability due to reliance on external information. User behavior chains contain endogenous logic suitable for explicit reasoning, but are affected by confounders leading to spurious correlations.", "method": "CNRE simulates human-like decision-making by first using hierarchical preference propagation to capture cross-behavior dependencies, then modeling endogenous logic rules based on preference strength and adaptively dispatching to neural-logic reasoning paths (e.g., conjunction, disjunction) to generate an explainable causal mediator isolated from confounding effects.", "result": "Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines in performance.", "conclusion": "CNRE effectively integrates neural networks, symbolic logic, and causal inference to provide a novel solution for explainable multi-behavior recommendation, offering multi-level explainability from model design to results while addressing confounding issues."}}
{"id": "2601.21147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21147", "abs": "https://arxiv.org/abs/2601.21147", "authors": ["Kevin Han", "Haolin Cong", "Bowen Deng", "Amir Barati Farimani"], "title": "Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials", "comment": null, "summary": "Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.", "AI": {"tldr": "Dynamic cutoff for MLIPs reduces memory and speeds inference while maintaining accuracy, enabling larger-scale simulations.", "motivation": "Address the bottlenecks of inference time and memory consumption in MLIPs that prevent realistic simulation scales by challenging the fixed cutoff radius paradigm.", "method": "Introduce a dynamic cutoff formulation that induces sparsity on the atom graph by targeting a specific number of neighbors per atom, implemented on four state-of-the-art MLIPs (MACE, Nequip, Orbv3, TensorNet).", "result": "Dynamic cutoff models achieve up to 2.26x less memory consumption and 2.04x faster inference time with minimal accuracy dropoff compared to fixed cutoff models on materials and molecular datasets.", "conclusion": "The dynamic cutoff formulation for MLIPs is a viable approach that significantly reduces memory consumption and inference time while maintaining accuracy, enabling more realistic simulation scales."}}
{"id": "2601.21339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21339", "abs": "https://arxiv.org/abs/2601.21339", "authors": ["Jennifer Haase", "Jana Gonnermann-M\u00fcller", "Paul H. P. Hanel", "Nicolas Leins", "Thomas Kosch", "Jan Mendling", "Sebastian Pokutta"], "title": "Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks", "comment": null, "summary": "How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.", "AI": {"tldr": "This paper analyzes the relative impact of prompts, model choice, and sampling stochasticity on LLM output variance, finding prompts significant for quality but less for quantity, highlighting risks of single-sample evaluations.", "motivation": "To quantify how much of LLM output variance is explained by prompts, model choice, and stochasticity from sampling, to inform evaluation practices and understand key drivers of output characteristics.", "method": "Evaluated 12 LLMs on 10 creativity prompts with 100 samples per prompt (total N=12,000), measuring output quality (originality) and quantity (fluency) to decompose variance contributions.", "result": "For output quality (originality), prompts explained 36.43% of variance, similar to model choice (40.94%). For output quantity (fluency), model choice dominated (51.25%) and within-LLM variance was high (33.70%), while prompts explained only 4.22%. Within-LLM variance ranged 10-34%.", "conclusion": "Prompts are effective for steering output quality but less so for quantity, with model choice and within-LLM variance playing larger roles. Single-sample evaluations risk confounding sampling noise with genuine prompt or model effects due to substantial stochasticity."}}
{"id": "2601.21149", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21149", "abs": "https://arxiv.org/abs/2601.21149", "authors": ["Maria Despoina Siampou", "Shushman Choudhury", "Shang-Ling Hsu", "Neha Arora", "Cyrus Shahabi"], "title": "Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement", "comment": null, "summary": "Recent progress in geospatial foundation models highlights the importance of learning general-purpose representations for real-world locations, particularly points-of-interest (POIs) where human activity concentrates. Existing approaches, however, focus primarily on place identity derived from static textual metadata, or learn representations tied to trajectory context, which capture movement regularities rather than how places are actually used (i.e., POI's function). We argue that POI function is a missing but essential signal for general POI representations. We introduce Mobility-Embedded POIs (ME-POIs), a framework that augments POI embeddings derived, from language models with large-scale human mobility data to learn POI-centric, context-independent representations grounded in real-world usage. ME-POIs encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations via contrastive learning to capture usage patterns across users and time. To address long-tail sparsity, we propose a novel mechanism that propagates temporal visit patterns from nearby, frequently visited POIs across multiple spatial scales. We evaluate ME-POIs on five newly proposed map enrichment tasks, testing its ability to capture both the identity and function of POIs. Across all tasks, augmenting text-based embeddings with ME-POIs consistently outperforms both text-only and mobility-only baselines. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, highlighting that POI function is a critical component of accurate and generalizable POI representations.", "AI": {"tldr": "ME-POIs is a framework that enhances POI representations by combining language model embeddings with human mobility data to capture both place identity and function, outperforming text-only and mobility-only baselines.", "motivation": "Existing POI representation approaches focus on static textual metadata or trajectory context, missing the essential signal of POI function - how places are actually used. There's a need for general-purpose POI representations that capture real-world usage patterns.", "method": "The framework augments language model embeddings with large-scale human mobility data using contrastive learning. It encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations. To address long-tail sparsity, it propagates temporal visit patterns from nearby frequently visited POIs across multiple spatial scales.", "result": "ME-POIs consistently outperforms both text-only and mobility-only baselines across five map enrichment tasks. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, demonstrating that POI function is critical for accurate representations.", "conclusion": "POI function is a missing but essential component for general POI representations. Augmenting text-based embeddings with mobility data through the ME-POIs framework creates more accurate and generalizable representations that capture both identity and function of places."}}
{"id": "2601.21340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21340", "abs": "https://arxiv.org/abs/2601.21340", "authors": ["Lang Cao", "Qingyu Chen", "Yue Guo"], "title": "EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation", "comment": null, "summary": "Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.", "AI": {"tldr": "EHR-RAG is a retrieval-augmented framework for interpreting long-horizon structured EHR data that outperforms existing LLM approaches by preserving clinical structure and temporal dynamics through specialized retrieval components.", "motivation": "Electronic Health Records contain rich longitudinal clinical evidence crucial for medical decision-making, but long-horizon EHRs often exceed LLM context limits. Existing approaches using truncation or vanilla retrieval discard clinically relevant events and temporal dependencies, limiting their effectiveness for clinical prediction tasks.", "method": "EHR-RAG introduces three key components: 1) Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, 2) Adaptive Iterative Retrieval to progressively refine queries for broader evidence coverage, and 3) Dual-Path Evidence Retrieval and Reasoning to jointly retrieve and reason over both factual and counterfactual evidence.", "result": "Experiments across four long-horizon EHR prediction tasks show EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%.", "conclusion": "The work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice, demonstrating that specialized retrieval frameworks can effectively handle the challenges of long-horizon clinical data."}}
{"id": "2601.21150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21150", "abs": "https://arxiv.org/abs/2601.21150", "authors": ["Henry Kvinge", "Andrew Aguilar", "Nayda Farnsworth", "Grace O'Brien", "Robert Jasper", "Sarah Scullen", "Helen Jenne"], "title": "Can Neural Networks Learn Small Algebraic Worlds? An Investigation Into the Group-theoretic Structures Learned By Narrow Models Trained To Predict Group Operations", "comment": "Presented at TAG-DS 2025", "summary": "While a real-world research program in mathematics may be guided by a motivating question, the process of mathematical discovery is typically open-ended. Ideally, exploration needed to answer the original question will reveal new structures, patterns, and insights that are valuable in their own right. This contrasts with the exam-style paradigm in which the machine learning community typically applies AI to math. To maximize progress in mathematics using AI, we will need to go beyond simple question answering. With this in mind, we explore the extent to which narrow models trained to solve a fixed mathematical task learn broader mathematical structure that can be extracted by a researcher or other AI system. As a basic test case for this, we use the task of training a neural network to predict a group operation (for example, performing modular arithmetic or composition of permutations). We describe a suite of tests designed to assess whether the model captures significant group-theoretic notions such as the identity element, commutativity, or subgroups. Through extensive experimentation we find evidence that models learn representations capable of capturing abstract algebraic properties. For example, we find hints that models capture the commutativity of modular arithmetic. We are also able to train linear classifiers that reliably distinguish between elements of certain subgroups (even though no labels for these subgroups are included in the data). On the other hand, we are unable to extract notions such as the concept of the identity element. Together, our results suggest that in some cases the representations of even small neural networks can be used to distill interesting abstract structure from new mathematical objects.", "AI": {"tldr": "This paper explores whether neural networks trained on mathematical tasks like group operations can learn and capture broader algebraic structures, showing evidence that models can represent abstract properties such as commutativity and subgroups, though not all concepts like identity elements are extractable.", "motivation": "The motivation is to move beyond simple question-answering in applying AI to mathematics, addressing the open-ended nature of mathematical discovery and aiming to maximize AI's utility in uncovering new structures and insights from mathematical tasks.", "method": "The method involves training neural networks on tasks such as predicting group operations (e.g., modular arithmetic or permutation composition), and designing tests to assess whether models capture group-theoretic notions like identity elements, commutativity, and subgroups through experimentation and linear classifiers.", "result": "Results indicate that models learn representations capable of capturing abstract algebraic properties; for example, evidence suggests models can capture commutativity and distinguish elements of subgroups using linear classifiers, but they fail to extract concepts like the identity element.", "conclusion": "The conclusion is that even small neural networks' representations can sometimes distill interesting abstract structures from mathematical objects, highlighting potential for AI to aid in mathematical exploration beyond direct problem-solving, though limitations exist in extracting all fundamental concepts."}}
{"id": "2601.21342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21342", "abs": "https://arxiv.org/abs/2601.21342", "authors": ["Zhiyong Shen", "Gongpeng Zhao", "Jun Zhou", "Li Yu", "Guandong Kou", "Jichen Li", "Chuanlei Dong", "Zuncheng Li", "Kaimao Li", "Bingkun Wei", "Shicheng Hu", "Wei Xia", "Wenguo Duan"], "title": "Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.", "AI": {"tldr": "Ostrakon-VL is a specialized multimodal LLM for food-service/retail stores that achieves state-of-the-art performance on the new ShopBench benchmark through quality-aware data curation and multi-stage training.", "motivation": "Current MLLMs face deployment challenges in FSRS scenarios due to noisy real-world data lacking auditable curation, and absence of unified standardized benchmarks for objective model evaluation across different input types.", "method": "Developed Ostrakon-VL based on Qwen3-VL-8B, introduced ShopBench benchmark, and proposed QUAD - a multi-stage multimodal instruction data curation pipeline with multi-stage training strategy.", "result": "Ostrakon-VL achieves 60.1 average score on ShopBench, surpassing larger Qwen3-VL-235B-A22B (59.4) by +0.7 and same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating improved parameter efficiency and robust FSRS capabilities.", "conclusion": "The approach delivers more robust FSRS-centric perception and decision-making, with both Ostrakon-VL and ShopBench benchmark to be publicly released for reproducible research."}}
{"id": "2601.21151", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21151", "abs": "https://arxiv.org/abs/2601.21151", "authors": ["Carlos A. Pereira", "St\u00e9phane Gaudreault", "Valentin Dallerit", "Christopher Subich", "Shoyon Panday", "Siqi Wei", "Sasa Zhang", "Siddharth Rout", "Eldad Haber", "Raymond J. Spiteri", "David Millard", "Emilia Diaconescu"], "title": "Learning to Advect: A Neural Semi-Lagrangian Architecture for Weather Forecasting", "comment": null, "summary": "Recent machine-learning approaches to weather forecasting often employ a monolithic architecture, where distinct physical mechanisms (advection, transport), diffusion-like mixing, thermodynamic processes, and forcing are represented implicitly within a single large network. This representation is particularly problematic for advection, where long-range transport must be treated with expensive global interaction mechanisms or through deep, stacked convolutional layers. To mitigate this, we present PARADIS, a physics-inspired global weather prediction model that imposes inductive biases on network behavior through a functional decomposition into advection, diffusion, and reaction blocks acting on latent variables. We implement advection through a Neural Semi-Lagrangian operator that performs trajectory-based transport via differentiable interpolation on the sphere, enabling end-to-end learning of both the latent modes to be transported and their characteristic trajectories. Diffusion-like processes are modeled through depthwise-separable spatial mixing, while local source terms and vertical interactions are modeled via pointwise channel interactions, enabling operator-level physical structure. PARADIS provides state-of-the-art forecast skill at a fraction of the training cost. On ERA5-based benchmarks, the 1 degree PARADIS model, with a total training cost of less than a GPU month, meets or exceeds the performance of 0.25 degree traditional and machine-learning baselines, including the ECMWF HRES forecast and DeepMind's GraphCast.", "AI": {"tldr": "PARADIS is a physics-inspired global weather prediction model that decomposes weather forecasting into advection, diffusion, and reaction components using neural operators, achieving state-of-the-art performance with significantly lower training costs.", "motivation": "Traditional monolithic machine learning approaches to weather forecasting implicitly represent different physical mechanisms within a single large network, which is problematic for advection requiring expensive global interactions or deep convolutional layers.", "method": "Functional decomposition into advection, diffusion, and reaction blocks acting on latent variables. Advection uses a Neural Semi-Lagrangian operator with differentiable interpolation on the sphere, diffusion uses depthwise-separable spatial mixing, and local source terms use pointwise channel interactions.", "result": "PARADIS achieves state-of-the-art forecast skill with much lower training costs. The 1-degree model with less than a GPU month of training meets or exceeds performance of 0.25-degree traditional and ML baselines including ECMWF HRES and DeepMind's GraphCast.", "conclusion": "Physics-inspired functional decomposition with explicit advection, diffusion, and reaction operators enables efficient, accurate weather forecasting with strong inductive biases, outperforming monolithic approaches while requiring significantly less computational resources."}}
{"id": "2601.21344", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21344", "abs": "https://arxiv.org/abs/2601.21344", "authors": ["Hassam Tahir", "Faizan Faisal", "Fady Alnajjar", "Muhammad Imran Taj", "Lucia Gordon", "Aila Khan", "Michael Lwin", "Omar Mubin"], "title": "Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms", "comment": "Publication Link: https://ieeexplore.ieee.org/document/11118419", "summary": "This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.", "AI": {"tldr": "A framework integrating LLMs into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity through AI-powered dynamic moderation.", "motivation": "To address limitations in static moderation and personalization in existing educational systems, and to enhance student engagement, critical thinking, and inclusivity in collaborative learning environments.", "method": "Uses advanced LLMs as dynamic moderators with robust feedback mechanisms, implemented through a modular architecture featuring ReactJS frontend, Flask backend, and efficient question retrieval for personalized interactions.", "result": "Testing demonstrates significant improvements in student collaboration, deeper comprehension, and effective scaling across various subjects and user groups.", "conclusion": "The framework establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes through dynamic AI moderation."}}
{"id": "2601.21160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21160", "abs": "https://arxiv.org/abs/2601.21160", "authors": ["Michael Ibrahim", "Nagi Gebraeel", "Weijun Xie"], "title": "A Federated Generalized Expectation-Maximization Algorithm for Mixture Models with an Unknown Number of Components", "comment": "49 Pages, Accepted at ICLR 2026", "summary": "We study the problem of federated clustering when the total number of clusters $K$ across clients is unknown, and the clients have heterogeneous but potentially overlapping cluster sets in their local data. To that end, we develop FedGEM: a federated generalized expectation-maximization algorithm for the training of mixture models with an unknown number of components. Our proposed algorithm relies on each of the clients performing EM steps locally, and constructing an uncertainty set around the maximizer associated with each local component. The central server utilizes the uncertainty sets to learn potential cluster overlaps between clients, and infer the global number of clusters via closed-form computations. We perform a thorough theoretical study of our algorithm, presenting probabilistic convergence guarantees under common assumptions. Subsequently, we study the specific setting of isotropic GMMs, providing tractable, low-complexity computations to be performed by each client during each iteration of the algorithm, as well as rigorously verifying assumptions required for algorithm convergence. We perform various numerical experiments, where we empirically demonstrate that our proposed method achieves comparable performance to centralized EM, and that it outperforms various existing federated clustering methods.", "AI": {"tldr": "FedGEM is a federated generalized EM algorithm for training mixture models with an unknown number of clusters in heterogeneous, overlapping client data, providing theoretical guarantees and outperforming existing methods.", "motivation": "In federated clustering, the total number of clusters K across clients is often unknown, and clients have heterogeneous data with potentially overlapping cluster sets, necessitating a method that can handle this complexity without centralizing data.", "method": "FedGEM involves clients performing local EM steps and constructing uncertainty sets around each local component maximizer. The central server uses these sets to learn cluster overlaps and infer the global number of clusters via closed-form computations.", "result": "The algorithm includes theoretical convergence guarantees under common assumptions, tractable computations for isotropic GMMs, and numerical experiments showing comparable performance to centralized EM and outperforming other federated clustering methods.", "conclusion": "FedGEM effectively addresses federated clustering with unknown and overlapping clusters, offering a theoretically sound and empirically superior approach for training mixture models in distributed settings."}}
{"id": "2601.21352", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21352", "abs": "https://arxiv.org/abs/2601.21352", "authors": ["Ziyu Lu", "Tengjin Weng", "Yiying Yang", "Yuhang Zhao", "Xinxin Huang", "Wenhao Jiang"], "title": "BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents", "comment": null, "summary": "GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.", "AI": {"tldr": "BEAP-Agent is a DFS-based GUI agent framework with Planner, Executor, and Tracker components that enables long-range backtracking and dynamic task tracking, improving recovery from incorrect exploration paths and achieving 28.2% accuracy on OSWorld.", "motivation": "Existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. There is a gap in systematic backtracking mechanisms for GUI agents, hindering robust long-horizon task exploration.", "method": "The paper proposes BEAP-Agent, a DFS-based framework consisting of three collaborative components: Planner, Executor, and Tracker. It models GUI task execution as a DFS process and supports long-range, multi-level state backtracking with dynamic task tracking and updating.", "result": "BEAP-Agent achieved an accuracy of 28.2% on the OSWorld benchmark, validating the effectiveness of the proposed method.", "conclusion": "BEAP-Agent provides a systematic DFS-based framework with long-range, multi-level backtracking that effectively addresses the recovery problem in GUI agents, demonstrating improved task execution accuracy on the OSWorld benchmark."}}
{"id": "2601.21167", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21167", "abs": "https://arxiv.org/abs/2601.21167", "authors": ["Shuai Liu", "Alireza Bakhtiari", "Alex Ayoub", "Botao Hao", "Csaba Szepesv\u00e1ri"], "title": "Efficient Simple Regret Algorithms for Stochastic Contextual Bandits", "comment": null, "summary": "We study stochastic contextual logistic bandits under the simple regret objective. While simple regret guarantees have been established for the linear case, no such results were previously known for the logistic setting. Building on ideas from contextual linear bandits and self-concordant analysis, we propose the first algorithm that achieves simple regret $\\tilde{\\mathcal{O}}(d/\\sqrt{T})$. Notably, the leading term of our regret bound is free of the constant $\u03ba= \\mathcal O(\\exp(S))$, where $S$ is a bound on the magnitude of the unknown parameter vector. The algorithm is shown to be fully tractable when the action set is finite. We also introduce a new variant of Thompson Sampling tailored to the simple-regret setting. This yields the first simple regret guarantee for randomized algorithms in stochastic contextual linear bandits, with regret $\\tilde{\\mathcal{O}}(d^{3/2}/\\sqrt{T})$. Extending this method to the logistic case, we obtain a similarly structured Thompson Sampling algorithm that achieves the same regret bound -- $\\tilde{\\mathcal{O}}(d^{3/2}/\\sqrt{T})$ -- again with no dependence on $\u03ba$ in the leading term. The randomized algorithms, as expected, are cheaper to run than their deterministic counterparts. Finally, we conducted a series of experiments to empirically validate these theoretical guarantees.", "AI": {"tldr": "First simple regret algorithms for stochastic contextual logistic bandits, with deterministic and randomized versions achieving bounds without dependence on exponential constant \u03ba.", "motivation": "Prior work established simple regret guarantees for linear contextual bandits but not for logistic settings, motivating the need for efficient algorithms in this more complex scenario.", "method": "Proposed a deterministic algorithm using contextual linear bandit ideas and self-concordant analysis, and a new Thompson Sampling variant tailored for simple regret.", "result": "Deterministic algorithm achieves simple regret \u00d5(d/\u221aT); randomized algorithms achieve \u00d5(d^(3/2)/\u221aT), both independent of \u03ba=O(exp(S)). Experiments validate theoretical guarantees.", "conclusion": "The paper introduces tractable algorithms for simple regret in logistic contextual bandits, removing dependence on \u03ba and providing efficient deterministic and randomized options."}}
{"id": "2601.21358", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21358", "abs": "https://arxiv.org/abs/2601.21358", "authors": ["Jiecong Wang", "Hao Peng", "Chunyang Liu"], "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization", "comment": null, "summary": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.", "AI": {"tldr": "PLaT (Planning with Latent Thoughts) is a framework that decouples reasoning from verbalization by modeling reasoning as deterministic latent planning states, allowing dynamic termination and offering a scalable foundation for inference-time search.", "motivation": "Current Chain-of-Thought (CoT) approaches are constrained by computational cost and reasoning path collapse in discrete token spaces, while existing latent reasoning methods operate as opaque end-to-end mappings with fixed inference steps.", "method": "PLaT reformulates latent reasoning as planning by decoupling reasoning from verbalization. It models reasoning as deterministic trajectories of latent planning states, with a separate Decoder for grounding thoughts into text only when necessary.", "result": "Empirical results on mathematical benchmarks show PLaT achieves lower greedy accuracy than baselines but demonstrates superior scalability in reasoning diversity, indicating it learns a robust, broader solution space.", "conclusion": "PLaT offers a transparent and scalable foundation for inference-time search by providing a framework that enables dynamic reasoning termination and robust solution space exploration beyond traditional CoT approaches."}}
{"id": "2601.21170", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21170", "abs": "https://arxiv.org/abs/2601.21170", "authors": ["Augusto Santos", "Teresa Santos", "Catarina Rodrigues", "Jos\u00e9 M. F. Moura"], "title": "The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset", "comment": null, "summary": "Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.", "AI": {"tldr": "A machine learning method for early detection of emergent phenomena by learning optimal feature representations from covariance/precision matrix powers, validated on seizure detection and churn prediction with competitive performance and interpretability.", "motivation": "To address the challenge of early detection of emergent phenomena (like epileptic seizures, customer churn, or pandemics) by unveiling and harnessing latent causal structure in complex systems where the data-generating process is unknown and partially observed.", "method": "The method learns an optimal feature representation from a one-parameter family of estimators (powers of the empirical covariance or precision matrix) and then uses a supervised learning module to classify this representation.", "result": "The approach demonstrates structural consistency and achieves competitive results in seizure detection and churn prediction. The optimal covariance power shows good identifiability and captures structural signatures, reconciling predictive performance with interpretable statistical structure.", "conclusion": "The method provides a unified approach for early detection of emergent phenomena that balances predictive performance with interpretability, offering both accurate classification and insights into the underlying causal structure."}}
{"id": "2601.21367", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21367", "abs": "https://arxiv.org/abs/2601.21367", "authors": ["Wenjia Hua", "Kejie Zhao", "Luziwei Leng", "Ran Cheng", "Yuxin Ma", "Qinghai Guo"], "title": "Hebbian Learning with Global Direction", "comment": "Accepted to ICASSP 2026", "summary": "Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja's rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.", "AI": {"tldr": "A novel Global-guided Hebbian Learning (GHL) framework that combines local Hebbian learning with global guidance signals to overcome scalability limitations of traditional Hebbian methods and achieve competitive performance with backpropagation.", "motivation": "Backpropagation has biological implausibility and high computational costs, while Hebbian learning is biologically plausible but lacks global guidance, limiting its scalability to complex tasks and large networks.", "method": "Proposes a model-agnostic GHL framework with two components: 1) Local component using Oja's rule with competitive learning for stable local updates, and 2) Global component providing sign-based signals to guide the direction of Hebbian plasticity updates.", "result": "The method consistently outperforms existing Hebbian approaches and achieves competitive results on large-scale networks and complex datasets like ImageNet, significantly narrowing the performance gap with standard backpropagation.", "conclusion": "The GHL framework successfully integrates local and global information to create a scalable, biologically-plausible alternative to backpropagation that works across diverse networks and tasks."}}
{"id": "2601.21171", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21171", "abs": "https://arxiv.org/abs/2601.21171", "authors": ["Kamal Berahmand", "Saman Forouzandeh", "Mehrnoush Mohammadi", "Parham Moradi", "Mahdi Jalili"], "title": "AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection", "comment": null, "summary": "Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.", "AI": {"tldr": "AC2L-GAD is an active counterfactual contrastive learning framework for graph anomaly detection that addresses semantic consistency in positive pairs and uninformative negative samples through strategic counterfactual reasoning with active selection.", "motivation": "Graph anomaly detection faces challenges from label scarcity and extreme class imbalance. Existing graph contrastive learning methods suffer from: 1) random augmentations breaking semantic consistency in positive pairs, and 2) naive negative sampling producing trivial, uninformative contrasts.", "method": "Combines information-theoretic active selection with counterfactual generation to identify structurally complex nodes and generate anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts. Restricts expensive counterfactual generation to a strategically selected subset to reduce computational overhead.", "result": "Reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Achieves competitive or superior performance on nine benchmark datasets, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.", "conclusion": "AC2L-GAD effectively addresses key limitations in graph contrastive learning for anomaly detection through principled counterfactual reasoning with active selection, offering both computational efficiency and improved detection performance."}}
{"id": "2601.21372", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21372", "abs": "https://arxiv.org/abs/2601.21372", "authors": ["Yang Song", "Anoushka Vyas", "Zirui Wei", "Sina Khoshfetrat Pakazad", "Henrik Ohlsson", "Graham Neubig"], "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents", "comment": null, "summary": "In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.\n  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.\n  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.", "AI": {"tldr": "NEMO translates natural language descriptions of decision problems into executable mathematical optimization implementations via autonomous coding agents, achieving SOTA performance on optimization benchmarks through execution-aware agentic architecture.", "motivation": "Existing approaches for translating natural language to optimization code rely on specialized LLMs or task-specific agents that are brittle, complex, and often generate non-executable code.", "method": "Centers on remote interaction with autonomous coding agents (ACAs) as first-class abstraction, building higher-level systems for structuring and iterative refinement. Introduces coordination patterns: asymmetric validation loops between optimizer/simulator implementations, external memory for experience reuse, robustness via MBR decoding and self-consistency.", "result": "Achieves state-of-the-art performance on majority of nine established optimization benchmarks, with substantial margins on several datasets.", "conclusion": "Demonstrates the power of execution-aware agentic architectures for automated optimization modeling, with ACAs producing executable code by construction through sandboxed environments enabling automated validation and repair."}}
{"id": "2601.21174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21174", "abs": "https://arxiv.org/abs/2601.21174", "authors": ["Yuanning Cui", "Zequn Sun", "Wei Hu", "Kexuan Xin", "Zhangjie Fu"], "title": "Breaking the Reasoning Horizon in Entity Alignment Foundation Models", "comment": null, "summary": "Entity alignment (EA) is critical for knowledge graph (KG) fusion. Existing EA models lack transferability and are incapable of aligning unseen KGs without retraining. While using graph foundation models (GFMs) offer a solution, we find that directly adapting GFMs to EA remains largely ineffective. This stems from a critical \"reasoning horizon gap\": unlike link prediction in GFMs, EA necessitates capturing long-range dependencies across sparse and heterogeneous KG structuresTo address this challenge, we propose a EA foundation model driven by a parallel encoding strategy. We utilize seed EA pairs as local anchors to guide the information flow, initializing and encoding two parallel streams simultaneously. This facilitates anchor-conditioned message passing and significantly shortens the inference trajectory by leveraging local structural proximity instead of global search. Additionally, we incorporate a merged relation graph to model global dependencies and a learnable interaction module for precise matching. Extensive experiments verify the effectiveness of our framework, highlighting its strong generalizability to unseen KGs.", "AI": {"tldr": "Proposed a transferable entity alignment foundation model using parallel encoding with local anchors to bridge the reasoning horizon gap, enabling alignment of unseen KGs without retraining.", "motivation": "Existing entity alignment models lack transferability and require retraining for unseen KGs. Direct adaptation of graph foundation models is ineffective due to a 'reasoning horizon gap'\u2014EA requires capturing long-range dependencies across sparse, heterogeneous KG structures, unlike link prediction.", "method": "A parallel encoding strategy using seed EA pairs as local anchors to guide information flow, combined with a merged relation graph for global dependencies and a learnable interaction module for precise matching.", "result": "Extensive experiments verify the framework's effectiveness and highlight its strong generalizability to unseen knowledge graphs.", "conclusion": "The proposed parallel encoding strategy effectively bridges the reasoning horizon gap for entity alignment, enabling strong generalization to unseen KGs without retraining."}}
{"id": "2601.21375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21375", "abs": "https://arxiv.org/abs/2601.21375", "authors": ["Zheng Li", "Siyao Song", "Jingyuan Ma", "Rui Li", "Ying Zeng", "Minghao Li", "Zhifang Sui"], "title": "TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models", "comment": null, "summary": "Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.", "AI": {"tldr": "A framework is proposed to evaluate LLM teaching ability through student performance improvement, showing variation across models and subjects, with limited benefit from examples.", "motivation": "Existing benchmarks focus on problem-solving, leaving knowledge-centered teaching underexplored, prompting the need for a structured evaluation of LLM teaching capabilities.", "method": "A syllabus-grounded framework is developed, restricting teacher agents to structured knowledge points and example problems to avoid information leakage, using Gaokao data across subjects.", "result": "Experiments show substantial variation in teaching effectiveness across models and domains, with challenges in physics and chemistry, and incorporation of example problems often leads to example-specific error correction rather than improvement.", "conclusion": "Teaching ability is a distinct and measurable dimension of LLM behavior, emphasizing the need for specialized evaluation frameworks."}}
{"id": "2601.21177", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21177", "abs": "https://arxiv.org/abs/2601.21177", "authors": ["Xin Peng", "Ang Gao"], "title": "Flow Perturbation++: Multi-Step Unbiased Jacobian Estimation for High-Dimensional Boltzmann Sampling", "comment": null, "summary": "The scalability of continuous normalizing flows (CNFs) for unbiased Boltzmann sampling remains limited in high-dimensional systems due to the cost of Jacobian-determinant evaluation, which requires $D$ backpropagation passes through the flow layers. Existing stochastic Jacobian estimators such as the Hutchinson trace estimator reduce computation but introduce bias, while the recently proposed Flow Perturbation method is unbiased yet suffers from high variance. We present \\textbf{Flow Perturbation++}, a variance-reduced extension of Flow Perturbation that discretizes the probability-flow ODE and performs unbiased stepwise Jacobian estimation at each integration step. This multi-step construction retains the unbiasedness of Flow Perturbation while achieves substantially lower estimator variance. Integrated into a Sequential Monte Carlo framework, Flow Perturbation++ achieves significantly improved equilibrium sampling on a 1000D Gaussian Mixture Model and the all-atom Chignolin protein compared with Hutchinson-based and single-step Flow Perturbation baselines.", "AI": {"tldr": "Flow Perturbation++ is a variance-reduced, unbiased method for scalable Boltzmann sampling using continuous normalizing flows, achieving better performance than existing approaches on high-dimensional systems.", "motivation": "Continuous normalizing flows (CNFs) for unbiased Boltzmann sampling face scalability issues in high-dimensional systems due to expensive Jacobian-determinant evaluation. Existing stochastic estimators either introduce bias (Hutchinson trace estimator) or suffer from high variance (Flow Perturbation method).", "method": "Flow Perturbation++ extends Flow Perturbation by discretizing the probability-flow ODE and performing unbiased stepwise Jacobian estimation at each integration step. This multi-step construction retains unbiasedness while achieving substantially lower estimator variance. The method is integrated into a Sequential Monte Carlo framework.", "result": "The method achieves significantly improved equilibrium sampling on a 1000D Gaussian Mixture Model and the all-atom Chignolin protein compared with Hutchinson-based and single-step Flow Perturbation baselines.", "conclusion": "Flow Perturbation++ provides a practical solution for scalable, unbiased Boltzmann sampling in high-dimensional systems by addressing the variance issues of previous methods while maintaining unbiasedness."}}
{"id": "2601.21182", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21182", "abs": "https://arxiv.org/abs/2601.21182", "authors": ["Xin Peng", "Ang Gao"], "title": "Rethinking Refinement: Correcting Generative Bias without Noise Injection", "comment": null, "summary": "Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \\textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \\textbf{state-of-the-art} FID of \\textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.", "AI": {"tldr": "The paper introduces Bi-stage Flow Refinement (BFR), a post-hoc method to correct biases in generative models using flow-matching, achieving state-of-the-art FID with minimal extra computation.", "motivation": "Generative models like diffusion and flow-based models often have systematic biases that reduce sample quality in high dimensions, needing efficient correction without altering the original sampling process.", "method": "Proposes BFR with two strategies: latent space alignment for invertible generators and data space refinement with lightweight augmentations, applying deterministic corrections without noise injection or multi-step resampling.", "result": "Experiments on MNIST, CIFAR-10, and FFHQ show improved fidelity and coverage; on MNIST, latent refinement reduces FID from 3.95 to 1.46 with just one additional function evaluation (1-NFE), maintaining diversity.", "conclusion": "BFR effectively corrects biases in generative models as a post-hoc procedure, enhancing sample quality efficiently without perturbing the original ODE trajectory, setting a new benchmark in refinement methods."}}
{"id": "2601.21414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21414", "abs": "https://arxiv.org/abs/2601.21414", "authors": ["Chenxu Yang", "Qingyi Si", "Chong Tian", "Xiyu Liu", "Dingyu Yao", "Chuanyu Qin", "Zheng Lin", "Weiping Wang", "Jiaqi Wang"], "title": "System 1&2 Synergy via Dynamic Model Interpolation", "comment": null, "summary": "Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \\textit{how models think} rather than \\textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \\textbf{DAMI} (\\textbf{D}yn\\textbf{A}mic \\textbf{M}odel \\textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $\u03bb(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.", "AI": {"tldr": "DAMI is a framework that dynamically interpolates between System 1 (fast, intuitive) and System 2 (slow, deliberative) language model checkpoints based on query-specific reasoning intensity, achieving better accuracy than System 2 models while maintaining System 1 efficiency.", "motivation": "Current approaches to make System 2 models more efficient focus on output control (limiting what models produce), which is misaligned because output length is just a symptom, not the root cause. The paper argues for shifting to capability control that modulates how models think rather than what they produce.", "method": "Leverages existing Instruct (System 1) and Thinking (System 2) checkpoints through dynamic parameter interpolation without additional training. Proposes DAMI framework that estimates query-specific Reasoning Intensity \u03bb(q) to configure cognitive depth. Uses two approaches: 1) training-based preference learning encoding accuracy and efficiency criteria, and 2) zero-shot confidence-based method leveraging inter-model cognitive discrepancy.", "result": "Experiments on five mathematical reasoning benchmarks show DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining System 1 efficiency with System 2 reasoning depth. Linear interpolation yields convex, monotonic Pareto frontier underpinned by representation continuity and structural connectivity.", "conclusion": "DAMI successfully shifts the paradigm from output control to capability control, enabling dynamic adaptation between cognitive modes through parameter interpolation. This approach allows models to balance efficiency and reasoning capability based on query requirements, achieving better performance than either System 1 or System 2 models alone."}}
