<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas](https://arxiv.org/abs/2601.19082)
*Trung-Kiet Huynh,Dao-Sy Duy-Minh,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Nguyen Lam Phu Quy,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Pham Phu Hoa,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.AI

TL;DR: Study of LLM strategic behavior in repeated social dilemmas (Prisoner's Dilemma) under varying payoffs and linguistic contexts, revealing consistent patterns, incentive sensitivity, and cross-linguistic effects, with implications for AI governance.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly acting as autonomous agents, requiring understanding of their strategic behavior for safety, coordination, and AI-driven systems in interactive and multi-agent settings.

Method: Use a payoff-scaled Prisoner's Dilemma game in repeated social dilemmas, test across models and languages, and analyze behavior with supervised classifiers trained on canonical repeated-game strategies applied to LLM decisions.

Result: Observed consistent behavioral patterns including incentive-sensitive conditional strategies and cross-linguistic divergence; classifiers revealed systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects.

Conclusion: The study provides a unified framework for auditing LLMs as strategic agents, highlights cooperation biases, and has direct implications for AI governance and multi-agent system design.

Abstract: As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.

</details>


### [2] [TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning](https://arxiv.org/abs/2601.19151)
*Patara Trirat,Jin Myung Kwak,Jay Heo,Heejun Lee,Sung Ju Hwang*

Main category: cs.AI

TL;DR: TS-Debate is a multi-agent debate framework for zero-shot time series reasoning that uses modality-specialized expert agents (text, visual, numerical) coordinated through structured debate protocols to improve accuracy while mitigating numeric hallucinations.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise in time series analysis but struggle with numeric fidelity, modality interference, and principled cross-modal integration, requiring a better approach for zero-shot reasoning.

Method: A collaborative multi-agent debate framework with dedicated expert agents for textual context, visual patterns, and numerical signals, coordinated via structured debate protocols with reviewer agents using verification-conflict-calibration mechanisms and lightweight code execution for programmatic verification.

Result: Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines including standard multimodal debate where all agents observe all inputs.

Conclusion: The modality-specialized, collaborative multi-agent debate framework effectively addresses LLM limitations in time series reasoning by preserving modality fidelity, exposing conflicting evidence, and mitigating numeric hallucinations without task-specific fine-tuning.

Abstract: Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.

</details>


### [3] [Agentic Business Process Management Systems](https://arxiv.org/abs/2601.18833)
*Marlon Dumas,Fredrik Milani,David Chapela-Campa*

Main category: cs.AI

TL;DR: This position paper outlines how generative AI and process mining enable a shift from automation to autonomy in Business Process Management, proposing a new architecture for Agentic Business Process Management Systems that integrate reasoning and learning.


<details>
  <summary>Details</summary>
Motivation: The paper responds to the rise of Generative and Agentic AI as a new wave in BPM evolution, arguing it differs from previous automation technologies by shifting focus from automation to autonomy and from design-driven to data-driven process management using process mining.

Method: The paper presents a position paper framework based on a keynote talk, proposing an architectural vision for Agentic Business Process Management Systems (A-BPMS) that integrate autonomy, reasoning, and learning into process management and execution.

Result: The paper outlines how process mining provides foundations for agents to sense process states, reason about improvement opportunities, and act to maintain/optimize performance. It proposes A-BPMS platforms supporting a continuum from human-driven to fully autonomous processes.

Conclusion: Generative and Agentic AI will redefine BPM by enabling autonomous, data-driven systems that blend human and automated process management, requiring new platforms that integrate process mining, reasoning, and learning capabilities.

Abstract: Since the early 90s, the evolution of the Business Process Management (BPM) discipline has been punctuated by successive waves of automation technologies. Some of these technologies enable the automation of individual tasks, while others focus on orchestrating the execution of end-to-end processes. The rise of Generative and Agentic Artificial Intelligence (AI) is opening the way for another such wave. However, this wave is poised to be different because it shifts the focus from automation to autonomy and from design-driven management of business processes to data-driven management, leveraging process mining techniques. This position paper, based on a keynote talk at the 2025 Workshop on AI for BPM, outlines how process mining has laid the foundations on top of which agents can sense process states, reason about improvement opportunities, and act to maintain and optimize performance. The paper proposes an architectural vision for Agentic Business Process Management Systems (A-BPMS): a new class of platforms that integrate autonomy, reasoning, and learning into process management and execution. The paper contends that such systems must support a continuum of processes, spanning from human-driven to fully autonomous, thus redefining the boundaries of process automation and governance.

</details>


### [4] [LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties](https://arxiv.org/abs/2601.18846)
*Urban Skvorc,Niki van Stein,Moritz Seiler,Britta Grimme,Thomas BÃ¤ck,Heike Trautmann*

Main category: cs.AI

TL;DR: LLaMEA framework uses LLMs in an evolutionary loop to generate benchmark optimization problems with specific landscape characteristics, expanding BBOB's diversity.


<details>
  <summary>Details</summary>
Motivation: Benchmarking in continuous black-box optimisation is limited by structural diversity in existing test suites like BBOB, needing more varied problems for analysis.

Method: Guide an LLM via LLaMEA to generate problem code from natural-language descriptions, score with ELA-based predictors, use ELA-space fitness-sharing for diversity.

Result: Generated functions exhibit intended traits like multimodality, verified by basin-of-attraction analysis, statistical testing, and visual inspection; t-SNE shows expansion of BBOB instance space.

Conclusion: Resulting library provides broad, interpretable benchmark problems for landscape analysis and automated algorithm selection.

Abstract: Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape characteristics. Using the LLaMEA framework, we guide an LLM to generate problem code from natural-language descriptions of target properties, including multimodality, separability, basin-size homogeneity, search-space homogeneity and globallocal optima contrast. Inside the loop we score candidates through ELA-based property predictors. We introduce an ELA-space fitness-sharing mechanism that increases population diversity and steers the generator away from redundant landscapes. A complementary basin-of-attraction analysis, statistical testing and visual inspection, verifies that many of the generated functions indeed exhibit the intended structural traits. In addition, a t-SNE embedding shows that they expand the BBOB instance space rather than forming an unrelated cluster. The resulting library provides a broad, interpretable, and reproducible set of benchmark problems for landscape analysis and downstream tasks such as automated algorithm selection.

</details>


### [5] [Explainable Uncertainty Quantification for Wastewater Treatment Energy Prediction via Interval Type-2 Neuro-Fuzzy System](https://arxiv.org/abs/2601.18897)
*Qusai Khaled,Bahjat Mallak,Uzay Kaymak,Laura Genga*

Main category: cs.AI

TL;DR: Developed an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) for interpretable uncertainty quantification to make accurate energy forecasting for wastewater treatment more transparent.


<details>
  <summary>Details</summary>
Motivation: Current machine learning models for energy forecasting in wastewater treatment lack explainable uncertainty information, which is needed for risk-aware decisions in safety-critical infrastructure.

Method: IT2-ANFIS uses a fuzzy rule-based approach to generate prediction intervals. It decomposes uncertainty across three levels: feature-level, rule-level, and instance-level.

Result: Validated on the Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS offers predictive performance similar to first-order ANFIS with lower variance, while providing interpretable uncertainty estimates tied to operational conditions and input variables.

Conclusion: The IT2-ANFIS framework enhances energy forecasting in wastewater treatment by adding reliable, explainable uncertainty quantification, aiding operational optimization and sustainability.

Abstract: Wastewater treatment plants consume 1-3% of global electricity, making accurate energy forecasting critical for operational optimization and sustainability. While machine learning models provide point predictions, they lack explainable uncertainty quantification essential for risk-aware decision-making in safety-critical infrastructure. This study develops an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that generates interpretable prediction intervals through fuzzy rule structures. Unlike black-box probabilistic methods, the proposed framework decomposes uncertainty across three levels: feature-level, footprint of uncertainty identify which variables introduce ambiguity, rule-level analysis reveals confidence in local models, and instance-level intervals quantify overall prediction uncertainty. Validated on Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS achieves comparable predictive performance to first order ANFIS with substantially reduced variance across training runs, while providing explainable uncertainty estimates that link prediction confidence directly to operational conditions and input variables.

</details>


### [6] [RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures](https://arxiv.org/abs/2601.18924)
*Andrew Jaffe,Noah Reicin,Jinho D. Choi*

Main category: cs.AI

TL;DR: RIFT (Reordered Instruction Following Testbed) reveals that LLMs show up to 72% performance drop on jumping prompts versus linear prompts, exposing fundamental limitations in handling non-sequential instructions.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on LLM performance. There's a need to assess how well LLMs can follow instructions when the order isn't linear.

Method: Introduces RIFT testbed using rephrased Jeopardy! question-answer pairs. Tests LLMs across two prompt structures: linear prompts (sequential progression) and jumping prompts (identical content but requiring non-sequential traversal). Conducts 10,000 evaluations across six state-of-the-art open-source LLMs.

Result: Accuracy dropped by up to 72% under jumping conditions compared to baseline linear prompts. Approximately 50% of failures stem from instruction-order violations and semantic drift. Shows strong dependence on positional continuity.

Conclusion: Current LLM architectures internalize instruction following as a sequential pattern rather than a reasoning skill. Structural sensitivity is a fundamental limitation with implications for workflow automation and multi-agent systems requiring non-sequential control flow.

Abstract: Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.

</details>


### [7] [Neural Theorem Proving for Verification Conditions: A Real-World Benchmark](https://arxiv.org/abs/2601.18944)
*Qiyuan Xu,Xiaokun Luan,Renxi Wang,Joshua Ong Jun Leang,Peixin Wang,Haonan Li,Wenda Li,Conrad Watt*

Main category: cs.AI

TL;DR: This paper introduces NTP4VC, the first real-world multi-language benchmark for neural theorem proving of verification conditions, evaluating LLMs on VC proving from real-world projects like Linux and Contiki-OS kernels.


<details>
  <summary>Details</summary>
Motivation: Automated proof of verification conditions is a major bottleneck in program verification, with hard VCs often requiring extensive manual proofs. While neural theorem proving has shown success in mathematical domains, its application to program verification VC proving remains unexplored, with no specific benchmarks for this fundamental task.

Method: The authors create NTP4VC benchmark using real-world projects (Linux and Contiki-OS kernels) and industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across three formal languages: Isabelle, Lean, and Rocq. They evaluate both general-purpose and theorem-proving fine-tuned large language models on this benchmark.

Result: Results show that while large language models demonstrate promise in verification condition proving, significant challenges remain for program verification applications, revealing a substantial gap between current capabilities and practical needs.

Conclusion: The paper establishes the first benchmark for neural theorem proving of verification conditions, highlighting both the potential of LLMs for this task and the considerable challenges that remain, identifying an important research opportunity in bridging this gap for practical program verification.

Abstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [NavFormer: IGRF Forecasting in Moving Coordinate Frames](https://arxiv.org/abs/2601.18800)
*Yoontae Hwang,Dongwoo Lee,Minseok Choi,Yong Sup Ihn,Daham Kim,Deok-Young Lee*

Main category: cs.LG

TL;DR: A model, NavFormer, uses rotation invariant scalar features and a Canonical SPD module to forecast the invariant IGRF total intensity target from triad magnetometer data, outperforming baselines across various training scenarios with open-source code.


<details>
  <summary>Details</summary>
Motivation: Triad magnetometer components vary with sensor attitude, but the IGRF total intensity target remains invariant; there is a need to forecast this invariant target robustly despite these variations, which existing methods may not fully address.

Method: NavFormer incorporates rotation invariant scalar features and a Canonical SPD module that stabilizes the spectrum of window-level second moments of the triad components without sign discontinuities by building a canonical frame from a Gram matrix per window and applying state-dependent spectral scaling in the original coordinates.

Result: Experiments across five flights show that NavFormer achieves lower error than strong baselines in standard training, few-shot training, and zero-shot transfer scenarios.

Conclusion: NavFormer is an effective method for robust IGRF forecasting in autonomous navigation, demonstrating superior performance and generalization capabilities, with open-source availability for further use and research.

Abstract: Triad magnetometer components change with sensor attitude even when the IGRF total intensity target stays invariant. NavFormer forecasts this invariant target with rotation invariant scalar features and a Canonical SPD module that stabilizes the spectrum of window level second moments of the triads without sign discontinuities. The module builds a canonical frame from a Gram matrix per window and applies state dependent spectral scaling in the original coordinates. Experiments across five flights show lower error than strong baselines in standard training, few shot training, and zero shot transfer. The code is available at: https://anonymous.4open.science/r/NavFormer-Robust-IGRF-Forecasting-for-Autonomous-Navigators-0765

</details>


### [9] [Latent Structural Similarity Networks for Unsupervised Discovery in Multivariate Time Series](https://arxiv.org/abs/2601.18803)
*Olusegun Owoeye*

Main category: cs.LG

TL;DR: The paper proposes a task-agnostic discovery method that constructs relational graphs over time series entities using unsupervised representation learning and similarity thresholding, focusing on relationship discovery rather than prediction optimization.


<details>
  <summary>Details</summary>
Motivation: Current multivariate time series analysis often assumes specific task objectives (prediction, trading, etc.), linearity, or stationarity. The authors aim to develop a more flexible, task-agnostic approach that can discover relationships between entities without these assumptions, serving as a discovery layer rather than an optimized predictive model.

Method: The method uses an unsupervised sequence-to-sequence autoencoder to learn window-level representations from multivariate time series. These representations are aggregated into entity-level embeddings. A sparse similarity network is then induced by thresholding a latent-space similarity measure, creating an analyzable relational graph that compresses the pairwise search space.

Result: The framework is demonstrated on hourly cryptocurrency returns data, showing that latent similarity induces coherent network structure. A classical econometric relation is used as an external diagnostic to contextualize discovered edges, validating that the method can reveal meaningful relationships without task-specific optimization.

Conclusion: The proposed discovery layer provides a task-agnostic abstraction for exploring relationships in multivariate time series, serving as a compressed representation that exposes candidate relationships for further investigation rather than as an optimized predictive model.

Abstract: This paper proposes a task-agnostic discovery layer for multivariate time series that constructs a relational hypothesis graph over entities without assuming linearity, stationarity, or a downstream objective. The method learns window-level sequence representations using an unsupervised sequence-to-sequence autoencoder, aggregates these representations into entity-level embeddings, and induces a sparse similarity network by thresholding a latent-space similarity measure. This network is intended as an analyzable abstraction that compresses the pairwise search space and exposes candidate relationships for further investigation, rather than as a model optimized for prediction, trading, or any decision rule. The framework is demonstrated on a challenging real-world dataset of hourly cryptocurrency returns, illustrating how latent similarity induces coherent network structure; a classical econometric relation is also reported as an external diagnostic lens to contextualize discovered edges.

</details>


### [10] [Variational Quantum Circuit-Based Reinforcement Learning for Dynamic Portfolio Optimization](https://arxiv.org/abs/2601.18811)
*Vincent Gurgul,Ying Chen,Stefan Lessmann*

Main category: cs.LG

TL;DR: This paper proposes Quantum Reinforcement Learning (QRL) using Variational Quantum Circuits for dynamic portfolio optimization, showing comparable or better performance than classical Deep RL with fewer parameters, but faces practical latency issues in cloud deployment.


<details>
  <summary>Details</summary>
Motivation: To address dynamic portfolio optimization in financial markets using quantum computing, exploiting potential advantages in parameter efficiency and robustness over classical reinforcement learning methods.

Method: Implements quantum analogues of Deep Deterministic Policy Gradient and Deep Q-Network algorithms via Variational Quantum Circuits, and evaluates on real-world financial data.

Result: Quantum agents achieve risk-adjusted performance comparable to or exceeding classical Deep RL models with orders of magnitude fewer parameters, with reduced variability across market regimes, though practical deployment suffers from latency due to infrastructural overhead.

Conclusion: QRL is theoretically competitive with state-of-the-art classical reinforcement learning and may become practically advantageous as deployment overheads diminish, positioning it as promising for complex environments like financial markets.

Abstract: This paper presents a Quantum Reinforcement Learning (QRL) solution to the dynamic portfolio optimization problem based on Variational Quantum Circuits. The implemented QRL approaches are quantum analogues of the classical neural-network-based Deep Deterministic Policy Gradient and Deep Q-Network algorithms. Through an empirical evaluation on real-world financial data, we show that our quantum agents achieve risk-adjusted performance comparable to, and in some cases exceeding, that of classical Deep RL models with several orders of magnitude more parameters. In addition to improved parameter efficiency, quantum agents exhibit reduced variability across market regimes, indicating robust behaviour under changing conditions. However, while quantum circuit execution is inherently fast at the hardware level, practical deployment on cloud-based quantum systems introduces substantial latency, making end-to-end runtime currently dominated by infrastructural overhead and limiting practical applicability. Taken together, our results suggest that QRL is theoretically competitive with state-of-the-art classical reinforcement learning and may become practically advantageous as deployment overheads diminish. This positions QRL as a promising paradigm for dynamic decision-making in complex, high-dimensional, and non-stationary environments such as financial markets. The complete codebase is released as open source at: https://github.com/VincentGurgul/qrl-dpo-public

</details>


### [11] [VAE with Hyperspherical Coordinates: Improving Anomaly Detection from Hypervolume-Compressed Latent Space](https://arxiv.org/abs/2601.18823)
*Alejandro Ascarate,Leo Lebrat,Rodrigo Santa Cruz,Clinton Fookes,Olivier Salvado*

Main category: cs.LG

TL;DR: VAEs face issues detecting anomalies in high-dimensional latent spaces; using hyperspherical coordinates improves anomaly detection by compressing latent vectors.


<details>
  <summary>Details</summary>
Motivation: High-dimensional latent spaces in VAEs cause hypervolume growth and latent vectors clustering on hypersphere equators, hindering anomaly detection.

Method: Formulate VAE latent variables with hyperspherical coordinates to compress latent vectors towards a direction on the hypersphere, enhancing posterior expressiveness.

Result: Improved fully unsupervised and OOD anomaly detection performance, outperforming existing methods on datasets like Mars Rover images, Galaxy images, Cifar10, and ImageNet subsets.

Conclusion: Hyperspherical coordinates in VAEs address high-dimensional limitations, enhancing anomaly detection capabilities in complex real-world datasets.

Abstract: Variational autoencoders (VAE) encode data into lower-dimensional latent vectors before decoding those vectors back to data. Once trained, one can hope to detect out-of-distribution (abnormal) latent vectors, but several issues arise when the latent space is high dimensional. This includes an exponential growth of the hypervolume with the dimension, which severely affects the generative capacity of the VAE. In this paper, we draw insights from high dimensional statistics: in these regimes, the latent vectors of a standard VAE are distributed on the `equators' of a hypersphere, challenging the detection of anomalies. We propose to formulate the latent variables of a VAE using hyperspherical coordinates, which allows compressing the latent vectors towards a given direction on the hypersphere, thereby allowing for a more expressive approximate posterior. We show that this improves both the fully unsupervised and OOD anomaly detection ability of the VAE, achieving the best performance on the datasets we considered, outperforming existing methods. For the unsupervised and OOD modalities, respectively, these are: i) detecting unusual landscape from the Mars Rover camera and unusual Galaxies from ground based imagery (complex, real world datasets); ii) standard benchmarks like Cifar10 and subsets of ImageNet as the in-distribution (ID) class.

</details>


### [12] [IPBC: An Interactive Projection-Based Framework for Human-in-the-Loop Semi-Supervised Clustering of High-Dimensional Data](https://arxiv.org/abs/2601.18828)
*Mohammad Zare*

Main category: cs.LG

TL;DR: IPBC enhances clustering through interactive visual feedback, integrating human intuition with machine learning for improved results in high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: High-dimensional datasets are hard to cluster due to distance metric issues and cluster collapse or overlap in lower dimensions; traditional static embeddings lack interpretability and user interaction.

Method: Proposes Interactive Project-Based Clustering (IPBC), combining nonlinear projection with a user feedback loop for adjusting embeddings and constraints like must-link/cannot-link relationships.

Result: Experiments show that few interactive refinement steps significantly improve cluster quality on benchmark datasets.

Conclusion: IPBC transforms clustering into a collaborative discovery process where machine representation and human insight reinforce each other.

Abstract: High-dimensional datasets are increasingly common across scientific and industrial domains, yet they remain difficult to cluster effectively due to the diminishing usefulness of distance metrics and the tendency of clusters to collapse or overlap when projected into lower dimensions. Traditional dimensionality reduction techniques generate static 2D or 3D embeddings that provide limited interpretability and do not offer a mechanism to leverage the analyst's intuition during exploration. To address this gap, we propose Interactive Project-Based Clustering (IPBC), a framework that reframes clustering as an iterative human-guided visual analysis process. IPBC integrates a nonlinear projection module with a feedback loop that allows users to modify the embedding by adjusting viewing angles and supplying simple constraints such as must-link or cannot-link relationships. These constraints reshape the objective of the projection model, gradually pulling semantically related points closer together and pushing unrelated points further apart. As the projection becomes more structured and expressive through user interaction, a conventional clustering algorithm operating on the optimized 2D layout can more reliably identify distinct groups. An additional explainability component then maps each discovered cluster back to the original feature space, producing interpretable rules or feature rankings that highlight what distinguishes each cluster. Experiments on various benchmark datasets show that only a small number of interactive refinement steps can substantially improve cluster quality. Overall, IPBC turns clustering into a collaborative discovery process in which machine representation and human insight reinforce one another.

</details>


### [13] [CP Loss: Channel-wise Perceptual Loss for Time Series Forecasting](https://arxiv.org/abs/2601.18829)
*Yaohua Zha,Chunlin Fan,Peiyuan Liu,Yong Jiang,Tao Dai,Hai Wu,Shu-Tao Xia*

Main category: cs.LG

TL;DR: This paper proposes a Channel-wise Perceptual Loss (CP Loss) that learns unique perceptual spaces for each channel of multi-channel time-series data to better capture channel-specific dynamics, unlike traditional channel-agnostic loss functions like MSE.


<details>
  <summary>Details</summary>
Motivation: Multi-channel time-series data has significant heterogeneity across channels, but existing forecasting models use channel-agnostic loss functions like MSE which apply uniform metrics across all channels. This fails to capture channel-specific dynamics such as sharp fluctuations or trend shifts.

Method: Propose Channel-wise Perceptual Loss (CP Loss) that learns unique perceptual spaces for each channel adapted to their characteristics. Design a learnable channel-wise filter that decomposes raw signals into disentangled multi-scale representations to form the perceptual space basis. The filter is optimized jointly with the main forecasting model to ensure the learned perceptual space is explicitly oriented towards prediction. Losses are calculated within these perception spaces to optimize the model.

Result: The proposed CP Loss method learns channel-specific perceptual spaces through joint optimization of learnable channel-wise filters with the main forecasting model, enabling better capture of channel-specific dynamics in multi-channel time-series forecasting.

Conclusion: Channel-wise Perceptual Loss addresses the limitations of channel-agnostic loss functions by learning channel-specific perceptual spaces that are optimized for prediction tasks, potentially improving forecasting performance for heterogeneous multi-channel time-series data.

Abstract: Multi-channel time-series data, prevalent across diverse applications, is characterized by significant heterogeneity in its different channels. However, existing forecasting models are typically guided by channel-agnostic loss functions like MSE, which apply a uniform metric across all channels. This often leads to fail to capture channel-specific dynamics such as sharp fluctuations or trend shifts. To address this, we propose a Channel-wise Perceptual Loss (CP Loss). Its core idea is to learn a unique perceptual space for each channel that is adapted to its characteristics, and to compute the loss within this space. Specifically, we first design a learnable channel-wise filter that decomposes the raw signal into disentangled multi-scale representations, which form the basis of our perceptual space. Crucially, the filter is optimized jointly with the main forecasting model, ensuring that the learned perceptual space is explicitly oriented towards the prediction task. Finally, losses are calculated within these perception spaces to optimize the model. Code is available at https://github.com/zyh16143998882/CP_Loss.

</details>


### [14] [How Much Temporal Modeling is Enough? A Systematic Study of Hybrid CNN-RNN Architectures for Multi-Label ECG Classification](https://arxiv.org/abs/2601.18830)
*Alireza Jafari,Fatemeh Jafari*

Main category: cs.LG

TL;DR: A CNN combined with a single BiLSTM layer performs best for multi-label ECG classification, balancing performance and complexity; deeper recurrent models add minimal benefit and can overfit.


<details>
  <summary>Details</summary>
Motivation: Accurate multi-label ECG classification is hard due to multiple coexisting conditions, class imbalance, and long-range dependencies in multi-lead recordings, and the necessity of complex deep recurrent architectures hasn't been rigorously studied.

Method: Systematic comparison of CNN with various recurrent configurations (LSTM, GRU, BiLSTM, stacked variants) on PTB-XL dataset with 23 diagnostic categories, using CNN as baseline for morphology and integrating recurrent layers for temporal modeling.

Result: CNN with single BiLSTM layer achieves best trade-off with lowest Hamming loss (0.0338), highest macro-AUPRC (0.4715), micro-F1 score (0.6979), and subset accuracy (0.5723); stacked recurrences sometimes improve recall for rare classes but often reduce precision and generalizability.

Conclusion: Architectural alignment with ECG's temporal structure matters more than increased recurrent depth for robust performance and clinical deployment; deeper models show diminishing returns and overfitting risks.

Abstract: Accurate multi-label classification of electrocardiogram (ECG) signals remains challenging due to the coexistence of multiple cardiac conditions, pronounced class imbalance, and long-range temporal dependencies in multi-lead recordings. Although recent studies increasingly rely on deep and stacked recurrent architectures, the necessity and clinical justification of such architectural complexity have not been rigorously examined. In this work, we perform a systematic comparative evaluation of convolutional neural networks (CNNs) combined with multiple recurrent configurations, including LSTM, GRU, Bidirectional LSTM (BiLSTM), and their stacked variants, for multi-label ECG classification on the PTB-XL dataset comprising 23 diagnostic categories. The CNN component serves as a morphology-driven baseline, while recurrent layers are progressively integrated to assess their contribution to temporal modeling and generalization performance. Experimental results indicate that a CNN integrated with a single BiLSTM layer achieves the most favorable trade-off between predictive performance and model complexity. This configuration attains superior Hamming loss (0.0338), macro-AUPRC (0.4715), micro-F1 score (0.6979), and subset accuracy (0.5723) compared with deeper recurrent combinations. Although stacked recurrent models occasionally improve recall for specific rare classes, our results provide empirical evidence that increasing recurrent depth yields diminishing returns and may degrade generalization due to reduced precision and overfitting. These findings suggest that architectural alignment with the intrinsic temporal structure of ECG signals, rather than increased recurrent depth, is a key determinant of robust performance and clinically relevant deployment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [15] [Reimagining Peer Review Process Through Multi-Agent Mechanism Design](https://arxiv.org/abs/2601.19778)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.MA

TL;DR: This position paper argues that peer review failure in software engineering research is a mechanism design problem that can be addressed through multi-agent reinforcement learning and computational solutions, proposing interventions like credit-based submission economy and optimized reviewer assignment.


<details>
  <summary>Details</summary>
Motivation: The software engineering research community faces a systemic crisis where peer review is failing due to growing submissions, misaligned incentives, and reviewer fatigue, with researchers perceiving the process as "broken."

Method: Modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. Proposed interventions include: credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency.

Result: The paper outlines threat models, equity considerations, and phased pilot metrics to evaluate proposed solutions. It charts a research agenda toward sustainable peer review using computational approaches.

Conclusion: Peer review dysfunctions in software engineering research are mechanism design failures that can be addressed through computational solutions, specifically by modeling the community as a multi-agent system and applying reinforcement learning to design better incentive structures and review protocols.

Abstract: The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as "broken." This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.

</details>
