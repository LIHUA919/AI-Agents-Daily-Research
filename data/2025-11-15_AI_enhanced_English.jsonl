{"id": "2511.10030", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10030", "abs": "https://arxiv.org/abs/2511.10030", "authors": ["Tao Jiang", "Zichuan Lin", "Lihe Li", "Yi-Chen Li", "Cong Guan", "Lei Yuan", "Zongzhang Zhang", "Yang Yu", "Deheng Ye"], "title": "Multi-agent In-context Coordination via Decentralized Memory Retrieval", "comment": null, "summary": "Large transformer models, trained on diverse datasets, have demonstrated impressive few-shot performance on previously unseen tasks without requiring parameter updates. This capability has also been explored in Reinforcement Learning (RL), where agents interact with the environment to retrieve context and maximize cumulative rewards, showcasing strong adaptability in complex settings. However, in cooperative Multi-Agent Reinforcement Learning (MARL), where agents must coordinate toward a shared goal, decentralized policy deployment can lead to mismatches in task alignment and reward assignment, limiting the efficiency of policy adaptation. To address this challenge, we introduce Multi-agent In-context Coordination via Decentralized Memory Retrieval (MAICC), a novel approach designed to enhance coordination by fast adaptation. Our method involves training a centralized embedding model to capture fine-grained trajectory representations, followed by decentralized models that approximate the centralized one to obtain team-level task information. Based on the learned embeddings, relevant trajectories are retrieved as context, which, combined with the agents' current sub-trajectories, inform decision-making. During decentralized execution, we introduce a novel memory mechanism that effectively balances test-time online data with offline memory. Based on the constructed memory, we propose a hybrid utility score that incorporates both individual- and team-level returns, ensuring credit assignment across agents. Extensive experiments on cooperative MARL benchmarks, including Level-Based Foraging (LBF) and SMAC (v1/v2), show that MAICC enables faster adaptation to unseen tasks compared to existing methods. Code is available at https://github.com/LAMDA-RL/MAICC.", "AI": {"tldr": "Introduces MAICC algorithm for improved coordination in multi-agent RL through decentralized memory retrieval and hybrid utility scoring", "motivation": "Decentralized policy deployment in cooperative MARL causes task alignment and reward assignment mismatches, limiting adaptation efficiency", "method": "Trains central embedding model for trajectory representations, then decentralized models approximating it to get team-level info; retrieves relevant trajectories as context with novel memory mechanism balancing online/offline data", "result": "MAICC achieves faster adaptation to unseen tasks in benchmarks like LBF and SMAC compared to existing methods", "conclusion": "The approach effectively enhances coordination through fast adaptation in cooperative MARL settings"}}
{"id": "2511.10283", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.10283", "abs": "https://arxiv.org/abs/2511.10283", "authors": ["Won Ik Cho", "Woonghee Han", "Kyung Seo Ki", "Young Min Kim"], "title": "Behavior Modeling for Training-free Building of Private Domain Multi Agent System", "comment": "10 pages, 1 figure, 2 tables", "summary": "The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems.", "AI": {"tldr": "A training-free framework for private-domain multi-agent systems using behavior modeling and documentation to overcome domain adaptation challenges.", "motivation": "Applying open-domain agentic systems to private domains is difficult due to heterogeneous tool formats, domain-specific jargon, API restrictions, and complex governance. Conventional fine-tuning approaches are burdensome, brittle, and risk general performance degradation.", "method": "Behavior modeling and documentation-based framework with structured specifications and domain-informed instructions, using an orchestrator, tool-calling agent, and general chat agent.", "result": "The framework enables scalable adaptation to private tools and evolving contexts, supports practical use cases including lightweight deployment, API specification retrieval, and synthetic dialogue generation for evaluation.", "conclusion": "The framework provides a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems, enabling scalable adaptation without continual retraining."}}
