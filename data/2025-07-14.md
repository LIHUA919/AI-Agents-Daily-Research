<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: The paper explores whether AI can exhibit creativity by reviewing historical and contemporary perspectives from psychology, cognitive neuroscience, and philosophy.


<details>
  <summary>Details</summary>
Motivation: To address the central question of AI's potential for creativity by integrating insights from multiple disciplines.

Method: Reviewing historical perspectives, analyzing definitions of creativity, and examining responses from naturalism and cognitive neuroscience.

Result: The paper synthesizes interdisciplinary views but does not conclude definitively on AI's creativity.

Conclusion: The study highlights the complexity of defining and attributing creativity, especially in AI, calling for further interdisciplinary research.

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [2] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: A system called TableReasoner is introduced for table question answering (TQA), addressing challenges like large table size and entity ambiguity using a large language model (LLM) and programming-based framework. It achieves top performance in SemEval-2025 Task 8.


<details>
  <summary>Details</summary>
Motivation: TQA tasks are challenging due to real-world tabular data issues like large size, incomplete column semantics, and entity ambiguity.

Method: Proposes TableReasoner, an LLM-powered framework with a multi-step schema linking plan and iterative thinking architecture for holistic table understanding.

Result: Achieves first place in both subtasks of SemEval-2025 Task 8.

Conclusion: TableReasoner effectively addresses TQA challenges by combining structural and semantic representations, enabling precise and efficient table reasoning.

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [3] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: A dynamic Stackelberg game framework models LLM jailbreaking interactions, proposing the 'Purple Agent' for proactive defense using RRT.


<details>
  <summary>Details</summary>
Motivation: Addressing the growing concern of jailbreaking in LLMs, where adversaries bypass safety mechanisms.

Method: Uses a sequential extensive-form game framework with the defender as the leader. The 'Purple Agent' employs RRT for adversarial exploration and proactive defense.

Result: Provides a principled method to analyze adversarial dynamics and mitigate jailbreaking risks.

Conclusion: The framework and Purple Agent offer a robust solution for safeguarding LLMs against adversarial manipulation.

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [4] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: The LLM-Nash framework models strategic interaction using LLMs, focusing on bounded rationality and prompt-based reasoning equilibria, diverging from classical Nash outcomes.


<details>
  <summary>Details</summary>
Motivation: To study strategic interactions in LLM-enabled systems by explicitly modeling bounded rationality and reasoning processes, addressing limitations of classical game theory.

Method: Agents select reasoning prompts for LLM-guided decision-making, with equilibrium defined over the prompt space and actions derived from LLM inference.

Result: The framework demonstrates how reasoning equilibria can differ from classical Nash outcomes, highlighting cognitive constraints and mindset expressiveness.

Conclusion: LLM-Nash provides a novel foundation for analyzing strategic interactions in systems powered by LLMs, emphasizing bounded rationality and reasoning dynamics.

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [5] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: The paper explores how intelligent agents balance curiosity (seeking knowledge) and competence (controlling the environment) using model-based agents with handcrafted or learned representations.


<details>
  <summary>Details</summary>
Motivation: Understanding the interplay between curiosity and competence in intelligent agents, inspired by cognitive theories and reinforcement learning.

Method: Comparison of two model-based agents: one with handcrafted state abstractions (Tabular) and another learning an internal world model (Dreamer).

Result: Tabular agent shows distinct exploration patterns for curiosity and competence, while Dreamer reveals a two-way interaction between exploration and representation learning.

Conclusion: The study formalizes adaptive exploration as balancing the unknown and controllable, offering insights for cognitive theories and efficient reinforcement learning.

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [6] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: The paper proposes a parametrized family of grounding methods for Neural-Symbolic (NeSy) systems, balancing expressiveness and scalability by generalizing Backward Chaining.


<details>
  <summary>Details</summary>
Motivation: Existing NeSy methods face a trade-off between exhaustive grounding (combinatorial explosion) and heuristic-based grounding (lack of guarantees). This work aims to address this gap.

Method: Introduces a parametrized family of grounding methods inspired by multi-hop symbolic reasoning, generalizing Backward Chaining to control expressiveness vs. scalability.

Result: Experimental results show the grounding criterion's importance, often matching the NeSy method's impact.

Conclusion: The proposed grounding methods offer a flexible trade-off between logic expressiveness and computational efficiency, enhancing NeSy scalability.

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [7] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: A novel multimodal quantum federated learning (QFL) approach using quantum entanglement is introduced, addressing missing modalities with a Missing Modality Agnostic (MMA) mechanism, improving accuracy by 6.84% (IID) and 7.25% (non-IID).


<details>
  <summary>Details</summary>
Motivation: Existing QFL frameworks are limited to unimodal systems, while real-world tasks often involve multiple modalities. This work fills the gap by proposing a multimodal QFL solution.

Method: The approach uses intermediate fusion with quantum entanglement and introduces MMA to handle missing modalities by isolating untrained quantum circuits.

Result: Simulations show accuracy improvements of 6.84% (IID) and 7.25% (non-IID) over state-of-the-art methods.

Conclusion: The proposed multimodal QFL with MMA effectively addresses missing modality challenges and outperforms existing methods.

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [8] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: This paper warns of new AI harm vectors from giving AI agents access to cryptocurrencies and smart contracts, urging more research to mitigate risks.


<details>
  <summary>Details</summary>
Motivation: Growing interest in AI agents using cryptocurrencies and smart contracts raises concerns about potential new harm vectors.

Method: The paper examines unique properties of cryptocurrencies and smart contracts that enable harm, then details these vectors.

Result: Identifies specific new harm vectors from AI-crypto interactions.

Conclusion: Calls for technical research to prevent and mitigate these harms, ensuring safer AI-crypto integration.

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [9] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: The paper reviews abductive reasoning in epistemology, science, and design, analyzes computational implementations, and identifies gaps in generating creative hypotheses.


<details>
  <summary>Details</summary>
Motivation: To understand how abductive reasoning is discussed and implemented across domains and to highlight limitations in generating creative hypotheses.

Method: Review of theoretical accounts and analysis of computational systems using abductive reasoning.

Result: Theoretical frameworks lack models for creative hypotheses, and computational systems mostly use syllogistic forms.

Conclusion: Future research should focus on advancing creative abductive reasoning in computational systems.

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [10] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: A unified safety-alignment framework for tool-using LLM agents addresses threats from adversarial prompts and malicious tool outputs via structured reasoning and sandboxed reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Autonomous LLM agents with tool usage face new safety risks beyond conversational misuse, requiring a robust framework to handle user-initiated and tool-initiated threats.

Method: Proposes a tri-modal taxonomy (benign, malicious, sensitive) for prompts/tool responses, a policy-driven decision model, and a sandbox environment for fine-grained reward shaping.

Result: Safety-aligned agents show improved resistance to threats while maintaining utility on benign tasks, as validated on benchmarks like Agent SafetyBench and InjecAgent.

Conclusion: The framework successfully balances safety and effectiveness, enabling trustworthy deployment of autonomous LLM agents.

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [11] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: M2-Reasoning-7B improves multimodal reasoning with dynamic spatial interactions via a novel data pipeline and dynamic training strategy, achieving SOTA on 8 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in dynamic spatial interactions in MLLMs, essential for real-world applications.

Method: Integrates a high-quality data pipeline (294.2K samples) and dynamic multi-task training with step-wise optimization and task-specific rewards.

Result: Sets new SOTA across 8 benchmarks, excelling in general and spatial reasoning.

Conclusion: M2-Reasoning-7B bridges the gap in dynamic spatial reasoning, enhancing MLLM capabilities.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [12] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: A framework using an ethics advocate agent in a multi-agent LLM setting to generate ethics requirements drafts, showing promise but needing human feedback for reliability.


<details>
  <summary>Details</summary>
Motivation: Ethics requirements are often overlooked in elicitation due to time, resource constraints, and low priority, despite their importance for ethically aligned systems.

Method: Proposes a framework with an ethics advocate agent in a multi-agent LLM setting to critique and generate ethics requirements from system descriptions.

Result: Case studies show the framework captures most ethics requirements from interviews and adds new ones, but reliability issues persist.

Conclusion: The framework aids ethics integration in requirements engineering but requires human oversight for sensitive ethical considerations.

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [13] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: The paper defines canonical problems for contrastive explanations ("Why P but not Q?") in propositional logic, analyzes their properties, computational complexities, and implements them using answer set programming.


<details>
  <summary>Details</summary>
Motivation: To formalize and analyze contrastive explanations, explicitly comparing differences between P and Q, and to extend existing frameworks with minimal cardinality.

Method: Defines problems in propositional logic, analyzes properties and computational complexities, and implements solutions for CNF-formulas using answer set programming.

Result: The framework captures minimal-cardinality contrastive explanations and provides computational complexity insights. Implementation demonstrates practical applicability.

Conclusion: The study advances contrastive explanation theory, offering formal definitions, complexity analysis, and practical tools for CNF-formulas.

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [14] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: A bi-level framework maps natural language to logic via task abstraction and logic generation, outperforming baselines by up to 40% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between unstructured language and formal logic for systematic reasoning.

Method: Two-stage process: high-level task abstraction (problem type, objectives, constraints) and low-level logic generation (executable programs). Optimized end-to-end.

Result: 40% accuracy gain on benchmarks; modular reasoning, domain generalization, and enhanced transparency.

Conclusion: The bi-level framework advances trustworthy, interpretable reasoning with LLMs.

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [15] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: A framework combining multi-granularity sparse activation and a hierarchical knowledge graph improves rare-disease diagnosis, achieving near-clinical accuracy and expert-validated quality.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in knowledge representation, concept understanding, and clinical reasoning for rare-disease diagnosis.

Method: Uses multi-granularity sparse activation, a hierarchical knowledge graph, four matching algorithms, diversity control, and a fallback strategy.

Result: BLEU gains of 0.09, ROUGE gains of 0.05, accuracy gains of 0.12, and peak accuracy of 0.89.

Conclusion: The framework enhances rare-disease diagnosis, reducing the diagnostic odyssey for patients.

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [16] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: A novel method using Large Multi-Modal Models (LMMs) to georeference biological samples by visually contextualizing spatial relations from locality descriptions, achieving ~1 km average error.


<details>
  <summary>Details</summary>
Motivation: Millions of biological samples lack georeferencing due to labor-intensive manual processes. Existing methods ignore maps, a key tool for spatial relations.

Method: Uses LMMs in a zero-shot grid-based approach to interpret locality descriptions with maps, outperforming uni-modal methods.

Result: Achieves ~1 km average distance error, better than existing tools and Large Language Models.

Conclusion: Proposes integrating LMMs into georeferencing workflows, highlighting their potential for fine-grained map comprehension.

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [17] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: The paper proposes a query rewriting framework with multi-LLM knowledge fusion to improve speech instruction dataset construction, addressing gaps in LLM-generated data and TTS limitations.


<details>
  <summary>Details</summary>
Motivation: Current speech instruction datasets are limited by biased training tasks and the gap between LLM-generated results and human responses, making high-quality dataset construction costly.

Method: A query rewriting framework leverages multi-LLM knowledge fusion to annotate and validate synthesized speech, enabling high-quality dataset creation without human annotation.

Result: The method improves data usability from 72% to 93% and excels in complex, context-related rewriting tasks.

Conclusion: The proposed framework offers a robust alternative to human annotation for constructing speech instruction datasets, enhancing TTS model performance.

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [18] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: A structured multi-agent system (MAS) outperforms a two-agent system (2AS) in early-stage engineering design tasks, improving design detail but struggling with requirement coverage and code fidelity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing LLM workflows in maintaining task continuity and generating executable models for early-stage engineering design.

Method: Comparison of a nine-role MAS and a simpler 2AS using Design-State Graph (DSG) representation, evaluated across 60 experiments with varying LLMs, configurations, and temperatures.

Result: MAS produced more granular DSGs (5-6 nodes) and maintained perfect JSON integrity, but requirement coverage was low (<20%). Code compatibility peaked at 100% for 2AS but averaged below 50% for MAS.

Conclusion: Structured multi-agent orchestration enhances design detail, but gaps in requirement coverage and code fidelity remain challenges.

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [19] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: Leanabell-Prover-V2 is a 7B LLM for formal theorem proofs in Lean 4, using verifier-integrated CoT and RL with verifier feedback for self-correction. It improves performance by 3.2% and 2.0% on MiniF2F.


<details>
  <summary>Details</summary>
Motivation: To enhance theorem-proving performance in Lean 4 by integrating verifier feedback for self-aware reasoning and error correction.

Method: Posttrains strong prover models with RL, verifier feedback, feedback token masking, and a simple reward strategy.

Result: Performance improved by 3.2% (Kimina-Prover-Preview-Distill-7B) and 2.0% (DeepSeek-Prover-V2-7B) on MiniF2F.

Conclusion: Leanabell-Prover-V2 effectively optimizes reasoning with verifier feedback, showing measurable performance gains.

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [20] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: The paper introduces INoT, a novel AI Agent Reasoning Framework that reduces token costs and improves performance by enabling LLMs to perform programmatic dialogue reasoning internally.


<details>
  <summary>Details</summary>
Motivation: Current AI Agents rely on LLMs and MLLMs but face limitations in natural language understanding and high inference costs due to external reasoning frameworks.

Method: The authors propose INoT, which uses a new LLM-Read code in prompts to allow LLMs to execute reasoning internally, reducing token costs.

Result: Experiments show INoT improves performance by 7.95% on average and reduces token costs by 58.3% compared to baselines.

Conclusion: INoT is effective and versatile, offering significant improvements in performance and cost efficiency for AI Agents.

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [21] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: elsciRL is an open-source Python library integrating language solutions with reinforcement learning, featuring a GUI for LLM-based instruction generation and self-completion, showing performance improvements.


<details>
  <summary>Details</summary>
Motivation: To accelerate the evaluation of language solutions in reward-based environments and enable new scientific opportunities.

Method: Extends the Language Adapter with Self-Completing Instruction framework using LLMs, with a novel GUI for text input and instruction generation.

Result: Empirical results show improved reinforcement learning agent performance through generated instructions.

Conclusion: elsciRL facilitates easy application of language solutions in RL, promoting further scientific exploration.

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [22] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: The paper discusses surrogate-based optimization, like Bayesian optimization, to address challenges in modeling and optimizing system-of-systems architectures.


<details>
  <summary>Details</summary>
Motivation: Challenges in exploring novel architectures using physics-based simulations, such as high evaluation costs and potential failures, motivate the need for efficient optimization methods.

Method: The paper proposes surrogate-based optimization algorithms, specifically Bayesian optimization with Gaussian process models, to mitigate these challenges.

Result: The approach aims to reduce computational complexity and improve efficiency in system-of-systems architecting.

Conclusion: Surrogate-based optimization is a promising solution for overcoming the limitations of traditional physics-based simulations in system-of-systems architecture development.

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis](https://arxiv.org/abs/2507.08050)
*Ming Wang,Zhaoyang Duan,Dong Xue,Fangzhou Liu,Zhongheng Zhang*

Main category: cs.LG

TL;DR: A federated few-shot learning framework with privacy-preserving mechanisms is proposed to address limited labeled data and privacy concerns in respiratory disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: Challenges include labor-intensive medical data annotation, scarcity of labeled datasets, and privacy concerns in data sharing.

Method: Uses meta-stochastic gradient descent to prevent overfitting and integrates differential privacy noise for gradient protection. Aggregates local models via weighted averaging.

Result: The method achieves strong performance with differential privacy, adapting well to diverse data structures and distributions.

Conclusion: The framework effectively balances privacy and diagnostic accuracy in resource-constrained settings.

Abstract: The labor-intensive nature of medical data annotation presents a significant
challenge for respiratory disease diagnosis, resulting in a scarcity of
high-quality labeled datasets in resource-constrained settings. Moreover,
patient privacy concerns complicate the direct sharing of local medical data
across institutions, and existing centralized data-driven approaches, which
rely on amounts of available data, often compromise data privacy. This study
proposes a federated few-shot learning framework with privacy-preserving
mechanisms to address the issues of limited labeled data and privacy protection
in diagnosing respiratory diseases. In particular, a meta-stochastic gradient
descent algorithm is proposed to mitigate the overfitting problem that arises
from insufficient data when employing traditional gradient descent methods for
neural network training. Furthermore, to ensure data privacy against gradient
leakage, differential privacy noise from a standard Gaussian distribution is
integrated into the gradients during the training of private models with local
data, thereby preventing the reconstruction of medical images. Given the
impracticality of centralizing respiratory disease data dispersed across
various medical institutions, a weighted average algorithm is employed to
aggregate local diagnostic models from different clients, enhancing the
adaptability of a model across diverse scenarios. Experimental results show
that the proposed method yields compelling results with the implementation of
differential privacy, while effectively diagnosing respiratory diseases using
data from different structures, categories, and distributions.

</details>


### [24] [Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial Optimization More Efficiently](https://arxiv.org/abs/2507.08053)
*Kenshin Abe,Yunzhuo Wang,Shuhei Watanabe*

Main category: cs.LG

TL;DR: The paper proposes an efficient combinatorial optimization algorithm for Tree-structured Parzen estimator (TPE) to address black-box combinatorial optimization, extending its applicability beyond deep learning.


<details>
  <summary>Details</summary>
Motivation: Combinatorial optimization is crucial in fields like chemistry and biology but remains underexplored in TPE. The paper aims to bridge this gap.

Method: The authors generalize the categorical kernel in TPE with a numerical kernel, introduce distance structure, and modify the kernel for large combinatorial spaces to reduce time complexity.

Result: Experiments on synthetic problems show the proposed method outperforms the original TPE, finding better solutions with fewer evaluations.

Conclusion: The proposed algorithm enhances TPE's versatility for combinatorial optimization and is integrated into Optuna, an open-source HPO framework.

Abstract: Tree-structured Parzen estimator (TPE) is a versatile hyperparameter
optimization (HPO) method supported by popular HPO tools. Since these HPO tools
have been developed in line with the trend of deep learning (DL), the problem
setups often used in the DL domain have been discussed for TPE such as
multi-objective optimization and multi-fidelity optimization. However, the
practical applications of HPO are not limited to DL, and black-box
combinatorial optimization is actively utilized in some domains, e.g.,
chemistry and biology. As combinatorial optimization has been an untouched, yet
very important, topic in TPE, we propose an efficient combinatorial
optimization algorithm for TPE. In this paper, we first generalize the
categorical kernel with the numerical kernel in TPE, enabling us to introduce a
distance structure to the categorical kernel. Then we discuss modifications for
the newly developed kernel to handle a large combinatorial search space. These
modifications reduce the time complexity of the kernel calculation with respect
to the size of a combinatorial search space. In the experiments using synthetic
problems, we verified that our proposed method identifies better solutions with
fewer evaluations than the original TPE. Our algorithm is available in Optuna,
an open-source framework for HPO.

</details>


### [25] [Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions](https://arxiv.org/abs/2507.08068)
*Simon Matrenok,Skander Moalla,Caglar Gulcehre*

Main category: cs.LG

TL;DR: QRPO bridges the gap between pointwise absolute rewards and offline learning by using quantile rewards, outperforming DPO, REBEL, and SimPO in evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DPO and REBEL are limited to preference pairs, while PPO and GRPO require online learning. QRPO aims to enable offline learning from absolute rewards.

Method: QRPO uses quantile rewards to solve the KL-regularized RL objective analytically, avoiding the need for relative signals and enabling offline learning.

Result: QRPO consistently outperforms DPO, REBEL, and SimPO in chat and coding evaluations, including reward model scores and benchmarks like AlpacaEval 2 and LeetCode.

Conclusion: QRPO is a simpler, scalable method for learning from absolute rewards offline, reducing length bias and achieving superior performance.

Abstract: Aligning large language models with pointwise absolute rewards has so far
required online, on-policy algorithms such as PPO and GRPO. In contrast,
simpler methods that can leverage offline or off-policy data, such as DPO and
REBEL, are limited to learning from preference pairs or relative signals. To
bridge this gap, we introduce \emph{Quantile Reward Policy Optimization}
(QRPO), which learns from pointwise absolute rewards while preserving the
simplicity and offline applicability of DPO-like methods. QRPO uses quantile
rewards to enable regression to the closed-form solution of the KL-regularized
RL objective. This reward yields an analytically tractable partition function,
removing the need for relative signals to cancel this term. Moreover, QRPO
scales with increased compute to estimate quantile rewards, opening a new
dimension for pre-computation scaling. Empirically, QRPO consistently achieves
top performance on chat and coding evaluations -- reward model scores,
AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse
datasets and 8B-scale models. Finally, we find that training with robust
rewards instead of converting them to preferences induces less length bias.

</details>


### [26] [Low-rank Momentum Factorization for Memory Efficient Training](https://arxiv.org/abs/2507.08091)
*Pouria Mahdavinia,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: MoFaSGD is a memory-efficient fine-tuning method for large models, using dynamically updated low-rank SVD for momentum, balancing memory reduction and performance.


<details>
  <summary>Details</summary>
Motivation: Address memory challenges in fine-tuning large models by improving upon existing methods like GaLore, which struggle with fixed subspaces or costly resampling.

Method: Proposes Momentum Factorized SGD (MoFaSGD), maintaining a low-rank SVD of first-order momentum, enabling adaptive subspace updates and efficient spectrally normalized updates.

Result: Achieves optimal convergence for non-convex optimization, with competitive memory reduction (comparable to LoRA) and performance on large language model benchmarks.

Conclusion: MoFaSGD offers an effective, memory-efficient alternative to existing low-rank optimization methods, with theoretical and empirical validation.

Abstract: Fine-tuning large foundation models presents significant memory challenges
due to stateful optimizers like AdamW, often requiring several times more GPU
memory than inference. While memory-efficient methods like parameter-efficient
fine-tuning (e.g., LoRA) and optimizer state compression exist, recent
approaches like GaLore bridge these by using low-rank gradient projections and
subspace moment accumulation. However, such methods may struggle with fixed
subspaces or computationally costly offline resampling (e.g., requiring
full-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which
maintains a dynamically updated low-rank SVD representation of the first-order
momentum, closely approximating its full-rank counterpart throughout training.
This factorization enables a memory-efficient fine-tuning method that
adaptively updates the optimization subspace at each iteration. Crucially,
MoFaSGD leverages the computed low-rank momentum factors to perform efficient
spectrally normalized updates, offering an alternative to subspace moment
accumulation. We establish theoretical convergence guarantees for MoFaSGD,
proving it achieves an optimal rate for non-convex stochastic optimization
under standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness
on large language model alignment benchmarks, achieving a competitive trade-off
between memory reduction (comparable to LoRA) and performance compared to
state-of-the-art low-rank optimization methods. Our implementation is available
at https://github.com/pmahdavi/MoFaSGD.

</details>


### [27] [PDE-aware Optimizer for Physics-informed Neural Networks](https://arxiv.org/abs/2507.08118)
*Hardik Shukla,Manurag Khullar,Vismay Churiwala*

Main category: cs.LG

TL;DR: The paper introduces a PDE-aware optimizer for PINNs to address gradient misalignment in stiff systems, outperforming Adam and SOAP in convergence and accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard optimizers like Adam struggle with balancing loss terms in stiff PDE systems, leading to inefficiencies in PINNs training.

Method: A PDE-aware optimizer adapts parameter updates based on the variance of per-sample PDE residual gradients, avoiding high computational costs of second-order methods.

Result: The optimizer achieves smoother convergence and lower errors on 1D Burgers', Allen-Cahn, and KdV equations, especially in sharp gradient regions.

Conclusion: The PDE-aware optimizer enhances PINNs training stability, though scaling to larger architectures and hardware remains a future challenge.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs) by embedding physical
constraints into the loss function. However, standard optimizers such as Adam
often struggle to balance competing loss terms, particularly in stiff or
ill-conditioned systems. In this work, we propose a PDE-aware optimizer that
adapts parameter updates based on the variance of per-sample PDE residual
gradients. This method addresses gradient misalignment without incurring the
heavy computational costs of second-order optimizers such as SOAP. We benchmark
the PDE-aware optimizer against Adam and SOAP on 1D Burgers', Allen-Cahn and
Korteweg-de Vries(KdV) equations. Across both PDEs, the PDE-aware optimizer
achieves smoother convergence and lower absolute errors, particularly in
regions with sharp gradients. Our results demonstrate the effectiveness of PDE
residual-aware adaptivity in enhancing stability in PINNs training. While
promising, further scaling on larger architectures and hardware accelerators
remains an important direction for future research.

</details>


### [28] [Quasi-Random Physics-informed Neural Networks](https://arxiv.org/abs/2507.08121)
*Tianchi Yu,Ivan Oseledets*

Main category: cs.LG

TL;DR: QRPINNs use quasi-random sampling for better convergence in solving PDEs, outperforming PINNs and adaptive methods, especially in high dimensions.


<details>
  <summary>Details</summary>
Motivation: Improve PINNs' performance by addressing sensitivity to point sampling, leveraging quasi Monte-Carlo methods for high-dimensional problems.

Method: Proposes QRPINNs, using low-discrepancy sequences for sampling instead of random points, and combines with adaptive sampling.

Result: QRPINNs show better convergence and outperform PINNs and adaptive methods, particularly in high-dimensional PDEs.

Conclusion: QRPINNs are a promising improvement over PINNs, with potential for further enhancement via adaptive sampling.

Abstract: Physics-informed neural networks have shown promise in solving partial
differential equations (PDEs) by integrating physical constraints into neural
network training, but their performance is sensitive to the sampling of points.
Based on the impressive performance of quasi Monte-Carlo methods in high
dimensional problems, this paper proposes Quasi-Random Physics-Informed Neural
Networks (QRPINNs), which use low-discrepancy sequences for sampling instead of
random points directly from the domain. Theoretically, QRPINNs have been proven
to have a better convergence rate than PINNs. Empirically, experiments
demonstrate that QRPINNs significantly outperform PINNs and some representative
adaptive sampling methods, especially in high-dimensional PDEs. Furthermore,
combining QRPINNs with adaptive sampling can further improve the performance.

</details>


### [29] [Physics-Informed Neural Networks with Hard Nonlinear Equality and Inequality Constraints](https://arxiv.org/abs/2507.08124)
*Ashfaq Iftakher,Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: KKT-Hardnet enforces strict constraints in PINNs via KKT conditions and log-exponential transformation, achieving higher accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs lack strict constraint satisfaction, which is critical for reliable predictions in engineering systems.

Method: KKT-Hardnet projects onto feasible regions using KKT conditions and reformulates nonlinear constraints with log-exponential transformation for differentiability.

Result: KKT-Hardnet outperforms MLPs and PINNs in accuracy and strict constraint satisfaction, validated on test problems and a chemical process simulation.

Conclusion: KKT-Hardnet enables reliable hybrid modeling by integrating domain knowledge and ensuring strict constraint adherence.

Abstract: Traditional physics-informed neural networks (PINNs) do not guarantee strict
constraint satisfaction. This is problematic in engineering systems where minor
violations of governing laws can significantly degrade the reliability and
consistency of model predictions. In this work, we develop KKT-Hardnet, a PINN
architecture that enforces both linear and nonlinear equality and inequality
constraints up to machine precision. It leverages a projection onto the
feasible region through solving Karush-Kuhn-Tucker (KKT) conditions of a
distance minimization problem. Furthermore, we reformulate the nonlinear KKT
conditions using log-exponential transformation to construct a general sparse
system with only linear and exponential terms, thereby making the projection
differentiable. We apply KKT-Hardnet on both test problems and a real-world
chemical process simulation. Compared to multilayer perceptrons and PINNs,
KKT-Hardnet achieves higher accuracy and strict constraint satisfaction. This
approach allows the integration of domain knowledge into machine learning
towards reliable hybrid modeling of complex systems.

</details>


### [30] [ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction](https://arxiv.org/abs/2507.08153)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: ALCo-FM is a foundation model for traffic accident risk forecasting, using adaptive long-context windows and multimodal fusion to outperform 20+ baselines with high accuracy and calibration.


<details>
  <summary>Details</summary>
Motivation: Traffic accidents are rare but impactful, requiring long-context multimodal reasoning for accurate risk prediction.

Method: ALCo-FM dynamically selects context windows, fuses multimodal data via shallow cross attention, uses GAT and BigBird-style transformers, and incorporates Monte Carlo dropout for confidence.

Result: Achieves 0.94 accuracy, 0.92 F1, and 0.04 ECE, outperforming 20+ baselines.

Conclusion: ALCo-FM is effective for urban risk prediction, with strong performance and calibration.

Abstract: Traffic accidents are rare, yet high-impact events that require long-context
multimodal reasoning for accurate risk forecasting. In this paper, we introduce
ALCo-FM, a unified adaptive long-context foundation model that computes a
volatility pre-score to dynamically select context windows for input data and
encodes and fuses these multimodal data via shallow cross attention. Following
a local GAT layer and a BigBird-style sparse global transformer over H3
hexagonal grids, coupled with Monte Carlo dropout for confidence, the model
yields superior, well-calibrated predictions. Trained on data from 15 US cities
with a class-weighted loss to counter label imbalance, and fine-tuned with
minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and
an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in
large-scale urban risk prediction. Code and dataset are available at:
https://github.com/PinakiPrasad12/ALCo-FM

</details>


### [31] [Just Read the Question: Enabling Generalization to New Assessment Items with Text Awareness](https://arxiv.org/abs/2507.08154)
*Arisha Khan,Nathaniel Li,Tori Shen,Anna N. Rafferty*

Main category: cs.LG

TL;DR: Text-LENS extends LENS for educational assessment by using item text embeddings, improving performance on unseen items while matching LENS on seen ones.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of incorporating new items in machine learning for educational assessment, leveraging item text embeddings.

Method: Extends the LENS partial variational auto-encoder to include item text embeddings, tested on Eedi and LLM-Sim datasets.

Result: Matches LENS on seen items and outperforms it on unseen items, effectively learning student proficiency and predicting performance on new items.

Conclusion: Text-LENS successfully leverages text embeddings to improve generalization to unseen items in educational assessment.

Abstract: Machine learning has been proposed as a way to improve educational assessment
by making fine-grained predictions about student performance and learning
relationships between items. One challenge with many machine learning
approaches is incorporating new items, as these approaches rely heavily on
historical data. We develop Text-LENS by extending the LENS partial variational
auto-encoder for educational assessment to leverage item text embeddings, and
explore the impact on predictive performance and generalization to previously
unseen items. We examine performance on two datasets: Eedi, a publicly
available dataset that includes item content, and LLM-Sim, a novel dataset with
test items produced by an LLM. We find that Text-LENS matches LENS' performance
on seen items and improves upon it in a variety of conditions involving unseen
items; it effectively learns student proficiency from and makes predictions
about student performance on new items.

</details>


### [32] [Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors](https://arxiv.org/abs/2507.08175)
*Md. Saif Hassan Onim,Travis S. Humble,Himanshu Thapliyal*

Main category: cs.LG

TL;DR: The paper explores using physiological signals for emotion recognition, comparing classical and quantum machine learning. Quantum-enhanced SVM outperforms classical methods, achieving high accuracy and recall.


<details>
  <summary>Details</summary>
Motivation: To provide a privacy-preserving alternative to facial recognition for emotion detection, especially useful for individuals with communication impairments like ADRD and PTSD.

Method: Comparison of classical machine learning algorithms and hybrid quantum machine learning (QML) with a quantum kernel-based model, using wearable sensor data.

Result: Quantum-enhanced SVM shows superior performance (F1 scores >80%, up to 36% recall improvement) even with limited datasets.

Conclusion: The approach enhances accuracy and robustness in emotion recognition, offering potential for clinical and assisted living applications.

Abstract: We investigate the feasibility of inferring emotional states exclusively from
physiological signals, thereby presenting a privacy-preserving alternative to
conventional facial recognition techniques. We conduct a performance comparison
of classical machine learning algorithms and hybrid quantum machine learning
(QML) methods with a quantum kernel-based model. Our results indicate that the
quantum-enhanced SVM surpasses classical counterparts in classification
performance across all emotion categories, even when trained on limited
datasets. The F1 scores over all classes are over 80% with around a maximum of
36% improvement in the recall values. The integration of wearable sensor data
with quantum machine learning not only enhances accuracy and robustness but
also facilitates unobtrusive emotion recognition. This methodology holds
promise for populations with impaired communication abilities, such as
individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans
with Post-Traumatic Stress Disorder (PTSD). The findings establish an early
foundation for passive emotional monitoring in clinical and assisted living
conditions.

</details>


### [33] [Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity](https://arxiv.org/abs/2507.08177)
*Arun Vignesh Malarkkan,Haoyue Bai,Xinyuan Wang,Anjali Kaushik,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: The paper proposes causal learning for spatio-temporal anomaly detection in cyber-physical systems, addressing interpretability and adaptability issues of black-box deep learning.


<details>
  <summary>Details</summary>
Motivation: Ensuring resilience against cyberattacks in interconnected systems requires better anomaly detection methods, as current data-driven approaches lack interpretability and robustness.

Method: The paper introduces causal learning with three key directions: causal graph profiling, multi-view fusion, and continual causal graph learning.

Result: Causal models provide early warnings and root cause attribution, outperforming black-box detectors in real-world systems like water treatment infrastructures.

Conclusion: The paper advocates for causality-driven approaches to enhance anomaly detection, outlining future research on multi-modality and scalable adaptive frameworks.

Abstract: As cyber-physical systems grow increasingly interconnected and spatially
distributed, ensuring their resilience against evolving cyberattacks has become
a critical priority. Spatio-Temporal Anomaly detection plays an important role
in ensuring system security and operational integrity. However, current
data-driven approaches, largely driven by black-box deep learning, face
challenges in interpretability, adaptability to distribution shifts, and
robustness under evolving system dynamics. In this paper, we advocate for a
causal learning perspective to advance anomaly detection in spatially
distributed infrastructures that grounds detection in structural cause-effect
relationships. We identify and formalize three key directions: causal graph
profiling, multi-view fusion, and continual causal graph learning, each
offering distinct advantages in uncovering dynamic cause-effect structures
across time and space. Drawing on real-world insights from systems such as
water treatment infrastructures, we illustrate how causal models provide early
warning signals and root cause attribution, addressing the limitations of
black-box detectors. Looking ahead, we outline the future research agenda
centered on multi-modality, generative AI-driven, and scalable adaptive causal
frameworks. Our objective is to lay a new research trajectory toward scalable,
adaptive, explainable, and spatially grounded anomaly detection systems. We
hope to inspire a paradigm shift in cybersecurity research, promoting
causality-driven approaches to address evolving threats in interconnected
infrastructures.

</details>


### [34] [CTRLS: Chain-of-Thought Reasoning via Latent State-Transition](https://arxiv.org/abs/2507.08182)
*Junda Wu,Yuxin Xiong,Xintong Li,Zhengmian Hu,Tong Yu,Rui Wang,Xiang Chen,Jingbo Shang,Julian McAuley*

Main category: cs.LG

TL;DR: CTRLS introduces a framework for Chain-of-Thought reasoning using Markov decision processes and reinforcement learning, improving reasoning accuracy and diversity.


<details>
  <summary>Details</summary>
Motivation: Conventional CoT methods lack structured modeling of reasoning transitions, limiting exploration of diverse reasoning paths.

Method: CTRLS models CoT reasoning as an MDP with latent state transitions, using distributional reinforcement learning for principled exploration.

Result: The framework enhances reasoning accuracy, diversity, and exploration efficiency in benchmark tasks.

Conclusion: CTRLS provides a theoretically grounded and practical approach to improve CoT reasoning in LLMs.

Abstract: Chain-of-thought (CoT) reasoning enables large language models (LLMs) to
break down complex problems into interpretable intermediate steps,
significantly enhancing model transparency and performance in reasoning tasks.
However, conventional CoT methods rely on heuristic sampling without structured
modeling of reasoning transitions, constraining their ability to systematically
explore and discover diverse and effective reasoning trajectories. In this
work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov
decision process (MDP) with latent state transitions, enabling principled and
state-aware exploration via distributional reinforcement learning. By modelling
reasoning actions as explicit probability distributions in latent space, our
approach explicitly models epistemic uncertainty, facilitating robust
exploration of the reasoning space. As part of our framework, we introduce an
on-policy reinforcement learning strategy incorporating epsilon-greedy
exploration and entropy-based regularization to iteratively refine latent state
transitions without requiring additional fine-tuning of the underlying LLM.
Theoretical analyses provide evidence lower bounds (ELBO), theoretically
grounding our transition-aware modeling of latent reasoning dynamics. Further
experiments demonstrate improvements in reasoning accuracy, diversity, and
exploration efficiency across benchmark reasoning tasks.

</details>


### [35] [EvA: Evolutionary Attacks on Graphs](https://arxiv.org/abs/2507.08212)
*Mohammad Sadegh Akhondzadeh,Soroush H. Zargarbashi,Jimin Cao,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: EvA, an evolutionary-based attack method, outperforms gradient-based attacks by directly solving discrete optimization, achieving an 11% higher accuracy drop in GNNs.


<details>
  <summary>Details</summary>
Motivation: Existing gradient-based attacks are suboptimal and limited to differentiable objectives, leaving untapped potential in GNN vulnerability.

Method: Enhances an evolutionary algorithm to directly solve discrete optimization, working with any black-box model and objective.

Result: EvA achieves an average 11% additional accuracy drop compared to previous attacks and enables novel attacks on robustness certificates and conformal sets.

Conclusion: EvA demonstrates superior performance and flexibility, uncovering significant untapped potential in GNN attack design.

Abstract: Even a slight perturbation in the graph structure can cause a significant
drop in the accuracy of graph neural networks (GNNs). Most existing attacks
leverage gradient information to perturb edges. This relaxes the attack's
optimization problem from a discrete to a continuous space, resulting in
solutions far from optimal. It also restricts the adaptability of the attack to
non-differentiable objectives. Instead, we introduce a few simple yet effective
enhancements of an evolutionary-based algorithm to solve the discrete
optimization problem directly. Our Evolutionary Attack (EvA) works with any
black-box model and objective, eliminating the need for a differentiable proxy
loss. This allows us to design two novel attacks that reduce the effectiveness
of robustness certificates and break conformal sets. The memory complexity of
our attack is linear in the attack budget. Among our experiments, EvA shows
$\sim$11\% additional drop in accuracy on average compared to the best previous
attack, revealing significant untapped potential in designing attacks.

</details>


### [36] [InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems](https://arxiv.org/abs/2507.08235)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: InsightBuild integrates causality analysis and LLMs to explain energy anomalies in smart buildings, improving diagnosis and mitigation.


<details>
  <summary>Details</summary>
Motivation: Facility managers lack clear explanations for anomalous energy usage in smart buildings, hindering efficiency improvements.

Method: A two-stage framework: causal inference (Granger tests, structural discovery) followed by LLM-based natural language generation for explanations.

Result: Evaluated on real-world datasets, InsightBuild provides precise, actionable explanations for energy inefficiencies.

Conclusion: Combining causal discovery with LLMs enhances clarity and utility of explanations for facility managers.

Abstract: Smart buildings generate vast streams of sensor and control data, but
facility managers often lack clear explanations for anomalous energy usage. We
propose InsightBuild, a two-stage framework that integrates causality analysis
with a fine-tuned large language model (LLM) to provide human-readable, causal
explanations of energy consumption patterns. First, a lightweight causal
inference module applies Granger causality tests and structural causal
discovery on building telemetry (e.g., temperature, HVAC settings, occupancy)
drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,
fine-tuned on aligned pairs of sensor-level causes and textual explanations,
receives as input the detected causal relations and generates concise,
actionable explanations. We evaluate InsightBuild on two real-world datasets
(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth
causes for a held-out set of anomalies. Our results demonstrate that combining
explicit causal discovery with LLM-based natural language generation yields
clear, precise explanations that assist facility managers in diagnosing and
mitigating energy inefficiencies.

</details>


### [37] [Self-Supervised Learning-Based Multimodal Prediction on Prosocial Behavior Intentions](https://arxiv.org/abs/2507.08238)
*Abinay Reddy Naini,Zhaobo K. Zheng,Teruhisa Misu,Kumar Akash*

Main category: cs.LG

TL;DR: A self-supervised learning approach is proposed to predict prosocial behavior in mobility scenarios, overcoming data scarcity by leveraging multi-modal data and fine-tuning with smaller labeled datasets.


<details>
  <summary>Details</summary>
Motivation: Predicting prosocial behavior intentions in mobility scenarios is underexplored due to the lack of large labeled datasets, limiting deep-learning model training.

Method: The method involves self-supervised learning using multi-modal data from existing datasets, pre-training on diverse tasks, and fine-tuning with a smaller labeled prosocial behavior dataset.

Result: The approach significantly enhances model performance, addressing data scarcity and improving prosocial behavior prediction.

Conclusion: This method provides an effective benchmark for prosocial behavior prediction and insights for intelligent vehicle systems and human-machine interaction.

Abstract: Human state detection and behavior prediction have seen significant
advancements with the rise of machine learning and multimodal sensing
technologies. However, predicting prosocial behavior intentions in mobility
scenarios, such as helping others on the road, is an underexplored area.
Current research faces a major limitation. There are no large, labeled datasets
available for prosocial behavior, and small-scale datasets make it difficult to
train deep-learning models effectively. To overcome this, we propose a
self-supervised learning approach that harnesses multi-modal data from existing
physiological and behavioral datasets. By pre-training our model on diverse
tasks and fine-tuning it with a smaller, manually labeled prosocial behavior
dataset, we significantly enhance its performance. This method addresses the
data scarcity issue, providing a more effective benchmark for prosocial
behavior prediction, and offering valuable insights for improving intelligent
vehicle systems and human-machine interaction.

</details>


### [38] [Data Generation without Function Estimation](https://arxiv.org/abs/2507.08239)
*Hadi Daneshmand,Ashkan Soleymani*

Main category: cs.LG

TL;DR: Proposes an estimation-free generative method using deterministic gradient descent to transport a uniform distribution to arbitrary data distribution, avoiding function estimation and neural networks.


<details>
  <summary>Details</summary>
Motivation: Function estimation for generative models is computationally and statistically challenging. The paper aims to bypass this by developing an estimation-free approach.

Method: Uses deterministic gradient descent on a set of points to transport a uniform distribution to any data distribution, leveraging physics of interacting particles.

Result: Demonstrates theoretically and experimentally that the method can generate data without function estimation or neural networks.

Conclusion: The proposed method offers a novel, efficient alternative to traditional generative models by eliminating the need for function estimation.

Abstract: Estimating the score function (or other population-density-dependent
functions) is a fundamental component of most generative models. However, such
function estimation is computationally and statistically challenging. Can we
avoid function estimation for data generation? We propose an estimation-free
generative method: A set of points whose locations are deterministically
updated with (inverse) gradient descent can transport a uniform distribution to
arbitrary data distribution, in the mean field regime, without function
estimation, training neural networks, and even noise injection. The proposed
method is built upon recent advances in the physics of interacting particles.
We show, both theoretically and experimentally, that these advances can be
leveraged to develop novel generative methods.

</details>


### [39] [CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry](https://arxiv.org/abs/2507.08243)
*Chandra Sekhar Mukherjee,Joonyoung Bae,Jiapeng Zhang*

Main category: cs.LG

TL;DR: CoreSPECT enhances clustering algorithms like K-Means and GMM by leveraging interactions between data distribution and geometry, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked interaction between data distribution and geometry in clustering, improving performance of simple algorithms.

Method: Uses a novel framework (CoreSPECT) with strategic region selection and multi-layer propagation via neighborhood graphs.

Result: Average ARI improvements: 40% for K-Means, 14% for GMM, outperforming density-based and manifold-based methods.

Conclusion: CoreSPECT effectively boosts clustering accuracy, supported by theoretical guarantees and robustness to noise.

Abstract: Density and geometry have long served as two of the fundamental guiding
principles in clustering algorithm design, with algorithm usually focusing
either on the density structure of the data (e.g., HDBSCAN and Density Peak
Clustering) or the complexity of underlying geometry (e.g., manifold clustering
algorithms).
  In this paper, we identify and formalize a recurring but often overlooked
interaction between distribution and geometry and leverage this insight to
design our clustering enhancement framework CoreSPECT (Core Space
Projection-based Enhancement of Clustering Techniques). Our framework boosts
the performance of simple algorithms like K-Means and GMM by applying them to
strategically selected regions, then extending the partial partition to a
complete partition for the dataset using a novel neighborhood graph based
multi-layer propagation procedure.
  We apply our framework on 15 datasets from three different domains and obtain
consistent and substantial gain in clustering accuracy for both K-Means and
GMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by
14%, often surpassing the performance of both manifold-based and recent
density-based clustering algorithms. We further support our framework with
initial theoretical guarantees, ablation to demonstrate the usefulness of the
individual steps and with evidence of robustness to noise.

</details>


### [40] [Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)](https://arxiv.org/abs/2507.08255)
*Hossein Jamali*

Main category: cs.LG

TL;DR: Quantum-UnIMP integrates quantum circuits into LLM-based imputation, outperforming classical methods by leveraging quantum phenomena for richer data representations.


<details>
  <summary>Details</summary>
Motivation: Missing data degrades ML performance; classical embeddings in LLMs like UnIMP struggle with complex correlations in mixed-type data.

Method: Quantum-UnIMP replaces classical embeddings with quantum feature maps from an IQP circuit, utilizing superposition and entanglement.

Result: Reduces imputation error by 15.2% (RMSE) for numerical features and improves classification accuracy by 8.7% (F1-Score) for categorical features.

Conclusion: Quantum-enhanced representations show significant potential for complex data imputation, even with near-term quantum hardware.

Abstract: Missing data presents a critical challenge in real-world datasets,
significantly degrading the performance of machine learning models. While Large
Language Models (LLMs) have recently demonstrated remarkable capabilities in
tabular data imputation, exemplified by frameworks like UnIMP, their reliance
on classical embedding methods often limits their ability to capture complex,
non-linear correlations, particularly in mixed-type data scenarios encompassing
numerical, categorical, and textual features. This paper introduces
Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into
an LLM-based imputation architecture. Our core innovation lies in replacing
conventional classical input embeddings with quantum feature maps generated by
an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the
model to leverage quantum phenomena such as superposition and entanglement,
thereby learning richer, more expressive representations of data and enhancing
the recovery of intricate missingness patterns. Our experiments on benchmark
mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by
up to 15.2% for numerical features (RMSE) and improves classification accuracy
by 8.7% for categorical features (F1-Score) compared to state-of-the-art
classical and LLM-based methods. These compelling results underscore the
profound potential of quantum-enhanced representations for complex data
imputation tasks, even with near-term quantum hardware.

</details>


### [41] [A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning](https://arxiv.org/abs/2507.08267)
*Hiroshi Yoshihara,Taiki Yamaguchi,Yuichi Inoue*

Main category: cs.LG

TL;DR: The paper introduces a training method combining extended Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance LLMs' mathematical reasoning, achieving top performance in benchmarks like AIMO.


<details>
  <summary>Details</summary>
Motivation: To improve LLMs' mathematical reasoning by systematically integrating SFT and RL for accuracy and efficiency.

Method: Extended SFT followed by RL (GRPO) to optimize accuracy and token efficiency.

Result: Achieved top-tier performance, including high rank in AIMO, with reproducible framework.

Conclusion: Provides a blueprint for state-of-the-art mathematical reasoners, balancing accuracy and efficiency.

Abstract: Enhancing the mathematical reasoning of Large Language Models (LLMs) is a
pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a
systematic methodology for combining them to maximize both accuracy and
efficiency remains largely unexplored. This paper introduces a practical and
effective training recipe that strategically integrates extended SFT with RL
from online inference (GRPO). We posit that these methods play complementary,
not competing, roles: a prolonged SFT phase first pushes the model's accuracy
to its limits, after which a GRPO phase dramatically improves token efficiency
while preserving this peak performance. Our experiments reveal that extending
SFT for as many as 10 epochs is crucial for performance breakthroughs, and that
the primary role of GRPO in this framework is to optimize solution length. The
efficacy of our recipe is rigorously validated through top-tier performance on
challenging benchmarks, including a high rank among over 2,200 teams in the
strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the
community with a battle-tested blueprint for developing state-of-the-art
mathematical reasoners that are both exceptionally accurate and practically
efficient. To ensure full reproducibility and empower future research, we will
open-source our entire framework, including all code, model checkpoints, and
training configurations at
https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.

</details>


### [42] [Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization](https://arxiv.org/abs/2507.08269)
*Woon Ryong Kim,Jaeheun Jung,Jeong Un Ha,Donghun Lee,Jae Kyung Shim*

Main category: cs.LG

TL;DR: A data-driven framework using supervised learning for dimensional synthesis of planar four-bar mechanisms, bypassing traditional methods.


<details>
  <summary>Details</summary>
Motivation: To simplify the challenging inverse problem of determining mechanism dimensions from motion specifications, making it accessible even for non-experts.

Method: Combines synthetic data, an LSTM-based neural network for sequential precision points, and a Mixture of Experts (MoE) architecture for different linkage types.

Result: Produces accurate, defect-free linkages across various configurations, validated by a novel simulation metric.

Conclusion: Enables intuitive and efficient mechanism design, offering scalable and flexible synthesis in kinematic design.

Abstract: Dimensional synthesis of planar four-bar mechanisms is a challenging inverse
problem in kinematics, requiring the determination of mechanism dimensions from
desired motion specifications. We propose a data-driven framework that bypasses
traditional equation-solving and optimization by leveraging supervised
learning. Our method combines a synthetic dataset, an LSTM-based neural network
for handling sequential precision points, and a Mixture of Experts (MoE)
architecture tailored to different linkage types. Each expert model is trained
on type-specific data and guided by a type-specifying layer, enabling both
single-type and multi-type synthesis. A novel simulation metric evaluates
prediction quality by comparing desired and generated motions. Experiments show
our approach produces accurate, defect-free linkages across various
configurations. This enables intuitive and efficient mechanism design, even for
non-expert users, and opens new possibilities for scalable and flexible
synthesis in kinematic design.

</details>


### [43] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin,Gor Matevosyan,Xueying Ma,Vladimir Eremin,Suhaa Dada,Muqun Li,Riyaaz Shaik,Haluk Noyan Tokgozoglu*

Main category: cs.LG

TL;DR: A lightweight safety guardrail framework for language models uses synthetic data and adversarial training to enable small models to outperform larger ones in content moderation.


<details>
  <summary>Details</summary>
Motivation: To reduce computational overhead while maintaining or improving content moderation performance, leveraging smaller models for efficiency.

Method: High-fidelity synthetic data generation from human-curated seeds, augmented and paraphrased, followed by adversarial training with reinforcement learning.

Result: Small language models (SLMs) achieve robust performance in content moderation, surpassing larger models, with improved resilience to adversarial attacks.

Conclusion: The framework offers a scalable, efficient solution for AI content moderation, balancing performance and computational cost.

Abstract: We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [44] [CAS Condensed and Accelerated Silhouette: An Efficient Method for Determining the Optimal K in K-Means Clustering](https://arxiv.org/abs/2507.08311)
*Krishnendu Das,Sumit Gupta,Awadhesh Kumar*

Main category: cs.LG

TL;DR: The paper discusses strategies for selecting the optimal k in clustering, balancing precision and efficiency, and introduces improved techniques for text and image data using the Condensed Silhouette method and other statistical approaches.


<details>
  <summary>Details</summary>
Motivation: Clustering accuracy in large datasets is a challenge, and the paper aims to address this by optimizing k selection and improving computational performance.

Method: The proposed approach uses the Condensed Silhouette method, Local Structures, Gap Statistics, Class Consistency Ratio, and a Cluster Overlap Index (CCR and COI-based algorithm) to determine the best k for K-Means clustering.

Result: Comparative experiments show the approach achieves up to 99% faster execution times on high-dimensional datasets while maintaining precision and scalability.

Conclusion: The method is highly suitable for real-time clustering or scenarios requiring efficient clustering with minimal resources.

Abstract: Clustering is a critical component of decision-making in todays data-driven
environments. It has been widely used in a variety of fields such as
bioinformatics, social network analysis, and image processing. However,
clustering accuracy remains a major challenge in large datasets. This paper
presents a comprehensive overview of strategies for selecting the optimal value
of k in clustering, with a focus on achieving a balance between clustering
precision and computational efficiency in complex data environments. In
addition, this paper introduces improvements to clustering techniques for text
and image data to provide insights into better computational performance and
cluster validity. The proposed approach is based on the Condensed Silhouette
method, along with statistical methods such as Local Structures, Gap
Statistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and
COIbased algorithm to calculate the best value of k for K-Means clustering. The
results of comparative experiments show that the proposed approach achieves up
to 99 percent faster execution times on high-dimensional datasets while
retaining both precision and scalability, making it highly suitable for real
time clustering needs or scenarios demanding efficient clustering with minimal
resource utilization.

</details>


### [45] [A Comprehensively Adaptive Architectural Optimization-Ingrained Quantum Neural Network Model for Cloud Workloads Prediction](https://arxiv.org/abs/2507.08317)
*Jitendra Kumar,Deepika Saxena,Kishu Gupta,Satyam Kumar,Ashutosh Kumar Singh*

Main category: cs.LG

TL;DR: A novel CA-QNN model combines quantum computing and structural optimization for superior cloud workload prediction, reducing errors by up to 93.40% compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional neural networks struggle with diverse, high-dimensional cloud workloads due to limited optimization, prompting the need for a more adaptive solution.

Method: The CA-QNN model uses quantum computing, qubit neurons, and a comprehensive architecture optimization algorithm for structural and parametric learning.

Result: The model outperforms seven state-of-the-art methods, reducing prediction errors by up to 93.40% and 91.27%.

Conclusion: CA-QNN offers a highly accurate and efficient solution for cloud workload prediction, leveraging quantum computing and adaptive optimization.

Abstract: Accurate workload prediction and advanced resource reservation are
indispensably crucial for managing dynamic cloud services. Traditional neural
networks and deep learning models frequently encounter challenges with diverse,
high-dimensional workloads, especially during sudden resource demand changes,
leading to inefficiencies. This issue arises from their limited optimization
during training, relying only on parametric (inter-connection weights)
adjustments using conventional algorithms. To address this issue, this work
proposes a novel Comprehensively Adaptive Architectural Optimization-based
Variable Quantum Neural Network (CA-QNN), which combines the efficiency of
quantum computing with complete structural and qubit vector parametric
learning. The model converts workload data into qubits, processed through qubit
neurons with Controlled NOT-gated activation functions for intuitive pattern
recognition. In addition, a comprehensive architecture optimization algorithm
for networks is introduced to facilitate the learning and propagation of the
structure and parametric values in variable-sized QNNs. This algorithm
incorporates quantum adaptive modulation and size-adaptive recombination during
training process. The performance of CA-QNN model is thoroughly investigated
against seven state-of-the-art methods across four benchmark datasets of
heterogeneous cloud workloads. The proposed model demonstrates superior
prediction accuracy, reducing prediction errors by up to 93.40% and 91.27%
compared to existing deep learning and QNN-based approaches.

</details>


### [46] [scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling](https://arxiv.org/abs/2507.08355)
*Hegang Chen,Yuyin Lu,Zhiming Dai,Fu Lee Wang,Qing Li,Yanghui Rao*

Main category: cs.LG

TL;DR: scE2TM is an external knowledge-guided single-cell embedded topic model that improves clustering performance and interpretability in scRNA-seq data analysis.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing single-cell embedded topic models, such as qualitative interpretability evaluation and neglect of external biological knowledge.

Method: Develops scE2TM, integrating external biological knowledge for embedding learning and clustering, and introduces 10 metrics for quantitative interpretability assessment.

Result: Outperforms 7 state-of-the-art methods in clustering and shows superior interpretability in diversity and biological signal consistency.

Conclusion: scE2TM enhances scRNA-seq analysis by providing high-quality embeddings and robust interpretability, revealing underlying biological mechanisms.

Abstract: Recent advances in sequencing technologies have enabled researchers to
explore cellular heterogeneity at single-cell resolution. Meanwhile,
interpretability has gained prominence parallel to the rapid increase in the
complexity and performance of deep learning models. In recent years, topic
models have been widely used for interpretable single-cell embedding learning
and clustering analysis, which we refer to as single-cell embedded topic
models. However, previous studies evaluated the interpretability of the models
mainly through qualitative analysis, and these single-cell embedded topic
models suffer from the potential problem of interpretation collapse.
Furthermore, their neglect of external biological knowledge constrains
analytical performance. Here, we present scE2TM, an external knowledge-guided
single-cell embedded topic model that provides a high-quality cell embedding
and strong interpretation, contributing to comprehensive scRNA-seq data
analysis. Our comprehensive evaluation across 20 scRNA-seq datasets
demonstrates that scE2TM achieves significant clustering performance gains
compared to 7 state-of-the-art methods. In addition, we propose a new
interpretability evaluation benchmark that introduces 10 metrics to
quantitatively assess the interpretability of single-cell embedded topic
models. The results show that the interpretation provided by scE2TM performs
encouragingly in terms of diversity and consistency with the underlying
biological signals, contributing to a better revealing of the underlying
biological mechanisms.

</details>


### [47] [Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text](https://arxiv.org/abs/2507.08362)
*Phuong Nam Lê,Charlotte Schneider-Depré,Alexandre Goossens,Alexander Stevens,Aurélie Leribaux,Johannes De Smedt*

Main category: cs.LG

TL;DR: The paper introduces an automated pipeline using machine learning and large language models to convert textual process documents into BPMN models, addressing challenges like parallel structures.


<details>
  <summary>Details</summary>
Motivation: Manual conversion of text to BPMN models is time-consuming and costly, with existing methods struggling with writing styles and parallel structures.

Method: The approach leverages machine learning and a newly annotated dataset (augmented PET dataset) with 15 documents and 32 parallel gateways to improve training.

Result: The method shows adequate reconstruction accuracy, better capturing parallel structures in process descriptions.

Conclusion: The proposed pipeline offers a promising solution for organizations to automate and accelerate BPMN model creation.

Abstract: Efficient planning, resource management, and consistent operations often rely
on converting textual process documents into formal Business Process Model and
Notation (BPMN) models. However, this conversion process remains time-intensive
and costly. Existing approaches, whether rule-based or machine-learning-based,
still struggle with writing styles and often fail to identify parallel
structures in process descriptions.
  This paper introduces an automated pipeline for extracting BPMN models from
text, leveraging the use of machine learning and large language models. A key
contribution of this work is the introduction of a newly annotated dataset,
which significantly enhances the training process. Specifically, we augment the
PET dataset with 15 newly annotated documents containing 32 parallel gateways
for model training, a critical feature often overlooked in existing datasets.
This addition enables models to better capture parallel structures, a common
but complex aspect of process descriptions. The proposed approach demonstrates
adequate performance in terms of reconstruction accuracy, offering a promising
foundation for organizations to accelerate BPMN model creation.

</details>


### [48] [Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer](https://arxiv.org/abs/2507.08365)
*Francesco De Cristofaro,Felix Hofbaur,Aixi Yang,Arno Eichberger*

Main category: cs.LG

TL;DR: The paper compares LSTM, CNN, and Transformer networks for predicting lane changes of preceding vehicles, finding Transformers outperform others with high accuracy (82.79% to 96.73%).


<details>
  <summary>Details</summary>
Motivation: Predicting lane changes improves safety and efficiency for automated vehicles, but existing methods lack comparisons of architectures and input configurations.

Method: Implemented LSTM, CNN, and Transformer networks using the highD dataset, focusing on feature selection, data preparation, and model design.

Result: Transformers showed superior performance (82.79%-96.73% accuracy) and were less prone to overfitting compared to LSTMs and CNNs.

Conclusion: Transformer networks are the best choice for lane change prediction, offering high accuracy and robustness.

Abstract: Lane changes of preceding vehicles have a great impact on the motion planning
of automated vehicles especially in complex traffic situations. Predicting them
would benefit the public in terms of safety and efficiency. While many research
efforts have been made in this direction, few concentrated on predicting
maneuvers within a set time interval compared to predicting at a set prediction
time. In addition, there exist a lack of comparisons between different
architectures to try to determine the best performing one and to assess how to
correctly choose the input for such models. In this paper the structure of an
LSTM, a CNN and a Transformer network are described and implemented to predict
the intention of human drivers to perform a lane change. We show how the data
was prepared starting from a publicly available dataset (highD), which features
were used, how the networks were designed and finally we compare the results of
the three networks with different configurations of input data. We found that
transformer networks performed better than the other networks and was less
affected by overfitting. The accuracy of the method spanned from $82.79\%$ to
$96.73\%$ for different input configurations and showed overall good
performances considering also precision and recall.

</details>


### [49] [Advances in Machine Learning: Where Can Quantum Techniques Help?](https://arxiv.org/abs/2507.08379)
*Samarth Kashyap,Rohit K Ramakrishnan,Kumari Jyoti,Apoorva D Patel*

Main category: cs.LG

TL;DR: QML combines quantum computing and AI to enhance data tasks, but faces challenges like NISQ device limitations. Potential lies in specific areas like quantum chemistry.


<details>
  <summary>Details</summary>
Motivation: Address computational bottlenecks in classical machine learning by leveraging quantum advantages.

Method: Review theoretical foundations, categorize QML approaches, and evaluate key developments like Quantum PCA and sensing.

Result: Quantum advantages are problem-dependent; practical utility is limited by NISQ challenges like noise and scalability.

Conclusion: QML shows promise in niche applications but requires quantum-native algorithms and error correction for broader real-world use.

Abstract: Quantum Machine Learning (QML) represents a promising frontier at the
intersection of quantum computing and artificial intelligence, aiming to
leverage quantum computational advantages to enhance data-driven tasks. This
review explores the potential of QML to address the computational bottlenecks
of classical machine learning, particularly in processing complex datasets. We
introduce the theoretical foundations of QML, including quantum data encoding,
quantum learning theory and optimization techniques, while categorizing QML
approaches based on data type and computational architecture. It is
well-established that quantum computational advantages are problem-dependent,
and so potentially useful directions for QML need to be systematically
identified. Key developments, such as Quantum Principal Component Analysis,
quantum-enhanced sensing and applications in material science, are critically
evaluated for their theoretical speed-ups and practical limitations. The
challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including
hardware noise, scalability constraints and data encoding overheads, are
discussed in detail. We also outline future directions, emphasizing the need
for quantum-native algorithms, improved error correction, and realistic
benchmarks to bridge the gap between theoretical promise and practical
deployment. This comprehensive analysis underscores that while QML has
significant potential for specific applications such as quantum chemistry and
sensing, its broader utility in real-world scenarios remains contingent on
overcoming technological and methodological hurdles.

</details>


### [50] [Two-cluster test](https://arxiv.org/abs/2507.08382)
*Xinying Liu,Lianyu Hu,Mudi Jiang,Simen Zhang,Jun Lou,Zengyou He*

Main category: cs.LG

TL;DR: The paper addresses the issue of inflated Type-I error rates in cluster analysis when using classic two-sample tests, proposing a new two-cluster test method based on boundary points to provide accurate p-values.


<details>
  <summary>Details</summary>
Motivation: Traditional two-sample tests are biased in cluster analysis, leading to incorrect conclusions due to inflated Type-I errors. The paper aims to solve this by introducing a specialized two-cluster test.

Method: The authors propose a new method using boundary points between subsets to derive analytical p-values, specifically designed for cluster analysis.

Result: Experiments show the proposed test reduces Type-I error rates compared to classic two-sample tests and is validated in practical applications like interpretable and hierarchical clustering.

Conclusion: The two-cluster test is a distinct and effective solution for significance testing in cluster analysis, with demonstrated practical utility.

Abstract: Cluster analysis is a fundamental research issue in statistics and machine
learning. In many modern clustering methods, we need to determine whether two
subsets of samples come from the same cluster. Since these subsets are usually
generated by certain clustering procedures, the deployment of classic
two-sample tests in this context would yield extremely smaller p-values,
leading to inflated Type-I error rate. To overcome this bias, we formally
introduce the two-cluster test issue and argue that it is a totally different
significance testing issue from conventional two-sample test. Meanwhile, we
present a new method based on the boundary points between two subsets to derive
an analytical p-value for the purpose of significance quantification.
Experiments on both synthetic and real data sets show that the proposed test is
able to significantly reduce the Type-I error rate, in comparison with several
classic two-sample testing methods. More importantly, the practical usage of
such two-cluster test is further verified through its applications in
tree-based interpretable clustering and significance-based hierarchical
clustering.

</details>


### [51] [Online Pre-Training for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2507.08387)
*Yongjae Shin,Jeonghye Kim,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngsoo Jang,Geonhyeong Kim,Jongseong Chae,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: OPT introduces Online Pre-Training to improve offline-to-online RL by addressing inaccurate value estimation, achieving 30% better performance.


<details>
  <summary>Details</summary>
Motivation: Offline pre-trained agents often underperform during online fine-tuning due to distribution shift and inaccurate value estimation.

Method: Proposes OPT, a method with an Online Pre-Training phase to train a new value function for effective online fine-tuning.

Result: 30% average performance improvement across D4RL environments (MuJoCo, Antmaze, Adroit).

Conclusion: OPT effectively bridges the gap between offline and online RL, enhancing fine-tuning performance.

Abstract: Offline-to-online reinforcement learning (RL) aims to integrate the
complementary strengths of offline and online RL by pre-training an agent
offline and subsequently fine-tuning it through online interactions. However,
recent studies reveal that offline pre-trained agents often underperform during
online fine-tuning due to inaccurate value estimation caused by distribution
shift, with random initialization proving more effective in certain cases. In
this work, we propose a novel method, Online Pre-Training for Offline-to-Online
RL (OPT), explicitly designed to address the issue of inaccurate value
estimation in offline pre-trained agents. OPT introduces a new learning phase,
Online Pre-Training, which allows the training of a new value function tailored
specifically for effective online fine-tuning. Implementation of OPT on TD3 and
SPOT demonstrates an average 30% improvement in performance across a wide range
of D4RL environments, including MuJoCo, Antmaze, and Adroit.

</details>


### [52] [Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling](https://arxiv.org/abs/2507.08390)
*Meihua Dang,Jiaqi Han,Minkai Xu,Kai Xu,Akash Srivastava,Stefano Ermon*

Main category: cs.LG

TL;DR: The paper introduces a particle Gibbs sampling approach for inference-time scaling in discrete diffusion models, improving reward-guided text generation by refining multiple trajectories iteratively.


<details>
  <summary>Details</summary>
Motivation: Inference-time scaling in discrete diffusion models is under-explored, limiting their performance in reward-guided settings.

Method: Proposes particle Gibbs sampling with conditional Sequential Monte Carlo to refine diffusion trajectories iteratively, optimizing for reward-weighted targets.

Result: Outperforms prior methods in reward-guided text generation, achieving higher accuracy under fixed compute budgets.

Conclusion: The approach effectively addresses inference-time scaling challenges, offering a robust framework for high-quality text generation.

Abstract: Discrete diffusion models have emerged as a powerful paradigm for language
modeling, rivaling auto-regressive models by training-time scaling. However,
inference-time scaling in discrete diffusion models remains relatively
under-explored. In this work, we study sampling-based approaches for achieving
high-quality text generation from discrete diffusion models in reward-guided
settings. We introduce a novel inference-time scaling approach based on
particle Gibbs sampling for discrete diffusion models. The particle Gibbs
sampling algorithm iteratively refines full diffusion trajectories using
conditional Sequential Monte Carlo as its transition mechanism. This process
ensures that the updated samples progressively improve and move closer to the
reward-weighted target distribution. Unlike existing inference-time scaling
methods, which are often limited to single diffusion trajectories, our approach
leverages iterative refinement across multiple trajectories. Within this
framework, we further analyze the trade-offs between four key axes for
inference-time scaling under fixed compute budgets: particle Gibbs iterations,
particle count, denoising steps, and reward estimation cost. Empirically, our
method consistently outperforms prior inference-time strategies on
reward-guided text generation tasks, achieving significant improvement in
accuracy under varying compute budgets.

</details>


### [53] [Space filling positionality and the Spiroformer](https://arxiv.org/abs/2507.08456)
*M. Maurin,M. Á. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: A transformer model, Spiroformer, is introduced for geometric domains like manifolds, using space-filling curve attention heads to address the lack of global order.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with geometric domains due to undefined global order, motivating a solution for sequential data on manifolds.

Method: Proposes attention heads following a space-filling curve, exemplified by the Spiroformer on a 2-sphere.

Result: Demonstrates the Spiroformer as a viable transformer model for geometric data.

Conclusion: The Spiroformer offers a promising approach to applying transformers in geometric domains.

Abstract: Transformers excel when dealing with sequential data. Generalizing
transformer models to geometric domains, such as manifolds, we encounter the
problem of not having a well-defined global order. We propose a solution with
attention heads following a space-filling curve. As a first experimental
example, we present the Spiroformer, a transformer that follows a polar spiral
on the $2$-sphere.

</details>


### [54] [RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices](https://arxiv.org/abs/2507.08424)
*Anirudh Varanasi,Robin Degraeve,Philippe Roussel,Clement Merckling*

Main category: cs.LG

TL;DR: RTNinja is an automated machine learning framework for unsupervised analysis of random telegraph noise (RTN) signals, outperforming conventional methods by accurately identifying hidden sources without prior knowledge.


<details>
  <summary>Details</summary>
Motivation: RTN impacts device reliability and performance, but existing analysis techniques are limited by assumptions or manual interventions, necessitating a more robust and automated solution.

Method: RTNinja uses Bayesian inference, model selection, probabilistic clustering, and optimization to deconvolve RTN signals into hidden sources. It includes LevelsExtractor for denoising and SourcesMapper for inferring source configurations.

Result: Tested on 7000 simulated datasets, RTNinja achieved high-fidelity signal reconstruction and accurate extraction of source characteristics, demonstrating scalability and robustness.

Conclusion: RTNinja is a versatile, device-agnostic tool for RTN analysis, enabling large-scale benchmarking, reliability qualification, and device physics exploration in nanoelectronics.

Abstract: Random telegraph noise is a prevalent variability phenomenon in
nanoelectronic devices, arising from stochastic carrier exchange at defect
sites and critically impacting device reliability and performance. Conventional
analysis techniques often rely on restrictive assumptions or manual
interventions, limiting their applicability to complex, noisy datasets. Here,
we introduce RTNinja, a generalized, fully automated machine learning framework
for the unsupervised analysis of random telegraph noise signals. RTNinja
deconvolves complex signals to identify the number and characteristics of
hidden individual sources, without requiring prior knowledge of the system. The
framework comprises two modular components: LevelsExtractor, which uses
Bayesian inference and model selection to denoise and discretize the signal;
and SourcesMapper, which infers source configurations through probabilistic
clustering and optimization. To evaluate performance, we developed a Monte
Carlo simulator that generates labeled datasets spanning broad signal-to-noise
ratios and source complexities; across 7000 such datasets, RTNinja consistently
demonstrated high-fidelity signal reconstruction and accurate extraction of
source amplitudes and activity patterns. Our results demonstrate that RTNinja
offers a robust, scalable, and device-agnostic tool for random telegraph noise
characterization, enabling large-scale statistical benchmarking,
reliability-centric technology qualification, predictive failure modeling, and
device physics exploration in next-generation nanoelectronics.

</details>


### [55] [Pre-Training LLMs on a budget: A comparison of three optimizers](https://arxiv.org/abs/2507.08472)
*Joel Schlotthauer,Christian Kroos,Chris Hinze,Viktor Hangya,Luzian Hahn,Fabian Küch*

Main category: cs.LG

TL;DR: Comparison of AdamW, Lion, and Sophia optimizers for LLMs shows Sophia has the lowest loss, Lion is fastest, and AdamW performs best downstream.


<details>
  <summary>Details</summary>
Motivation: To determine the most effective optimizer for reducing pre-training times and improving model performance in LLMs.

Method: Compared AdamW, Lion, and Sophia using two base architectures and single/multiple-epoch approaches, tuning hyperparameters with Maximal Update Parametrization.

Result: Sophia had the lowest training/validation loss, Lion was fastest, and AdamW achieved the best downstream results.

Conclusion: Optimizer choice depends on specific goals: Sophia for loss, Lion for speed, and AdamW for downstream performance.

Abstract: Optimizers play a decisive role in reducing pre-training times for LLMs and
achieving better-performing models. In this study, we compare three major
variants: the de-facto standard AdamW, the simpler Lion, developed through an
evolutionary search, and the second-order optimizer Sophia. For better
generalization, we train with two different base architectures and use a
single- and a multiple-epoch approach while keeping the number of tokens
constant. Using the Maximal Update Parametrization and smaller proxy models, we
tune relevant hyperparameters separately for each combination of base
architecture and optimizer. We found that while the results from all three
optimizers were in approximately the same range, Sophia exhibited the lowest
training and validation loss, Lion was fastest in terms of training GPU hours
but AdamW led to the best downstream evaluation results.

</details>


### [56] [KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations](https://arxiv.org/abs/2507.08443)
*Georgios Balanos,Evangelos Chasanis,Konstantinos Skianis,Evaggelia Pitoura*

Main category: cs.LG

TL;DR: KGRAG-Ex enhances RAG by using knowledge graphs for structured, explainable retrieval, improving factual grounding and interpretability through pseudo-paragraphs and perturbation-based explanations.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of explainability in RAG systems by leveraging structured knowledge graphs for transparent retrieval and reasoning.

Method: Uses a domain-specific KG constructed via prompt-based extraction, identifies relevant entities/paths, transforms them into pseudo-paragraphs, and employs perturbation-based explanations.

Result: Analyzes system sensitivity to perturbations, graph component importance, semantic node types, and graph metrics' influence on explanations.

Conclusion: KGRAG-Ex improves RAG by combining structured KG retrieval with explainability, demonstrating the value of semantic paths and perturbation methods.

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by grounding
responses in external information, yet explainability remains a critical
challenge, particularly when retrieval relies on unstructured text. Knowledge
graphs (KGs) offer a solution by introducing structured, semantically rich
representations of entities and their relationships, enabling transparent
retrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,
a RAG system that improves both factual grounding and explainability by
leveraging a domain-specific KG constructed via prompt-based information
extraction. Given a user query, KGRAG-Ex identifies relevant entities and
semantic paths in the graph, which are then transformed into pseudo-paragraphs:
natural language representations of graph substructures that guide corpus
retrieval. To improve interpretability and support reasoning transparency, we
incorporate perturbation-based explanation methods that assess the influence of
specific KG-derived components on the generated answers. We conduct a series of
experiments to analyze the sensitivity of the system to different perturbation
methods, the relationship between graph component importance and their
structural positions, the influence of semantic node types, and how graph
metrics correspond to the influence of components within the explanations
process.

</details>


### [57] [Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift](https://arxiv.org/abs/2507.08617)
*Tianrun Yu,Jiaqi Wang,Haoyu Wang,Mingquan Lin,Han Liu,Nelson S. Yee,Fenglong Ma*

Main category: cs.LG

TL;DR: FedAKD addresses collaborative fairness in federated learning by tackling imbalanced covariate shift using asynchronous knowledge distillation, improving accuracy and fairness.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook imbalanced covariate shift, a key challenge in federated learning, leading to unfair collaboration.

Method: FedAKD uses client and server updates: asynchronous knowledge distillation for clients and global model updates with high-confidence samples.

Result: FedAKD improves fairness, accuracy, and client participation on datasets like FashionMNIST, CIFAR10, and EHR.

Conclusion: FedAKD effectively balances fairness and accuracy in federated learning, even with heterogeneous data.

Abstract: Collaborative fairness is a crucial challenge in federated learning. However,
existing approaches often overlook a practical yet complex form of
heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of
this setting, which motivates the design of FedAKD (Federated Asynchronous
Knowledge Distillation)- simple yet effective approach that balances accurate
prediction with collaborative fairness. FedAKD consists of client and server
updates. In the client update, we introduce a novel asynchronous knowledge
distillation strategy based on our preliminary analysis, which reveals that
while correctly predicted samples exhibit similar feature distributions across
clients, incorrectly predicted samples show significant variability. This
suggests that imbalanced covariate shift primarily arises from misclassified
samples. Leveraging this insight, our approach first applies traditional
knowledge distillation to update client models while keeping the global model
fixed. Next, we select correctly predicted high-confidence samples and update
the global model using these samples while keeping client models fixed. The
server update simply aggregates all client models. We further provide a
theoretical proof of FedAKD's convergence. Experimental results on public
datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records
(EHR) dataset demonstrate that FedAKD significantly improves collaborative
fairness, enhances predictive accuracy, and fosters client participation even
under highly heterogeneous data distributions.

</details>


### [58] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

Main category: cs.LG

TL;DR: WERSA introduces linear-time attention for long sequences, outperforming other methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency of quadratic-time attention in transformers for long sequences.

Method: Combines content-adaptive random spectral features, multi-resolution Haar wavelets, and learnable parameters for selective attention.

Result: Achieves best accuracy in benchmarks, reduces training time by 81%, and cuts FLOPS by 73.4%. Excels on extremely long sequences.

Conclusion: WERSA enables practical, efficient long-context models, especially for low-resource hardware, advancing scalable AI.

Abstract: Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [59] [Ranked Set Sampling-Based Multilayer Perceptron: Improving Generalization via Variance-Based Bounds](https://arxiv.org/abs/2507.08465)
*Feijiang Li,Liuya Zhang,Jieting Wang,Tao Yan,Yuhua Qian*

Main category: cs.LG

TL;DR: The paper introduces a new generalization error bound for MLPs, advocating variance reduction of empirical loss. It proposes RSS-MLP, using Rank Set Sampling (RSS) instead of Simple Random Sampling (SRS) to reduce variance further, validated by experiments on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance MLP's generalization ability by reducing the variance of empirical loss, addressing the high randomness in traditional bagging methods.

Method: Proposes RSS-MLP, replacing SRS in bagging with RSS to introduce ordered structure in training data, theoretically and experimentally showing reduced variance.

Result: Theoretical and experimental results confirm RSS-MLP reduces variance of empirical loss more effectively than SRS, improving performance.

Conclusion: RSS-MLP is effective and rational for enhancing MLP's generalization ability by reducing empirical loss variance.

Abstract: Multilayer perceptron (MLP), one of the most fundamental neural networks, is
extensively utilized for classification and regression tasks. In this paper, we
establish a new generalization error bound, which reveals how the variance of
empirical loss influences the generalization ability of the learning model.
Inspired by this learning bound, we advocate to reduce the variance of
empirical loss to enhance the ability of MLP. As is well-known, bagging is a
popular ensemble method to realize variance reduction. However, bagging
produces the base training data sets by the Simple Random Sampling (SRS)
method, which exhibits a high degree of randomness. To handle this issue, we
introduce an ordered structure in the training data set by Rank Set Sampling
(RSS) to further reduce the variance of loss and develop a RSS-MLP method.
Theoretical results show that the variance of empirical exponential loss and
the logistic loss estimated by RSS are smaller than those estimated by SRS,
respectively. To validate the performance of RSS-MLP, we conduct comparison
experiments on twelve benchmark data sets in terms of the two convex loss
functions under two fusion methods. Extensive experimental results and analysis
illustrate the effectiveness and rationality of the propose method.

</details>


### [60] [Monitoring Risks in Test-Time Adaptation](https://arxiv.org/abs/2507.08721)
*Mona Schirmer,Metod Jazbec,Christian A. Naesseth,Eric Nalisnick*

Main category: cs.LG

TL;DR: The paper proposes pairing test-time adaptation (TTA) with risk monitoring frameworks to detect model degradation, extending sequential testing tools for TTA scenarios without labeled test data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of model degradation over time due to shifted test data, ensuring timely detection of performance failure.

Method: Extends sequential testing with confidence sequences to monitor performance without labeled test data, adapting existing tools for TTA.

Result: Demonstrates effectiveness across diverse datasets, shift types, and TTA methods.

Conclusion: Proposed framework enables rigorous statistical risk monitoring for TTA, ensuring timely model retraining.

Abstract: Encountering shifted data at test time is a ubiquitous challenge when
deploying predictive models. Test-time adaptation (TTA) methods address this
issue by continuously adapting a deployed model using only unlabeled test data.
While TTA can extend the model's lifespan, it is only a temporary solution.
Eventually the model might degrade to the point that it must be taken offline
and retrained. To detect such points of ultimate failure, we propose pairing
TTA with risk monitoring frameworks that track predictive performance and raise
alerts when predefined performance criteria are violated. Specifically, we
extend existing monitoring tools based on sequential testing with confidence
sequences to accommodate scenarios in which the model is updated at test time
and no test labels are available to estimate the performance metrics of
interest. Our extensions unlock the application of rigorous statistical risk
monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA
monitoring framework across a representative set of datasets, distribution
shift types, and TTA methods.

</details>


### [61] [Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling](https://arxiv.org/abs/2507.08736)
*Idan Mashiach,Oren Glickman,Tom Tirer*

Main category: cs.LG

TL;DR: A novel method to mitigate catastrophic forgetting in deep neural networks by tracking parameter activity during the final training plateau, identifying flat loss landscape directions for better adaptation.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting degrades performance on old tasks when learning new ones; current regularization techniques may not optimally identify important parameters.

Method: Track parameter activity during the final training plateau to identify flat loss landscape directions, enabling adaptation to new tasks while preserving old knowledge.

Result: Superior performance in balancing catastrophic forgetting mitigation with strong performance on new tasks.

Conclusion: Monitoring parameter activity during the final plateau is more effective for mitigating catastrophic forgetting than tracking throughout training.

Abstract: Catastrophic forgetting in deep neural networks occurs when learning new
tasks degrades performance on previously learned tasks due to knowledge
overwriting. Among the approaches to mitigate this issue, regularization
techniques aim to identify and constrain "important" parameters to preserve
previous knowledge. In the highly nonconvex optimization landscape of deep
learning, we propose a novel perspective: tracking parameters during the final
training plateau is more effective than monitoring them throughout the entire
training process. We argue that parameters that exhibit higher activity
(movement and variability) during this plateau reveal directions in the loss
landscape that are relatively flat, making them suitable for adaptation to new
tasks while preserving knowledge from previous ones. Our comprehensive
experiments demonstrate that this approach achieves superior performance in
balancing catastrophic forgetting mitigation with strong performance on newly
learned tasks.

</details>


### [62] [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473)
*Gonçalo Paulo,Nora Belrose*

Main category: cs.LG

TL;DR: The paper proposes a method to assess the interpretability of sparse autoencoders (SAEs) and transcoders without relying on natural language explanations, enabling more direct evaluation. It compares these metrics with human evaluations and suggests improvements for community standards.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating the interpretability of SAEs and transcoders rely on generating natural language explanations, which complicates the process and may not directly measure interpretability. The paper aims to provide a more straightforward and standardized approach.

Method: The authors adapt existing methods to evaluate sparse coders without needing natural language explanations. They compare their interpretability metrics with human evaluations across various tasks and setups.

Result: The proposed metrics offer a more direct and potentially standardized way to assess interpretability. Comparisons with human evaluations provide insights into improving evaluation techniques.

Conclusion: The work suggests that avoiding natural language explanations can lead to better interpretability assessments and offers recommendations for the community to refine evaluation methods for sparse coding techniques.

Abstract: Sparse autoencoders (SAEs) and transcoders have become important tools for
machine learning interpretability. However, measuring how interpretable they
are remains challenging, with weak consensus about which benchmarks to use.
Most evaluation procedures start by producing a single-sentence explanation for
each latent. These explanations are then evaluated based on how well they
enable an LLM to predict the activation of a latent in new contexts. This
method makes it difficult to disentangle the explanation generation and
evaluation process from the actual interpretability of the latents discovered.
In this work, we adapt existing methods to assess the interpretability of
sparse coders, with the advantage that they do not require generating natural
language explanations as an intermediate step. This enables a more direct and
potentially standardized assessment of interpretability. Furthermore, we
compare the scores produced by our interpretability metrics with human
evaluations across similar tasks and varying setups, offering suggestions for
the community on improving the evaluation of these techniques.

</details>


### [63] [Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series](https://arxiv.org/abs/2507.08738)
*Azimov Sherkhon,Susana Lopez-Moreno,Eric Dolores-Cuenca,Sieun Lee,Sangil Kim*

Main category: cs.LG

TL;DR: The paper proposes an adaptive NVAR model combining delay-embedded linear inputs with a learnable MLP, outperforming standard NVAR in accuracy and robustness under noise.


<details>
  <summary>Details</summary>
Motivation: Standard NVAR and RC methods have fixed nonlinearities and poor scalability, limiting their adaptability to noisy or high-dimensional data.

Method: The adaptive NVAR uses a shallow MLP for feature generation and a linear readout, jointly trained with gradient-based optimization.

Result: The model outperformed standard NVAR in predictive accuracy and robustness under noisy conditions.

Conclusion: The adaptive NVAR offers improved performance and scalability, avoiding exhaustive parameter tuning.

Abstract: Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have
shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63
model and El Nino-Southern Oscillation. However, their reliance on fixed
nonlinearities - polynomial expansions in NVAR or random feature maps in RC -
limits their adaptability to high noise or real-world data. These methods also
scale poorly in high-dimensional settings due to costly matrix inversion during
readout computation. We propose an adaptive NVAR model that combines
delay-embedded linear inputs with features generated by a shallow, learnable
multi-layer perceptron (MLP). The MLP and linear readout are jointly trained
using gradient-based optimization, enabling the model to learn data-driven
nonlinearities while preserving a simple readout structure. Unlike standard
NVAR, our approach avoids the need for an exhaustive and sensitive grid search
over ridge and delay parameters. Instead, tuning is restricted to neural
network hyperparameters, improving scalability. Initial experiments on chaotic
systems tested under noise-free and synthetically noisy conditions showed that
the adaptive model outperformed the standard NVAR in predictive accuracy and
showed robust forecasting under noisy conditions with a lower observation
frequency.

</details>


### [64] [SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction](https://arxiv.org/abs/2507.08475)
*Haitao Lin,Junjie Wang,Zhifeng Gao,Xiaohong Ji,Rong Zhu,Linfeng Zhang,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: SynBridge is a bidirectional flow-based generative model for multi-task reaction prediction, achieving state-of-the-art results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Chemical reactions involve discrete electron redistribution, requiring models to capture abrupt state changes like bond formation/breaking.

Method: Uses a graph-to-graph transformer and discrete flow bridges to model bidirectional transformations between reactants and products.

Result: Achieves top performance on USPTO-50K, USPTO-MIT, and Pistachio datasets for forward and retrosynthesis tasks.

Conclusion: Structured diffusion over discrete spaces enhances reaction prediction, as validated by ablation studies.

Abstract: The essence of a chemical reaction lies in the redistribution and
reorganization of electrons, which is often manifested through electron
transfer or the migration of electron pairs. These changes are inherently
discrete and abrupt in the physical world, such as alterations in the charge
states of atoms or the formation and breaking of chemical bonds. To model the
transition of states, we propose SynBridge, a bidirectional flow-based
generative model to achieve multi-task reaction prediction. By leveraging a
graph-to-graph transformer network architecture and discrete flow bridges
between any two discrete distributions, SynBridge captures bidirectional
chemical transformations between graphs of reactants and products through the
bonds' and atoms' discrete states. We further demonstrate the effectiveness of
our method through extensive experiments on three benchmark datasets
(USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in
both forward and retrosynthesis tasks. Our ablation studies and noise
scheduling analysis reveal the benefits of structured diffusion over discrete
spaces for reaction prediction.

</details>


### [65] [Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data](https://arxiv.org/abs/2507.08761)
*Jeonghye Kim,Yongjae Shin,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: PARS algorithm mitigates Q-value extrapolation errors in offline RL using RS-LN and PA, outperforming state-of-the-art methods on D4RL benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address Q-value extrapolation errors in offline reinforcement learning, which degrade performance.

Method: Propose RS-LN (reward scaling with layer normalization) and PA (penalization for infeasible actions), combined into PARS.

Result: PARS achieves superior performance on D4RL benchmarks, especially in AntMaze Ultra.

Conclusion: PARS effectively reduces extrapolation errors and enhances offline and online RL performance.

Abstract: Reinforcement learning with offline data suffers from Q-value extrapolation
errors. To address this issue, we first demonstrate that linear extrapolation
of the Q-function beyond the data range is particularly problematic. To
mitigate this, we propose guiding the gradual decrease of Q-values outside the
data range, which is achieved through reward scaling with layer normalization
(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining
RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a
range of tasks, demonstrating superior performance compared to state-of-the-art
algorithms in both offline training and online fine-tuning on the D4RL
benchmark, with notable success in the challenging AntMaze Ultra task.

</details>


### [66] [Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R](https://arxiv.org/abs/2507.08505)
*Pablo Robin Guerrero,Yueyang Pan,Sanidhya Kashyap*

Main category: cs.LG

TL;DR: A survey of deployment frameworks for Vision-Language Models (VLMs) on mobile devices reveals CPU overuse and inefficient GPU/NPU utilization, impacting performance and user experience.


<details>
  <summary>Details</summary>
Motivation: Deploying VLMs on mobile devices is challenging due to computational and energy constraints, especially for real-time applications.

Method: Evaluated llama.cpp, MLC-Imp, and mllm frameworks running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B on a OnePlus 13R, measuring CPU, GPU, NPU usage, temperature, inference time, power, and user experience.

Result: CPU overuse during token generation and inefficient GPU/NPU utilization (saturated GPU for image tasks) degraded performance and responsiveness.

Conclusion: Current frameworks have hardware utilization bottlenecks, emphasizing the need for better CPU, GPU, and NPU optimization in VLM deployment.

Abstract: Vision-Language Models (VLMs) offer promising capabilities for mobile
devices, but their deployment faces significant challenges due to computational
limitations and energy inefficiency, especially for real-time applications.
This study provides a comprehensive survey of deployment frameworks for VLMs on
mobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of
running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads
on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R
while running VLMs, with measurements covering CPU, GPU, and NPU utilization,
temperature, inference time, power consumption, and user experience.
Benchmarking revealed critical performance bottlenecks across frameworks: CPU
resources were consistently over-utilized during token generation, while GPU
and NPU accelerators were largely unused. When the GPU was used, primarily for
image feature extraction, it was saturated, leading to degraded device
responsiveness. The study contributes framework-level benchmarks, practical
profiling tools, and an in-depth analysis of hardware utilization bottlenecks,
highlighting the consistent overuse of CPUs and the ineffective or unstable use
of GPUs and NPUs in current deployment frameworks.

</details>


### [67] [Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning](https://arxiv.org/abs/2507.08793)
*James McCarthy,Radu Marinescu,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: ORAC is an exploration-based method for risk-averse constrained RL, balancing reward maximization and safety constraint satisfaction by adjusting cost weights dynamically.


<details>
  <summary>Details</summary>
Motivation: Risk-averse RL often leads to overly conservative policies, resulting in sub-optimal rewards or failure to achieve goals. ORAC addresses this by encouraging exploration while maintaining safety.

Method: ORAC constructs an exploratory policy by maximizing reward upper confidence bounds and minimizing risk-averse cost lower bounds, dynamically adjusting cost weights based on constraint violations.

Result: ORAC avoids sub-optimal policies and improves the reward-cost trade-off in tasks like Safety-Gymnasium and CityLearn.

Conclusion: ORAC effectively balances exploration and safety in risk-averse RL, outperforming traditional methods in complex environments.

Abstract: Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies
that minimise the likelihood of rare and catastrophic constraint violations
caused by an environment's inherent randomness. In general, risk-aversion leads
to conservative exploration of the environment which typically results in
converging to sub-optimal policies that fail to adequately maximise reward or,
in some cases, fail to achieve the goal. In this paper, we propose an
exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic
(ORAC), which constructs an exploratory policy by maximising a local upper
confidence bound of the state-action reward value function whilst minimising a
local lower confidence bound of the risk-averse state-action cost value
function. Specifically, at each step, the weighting assigned to the cost value
is increased or decreased if it exceeds or falls below the safety constraint
value. This way the policy is encouraged to explore uncertain regions of the
environment to discover high reward states whilst still satisfying the safety
constraints. Our experimental results demonstrate that the ORAC approach
prevents convergence to sub-optimal policies and improves significantly the
reward-cost trade-off in various continuous control tasks such as
Safety-Gymnasium and a complex building energy management environment
CityLearn.

</details>


### [68] [SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2507.08508)
*Haotian Xu,Jinrui Zhou,Xichong Zhang,Mingjun Xiao,He Sun,Yin Xu*

Main category: cs.LG

TL;DR: SFedKD is a novel SFL framework using multi-teacher knowledge distillation to mitigate catastrophic forgetting in heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: SFL suffers from catastrophic forgetting in heterogeneous settings, degrading model performance.

Method: SFedKD employs discrepancy-aware multi-teacher knowledge distillation, fine-grained weighting, and a complementary-based teacher selection mechanism.

Result: SFedKD outperforms state-of-the-art FL methods by effectively addressing catastrophic forgetting.

Conclusion: SFedKD enhances SFL training efficacy and mitigates forgetting, validated by extensive experiments.

Abstract: Federated Learning (FL) is a distributed machine learning paradigm which
coordinates multiple clients to collaboratively train a global model via a
central server. Sequential Federated Learning (SFL) is a newly-emerging FL
training framework where the global model is trained in a sequential manner
across clients. Since SFL can provide strong convergence guarantees under data
heterogeneity, it has attracted significant research attention in recent years.
However, experiments show that SFL suffers from severe catastrophic forgetting
in heterogeneous environments, meaning that the model tends to forget knowledge
learned from previous clients. To address this issue, we propose an SFL
framework with discrepancy-aware multi-teacher knowledge distillation, called
SFedKD, which selects multiple models from the previous round to guide the
current round of training. In SFedKD, we extend the single-teacher Decoupled
Knowledge Distillation approach to our multi-teacher setting and assign
distinct weights to teachers' target-class and non-target-class knowledge based
on the class distributional discrepancy between teacher and student data.
Through this fine-grained weighting strategy, SFedKD can enhance model training
efficacy while mitigating catastrophic forgetting. Additionally, to prevent
knowledge dilution, we eliminate redundant teachers for the knowledge
distillation and formalize it as a variant of the maximum coverage problem.
Based on the greedy strategy, we design a complementary-based teacher selection
mechanism to ensure that the selected teachers achieve comprehensive knowledge
space coverage while reducing communication and computational costs. Extensive
experiments show that SFedKD effectively overcomes catastrophic forgetting in
SFL and outperforms state-of-the-art FL methods.

</details>


### [69] [Recursive Reward Aggregation](https://arxiv.org/abs/2507.08537)
*Yuting Tang,Yivan Zhang,Johannes Ackermann,Yu-Jie Zhang,Soichiro Nishimori,Masashi Sugiyama*

Main category: cs.LG

TL;DR: Proposes a flexible behavior alignment method in RL by selecting reward aggregation functions, eliminating the need for reward function redesign.


<details>
  <summary>Details</summary>
Motivation: Aligning agent behavior with complex objectives in RL is challenging due to reward function design.

Method: Introduces an algebraic perspective on MDPs, generalizing reward aggregation (e.g., discounted max, Sharpe ratio) beyond standard discounted sums.

Result: Effective optimization of diverse objectives in deterministic and stochastic settings, compatible with value-based and actor-critic algorithms.

Conclusion: The approach is versatile and promising for real-world RL applications.

Abstract: In reinforcement learning (RL), aligning agent behavior with specific
objectives typically requires careful design of the reward function, which can
be challenging when the desired objectives are complex. In this work, we
propose an alternative approach for flexible behavior alignment that eliminates
the need to modify the reward function by selecting appropriate reward
aggregation functions. By introducing an algebraic perspective on Markov
decision processes (MDPs), we show that the Bellman equations naturally emerge
from the recursive generation and aggregation of rewards, allowing for the
generalization of the standard discounted sum to other recursive aggregations,
such as discounted max and Sharpe ratio. Our approach applies to both
deterministic and stochastic settings and integrates seamlessly with
value-based and actor-critic algorithms. Experimental results demonstrate that
our approach effectively optimizes diverse objectives, highlighting its
versatility and potential for real-world applications.

</details>


### [70] [CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes](https://arxiv.org/abs/2507.08542)
*Tianyou Jiang*

Main category: cs.LG

TL;DR: CircFormerMoE, a deep learning framework using transformers and mixture-of-experts, predicts plant circRNAs directly from genomic DNA, overcoming limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional circRNA identification methods rely on RNA-seq data, are computationally expensive, and inefficient for large-scale prediction, especially in plants with non-canonical splice sites.

Method: Proposes CircFormerMoE, combining transformers and mixture-of-experts, with subtasks for splicing site detection (SSD) and pairing (SSP), validated on 10 plant species.

Result: Effectively predicts circRNAs, including unannotated ones, and provides interpretability of sequence patterns.

Conclusion: Offers a fast, accurate tool for large-scale plant circRNA discovery, aiding future functional genomics and non-coding RNA research.

Abstract: Circular RNAs (circRNAs) are important components of the non-coding RNA
regulatory network. Previous circRNA identification primarily relies on
high-throughput RNA sequencing (RNA-seq) data combined with alignment-based
algorithms that detect back-splicing signals. However, these methods face
several limitations: they can't predict circRNAs directly from genomic DNA
sequences and relies heavily on RNA experimental data; they involve high
computational costs due to complex alignment and filtering steps; and they are
inefficient for large-scale or genome-wide circRNA prediction. The challenge is
even greater in plants, where plant circRNA splice sites often lack the
canonical GT-AG motif seen in human mRNA splicing, and no efficient deep
learning model with strong generalization capability currently exists.
Furthermore, the number of currently identified plant circRNAs is likely far
lower than their true abundance. In this paper, we propose a deep learning
framework named CircFormerMoE based on transformers and mixture-of experts for
predicting circRNAs directly from plant genomic DNA. Our framework consists of
two subtasks known as splicing site detection (SSD) and splicing site pairing
(SSP). The model's effectiveness has been validated on gene data of 10 plant
species. Trained on known circRNA instances, it is also capable of discovering
previously unannotated circRNAs. In addition, we performed interpretability
analyses on the trained model to investigate the sequence patterns contributing
to its predictions. Our framework provides a fast and accurate computational
method and tool for large-scale circRNA discovery in plants, laying a
foundation for future research in plant functional genomics and non-coding RNA
annotation.

</details>


### [71] [STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving](https://arxiv.org/abs/2507.08563)
*Xinyi Ning,Zilin Bian,Kaan Ozbay,Semiha Ergan*

Main category: cs.LG

TL;DR: A novel spatial-temporal risk-attentive framework improves vehicle trajectory prediction by incorporating risk potential fields, reducing errors by 4.8% (NGSIM) and 31.2% (HighD).


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect risks from uncertain/aggressive behaviors of surrounding vehicles, compromising safety in autonomous driving.

Method: Proposes a framework with a spatial-temporal encoder, risk-attentive decoder, and risk-scaled loss function to embed risk potential fields.

Result: Reduces prediction errors by 4.8% (NGSIM) and 31.2% (HighD), excelling in high-risk scenarios.

Conclusion: The framework offers interpretable, risk-aware predictions, enhancing robustness for autonomous driving systems.

Abstract: Accurate vehicle trajectory prediction is essential for ensuring safety and
efficiency in fully autonomous driving systems. While existing methods
primarily focus on modeling observed motion patterns and interactions with
other vehicles, they often neglect the potential risks posed by the uncertain
or aggressive behaviors of surrounding vehicles. In this paper, we propose a
novel spatial-temporal risk-attentive trajectory prediction framework that
incorporates a risk potential field to assess perceived risks arising from
behaviors of nearby vehicles. The framework leverages a spatial-temporal
encoder and a risk-attentive feature fusion decoder to embed the risk potential
field into the extracted spatial-temporal feature representations for
trajectory prediction. A risk-scaled loss function is further designed to
improve the prediction accuracy of high-risk scenarios, such as short relative
spacing. Experiments on the widely used NGSIM and HighD datasets demonstrate
that our method reduces average prediction errors by 4.8% and 31.2%
respectively compared to state-of-the-art approaches, especially in high-risk
scenarios. The proposed framework provides interpretable, risk-aware
predictions, contributing to more robust decision-making for autonomous driving
systems.

</details>


### [72] [AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling](https://arxiv.org/abs/2507.08567)
*Preslav Aleksandrov,Meghdad Kurmanji,Fernando Garcia Redondo,David O'Shea,William Shen,Alex Iacob,Lorenzo Sani,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.LG

TL;DR: AbbIE is a recursive Transformer variant improving perplexity and compute scaling, outperforming standard and iterative methods by up to 12% in zero-shot tasks and 5% in perplexity.


<details>
  <summary>Details</summary>
Motivation: To enhance Transformer performance without relying solely on parameter or token scaling, and to enable dynamic compute adjustment.

Method: Recursive, autoregressive block-based iterative encoder (AbbIE) operating in latent space, trained with 2 iterations but generalizing to longer ones.

Result: 12% better zero-shot learning and 5% lower perplexity than alternatives, with dynamic compute scaling.

Conclusion: AbbIE offers a new path for scaling Transformer performance beyond traditional methods.

Abstract: We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a
novel recursive generalization of the encoder-only Transformer architecture,
which achieves better perplexity than a standard Transformer and allows for the
dynamic scaling of compute resources at test time. This simple, recursive
approach is a complement to scaling large language model (LLM) performance
through parameter and token counts. AbbIE performs its iterations in latent
space, but unlike latent reasoning models, does not require a specialized
dataset or training protocol. We show that AbbIE upward generalizes (ability to
generalize to arbitrary iteration lengths) at test time by only using 2
iterations during train time, far outperforming alternative iterative methods.
AbbIE's ability to scale its computational expenditure based on the complexity
of the task gives it an up to \textbf{12\%} improvement in zero-shot in-context
learning tasks versus other iterative and standard methods and up to 5\%
improvement in language perplexity. The results from this study open a new
avenue to Transformer performance scaling. We perform all of our evaluations on
model sizes up to 350M parameters.

</details>


### [73] [ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware Detection](https://arxiv.org/abs/2507.08597)
*Md Tanvirul Alam,Aritran Piplai,Nidhi Rastogi*

Main category: cs.LG

TL;DR: ADAPT, a novel pseudo-labeling semi-supervised algorithm, addresses concept drift in malware detection, outperforming baselines across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Performance degradation of ML models in malware classification due to concept drift and the high cost of ground truth annotations.

Method: Introduces ADAPT, a model-agnostic pseudo-labeling semi-supervised algorithm applicable to various ML models.

Result: Outperforms baseline models and benchmarks on five malware detection datasets.

Conclusion: ADAPT enables more effective adaptation of ML models to concept drift in malware detection.

Abstract: Machine learning models are commonly used for malware classification;
however, they suffer from performance degradation over time due to concept
drift. Adapting these models to changing data distributions requires frequent
updates, which rely on costly ground truth annotations. While active learning
can reduce the annotation burden, leveraging unlabeled data through
semi-supervised learning remains a relatively underexplored approach in the
context of malware detection. In this research, we introduce \texttt{ADAPT}, a
novel pseudo-labeling semi-supervised algorithm for addressing concept drift.
Our model-agnostic method can be applied to various machine learning models,
including neural networks and tree-based algorithms. We conduct extensive
experiments on five diverse malware detection datasets spanning Android,
Windows, and PDF domains. The results demonstrate that our method consistently
outperforms baseline models and competitive benchmarks. This work paves the way
for more effective adaptation of machine learning models to concept drift in
malware detection.

</details>


### [74] [Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India](https://arxiv.org/abs/2507.08605)
*Ando Shah,Rajveer Singh,Akram Zaytar,Girmaw Abebe Tadesse,Caleb Robinson,Negar Tafti,Stephen A. Wood,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: A remote sensing framework was developed to monitor sustainable water management practices in rice cultivation, achieving 78% accuracy in distinguishing direct seeded rice (DSR) from traditional methods, aiding policymakers in tracking adoption and resource allocation.


<details>
  <summary>Details</summary>
Motivation: Rice cultivation's high water use (24-30% of global freshwater) and groundwater depletion in regions like Punjab, India, necessitate sustainable practices like DSR and AWD, but lack of adoption data hinders policymaking.

Method: A novel remote sensing framework using Sentinel-1 satellite imagery and ground truth data from farmer training programs classified water management practices, achieving 78% F1-score for DSR identification.

Result: The framework successfully mapped DSR adoption across 3 million plots in Punjab, with strong correlation (Pearson=0.77) to government records, demonstrating scalability.

Conclusion: The study offers a scalable tool for policymakers to monitor sustainable water management adoption, target interventions, and assess program impacts effectively.

Abstract: Rice cultivation consumes 24-30% of global freshwater, creating critical
water management challenges in major rice-producing regions. Sustainable
irrigation practices like direct seeded rice (DSR) and alternate wetting and
drying (AWD) can reduce water use by 20-40% while maintaining yields, helping
secure long-term agricultural productivity as water scarcity intensifies - a
key component of the Zero Hunger Sustainable Development Goal. However, limited
data on adoption rates of these practices prevents evidence-based policymaking
and targeted resource allocation. We developed a novel remote sensing framework
to monitor sustainable water management practices at scale in Punjab, India - a
region facing severe groundwater depletion of 41.6 cm/year. To collect
essential ground truth data, we partnered with the Nature Conservancy's
Promoting Regenerative and No-burn Agriculture (PRANA) program, which trained
approximately 1,400 farmers on water-saving techniques while documenting their
field-level practices. Using this data, we created a classification system with
Sentinel-1 satellite imagery that separates water management along sowing and
irrigation dimensions. Our approach achieved a 78% F1-score in distinguishing
DSR from traditional puddled transplanted rice without requiring prior
knowledge of planting dates. We demonstrated scalability by mapping DSR
adoption across approximately 3 million agricultural plots in Punjab, with
district-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)
with government records. This study provides policymakers with a powerful tool
to track sustainable water management adoption, target interventions, and
measure program impacts at scale.

</details>


### [75] [Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data](https://arxiv.org/abs/2507.08610)
*Parag Dutta,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: The paper introduces LoGIC, a multi-agent reinforcement learning game for unsupervised image captioning, achieving improved performance without additional labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing labeled datasets are exhausted for training VLMs, making unsupervised methods essential for further improvements in image captioning.

Method: Proposes LoGIC with two agents (speaker and listener) trained using GRPO algorithm, leveraging pre-trained VLMs and LLMs, and lightweight components for unsupervised settings.

Result: Achieves 46 BLEU score with pre-trained models (2 points over vanilla VLM) and 31 BLEU score with lightweight components (10 points over existing unsupervised methods).

Conclusion: LoGIC effectively improves unsupervised image captioning performance, demonstrating the potential of multi-agent reinforcement learning in this domain.

Abstract: Image captioning is an important problem in developing various AI systems,
and these tasks require large volumes of annotated images to train the models.
Since all existing labelled datasets are already used for training the large
Vision Language Models (VLMs), it becomes challenging to improve the
performance of the same. Considering this, it is essential to consider the
unsupervised image captioning performance, which remains relatively
under-explored. To that end, we propose LoGIC (Lewis Communication Game for
Image Captioning), a Multi-agent Reinforcement Learning game. The proposed
method consists of two agents, a 'speaker' and a 'listener', with the objective
of learning a strategy for communicating in natural language. We train agents
in the cooperative common-reward setting using the GRPO algorithm and show that
improvement in image captioning performance emerges as a consequence of the
agents learning to play the game. We show that using pre-trained VLMs as the
'speaker' and Large Language Model (LLM) for language understanding in the
'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without
additional labels, a $2$ units advantage in absolute metrics compared to the
$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the
'speaker' with lightweight components: (i) a ViT for image perception and (ii)
a GPT2 language generation, and train them from scratch using LoGIC, obtaining
a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over
existing unsupervised image-captioning methods.

</details>


### [76] [Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation](https://arxiv.org/abs/2507.08686)
*Uri Stern,Eli Corn,Daphna Weinshall*

Main category: cs.LG

TL;DR: The paper introduces a novel score to measure local overfitting in deep neural networks, proposes a two-stage method to recover forgotten knowledge, and demonstrates its effectiveness across various datasets.


<details>
  <summary>Details</summary>
Motivation: Despite theory predicting overfitting with increased model capacity, it rarely occurs globally. The study explores local overfitting and its link to the double descent phenomenon.

Method: A two-stage approach: (1) aggregating checkpoints into an ensemble, and (2) distilling it into a single model to retain forgotten knowledge.

Result: The method outperforms original models and ensembles, especially in noisy label scenarios, reducing training and inference complexity.

Conclusion: Local overfitting is a critical issue, and the proposed method effectively mitigates it while improving performance.

Abstract: Overfitting in deep neural networks occurs less frequently than expected.
This is a puzzling observation, as theory predicts that greater model capacity
should eventually lead to overfitting -- yet this is rarely seen in practice.
But what if overfitting does occur, not globally, but in specific sub-regions
of the data space? In this work, we introduce a novel score that measures the
forgetting rate of deep models on validation data, capturing what we term local
overfitting: a performance degradation confined to certain regions of the input
space. We demonstrate that local overfitting can arise even without
conventional overfitting, and is closely linked to the double descent
phenomenon.
  Building on these insights, we introduce a two-stage approach that leverages
the training history of a single model to recover and retain forgotten
knowledge: first, by aggregating checkpoints into an ensemble, and then by
distilling it into a single model of the original size, thus enhancing
performance without added inference cost.
  Extensive experiments across multiple datasets, modern architectures, and
training regimes validate the effectiveness of our approach. Notably, in the
presence of label noise, our method -- Knowledge Fusion followed by Knowledge
Distillation -- outperforms both the original model and independently trained
ensembles, achieving a rare win-win scenario: reduced training and inference
complexity.

</details>


### [77] [Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning](https://arxiv.org/abs/2507.08697)
*Waqar Muhammad Ashraf,Amir H. Keshavarzzadeh,Abdulelah S. Alshehri,Abdulrahman bin Jumah,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: The paper introduces MAD-OPT, a framework integrating domain knowledge via Mahalanobis distance to optimize thermal efficiency and turbine heat rate in gas turbines, showing robust and implementable solutions.


<details>
  <summary>Details</summary>
Motivation: Low AI adoption in thermal power plants due to black-box AI and lack of domain knowledge in data-centric analytics.

Method: Developed MAD-OPT framework with Mahalanobis distance-based constraints to incorporate domain knowledge into analytics.

Result: MAD-OPT successfully estimated optimal process conditions, validated by Monte Carlo simulations and real plant data.

Conclusion: Domain-informed constraints are crucial for effective AI solutions in thermal power systems, enhancing operational excellence and safe AI adoption.

Abstract: The domain-consistent adoption of artificial intelligence (AI) remains low in
thermal power plants due to the black-box nature of AI algorithms and low
representation of domain knowledge in conventional data-centric analytics. In
this paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)
framework that incorporates the Mahalanobis distance-based constraint to
introduce domain knowledge into data-centric analytics. The developed MAD-OPT
framework is applied to maximize thermal efficiency and minimize turbine heat
rate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT
framework can estimate domain-informed optimal process conditions under
different ambient conditions, and the optimal solutions are found to be robust
as evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to
estimate optimal process conditions beyond the design power generation limit of
the gas turbine system, and have found comparable results with the actual data
of the power plant. We demonstrate that implementing data-centric optimization
analytics without incorporating domain-informed constraints may provide
ineffective solutions that may not be implementable in the real operation of
the gas turbine system. This research advances the integration of the
data-driven domain knowledge into machine learning-powered analytics that
enhances the domain-informed operation excellence and paves the way for safe AI
adoption in thermal power systems.

</details>


### [78] [SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations](https://arxiv.org/abs/2507.08707)
*Peter Crowley,Zachary Serlin,Tyler Paine,Makai Mann,Michael Benjamin,Calin Belta*

Main category: cs.LG

TL;DR: SPLASH improves IRL for long-horizon adversarial tasks using suboptimal hierarchical demonstrations, outperforming state-of-the-art methods in reward learning.


<details>
  <summary>Details</summary>
Motivation: Existing IRL methods assume optimal demonstrations or fail in long-horizon/adversarial tasks, limiting real-world robotic applications.

Method: SPLASH leverages suboptimal hierarchical demonstrations for efficient preference-based IRL, validated in maritime capture-the-flag simulations and real-world USV experiments.

Result: SPLASH outperforms current methods in learning rewards from suboptimal demonstrations, with successful sim-to-real translation.

Conclusion: SPLASH advances IRL for complex, adversarial tasks, demonstrating practical viability in robotics.

Abstract: Inverse Reinforcement Learning (IRL) presents a powerful paradigm for
learning complex robotic tasks from human demonstrations. However, most
approaches make the assumption that expert demonstrations are available, which
is often not the case. Those that allow for suboptimality in the demonstrations
are not designed for long-horizon goals or adversarial tasks. Many desirable
robot capabilities fall into one or both of these categories, thus highlighting
a critical shortcoming in the ability of IRL to produce field-ready robotic
agents. We introduce Sample-efficient Preference-based inverse reinforcement
learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical
demonstrations (SPLASH), which advances the state-of-the-art in learning from
suboptimal demonstrations to long-horizon and adversarial settings. We
empirically validate SPLASH on a maritime capture-the-flag task in simulation,
and demonstrate real-world applicability with sim-to-real translation
experiments on autonomous unmanned surface vehicles. We show that our proposed
methods allow SPLASH to significantly outperform the state-of-the-art in reward
learning from suboptimal demonstrations.

</details>


### [79] [On the Effect of Regularization in Policy Mirror Descent](https://arxiv.org/abs/2507.08718)
*Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: Policy Mirror Descent (PMD) combines two regularization techniques in RL, but empirical studies are limited. This paper analyzes their interplay, showing their combination is key for robust performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical understanding and empirical validation of PMD in RL, focusing on the role of its two regularization components.

Method: Conducted a large-scale empirical analysis with over 500k training seeds on small RL environments to study the interaction between the distance term and MDP regularizer.

Result: The two regularizers can partially substitute each other, but their precise combination is crucial for robust performance.

Conclusion: The findings emphasize the importance of carefully balancing regularization techniques in PMD to improve algorithm robustness and reduce hyperparameter sensitivity.

Abstract: Policy Mirror Descent (PMD) has emerged as a unifying framework in
reinforcement learning (RL) by linking policy gradient methods with a
first-order optimization method known as mirror descent. At its core, PMD
incorporates two key regularization components: (i) a distance term that
enforces a trust region for stable policy updates and (ii) an MDP regularizer
that augments the reward function to promote structure and robustness. While
PMD has been extensively studied in theory, empirical investigations remain
scarce. This work provides a large-scale empirical analysis of the interplay
between these two regularization techniques, running over 500k training seeds
on small RL environments. Our results demonstrate that, although the two
regularizers can partially substitute each other, their precise combination is
critical for achieving robust performance. These findings highlight the
potential for advancing research on more robust algorithms in RL, particularly
with respect to hyperparameter sensitivity.

</details>


### [80] [Partitioned Hybrid Quantum Fourier Neural Operators for Scientific Quantum Machine Learning](https://arxiv.org/abs/2507.08746)
*Paolo Marcandelli,Yuanchun He,Stefano Mariani,Martina Siena,Stefano Markidis*

Main category: cs.LG

TL;DR: PHQFNO is a hybrid quantum-classical neural operator for scientific machine learning, combining quantum and classical resources for improved accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: To extend Quantum Fourier Neural Operators (QFNOs) to higher dimensions and enable distributed execution across quantum and classical devices, enhancing performance in scientific machine learning tasks.

Method: PHQFNO partitions Fourier operator computation between classical and quantum resources, uses unary encoding for quantum states, and optimizes parameters via a variational scheme. It employs a message-passing framework for data distribution.

Result: PHQFNO matches classical FNO accuracy on Burgers' equation and outperforms classical methods on incompressible Navier-Stokes. It also shows improved stability under input noise.

Conclusion: PHQFNO successfully hybridizes quantum and classical resources, achieving competitive or superior performance in scientific machine learning tasks while demonstrating robustness to noise.

Abstract: We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),
a generalization of the Quantum Fourier Neural Operator (QFNO) for scientific
machine learning. PHQFNO partitions the Fourier operator computation across
classical and quantum resources, enabling tunable quantum-classical
hybridization and distributed execution across quantum and classical devices.
The method extends QFNOs to higher dimensions and incorporates a
message-passing framework to distribute data across different partitions. Input
data are encoded into quantum states using unary encoding, and quantum circuit
parameters are optimized using a variational scheme. We implement PHQFNO using
PennyLane with PyTorch integration and evaluate it on Burgers' equation,
incompressible and compressible Navier-Stokes equations. We show that PHQFNO
recovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO
achieves higher accuracy than its classical counterparts. Finally, we perform a
sensitivity analysis under input noise, confirming improved stability of PHQFNO
over classical baselines.

</details>


### [81] [Modeling Partially Observed Nonlinear Dynamical Systems and Efficient Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network](https://arxiv.org/abs/2507.08749)
*Chuanqi Chen,Zhongrui Wang,Nan Chen,Jin-Long Wu*

Main category: cs.LG

TL;DR: A discrete-time conditional Gaussian Koopman network (CGKN) is developed for efficient state forecast and data assimilation in high-dimensional nonlinear systems, unifying scientific machine learning and data assimilation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling high-dimensional nonlinear dynamical systems, especially those governed by PDEs, by leveraging Koopman embedding for latent state representation.

Method: Uses Koopman embedding to create a latent representation with conditional linear dynamics, forming a conditional Gaussian system for efficient analytical data assimilation.

Result: Demonstrates comparable performance to state-of-the-art methods in state forecast and provides accurate data assimilation for nonlinear PDEs like Burgers' and Navier-Stokes equations.

Conclusion: CGKN unifies SciML and data assimilation, offering a framework for broader applications like optimization and control.

Abstract: A discrete-time conditional Gaussian Koopman network (CGKN) is developed in
this work to learn surrogate models that can perform efficient state forecast
and data assimilation (DA) for high-dimensional complex dynamical systems,
e.g., systems governed by nonlinear partial differential equations (PDEs).
Focusing on nonlinear partially observed systems that are common in many
engineering and earth science applications, this work exploits Koopman
embedding to discover a proper latent representation of the unobserved system
states, such that the dynamics of the latent states are conditional linear,
i.e., linear with the given observed system states. The modeled system of the
observed and latent states then becomes a conditional Gaussian system, for
which the posterior distribution of the latent states is Gaussian and can be
efficiently evaluated via analytical formulae. The analytical formulae of DA
facilitate the incorporation of DA performance into the learning process of the
modeled system, which leads to a framework that unifies scientific machine
learning (SciML) and data assimilation. The performance of discrete-time CGKN
is demonstrated on several canonical problems governed by nonlinear PDEs with
intermittency and turbulent features, including the viscous Burgers' equation,
the Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with
which we show that the discrete-time CGKN framework achieves comparable
performance as the state-of-the-art SciML methods in state forecast and
provides efficient and accurate DA results. The discrete-time CGKN framework
also serves as an example to illustrate unifying the development of SciML
models and their other outer-loop applications such as design optimization,
inverse problems, and optimal control.

</details>


### [82] [ML-Based Automata Simplification for Symbolic Accelerators](https://arxiv.org/abs/2507.08751)
*Tiffany Yu,Rye Stahle-Smith,Darssan Eswaramoorthi,Rasha Karakchi*

Main category: cs.LG

TL;DR: AutoSlim is a machine learning-based framework for simplifying symbolic accelerators, reducing memory and routing complexity while preserving correctness.


<details>
  <summary>Details</summary>
Motivation: Symbolic accelerators face scalability issues due to high memory use and routing complexity, especially for large datasets.

Method: AutoSlim uses Random Forest classification to prune low-impact transitions in NFA graphs, leveraging edge scores and structural features.

Result: AutoSlim reduces FPGA LUTs by 40% and prunes transitions by 30%, scaling to larger graphs than benchmarks.

Conclusion: AutoSlim effectively mitigates resource blowup in symbolic accelerators, with hardware interconnection being a key cost factor.

Abstract: Symbolic accelerators are increasingly used for symbolic data processing in
domains such as genomics, NLP, and cybersecurity. However, these accelerators
face scalability issues due to excessive memory use and routing complexity,
especially when targeting a large set. We present AutoSlim, a machine
learning-based graph simplification framework designed to reduce the complexity
of symbolic accelerators built on Non-deterministic Finite Automata (NFA)
deployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest
classification to prune low-impact transitions based on edge scores and
structural features, significantly reducing automata graph density while
preserving semantic correctness. Unlike prior tools, AutoSlim targets automated
score-aware simplification with weighted transitions, enabling efficient
ranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in
NAPOLY+ and conducted performance measurements including latency, throughput,
and resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs
and over 30 percent pruning in transitions, while scaling to graphs an order of
magnitude larger than existing benchmarks. Our results also demonstrate how
hardware interconnection (fanout) heavily influences hardware cost and that
AutoSlim's pruning mitigates resource blowup.

</details>


### [83] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song,Weilin Zhao,Xu Han,Chaojun Xiao,Yingfa Chen,Yuxuan Li,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: BlockFFN introduces a novel MoE architecture with improved routing and sparsity for efficient LLM acceleration, achieving high token- and chunk-level sparsity and significant speedup on end-side devices.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency and inflexibility of vanilla MoE architectures in LLMs, particularly for low-resource conditions and acceleration techniques like speculative decoding.

Method: Proposes BlockFFN with ReLU-RMSNorm routing, CLS-aware training objectives, and efficient acceleration kernels combining sparsity and speculative decoding.

Result: Achieves over 80% token-level sparsity, 70% 8-token chunk-level sparsity, and up to 3.67x speedup on end-side devices.

Conclusion: BlockFFN outperforms MoE baselines, offering a practical solution for efficient LLM deployment in resource-constrained environments.

Abstract: To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [84] [Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees](https://arxiv.org/abs/2507.08784)
*Chuyan Chen,Yutong He,Pengrui Li,Weichen Jia,Kun Yuan*

Main category: cs.LG

TL;DR: GreedyLore is a greedy low-rank gradient compression algorithm for distributed learning, combining error feedback and semi-lazy subspace updates to ensure convergence with a linear speedup rate.


<details>
  <summary>Details</summary>
Motivation: Communication overhead in distributed optimization is a bottleneck; existing low-rank compression methods either lack convergence guarantees or perform poorly empirically.

Method: GreedyLore uses greedy compression with error feedback and semi-lazy subspace updates to maintain contractive compression and ensure convergence.

Result: GreedyLore achieves a convergence rate of O(σ/√NT + 1/T), the first linear speedup for low-rank compression, validated by experiments.

Conclusion: GreedyLore bridges the gap between empirical performance and theoretical guarantees in low-rank gradient compression for distributed learning.

Abstract: Distributed optimization is pivotal for large-scale signal processing and
machine learning, yet communication overhead remains a major bottleneck.
Low-rank gradient compression, in which the transmitted gradients are
approximated by low-rank matrices to reduce communication, offers a promising
remedy. Existing methods typically adopt either randomized or greedy
compression strategies: randomized approaches project gradients onto randomly
chosen subspaces, introducing high variance and degrading empirical
performance; greedy methods select the most informative subspaces, achieving
strong empirical results but lacking convergence guarantees. To address this
gap, we propose GreedyLore--the first Greedy Low-Rank gradient compression
algorithm for distributed learning with rigorous convergence guarantees.
GreedyLore incorporates error feedback to correct the bias introduced by greedy
compression and introduces a semi-lazy subspace update that ensures the
compression operator remains contractive throughout all iterations. With these
techniques, we prove that GreedyLore achieves a convergence rate of
$\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD
and Adam--marking the first linear speedup convergence rate for low-rank
gradient compression. Extensive experiments are conducted to validate our
theoretical findings.

</details>


### [85] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao,Haolin Liu,Dian Yu,S. Y. Kung,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: Generative reward models (LLMs-as-judges) are vulnerable to superficial manipulations like non-word symbols or reasoning openers, leading to false rewards. A data augmentation strategy improves robustness, and a new model is released.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in generative reward models used for evaluating answer quality in RLVR, which can be exploited by trivial manipulations.

Method: Introduces a data augmentation strategy to train a more robust generative reward model.

Result: Demonstrates widespread vulnerabilities in current models and presents a new, improved reward model.

Conclusion: Highlights the need for reliable LLM-based evaluation methods and releases a robust model and training data.

Abstract: Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


### [86] [The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/abs/2507.08802)
*Denis Sutter,Julian Minder,Thomas Hofmann,Tiago Pimentel*

Main category: cs.LG

TL;DR: The paper critiques causal abstraction in interpretability, showing unrestricted alignment maps can trivialize it, and highlights the need for assumptions about information encoding.


<details>
  <summary>Details</summary>
Motivation: To examine the limitations of causal abstraction in interpretability, especially when alignment maps are not constrained by linearity.

Method: Theoretical analysis and empirical experiments, including mapping randomly initialized models to algorithms.

Result: Unrestricted alignment maps make causal abstraction trivial, and empirical tests show perfect mapping even for incapable models.

Conclusion: Causal abstraction alone is insufficient for interpretability without assumptions about information encoding, suggesting future research directions.

Abstract: The concept of causal abstraction got recently popularised to demystify the
opaque decision-making processes of machine learning models; in short, a neural
network can be abstracted as a higher-level algorithm if there exists a
function which allows us to map between them. Notably, most interpretability
papers implement these maps as linear functions, motivated by the linear
representation hypothesis: the idea that features are encoded linearly in a
model's representations. However, this linearity constraint is not required by
the definition of causal abstraction. In this work, we critically examine the
concept of causal abstraction by considering arbitrarily powerful alignment
maps. In particular, we prove that under reasonable assumptions, any neural
network can be mapped to any algorithm, rendering this unrestricted notion of
causal abstraction trivial and uninformative. We complement these theoretical
findings with empirical evidence, demonstrating that it is possible to
perfectly map models to algorithms even when these models are incapable of
solving the actual task; e.g., on an experiment using randomly initialised
language models, our alignment maps reach 100% interchange-intervention
accuracy on the indirect object identification task. This raises the non-linear
representation dilemma: if we lift the linearity constraint imposed to
alignment maps in causal abstraction analyses, we are left with no principled
way to balance the inherent trade-off between these maps' complexity and
accuracy. Together, these results suggest an answer to our title's question:
causal abstraction is not enough for mechanistic interpretability, as it
becomes vacuous without assumptions about how models encode information.
Studying the connection between this information-encoding assumption and causal
abstraction should lead to exciting future work.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [87] [AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08616)
*Florian Grötschla,Luis Müller,Jan Tönshoff,Mikhail Galkin,Bryan Perozzi*

Main category: cs.MA

TL;DR: AgentsNet is a new benchmark for evaluating multi-agent systems' ability to self-organize, collaborate, and solve problems given a network topology, scaling beyond existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks measuring multi-agent systems' effectiveness in leveraging network topology for collaboration and problem-solving.

Method: AgentsNet draws from distributed systems and graph theory to test multi-agent systems' strategy formation, self-organization, and communication. Baseline methods, including homogeneous agent networks, are evaluated.

Result: Frontier LLMs perform well in small networks but decline as network size scales. AgentsNet supports up to 100 agents, surpassing existing benchmarks.

Conclusion: AgentsNet provides a scalable benchmark for assessing multi-agent systems' collaborative and organizational capabilities, highlighting current LLMs' limitations in larger networks.

Abstract: Large-language models (LLMs) have demonstrated powerful problem-solving
capabilities, in particular when organized in multi-agent systems. However, the
advent of such systems also raises several questions on the ability of a
complex network of agents to effectively self-organize and collaborate. While
measuring performance on standard reasoning benchmarks indicates how well
multi-agent systems can solve reasoning tasks, it is unclear whether these
systems are able to leverage their topology effectively. Here, we propose
AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration
from classical problems in distributed systems and graph theory, AgentsNet
measures the ability of multi-agent systems to collaboratively form strategies
for problem-solving, self-organization, and effective communication given a
network topology. We evaluate a variety of baseline methods on AgentsNet
including homogeneous networks of agents which first have to agree on basic
protocols for organization and communication. We find that some frontier LLMs
are already demonstrating strong performance for small networks but begin to
fall off once the size of the network scales. While existing multi-agent
benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size
and can scale with new generations of LLMs. As such, we also probe frontier
models in a setup with up to 100 agents.

</details>
