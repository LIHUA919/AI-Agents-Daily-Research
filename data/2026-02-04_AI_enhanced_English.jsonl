{"id": "2602.02515", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02515", "abs": "https://arxiv.org/abs/2602.02515", "authors": ["Yiliang Song", "Hongjun An", "Jiangong Xiao", "Haofei Zhao", "Jiawei Shao", "Xuelong Li"], "title": "CreditAudit: 2D Auditing for LLM Evaluation and Selection", "comment": "First update", "summary": "Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.", "AI": {"tldr": "CreditAudit is a deployment-oriented framework that evaluates AI models on both average performance and stability across different system prompts, providing credit grades to help practitioners choose models for real-world deployment.", "motivation": "Current benchmark scores show diminishing differentiation between top models while failing to reflect real-world performance, where system prompt variations and agentic workflows can cause unexpected failures. Practitioners lack tools to assess model stability under operational variations.", "method": "Proposes CreditAudit framework that: 1) Tests models under semantically aligned, non-adversarial system prompt variations, 2) Measures both mean ability (average performance) and fluctuation sigma (stability risk), 3) Maps volatility into credit grades (AAA to BBB) using cross-model quantiles, 4) Includes diagnostics to mitigate template difficulty drift.", "result": "Experiments on GPQA, TruthfulQA, and MMLU Pro show models with similar mean ability can have substantially different fluctuation. Stability risk can overturn deployment decisions in agentic or high-failure-cost scenarios, demonstrating the framework's utility for real-world model selection.", "conclusion": "CreditAudit provides a 2D (performance+stability) and grade-based evaluation system that supports tiered deployment, better resource allocation for testing/monitoring, and more objective, trustworthy model evaluation for real-world applications."}}
{"id": "2602.02559", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02559", "abs": "https://arxiv.org/abs/2602.02559", "authors": ["Pengyu Dai", "Weihao Xuan", "Junjue Wang", "Hongruixuan Chen", "Jian Song", "Yafei Ou", "Naoto Yokoya"], "title": "Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers", "comment": "21 pages, 6 figures", "summary": "Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \\textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.", "AI": {"tldr": "GeoEvolver is a multi-agent system that enables LLM agents to self-evolve expertise for Earth Observation tasks without parameter updates, improving task success by 12% on average.", "motivation": "LLM agents struggle in specialized domains like Earth Observation due to complex, multi-modal data, strict tool constraints, and the need for long-horizon execution without fine-grained expertise.", "method": "Introduces GeoEvolver, which decomposes queries into sub-goals via a retrieval-augmented multi-agent orchestrator, explores tool configurations, and distills successful patterns and failure analyses into an evolving memory bank.", "result": "Experiments on three EO benchmarks show GeoEvolver consistently improves end-to-end task success rates by an average of 12% across multiple LLM backbones.", "conclusion": "GeoEvolver demonstrates that LLM agents can progressively acquire EO expertise through efficient, fine-grained environmental interactions, enhancing reliability in complex tool-intensive workflows."}}
{"id": "2602.02582", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.IR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02582", "abs": "https://arxiv.org/abs/2602.02582", "authors": ["Chandan Kumar Sah", "Xiaoli Lian", "Li Zhang", "Tony Xu", "Syed Shazaib Shah"], "title": "Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems", "comment": "Accepted at the Second Conference of the International Association for Safe and Ethical Artificial Intelligence, IASEAI26, 14 pages", "summary": "Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.", "AI": {"tldr": "This paper studies uncertainty and fairness in LLM-based zero-shot recommendations, introducing evaluation metrics, a dataset with demographic annotations, and showing systematic unfairness in Gemini 1.5 Flash that persists across prompt variations. It integrates personality-aware fairness analysis and proposes uncertainty-aware evaluation methods.", "motivation": "While LLMs enable powerful zero-shot recommendations by leveraging contextual knowledge, predictive uncertainty and embedded biases threaten their reliability and fairness. There's a need to systematically evaluate how these factors affect accuracy, consistency, and trustworthiness of LLM-generated recommendations.", "method": "The authors: 1) Introduce a benchmark of curated evaluation metrics and a dataset annotated for eight demographic attributes (31 categorical values) across movies and music domains, 2) Quantify predictive uncertainty using entropy, 3) Conduct case studies measuring fairness disparities in Google DeepMind's Gemini 1.5 Flash, 4) Test disparities under prompt perturbations (typographical errors, multilingual inputs), 5) Integrate personality-aware fairness into RecLLM evaluation pipeline to reveal personality-linked bias patterns.", "result": "Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes with similarity-based gaps of SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations. The personality-aware fairness analysis reveals personality-linked bias patterns and exposes trade-offs between personalization and group fairness.", "conclusion": "The paper establishes a foundation for safer, more interpretable RecLLMs through novel uncertainty-aware evaluation methodology, empirical insights from uncertainty case studies, and personality profile-informed fairness benchmark. It motivates future work on multi-model benchmarks and adaptive calibration for trustworthy deployment."}}
{"id": "2602.02589", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02589", "abs": "https://arxiv.org/abs/2602.02589", "authors": ["Yanki Margalit", "Erni Avram", "Ran Taig", "Oded Margalit", "Nurit Cohen-Inger"], "title": "PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review", "comment": null, "summary": "Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.", "AI": {"tldr": "PeerRank is a fully autonomous evaluation framework for LLMs that uses multi-agent peer assessment with web grounding, generating tasks, judging responses, and aggregating scores without human supervision.", "motivation": "Traditional evaluation of large language models relies on human benchmarks and judgments, which scale poorly, become outdated, and mismatch open-world deployments like web retrieval and synthesis.", "method": "PeerRank is a multi-agent process where models symmetrically generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses, and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references.", "result": "In a study with 12 commercial models and 420 autonomously generated questions, PeerRank produced stable, discriminative rankings, revealed identity and presentation biases, showed robustness, and mean peer scores agreed with Elo. Validation on TruthfulQA and GSM8K showed peer scores correlated with objective accuracy.", "conclusion": "Bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static, human-curated benchmarks, providing an effective autonomous alternative."}}
{"id": "2602.02500", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.02500", "abs": "https://arxiv.org/abs/2602.02500", "authors": ["Chen Hu", "Qianxi Zhao", "Yuming Li", "Mingyu Zhou", "Xiyin Li"], "title": "UNSO: Unified Newton Schulz Orthogonalization", "comment": null, "summary": "The Newton-Schulz (NS) iteration has gained increasing interest for its role in the Muon optimizer and the Stiefel manifold. However, the conventional NS iteration suffers from inefficiency and instability. Although various improvements have been introduced to NS iteration, they fail to deviate from the conventional iterative paradigm, which could increase computation burden largely due to the matrix products along the long dimension repeatedly. To address this, we consolidate the iterative structure into a unified framework, named Unified Newton-Schulz Orthogonalization (UNSO). To do so, we could avoid a polynomial expansion. Instead, we evaluate the role of each matrix power, remove the insignificant terms, and provide a recommended polynomial with learnable coefficients. These learnable coefficients are then optimized, and achieve an outstanding performance with stable convergence. The code of our method is available: https://github.com/greekinRoma/Unified_Newton_Schulz_Orthogonalization.", "AI": {"tldr": "Paper presents UNSO, a unified Newton-Schulz orthogonalization framework that replaces conventional iterative polynomial expansions with learnable coefficients, improving efficiency and stability.", "motivation": "Newton-Schulz iteration is important for Muon optimizer and Stiefel manifold but suffers from inefficiency and instability. Existing improvements still use conventional iterative paradigm with matrix products along long dimensions, creating computational burden.", "method": "Unified Newton-Schulz Orthogonalization (UNSO) consolidates iterative structure into unified framework. Instead of polynomial expansion, analyzes role of each matrix power, removes insignificant terms, and provides recommended polynomial with learnable coefficients that are optimized for performance.", "result": "UNSO achieves outstanding performance with stable convergence, avoiding computational burden of repeated matrix products. Method code is publicly available on GitHub.", "conclusion": "UNSO provides an efficient and stable alternative to conventional Newton-Schulz iteration by using learnable coefficients in unified framework, addressing inefficiency and instability issues while reducing computational complexity."}}
{"id": "2602.02613", "categories": ["cs.MA", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.02613", "abs": "https://arxiv.org/abs/2602.02613", "authors": ["Yu-Zheng Lin", "Bono Po-Jen Shih", "Hsuan-Ying Alessandra Chien", "Shalaka Satam", "Jesus Horacio Pacheco", "Sicong Shao", "Soheil Salehi", "Pratik Satam"], "title": "Exploring Silicon-Based Societies: An Early Study of the Moltbook Agent Community", "comment": "10 pages, 3 figures, a pilot study for silicon-based societies", "summary": "The rapid emergence of autonomous large language model agents has given rise to persistent, large-scale agent ecosystems whose collective behavior cannot be adequately understood through anecdotal observation or small-scale simulation. This paper introduces data-driven silicon sociology as a systematic empirical framework for studying social structure formation among interacting artificial agents. We present a pioneering large-scale data mining investigation of an in-the-wild agent society by analyzing Moltbook, a social platform designed primarily for agent-to-agent interaction. At the time of study, Moltbook hosted over 150,000 registered autonomous agents operating across thousands of agent-created sub-communities. Using programmatic and non-intrusive data acquisition, we collected and analyzed the textual descriptions of 12,758 submolts, which represent proactive sub-community partitioning activities within the ecosystem. Treating agent-authored descriptions as first-class observational artifacts, we apply rigorous preprocessing, contextual embedding, and unsupervised clustering techniques to uncover latent patterns of thematic organization and social space structuring. The results show that autonomous agents systematically organize collective space through reproducible patterns spanning human-mimetic interests, silicon-centric self-reflection, and early-stage economic and coordination behaviors. Rather than relying on predefined sociological taxonomies, these structures emerge directly from machine-generated data traces. This work establishes a methodological foundation for data-driven silicon sociology and demonstrates that data mining techniques can provide a powerful lens for understanding the organization and evolution of large autonomous agent societies.", "AI": {"tldr": "Large-scale data mining study of an autonomous agent ecosystem (Moltbook) reveals systematic social structure formation through machine-generated data traces, establishing data-driven silicon sociology as a methodological framework.", "motivation": "The rapid emergence of autonomous LLM agents has created persistent, large-scale ecosystems whose collective behavior cannot be understood through anecdotal observation or small-scale simulation alone.", "method": "Programmatic data acquisition from Moltbook (150k+ agents), analysis of 12,758 agent-authored sub-community descriptions using preprocessing, contextual embedding, and unsupervised clustering to uncover latent patterns of thematic organization.", "result": "Autonomous agents systematically organize collective space through reproducible patterns including human-mimetic interests, silicon-centric self-reflection, and early-stage economic/coordination behaviors, emerging directly from machine-generated data.", "conclusion": "This work establishes data-driven silicon sociology as a methodological foundation for studying autonomous agent societies, demonstrating data mining provides a powerful lens for understanding their organization and evolution."}}
{"id": "2602.02639", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02639", "abs": "https://arxiv.org/abs/2602.02639", "authors": ["Harry Mayne", "Justin Singh Kang", "Dewi Gould", "Kannan Ramchandran", "Adam Mahdi", "Noah Y. Siegel"], "title": "A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior", "comment": null, "summary": "LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.", "AI": {"tldr": "Self-explanations from LLMs aid in predicting model behavior but vary in faithfulness; NSG is a new metric for assessing this, showing improvement in predictability while highlighting some misleading cases.", "motivation": "Existing faithfulness metrics for LLM self-explanations are limited, often missing predictive value, prompting the need for a better assessment method.", "method": "Introduced Normalized Simulatability Gain (NSG), a metric based on observers learning model criteria from explanations. Evaluated 18 models on 7,000 counterfactuals from diverse datasets.", "result": "Self-explanations improve behavior prediction by 11-37% NSG, outperform external explanations, and 5-15% are egregiously misleading.", "conclusion": "Self-explanations encode predictive information beneficial for oversight, despite imperfections, with self-knowledge providing an advantage over external methods."}}
{"id": "2602.02501", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02501", "abs": "https://arxiv.org/abs/2602.02501", "authors": ["Saurabh Anand", "Shubham Malaviya", "Manish Shukla", "Sachin Lodha"], "title": "Augmenting Parameter-Efficient Pre-trained Language Models with Large Language Models", "comment": "22 pages, 9 figures, 11 tables, short paper was accepted in ACM SAC 2024", "summary": "Training AI models in cybersecurity with help of vast datasets offers significant opportunities to mimic real-world behaviors effectively. However, challenges like data drift and scarcity of labelled data lead to frequent updates of models and the risk of overfitting. To address these challenges, we used parameter-efficient fine-tuning techniques for pre-trained language models wherein we combine compacters with various layer freezing strategies. To enhance the capabilities of these pre-trained language models, in this work we introduce two strategies that use large language models. In the first strategy, we utilize large language models as data-labelling tools wherein they generate labels for unlabeled data. In the second strategy, large language modes are utilized as fallback mechanisms for predictions having low confidence scores. We perform comprehensive experimental analysis on the proposed strategies on different downstream tasks specific to cybersecurity domain. We empirically demonstrate that by combining parameter-efficient pre-trained models with large language models, we can improve the reliability and robustness of models, making them more suitable for real-world cybersecurity applications.", "AI": {"tldr": "Parameter-efficient fine-tuning of pre-trained language models combined with LLMs for data labeling and fallback predictions improves cybersecurity AI reliability.", "motivation": "Cybersecurity AI training faces challenges with data drift and scarce labeled data, leading to frequent model updates and overfitting risks.", "method": "Combine parameter-efficient fine-tuning (compacters with layer freezing) with two LLM strategies: 1) LLMs as data-labeling tools for unlabeled data, 2) LLMs as fallback mechanisms for low-confidence predictions.", "result": "Comprehensive experiments on cybersecurity downstream tasks show improved reliability and robustness when combining parameter-efficient pre-trained models with LLMs.", "conclusion": "The combination of parameter-efficient fine-tuning and LLM strategies makes cybersecurity AI models more suitable for real-world applications by enhancing reliability and robustness."}}
{"id": "2602.02751", "categories": ["cs.MA", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02751", "abs": "https://arxiv.org/abs/2602.02751", "authors": ["Lisa Alazraki", "William F. Shen", "Yoram Bachrach", "Akhil Mathur"], "title": "Scaling Small Agents Through Strategy Auctions", "comment": null, "summary": "Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively \"scaled up\" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.", "AI": {"tldr": "SALE (Strategy Auctions for Workload Efficiency) is a new agent framework that reduces reliance on large models by using auction-based task routing and strategic bidding, cutting costs by 35% while maintaining performance on complex tasks.", "motivation": "Small language models are seen as cost-effective for agentic AI, but their performance doesn't scale well with task complexity. Current approaches either require running all models or rely on inadequate task-description-based routers that often underperform.", "method": "SALE implements a marketplace-inspired framework where agents bid with short strategic plans. Bids are scored using a cost-value mechanism and refined via shared auction memory, enabling per-task routing and continual self-improvement without training separate routers or running all models to completion.", "result": "Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with negligible overhead beyond executing the final trace.", "conclusion": "Small agents can be effectively scaled up through coordinated task allocation and test-time self-improvement. Performance gains in agentic AI come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems."}}
{"id": "2602.02660", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02660", "abs": "https://arxiv.org/abs/2602.02660", "authors": ["Jiefeng Chen", "Bhavana Dalvi Mishra", "Jaehyun Nam", "Rui Meng", "Tomas Pfister", "Jinsung Yoon"], "title": "MARS: Modular Agent with Reflective Search for Automated AI Research", "comment": null, "summary": "Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.", "AI": {"tldr": "MARS is a framework for autonomous AI research that uses budget-aware planning, modular construction, and comparative reflective memory to overcome computational cost and opaque performance attribution challenges.", "motivation": "Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle with monolithic scripts that ignore execution costs and causal factors.", "method": "Three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to balance performance with execution expense; (2) Modular Construction using a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; (3) Comparative Reflective Memory that analyzes solution differences to distill high-signal insights and address credit assignment.", "result": "Achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with top methods on global leaderboard. Exhibits qualitative \"Aha!\" moments where 63% of utilized lessons originate from cross-branch transfer, showing effective generalization across search paths.", "conclusion": "MARS demonstrates that combining budget-aware search planning, modular construction, and comparative reflection creates an effective framework for autonomous AI research that can handle computational costs and performance attribution challenges."}}
