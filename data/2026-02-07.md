<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: The paper examines whether LLMs can predict individual-level cognitive biases and emulate biased human behavior in interactive decision-making scenarios under factors like cognitive load, finding that LLMs like GPT-4 and GPT-5 can reproduce these biases with precision.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding whether LLMs can predict cognitive biases at the individual level and emulate the dynamics of biased human behavior in interactive contexts, especially with factors like cognitive load.

Method: Adapted three decision scenarios into a conversational setting, conducted a human experiment with 1100 participants using a chatbot for simple or complex dialogues, and simulated conditions with GPT-4 and GPT-5 based on participant demographics and dialogue transcripts.

Result: LLMs reproduced human biases with precision, revealing robust biases in human participants and notable differences in how models aligned with human behavior.

Conclusion: This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [2] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: The paper critiques the linear model of AI progress, introduces 'familiar' and 'strange' intelligence, and proposes a nonlinear model where general intelligence is a broad ability to achieve goals in varied environments.


<details>
  <summary>Details</summary>
Motivation: To challenge the linear view of AI progress and address the limitations of evaluating AI capacities with familiar human-like patterns, highlighting that AI intelligence may not fit into traditional linear or unified models.

Method: Introduces and defends two novel concepts: 'familiar intelligence' (human-like patterns) and 'strange intelligence' (AI's superhuman and subhuman performance across domains). Develops a nonlinear model where general intelligence is defined as the ability to achieve goals in diverse environments without reduction to a single linear quantity.

Result: Argues that AI intelligence is likely 'strange intelligence', defying familiar patterns, combining superhuman capacities with subhuman performance and surprising errors. Emphasizes that general intelligence is not a unified capacity but a broad, nonlinear ability.

Conclusion: Implications for AI evaluation: even capable systems may fail on obvious tasks; errors don't negate general intelligence, and performance on one task (e.g., IQ test) shouldn't be extrapolated to broad capacities. Recommends a nuanced, nonlinear approach to adversarial testing.

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [3] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead: A structure-aware, multi-turn document reasoning agent for long-document QA, using hierarchical and sequential document priors with Retrieve and ReadSection tools, outperforming agentic search methods.


<details>
  <summary>Details</summary>
Motivation: Existing agentic search frameworks treat long documents as flat chunk collections, underutilizing document-native priors like hierarchical organization and sequential structure, limiting effectiveness in long-document question answering.

Method: Convert PDFs to structured Markdown preserving headings and paragraphs, index at paragraph level with metadata encoding section identity and order, and equip LLM with Retrieve (localize relevant paragraphs with structural coordinates) and ReadSection (read contiguously within specified section and range) tools.

Result: DeepRead significantly improves over Search-o1-style agentic search in document QA, validates synergistic effect between tools, and reveals a human-like 'locate then read' reasoning paradigm in behavioral analysis.

Conclusion: DeepRead successfully operationalizes document structure priors for enhanced long-document QA, demonstrating the value of explicit structural awareness in multi-turn reasoning agents.

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [4] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: This paper proposes Minimal Information Neuro-Symbolic Tree (MINT) to optimize AI agents' query strategies for eliciting human inputs in object-driven planning with knowledge gaps, achieving near-expert performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Joint planning in human-AI teams often faces challenges due to incomplete information and unknowns (e.g., objects and human goals), creating knowledge gaps that hinder effective planning.

Method: The paper introduces MINT, which builds a symbolic tree to simulate possible human-AI interactions and uses a neural planning policy to estimate planning outcome uncertainty from knowledge gaps. Self-play with MINT optimizes elicitation strategies, and an LLM is employed to summarize reasoning and curate queries for human input.

Result: Evaluation on three benchmarks involving unseen/unknown objects shows that MINT-based planning attains near-expert returns, improves rewards and success rates, and uses a limited number of questions per task.

Conclusion: MINT effectively addresses knowledge gaps in object-driven planning by optimizing query strategies, enhancing AI agents' ability to actively elicit human inputs for improved joint planning performance.

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [5] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: LLMs perform well on solved graph theory problems but struggle with open problems, needing verification in education.


<details>
  <summary>Details</summary>
Motivation: To assess LLM reliability in supporting mathematically rigorous thinking as they become integrated into computer science coursework.

Method: Use an eight-stage evaluation protocol reflecting authentic mathematical inquiry on two graph theory problems (one solved, one open).

Result: For the solved problem: correct definitions, no hallucination, valid proof. For the open problem: coherent interpretations but no solution, acknowledged uncertainty.

Conclusion: LLMs can aid conceptual exploration of established material but are limited in novel mathematical insight, emphasizing the need for independent verification in education.

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [6] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: A general formulation for uncertainty quantification in LLM agents is proposed, shifting focus from single-turn QA to interactive settings with a new conditional uncertainty reduction framework.


<details>
  <summary>Details</summary>
Motivation: Current UQ research for LLMs is limited to single-turn question-answering, but as LLM agents are deployed in complex interactive tasks, a new framework is needed for realistic agent UQ.

Method: Presents a general formulation that subsumes existing UQ setups, critiques prior implicit uncertainty accumulation views, and proposes a novel perspective as a conditional uncertainty reduction process emphasizing agent interactivity.

Result: Develops a conceptual framework for actionable guidance in designing UQ for LLM agents, demonstrating the need to model reducible uncertainty over agent trajectories.

Conclusion: Highlights practical implications for frontier LLM development and domain-specific applications, identifying open problems and advocating for a shift to interactive agent UQ research.

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [7] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: RL framework for multi-debris ADR missions using small satellites with adaptive collision avoidance and refueling strategies.


<details>
  <summary>Details</summary>
Motivation: The increasing debris in Earth's orbit necessitates safe and efficient active debris removal (ADR) missions to mitigate collision risks.

Method: A masked Proximal Policy Optimization (PPO) RL algorithm integrates refueling, mission planning, and adaptive collision avoidance for optimizing spacecraft rendezvous operations.

Result: Simulations using the Iridium 33 debris dataset show the RL framework reduces collision risk and improves mission efficiency compared to traditional heuristic approaches.

Conclusion: The RL-based approach offers a scalable solution for multi-debris ADR and other multi-target rendezvous problems in autonomous space missions.

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [8] [VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health](https://arxiv.org/abs/2602.05088)
*Kate H. Bentley,Luca Belli,Adam M. Chekroud,Emily J. Ward,Emily R. Dworkin,Emily Van Ark,Kelly M. Johnston,Will Alexander,Millard Brown,Matt Hawrilenko*

Main category: cs.AI

TL;DR: This paper validates the VERA-MH evaluation for assessing AI safety in mental health chatbots, specifically for suicide risk. It compares clinician and AI judge ratings of simulated conversations, showing strong agreement and supporting VERA-MH's clinical validity and reliability.


<details>
  <summary>Details</summary>
Motivation: The growing use of AI chatbots for psychological support raises safety concerns, especially in mental health applications like suicide risk detection. An evidence-based automated safety benchmark is urgently needed to ensure these tools are effective and safe.

Method: The study simulated conversations between LLM-based users and general-purpose AI chatbots. Licensed clinicians rated the conversations for safety and user-agent realism using a scoring rubric. An LLM judge then evaluated the same conversations. Rating alignment was compared between clinicians and the LLM judge.

Result: Clinicians showed high inter-rater reliability (IRR: 0.77), establishing a gold-standard reference. The LLM judge was strongly aligned with clinician consensus (IRR: 0.81), and clinicians generally found the user-agents realistic.

Conclusion: The findings support the clinical validity and reliability of the VERA-MH evaluation as an open-source, automated AI safety tool for mental health. Further research will focus on its generalizability and robustness to ensure safe AI implementation in mental health care.

Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based automated safety benchmark. This study aimed to examine the clinical validity and reliability of the VERA-MH evaluation for AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then compared rating alignment across (a) individual clinicians and (b) clinician consensus and the LLM judge, and (c) examined clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR]: 0.77), thus establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus (IRR: 0.81) overall and within key conditions. Clinician raters generally perceived the user-agents to be realistic. For the potential mental health benefits of AI chatbots to be realized, attention to safety is paramount. Findings from this human evaluation study support the clinical validity and reliability of VERA-MH: an open-source, fully automated AI safety evaluation for mental health. Further research will address VERA-MH generalizability and robustness.

</details>


### [9] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: This paper compares three planners—nominal PPO, domain-randomized PPO, and MCTS—for constrained multi-debris rendezvous in LEO, evaluating them under various mission constraints. Results show domain-randomized PPO balances adaptability and performance, while MCTS handles constraints best but is computationally expensive, suggesting a combined approach for future planners.


<details>
  <summary>Details</summary>
Motivation: Autonomous mission planning for Active Debris Removal (ADR) requires balancing efficiency, adaptability, and strict constraints on fuel and duration, but existing methods may lack robustness under shifting conditions.

Method: The study compares three planners: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed parameters, a domain-randomized Masked PPO policy trained across varying constraints for robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluation is done in a high-fidelity orbital simulation with refueling, realistic dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios.

Result: Nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift; domain-randomized PPO shows improved adaptability with moderate loss in nominal performance; MCTS consistently handles constraint changes best due to online replanning but has significantly higher computation time.

Conclusion: The findings highlight a trade-off between the speed of learned policies and the adaptability of search-based methods, suggesting that combining training-time diversity with online planning could be a promising approach for future resilient ADR mission planners.

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [10] [GAMMS: Graph based Adversarial Multiagent Modeling Simulator](https://arxiv.org/abs/2602.05105)
*Rohan Patil,Jai Malegaonkar,Xiao Jiang,Andre Dion,Gaurav S. Sukhatme,Henrik I. Christensen*

Main category: cs.AI

TL;DR: GAMMS is a lightweight, graph-based simulation framework for scalable and fast prototyping of multi-agent systems, integrating with external tools and supporting various agent types.


<details>
  <summary>Details</summary>
Motivation: Existing high-fidelity simulators are computationally expensive and not suited for rapid prototyping or large-scale agent deployments.

Method: Developed GAMMS as an extensible framework using graph representations for environments, with emphasis on scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding.

Result: GAMMS enables efficient simulation of complex domains like urban road networks and communication systems, supports integration with external tools, and provides built-in visualization.

Conclusion: GAMMS lowers the barrier to entry for researchers, facilitates experimentation and innovation in multi-agent systems, and is open-source available at https://github.com/GAMMSim/GAMMS/.

Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/

</details>


### [11] [Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment](https://arxiv.org/abs/2602.05110)
*Liang Wang,Junpeng Wang,Chin-chia Michael Yeh,Yan Zheng,Jiarui Sun,Xiran Fan,Xin Dai,Yujie Fan,Yiwei Cai*

Main category: cs.AI

TL;DR: A structured multi-evaluator framework assesses LLM reasoning in payment-risk merchant assessments, revealing bias heterogeneity and alignment with human judgment.


<details>
  <summary>Details</summary>
Motivation: LLMs are used as evaluators in reasoning tasks, but their reliability and bias in payments-risk settings are poorly understood, necessitating a systematic evaluation framework.

Method: Introduces a five-criterion rubric with Monte-Carlo scoring, uses five frontier LLMs to generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions, and employs a consensus-deviation metric to eliminate circularity.

Result: Results show substantial heterogeneity in self-evaluation bias among LLMs (e.g., GPT-5.1 negative bias, Gemini-2.5 Pro positive bias), bias attenuation of 25.8% under anonymization, LLM scores averaging +0.46 above human consensus, and significant alignment with payment-network data (Spearman rho = 0.56 to 0.77).

Conclusion: The framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and emphasizes the need for bias-aware protocols in financial settings.

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

</details>


### [12] [Democratic Preference Alignment via Sortition-Weighted RLHF](https://arxiv.org/abs/2602.05113)
*Suvadip Sana,Jinzhou Wu,Martin T. Wells*

Main category: cs.AI

TL;DR: Democratic Preference Optimization (DemPO) uses algorithmic sortition to make AI preference training more demographically representative, with Hard Panel and Soft Panel schemes that improve model alignment compared to unweighted baselines.


<details>
  <summary>Details</summary>
Motivation: Current AI preference-based alignment methods like RLHF rely on human raters from convenience samples that often over- or under-represent certain demographics, leading to biased models. DemPO addresses this by ensuring representativeness in the training data through sortition, similar to citizen assemblies.

Method: DemPO introduces two training schemes: Hard Panel trains only on preferences from a quota-satisfying mini-public sampled via sortition, while Soft Panel retains all data but reweights each rater based on their inclusion probability under the sortition lottery. Theoretical proof shows Soft Panel recovers Hard Panel's expected objective in closed form. Evaluated on a public preference dataset with demographic info, using Llama models (1B to 8B parameters) fine-tuned under each scheme.

Result: Across six aggregation methods, the Hard Panel consistently ranks first, and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes increasing as model capacity grows.

Conclusion: Enforcing demographic representativeness at the preference collection stage (e.g., through DemPO) yields AI models whose behavior better reflects values from representative publics, outperforming post hoc corrections.

Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.

</details>


### [13] [SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers](https://arxiv.org/abs/2602.05115)
*Keyang Xuan,Pengda Wang,Chongrui Ye,Haofei Yu,Tal August,Jiaxuan You*

Main category: cs.AI

TL;DR: The paper introduces SocialVeil, a social learning environment to evaluate LLMs' social intelligence under realistic communication barriers like semantic vagueness, sociocultural mismatch, and emotional interference, showing significant performance drops and limited effectiveness of adaptation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating LLMs in social interactions assume idealized communication, limiting diagnosis of their ability to handle realistic, imperfect settings with communication disruptions.

Method: The paper develops SocialVeil, based on a literature review of human communication challenges, to simulate interactions with three types of barriers. It uses two evaluation metrics (unresolved confusion and mutual understanding) and conducts experiments across 720 scenarios with four frontier LLMs, including human evaluations for fidelity.

Result: Communication barriers consistently impair LLM performance, reducing mutual understanding by over 45% on average and increasing confusion by nearly 50%. Adaptation strategies like Repair Instruction and Interactive learning have only modest effects, not reaching barrier-free levels.

Conclusion: SocialVeil advances social interaction environments toward real-world communication, highlighting LLMs' current limitations in handling disruptions and opening opportunities for improving their social intelligence in more authentic contexts.

Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Denoising diffusion networks for normative modeling in neuroimaging](https://arxiv.org/abs/2602.04886)
*Luke Whitbread,Lyle J. Palmer,Mark Jenkinson*

Main category: cs.LG

TL;DR: DDPMs are proposed for normative modeling of tabular neuroimaging phenotypes, enabling scalable joint modeling with well-calibrated outputs and preserved dependence structure, using FiLM-MLP and SAINT transformer backbones, evaluated on synthetic and UK Biobank data.


<details>
  <summary>Details</summary>
Motivation: Traditional normative modeling in neuroimaging fits one model per imaging-derived phenotype (IDP), which scales efficiently but ignores multivariate dependence that could reveal coordinated biological patterns. A unified approach is needed to jointly model multiple IDPs while maintaining interpretability and calibration.

Method: Denoising diffusion probabilistic models (DDPMs) are used as conditional density estimators for tabular IDPs. Two denoiser backbones are employed: a FiLM-conditioned multilayer perceptron (MLP) and a tabular transformer (SAINT) with feature self-attention and intersample attention, conditioning covariates via learned embeddings. Evaluation scales from 2 to 200 dimensions.

Result: In low dimensions, diffusion models provide well-calibrated per-IDP outputs similar to traditional baselines while modeling realistic multivariate dependence. At higher dimensions, the transformer backbone outperforms the MLP in calibration and preserves higher-order dependence, supporting scalable joint normative modeling compatible with standard pipelines.

Conclusion: Diffusion-based normative modeling is a practical method for generating calibrated multivariate deviation profiles in neuroimaging, addressing limitations of univariate approaches by efficiently capturing dependence without sacrificing interpretability.

Abstract: Normative modeling estimates reference distributions of biological measures conditional on covariates, enabling centiles and clinically interpretable deviation scores to be derived. Most neuroimaging pipelines fit one model per imaging-derived phenotype (IDP), which scales well but discards multivariate dependence that may encode coordinated patterns. We propose denoising diffusion probabilistic models (DDPMs) as a unified conditional density estimator for tabular IDPs, from which univariate centiles and deviation scores are derived by sampling. We utilise two denoiser backbones: (i) a feature-wise linear modulation (FiLM) conditioned multilayer perceptron (MLP) and (ii) a tabular transformer with feature self-attention and intersample attention (SAINT), conditioning covariates through learned embeddings. We evaluate on a synthetic benchmark with heteroscedastic and multimodal age effects and on UK Biobank FreeSurfer phenotypes, scaling from dimension of 2 to 200. Our evaluation suite includes centile calibration (absolute centile error, empirical coverage, and the probability integral transform), distributional fidelity (Kolmogorov-Smirnov tests), multivariate dependence diagnostics, and nearest-neighbour memorisation analysis. For low dimensions, diffusion models deliver well-calibrated per-IDP outputs comparable to traditional baselines while jointly modeling realistic dependence structure. At higher dimensions, the transformer backbone remains substantially better calibrated than the MLP and better preserves higher-order dependence, enabling scalable joint normative models that remain compatible with standard per-IDP pipelines. These results support diffusion-based normative modeling as a practical route to calibrated multivariate deviation profiles in neuroimaging.

</details>


### [15] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: Causal Analyst: a framework using LLMs and causal discovery to identify direct causes of jailbreaks in LLMs, improving attack and defense via features like 'Positive Character'.


<details>
  <summary>Details</summary>
Motivation: Mechanisms behind jailbreaks in LLMs are poorly understood, with existing studies overlooking causal relationships between interpretable prompt features and jailbreak occurrences.

Method: Propose Causal Analyst, integrating LLMs into causal discovery with a dataset of 35k jailbreak attempts across 7 LLMs, annotated with 37 human-readable prompt features, and jointly training LLM-based encoding and GNN-based causal graph learning.

Result: Analysis reveals specific features like 'Positive Character' and 'Number of Task Steps' as direct causal drivers; applications include Jailbreaking Enhancer boosting attack success rates and Guardrail Advisor extracting malicious intent.

Conclusion: Analyzing jailbreak features from a causal perspective is effective and interpretable for improving LLM reliability, as confirmed by experiments.

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [16] [Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability](https://arxiv.org/abs/2602.04902)
*Kingsuk Maitra*

Main category: cs.LG

TL;DR: The paper introduces Momentum Attention, a symplectic augmentation that embeds physical priors into the Transformer via kinematic momentum, enabling single-layer induction heads and linking Generative AI with Hamiltonian physics and signal processing.


<details>
  <summary>Details</summary>
Motivation: To extend the Transformer computational graph with physical circuit principles, using conservation laws and time-varying dynamics to overcome topological depth constraints in induction head formation.

Method: Propose Momentum Attention by applying a symplectic shear to queries and keys using the kinematic difference operator, establishing Symplectic-Filter Duality to equate physical shear with high-pass filtering.

Result: Validated in over 5,100 experiments, the 125M Momentum model outperforms on induction-heavy tasks, stays within ~2.9% validation loss of a 350M baseline, and shows a scaling law for momentum-depth fungibility.

Conclusion: This framework offers a toolkit that connects Generative AI, Hamiltonian Physics, and Signal Processing, enabling enhanced mechanistic interpretability and performance.

Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $γ^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.

</details>


### [17] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: Feature steering methods effectively control LLM behaviors but cause significant degradation in performance and coherence, making prompt engineering a more balanced practical approach.


<details>
  <summary>Details</summary>
Motivation: To assess the practical effectiveness of feature steering for LLM control compared to prompt engineering, focusing on trade-offs with output quality in real-world applications.

Method: Evaluated Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries on 171 MMLU questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control.

Result: Auto Steer successfully modified target behaviors (3.33 vs. 2.98 on Llama-8B and 3.57 vs. 3.10 on Llama-70B) but led to dramatic drops: accuracy from 66% to 46% (Llama-8B) and 87% to 73% (Llama-70B), coherence from 4.62 to 2.24 and 4.94 to 3.89 respectively.

Conclusion: Current feature steering methods are impractical for deployment where task performance is critical; prompt engineering offers a better balance, and mechanistic control methods require empirical characterization of capability-behavior trade-offs.

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [18] [DCER: Dual-Stage Compression and Energy-Based Reconstruction](https://arxiv.org/abs/2602.04904)
*Yiwen Wang,Jiahao Qin*

Main category: cs.LG

TL;DR: DCER is a unified multimodal fusion framework that uses dual-stage compression and energy-based reconstruction to handle noisy inputs and missing modalities, achieving state-of-the-art performance with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Multimodal fusion faces challenges with noisy inputs degrading representation quality and missing modalities causing prediction failures, necessitating a robust solution.

Method: The method includes two stages: compression via within-modality frequency transforms and cross-modality bottleneck tokens, and energy-based reconstruction for missing modalities using gradient descent on a learned energy function.

Result: Experiments on CMU-MOSI, CMU-MOSEI, and CH-SIMS show state-of-the-art performance, with a U-shaped robustness pattern, and energy-based reconstruction provides uncertainty quantification with high correlation (>0.72) to prediction error.

Conclusion: DCER effectively addresses robustness in multimodal fusion, offering strong performance in both complete and high-missing conditions, with code availability on Github.

Abstract: Multimodal fusion faces two robustness challenges: noisy inputs degrade representation quality, and missing modalities cause prediction failures. We propose DCER, a
  unified framework addressing both challenges through dual-stage compression and energy-based reconstruction. The compression stage operates at two levels:
  within-modality frequency transforms (wavelet for audio, DCT for video) remove noise while preserving task-relevant patterns, and cross-modality bottleneck tokens
  force genuine integration rather than modality-specific shortcuts. For missing modalities, energy-based reconstruction recovers representations via gradient descent
  on a learned energy function, with the final energy providing intrinsic uncertainty quantification (\r{ho} > 0.72 correlation with prediction error). Experiments on
  CMU-MOSI, CMU-MOSEI, and CH-SIMS demonstrate state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion at
  both complete and high-missing conditions. The code will be available on Github.

</details>


### [19] [LISA: Laplacian In-context Spectral Analysis](https://arxiv.org/abs/2602.04906)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LISA is a method for inference-time adaptation of Laplacian-based time-series models using observed data prefixes, combining delay embeddings and spectral learning for improved forecasting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance time-series forecasting under changing dynamics by adapting Laplacian-based models at inference time with minimal computational overhead, linking in-context adaptation to nonparametric spectral methods.

Method: LISA uses delay-coordinate embeddings and Laplacian spectral learning to create diffusion-coordinate state representations, with lightweight latent-space residual adapters (Gaussian-process regression or attention-like Markov operator) over context windows.

Result: In forecasting and autoregressive rollout experiments, LISA outperforms frozen baselines and is particularly beneficial under changing dynamics.

Conclusion: LISA effectively bridges in-context adaptation with nonparametric spectral methods, offering a practical approach for inference-time model adaptation in dynamic time-series systems.

Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.

</details>


### [20] [Physics as the Inductive Bias for Causal Discovery](https://arxiv.org/abs/2602.04907)
*Jianhong Chen,Naichen Shi,Xubo Yue*

Main category: cs.LG

TL;DR: A framework integrating partial physical knowledge (as ODEs) into causal discovery for dynamical systems using stochastic differential equations, improving graph recovery and stability.


<details>
  <summary>Details</summary>
Motivation: Integrate physics-based models (e.g., ODEs) with data-driven causal discovery to handle complex dynamics with feedback, cycles, and non-stationarity, overcoming limitations of acyclic or equilibrium-based methods.

Method: Model system evolution as a stochastic differential equation (SDE) where drift encodes known ODE dynamics and diffusion represents unknown causal couplings. Use a scalable sparsity-inducing MLE algorithm exploiting causal graph structure for efficient parameter estimation.

Result: Guarantees for causal graph recovery under mild conditions; experiments on diverse dynamical systems show improved causal graph recovery, stability, and physical consistency compared to purely data-driven baselines.

Conclusion: The framework successfully leverages partial physical knowledge as an inductive bias, enhancing identifiability and robustness in causal discovery for dynamical systems, particularly in real-world scenarios with complex interactions.

Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.

</details>


### [21] [Temporal Pair Consistency for Variance-Reduced Flow Matching](https://arxiv.org/abs/2602.04908)
*Chika Maduabuchi,Jindong Wang*

Main category: cs.LG

TL;DR: Temporal Pair Consistency (TPC) is introduced to reduce variance in continuous-time generative models by coupling predictions at paired timesteps, improving efficiency and sample quality.


<details>
  <summary>Details</summary>
Motivation: Current continuous-time generative models like diffusion and flow matching suffer from high estimator variance and inefficient sampling due to independent timestep training objectives, which prior methods address with complex modifications.

Method: TPC uses a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path without changing architecture, probability path, or solver, providing theoretical analysis of its quadratic regularization effects.

Result: TPC improves sample quality and efficiency on datasets like CIFAR-10 and ImageNet at various resolutions, achieving lower FID at similar or lower computational costs compared to prior methods, and integrates with advanced pipelines like noise-augmented training and rectified flow.

Conclusion: TPC effectively addresses variance issues in generative models through a simple estimator-level modification, offering practical benefits in training and sampling while maintaining compatibility with existing techniques.

Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.

</details>


### [22] [Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment](https://arxiv.org/abs/2602.04909)
*Youngjae Cho,Jongsuk Kim,Ji-Hoon Kim*

Main category: cs.LG

TL;DR: GAPO improves LLM alignment by replacing static reference policies with dynamic, geometry-aware anchors to handle noise and prevent reward drift.


<details>
  <summary>Details</summary>
Motivation: Static reference policies in methods like DPO can become miscalibrated as the policy drifts, causing distributional mismatch and amplifying noise, while reference-free variants risk unconstrained reward drift.

Method: Proposes Geometric Anchor Preference Optimization (GAPO), which uses a dynamic anchor: an adversarial local perturbation of the current policy within a small radius, serving as a pessimistic baseline. Introduces an adaptive reweighting mechanism based on local sensitivity and optimizes a logistic objective weighted by the Anchor Gap.

Result: GAPO consistently improves robustness in diverse noise settings and matches or improves performance on standard LLM alignment and reasoning benchmarks.

Conclusion: GAPO effectively addresses limitations of fixed references in preference optimization by leveraging dynamic anchors, enhancing robustness without sacrificing performance.

Abstract: Direct Preference Optimization (DPO) and related methods align large language models from pairwise preferences by regularizing updates against a fixed reference policy. As the policy drifts, a static reference, however, can become increasingly miscalibrated, leading to distributional mismatch and amplifying spurious preference signals under noisy supervision. Conversely, reference-free variants avoid mismatch but often suffer from unconstrained reward drift. We propose Geometric Anchor Preference Optimization (GAPO), which replaces the fixed reference with a dynamic, geometry-aware anchor: an adversarial local perturbation of the current policy within a small radius that serves as a pessimistic baseline. This anchor enables an adaptive reweighting mechanism, modulating the importance of each preference pair based on its local sensitivity. We further introduce the Anchor Gap, the reward discrepancy between the policy and its anchor, and show under smoothness conditions that it approximates worst-case local margin degradation. Optimizing a logistic objective weighted by this gap downweights geometrically brittle instances while emphasizing robust preference signals. Across diverse noise settings, GAPO consistently improves robustness while matching or improving performance on standard LLM alignment and reasoning benchmarks.

</details>


### [23] [A logical re-conception of neural networks: Hamiltonian bitwise part-whole architecture](https://arxiv.org/abs/2602.04911)
*E Bowen,R Granger,A Rodriguez*

Main category: cs.LG

TL;DR: A novel neural architecture using graph-Hamiltonian operators with low-precision arithmetic for relational encoding and abductive inference, distinct from standard ANNs.


<details>
  <summary>Details</summary>
Motivation: To develop a system where relational representations like part-whole are intrinsic, not added on, enabling symbolic computation and efficient processing with linear scaling.

Method: Encode data as graphs with edges from primitive relations; use a graph-Hamiltonian operator to compute energies, achieving constraint satisfaction via ground states with radically low-precision arithmetic.

Result: The system processes ANN examples, produces symbolic-like representations, identifies logical structures, enables abductive inference, and derives equivalent ANN operations for semantic representation.

Conclusion: The architecture offers a promising approach for relational and symbolic computation in neural systems, with potential for further improvements and tools.

Abstract: We introduce a simple initial working system in which relations (such as part-whole) are directly represented via an architecture with operating and learning rules fundamentally distinct from standard artificial neural network methods. Arbitrary data are straightforwardly encoded as graphs whose edges correspond to codes from a small fixed primitive set of elemental pairwise relations, such that simple relational encoding is not an add-on, but occurs intrinsically within the most basic components of the system. A novel graph-Hamiltonian operator calculates energies among these encodings, with ground states denoting simultaneous satisfaction of all relation constraints among graph vertices. The method solely uses radically low-precision arithmetic; computational cost is correspondingly low, and scales linearly with the number of edges in the data. The resulting unconventional architecture can process standard ANN examples, but also produces representations that exhibit characteristics of symbolic computation. Specifically, the method identifies simple logical relational structures in these data (part-of; next-to), building hierarchical representations that enable abductive inferential steps generating relational position-based encodings, rather than solely statistical representations. Notably, an equivalent set of ANN operations are derived, identifying a special case of embedded vector encodings that may constitute a useful approach to current work in higher-level semantic representation. The very simple current state of the implemented system invites additional tools and improvements.

</details>


### [24] [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)
*Xiaolin Hu,Hang Yuan,Xinzhu Sang,Binbin Yan,Zhou Yu,Cong Huang,Kai Chen*

Main category: cs.LG

TL;DR: Proposes A^2-LLM, an end-to-end conversational audio avatar LLM that jointly models language, audio prosody, and 3D facial motion to enhance emotional expressiveness and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Current systems use cascaded architectures with independent modules, leading to error accumulation, high latency, poor real-time performance, and focus on lip-sync over emotional depth.

Method: Introduces A^2-LLM, a unified framework for reasoning about language, audio prosody, and 3D facial motion, trained on FLAME-QA, a high-quality multimodal dataset aligning semantic intent with expressive facial dynamics in QA format.

Result: Achieves superior emotional expressiveness with emotionally rich facial movements beyond lip-sync, while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

Conclusion: A^2-LLM effectively addresses limitations of cascaded systems by enabling end-to-end joint modeling, enhancing conversational digital humans with deep semantic understanding and real-time responsiveness.

Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

</details>


### [25] [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)
*Jose Miguel Luna,Taha Bouhsine,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SLAY is a linear-time attention mechanism that approximates softmax attention closely using spherical constraints and Yat-kernels, enabling scalable Transformers without performance loss.


<details>
  <summary>Details</summary>
Motivation: To overcome the quadratic complexity of standard softmax attention in Transformers, which limits scalability, by developing a linear-time alternative that retains performance.

Method: Constrains queries and keys to the unit sphere for angular-dependent attention, uses Bernstein's theorem to express spherical Yat-kernel as a nonnegative mixture of polynomial-exponential kernels, and derives a positive random-feature approximation for O(L) complexity.

Result: Empirically achieves performance nearly indistinguishable from softmax attention, with linear time and memory scaling, and outperforms prior linear-time methods like Performers and Cosformers.

Conclusion: SLAY provides the closest linear-time approximation to softmax attention to date, enabling scalable Transformers without typical trade-offs in attention linearization.

Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interactions in physics. Our method, Spherical Linearized Attention with Yat Kernels (SLAY), constrains queries and keys to the unit sphere so that attention depends only on angular alignment. Using Bernstein's theorem, we express the spherical Yat-kernel as a nonnegative mixture of polynomial-exponential product kernels and derive a strictly positive random-feature approximation enabling linear-time O(L) attention. We establish positive definiteness and boundedness on the sphere and show that the estimator yields well-defined, nonnegative attention scores. Empirically, SLAY achieves performance that is nearly indistinguishable from standard softmax attention while retaining linear time and memory scaling, and consistently outperforms prior linear-time attention mechanisms such as Performers and Cosformers. To the best of our knowledge, SLAY represents the closest linear-time approximation to softmax attention reported to date, enabling scalable Transformers without the typical performance trade-offs of attention linearization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [26] [AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval](https://arxiv.org/abs/2602.05524)
*Konosuke Yoshizato,Kazuma Shimizu,Ryota Higa,Takanobu Otsuka*

Main category: cs.MA

TL;DR: This paper explores using LLM-based multi-agent systems for inventory management, showing they can achieve optimal decisions in limited scenarios with fixed prompts, and proposes AIM-RM, a new agent that improves adaptability by leveraging historical data, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLM-based MASs are promising for inventory management to overcome challenges of traditional methods, but uncertainties remain about their ability to consistently deliver optimal policies and adapt to various supply chain scenarios.

Method: The study examines an LLM-based MAS with a fixed-ordering strategy prompt encoding stepwise problem processes and safe-stock strategies, and proposes AIM-RM, an agent using similarity matching to leverage historical experiences for enhanced adaptability.

Result: Empirical results show that LLM-based MASs can determine optimal ordering decisions in restricted scenarios without detailed prompt adjustments, and AIM-RM outperforms benchmark methods across diverse supply chain scenarios.

Conclusion: LLM-based MASs are effective for inventory management, particularly with AIM-RM's approach, highlighting their potential for robust and adaptable solutions in supply chain contexts.

Abstract: This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.

</details>


### [27] [Learning to Share: Selective Memory for Efficient Parallel Agentic Systems](https://arxiv.org/abs/2602.05965)
*Joseph Fioresi,Parth Parag Kulkarni,Ashmal Vayani,Song Wang,Mubarak Shah*

Main category: cs.MA

TL;DR: Learning to Share (LTS) introduces a learned shared-memory mechanism for parallel agentic systems to reduce computational redundancy by enabling selective cross-team information reuse.


<details>
  <summary>Details</summary>
Motivation: Parallel agentic systems often involve multiple teams performing overlapping computations independently, leading to significant inefficiencies and high computational costs.

Method: LTS uses a global memory bank and a lightweight controller trained with stepwise reinforcement learning to decide which intermediate agent steps to store for cross-team reuse.

Result: Experiments on AssistantBench and GAIA benchmarks show LTS reduces runtime while maintaining or improving task performance compared to memory-free parallel baselines.

Conclusion: LTS effectively improves the efficiency of parallel agentic systems through learned memory admission, demonstrating its potential for scalable agent coordination.

Abstract: Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/

</details>


### [28] [PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling](https://arxiv.org/abs/2602.06030)
*Kavana Venkatesh,Yinhan He,Jundong Li,Jiaming Cui*

Main category: cs.MA

TL;DR: PhysicsAgentABM combines neural and symbolic models for scalable, calibrated multi-agent simulation with reduced LLM calls via clustering.


<details>
  <summary>Details</summary>
Motivation: LLM-based multi-agent systems are expensive and poorly calibrated for simulation, while classical ABMs struggle with rich individual behaviors.

Method: Shifts inference to agent clusters using symbolic agents, neural transition models, and uncertainty-aware fusion, plus ANCHOR clustering with LLMs.

Result: Gains in event-time accuracy and calibration over baselines in public health, finance, and social science experiments, with up to 6-8x LLM call reduction.

Conclusion: Establishes a new paradigm for scalable and calibrated simulation by re-architecting ABMs around population-level inference with neuro-symbolic fusion.

Abstract: Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.

</details>
