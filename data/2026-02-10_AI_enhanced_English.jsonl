{"id": "2602.07092", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07092", "abs": "https://arxiv.org/abs/2602.07092", "authors": ["Haipeng Jiang", "Kailong Ren", "Zimo Yin", "Zhetao Sun", "Xin Gan", "Guangyi Lv", "Ming He", "Peng Wang", "Congli Yin", "Hong Pan", "Changwen Zhang", "Shan Tong", "Zhengyu Xu", "Zeping Chen", "Yubin Huangfu", "Yanzhi Xu", "Xing Su", "Qin Feng", "Dong An", "Jianping Fan"], "title": "Lemon Agent Technical Report", "comment": null, "summary": "Recent advanced LLM-powered agent systems have exhibited their remarkable capabilities in tackling complex, long-horizon tasks. Nevertheless, they still suffer from inherent limitations in resource efficiency, context management, and multimodal perception. Based on these observations, Lemon Agent is introduced, a multi-agent orchestrator-worker system built on a newly proposed AgentCortex framework, which formalizes the classic Planner-Executor-Memory paradigm through an adaptive task execution mechanism. Our system integrates a hierarchical self-adaptive scheduling mechanism that operates at both the overall orchestrator layer and workers layer. This mechanism can dynamically adjust computational intensity based on task complexity. It enables orchestrator to allocate one or more workers for parallel subtask execution, while workers can further improve operational efficiency by invoking tools concurrently. By virtue of this two-tier architecture, the system achieves synergistic balance between global task coordination and local task execution, thereby optimizing resource utilization and task processing efficiency in complex scenarios. To reduce context redundancy and increase information density during parallel steps, we adopt a three-tier progressive context management strategy. To make fuller use of historical information, we propose a self-evolving memory system, which can extract multi-dimensional valid information from all historical experiences to assist in completing similar tasks. Furthermore, we provide an enhanced MCP toolset. Empirical evaluations on authoritative benchmarks demonstrate that our Lemon Agent can achieve a state-of-the-art 91.36% overall accuracy on GAIA and secures the top position on the xbench-DeepSearch leaderboard with a score of 77+.", "AI": {"tldr": "Lemon Agent is a multi-agent system with a hierarchical scheduling mechanism, progressive context management, self-evolving memory, and an enhanced toolset, achieving state-of-the-art performance on benchmarks.", "motivation": "Current LLM-powered agent systems face limitations in resource efficiency, context management, and multimodal perception.", "method": "Introduces Lemon Agent based on the AgentCortex framework, featuring a hierarchical self-adaptive scheduling mechanism, three-tier progressive context management, a self-evolving memory system, and an enhanced MCP toolset.", "result": "Achieves 91.36% overall accuracy on GAIA and top score of 77+ on xbench-DeepSearch leaderboard.", "conclusion": "Lemon Agent optimizes resource utilization and task efficiency through its innovative architecture, addressing key limitations in agent systems."}}
{"id": "2602.07186", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07186", "abs": "https://arxiv.org/abs/2602.07186", "authors": ["Luoxi Tang", "Yuqiao Meng", "Joseph Costa", "Yingxue Zhang", "Muchao Ye", "Zhaohan Xi"], "title": "The Value of Variance: Mitigating Debate Collapse in Multi-Agent Systems via Uncertainty-Driven Policy Optimization", "comment": null, "summary": "Multi-agent debate (MAD) systems improve LLM reasoning through iterative deliberation, but remain vulnerable to debate collapse, a failure type where final agent decisions are compromised on erroneous reasoning. Existing methods lack principled mechanisms to detect or prevent such failures. To address this gap, we first propose a hierarchical metric that quantifies behavioral uncertainty at three levels: intra-agent (individual reasoning uncertainty), inter-agent (interactive uncertainty), and system-level (output uncertainty). Empirical analysis across several benchmarks reveals that our proposed uncertainty quantification reliably indicates system failures, which demonstrates the validity of using them as diagnostic metrics to indicate the system failure. Subsequently, we propose a mitigation strategy by formulating an uncertainty-driven policy optimization to penalize self-contradiction, peer conflict, and low-confidence outputs in a dynamic debating environment. Experiments demonstrate that our proposed uncertainty-driven mitigation reliably calibrates the multi-agent system by consistently improving decision accuracy while reducing system disagreement.", "AI": {"tldr": "Proposes a hierarchical uncertainty metric and mitigation strategy to detect and prevent debate collapse in multi-agent debate systems, improving accuracy and reducing disagreement.", "motivation": "Multi-agent debate systems enhance LLM reasoning through deliberation but are vulnerable to debate collapse where erroneous reasoning compromises decisions, and current methods lack mechanisms to detect or prevent such failures.", "method": "Develops a hierarchical metric quantifying uncertainty at intra-agent (individual reasoning), inter-agent (interactive), and system-level (output) to diagnose failures, and proposes an uncertainty-driven policy optimization to penalize self-contradiction, peer conflict, and low-confidence outputs.", "result": "Empirical analysis shows the uncertainty quantification reliably indicates system failures, and experiments demonstrate the mitigation strategy effectively calibrates the system, improving decision accuracy and reducing disagreement.", "conclusion": "The proposed uncertainty metrics serve as valid diagnostic tools for detecting debate collapse, and the mitigation strategy based on policy optimization successfully addresses this vulnerability, leading to more reliable multi-agent debate systems."}}
{"id": "2602.07777", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.07777", "abs": "https://arxiv.org/abs/2602.07777", "authors": ["Shuhui Zhu", "Yue Lin", "Shriya Kaistha", "Wenhao Li", "Baoxiang Wang", "Hongyuan Zha", "Gillian K. Hadfield", "Pascal Poupart"], "title": "Talk, Judge, Cooperate: Gossip-Driven Indirect Reciprocity in Self-Interested LLM Agents", "comment": null, "summary": "Indirect reciprocity, which means helping those who help others, is difficult to sustain among decentralized, self-interested LLM agents without reliable reputation systems. We introduce Agentic Linguistic Gossip Network (ALIGN), an automated framework where agents strategically share open-ended gossip using hierarchical tones to evaluate trustworthiness and coordinate social norms. We demonstrate that ALIGN consistently improves indirect reciprocity and resists malicious entrants by identifying and ostracizing defectors without changing intrinsic incentives. Notably, we find that stronger reasoning capabilities in LLMs lead to more incentive-aligned cooperation, whereas chat models often over-cooperate even when strategically suboptimal. These results suggest that leveraging LLM reasoning through decentralized gossip is a promising path for maintaining social welfare in agentic ecosystems. Our code is available at https://github.com/shuhui-zhu/ALIGN.", "AI": {"tldr": "ALIGN framework uses automated gossip with hierarchical tones to enhance indirect reciprocity among LLM agents, improving cooperation and resisting defectors without changing incentives.", "motivation": "Indirect reciprocity is hard to sustain in decentralized, self-interested LLM agents lacking reliable reputation systems, necessitating a new approach to coordinate social norms.", "method": "Introduces Agentic Linguistic Gossip Network (ALIGN), where agents strategically share open-ended gossip using hierarchical tones to evaluate trustworthiness and coordinate norms automatically.", "result": "ALIGN consistently improves indirect reciprocity, resists malicious entrants by identifying and ostracizing defectors, and shows that stronger LLM reasoning leads to more incentive-aligned cooperation, while chat models over-cooperate.", "conclusion": "Leveraging LLM reasoning through decentralized gossip like ALIGN is a promising way to maintain social welfare in agentic ecosystems, with code available for implementation."}}
