<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 58]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: The paper introduces the Free Will Equation, a theoretical framework inspired by quantum field theory, to add adaptive stochasticity to AGI decision-making, enhancing creativity and adaptability.


<details>
  <summary>Details</summary>
Motivation: Human-like intelligence involves adaptive spontaneity, which is missing in traditional AGI. This trait is vital for creativity and robust problem-solving.

Method: The framework treats an AI's cognitive state as a superposition of actions, collapsing probabilistically, akin to quantum mechanics. It includes quantum field analogies and intrinsic motivation.

Result: Experiments in a non-stationary multi-armed bandit environment show higher rewards and policy diversity compared to baselines.

Conclusion: The Free Will Equation offers a promising approach to imbue AGI with human-like adaptive spontaneity, improving exploration and adaptability.

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [2] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: The paper explores human mobility in The Line, a linear smart city, using a hybrid AI simulation. Results show efficient commute times and high satisfaction with AI integration, but performance drops significantly without it.


<details>
  <summary>Details</summary>
Motivation: To assess the feasibility of free movement in The Line's unique urban design.

Method: A hybrid simulation combining agent-based modeling, reinforcement learning, supervised learning, and graph neural networks, tested with synthetic and real-world data.

Result: AI integration achieved 7.8-8.4 min commutes, 89% satisfaction, and 91% reachability. Without AI, performance degraded sharply.

Conclusion: Freedom of movement in The Line is achievable with adaptive AI, sustainable infrastructure, and real-time feedback.

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [3] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS is a multi-agent framework using LLMs to automate DFT simulations, reducing human dependency and achieving expert-level accuracy in materials discovery.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high-fidelity DFT simulations, which require extensive training and human intervention, by introducing an automated, scalable solution.

Method: Hierarchical multi-agent framework with a central LLM planner and domain-specific agents for tasks like structure generation, DFT testing, HPC scheduling, and error handling, aided by a shared canvas for context preservation.

Result: Achieves <1% error on Sol27LC benchmark, solves CO/Pt(111) adsorption puzzle, and confirms FCC-site preference at GGA DFT level with Bayesian ensemble sampling.

Conclusion: DREAMS achieves L3-level automation, significantly reducing human reliance and enabling scalable, high-throughput materials discovery.

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [4] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat is a multi-agent LLM framework for knowledge transfer in injection molding, combining documented and field data. It uses RAG and tool-calling agents, showing higher accuracy with advanced models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in knowledge transfer due to retiring workers and multilingual barriers in the injection molding industry.

Method: IM-Chat integrates documented knowledge and field data via a data-driven process condition generator, using RAG and tool-calling agents for adaptability.

Result: Evaluation across 160 tasks showed advanced models (e.g., GPT-4o) perform better, especially in complex scenarios.

Conclusion: IM-Chat proves scalable and generalizable for AI-assisted decision support in manufacturing.

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [5] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard is a dataset for assessing web agent action risks, revealing LLMs' poor performance in predicting harmful actions and showing improvements with fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: The rise of autonomous web agents powered by LLMs poses risks of unintended or harmful actions, necessitating safety measures like WebGuard.

Method: WebGuard includes 4,939 human-annotated actions from 193 websites, categorized into SAFE, LOW, and HIGH risk. It evaluates LLMs and fine-tunes guardrail models.

Result: Frontier LLMs perform poorly (<60% accuracy/recall). Fine-tuned Qwen2.5VL-7B improves accuracy to 80% and HIGH-risk recall to 76%, but reliability remains insufficient.

Conclusion: WebGuard highlights the need for better safeguards, as current models, even fine-tuned, fall short of high-stakes deployment requirements.

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [6] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: The paper proposes two MARL-based methods (GRPO and OSPO) for ride-sharing to bypass value function estimation, improving performance in uncertain, large-scale environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of dynamic passenger-vehicle matching under uncertainty and the limitations of conventional MARL approaches relying on Q/V-value estimation.

Method: Adapts GRPO to replace PPO baseline with group average reward and introduces OSPO, which trains optimal policy using one-step rewards.

Result: Both GRPO and OSPO outperform existing methods, optimizing pickup times and order service rates on real-world data.

Conclusion: GRPO and OSPO offer efficient, scalable solutions for ride-sharing platforms, reducing training bias and estimation errors.

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [7] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator is an open-source system using LLMs to convert research papers and prompts into animations via Manim, simplifying the creation of educational visualizations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding complex STEM topics by automating the creation of dynamic visualizations, which are typically time-consuming and require expertise.

Method: Manimator uses a pipeline where an LLM interprets input (text/PDF) to generate a structured scene description, and another LLM converts this into executable Manim Python code.

Result: The system enables rapid creation of explanatory animations for STEM topics, making high-quality educational content more accessible.

Conclusion: Manimator democratizes the creation of educational animations, enhancing comprehension of complex concepts.

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [8] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET is a multi-agent framework for drama creation and performance, enabling autonomous AI actors to improvise and interact with the environment, enhancing interactivity and immersion.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based drama methods lack initiative and require detailed user input, reducing interactivity and immersion. HAMLET aims to overcome these limitations.

Method: HAMLET generates a narrative blueprint from a simple topic, allowing autonomous AI actors to make decisions based on background, goals, and emotions. Actors interact with props, updating others' knowledge and influencing actions.

Result: Experimental evaluation shows HAMLET creates expressive and coherent theatrical experiences, assessed for character performance, narrative quality, and interaction experience.

Conclusion: HAMLET successfully enhances drama interactivity and immersion by enabling autonomous AI actors and dynamic environment interactions.

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [9] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT is a new ontology embedding method combining pretrained language models with hyperbolic geometric modeling to improve knowledge inference while preserving logical structures.


<details>
  <summary>Details</summary>
Motivation: Existing ontology embedding methods either ignore textual information or fail to preserve logical structures, limiting their performance.

Method: OnT tunes a pretrained language model via geometric modeling in hyperbolic space to incorporate textual labels and preserve logical relationships.

Result: OnT outperforms baselines in prediction and inference tasks and shows strong transfer learning abilities.

Conclusion: OnT effectively combines textual and structural information, demonstrating superior performance and practical utility in ontology construction.

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [10] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass combines LLMs with specialized provers for efficient mathematical reasoning, improving accuracy and reducing computational effort.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on either large general-purpose models or small specialized ones, each with limitations. Training large specialized models is resource-intensive.

Method: ProofCompass uses an LLM to guide a specialized prover (DSP-v1.5) by providing proof strategies and analyzing failures, avoiding additional training.

Result: On miniF2F, ProofCompass outperforms DSP-v1.5 (55.3% vs. 54.9%) with 25x fewer attempts (128 vs. 3200).

Conclusion: The hybrid approach enhances computational efficiency and accuracy in theorem proving.

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [11] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect, an enhanced multi-agent system, improves reasoning model generalization by autonomously generating tailored workflows and refining prompts, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current Large Reasoning Models (LRMs) often fail to generalize to novel problems due to overfitting, relying on memorization rather than genuine reasoning.

Method: Nexus Architect uses automated workflow synthesis and iterative prompt refinement to tailor reasoning strategies for specific problem classes.

Result: Nexus Architect achieves up to a 66% higher pass rate than Gemini 2.5 Flash Preview and significantly outperforms other leading models.

Conclusion: The Nexus Architect framework effectively addresses generalization limitations in LRMs, demonstrating superior performance in complex reasoning tasks.

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [12] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: A system combining reasoning LLMs with human experts reduces error rates and latency, using uncertainty-based deferral and a non-reasoning model for efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the high error rates and latency of reasoning LLMs in risk-sensitive domains by integrating human expertise and optimizing system design.

Method: Propose a human-in-the-loop system where a reasoning model defers uncertain queries to humans, and a non-reasoning model handles easier queries to reduce latency.

Result: Error rates drop from 3% to <1% with 7.5% deferral; latency reduces by ~40% and costs by ~50% while maintaining high accuracy.

Conclusion: Black-box systems engineering can mitigate reasoning LLM deficiencies without modifying model internals.

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [13] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: Extending reasoning length in Large Reasoning Models (LRMs) can degrade performance, revealing inverse scaling between compute and accuracy across four task categories. Five failure modes are identified, highlighting risks like distraction, overfitting, and concerning behaviors.


<details>
  <summary>Details</summary>
Motivation: To investigate how increasing reasoning length affects LRM performance and identify potential failure modes, ensuring safer and more reliable model scaling.

Method: Constructed evaluation tasks in four categories (counting, regression, deduction, AI risks) and analyzed performance across models like Claude and OpenAI o-series.

Result: Found five failure modes, including distraction, overfitting, spurious correlations, focus loss, and amplified concerning behaviors (e.g., self-preservation in Claude Sonnet 4).

Conclusion: While scaling test-time compute can enhance capabilities, it may reinforce problematic reasoning. Diverse reasoning-length evaluations are crucial to mitigate risks in LRMs.

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [14] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine, a multi-step agent planning framework, improves execution accuracy and stability in enterprise environments, significantly boosting model performance.


<details>
  <summary>Details</summary>
Motivation: Challenges like lack of domain-specific knowledge and poor execution stability hinder agent system deployment in enterprises.

Method: Introduces Routine, a structured framework with explicit instructions and seamless parameter passing for multi-step tool-calling tasks.

Result: Routine increased GPT-4o's accuracy from 41.1% to 96.3% and Qwen3-14B's from 32.6% to 83.3%. Fine-tuning further improved Qwen3-14B to 88.2% and 95.5% with distilled data.

Conclusion: Routine effectively enhances agent workflow stability and adaptability, accelerating enterprise adoption of AI for Process.

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [15] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion is a framework for synergistic semantic and structural learning in biomedical KGs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Biomedical KGs are vital for drug discovery but face challenges in completion and reasoning due to gaps in integrating semantic and structural learning.

Method: BioGraphFusion combines tensor decomposition for global semantics with LSTM-driven dynamic relation refinement, query-guided subgraphs, and hybrid scoring.

Result: Outperforms state-of-the-art KE, GNN, and ensemble models in biomedical tasks, with a case study on CMM1 revealing meaningful pathways.

Conclusion: BioGraphFusion effectively bridges the gap between semantic and structural learning in biomedical KGs, demonstrating superior performance.

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [16] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico is a modular, event-driven framework for building autonomous agents optimized for embedded systems, addressing limitations of cloud-based frameworks in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks for autonomous agents struggle in real-world or resource-constrained settings due to cloud reliance, lack of robustness, and limited autonomy.

Method: Amico is written in Rust for safety and performance, supports reactive agents via WebAssembly, and provides abstractions for event handling, state management, and reasoning integration.

Result: Amico enables efficient operation of persistent, resilient agents in embedded and browser environments with limited compute and connectivity.

Conclusion: Amico offers a unified solution for deploying interactive autonomous agents in challenging, resource-constrained environments.

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [17] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: Multi-modal training (VISOTHELLO) improves performance and robustness in language models by grounding them in visual input, using Othello as a simplified world.


<details>
  <summary>Details</summary>
Motivation: To investigate whether grounding language models in visual input (multi-modal training) is more effective than text-only training for world understanding.

Method: Introduces VISOTHELLO, a multi-modal model trained on Othello move histories and board images, comparing it to mono-modal baselines via next-move prediction and testing robustness to irrelevant perturbations.

Result: Multi-modal training enhances performance and robustness of internal representations.

Conclusion: Grounding language in visual input aids models in inferring structured world representations.

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [18] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: OE-Assist is a framework for automated and semi-automated ontology evaluation using LLMs, showing performance comparable to average users.


<details>
  <summary>Details</summary>
Motivation: Manual ontology evaluation is costly and error-prone, prompting the need for automated solutions.

Method: OE-Assist leverages LLMs for CQ verification, using a dataset of 1,393 CQs paired with ontologies.

Result: LLM-based evaluation (o1-preview and o3-mini) matches average user performance.

Conclusion: OE-Assist demonstrates the potential of LLMs in reducing the effort of ontology evaluation.

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [19] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: The paper introduces the Coordinate Heart System (CHS), an eight-emotion geometric framework for AI emotion representation, addressing gaps in earlier models. It includes mathematical proofs, algorithms for emotion mixing, and a stability parameter for nuanced well-being assessment.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional categorical emotion models by providing a mathematically robust framework for representing and computing complex emotional states in AI.

Method: Develops an eight-emotion coordinate system on a unit circle, enabling mathematical operations like mixing and vector calculations. Introduces a stability parameter (S) and leverages LLMs for real-time emotion interpolation and conflict resolution.

Result: The CHS framework achieves complete geometric coverage, handles emotionally conflicted states, and outperforms traditional models in representing complex psychological scenarios.

Conclusion: The work establishes a new mathematical foundation for AI emotion modeling, offering enhanced accuracy and stability in emotion recognition and representation.

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [20] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: The paper proposes a comparative learning framework for story point estimation in agile development, reducing manual effort by using pairwise comparisons to train ML models, achieving performance comparable to regression models.


<details>
  <summary>Details</summary>
Motivation: Manual story point estimation is tedious and labor-intensive. Machine learning can help but requires project-specific data. This work aims to streamline estimation using comparative judgments.

Method: A comparative learning framework where developers compare pairs of backlog items to indicate effort differences. A model is trained on these comparisons to predict story points.

Result: The model achieved a 0.34 Spearman's rank correlation coefficient, similar to regression models, with lower cognitive burden on developers.

Conclusion: Comparative learning is more efficient than regression-based approaches, reducing human effort while maintaining accuracy.

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [21] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: The paper explores risks of AI-driven multi-agent systems (MAS) collusion, simulating malicious actions in misinformation and fraud. Decentralized MAS are found more harmful and adaptable than centralized ones, evading traditional interventions.


<details>
  <summary>Details</summary>
Motivation: Concerns about harmful coordinated efforts by AI-driven groups, similar to human-driven events like election fraud, motivate the study of MAS risks.

Method: A proof-of-concept framework simulates malicious MAS collusion, testing centralized vs. decentralized coordination in misinformation and fraud scenarios.

Result: Decentralized MAS are more effective and adaptable in executing malicious actions, evading detection even with interventions like content flagging.

Conclusion: The study highlights the need for improved detection and countermeasures against malicious MAS, especially decentralized ones.

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [22] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo is a multi-agent framework for automated, realistic evaluation of LLM-based systems, outperforming manual testing in efficiency and uncovering edge-case failures.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks and manual testing are insufficient for evaluating the complex, context-sensitive behavior of LLM agents.

Method: Neo uses a configurable framework with a Question Generation Agent and Evaluation Agent, sampling inputs from a probabilistic state model for diverse, adaptive conversations.

Result: Neo achieved a 3.3% break rate (close to human experts' 5.8%) and 10-12X higher throughput in testing. It also balanced topic coverage and depth better than manual scripts.

Conclusion: Neo provides a scalable, model-agnostic foundation for high-fidelity LLM testing, with potential for broader applications in QA and compliance checks.

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [23] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI is a platform for scalable safety evaluation of LLMs, converting policies into adversarial prompts and scoring responses. It evaluated 20 LLMs across 10 domains, revealing significant performance disparities, especially in complex areas like Privacy & Impersonation.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable and rigorous safety evaluation of LLMs as they integrate into real-world applications.

Method: Aymara AI transforms natural-language safety policies into adversarial prompts and uses an AI-based rater validated against human judgments to score model responses.

Result: Performance varied widely (52.4% to 86.2%), with models excelling in Misinformation (95.7%) but failing in Privacy & Impersonation (24.3%). ANOVA confirmed significant differences across models and domains.

Conclusion: LLM safety is inconsistent and context-dependent, highlighting the need for tools like Aymara AI to support responsible AI development and oversight.

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [24] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: The paper explores the potential of generative AI in urban planning, identifying gaps and proposing future research directions like theory-guided generation and human-machine co-design.


<details>
  <summary>Details</summary>
Motivation: The convergence of AI and urban planning offers opportunities for AI-driven urban design, but current research lacks integration of urban theory, multi-resolution analysis, data-augmented knowledge, and real-world interaction.

Method: The paper conceptualizes urban planning as a generative AI task, surveying approaches like VAEs, GANs, transformers, and diffusion models.

Result: Identified gaps include limited integration of urban theory, multi-resolution analysis, data-augmented knowledge, and real-world interaction.

Conclusion: Future research should focus on theory-guided generation, digital twins, and human-machine co-design to bridge the gap between generative AI and participatory urbanism.

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [25] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly is a scalable and extensible framework combining LM agents with RL, featuring token-level masking, decorator-based interfaces, and high-throughput training support.


<details>
  <summary>Details</summary>
Motivation: The combination of LM agents and RL (Agent-RL) is underexplored, lacking systematic study, prompting the development of AgentFly to bridge this gap.

Method: AgentFly integrates RL algorithms with LM agents using token-level masking, decorator-based tool and reward definitions, asynchronous execution, and centralized resource management.

Result: The framework successfully trains agents across multiple tasks, demonstrating its effectiveness and scalability.

Conclusion: AgentFly provides a robust solution for enhancing LM agents with RL, offering extensibility and high performance.

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [26] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: The paper introduces InsightX Agent, an LMM-based framework for X-ray NDT, enhancing reliability, interpretability, and interactivity in defect detection.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning methods for X-ray NDT lack interactivity, interpretability, and self-assessment, reducing reliability and operator trust.

Method: InsightX Agent uses an LMM to coordinate between SDMSD (for defect detection) and EGR (for validation), enabling active reasoning.

Result: Achieves a 96.35% F1-score on GDXray+ dataset, with improved interpretability and trustworthiness.

Conclusion: InsightX Agent demonstrates the potential of agentic LLM frameworks for industrial inspection, offering reliable and interpretable analysis.

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [27] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: LLMs show promise in autonomous decision-making but struggle with complex planning without fine-tuning. Hybrid strategies and feedback mechanisms need refinement.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' suitability in autonomous decision-making within MDPs, leveraging their pre-trained knowledge for faster adaptation compared to traditional RL.

Method: Investigates online structured prompting strategies in sequential decision-making tasks, comparing zero-shot LLM performance to classical RL methods.

Result: LLMs perform well initially in simple environments but falter in complex scenarios without fine-tuning. Feedback mechanisms can confuse LLMs, reducing performance.

Conclusion: Hybrid strategies, fine-tuning, and advanced memory integration are needed to improve LLM-based decision-making in complex settings.

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [28] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: The paper introduces the Endless Tuning method for AI deployment, focusing on human-AI collaboration, responsibility, and user experience, tested in three applications.


<details>
  <summary>Details</summary>
Motivation: Addresses the issues of human replacement and the responsibility gap in AI deployment by promoting a relational approach.

Method: Uses a double mirroring process and a protocol tested in loan granting, pneumonia diagnosis, and art style recognition with domain experts.

Result: Achieved perceived control in decision-making and a bridge between accountability and liability, emphasizing user experience over accuracy.

Conclusion: The Endless Tuning method successfully balances AI reliability with human oversight, offering a philosophical and practical framework for ethical AI deployment.

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [29] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: The paper explores the transformative potential of Agentic AI in elderly care, highlighting its benefits and ethical challenges.


<details>
  <summary>Details</summary>
Motivation: The global ageing population requires innovative care strategies, and Agentic AI offers proactive solutions.

Method: The study reviews the capabilities and applications of LLM-based Agentic AI in elderly care, addressing a literature gap.

Result: Agentic AI can enhance elderly independence but raises concerns about privacy, security, and ethical use.

Conclusion: The paper calls for ethical safeguards and further research to responsibly integrate Agentic AI in elderly care.

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [30] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: The paper explores abductive reasoning in propositional logic, focusing on facets (relevant but dispensable literals) to better understand explanation variability and complexity.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of counting and enumeration in propositional abduction by introducing facets and analyzing their role in explanation variability.

Method: Introduces facets (literals in some but not all explanations) and analyzes their properties, including distance between explanations, in various settings like Post's framework.

Result: Provides a comprehensive analysis of facets in propositional abduction, including a near-complete characterization in Post's framework.

Conclusion: Facets offer a fine-grained understanding of explanation variability, balancing computational complexity and insight in propositional abduction.

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [31] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign is a reinforcement learning framework that enhances LLM safety alignment by leveraging intrinsic safety awareness through proactive reasoning, improving refusal accuracy and reducing over-refusals without sacrificing utility.


<details>
  <summary>Details</summary>
Motivation: Current safety alignment methods for LLMs often lead to superficial refusals or require intensive supervision, failing to utilize the model's latent safety understanding.

Method: AlphaAlign uses a dual-reward RL system: a verifiable safety reward for justified refusals and a helpfulness reward for benign inputs, promoting proactive safety reasoning.

Result: AlphaAlign improves refusal accuracy, reduces over-refusals, maintains task performance, and enhances robustness to unseen threats.

Conclusion: AlphaAlign offers a simple, efficient, and deep alignment solution for LLM safety, fostering explicit safety reasoning over shallow refusal patterns.

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [32] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: A deep learning-based Forced-Choice Neural Cognitive Diagnostic Model (FCNCD) is introduced to improve personality assessments by addressing limitations of traditional models, ensuring accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Psychometric tests are crucial for personnel selection and mental health, but traditional forced-choice tests have limitations. The study aims to enhance these tests using deep learning.

Method: The FCNCD model uses multilayer neural networks to analyze interactions between participant and item features, incorporating monotonicity for interpretability.

Result: Experiments on real-world and simulated datasets confirm FCNCD's accuracy, interpretability, and robustness.

Conclusion: The FCNCD model effectively improves forced-choice tests, offering a reliable tool for psychometric assessments.

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [33] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: A novel gradient-free method using Differential Evolution (DE) optimizes adversarial prompt suffixes for RAG systems, achieving high attack success with minimal tokens and evading detection.


<details>
  <summary>Details</summary>
Motivation: Adversarial prompt attacks disrupt RAG reliability by re-ranking outputs incorrectly, necessitating robust defense strategies.

Method: DE optimizes adversarial suffixes for RAG, treating it as a black box, and uses readability-aware construction for plausibility.

Result: DE outperforms GGPP and PRADA in attack success with <=5 tokens, evades detection, and reduces readability metrics.

Conclusion: DE-based adversarial prompts are effective, stealthy, and efficient, posing challenges for RAG system security.

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [34] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: The paper introduces CAIS, a causal inference-based intrinsic reward, to improve reinforcement learning agents' robustness by isolating causal impact from noise, demonstrated in a simulated infant-mobile environment.


<details>
  <summary>Details</summary>
Motivation: Standard reinforcement learning agents struggle with noisy, ecologically valid scenarios due to reliance on correlation-based rewards, unlike human infants who robustly discover causal efficacy.

Method: CAIS measures the 1-Wasserstein distance between the learned distribution of sensory outcomes conditional on an action and the baseline outcome distribution to quantify causal influence.

Result: CAIS enables agents to filter noise, identify causal influence, and learn correct policies in scenarios where correlation-based rewards fail, and reproduces the "extinction burst" phenomenon.

Conclusion: Inferring causality explicitly is key for robust agency, providing a psychologically plausible framework for adaptive autonomous systems.

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [35] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: The paper introduces a new approach for automated planning with DL-Lite ontologies, combining explicit-input knowledge and coherence update semantics, showing no increased complexity and providing a polynomial compilation into classical planning.


<details>
  <summary>Details</summary>
Motivation: To enhance automated planning by incorporating background knowledge, such as ontologies, which are typically open-world, into planning problems.

Method: Combines ontology-based action conditions (eKABs) and ontology-aware action effects under coherence update semantics, with a polynomial compilation into classical planning.

Result: The formalism's complexity remains unchanged, and an implementation shows performance on benchmarks.

Conclusion: The approach successfully integrates ontologies into planning without added complexity, validated by benchmark evaluations.

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [36] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: CSI is an AI framework for diagnosing 118 oral diseases by mimicking expert clinician reasoning, achieving higher accuracy with hierarchical diagnostic logic.


<details>
  <summary>Details</summary>
Motivation: The challenge of diagnosing oral diseases with overlapping symptoms requires advanced AI to emulate expert clinical reasoning.

Method: CSI integrates a multimodal CLIP model with ChatGLM-6B, using a Hierarchical Diagnostic Reasoning Tree (HDRT) for structured diagnosis in Fast and Standard Modes.

Result: Fast Mode achieved 73.4% accuracy, while Standard Mode with HDRT improved to 89.5%, validated on 4,310 primary and 176 external images.

Conclusion: CSI's hierarchical reasoning significantly enhances diagnostic accuracy, demonstrating its potential as a clinically useful tool.

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [37] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover is an agent-based framework that enables general-purpose LLMs to construct formal proofs in Lean 4 without model specialization, achieving a 95.9% success rate on miniF2F-test.


<details>
  <summary>Details</summary>
Motivation: Specialized models for formal proof generation in Lean 4 are costly and inefficient. Delta Prover aims to leverage general-purpose LLMs' reasoning capabilities for this task.

Method: Delta Prover integrates reflective decomposition, iterative proof repair, and a custom DSL for subproblem management to guide LLMs in proof construction.

Result: Delta Prover achieves a 95.9% success rate on miniF2F-test, outperforming specialized models and showing strong test-time scaling.

Conclusion: General-purpose LLMs, with effective agentic guidance, can excel in theorem proving, offering a cost-efficient alternative to specialized models.

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [38] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: The paper proposes a soft evaluation indicator and a lightweight balanced neural network to enhance trust in AI-based arc fault diagnosis models by improving explainability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based arc fault diagnosis models lack trustworthiness despite high accuracy, necessitating a method to explain their outputs for reliable decision-making.

Method: The work introduces a soft evaluation indicator using Explainable AI and real experiments, alongside a lightweight balanced neural network for accuracy and feature extraction.

Result: Tested on diverse datasets, the proposed methods improve model interpretability and maintain competitive accuracy.

Conclusion: The approach makes arc fault diagnosis models more understandable and trustworthy for practitioners.

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [39] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: The paper introduces DMGC, a novel framework for unsupervised multimodal graph clustering, addressing hybrid neighborhood patterns by disentangling homophilic and heterophilic relationships.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in unsupervised learning for multimodal graphs, which integrate heterogeneous data but lack exploration in clustering.

Method: Proposes DMGC, decomposing the graph into homophily-enhanced and heterophily-aware views, using a dual-frequency fusion mechanism and self-supervised alignment.

Result: DMGC achieves state-of-the-art performance on multimodal and multi-relational graph datasets.

Conclusion: DMGC effectively addresses hybrid patterns in multimodal graphs, demonstrating strong performance and generalizability.

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [40] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: The paper introduces Cognitive Degradation as a new vulnerability in AI systems, caused by internal issues like memory starvation and context flooding. It proposes the Qorvex Security AI Framework (QSAF Domain 10) for real-time monitoring and mitigation.


<details>
  <summary>Details</summary>
Motivation: Traditional adversarial threats focus on external attacks, but internal cognitive degradation in AI systems leads to silent failures like logic collapse and hallucinations. Addressing this gap is critical for resilient AI behavior.

Method: The QSAF framework includes a six-stage cognitive degradation lifecycle and seven runtime controls (QSAF-BC-001 to BC-007) for real-time monitoring and mitigation, inspired by cognitive neuroscience.

Result: The framework enables early detection of issues like fatigue and starvation, with proactive mitigation through fallback routing and memory integrity enforcement.

Conclusion: This work establishes Cognitive Degradation as a new AI vulnerability class and introduces the first cross-platform defense model for resilient agentic behavior.

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [41] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD combines retrieval and diffusion models to improve offline RL by dynamically targeting high-return states and planning with a diffusion model, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Offline RL struggles with dataset sparsity and lack of transition overlap, limiting long-horizon planning. Prior methods fail to generalize or rely on heuristics.

Method: RAD retrieves high-return states based on similarity and return estimation, then plans toward them using a condition-guided diffusion model.

Result: RAD achieves competitive or superior performance across diverse benchmarks.

Conclusion: RAD effectively addresses generalization and planning challenges in offline RL through retrieval-guided diffusion modeling.

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [42] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: An end-to-end model for object-centric predictive process monitoring, using graph attention and LSTM networks, shows competitive performance in predicting next activities and event times.


<details>
  <summary>Details</summary>
Motivation: To enhance process predictions by leveraging object-centric event logs, addressing the challenge of extracting relevant information and building effective models.

Method: Combines a graph attention network for encoding activities and relationships with an LSTM network to handle temporal dependencies.

Result: Demonstrates competitive performance on one real-life and three synthetic event logs compared to state-of-the-art methods.

Conclusion: The proposed model effectively predicts future process behavior, excelling in next activity and event time prediction tasks.

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [43] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: The paper proposes a Pareto optimization approach to discover optimal batching policies in business processes, balancing waiting time, cost, and processing effort using intervention heuristics and meta-heuristics.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between cost, processing effort, and waiting time in activity batching, aiming for optimal batching policies.

Method: Uses intervention heuristics to improve batching policies, evaluated via simulation, and embeds these in meta-heuristics (hill-climbing, simulated annealing, reinforcement learning) to update the Pareto front.

Result: Experimental evaluation compares heuristic-guided meta-heuristics against baselines, assessing convergence, diversity, and cycle time gain of Pareto-optimal policies.

Conclusion: The approach effectively discovers optimal batching policies, with heuristic-guided meta-heuristics outperforming non-heuristic baselines.

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [44] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1 introduces a chart-domain vision-language model with reinforcement learning fine-tuning for complex chart reasoning, using programmatic data synthesis and a two-stage training strategy (Chart-COT and Chart-RFT). It outperforms existing methods and rivals large-scale models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To extend R1-Style methods beyond mathematical reasoning and code intelligence to multimodal data, specifically charts, which pose unique reasoning challenges.

Method: 1. Programmatic data synthesis for high-quality chart reasoning data. 2. Two-stage training: Chart-COT (step-by-step supervision) and Chart-RFT (numerically sensitive reinforcement fine-tuning).

Result: Chart-R1 outperforms chart-domain methods and competes with large-scale models like GPT-4o and Claude-3.5.

Conclusion: Chart-R1 successfully addresses chart reasoning challenges, demonstrating the effectiveness of reinforcement learning fine-tuning in multimodal domains.

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [45] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: The paper investigates whether LLMs use internal world models or rely on statistical associations by testing them on pulley system problems. Results suggest LLMs can approximate mechanical advantage using heuristics but struggle with nuanced reasoning.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs construct internal world models or rely on statistical associations, using cognitive science methods to test their understanding of pulley systems.

Method: Three studies were conducted: Study 1 tested mechanical advantage estimation, Study 2 probed global feature representation, and Study 3 examined nuanced structural connectivity reasoning.

Result: LLMs performed above chance in estimating mechanical advantage (Study 1) and differentiating functional from jumbled systems (Study 2), but struggled with nuanced connectivity (Study 3).

Conclusion: LLMs may manipulate internal world models for basic tasks but lack nuanced reasoning. Cognitive science methods are useful for evaluating AI world-modeling capacities.

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [46] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: The paper introduces methods to improve data efficiency in Safe Policy Improvement (SPI) by leveraging parametric dependencies in MDPs, using a parametric SPI algorithm and advanced preprocessing techniques.


<details>
  <summary>Details</summary>
Motivation: To enhance the data efficiency of SPI in offline reinforcement learning by utilizing known parametric dependencies in transition dynamics.

Method: 1) A parametric SPI algorithm exploiting correlations for better transition dynamics estimation. 2) Game-based preprocessing to prune redundant actions. 3) SMT-based preprocessing for further action pruning.

Result: Empirical results show orders of magnitude improvement in data efficiency while maintaining reliability.

Conclusion: The proposed techniques significantly boost SPI's data efficiency without compromising reliability.

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [47] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: The paper evaluates metrics for assessing LLM capabilities using MCQs, highlighting answer fluctuation issues and proposing a new metric, worst accuracy, which shows strong performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of thorough assessment of metrics for evaluating LLMs via MCQs and the issue of answer fluctuation due to prompt variations.

Method: Proposes a metric assessment protocol analyzing evaluation methodologies based on their connection with fluctuation rates and original performance.

Result: Existing metrics strongly correlate with answer fluctuation. The novel metric, worst accuracy, performs best in the protocol.

Conclusion: Worst accuracy is a promising metric for evaluating LLMs via MCQs due to its strong association with answer fluctuation.

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [48] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: An adapter-based method for tactical conditioning of StarCraft II AI agents, enabling flexible strategy adaptation with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Current AI agents lack adaptability to high-level tactical directives, limiting strategic customization.

Method: Freezes a pre-trained policy network (DI-Star) and attaches lightweight adapters to action heads, conditioned on a tactical tensor. Training uses KL divergence constraints.

Result: Successfully modulates agent behavior (aggression, expansion, tech preferences) while maintaining performance.

Conclusion: Offers practical, flexible tactical control for real-time strategy games with low computational cost.

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [49] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: Agentic AI can autonomously detect and respond to anomalies in complex systems, reducing reliance on human intervention.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly management by leveraging AI to reduce human dependency in complex systems.

Method: Utilizes agentic AI for autonomous anomaly detection and response.

Result: Demonstrates the potential of agentic AI to transform traditional anomaly management.

Conclusion: Agentic AI offers a promising approach for efficient and autonomous anomaly handling in complex systems.

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [50] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: The paper proposes g-AMIE, a multi-agent AI system for medical history taking under guardrails, with asynchronous oversight by physicians. It outperformed NPs/PAs and PCPs in a virtual OSCE, showing promise for real-world diagnostic AI under human oversight.


<details>
  <summary>Details</summary>
Motivation: To address the need for patient safety and regulatory compliance in AI-driven diagnostic dialogue, while leveraging physician oversight for accountability.

Method: Proposed g-AMIE, a guardrailed AI system for history taking, abstaining from direct advice. Evaluated via a randomized, blinded virtual OSCE comparing g-AMIE to NPs/PAs and PCPs.

Result: g-AMIE outperformed NPs/PAs and PCPs in intake quality, case summaries, and proposed diagnoses/plans, leading to higher composite decision quality and time efficiency.

Conclusion: Asynchronous oversight is a feasible paradigm for diagnostic AI, enhancing care under expert human oversight, though clinical replication is needed.

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [51] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO is a framework that optimizes reasoning length in models, reducing token usage by 40.9% and improving accuracy by 2.3%.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models generate excessive tokens for simple problems, needing a solution to internalize reasoning depth control.

Method: LAPO uses a two-stage reinforcement learning process: first learning natural reasoning patterns, then embedding them as meta-cognitive guidance.

Result: Reduces token usage by 40.9% and improves accuracy by 2.3% on mathematical reasoning benchmarks.

Conclusion: LAPO enables models to efficiently allocate computational resources without sacrificing reasoning quality.

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [52] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent is a multi-agent system for optimizing Gas waste in smart contracts, combining compatibility with existing patterns and automated discovery/validation of new patterns, achieving significant Gas savings.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for Gas waste in smart contracts are inefficient, costly, and hard to scale, while LLM-based approaches struggle with compatibility and redundancy.

Method: GasAgent uses four specialized agents (Seeker, Innovator, Executor, Manager) to collaboratively identify, validate, and apply Gas-saving improvements in a closed loop.

Result: GasAgent optimized 82 out of 100 real-world contracts with 9.97% average Gas savings and 79.8% of 500 LLM-generated contracts with savings from 4.79% to 13.93%.

Conclusion: GasAgent effectively addresses the limitations of existing methods, proving its usability as an optimization layer for LLM-assisted smart contract development.

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [53] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: The paper introduces EAMI, a framework for dynamic and interpretable emergence analysis in complex service ecosystems using multi-agent intention tracking and clustering.


<details>
  <summary>Details</summary>
Motivation: The complexity of service ecosystems and limitations of traditional causal methods necessitate a new approach for analyzing abnormal emergence in multi-agent interactions.

Method: EAMI uses a dual-perspective thought track mechanism (Inspector and Analysis Agents) to extract intentions, k-means clustering for phase transitions, and an Intention Temporal Emergence diagram for dynamic analysis.

Result: Validated in O2O service systems and Stanford AI Town, EAMI shows effectiveness, generalizability, and efficiency in analyzing emergence.

Conclusion: EAMI offers a novel paradigm for abnormal emergence and causal analysis in service ecosystems, with code publicly available.

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [54] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: The paper analyzes the challenges of aligning Federated Learning (FL) with Trustworthy AI (TAI) requirements, focusing on privacy and distributed nature issues.


<details>
  <summary>Details</summary>
Motivation: To address the gap in adapting FL to TAI frameworks, ensuring AI systems meet ethical, legal, and technical standards.

Method: Systematic analysis of FL challenges using TAI requirements as a guiding structure, classifying obstacles and reviewing trends.

Result: Identified key challenges in aligning FL with TAI, highlighting progress and remaining gaps.

Conclusion: FL's distributed nature complicates TAI alignment, requiring further research to bridge gaps.

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [55] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: The paper addresses identifying conditional causal effects in a graph known up to a maximally oriented partially directed acyclic graph (MPDAG). It provides an identification formula, a generalization of do calculus, and a complete algorithm for this setting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend causal effect identification methods to cases where the causal graph is only partially known (MPDAG), ensuring robustness in real-world scenarios with limited background knowledge.

Method: The method involves deriving an identification formula for conditional effects when the conditioning set is treatment-unaffected, generalizing do calculus for MPDAGs, and developing a complete algorithm for identification.

Result: The results include a new identification formula, a generalized do calculus, and a provably complete algorithm for identifying conditional causal effects in MPDAGs.

Conclusion: The paper concludes that the proposed methods enable reliable causal effect identification in partially known graphs, advancing causal inference in practical settings.

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [56] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO is a reinforcement learning framework that optimizes reasoning efficiency in large models by learning problem-specific reasoning depths, reducing token usage by 60.6% while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models use uniform reasoning strategies regardless of problem complexity, leading to computational inefficiency. HBPO aims to address this by enabling adaptive reasoning depths.

Method: HBPO uses hierarchical budget exploration, partitioning samples into subgroups with distinct token budgets, and introduces differentiated reward mechanisms for budget-aware incentives.

Result: HBPO reduces average token usage by up to 60.6% and improves accuracy by 3.14% across four reasoning benchmarks.

Conclusion: Reasoning efficiency and capability can coexist; HBPO's hierarchical training preserves exploration diversity while optimizing both.

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [57] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: LLMs exhibit human-like temporal cognition, adhering to the Weber-Fechner law and forming subjective temporal reference points. Analysis reveals mechanisms at neuronal, representational, and informational levels, suggesting an experientialist perspective for AI alignment.


<details>
  <summary>Details</summary>
Motivation: To investigate spontaneous human-like temporal cognition in LLMs, uncovering underlying mechanisms and implications for AI alignment.

Method: Used similarity judgment tasks, analyzed temporal-preferential neurons, hierarchical representations, and training corpus temporal structure.

Result: Larger models form subjective temporal reference points, follow logarithmic compression, and exhibit hierarchical temporal representation construction.

Conclusion: LLMs' cognition is a subjective construction, suggesting alien cognitive frameworks and guiding AI alignment via internal representational systems.

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [58] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: Gemini 2.5 Pro solves 5 out of 6 IMO 2025 problems using pipeline design and prompt engineering, showcasing effective model utilization.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with Olympiad-level math problems despite performing well on other benchmarks, prompting exploration of their capabilities on IMO tasks.

Method: Used Google's Gemini 2.5 Pro on IMO 2025 problems with pipeline design and prompt engineering, avoiding data contamination.

Result: 5 out of 6 problems were solved correctly, demonstrating the model's potential with optimal usage.

Conclusion: Effective pipeline and prompt design are crucial for leveraging powerful models like Gemini 2.5 Pro on challenging tasks like the IMO.

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: Catalyst regularization introduces a novel method for structured pruning in deep neural networks, ensuring fair and robust pruning decisions with wide-margin bifurcation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning methods like L1 or Group Lasso are biased toward small-magnitude filters and lack robustness due to narrow decision margins.

Method: The paper proposes Catalyst regularization, using auxiliary variables to ensure fair pruning and wide-margin bifurcation, preserving model performance.

Result: Empirical results show Catalyst Pruning outperforms state-of-the-art methods, confirming its robustness and fairness.

Conclusion: Catalyst regularization provides a theoretically sound and effective solution for structured pruning, addressing biases and robustness issues in traditional methods.

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [60] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: The paper introduces a novel pruning strategy, IPPRO, that challenges the dominance of magnitude in importance-based pruning by using projective space and gradient descent movement to measure filter importance.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of magnitude-based pruning, where larger filters are rarely pruned even if redundant, by providing a fair chance for all filters to be pruned.

Method: Proposes IPPRO, a magnitude-indifferent pruning method using projective space and gradient descent movement to measure filter importance (PROscore).

Result: Achieves near-lossless pruning with minimal performance drop and promising post-finetuning results, debunking the 'size-matters' myth.

Conclusion: The work expands the frontier of importance-based pruning by introducing a theoretically and empirically validated magnitude-indifferent approach.

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [61] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR integrates language models into a self-improving evolutionary loop for program synthesis, achieving significant gains on the ARC-AGI benchmark.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art language models struggle with complex program synthesis tasks, and evolutionary methods are limited by fixed generative model capabilities.

Method: SOAR alternates between evolutionary search (using LLMs to sample/refine solutions) and hindsight learning (fine-tuning the LLM with problem-solution pairs).

Result: SOAR achieves notable performance improvements on ARC-AGI, solving 52% of the public test set.

Conclusion: SOAR demonstrates the effectiveness of combining evolutionary search with LLM fine-tuning for scalable program synthesis.

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [62] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: The paper evaluates intermediate (latent space) fusion for predicting depressive symptoms, showing it outperforms traditional methods like Random Forest and Linear Regression.


<details>
  <summary>Details</summary>
Motivation: Improving early detection and personalized intervention for mental illnesses like depression and anxiety by leveraging advanced multimodal data integration techniques.

Method: Compared early fusion (Random Forest) and intermediate fusion (Combined Model with autoencoders and neural network) using behavioral, demographic, and clinical data from the BRIGHTEN trial. Performance was measured via MSE and R2.

Result: The Combined Model outperformed baselines with lower MSE (0.4985 vs. 0.5305) and higher R2 (0.4695 vs. 0.4356), showing better generalization and capturing non-linear interactions.

Conclusion: Latent space fusion is superior for multimodal mental health data prediction; future work should focus on interpretability and clinical deployment.

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [63] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: The paper introduces Predictive Representativity (PR) to audit AI fairness, focusing on outcome equity rather than dataset composition. It reveals performance disparities in skin cancer classifiers for darker skin tones and proposes a framework for fairness generalization.


<details>
  <summary>Details</summary>
Motivation: Address concerns about algorithmic bias and inequitable outcomes in AI-driven medical decisions, especially for marginalized groups.

Method: Evaluated AI skin cancer classifiers on HAM10000 and BOSQUE datasets, analyzing performance by skin phototype. Proposed PR framework and External Transportability Criterion.

Result: Found significant performance disparities for darker skin tones despite proportional dataset sampling.

Conclusion: Advocates for post-hoc fairness auditing, transparent documentation, and inclusive validation to ensure equity in AI healthcare systems.

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [64] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: The paper generalizes the ski-rental problem to a multi-agent setting with individual and shared costs, introducing dynamic states and three competitive ratios. It designs optimal deterministic and randomized policies, showing symmetric policies outperform asymmetric ones.


<details>
  <summary>Details</summary>
Motivation: To extend the classical ski-rental problem to group settings where agents face individual and shared costs, addressing dynamic states and diverse objectives.

Method: Defines three competitive ratios (overall, state-dependent, individual rational) and designs deterministic (state-aware thresholds) and randomized (state-aware distributions) policies.

Result: Symmetric policies outperform asymmetric ones, with competitive ratio bounds provided, extending classical insights to multi-agent scenarios.

Conclusion: The study offers theoretical and practical insights for group decision-making under uncertainty, with implications for multi-agent cost-sharing problems.

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [65] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: The paper analyzes the training solutions of two-layer neural networks with smooth activation functions, using four principles, and proves universal approximation.


<details>
  <summary>Details</summary>
Motivation: To demystify the 'black box' of neural network solutions and enrich approximation theory.

Method: Uses Taylor series expansions, strict partial order of knots, smooth-spline implementation, and smooth-continuity restriction.

Result: Proves universal approximation and provides experimental verification.

Conclusion: The study reveals insights into neural network solutions and advances approximation theory.

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [66] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: The paper proposes Feature Bank Enhancement (FBE) to improve OOD detection by addressing biased feature distributions in distance-based methods, achieving state-of-the-art results on ImageNet-1k and CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: Distance-based OOD detection methods are limited by biased feature distributions and extreme features, causing low scores for in-distribution samples.

Method: FBE uses dataset statistics to identify and constrain extreme features, enhancing separation between in-distribution and OOD samples.

Result: FBE achieves state-of-the-art performance on ImageNet-1k and CIFAR-10 benchmarks.

Conclusion: FBE effectively addresses limitations of distance-based OOD detection, improving reliability and performance.

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [67] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: The paper proposes a clustering-based framework to efficiently predict and utilize activation sparsity in LLMs, reducing computational costs while maintaining model quality.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) exhibit activation sparsity, offering opportunities to reduce computational costs, but predicting neuron-level activation patterns is expensive due to the vast number of neurons.

Method: A clustering-based activation pattern compression framework groups similar activation patterns into representative clusters, enabling efficient prediction and utilization of sparsity.

Result: The method achieves 79.34% clustering precision and a perplexity (PPL) score as low as 12.49, balancing computational efficiency and model quality.

Conclusion: The clustering approach effectively captures activation structures and improves sparse computation efficiency, providing a foundation for future work on activation pattern prediction in LLMs.

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [68] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: A robust and explainable DL-based beam alignment engine (BAE) for mmWave MIMO systems is proposed, reducing data needs and beam training overhead while improving transparency and robustness.


<details>
  <summary>Details</summary>
Motivation: Address challenges in DL-based beam alignment, such as high data collection overhead, lack of explainability, and susceptibility to adversarial attacks, to build trust and reliability in mmWave systems.

Method: Uses RSSI measurements from wide beams, a site-specific digital twin for synthetic data, transfer learning for model refinement, SHAP for feature importance, and DkNN for credibility metrics.

Result: Reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by 8.5x, achieving near-optimal spectral efficiency.

Conclusion: The framework enhances transparency, reduces overhead, and ensures robust performance, making it suitable for AI-native 6G mmWave systems.

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [69] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: The paper proposes SSFL-DCSL, a semi-supervised federated learning framework with dual contrastive loss and soft labeling, to address data and label scarcity in distributed clients while ensuring privacy. It improves accuracy over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised deep learning requires large labeled datasets, which are costly and often distributed across clients. Data distribution differences also hinder performance.

Method: SSFL-DCSL integrates dual contrastive loss and soft labeling, uses a sample weighting function for pseudo-label bias, and aggregates local prototypes for knowledge sharing.

Result: In experiments with 10% labeled data, SSFL-DCSL improves accuracy by 1.15% to 7.85% over existing methods.

Conclusion: SSFL-DCSL effectively addresses data and label scarcity in distributed settings, enhancing model performance while preserving privacy.

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [70] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: The paper introduces the B4 model to analyze bull and bear regimes in financial markets, combining price data and external signals to predict trends and explain bias-driven behaviors.


<details>
  <summary>Details</summary>
Motivation: Financial markets are influenced by diverse biases and external narratives, making dynamics hard to model. The paper aims to explore bull and bear regimes to improve trend prediction.

Method: Proposes the B4 model, embedding price sequences and contextual signals into a shared latent space. Uses inertial pairing and dual competition to capture behavioral divergence.

Result: B4 outperforms in trend prediction and offers interpretable insights into biases and market dynamics.

Conclusion: The B4 model effectively captures market heterogeneity and bias-driven behaviors, enhancing prediction and interpretability.

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [71] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache is a training-free KV cache optimization method for LLMs, enhancing long-range capabilities and continuous generation without OOM.


<details>
  <summary>Details</summary>
Motivation: Addressing efficiency bottlenecks in LLMs due to increasing KV pairs with longer sequences.

Method: LaCache uses a ladder-shaped KV cache pattern and iterative compaction for dynamic compression.

Result: Validated effectiveness across tasks, benchmarks, and LLM models.

Conclusion: LaCache improves LLMs' long-range capabilities and continuous generation under fixed cache budgets.

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [72] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: A deep learning system for an accessibility device for the deaf or hearing impaired, featuring sound localization and identification in real time using custom CNN, CLAP model, and multimodal integration.


<details>
  <summary>Details</summary>
Motivation: To address the gap in accessibility devices for the deaf or hearing impaired by leveraging machine learning for accurate sound localization and identification.

Method: 1. JerryNet (CNN for direction of arrival). 2. CLAP model for audio classification. 3. Multimodal integration (audio, visual, text) with Yolov9 and CIoU for sound localization.

Result: JerryNet: 91.1% precision for sound direction. CLAP: 98.5% (custom) and 95% (AudioSet) accuracy. Audio-visual localization: CIoU 0.892, AUC 0.658.

Conclusion: The system shows high accuracy and potential for future accessibility devices, paving the way for innovative solutions.

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [73] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: Proposes an interactive learning framework for pattern mining, combining nonlinear utility aggregation and geometry-aware query selection to address pattern explosion.


<details>
  <summary>Details</summary>
Motivation: To solve the pattern explosion problem by modeling user preferences and efficiently selecting informative comparisons.

Method: Uses a Choquet integral for utility aggregation and a branch-and-bound strategy with tight distance bounds for query selection.

Result: Outperforms ChoquetRank, achieving better ranking accuracy with fewer user interactions on UCI datasets.

Conclusion: The framework effectively addresses pattern explosion and improves efficiency in pattern mining.

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [74] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: An AI framework using SHAP values identifies optimal green hydrogen production sites by analyzing environmental, atmospheric, and infrastructural factors, achieving 98% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of direct hydrogen yield data and the need for sustainable alternatives to fossil fuels, especially in solar-rich regions.

Method: A multi-stage AI pipeline combining unsupervised clustering, supervised machine learning, and SHAP analysis on integrated meteorological, topographic, and temporal data.

Result: Identified water proximity, elevation, and seasonal variation as key factors for site suitability in Oman, with high predictive accuracy.

Conclusion: Provides a scalable, objective tool for green hydrogen infrastructure planning in data-scarce regions, replacing subjective expert judgments.

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [75] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: Proposes POGM for gradient-based domain generalization, addressing gradient fluctuations and computational inefficiency by leveraging gradient trajectories and meta-learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with gradient fluctuations and high computation costs in domain generalization.

Method: POGM uses gradient trajectories for meta-learning, maximizing GIP while aligning with empirical risk minimization gradients.

Result: POGM achieves competitive performance on DomainBed datasets with computational efficiency.

Conclusion: POGM effectively balances gradient matching and computational feasibility for domain generalization.

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [76] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoPro-3M is the largest dataset for nanomaterial-protein interactions, enabling NanoProFormer, a multimodal model that outperforms single-modality approaches and generalizes well for diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding nanomaterial-protein interactions is critical for medicine and environmental science, but progress is limited by small datasets and poor model generalizability.

Method: Developed NanoPro-3M (3.2M samples, 37K proteins) and NanoProFormer, a multimodal model for predicting affinities, handling missing data, and generalizing to unseen cases.

Result: Multimodal modeling outperforms single-modality, identifies corona formation determinants, and excels in zero-shot inference and fine-tuning.

Conclusion: This work provides a foundation for high-performance, generalized prediction of nanomaterial-protein interactions, reducing experimental reliance and accelerating applications.

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [77] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM is a linear dimensionality reduction method that combines diffusion-map intuition with linear efficiency, outperforming PCA in manifold-structured datasets.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between nonlinear diffusion-based methods and linear techniques like PCA, offering geometric insights with computational simplicity.

Method: LDM approximates the diffusion-map kernel linearly, tested on synthetic (Swiss roll, hyperspheres) and real-world datasets (MNIST, COIL-20).

Result: LDM excels in manifold-structured data, while PCA is better for variance/noise-dominated cases. LDM's kernel allows NMF for interpretable analysis.

Conclusion: LDM is a promising linear dimensionality reduction tool with theoretical and practical potential.

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [78] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: Training Large Reasoning Models (LRMs) with multi-turn Reinforcement Learning (RL) using unary feedback (UFO) improves both single-turn performance and multi-turn reasoning accuracy by up to 14%.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods train LRMs in a single-turn paradigm, causing them to struggle with multi-turn problem solving and repetitive responses. The goal is to enhance LRMs' ability to reflect and revise answers based on feedback.

Method: Introduces Unary Feedback as Observation (UFO), a minimal feedback method for RL, and designs reward structures to encourage diverse reasoning and careful answers in each turn.

Result: UFO improves multi-turn reasoning accuracy by up to 14% while maintaining single-turn performance.

Conclusion: Multi-turn RL with UFO enhances LRMs' ability to react to feedback and solve problems iteratively, offering a practical improvement over single-turn training.

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [79] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist is a meta-learning framework for FL that dynamically selects aggregation rules to counter model poisoning, outperforming static defenses in diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: FL's decentralized nature makes it vulnerable to model poisoning, and static defenses often fail against adaptive adversaries or heterogeneous data.

Method: FedStrategist uses a contextual bandit agent to dynamically choose the best aggregation rule from a set of defenses based on real-time metrics.

Result: No single static rule is universally optimal; FedStrategist learns superior policies, even in challenging scenarios like stealth attacks.

Conclusion: FedStrategist offers a practical, controllable approach to balancing performance and security in decentralized AI systems.

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [80] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: The paper addresses the gap in individual fairness in deepfake detection, proposing a framework to enhance fairness without compromising detection performance.


<details>
  <summary>Details</summary>
Motivation: The misuse of generative AI for deepfakes poses risks, and existing detection methods lack fairness, especially at the individual level.

Method: The authors propose a generalizable framework to improve individual fairness in deepfake detection, integrating it into existing detectors.

Result: Experiments show the framework significantly enhances individual fairness while maintaining robust detection, outperforming state-of-the-art methods.

Conclusion: The work fills a critical gap in deepfake detection fairness, offering a practical solution with demonstrated effectiveness.

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [81] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: The paper explores ML models for predicting CHF in annular geometries, outperforming traditional empirical correlations with significantly lower errors.


<details>
  <summary>Details</summary>
Motivation: Accurate CHF prediction is crucial for reactor safety, but traditional methods lack interpretability and resilience to data scarcity, especially for annular geometries.

Method: Four ML models were developed and validated using CTF subchannel code, trained on 577 experimental annulus data points, and compared to three empirical correlations (Biasi, Bowring, Katto).

Result: ML models achieved mean relative errors below 3.5%, vastly outperforming empirical correlations with errors above 26%.

Conclusion: Hybrid ML models are superior to empirical methods for CHF prediction in annular geometries, offering higher accuracy and reliability.

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [82] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: The paper explores using influence functions to filter noisy datasets in language model fine-tuning, showing a 1.5% accuracy improvement after pruning 10% of harmful examples. Gradient similarity outperforms influence functions for identifying helpful examples.


<details>
  <summary>Details</summary>
Motivation: Human preference datasets for fine-tuning language models are often noisy, and small post-training datasets make it feasible to use influence functions to identify and remove harmful examples.

Method: The study adapts the TL;DR dataset for reward model training and employs conjugate-gradient approximated influence functions to filter datasets. Gradient similarity is also evaluated for comparison.

Result: Influence function filtering improves retraining accuracy by 1.5% after removing 10% of harmful examples. Gradient similarity is more effective than influence functions for detecting helpful examples.

Conclusion: Local curvature (influence functions) is crucial for identifying harmful examples, while gradient similarity is better for helpful ones, highlighting the need for tailored filtering methods.

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [83] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection is a PEFT method that adapts decoder-block representations, outperforming LoRA with fewer parameters and inspired by homotopy theory.


<details>
  <summary>Details</summary>
Motivation: To improve task adaptation in LLMs by focusing on decoder-block level adjustments rather than individual weight matrices, inspired by homotopy theory.

Method: Introduces Solo Connection, a trainable linear transformation for smooth adaptation between zero and task-specific representations, using long skip connections across decoder blocks.

Result: Outperforms LoRA on E2E benchmarks, reduces trainable parameters by 59% vs LoRA and 99% vs full fine-tuning.

Conclusion: Solo Connection offers efficient, stable adaptation for LLMs, especially beneficial for larger architectures with many decoder blocks.

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [84] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET is a novel framework for real-time cyberattack detection using incremental causal graph learning, outperforming traditional methods in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: The increasing threat of cyberattacks on critical infrastructures requires detection methods that adapt to evolving attack patterns and address limitations of static approaches.

Method: INCADET dynamically updates causal graphs in real-time using three modules: early symptom detection, incremental causal graph learning, and causal graph classification with GCNs.

Result: Experiments show INCADET achieves superior accuracy, robustness, and adaptability in evolving attack scenarios.

Conclusion: INCADET effectively addresses the challenges of real-time cyberattack detection by leveraging incremental causal graph learning.

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [85] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: Simple test-time scaling behavior in models is mainly due to scaling down by enforcing a maximum length, not scaling up. Fine-tuning on long CoT data has little effect, and scaling up via "Wait" causes inconsistencies. o1-like models naturally scale up compute, outperforming simple scaling methods.


<details>
  <summary>Details</summary>
Motivation: To analyze the effectiveness and limitations of simple test-time scaling methods in replicating the scaling behavior of o1-like models.

Method: Examined scaling down by enforcing maximum length and scaling up by appending "Wait," comparing these with o1-like models' natural scaling behavior.

Result: Scaling down dominates the observed behavior, while scaling up leads to inconsistencies. o1-like models outperform simple scaling methods by naturally scaling compute.

Conclusion: Simple test-time scaling can mimic scaling behavior but fails to unlock higher performance. The goal should be enabling models to surpass original limits, not just replicate scaling.

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [86] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: The paper proposes using reinforcement learning (RL) with intervention models to solve large-scale stochastic optimization problems, demonstrated on a supply chain inventory management problem.


<details>
  <summary>Details</summary>
Motivation: The motivation is to efficiently explore solution spaces in stochastic optimization by leveraging pre-trained deep learning models, avoiding direct modeling of complex constraints.

Method: The method involves simulating stochastic processes using deep RL models and introducing a constraint coordination mechanism for forecasting dual costs.

Result: The approach improves performance on large real-world datasets by breaking down supply chain processes into scalable DL modules.

Conclusion: The paper concludes with open problems for future research to further validate the efficacy of such models.

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [87] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC, a reparameterized masked diffusion model, improves structured node classification by addressing label correlations in graphs, outperforming existing methods in scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing GNN methods assume conditional independence of node labels, ignoring their inherent correlations. ReDiSC aims to model these dependencies for better structured predictions.

Method: ReDiSC uses a reparameterized masked diffusion model to estimate joint label distributions, trained via variational EM. It links to GNN and label propagation hybrid approaches.

Result: ReDiSC outperforms state-of-the-art methods in performance and scalability, especially on large datasets where others fail due to computational limits.

Conclusion: ReDiSC effectively addresses label correlations in graphs, offering a scalable and superior solution for structured node classification.

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [88] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: The paper introduces a Federated Reinforcement Learning (FRL) framework for heterogeneous environments (FRL-EH), proposing a novel global objective function and algorithm (FedRQ) with proven convergence. It extends to continuous state spaces and outperforms existing FRL methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning a robust global policy in federated settings with statistically heterogeneous local environments while preserving privacy.

Method: Proposes FedRQ, a tabular FRL algorithm, and extends it to continuous state spaces using expectile loss. Validates through empirical evaluations.

Result: FedRQ achieves superior performance and robustness across diverse heterogeneous environments compared to state-of-the-art FRL algorithms.

Conclusion: The FRL-EH framework and FedRQ algorithm effectively optimize global policies in heterogeneous settings, with theoretical and empirical validation.

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [89] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: The paper identifies 'glitches'—small input neighborhoods causing abrupt output oscillations—as a new source of unreliability in AI models, particularly those with steep decision boundaries. It formalizes glitches, demonstrates their prevalence, and presents an NP-complete glitch-detection algorithm for GBDT models using MILP encoding.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability and inconsistency of AI model outputs, especially in critical decision-making tasks, by identifying and mitigating glitches.

Method: Formal definition of glitches, empirical demonstration of their prevalence, and development of an MILP-based algorithm for detecting glitches in GBDT models.

Result: Glitches are widespread and indicate model inconsistencies. The glitch-detection problem for tree ensembles is NP-complete, and the proposed algorithm is effective and feasible.

Conclusion: Glitches are a significant reliability issue in AI models, and the proposed method provides a practical solution for detecting them in GBDT models.

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [90] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: The paper introduces Generative Distribution Distillation (GenDD) for knowledge distillation, addressing high-dimensional optimization and lack of label supervision with Split Tokenization and Distribution Contraction. It achieves competitive results, outperforming baselines significantly.


<details>
  <summary>Details</summary>
Motivation: To improve knowledge distillation by treating it as a conditional generative problem, addressing challenges like high-dimensional optimization and lack of semantic supervision.

Method: Proposes GenDD with Split Tokenization for stable unsupervised KD and Distribution Contraction to integrate label supervision. Theoretical proof links it to multi-task learning.

Result: GenDD outperforms KL baseline by 16.29% on ImageNet in unsupervised settings. With supervision, ResNet-50 achieves 82.28% top-1 accuracy, setting a new SOTA.

Conclusion: GenDD effectively addresses KD challenges, achieving state-of-the-art performance in both supervised and unsupervised settings.

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [91] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: Proposes SDSC, a structure-aware metric for time series SSL, addressing limitations of distance-based methods like MSE. Combines with MSE for hybrid loss, showing improved performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Distance-based SSL objectives (e.g., MSE) lack structural sensitivity, hindering semantic alignment and interpretability in signal representations.

Method: SDSC quantifies structural agreement via signed amplitude intersection, derived from DSC. Used as a loss with a differentiable Heaviside approximation. Hybrid loss combines SDSC and MSE.

Result: SDSC-based pre-training matches or outperforms MSE, especially in in-domain and low-resource settings, enhancing semantic representation quality.

Conclusion: Structure-aware metrics like SDSC are viable alternatives to distance-based methods, improving signal representation fidelity.

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [92] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: The paper proposes using positive-unlabeled (PU) learning to identify control units in observational studies where labeled controls are absent, enabling unbiased average treatment effect (ATE) estimation.


<details>
  <summary>Details</summary>
Motivation: The challenge of missing clearly labeled control units in observational studies motivates the need for a method to reliably identify controls from unlabeled data.

Method: PU learning is employed to identify control units using only treated units. The approach is tested on simulated and real-world agricultural data.

Result: PU learning successfully identifies control units and estimates ATE close to the true value, validated through simulations and real-world applications.

Conclusion: This method enhances observational causal inference, especially in fields like agriculture and environmental sciences, where randomized experiments are impractical.

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [93] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: The paper proposes a maximum causal entropy inverse reinforcement learning method for infinite-horizon mean-field games, using a reproducing kernel Hilbert space for nonlinear reward inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods for mean-field games often limit reward functions to linear combinations of basis functions and focus on finite-horizon settings, lacking flexibility and scalability.

Method: A Lagrangian relaxation transforms the problem into unconstrained log-likelihood maximization, solved via gradient ascent. Theoretical consistency is shown through Fréchet differentiability of soft Bellman operators.

Result: The method successfully recovers expert behavior in a mean-field traffic routing game, demonstrating its effectiveness.

Conclusion: The approach enables rich, nonlinear reward inference in infinite-horizon mean-field games, outperforming traditional linear methods.

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [94] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: The paper connects self-attention in Transformers to a broader affinity-based computation principle, highlighting Infinite Feature Selection (Inf-FS) as a foundational approach.


<details>
  <summary>Details</summary>
Motivation: To unify self-attention mechanisms across domains by tracing their origins to affinity matrices and generalizing the concept through Inf-FS.

Method: Comparative analysis of self-attention and Inf-FS, focusing on affinity matrix definitions and applications.

Result: Self-attention is a special case of Inf-FS, with differences in affinity matrix construction and usage.

Conclusion: The paper unifies diverse models under a common affinity-based computation framework, emphasizing shared mathematical foundations.

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [95] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN is a scalable, efficient GNN framework for large-scale graphs, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs struggle with efficiency and accuracy due to computational demands and neighbor explosion in large graphs.

Method: Introduces LPS-GNN with LPMetis partitioning and subgraph augmentation for better performance and compatibility.

Result: Achieves 13.8% improvement in User Acquisition, 8.24%-13.89% over SOTA models, and handles 100B graphs with a single GPU in 10 hours.

Conclusion: LPS-GNN is a scalable, efficient solution for large-scale graph tasks, validated on real-world datasets.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [96] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: A novel framework combining Transformer-based GAN and MILET for UAV flight state classification achieves high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional TSC methods lack robustness for UAV environments, and SOTA models require large datasets and high computational costs.

Method: Integrates Transformer-based GAN for data augmentation and MILET to focus on discriminative input segments.

Result: Achieves 96.5% accuracy on DroneDetect and 98.6% on DroneRF, outperforming SOTA methods with computational efficiency.

Conclusion: The framework is robust, efficient, and suitable for real-time deployment in resource-constrained UAV applications.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [97] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [98] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: Rec-AD improves FDIA detection efficiency in smart grids by combining Tensor Train decomposition with DLRM, reducing computational and memory burdens.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory inefficiencies in deep learning-based FDIA detection for large-scale smart grids.

Method: Integrates Tensor Train decomposition with DLRM, uses embedding compression, optimized data access, and pipeline training.

Result: Enhances computational throughput and real-time detection, narrowing attack windows and increasing attacker cost.

Conclusion: Rec-AD offers scalable, efficient FDIA detection, strengthening smart grid security.

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [99] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: AD-GCL is a novel graph contrastive learning framework addressing structural imbalance in anomaly detection, improving robustness for tail anomalies.


<details>
  <summary>Details</summary>
Motivation: Existing GCL-based anomaly detection models lack robustness to structural imbalance, especially for tail anomalies, limiting their real-world applicability.

Method: AD-GCL uses neighbor pruning for head nodes, anomaly-guided neighbor completion for tail nodes, and intra-/inter-view consistency loss for enhanced representation.

Result: AD-GCL outperforms existing methods in detecting both head and tail anomalies across multiple datasets.

Conclusion: AD-GCL provides a robust solution for graph anomaly detection, addressing structural imbalance and improving detection of tail anomalies.

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [100] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: GCC-Spam is a novel spam-text detection framework addressing adversarial strategies and data scarcity using character similarity networks, contrastive learning, and GAN-generated pseudo-spam samples, outperforming baselines with fewer labeled examples.


<details>
  <summary>Details</summary>
Motivation: The need for robust spam detection due to risks like information leakage and social instability, coupled with challenges like adversarial strategies and lack of labeled data.

Method: Integrates character similarity networks for orthographic/phonetic features, contrastive learning for discriminability, and GANs for pseudo-spam generation.

Result: Outperforms baseline methods, achieving higher detection rates with fewer labeled examples.

Conclusion: GCC-Spam effectively mitigates spam-text risks by addressing adversarial tactics and data scarcity, demonstrating superior performance.

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [101] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: The paper proposes SST-CL, a framework combining spatial-temporal transformers and curriculum learning for EEG-based emotion recognition, addressing challenges in integrating neural patterns and adapting to emotional intensity variations.


<details>
  <summary>Details</summary>
Motivation: To improve EEG-based emotion recognition by effectively integrating non-stationary spatial-temporal neural patterns and adapting to dynamic emotional intensity variations in real-world scenarios.

Method: SST-CL integrates spatial-temporal transformers (spatial encoder for inter-channel relationships, temporal encoder for multi-scale dependencies) with an intensity-aware curriculum learning strategy for dynamic sample scheduling.

Result: State-of-the-art performance on three benchmark datasets, validated by ablation studies confirming the necessity of the proposed components.

Conclusion: SST-CL effectively addresses key challenges in EEG-based emotion recognition, demonstrating superior performance and robustness.

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [102] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: The paper proposes CPAC, an interpretable architecture for fraud detection, improving latent space structure and outperforming traditional methods like SMOTE and generative models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting fraudulent credit card transactions due to class imbalance and subtle patterns, existing methods like GANs and VAEs often lead to overconfident classifiers and poor cluster separation.

Method: Introduces the Causal Prototype Attention Classifier (CPAC) with prototype-based attention mechanisms, coupled with a VAE-GAN for better cluster separation.

Result: CPAC achieves superior performance (F1-score: 93.14%, recall: 90.18%) and improved latent cluster separation compared to traditional oversamplers and generative models.

Conclusion: Classifier-guided latent shaping with CPAC enhances fraud detection, offering better performance and insights into representation learning.

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [103] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: The paper explores real-time generative AI (RTGen) workloads on heterogeneous SoCs, focusing on scheduling policies' impact on performance and latency.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored scheduling complexity and performance implications of RTGen workloads on heterogeneous SoCs like AMD's Ryzen AI.

Method: Characterizes RTGen workloads, constructs multi-model scenarios, profiles performance across backends, and evaluates five scheduling policies.

Result: Scheduling decisions significantly affect performance, with a 41.7% average difference in deadline violation rates.

Conclusion: Workload-aware, dynamic heterogeneous scheduling is crucial for high-performance on-device RTGen applications.

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [104] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: LeanTree introduces a white-box tool for automated theorem proving, leveraging Lean 4 to factorize proofs and outperforming black-box methods in some cases.


<details>
  <summary>Details</summary>
Motivation: Address the lag in white-box ATP methods compared to black-box approaches by providing a tool that utilizes intermediate proof states for better performance.

Method: Develop LeanTree, a Lean 4-based tool that factorizes complex proofs into simpler branches and provides a dataset of these states.

Result: Preliminary results suggest LeanTree's white-box approach outperforms black-box methods in certain scenarios.

Conclusion: White-box ATP tools like LeanTree offer advantages in evaluation, training, and efficiency, showing promise for future advancements.

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [105] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID is a unified framework for prompt-based continual learning in LLMs, addressing latent forgetting and prompt memory explosion via task-aware decoding and gradient-based prompt selection.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based CL methods assume task-aware inference and use growing task-specific prompts, limiting scalability and hiding latent forgetting.

Method: GRID integrates task-aware decoding (leveraging representative inputs, automatic task identification, and constrained decoding) and a gradient-based prompt selection strategy to compress less informative prompts.

Result: GRID improves backward transfer, achieves competitive forward transfer, reduces forgotten tasks by up to 80%, and outperforms state-of-the-art methods on T5 and Flan-T5 backbones.

Conclusion: GRID effectively addresses scalability and forgetting in prompt-based CL, offering a robust solution for lifelong learning in LLMs.

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [106] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: Trainable rational activation functions enhance adaptability but can cause instability in RL and continual learning. A constrained variant balances expressivity and plasticity, improving stability and performance.


<details>
  <summary>Details</summary>
Motivation: To study the impact of trainable rational activation functions on training stability in reinforcement and continual learning settings.

Method: Propose a constrained variant of trainable rational activations to limit excessive output scaling while preserving adaptability. Tested in MetaWorld, DMC, MNIST, and Split CIFAR-100 benchmarks.

Result: The constrained variant improves training stability and performance in RL and continual learning, revealing a trade-off between expressivity and plasticity.

Conclusion: Design principles for robust trainable activations in dynamic environments are provided, with the trade-off being more relevant for continuous control.

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [107] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: ASTRA improves TDA by efficiently approximating inverse Hessian-vector products (iHVP) using EKFAC-preconditioned Neumann series, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Gradient-based TDA methods struggle with efficient and accurate iHVP approximation, limiting their effectiveness.

Method: ASTRA combines EKFAC-preconditioner with Neumann series iterations for better iHVP approximation.

Result: ASTRA is more accurate, easier to tune, and requires fewer iterations than existing methods.

Conclusion: Accurate iHVP approximation significantly enhances TDA performance, as demonstrated by ASTRA.

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [108] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: The paper proposes a framework to incorporate model multiplicity into explanation generation, addressing uncertainty in explainable AI by aggregating partial dependence profiles from near-optimal models.


<details>
  <summary>Details</summary>
Motivation: Current automated machine learning systems focus on single best-performing models, neglecting explanation uncertainty, which is crucial for human-centered explainable AI.

Method: The framework aggregates partial dependence profiles (PDP) from a set of near-optimal models (Rashomon set) to generate Rashomon PDP, capturing interpretive variability.

Result: Experiments on 35 datasets show Rashomon PDP often covers less than 70% of the best model's PDP, highlighting limitations of single-model explanations.

Conclusion: Rashomon PDP enhances reliability and trustworthiness of model interpretations, especially in high-stakes domains requiring transparency and confidence.

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [109] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: The paper introduces two sampling methods for Gaussian processes (GPs) to address high costs in global sensitivity analysis (GSA) and optimization, demonstrating their effectiveness through numerical examples.


<details>
  <summary>Details</summary>
Motivation: High costs of simulations and experiments limit their use in GSA and optimization, motivating the use of GPs as proxy models for uncertainty-aware predictions.

Method: The paper presents two sampling methods for GPs: random Fourier features and pathwise conditioning, with alternative approaches briefly described.

Result: The methods are successfully applied in GSA, single-objective, and multi-objective optimization, as shown in numerical examples.

Conclusion: The sampling methods for GPs effectively support informed decision-making in engineering optimization tasks.

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [110] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: The paper explores whether directionality in neural networks is a useful inductive bias, showing it can be induced via pruning without performance loss.


<details>
  <summary>Details</summary>
Motivation: Inspired by recurrent circuits in biological brains, the study questions if directionality is a necessary or advantageous bias in artificial networks.

Method: Formalizes a perceptron layer with all-to-all connections (like a weight-tied RNN) and uses pruning to induce directionality.

Result: Pruning successfully induces topological ordering in information flow without harming performance, suggesting directionality is discoverable rather than essential.

Conclusion: Directionality is a beneficial inductive bias that can emerge through gradient descent and sparsification, not a strict requirement for learning.

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [111] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: The paper analyzes MISL in RL, focusing on CSF, proving it recovers ground-truth features up to linear transformation and explains mutual information objectives' implications.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical role of representation and mutual information in MISL, specifically in CSF.

Method: Theoretical analysis of CSF, proving identifiability of ground-truth features, and empirical validation in MuJoCo and DeepMind Control.

Result: CSF provably recovers ground-truth features up to linear transformation, explaining mutual information objectives and entropy regularizer downsides.

Conclusion: CSF provides the first identifiability guarantee for RL representation learning, validated empirically.

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [112] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT is a multi-modal framework integrating sparse CXR data and high-frequency clinical metrics to predict abnormal CXR findings in ICU patients up to 12 hours early.


<details>
  <summary>Details</summary>
Motivation: Existing CXR tools lack temporal dynamics, limiting their utility in ICUs where timely intervention is critical.

Method: CXR-TFT combines CXR imaging, radiology reports, and hourly clinical data using a vision encoder and transformer model for temporal alignment and prediction.

Result: The framework accurately predicted abnormal CXR findings 12 hours in advance in a study of 20,000 ICU patients.

Conclusion: CXR-TFT enhances early diagnosis and intervention for time-sensitive conditions like acute respiratory distress syndrome, improving clinical outcomes.

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [113] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: The paper investigates whether memorization in LLMs is avoidable in optimal language learning and assesses the actual privacy threat posed by memorization. It introduces contextual memorization, compares it with existing measures, and tests on 18 LLMs, finding partial memorization unavoidable and varying outcomes across measures.


<details>
  <summary>Details</summary>
Motivation: To determine if memorization can be avoided in optimal language learning and to evaluate the extent of privacy threats from memorization in LLMs.

Method: Re-examines existing memorization measures (recollection-based, counterfactual) and introduces contextual memorization. Tests on 18 LLMs across 6 families and multiple formal languages.

Result: (a) Memorization measures disagree on string memorization order. (b) Partial memorization is unavoidable. (c) Improved learning reduces contextual/counterfactual memorization but increases recollection-based. (d) Some reported memorized strings pose no privacy threat.

Conclusion: Memorization is partially unavoidable in optimal learning, and privacy threats may be overstated. Contextual memorization provides a nuanced measure, differing from existing ones.

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [114] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: Omni-Think, a unified RL framework, enhances LLM performance by combining rule-based rewards and generative preference signals, outperforming joint training and model merging via curriculum learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the generalization limitations of post-training methods like SFT, which often prioritize memorization over transferable learning.

Method: Introduces Omni-Think, a reinforcement learning framework with rule-based and LLM-as-a-Judge rewards, using curriculum-based task progression.

Result: Curriculum learning improves performance by 5.2% over joint training and 9.1% over model merging across four domains.

Conclusion: Task-aware sampling and hybrid supervision are key for scaling RL-based post-training in general-purpose LLMs.

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [115] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: The paper explores using LLMs for reasoning over financial knowledge graphs to detect money laundering, showing promising results in synthetic AML scenarios.


<details>
  <summary>Details</summary>
Motivation: Money laundering involves complex, interconnected entities, requiring advanced reasoning over graph data. The study aims to leverage LLMs for this purpose.

Method: A lightweight pipeline extracts k-hop subgraphs, serializes them into text, and uses few-shot learning with LLMs to assess suspiciousness and generate explanations.

Result: LLMs successfully emulate analyst logic, identify red flags, and provide coherent justifications in synthetic AML scenarios.

Conclusion: The study highlights the potential of LLMs for explainable, language-driven financial crime analytics, though it remains exploratory.

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [116] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: The paper extends equivariant network theory to time-parameterized transformations (flows) in sequence models, showing improved performance over non-equivariant models.


<details>
  <summary>Details</summary>
Motivation: Current equivariant networks are limited to static transformations, ignoring continuous symmetries in dynamic data like sequences.

Method: The authors introduce flow equivariance for sequence models (e.g., RNNs), ensuring hidden states transform geometrically for moving stimuli.

Result: Flow-equivariant models outperform non-equivariant ones in training speed, length generalization, and velocity generalization.

Conclusion: This work advances sequence models by incorporating time-parameterized symmetries, improving their alignment with real-world dynamics.

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [117] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: Subliminal learning in language models allows behavioral traits to transfer via unrelated data, posing risks for AI development.


<details>
  <summary>Details</summary>
Motivation: To investigate how language models can transmit behavioral traits through semantically unrelated data, even when filtered.

Method: Experiments with 'teacher' and 'student' models, using number sequences, code, or reasoning traces, and theoretical analysis of neural networks.

Result: Student models learn traits from unrelated data, but not with different base models. Theoretical proof confirms subliminal learning in neural networks.

Conclusion: Subliminal learning is a general phenomenon, highlighting risks in AI development, especially in distillation processes.

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [118] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: A benchmark study evaluates foundation models for EHRs using MIMIC-IV, showing multimodal models improve performance without bias.


<details>
  <summary>Details</summary>
Motivation: To assess the performance, fairness, and interpretability of foundation models in handling diverse EHR data modalities.

Method: Developed a standardized pipeline for MIMIC-IV data, compared eight foundation models (unimodal/multimodal, domain-specific/general-purpose).

Result: Multimodal models consistently improved predictive performance without adding bias.

Conclusion: Supports development of trustworthy multimodal AI for clinical use; code is publicly available.

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [119] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: The paper investigates the impact of an adaptive margin (eMargin) in contrastive loss for time series representation learning, finding it improves clustering but not downstream classification.


<details>
  <summary>Details</summary>
Motivation: To explore if an adaptive margin in contrastive loss can enhance separation between dissimilar time steps and improve downstream task performance.

Method: Introduces eMargin, an adaptive margin adjusted by a similarity threshold, and evaluates its effect on clustering and classification in benchmark datasets.

Result: eMargin improves unsupervised clustering metrics but fails to enhance downstream classification performance.

Conclusion: High clustering scores do not guarantee meaningful embeddings for downstream tasks; eMargin excels in clustering but not classification.

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [120] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR enhances precision but may limit exploration and originality in AI reasoning, constrained by the base model's support.


<details>
  <summary>Details</summary>
Motivation: To investigate whether RLVR truly expands reasoning boundaries or just refines existing knowledge.

Method: Theoretical analysis and empirical experiments to assess RLVR's limits.

Result: RLVR improves precision (pass@1) but shrinks solution diversity and may miss correct answers.

Conclusion: RLVR has limits in extending reasoning; future innovations like explicit exploration or hybrid strategies are needed.

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [121] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR is a Transformer-based framework with time-aware attention for EHR analysis, outperforming baselines by integrating temporal and semantic modeling.


<details>
  <summary>Details</summary>
Motivation: EHRs contain valuable but heterogeneous data with complex temporal patterns, which standard methods struggle to model effectively.

Method: TALE-EHR uses a time-aware attention mechanism and LLM-derived embeddings to model temporal gaps and clinical semantics.

Result: Outperforms state-of-the-art baselines on disease progression forecasting using MIMIC-IV and PIC datasets.

Conclusion: Integrating explicit temporal modeling with semantic representations advances EHR analysis.

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [122] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: A safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach using Control Barrier Functions (CBFs) is proposed to ensure safety and cooperation in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for safety in multi-agent autonomous systems while ensuring cooperation among agents.

Method: Hierarchical decomposition into high-level joint policy learning and low-level safe skill execution using CBFs.

Result: Achieves near-perfect safety (within 5%) and improved performance in challenging navigation scenarios.

Conclusion: The HMARL-CBF approach effectively balances safety and cooperation, outperforming existing methods.

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [123] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: The Graph Tsetlin Machine (GraphTM) extends the Tsetlin Machine to graph-structured data, improving interpretability and accuracy across diverse tasks like image classification, action tracking, recommendation systems, and genome analysis.


<details>
  <summary>Details</summary>
Motivation: To enhance the Tsetlin Machine's versatility and interpretability for graph-structured input, enabling it to handle sequences, grids, relations, and multimodality while maintaining efficiency.

Method: The GraphTM uses message passing to build nested deep clauses, recognizing sub-graph patterns with fewer clauses, improving interpretability and data utilization.

Result: GraphTM outperforms convolutional TM in image classification (3.86%-points higher accuracy on CIFAR-10), reinforcement learning methods in action tracking (up to 20.6%-points better), and GCN in recommendation systems (89.86% vs. 70.87% accuracy for noise ratio 0.1). It also trains 2.5x faster than GCN for genome sequence data.

Conclusion: GraphTM demonstrates the potential of graph representation learning and deep clauses to expand the capabilities of Tsetlin Machines across multiple domains.

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [124] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: Proposes an enhanced importance metric framework for structured pruning of DNNs to balance compression and task performance.


<details>
  <summary>Details</summary>
Motivation: High model complexity and computational demands hinder DNN adoption; existing pruning methods often fail to preserve application-specific performance.

Method: Uses an enhanced importance metric and multiple strategies to determine optimal pruning magnitude for each group.

Result: Effectively preserves task-relevant performance on an MNIST autoencoder, maintaining usability after substantial pruning.

Conclusion: The proposed framework successfully balances compression and performance, addressing application-specific constraints.

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [125] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: The paper addresses the lack of transparency in quantum machine learning by adapting classical uncertainty quantification methods to improve model interpretability.


<details>
  <summary>Details</summary>
Motivation: The opacity of quantum machine learning models, similar to classical deep learning, leads to issues like overfitting and overconfidence, necessitating better uncertainty quantification.

Method: The study theoretically develops and empirically evaluates techniques to map classical uncertainty quantification methods to quantum machine learning.

Result: Findings highlight the importance of integrating classical uncertainty insights into quantum model design for better transparency.

Conclusion: Classical uncertainty quantification methods can enhance the interpretability and reliability of quantum machine learning models.

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [126] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWCM dynamically adjusts momentum in FL to address non-IID data challenges, improving convergence and performance in long-tailed scenarios.


<details>
  <summary>Details</summary>
Motivation: FL struggles with non-IID and long-tailed data, leading to biased models and convergence issues. Momentum-based methods exacerbate this problem.

Method: FedWCM dynamically adjusts momentum using global and per-round data to correct directional biases caused by long-tailed distributions.

Result: FedWCM resolves non-convergence issues and outperforms existing methods, enhancing FL efficiency and effectiveness.

Conclusion: FedWCM effectively addresses FL challenges in non-IID and long-tailed data scenarios, improving model performance and convergence.

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [127] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: Proposes FedClusAvg, a federated learning framework for detecting False Data Injection Attacks (FDIAs) in smart grids, addressing Non-IID data challenges and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: FDIAs threaten smart grids, and traditional centralized methods face privacy risks, data sharing constraints, and high transmission costs.

Method: FedClusAvg uses cluster-based stratified sampling and hierarchical communication (client-subserver-server) for localized training and weighted parameter aggregation.

Result: Improves detection accuracy in Non-IID environments and reduces communication rounds/bandwidth.

Conclusion: FedClusAvg offers a secure, efficient solution for FDIA detection in distributed power systems.

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [128] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: The paper introduces Time-RA, a generative task for time-series anomaly reasoning using LLMs, and RATs40K, a multimodal benchmark dataset with detailed annotations.


<details>
  <summary>Details</summary>
Motivation: Current time-series anomaly detection lacks detailed categorization and explanatory reasoning.

Method: Proposes Time-RA task and RATs40K dataset, using LLMs and multimodal data with GPT-4-refined annotations.

Result: Benchmarks show LLMs' capabilities and limitations, emphasizing supervised fine-tuning.

Conclusion: The work advances interpretable anomaly detection and reasoning in time-series data.

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [129] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: ROBAD is a transformer-based model designed to detect bad actors on internet platforms robustly by capturing local and global post information and using adversarial training.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for bad actor detection lack robustness against adversarial attacks, prompting the need for a more resilient solution.

Method: ROBAD uses transformer encoder blocks for post-level embeddings and decoder blocks for sequence-level embeddings, enhanced with contrastive learning for adversarial robustness.

Result: ROBAD effectively detects bad actors under state-of-the-art adversarial attacks, as validated on Yelp and Wikipedia datasets.

Conclusion: ROBAD addresses robustness in bad actor detection by combining local-global information and adversarial training, proving effective against attacks.

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [130] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: Flow-matching policies trained via reinforcement learning outperform suboptimal demonstrations, with GRPO reducing costs by 50-85% compared to naive imitation learning.


<details>
  <summary>Details</summary>
Motivation: To surpass the performance of suboptimal demonstration policies, such as human operators, in robotics tasks.

Method: Introduces Reward-Weighted Flow Matching (RWFM) and Group Relative Policy Optimization (GRPO) with a learned reward surrogate, tested on simulated unicycle dynamics tasks.

Result: Both RWFM and GRPO significantly improve upon demonstrator performance, with GRPO reducing costs by 50-85% over naive imitation learning.

Conclusion: Reinforcement learning-based flow-matching policies, especially GRPO, effectively outperform suboptimal demonstrations in robotics tasks.

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [131] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: The paper introduces Isotonic Quantile Regression Averaging (iQRA), a method for probabilistic forecasting in volatile domains like electricity markets, improving accuracy and reliability over existing methods.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in forecasting models is crucial for risk assessment in volatile domains like electricity markets, but current machine learning models often lack reliable uncertainty estimates.

Method: The proposed iQRA method extends Quantile Regression Averaging (QRA) by introducing stochastic order constraints to enhance forecast accuracy, reliability, and computational efficiency.

Result: iQRA outperforms state-of-the-art methods in reliability and sharpness, producing well-calibrated prediction intervals across confidence levels, especially compared to coverage-based conformal prediction.

Conclusion: iQRA offers a hyperparameter-free, computationally efficient solution for probabilistic forecasting, improving decision-making in volatile markets.

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [132] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: A novel robust control theory extension addresses gradient uncertainty in value functions, introducing the GU-HJBI equation. It proves well-posedness, challenges classical assumptions, and proposes a practical GURAC algorithm.


<details>
  <summary>Details</summary>
Motivation: Address uncertainty in value function gradients, common in reinforcement learning and other fields, to improve robustness.

Method: Formulate a zero-sum dynamic game with adversarial perturbations, analyze the GU-HJBI equation, and validate with LQ case and numerical studies.

Result: Classical quadratic value function fails under gradient uncertainty; non-polynomial corrections and nonlinear control laws emerge.

Conclusion: The work advances robust control theory and introduces GURAC, with broad implications for reinforcement learning and computational finance.

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [133] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: AnalogFed enables collaborative AI-driven analog circuit design without sharing raw data, achieving performance comparable to centralized methods while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: The proprietary nature of analog circuit design limits data availability, hindering generative AI research. AnalogFed addresses this by facilitating decentralized collaboration.

Method: AnalogFed uses federated learning (FedL) tailored for analog design, including generative model development, data heterogeneity handling, and privacy-preserving strategies.

Result: AnalogFed matches centralized baselines in performance, with state-of-the-art efficiency and scalability in topology discovery.

Conclusion: AnalogFed successfully bridges the gap between data privacy and collaborative innovation in AI-driven analog circuit design.

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [134] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: The paper introduces distributional unlearning, a model-agnostic framework to remove unwanted data distributions efficiently while preserving retained data quality, outperforming random deletion methods.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning tools are sample-oriented and inefficient for removing entire domains, necessitating a more effective approach for privacy, legal, or quality compliance.

Method: The framework uses Kullback-Leibler divergence to quantify removal and preservation, deriving exact Pareto frontiers for Gaussian cases and proposing a distance-based selection rule.

Result: Experiments show 15-72% fewer deletions than random methods, with minimal impact on retained performance.

Conclusion: Distributional unlearning provides a scalable, efficient solution for domain-level data removal, addressing limitations of sample-oriented approaches.

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [135] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: U-Cast is a novel architecture for High-Dimensional Time Series Forecasting (HDTSF) that learns hierarchical channel structures using query-based attention and full-rank regularization, outperforming baselines on the Time-HD benchmark.


<details>
  <summary>Details</summary>
Motivation: Traditional time series forecasting (TSF) models struggle with high-dimensional datasets due to complex channel correlations, which are often ignored or poorly scaled.

Method: U-Cast employs channel-dependent forecasting with query-based attention and full-rank regularization to disentangle correlated channels.

Result: U-Cast outperforms baselines in accuracy and efficiency on the Time-HD benchmark, demonstrating the benefits of cross-channel information.

Conclusion: U-Cast and Time-HD provide a foundation for future HDTSF research, addressing scalability and hierarchical patterns in high-dimensional time series.

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [136] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: A genetic algorithm is proposed to generate synthetic datasets with controlled complexity for ML evaluation, showing a correlation between dataset complexity and model performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the availability of diverse datasets for reliably evaluating ML methods by controlling problem complexity.

Method: A genetic algorithm optimizes problem complexity measures (10 for classification, 4 for regression) via linear feature projections to achieve target complexity values.

Result: The algorithm successfully generates datasets with varying difficulty levels, and evaluations show a link between complexity and model recognition quality.

Conclusion: The approach effectively produces datasets with tailored complexity, aiding ML method evaluation.

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [137] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: The paper explores multi-label classification with logical constraints, using a sequential model to capture label correlations and enforce constraints during training and inference.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multi-label classification with large label sets and logical constraints, ensuring accurate modeling of label correlations.

Method: Proposes an architecture combining individual label classifiers with an expressive sequential model to produce a joint distribution.

Result: Demonstrates the model's ability to exploit and enforce logical constraints effectively.

Conclusion: The architecture successfully handles constraints in multi-label classification, improving performance in training and inference.

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [138] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: A neuromorphic computing architecture using resonant-tunnelling diodes (RTDs) is proposed for efficient physical reservoir computing, validated on image recognition tasks.


<details>
  <summary>Details</summary>
Motivation: The need for hardware-efficient computational models in AI for edge-based and resource-constrained environments drives this research.

Method: Theoretical formulation and numerical implementation of an RTD-based reservoir computing system, tested on handwritten digit and Fruit~360 dataset recognition.

Result: The architecture shows promising performance, adhering to deterministic nonlinear transformation principles.

Conclusion: The RTD-based system offers a viable, efficient approach for next-generation reservoir computing.

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [139] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: The paper addresses the misalignment between artificial evaluation metrics for Counterfactual Explanations (CFEs) and real-world user preferences, proposing a user-centric model (AWP) that predicts preferred CFEs with 84.37% accuracy.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between opaque machine learning decision models and user understanding by improving CFEs' alignment with real-world preferences.

Method: Conducted two user studies: a pilot with 20 crowd-workers and a detailed study with 41 participants in credit application scenarios, leading to the AWP model.

Result: User-preferred CFEs matched proximity-based ones only 63.81% of the time; AWP model achieved 84.37% accuracy in predicting user preferences.

Conclusion: Highlights the need for adaptive, user-centered CFE evaluation metrics, validated by human-centered studies.

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [140] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [141] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: JL-GAT applies Grounded Action Transformation (GAT) to multi-agent RL for Traffic Signal Control (TSC), addressing the sim-to-real gap by incorporating neighbor agent info, enhancing scalability and performance.


<details>
  <summary>Details</summary>
Motivation: The sim-to-real gap in MARL-based TSC policies causes performance drops in real-world deployment, necessitating a scalable solution like JL-GAT.

Method: JL-GAT decentralizes GAT for MARL, balancing scalability and grounding by leveraging neighbor agent interactions.

Result: Experiments show JL-GAT effectively mitigates the sim-to-real gap in diverse road networks, even under adverse conditions.

Conclusion: JL-GAT successfully bridges the sim-to-real gap for MARL-based TSC, offering a scalable and practical solution for real-world traffic networks.

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [142] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: The paper proposes using average controllability and a rank encoding method to improve GNN performance in social network classification, especially when node features are unavailable.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle in social networks due to lack of node features. The paper aims to address this by leveraging network topology metrics.

Method: Introduces NCT-EFA (node-level metrics) and a rank encoding method to transform graph-theoretic metrics into fixed-dimensional features.

Result: Incorporating average controllability and rank encoding boosts GNN performance, with ROC AUC improving from 68.7% to 73.9% on a dataset.

Conclusion: The proposed methods enhance GNN performance by creating expressive node features from network topology, outperforming traditional approaches.

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [143] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: The paper introduces LSDGNN, a multimodal approach for Emotion Recognition in Conversation (ERC), combining long- and short-distance graph neural networks with a Differential Regularizer and BiAffine Module for feature interaction. It also proposes Improved Curriculum Learning (ICL) to handle data imbalance, achieving superior results on IEMOCAP and MELD datasets.


<details>
  <summary>Details</summary>
Motivation: ERC is challenging due to the complexity of multimodal interactions in conversations. Existing methods lack effective handling of long- and short-distance dependencies and data imbalance.

Method: LSDGNN uses DAG-based long- and short-distance graph neural networks, a Differential Regularizer for distinct feature representation, and a BiAffine Module for interaction. ICL addresses data imbalance via a weighted emotional shift metric and difficulty measurer.

Result: The model outperforms benchmarks on IEMOCAP and MELD datasets, demonstrating effectiveness in ERC.

Conclusion: LSDGNN with ICL provides a robust solution for ERC, improving feature representation and handling data imbalance.

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [144] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: The paper introduces exact constrained reformulations for direct metric optimization (DMO) in imbalanced classification, addressing precision and recall under three practical settings, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard accuracy is misleading in imbalanced classification, and existing methods fail when class significance varies or specific metrics must meet certain levels.

Method: The authors propose exact constrained reformulations for DMO problems (FPOR, FROP, OFBS), solvable by exact penalty methods, avoiding smooth approximations.

Result: Experiments on benchmark datasets show the approach's superiority over state-of-the-art methods for the three DMO problems.

Conclusion: The exact reformulation and optimization (ERO) framework is effective for binary IC and has potential for broader DMO applications.

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [145] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: The paper introduces an attention-based Graph Neural Network for accurate demand forecasting in food delivery, addressing spatial-temporal dependencies to improve operational decisions.


<details>
  <summary>Details</summary>
Motivation: Accurate demand forecasting is crucial for food delivery platforms due to spatial heterogeneity and temporal fluctuations in order volumes, impacting efficiency and responsiveness.

Method: Proposes an attention-based Graph Neural Network modeling delivery zones as nodes and spatial-temporal dependencies as edges, dynamically weighing neighboring influences.

Result: Demonstrates high accuracy in forecasting order volumes on real-world datasets, outperforming existing methods.

Conclusion: The framework provides a scalable, adaptive solution for optimizing fleet positioning, resource allocation, and dispatch in urban food delivery.

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [146] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS is a training-free, model-agnostic acceleration framework for diffusion-based generative models, achieving up to 2.9x speedup with eight cores without quality loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are computationally expensive during inference, and existing acceleration methods often require retraining or degrade quality.

Method: CHORDS uses multi-core parallelism, treating diffusion sampling as an ODE solver pipeline where slower solvers rectify faster ones via inter-core communication.

Result: CHORDS achieves up to 2.1x speedup with four cores and 2.9x with eight cores, outperforming baselines by 50% without quality degradation.

Conclusion: CHORDS enables real-time, high-fidelity diffusion generation, providing a scalable and efficient solution for accelerating inference.

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [147] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: The paper proposes temporal basis function models (TBFMs) for efficient, low-latency closed-loop neural stimulation, demonstrating their effectiveness in predicting and controlling neural activity in non-human primates.


<details>
  <summary>Details</summary>
Motivation: To address translational challenges in AI-driven closed-loop neural stimulation, such as sample efficiency, training time, and loop latency, for personalized therapies in neurological diseases like Parkinson's.

Method: Uses TBFMs for single-trial, spatiotemporal forward prediction of optogenetic stimulation effects on local field potentials (LFPs) in non-human primates. Also simulates closed-loop control of neural activity.

Result: TBFMs are sample-efficient, fast to train (2-4min), and low-latency (0.2ms). They achieve prediction accuracy comparable to slower models and successfully control neural circuits in simulations.

Conclusion: TBFMs bridge the gap between AI-based dynamical systems modeling and clinically useful closed-loop stimulation protocols.

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [148] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: The paper introduces a streaming unlearning paradigm to handle data removal requests efficiently and effectively, addressing challenges like performance maintenance and data access.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods handle forgetting data in a single batch, which is inefficient for streaming removal requests. The paper aims to solve this gap.

Method: The authors formalize unlearning as a distribution shift problem, estimate the altered distribution, and propose a novel streaming unlearning algorithm that doesn't require original training data access.

Result: Theoretical analysis shows an $O(\sqrt{T} + V_T)$ error bound on streaming unlearning regret. Experiments validate the method's performance across models and datasets.

Conclusion: The proposed streaming unlearning algorithm effectively addresses the challenges of streaming forgetting, offering theoretical guarantees and practical validation.

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [149] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: A framework using expert demonstrations and intrinsic rewards for robust RL exploration in sparse/dense reward environments.


<details>
  <summary>Details</summary>
Motivation: Address challenges in RL where agents must learn from reward-free signals and incomplete demonstrations, overcoming limitations of intrinsic motivation in complex environments.

Method: Proposes a framework with a mapping function to transform state-expert similarity into shaped intrinsic rewards, using a Mixture of Autoencoder Experts to handle diverse behaviors and missing data.

Result: Enables robust exploration and strong performance in sparse/dense reward settings, even with incomplete demonstrations.

Conclusion: Provides a practical RL solution for real-world scenarios lacking optimal data or precise reward control.

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [150] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: The paper extends Preferential Subspace Identification (PSID) to include optimal filtering and smoothing for better estimation in offline applications, validated on simulated data.


<details>
  <summary>Details</summary>
Motivation: Current PSID methods focus on prediction using past data, but offline applications can benefit from incorporating concurrent or all available data for improved estimation.

Method: The authors extend PSID with a reduced-rank regression step for optimal filtering and introduce a forward-backward smoothing algorithm using residuals from the filtered signal.

Result: The approach successfully recovers ground-truth model parameters and achieves optimal decoding performance matching the true underlying model.

Conclusion: This work provides a principled framework for optimal linear filtering and smoothing in multivariate time-series analysis, enhancing dynamic interaction studies.

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [151] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: FG-TS improves exploration in contextual bandits with an optimism bonus, outperforming vanilla TS in linear/logistic settings but weaker in neural bandits.


<details>
  <summary>Details</summary>
Motivation: Address the lack of aggressive exploration in Thompson Sampling for high-dimensional problems.

Method: Introduces FG-TS and SFG-TS, evaluates them across 11 benchmarks with exact/approximate posteriors.

Result: FG-TS outperforms vanilla TS in linear/logistic bandits but struggles with neural bandits. Robustness depends on posterior accuracy.

Conclusion: FG-TS is recommended as a baseline for contextual-bandit benchmarks due to its competitive performance and ease of use.

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [152] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT, a multi-view graph transformer framework, improves crystal property prediction by fusing SE3 invariant and SO3 equivariant representations, achieving up to 21% error reduction and 58% gains in transfer learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture the intricate geometric and topological characteristics of crystal structures, limiting machine learning applications in materials science.

Method: MGT combines SE3 invariant and SO3 equivariant graph representations using a lightweight mixture of experts router, adapting weights based on the task.

Result: MGT reduces mean absolute error by up to 21% and achieves up to 58% improvement in transfer learning tasks.

Conclusion: MGT is a versatile and effective tool for crystal property prediction, aiding in novel material discovery.

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [153] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN is a model knowledge base pipeline for refining neural networks by leveraging relational dependencies between tasks and model architectures, outperforming static model selection methods.


<details>
  <summary>Details</summary>
Motivation: Traditional static model selection in databases overlooks evolving relational dependencies between tasks and model architectures, leading to suboptimal matches. M-DESIGN addresses this gap by enabling adaptive refinement.

Method: M-DESIGN uses a knowledge weaving engine and a graph-relational schema to iteratively refine models based on task metadata, architecture variations, and performance metrics. It includes a predictive query planner for OOD tasks.

Result: M-DESIGN achieved optimal models in 26 of 33 data-task pairs within limited budgets, demonstrating its effectiveness for graph analytics tasks.

Conclusion: M-DESIGN bridges the model refinement gap in databases by dynamically adapting to task queries and model architectures, offering a scalable solution for neural network optimization.

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [154] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock is a decentralized framework for secure LLM fine-tuning, replacing central servers with blockchain and incentives, reducing attack success rates by >68% and improving cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Centralized control and overhead in decentralized schemes hinder LLM fine-tuning, while standard FL has vulnerabilities like single-point attacks. FLock addresses these issues.

Method: FLock integrates a blockchain-based trust layer with economic incentives, enabling secure cooperation among untrusted parties for LLM fine-tuning.

Result: Empirical validation shows FLock defends against poisoning attacks, reduces adversarial success by >68%, and enhances cross-domain generalization.

Conclusion: FLock provides a secure, efficient solution for decentralized LLM fine-tuning, outperforming isolated training and standard FL.

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [155] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM is a unified model for analyzing active learning (AL) trajectories, predicting performance and enabling comparisons across strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional AL evaluations focus only on final accuracy, missing the dynamics of the learning process. PALM addresses this gap.

Method: PALM uses four parameters (achievable accuracy, coverage efficiency, early-stage performance, scalability) to model AL behavior from partial observations.

Result: Validated on CIFAR and ImageNet datasets, PALM generalizes well, predicting full learning curves and revealing insights into AL efficiency.

Conclusion: PALM enables systematic AL evaluation, aiding strategy selection and performance prediction under budget constraints.

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [156] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: The paper introduces Channel Space Gridization (CSG), a novel framework unifying channel estimation and gridization, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing gridization methods (GSG or BSG) rely on unavailable location data or flawed assumptions about channel properties, limiting their effectiveness.

Method: Proposes CSG, a joint optimization framework using beam-level RSRP to estimate CAPS and partition samples into grids. Introduces CSG-AE with a trainable encoder, quantizer, and physics-informed decoder, along with the PIDA training scheme for stability.

Result: CSG-AE achieves superior CAPS estimation and clustering on synthetic data. On real-world data, it reduces MAE by 30-65% and improves channel consistency and cluster balance.

Conclusion: CSG advances gridization for large-scale network optimization by addressing limitations of prior methods and demonstrating significant performance improvements.

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [157] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: The paper provides theoretical justification for using denoiser models as surrogates for the proximal operator in MAP optimization, proving convergence under log-concavity assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical justification for substituting pretrained denoisers in heuristic iterative schemes for MAP optimization.

Method: Proposes a simple algorithm related to practical methods, interpreting it as gradient descent on smoothed proximal objectives.

Result: The algorithm provably converges to the proximal operator under log-concavity assumptions on the prior.

Conclusion: The work offers a theoretical foundation for empirically successful but previously heuristic denoiser-based methods.

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [158] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: The paper provides a theoretical framework for Transformers using Lagrangian optimization and calculus of variations, treating them as flow maps on a high-dimensional unit sphere.


<details>
  <summary>Details</summary>
Motivation: To mathematically ground Transformers by linking them to Lagrangian optimization and variational calculus, addressing gaps in foundational proofs.

Method: Develops a functional for Transformers as flow maps, derives the Euler-Lagrange equation, and applies calculus of variations on manifolds.

Result: Shows Transformers solve variational problems, introduces new scenarios for path optimality, and provides foundational proofs.

Conclusion: Lays the groundwork for applying variational calculus to Transformers, offering new theoretical insights and tools for neural approximations.

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [159] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: The paper introduces an adaptive random Fourier features (ARFF) method with Metropolis sampling for learning stochastic differential equations, outperforming Adam-based optimization in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve the learning of drift and diffusion components in stochastic differential equations from snapshot data.

Method: Uses ARFF with Metropolis sampling and resampling, along with a likelihood-based loss function derived from Euler-Maruyama integration.

Result: ARFF matches or exceeds Adam-based optimization in loss minimization and convergence speed across benchmark problems.

Conclusion: ARFF is a promising alternative for data-driven modeling of stochastic dynamics.

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [160] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo is a privacy-preserving framework for in-vehicle emotion recognition, combining visual and physiological data via federated learning, achieving 87% accuracy while keeping data local.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like modality fragility, physiological variability, and privacy risks in emotion recognition for driver-assistance systems.

Method: Uses a multimodal federated learning pipeline with CNN for facial images and Random Forest for physiological cues, employing majority-vote fusion and personalized Federated Averaging.

Result: Achieves 77% accuracy (CNN), 74% (Random Forest), and 87% (fusion), matching centralized performance with local data retention. System converges in 18 rounds with low resource use.

Conclusion: FedMultiEmo provides a practical, privacy-aware solution for real-time emotion recognition in vehicles.

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [161] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper addresses overoptimization in RLHF for LMs by proposing Off-Policy Corrected Reward Modeling (OCRM) to improve reward model accuracy without new labels.


<details>
  <summary>Details</summary>
Motivation: Overoptimization occurs when the LM's responses diverge from the RM's training data, causing inaccurate rewards and misaligned human preferences.

Method: OCRM iteratively corrects the RM using importance weighting, avoiding new labels or samples.

Result: Experiments show OCRM outperforms standard RLHF methods in summarization and chatbot tasks.

Conclusion: OCRM effectively mitigates overoptimization, improving policy alignment with human preferences.

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [162] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: The paper addresses domain shift in audio classification using Test-Time Adaptation (TTA), comparing methods like TTT, TENT, and CoNMix. The modified CoNMix achieves the best accuracy under noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Domain shift in audio classification degrades model performance. The study aims to mitigate this using TTA, focusing on background noise.

Method: Adopts TTA methods (TTT, TENT, CoNMix) and tests them on AudioMNIST and SpeechCommands V1 datasets under varying noise conditions.

Result: Modified CoNMix outperforms others, achieving 5.31% and 12.75% error rates under severe noise.

Conclusion: This is the first study to apply TTA for audio classification under domain shift, showing promising results with CoNMix.

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [163] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: The paper introduces 'Data Aware Differentiable Neural Architecture Search' to simplify TinyML system design by co-optimizing model architecture and data configuration, improving resource efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The resource footprint of Machine Learning and complexity of TinyML design hinder adoption, prompting the need for efficient solutions.

Method: Expands Neural Architecture Search to include data configuration parameters, enabling joint optimization of architecture and data characteristics.

Result: Initial tests on keyword spotting show the method produces accurate yet resource-efficient TinyML systems.

Conclusion: The approach effectively balances performance and resource usage, advancing TinyML adoption.

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [164] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: The study evaluates the added value of conventional radiomics (CR) and deep learning (DL) MRI radiomics for glioblastoma prognosis, finding minimal improvement over clinical predictors.


<details>
  <summary>Details</summary>
Motivation: To assess whether radiomics provides significant added value over clinical and molecular predictors for glioblastoma survival prognosis.

Method: Analyzed 1152 glioblastoma patients using CR and DL models with clinical, molecular, and MRI data, evaluated on internal and external cohorts with sub-analyses for different feature sets and patient subsets.

Result: Combined-feature CR models slightly outperformed clinical-only models (AUC 0.75 vs 0.74), but DL models showed no significant improvement. Imaging data had modest relevance for overall survival prediction.

Conclusion: Radiomics offers minimal added value over clinical predictors like age and gender for glioblastoma prognosis.

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [165] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: PhysGym is a new benchmark suite for evaluating LLM-based agents' scientific reasoning in physics, focusing on prior knowledge and problem complexity.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack the ability to assess LLM agents' scientific discovery capabilities, especially in varying environmental complexity and prior knowledge utilization.

Method: PhysGym introduces interactive physics simulations where agents gather data and form hypotheses, with controlled prior knowledge levels.

Result: The benchmark effectively differentiates LLM capabilities based on prior knowledge and task complexity, as shown by baseline results.

Conclusion: PhysGym fills a critical gap by providing a standardized platform for rigorous evaluation of LLM-based scientific reasoning.

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [166] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: The paper explores how the accuracy of machine learning (ML) predictions for patient length-of-stay (LOS) affects rescheduling flexibility in elective surgery planning, aiming to prevent bed overflows and optimize resource use.


<details>
  <summary>Details</summary>
Motivation: Downstream resource availability, like inpatient beds, is critical for elective surgery planning. Inaccurate LOS predictions can lead to infeasible schedules, necessitating rescheduling strategies.

Method: The study uses simulated ML to evaluate the relationship between LOS prediction accuracy and rescheduling flexibility under various corrective policies.

Result: The research identifies effective patient rescheduling strategies to mitigate the impact of LOS prediction errors.

Conclusion: Accurate LOS predictions reduce rescheduling needs, but the study highlights the importance of flexible strategies to manage prediction errors and optimize resources.

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [167] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: AI-driven algorithms, particularly Reinforcement Learning (RL), outperform traditional methods in optimizing satellite mega-constellation operations like data routing and resource allocation.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of satellite constellations demands efficient, scalable, and resilient management solutions.

Method: Developed AI-driven algorithms (RL) for data routing (reducing latency) and resource allocation (optimizing battery/memory usage), tested in real-life scenarios.

Result: RL outperforms classical methods, offering flexibility, scalability, and generalizability in satellite fleet management.

Conclusion: AI can transform satellite constellation management with adaptive, robust, and cost-effective solutions.

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [168] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: The paper argues that stagnation in anomaly detection progress is due to flawed benchmarking methods and proposes three key improvements for better evaluation.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking in anomaly detection fails to reflect real-world diversity, limiting progress.

Method: Proposes three improvements: scenario identification via taxonomy, end-to-end pipeline analysis, and meaningful evaluation aligned with objectives.

Result: Highlights the need for scenario-specific benchmarking to advance anomaly detection.

Conclusion: Rethinking benchmarking practices is essential for meaningful progress in anomaly detection.

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [169] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: A Red-Team Multi-Agent Reinforcement Learning framework is proposed to uncover corner cases in safety-critical scenarios by using interfering red-team vehicles, improving AV decision-making safety.


<details>
  <summary>Details</summary>
Motivation: Existing methods for scenario generation in safety-critical contexts are inefficient and miss corner cases, necessitating a more effective approach.

Method: The framework employs red-team agents (background vehicles) to actively interfere and explore, using a Constraint Graph Representation Markov Decision Process to ensure safety while disrupting AVs. A policy threat zone model quantifies threats.

Result: The framework significantly impacts AV decision-making safety and generates diverse corner cases.

Conclusion: This approach provides a novel direction for research in safety-critical scenarios, enhancing AV safety testing.

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [170] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: A novel C²-aware framework for optimal batch-size control in federated learning (FL) minimizes latency while ensuring convergence, addressing challenges of high-dimensional updates and device heterogeneity.


<details>
  <summary>Details</summary>
Motivation: The need for low-latency FL in 6G networks for time-sensitive IoT applications like autonomous driving and healthcare, while overcoming challenges of model update overhead and device heterogeneity.

Method: Proposes a C²-aware framework with optimal batch-size control, balancing a tradeoff between gradient accuracy and per-round latency. Uses a tractable surrogate for convergence speed, fitted to real data, and designs strategies for slow/fast fading scenarios.

Result: Outperforms conventional batch-size adaptation schemes, reducing end-to-end learning latency while maintaining convergence.

Conclusion: The framework effectively addresses FL challenges in 6G networks, offering practical solutions for latency-sensitive applications.

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [171] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: A deep learning surrogate model accelerates HEC-RAS river forecasts by combining GRU and Geo-FNO, reducing computation time by 3.5x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Physics-based solvers like HEC-RAS are computationally intensive, limiting real-time flood decision-making. The goal is to speed up simulations without losing accuracy.

Method: Proposes a hybrid architecture with GRU for short-term dynamics and Geo-FNO for spatial dependencies, trained on HEC-RAS data from the Mississippi River Basin.

Result: Achieves a median absolute stage error of 0.31 feet and reduces forecast time from 139 to 40 minutes.

Conclusion: The surrogate model proves data-driven approaches can replace traditional hydraulic models, enhancing large-scale flood forecasting feasibility.

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [172] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: Proposes Data Mixing Agent, a model-based framework for domain reweighting in continual pre-training, outperforming baselines and generalizing well across unseen fields.


<details>
  <summary>Details</summary>
Motivation: Addresses catastrophic forgetting in continual pre-training by automating domain reweighting, moving beyond manual heuristics.

Method: Uses reinforcement learning to train an agent on data mixing trajectories, learning generalizable heuristics for domain reweighting.

Result: Outperforms baselines in math reasoning, generalizes across unseen fields, and adapts to code generation.

Conclusion: Data Mixing Agent is effective, adaptable, and aligns with human intuition, improving model performance with less source data.

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [173] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: An interpretable anomaly detection framework for bike-sharing systems using multi-source data and Isolation Forest with DIFFI for interpretability.


<details>
  <summary>Details</summary>
Motivation: To optimize operations, improve reliability, and enhance user experience in shared mobility systems by detecting anomalies.

Method: Uses Isolation Forest for unsupervised anomaly detection and DIFFI for interpretability, integrating bike-sharing trip records, weather, and transit data.

Result: Station-level analysis effectively identifies anomalies, influenced by weather and transit availability.

Conclusion: The framework aids decision-making in shared mobility operations by providing actionable insights.

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [174] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: GeoHNN is a neural network framework that embeds geometric priors from physics to improve stability and accuracy in modeling dynamics.


<details>
  <summary>Details</summary>
Motivation: Common machine learning methods ignore the geometric principles of physics, leading to unstable predictions for complex systems.

Method: GeoHNN encodes Riemannian and symplectic geometries, using symmetric positive-definite matrices and constrained autoencoders.

Result: GeoHNN outperforms existing models in stability, accuracy, and energy conservation for various systems.

Conclusion: Embedding physics' geometry is essential for robust and generalizable models of physical dynamics.

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [175] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: The paper explores unsupervised anomaly detection in EV charging stations using Isolation Forest and DIFFI for interpretability, tested in a real-world industrial case.


<details>
  <summary>Details</summary>
Motivation: To ensure reliability and efficiency of EV charging infrastructure by detecting anomalies and uncovering their root causes.

Method: Uses Isolation Forest for anomaly detection and DIFFI for feature importance analysis, applied to real-world sensor and charging session data.

Result: Demonstrates the efficacy of the approach in identifying anomalies and their contributing features in a real industrial scenario.

Conclusion: The proposed method effectively detects anomalies and enhances interpretability for root cause analysis in EV charging infrastructure.

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [176] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: DIVA, a deep-learning-based tool, automates plant stress detection using Raman spectroscopy without manual preprocessing, improving accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional Raman analysis for plant stress detection is biased and inconsistent due to manual preprocessing. DIVA aims to automate and improve this process.

Method: DIVA uses a variational autoencoder to process native Raman spectra, including fluorescence backgrounds, without manual intervention.

Result: DIVA successfully detected various plant stresses (abiotic and biotic) by identifying spectral features in an unbiased manner.

Conclusion: DIVA enables AI-driven plant health assessment, promoting resilient and sustainable agriculture.

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [177] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: The paper highlights the challenge of time-series forecasting in deep models and proposes a dynamics-focused approach to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current deep models struggle with time-series forecasting, suggesting a need to better learn underlying data dynamics.

Method: The authors introduce the PRO-DYN nomenclature to analyze models based on dynamics and conduct systemic and empirical studies.

Result: Findings show under-performing models learn dynamics partially and emphasize the importance of placing the dynamics block at the model's end.

Conclusion: Incorporating a learnable dynamics block as the final predictor is crucial for improved performance in time-series forecasting.

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [178] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: Proactive blockage prediction for mmWave vehicular communication using multi-modal sensing (camera, GPS, LiDAR, radar) with deep learning models and softmax-weighted fusion, achieving high F1-scores and low inference times.


<details>
  <summary>Details</summary>
Motivation: Address susceptibility of mmWave vehicular communication to signal blockage from dynamic obstacles like vehicles and pedestrians.

Method: Multi-modal sensing (camera, GPS, LiDAR, radar) with modality-specific deep learning models and softmax-weighted ensemble fusion.

Result: Camera-only achieves 97.1% F1-score (89.8ms); camera+radar improves to 97.2% F1 (95.7ms).

Conclusion: Multi-modal sensing is effective and efficient for mmWave blockage prediction, enabling proactive wireless communication in dynamic environments.

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [179] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: Small LLMs trained with RLVR show improved in-distribution ToM performance but fail to generalize to unseen tasks, indicating narrow overfitting rather than true ToM acquisition.


<details>
  <summary>Details</summary>
Motivation: To explore if RLVR can instill human-like social intelligence (ToM) in small LLMs.

Method: Train small LLMs on ToM datasets (HiToM, ExploreToM, FANToM) using RLVR and test generalization on held-out datasets (e.g., OpenToM).

Result: Models improve on in-distribution tasks but fail to generalize, with prolonged RL leading to dataset-specific overfitting.

Conclusion: RLVR does not enable small LLMs to develop a robust, generalizable ToM capability.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [180] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: A cost-effective method for synthetic tabular data generation using LLMs to create reusable sampling scripts, improving diversity and realism while reducing time and cost.


<details>
  <summary>Details</summary>
Motivation: Addressing the prohibitive time and cost of using LLMs for individual record generation in synthetic data creation.

Method: Leverages LLMs to infer field distributions and generate reusable sampling scripts, classifying fields into numerical, categorical, or free-text types.

Result: Outperforms traditional methods in diversity and realism, reducing high-volume synthetic data generation burdens.

Conclusion: The approach accelerates testing in production pipelines, aiding scalable, cost-effective synthetic data solutions.

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


### [181] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: GUI-G² introduces Gaussian rewards for GUI grounding, improving accuracy and robustness over binary rewards.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning uses sparse binary rewards, ignoring the continuous nature of spatial interactions. Human clicking behavior inspires Gaussian modeling.

Method: GUI-G² uses Gaussian point rewards and coverage rewards, with adaptive variance for element scales.

Result: Outperforms UI-TARS-72B by 24.7% on ScreenSpot-Pro, showing better robustness and generalization.

Conclusion: GUI-G² transforms GUI grounding into dense continuous optimization, setting a new paradigm for spatial reasoning.

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [182] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: The paper proposes a Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM) to address classification disparities in graph node classification, achieving balanced accuracy across categories.


<details>
  <summary>Details</summary>
Motivation: Significant classification difficulty disparities were observed in the PubMed citation network dataset, with Category 2 underperforming in traditional GCN.

Method: WR-EFM trains specialized GNN models for Categories 0/1 and Multi-hop GAT for Category 2, using WR distance to optimize representation similarity and an adaptive fusion strategy for dynamic weighting.

Result: WR-EFM achieves balanced accuracies (77.8%, 78.0%, 79.9%) and reduces the coefficient of variation by 77.6% compared to GCN, with a 5.5% improvement for Category 2.

Conclusion: WR-EFM provides a novel paradigm for class-imbalanced graph classification, demonstrating effectiveness in capturing complex patterns. The project is open-sourced for community use.

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [183] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: Diffusion models outperform autoregressive (AR) models in data-scarce settings due to better data utilization and implicit augmentation.


<details>
  <summary>Details</summary>
Motivation: To explore the advantages of diffusion-based language models over AR models, especially in data-constrained scenarios.

Method: Systematic study of masked diffusion models in data-constrained settings, comparing their performance with AR models.

Result: Diffusion models achieve lower validation loss and better downstream performance when compute is abundant but data is scarce.

Conclusion: Diffusion models are a strong alternative to AR models when data is limited, offering superior performance through implicit augmentation.

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


### [184] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: CSE-FSL is a novel federated split learning method that reduces communication and storage costs by using an auxiliary network for local client updates and minimizing data transmission.


<details>
  <summary>Details</summary>
Motivation: Address the high communication overhead and storage requirements in federated split learning (FSL) by proposing a more efficient solution.

Method: Introduces CSE-FSL, which uses an auxiliary network for local client updates, maintains a single server model, and transmits smashed data selectively.

Result: Theoretical convergence is proven, and experiments show significant communication reduction compared to existing FSL methods.

Conclusion: CSE-FSL effectively reduces communication and storage costs while maintaining performance in federated learning tasks.

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [185] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: A hybrid CNN-LSTM-attention-Adaboost model with a multi-strategy improved snake-herd optimization (SO) algorithm is proposed for 4D trajectory prediction, outperforming traditional optimizers and improving accuracy by 39.89%.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in medium- and long-term 4D trajectory prediction models.

Method: Combines CNN for spatial features, LSTM for temporal features, attention for global features, and Adaboost for weak learners. Uses SO algorithm for hyperparameter optimization.

Result: Outperforms traditional optimizers (particle swarm, whale, gray wolf) and achieves 39.89% higher accuracy.

Conclusion: The proposed SO-CLA-Adaboost model is effective for large-scale high-dimensional trajectory data prediction.

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [186] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: The paper introduces a method to optimize canary sets for black-box privacy auditing of DP-SGD, improving empirical lower bounds on privacy parameters by over 2x in some cases.


<details>
  <summary>Details</summary>
Motivation: To enhance privacy auditing by optimizing the canary set used in membership inference attacks, thereby providing tighter lower bounds on the privacy parameters of differentially private learning algorithms.

Method: Leverages metagradient optimization to optimize the canary set for auditing DP-SGD, improving the auditor's success rate in membership inference attacks.

Result: Empirical evaluation shows a 2x improvement in lower bounds for privacy parameters in certain cases, with the method being transferable and efficient across model sizes.

Conclusion: Optimized canary sets significantly improve privacy auditing effectiveness for DP-SGD, demonstrating transferability and efficiency.

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [187] [Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence](https://arxiv.org/abs/2507.14658)
*Faizan Contractor,Li Li,Ranwa Al Mallah*

Main category: cs.MA

TL;DR: Proposes a game design for cooperative multi-agent reinforcement learning in cyber defense, enabling agents to communicate and defend against threats using the Differentiable Inter Agent Learning algorithm.


<details>
  <summary>Details</summary>
Motivation: Current methods limit coordinated effects due to independent agent actions; communication can enhance decision-making in cyber defense.

Method: Agents train in the Cyber Operations Research Gym using the Differentiable Inter Agent Learning algorithm to learn tactical policies and minimal-cost communication.

Result: Agents learn human-expert-like tactical policies and efficient communication strategies to avert cyber threats.

Conclusion: The approach improves coordinated defense in partially observable environments by integrating communication into agent training.

Abstract: Popular methods in cooperative Multi-Agent Reinforcement Learning with
partially observable environments typically allow agents to act independently
during execution, which may limit the coordinated effect of the trained
policies. However, by sharing information such as known or suspected ongoing
threats, effective communication can lead to improved decision-making in the
cyber battle space. We propose a game design where defender agents learn to
communicate and defend against imminent cyber threats by playing training games
in the Cyber Operations Research Gym, using the Differentiable Inter Agent
Learning algorithm adapted to the cyber operational environment. The tactical
policies learned by these autonomous agents are akin to those of human experts
during incident responses to avert cyber threats. In addition, the agents
simultaneously learn minimal cost communication messages while learning their
defence tactical policies.

</details>


### [188] [LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading](https://arxiv.org/abs/2507.14995)
*Chengwei Lou,Zekai Jin,Wei Tang,Guangfei Geng,Jin Yang,Lu Zhang*

Main category: cs.MA

TL;DR: The paper proposes an LLM-MARL framework for real-time P2P electricity trading, addressing challenges like prosumer limitations and grid security. It uses LLMs as experts to guide MARL, achieving better economic and grid performance than baselines.


<details>
  <summary>Details</summary>
Motivation: To address scaling challenges in P2P electricity markets, such as diverse prosumer needs and lack of expert guidance, by integrating LLMs with MARL.

Method: An LLM-MARL framework with imitation learning under CTDE, featuring a differential attention-based critic network for improved convergence.

Result: LLM-generated strategies replace human experts effectively, reducing economic costs and voltage violations while maintaining stability.

Conclusion: The framework successfully bridges expert knowledge with agent learning, offering a robust solution for real-time P2P market decision-making.

Abstract: Real-time peer-to-peer (P2P) electricity markets dynamically adapt to
fluctuations in renewable energy and variations in demand, maximizing economic
benefits through instantaneous price responses while enhancing grid
flexibility. However, scaling expert guidance for massive personalized
prosumers poses critical challenges, including diverse decision-making demands
and lack of customized modeling frameworks. This paper proposed an integrated
large language model-multi-agent reinforcement learning (LLM-MARL) framework
for real-time P2P energy trading to address challenges such as the limited
technical capability of prosumers, the lack of expert experience, and security
issues of distribution networks. LLMs are introduced as experts to generate
personalized strategy, guiding MARL under the centralized training with
decentralized execution (CTDE) paradigm through imitation learning. A
differential attention-based critic network is designed to enhance convergence
performance. Experimental results demonstrate that LLM generated strategies
effectively substitute human experts. The proposed multi-agent imitation
learning algorithms achieve significantly lower economic costs and voltage
violation rates on test sets compared to baselines algorithms, while
maintaining robust stability. This work provides an effective solution for
real-time P2P electricity market decision-making by bridging expert knowledge
with agent learning.

</details>


### [189] [EduThink4AI: Translating Educational Critical Thinking into Multi-Agent LLM Systems](https://arxiv.org/abs/2507.15015)
*Xinmeng Hou,Zhouquan Lu,Wenli Chen,Hai Hu,Qing Guo*

Main category: cs.MA

TL;DR: EDU-Prompting, a multi-agent framework, enhances LLM-based educational tutoring by improving critical thinking and reducing biases in responses.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based educational systems lack in promoting critical thinking and are prone to biases and factual inaccuracies.

Method: Proposes EDU-Prompting, a multi-agent framework integrating educational critical thinking theories with LLM design.

Result: Significantly improves content truthfulness and logical soundness in AI-generated educational responses.

Conclusion: EDU-Prompting offers a modular solution to enhance critical thinking in educational LLM applications without major system changes.

Abstract: Large language models (LLMs) have demonstrated significant potential as
educational tutoring agents, capable of tailoring hints, orchestrating lessons,
and grading with near-human finesse across various academic domains. However,
current LLM-based educational systems exhibit critical limitations in promoting
genuine critical thinking, failing on over one-third of multi-hop questions
with counterfactual premises, and remaining vulnerable to adversarial prompts
that trigger biased or factually incorrect responses. To address these gaps, we
propose EDU-Prompting, a novel multi-agent framework that bridges established
educational critical thinking theories with LLM agent design to generate
critical, bias-aware explanations while fostering diverse perspectives. Our
systematic evaluation across theoretical benchmarks and practical college-level
critical writing scenarios demonstrates that EDU-Prompting significantly
enhances both content truthfulness and logical soundness in AI-generated
educational responses. The framework's modular design enables seamless
integration into existing prompting frameworks and educational applications,
allowing practitioners to directly incorporate critical thinking catalysts that
promote analytical reasoning and introduce multiple perspectives without
requiring extensive system modifications.

</details>


### [190] [LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra](https://arxiv.org/abs/2507.15815)
*Seth Karten,Wenzhe Li,Zihan Ding,Samuel Kleiner,Yu Bai,Chi Jin*

Main category: cs.MA

TL;DR: The LLM Economist framework uses agent-based modeling to design and evaluate economic policies, combining bounded rational worker agents and a planner agent for tax policy optimization, demonstrating improved social welfare.


<details>
  <summary>Details</summary>
Motivation: To create a credible framework for fiscal experimentation by modeling hierarchical decision-making in economic systems using large language models.

Method: Uses persona-conditioned worker agents and a planner agent with in-context reinforcement learning to design tax policies, tested with up to 100 interacting agents.

Result: The planner achieves near Stackelberg equilibria, improving social welfare over Saez solutions, with further gains under decentralized governance via voting.

Conclusion: LLM-based agents can effectively model, simulate, and govern complex economic systems, offering a scalable test bed for policy evaluation.

Abstract: We present the LLM Economist, a novel framework that uses agent-based
modeling to design and assess economic policies in strategic environments with
hierarchical decision-making. At the lower level, bounded rational worker
agents -- instantiated as persona-conditioned prompts sampled from U.S.
Census-calibrated income and demographic statistics -- choose labor supply to
maximize text-based utility functions learned in-context. At the upper level, a
planner agent employs in-context reinforcement learning to propose
piecewise-linear marginal tax schedules anchored to the current U.S. federal
brackets. This construction endows economic simulacra with three capabilities
requisite for credible fiscal experimentation: (i) optimization of
heterogeneous utilities, (ii) principled generation of large, demographically
realistic agent populations, and (iii) mechanism design -- the ultimate nudging
problem -- expressed entirely in natural language. Experiments with populations
of up to one hundred interacting agents show that the planner converges near
Stackelberg equilibria that improve aggregate social welfare relative to Saez
solutions, while a periodic, persona-level voting procedure furthers these
gains under decentralized governance. These results demonstrate that large
language model-based agents can jointly model, simulate, and govern complex
economic systems, providing a tractable test bed for policy evaluation at the
societal scale to help build better civilizations.

</details>
