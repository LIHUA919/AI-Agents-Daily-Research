<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT improves traffic management by using a graph-based architecture for parallel task execution, reducing token usage and latency compared to TrafficGPT.


<details>
  <summary>Details</summary>
Motivation: Current chain-based systems like TrafficGPT are inefficient due to sequential execution, high token usage, and poor scalability.

Method: Proposes GraphTrafficGPT, a graph-based architecture with a Brain Agent for task decomposition, dependency graph construction, and coordination of specialized agents.

Result: Reduces token consumption by 50.2%, latency by 19.0%, and improves multi-query efficiency by 23.0%.

Conclusion: GraphTrafficGPT offers a scalable, efficient solution for LLM-driven traffic management.

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [2] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette is a framework that decomposes preferences into interpretable attributes, outperforming GPT-4o by 46.6% in prediction accuracy while revealing community-specific value profiles.


<details>
  <summary>Details</summary>
Motivation: Current preference models treat human judgment as a black box, lacking insight into underlying reasons for preferences. PrefPalette aims to address this by modeling attribute-mediated decision-making.

Method: PrefPalette uses (1) scalable counterfactual attribute synthesis to isolate attribute effects and (2) attention-based preference modeling to dynamically weight attributes for different social communities.

Result: PrefPalette outperforms GPT-4o by 46.6% in prediction accuracy and reveals intuitive community-specific profiles (e.g., scholarly communities prioritize verbosity, conflict-oriented communities value sarcasm).

Conclusion: PrefPalette advances preference modeling by combining superior accuracy with interpretability, paving the way for more trustworthy, value-aware personalized AI systems.

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [3] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: A hybrid approach combines LLMs with symbolic systems (Prolog) for controlled, transparent, and reliable expert systems, validated by humans.


<details>
  <summary>Details</summary>
Motivation: Address LLM disadvantages like hallucinations and unverifiable facts by ensuring controlled, interpretable, and scalable knowledge extraction.

Method: Domain-limited, prompt-based extraction to generate symbolic Prolog representations, validated by experts.

Result: Strong adherence to facts and semantic coherence in knowledge bases, demonstrated with Claude Sonnet 3.7 and GPT-4.1.

Conclusion: The hybrid solution ensures dependable AI applications in sensitive domains by balancing LLM recall with symbolic precision.

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [4] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: The paper argues for modeling entities and relations over pixels and words, highlighting the gap in AI's focus despite valuable relational data in industries.


<details>
  <summary>Details</summary>
Motivation: Current AI focuses on pixels and words, but real-world data is relational (e.g., spreadsheets, databases). The paper questions why relational learning isn't dominant.

Method: The paper critiques the neglect of relational learning and identifies limitations in current approaches.

Result: Relational learning remains niche despite its potential, restricted by certain limitations.

Conclusion: The paper calls for advancements to elevate relational learning's prominence in AI.

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [5] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG is a dual-graph RAG system for multi-hop question answering in regulatory texts, combining linguistic and structural modeling to outperform traditional methods.


<details>
  <summary>Details</summary>
Motivation: The complexity of regulatory texts and multi-hop queries challenge traditional RAG systems, necessitating a hybrid approach.

Method: BifrostRAG uses an Entity Network Graph for linguistic relationships and a Document Navigator Graph for document structure, enabling hybrid retrieval.

Result: Achieves 92.8% precision, 85.5% recall, and 87.3% F1 score, outperforming vector-only and graph-only RAG baselines.

Conclusion: BifrostRAG is a robust solution for compliance checking, offering a transferable blueprint for complex technical documents.

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [6] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: The paper explores using final answers for error diagnosis in intelligent tutoring systems to mitigate combinatorial explosion in stepwise tasks, showing promising results in diagnosing student errors.


<details>
  <summary>Details</summary>
Motivation: Combinatorial explosion in diagnosing student errors for stepwise tasks makes error diagnosis challenging. Using final answers can simplify this process.

Method: The study designs a service for buggy rule diagnosis based on final answers and validates it using a dataset of student steps in solving quadratic equations.

Result: Final answer evaluation diagnosed 29.4% of previously undiagnosed steps, with 97% alignment with teacher diagnoses in a subset.

Conclusion: The approach shows potential for further exploration in intelligent tutoring systems for error diagnosis.

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [7] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: A hybrid approach combining model tracing and constraint-based modeling for diagnosing student input in stepwise tasks, validated with high alignment to teacher diagnoses.


<details>
  <summary>Details</summary>
Motivation: To improve student input diagnosis by merging the strengths of model tracing (tracking steps) and constraint-based modeling (handling combined steps).

Method: Define constraints as shared properties between student input and strategy steps, enabling diagnosis of deviations even in combined steps.

Result: System diagnoses aligned perfectly with teacher coding in 140 student steps (n=2136 dataset).

Conclusion: The hybrid approach effectively diagnoses multistep strategies, validated by teacher agreement.

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [8] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM is a novel system for generating and summarizing activity logs by integrating contextual data from smartphones and smartwatches, outperforming SOTA methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing activity log generation methods lack accuracy, efficiency, and semantic richness, despite the potential of LLMs.

Method: DailyLLM uses a lightweight LLM framework with structured prompting and efficient feature extraction to integrate location, motion, environment, and physiology data.

Result: DailyLLM achieves a 17% improvement in BERTScore precision and 10x faster inference speed compared to a 70B-parameter SOTA baseline.

Conclusion: DailyLLM demonstrates superior performance in log generation and summarization, making it practical for deployment on personal devices.

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [9] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView is an ontology viewer offering intuitive visualization of ontology structures, including General Concept Inclusions (GCI), and provides simplified views to avoid information overload.


<details>
  <summary>Details</summary>
Motivation: Existing ontology visualization tools fail to represent structures meaningfully, hindering comprehension of large ontologies.

Method: OntView uses a DL reasoner and follows a 'What you see is what you meant' paradigm, visualizing inferred knowledge and offering simplified views through summaries, focused TBox elements, and dynamic branch hiding.

Result: OntView effectively visualizes complex ontologies, including GCIs, and reduces information overload with customizable views.

Conclusion: OntView addresses visualization challenges in ontologies, providing a user-friendly, open-source tool for the community.

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [10] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: A hybrid architecture combines heuristic extraction, semantic activation, and compositional synthesis for strategic reasoning, fusing conflicting heuristics into coherent narratives.


<details>
  <summary>Details</summary>
Motivation: To improve strategic reasoning by integrating diverse heuristics and leveraging semantic interdependence, inspired by quantum cognition.

Method: Combines heuristic extraction, semantic activation, and compositional synthesis, guided by semantic interaction modeling and rhetorical framing.

Result: Demonstrated via a Meta vs. FTC case study with preliminary validation using semantic metrics.

Conclusion: The framework shows promise but has limitations; extensions like dynamic interference tuning are suggested.

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [11] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE is a lightweight framework for temporal link prediction in dynamic graphs, combining short-term recency and long-term structural patterns for efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing Temporal Graph Neural Networks (T-GNNs) face scalability and efficiency issues due to high computational overhead.

Method: EAGLE integrates a time-aware module for recent neighbor aggregation and a structure-aware module using temporal personalized PageRank, with adaptive weighting.

Result: EAGLE outperforms state-of-the-art T-GNNs, achieving a 50x speedup over transformer-based T-GNNs.

Conclusion: EAGLE offers an efficient and scalable solution for temporal link prediction without complex architectures.

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [12] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: A causal knowledge transfer framework for MARL enables agents to share compact causal representations, improving adaptation in non-stationary environments without retraining.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of knowledge transfer in MARL for non-stationary environments with changing goals, where traditional methods fail to generalize.

Method: Introduces a causal knowledge transfer framework where agents model collisions as causal interventions, sharing recovery action macros online.

Result: Agents bridged half the gap between random exploration and full retraining, with effectiveness depending on environment complexity and goal heterogeneity.

Conclusion: Causal knowledge transfer enhances MARL adaptability, offering a scalable solution for dynamic environments.

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [13] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: A model-agnostic latent-space ideation framework is proposed to enhance AI creativity by navigating embedding spaces, avoiding brittle domain-specific heuristics.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) often lack novelty and relevance in idea generation, relying on training patterns and requiring extensive prompt engineering.

Method: The paper introduces a latent-space ideation framework that navigates continuous embedding spaces, eliminating the need for handcrafted rules.

Result: Preliminary results show the framework's potential as a general-purpose co-ideator for human-AI collaboration.

Conclusion: The proposed framework offers scalable, controlled creativity adaptable to various domains and tasks, marking a step forward in AI-assisted ideation.

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [14] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: The paper proposes ADPC, a visual-language causal intervention framework, to improve Alzheimer's Disease (AD) diagnosis by addressing confounders in multimodal data.


<details>
  <summary>Details</summary>
Motivation: Early AD diagnosis is challenging due to confounders like selection bias and complex variable relationships.

Method: ADPC uses LLM for structured clinical data summarization and integrates MRI/fMRI with textual data for classification (CN/MCI/AD), applying causal intervention to eliminate confounders.

Result: ADPC achieves SOTA performance in distinguishing CN/MCI/AD cases.

Conclusion: The study highlights the potential of causal reasoning in multimodal learning for neurological disease diagnosis.

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [15] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: A novel temporal and constraint-based extension of the logic of Here-and-There is introduced for nonmonotonic temporal reasoning in ASP, combining linear-time logic and constraint integration.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in reasoning about dynamic systems with fine-grained temporal and numeric resolution in logic-based approaches like ASP.

Method: Combines linear-time logic of Here-and-There (for nonmonotonic temporal reasoning) and logic of Here-and-There with constraints (for numeric constraint integration).

Result: An expressive system for nonmonotonic temporal reasoning with constraints, tailored for ASP.

Conclusion: Establishes a foundational logical framework for high-resolution dynamic systems within ASP.

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [16] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA is a new OM framework using LLMs and RAG to improve semantic context in ontology matching, outperforming existing systems with optimized efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing OM systems rely on rigid rules or specialized models, lacking adaptability. KROMA aims to enhance flexibility and performance.

Method: KROMA combines LLMs with RAG, using bisimilarity-based matching and lightweight ontology refinement to optimize efficiency.

Result: Experiments show KROMA outperforms classic and LLM-based OM systems while maintaining low communication overhead.

Conclusion: The study demonstrates the effectiveness of KROMA's techniques (knowledge retrieval, prompt enrichment, refinement) for scalable OM.

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [17] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: The paper introduces Glucose-ML, a collection of 10 publicly available diabetes datasets, to address barriers in AI development for diabetes management. It includes over 300,000 days of CGM data and provides benchmarks for blood glucose prediction.


<details>
  <summary>Details</summary>
Motivation: Access to large, high-quality datasets is a barrier in developing robust AI solutions for diabetes management. The authors aim to accelerate transparent and reproducible AI development by providing a curated dataset collection.

Method: The authors compile 10 diabetes datasets (Glucose-ML) with 38 million glucose samples from 2500+ people. They conduct a comparative analysis and a case study on blood glucose prediction to benchmark performance across datasets.

Result: The study shows that AI algorithms yield significantly different prediction results across datasets, highlighting the importance of dataset selection for robust AI solutions.

Conclusion: The Glucose-ML collection and benchmarks support researchers in developing robust AI solutions for diabetes. The authors provide open access to datasets and code to foster reproducibility and innovation.

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [18] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: G-AI-HMS uses Generative AI to improve human motion simulation by integrating text-to-text and text-to-motion models, outperforming human descriptions in accuracy and alignment.


<details>
  <summary>Details</summary>
Motivation: Existing human motion simulation methods lack fidelity. G-AI-HMS aims to enhance simulation quality for industrial tasks using AI.

Method: Combines Large Language Models with MotionGPT for motion-aware task descriptions and validates motions using computer vision and posture estimation.

Result: AI-enhanced motions outperformed human descriptions in most tasks, reducing joint error and temporal misalignment significantly.

Conclusion: G-AI-HMS demonstrates the potential of Generative AI to improve motion simulation fidelity for industrial applications.

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [19] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: The study explores using LLMs to interpret NDE contour maps for bridge inspections, showing improved efficiency and accuracy, with ChatGPT-4 and Claude 3.5 Sonnet excelling in summaries.


<details>
  <summary>Details</summary>
Motivation: Bridge maintenance relies on NDE data, but manual interpretation is slow and expertise-heavy. LLMs offer automation potential to streamline this process.

Method: Several LLMs were tested with tailored prompts to interpret NDE contour maps, evaluating their ability to describe, identify defects, and recommend actions.

Result: Four of nine LLMs excelled in detailed descriptions, with ChatGPT-4 and Claude 3.5 Sonnet producing the best summaries.

Conclusion: LLMs can enhance bridge inspection efficiency and accuracy, offering a promising tool for faster decision-making in infrastructure management.

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [20] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1 is an automated reinforcement learning framework for CUDA optimization, achieving significant speedups across various GPU architectures and uncovering key optimization principles.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in GPU computing demand, driven by Large Language Models, necessitates automated CUDA optimization strategies due to the low success rates of current models.

Method: CUDA-L1 uses reinforcement learning to optimize CUDA kernels, trained on NVIDIA A100, and tested across multiple GPU architectures.

Result: Achieves average speedups of x17.7 on A100, with peak speedups up to x449, and demonstrates portability across other GPUs (e.g., x19.0 on RTX 3090). Discovers optimization techniques and principles.

Conclusion: CUDA-L1 shows RL can transform LLMs into effective CUDA optimizers without human expertise, extending reasoning to new kernels and promising improved GPU efficiency.

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Scalable Submodular Policy Optimization via Pruned Submodularity Graph](https://arxiv.org/abs/2507.13834)
*Aditi Anand,Suman Banerjee,Dildar Ali*

Main category: cs.LG

TL;DR: The paper introduces a variant of RL with submodular rewards, proposing a pruned submodularity graph-based method for efficient optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional RL assumes additive rewards, but many real-world problems (e.g., path planning) exhibit diminishing returns, modeled as submodular functions.

Method: A pruned submodularity graph-based approach is developed to maximize submodular rewards with provable approximation guarantees.

Result: Experiments show the proposed method outperforms baselines, yielding higher rewards.

Conclusion: The approach efficiently solves RL with submodular rewards, offering theoretical and practical advantages.

Abstract: In Reinforcement Learning (abbreviated as RL), an agent interacts with the
environment via a set of possible actions, and a reward is generated from some
unknown distribution. The task here is to find an optimal set of actions such
that the reward after a certain time step gets maximized. In a traditional
setup, the reward function in an RL Problem is considered additive. However, in
reality, there exist many problems, including path planning, coverage control,
etc., the reward function follows the diminishing return, which can be modeled
as a submodular function. In this paper, we study a variant of the RL Problem
where the reward function is submodular, and our objective is to find an
optimal policy such that this reward function gets maximized. We have proposed
a pruned submodularity graph-based approach that provides a provably
approximate solution in a feasible computation time. The proposed approach has
been analyzed to understand its time and space requirements as well as a
performance guarantee. We have experimented with a benchmark agent-environment
setup, which has been used for similar previous studies, and the results are
reported. From the results, we observe that the policy obtained by our proposed
approach leads to more reward than the baseline methods.

</details>


### [22] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

Main category: cs.LG

TL;DR: The paper explores the transformer architecture from a physical perspective, modeling it as an open quantum system in Fock space to bridge theoretical gaps.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of why the transformer architecture works, by examining it from a physical standpoint.

Method: Constructs physical models in Fock space over the Hilbert space of tokens, treating large language models as open quantum systems.

Result: Provides physical models that underlie the transformer architecture for large language models.

Conclusion: The study offers a novel physical perspective on transformers, enhancing theoretical understanding of their functionality.

Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [23] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: The paper introduces pluralistic alignment in text-to-image (T2I) models to address diverse human values, using a novel dataset (DIVE) and demographic insights for better alignment.


<details>
  <summary>Details</summary>
Motivation: Current T2I models often misalign with diverse human experiences, necessitating a pluralistic approach to account for conflicting values.

Method: The work introduces the DIVE dataset, uses intersectional human raters for feedback, and analyzes demographic influences on harm perception.

Result: Demographics significantly impact harm perception, diverging from conventional evaluations, and the DIVE dataset enables nuanced alignment.

Conclusion: The research provides tools for equitable T2I systems, emphasizing data collection, LLM judgments, and model steerability for diverse perspectives.

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [24] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa,Jarek Duda*

Main category: cs.LG

TL;DR: The paper highlights the benefits of CDF normalization (from copula theory) in machine learning, demonstrating improved predictions in Kolmogorov-Arnold Networks (KANs) and offering interpretability through mixed moments.


<details>
  <summary>Details</summary>
Motivation: To introduce CDF normalization, a method popular in finance but underutilized in machine learning, and showcase its advantages over traditional normalization techniques.

Method: Replace traditional rescaling with CDF normalization in Kolmogorov-Arnold Networks (KANs) and analyze its impact on predictions and interpretability.

Result: CDF normalization improves predictions in KANs (e.g., over Legendre-KAN) and provides interpretable neuron weights as mixed moments, enabling probabilistic propagation.

Conclusion: CDF normalization is a promising alternative to traditional methods, offering better performance and interpretability in machine learning models like KANs.

Abstract: Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [25] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri,Zehui Hua,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: The paper introduces selective embedding, a data loading strategy that alternates short segments from multiple sources in a single channel to improve deep learning performance under nonstationary conditions.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with nonstationary conditions and dissimilar domains, especially in time-domain data. Conventional methods limit generalization or increase costs.

Method: Selective embedding alternates short data segments from multiple sources within one input channel, inspired by human-like information processing.

Result: Validated on six datasets, it achieves high accuracy across architectures and reduces training time, proving effective for multi-source systems.

Conclusion: Selective embedding offers a scalable, efficient solution for real-world applications requiring robustness and adaptability.

Abstract: Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [26] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin,Igor Hromov,Stanislav Chumakov,Mile Mitrovic,Dmitry Simakov,Nikolay O. Nikitin,Andrey V. Savchenko*

Main category: cs.LG

TL;DR: LightAutoDS-Tab is a multi-AutoML system for tabular data, combining LLM-based code generation with AutoML tools, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: AutoML's efficiency is limited by tool dependency; LightAutoDS-Tab aims to enhance flexibility and robustness in pipeline design.

Method: Integrates LLM-based code generation with multiple AutoML tools for tabular data tasks.

Result: Outperforms state-of-the-art open-source solutions on Kaggle data science tasks.

Conclusion: LightAutoDS-Tab improves AutoML performance and is open-source.

Abstract: AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [27] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: Gauge Flow Models, a new class of Generative Flow Models, outperform traditional Flow Models by incorporating a learnable Gauge Field in the Flow ODE.


<details>
  <summary>Details</summary>
Motivation: To improve generative modeling performance by integrating a learnable Gauge Field into Flow ODEs.

Method: Introduces Gauge Flow Models with a mathematical framework and tests them using Flow Matching on Gaussian Mixture Models.

Result: Gauge Flow Models show significantly better performance than traditional Flow Models, even when smaller in size.

Conclusion: Gauge Flow Models offer promising improvements in generative tasks, with potential for broader applications.

Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [28] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi,Bernardo P. Ferreira,Miguel A. Bessa*

Main category: cs.LG

TL;DR: A hierarchical, data-driven learning method generalizes multi-fidelity data handling, quantifying epistemic and aleatoric uncertainty, and applies to diverse scenarios, from simple neural networks to complex Bayesian recurrent networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of handling multi-fidelity data while distinguishing and quantifying epistemic (model) and aleatoric (data noise) uncertainty in learning tasks.

Method: Proposes a hierarchical, adaptable framework for multi-fidelity data, extending from deterministic neural networks to Bayesian recurrent neural networks with variance estimation.

Result: Accurately predicts responses, quantifies model error, and identifies noise distributions, demonstrating versatility in various data-driven constitutive modeling scenarios.

Conclusion: The method's adaptability and accuracy open doors for applications in scientific and engineering domains, particularly in uncertainty-aware design and analysis.

Abstract: Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [29] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga,Thomas Guyet,Violaine Antoine*

Main category: cs.LG

TL;DR: The paper introduces Soft-ECM, a belief function-based clustering algorithm for complex data, addressing limitations of existing methods by using semi-metrics.


<details>
  <summary>Details</summary>
Motivation: Existing belief function-based clustering algorithms cannot handle complex data like mixed or non-tabular data, necessitating a new approach.

Method: The authors reformulate the Evidential C-Means (ECM) problem and propose Soft-ECM, which uses semi-metrics to position centroids for imprecise clusters.

Result: Soft-ECM performs comparably to fuzzy clustering on numerical data and effectively handles mixed data and time series using semi-metrics like DTW.

Conclusion: Soft-ECM is a versatile solution for clustering complex data, bridging gaps in belief function-based methods.

Abstract: Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [30] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson,Dewi Gould,Richard Everson,George De Ath,Nick Pepper*

Main category: cs.LG

TL;DR: The paper introduces an interpretable Graph Neural Network (GNN) framework to predict ATCO task demand by analyzing aircraft interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing complexity metrics fail to capture nuanced operational drivers in crowded airspace, necessitating a better tool for real-time task demand assessment.

Method: An attention-based GNN predicts upcoming clearances from static traffic scenarios, with a per-aircraft task demand score derived via systematic ablation.

Result: The framework outperforms ATCO-inspired heuristics and established baselines, providing reliable complexity estimation and attributing demand to specific aircraft.

Conclusion: The tool offers a novel way to analyze complexity drivers, useful for controller training and airspace redesign.

Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [31] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: A cross-modal self-supervised pretraining approach improves generalizability in Human Activity Recognition (HAR) using IMU-video data, outperforming existing methods on out-of-distribution datasets.


<details>
  <summary>Details</summary>
Motivation: To address the lack of generalizability in current HAR methods for movement disorder patients by leveraging unlabeled IMU-video data.

Method: Proposes a cross-modal self-supervised pretraining approach using large-scale unlabeled IMU-video data.

Result: Outperforms state-of-the-art IMU-video and IMU-only pretraining in zero-shot and few-shot evaluations, including on Parkinson's disease patient data.

Conclusion: Cross-modal pretraining is effective for learning generalizable representations in dynamic data like IMU signals.

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [32] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker,Ali Mesbah*

Main category: cs.LG

TL;DR: The paper advocates for model-based agents as an alternative to model-free RL, highlighting their benefits in safety, interpretability, and sample efficiency while addressing challenges like model mismatch.


<details>
  <summary>Details</summary>
Motivation: Model-free RL, despite its success, suffers from sample inefficiency, unsafe learning, and limited interpretability due to reliance on deep neural networks.

Method: Proposes model-based agents (e.g., model predictive control) that leverage adaptable models of system dynamics, cost, and constraints. Combines these with model-free RL to address model mismatch.

Result: Model-based agents offer safer, more interpretable, and sample-efficient learning, with potential synergies when combined with model-free RL.

Conclusion: The interplay between model-based and model-free RL holds promise for developing efficient, safe, and interpretable decision-making agents.

Abstract: Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [33] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek,Dawid Płudowski,Piotr Wilczyński,Przemysław Biecek,Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: The Kaggle competition 'Fake or Real' challenges participants to detect maliciously modified outputs from Large Language Models (LLMs), addressing AI security threats like data poisoning and overreliance.


<details>
  <summary>Details</summary>
Motivation: The competition aims to tackle under-researched AI security threats, specifically data poisoning and overreliance in LLMs, identified in the ESA-funded 'Assurance for Space Domain AI Applications' project.

Method: Participants must develop or adapt techniques to distinguish between genuine and maliciously altered LLM outputs.

Result: The competition seeks innovative solutions to a novel problem, fostering research in AI security.

Conclusion: The initiative highlights the need for robust methods to detect manipulated AI outputs, advancing AI assurance in critical domains.

Abstract: The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [34] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang,Hidenori Tanaka,Wei Hu*

Main category: cs.LG

TL;DR: The paper explains how in-context learning (ICL) in LLMs achieves superior performance through a double convergence framework, leading to smooth representations and robustness to noise.


<details>
  <summary>Details</summary>
Motivation: To uncover the mechanisms behind ICL's ability to surpass pretraining performance by internalizing data-generating processes.

Method: Introduces a unified framework of double convergence (convergence over context and layers) and analyzes its implicit bias towards smooth representations.

Result: Proves and empirically verifies that double convergence leads to smooth representations, explains geometric distortions, and predicts noise robustness.

Conclusion: Provides a theoretical foundation for ICL, explaining its mechanisms and suggesting broader applicability.

Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [35] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili,Carlos J. Fernandez-Candel,Matías Pérez Paredes*

Main category: cs.LG

TL;DR: The paper introduces the Acoustic Index, an AI-derived echocardiographic parameter, to overcome limitations of traditional measures like EF and GLS. It combines EDMD and neural networks for robust cardiac dysfunction detection, achieving high AUC (0.89) in validation.


<details>
  <summary>Details</summary>
Motivation: Traditional parameters (EF, GLS) lack early detection and reproducibility. The need for operator-independent, interpretable metrics drives the development of the Acoustic Index.

Method: Uses Extended Dynamic Mode Decomposition (EDMD) and a hybrid neural network with clinical metadata. Spatiotemporal dynamics and attention mechanisms fuse data into a continuous risk score (0-1).

Result: Validated on 736 patients, the Acoustic Index achieved AUC 0.89, with sensitivity and specificity >0.8 in cross-validation. Stable performance across thresholds.

Conclusion: The Acoustic Index is a promising, interpretable AI biomarker for early cardiac dysfunction detection. Future work includes external validation and disease-specific adaptations.

Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [36] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang,Steven Klee,Alexis Roos*

Main category: cs.LG

TL;DR: The paper introduces two metrics—spectral predictability score and largest Lyapunov exponent—to assess time series forecastability before model development, showing their correlation with actual forecast performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate the inherent forecastability of time series data before model training, helping practitioners prioritize efforts and set realistic expectations.

Method: Uses spectral predictability score for frequency regularity and Lyapunov exponents for chaos/stability, tested on synthetic and M5 competition datasets.

Result: Metrics effectively reflect inherent forecastability and correlate with model performance, aiding in planning and strategy.

Conclusion: Understanding forecastability beforehand improves resource allocation and expectation setting for time series forecasting.

Abstract: This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [37] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur,Mike Doan,Barak Pearlmutter,Sergey Plis*

Main category: cs.LG

TL;DR: The paper introduces the SELF-Transformer, an encoder layer that iteratively refines its attention weights to boost expressive power without token-level autoregression, achieving up to 20% accuracy gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Transformers with fixed-depth passes are limited in expressive power, and autoregressive methods rely on externalizing intermediate states. The goal is to enhance encoder Transformers without autoregression.

Method: The SELF-Transformer iteratively updates its attention weights internally to a fixed point, scaling computation with input difficulty.

Result: The method yields up to 20% accuracy gains on benchmarks without increasing parameters, showing benefits of input-adaptive alignment.

Conclusion: SELF-Transformers enhance expressive power while maintaining encoder simplicity, offering a balance between iterative reasoning and pure encoder architectures.

Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [38] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou,Erik Hornberger,Pengsheng Guo,Xiyou Zhou,Saiwen Wang,Xin Wang,Yifei He,Xuankai Chang,Rene Rauch,Louis D'hauwe,John Peebles,Alec Doane,Kohen Chia,Jenna Thibodeau,Zi-Yi Dou,Yuanyang Zhang,Ruoming Pang,Reed Li,Zhifeng Chen,Jeremy Warner,Zhaoyang Xu,Sophy Lee,David Mizrahi,Ramsey Tantawi,Chris Chaney,Kelsey Peterson,Jun Qin,Alex Dombrowski,Mira Chiang,Aiswarya Raghavan,Gerard Casamayor,Qibin Chen,Aonan Zhang,Nathalie Tran,Jianyu Wang,Hang Su,Thomas Voice,Alessandro Pappalardo,Brycen Wershing,Prasanth Yadla,Rui Li,Priyal Chhatrapati,Ismael Fernandez,Yusuf Goren,Xin Zheng,Forrest Huang,Tao Lei,Eray Yildiz,Alper Kokmen,Gokul Santhanam,Areeba Kamal,Kaan Elgin,Dian Ang Yap,Jeremy Liu,Peter Gray,Howard Xing,Kieran Liu,Matteo Ronchi,Moritz Schwarzer-Becker,Yun Zhu,Mandana Saebi,Jeremy Snow,David Griffiths,Guillaume Tartavel,Erin Feldman,Simon Lehnerer,Fernando Bermúdez-Medina,Hans Han,Joe Zhou,Xiaoyi Ren,Sujeeth Reddy,Zirui Wang,Tom Gunter,Albert Antony,Yuanzhi Li,John Dennison,Tony Sun,Yena Han,Yi Qin,Sam Davarnia,Jeffrey Bigham,Wayne Shan,Hannah Gillis Coleman,Guillaume Klein,Peng Liu,Muyang Yu,Jack Cackler,Yuan Gao,Crystal Xiao,Binazir Karimzadeh,Zhengdong Zhang,Felix Bai,Albin Madappally Jose,Feng Nan,Nazir Kamaldin,Dong Yin,Hans Hao,Yanchao Sun,Yi Hua,Charles Maalouf,Alex Guillen Garcia,Guoli Yin,Lezhi Li,Mohana Prasad Sathya Moorthy,Hongbin Gao,Jay Tang,Joanna Arreaza-Taylor,Faye Lao,Carina Peng,Josh Shaffer,Dan Masi,Sushma Rao,Tommi Vehvilainen,Senyu Tong,Dongcai Shen,Yang Zhao,Chris Bartels,Peter Fu,Qingqing Cao,Christopher Neubauer,Ethan Li,Mingfei Gao,Rebecca Callahan,Richard Wei,Patrick Dong,Alex Braunstein,Sachin Ravi,Adolfo Lopez Mendez,Kaiwei Huang,Kun Duan,Haoshuo Huang,Rui Qian,Stefano Ligas,Jordan Huffaker,Dongxu Li,Bailin Wang,Nanzhu Wang,Anuva Agarwal,Tait Madsen,Josh Newnham,Abhishek Sharma,Zhile Ren,Deepak Gopinath,Erik Daxberger,Saptarshi Guha,Oron Levy,Jing Lu,Nan Dun,Marc Kirchner,Yinfei Yang,Manjot Bilkhu,Dave Nelson,Anthony Spalvieri-Kruse,Juan Lao Tebar,Yang Xu,Phani Mutyala,Gabriel Jacoby-Cooper,Yingbo Wang,Karla Vega,Vishaal Mahtani,Darren Botten,Eric Wang,Hanli Li,Matthias Paulik,Haoran Yan,Navid Shiee,Yihao Qian,Bugu Wu,Qi Zhu,Ob Adaranijo,Bhuwan Dhingra,Zhe Gan,Nicholas Seidl,Grace Duanmu,Rong Situ,Yiping Ma,Yin Xia,David Riazati,Vasileios Saveris,Anh Nguyen,Michael,Lee,Patrick Sonnenberg,Chinguun Erdenebileg,Yanghao Li,Vivian Ma,James Chou,Isha Garg,Mark Lee,Keen You,Yuhong Li,Ransen Niu,Nandhitha Raghuram,Pulkit Agrawal,Henry Mason,Sumeet Singh,Keyu He,Hong-You Chen,Lucas Guibert,Shiyu Li,Varsha Paidi,Narendran Raghavan,Mingze Xu,Yuli Yang,Sergiu Sima,Irina Belousova,Sprite Chu,Afshin Dehghan,Philipp Dufter,David Haldimann,Zhen Yang,Margit Bowler,Chang Liu,Ying-Chang Cheng,Vivek Rathod,Syd Evans,Wilson Tsao,Dustin Withers,Haitian Sun,Biyao Wang,Peter Grasch,Walker Cheng,Yihao Feng,Vivek Kumar,Frank Chu,Victoria MönchJuan Haladjian,Doug Kang,Jiarui Lu,Ciro Sannino,Max Lam,Floris Weers,Bowen Pan,Kenneth Jung,Dhaval Doshi,Fangping Shi,Olli Saarikivi,Alp Aygar,Josh Elman,Cheng Leong,Eshan Verma,Matthew Lei,Jeff Nichols,Jiulong Shan,Donald Zhang,Lawrence Zhou,Stephen Murphy,Xianzhi Du,Chang Lan,Ankur Jain,Elmira Amirloo,Marcin Eichner,Naomy Sabo,Anupama Mann Anupama,David Qiu,Zhao Meng,Michael FitzMaurice,Peng Zhang,Simon Yeung,Chen Chen,Marco Zuliani,Andrew Hansen,Yang Lu,Brent Ramerth,Ziyi Zhong,Parsa Mazaheri,Matthew Hopkins,Mengyu Li,Simon Wang,David Chen,Farzin Rasteh,Chong Wang,Josh Gardner,Asaf Liberman,Haoxuan You,Andrew Walkingshaw,Xingyu Zhou,Jinhao Lei,Yan Meng,Quentin Keunebroek,Sam Wiseman,Anders Boesen Lindbo Larsen,Yi Zhang,Zaid Ahmed,Haiming Gang,Aaron Franklin,Kelvin Zou,Guillaume Seguin,Jonathan Janke,Rachel Burger,Co Giang,Cheng Shen,Jen Liu,Sanskruti Shah,Xiang Kong,Yiran Fei,TJ Collins,Chen Zhang,Zhiyun Lu,Michael Booker,Qin Ba,Yasutaka Tanaka,Andres Romero Mier Y Teran,Federico Scozzafava,Regan Poston,Jane Li,Eduardo Jimenez,Bas Straathof,Karanjeet Singh,Lindsay Hislop,Rajat Arora,Deepa Seshadri,Boyue Li,Colorado Reed,Zhen Li,TJ Lu,Yi Wang,Kaelen Haag,Nicholas Lusskin,Raunak Sinha,Rahul Nair,Eldon Schoop,Mary Beth Kery,Mehrdad Farajtbar,Brenda Yang,George Horrell,Shiwen Zhao,Dhruti Shah,Cha Chen,Bowen Zhang,Chang Gao,Devi Krishna,Jennifer Mallalieu,Javier Movellan,Di Feng,Emily Zhang,Sam Xu,Junting Pan,Dominik Moritz,Suma Jayaram,Kevin Smith,Dongseong Hwang,Daniel Parilla,Jiaming Hu,You-Cyuan Jhang,Emad Soroush,Fred Hohman,Nan Du,Emma Wang,Sam Dodge,Pragnya Sridhar,Joris Pelemans,Wei Fang,Nina Wenzel,Joseph Yitan Cheng,Hadas Kotek,Chung-Cheng Chiu,Meng Cao,Haijing Fu,Ruixuan Hou,Ke Ye,Diane Zhu,Nikhil Bhendawade,Joseph Astrauskas,Jian Liu,Sai Aitharaju,Wentao Wu,Artsiom Peshko,Hyunjik Kim,Nilesh Shahdadpuri,Andy De Wang,Qi Shan,Piotr Maj,Raul Rea Menacho,Justin Lazarow,Eric Liang Yang,Arsalan Farooq,Donghan Yu,David Güera,Minsik Cho,Kavya Nerella,Yongqiang Wang,Tao Jia,John Park,Jeff Lai,Haotian Zhang,Futang Peng,Daniele Molinari,Aparna Rajamani,Tyler Johnson,Lauren Gardiner,Chao Jia,Violet Yao,Wojciech Kryscinski,Xiujun Li,Shang-Chen Wu*

Main category: cs.LG

TL;DR: Apple introduces two multilingual, multimodal foundation models for on-device and server use, optimized for performance and privacy, surpassing benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance Apple Intelligence features with efficient, high-quality models while prioritizing privacy and responsible AI.

Method: Developed a 3B-parameter on-device model with KV-cache sharing and 2-bit quantization, and a server model using PT-MoE transformer with track parallelism and sparse computation. Trained on multilingual, multimodal data with supervised fine-tuning and reinforcement learning.

Result: Models outperform comparably sized baselines in benchmarks and human evaluations, supporting additional languages, images, and tool calls.

Conclusion: Apple's models advance AI capabilities with a focus on privacy, efficiency, and developer accessibility through a new Swift framework.

Abstract: We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [39] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam,Yanming Wan,Mickel Liu,Jianxun Lian,Natasha Jaques*

Main category: cs.LG

TL;DR: PLUS introduces a framework for personalized LLM responses by learning user-specific summaries to condition reward models, outperforming traditional RLHF and in-context learning.


<details>
  <summary>Details</summary>
Motivation: Current RLHF methods lack personalization, modeling all users with a single reward model, which fails to address individual preferences.

Method: PLUS learns text-based summaries of user preferences and past conversations to condition the reward model, trained via reinforcement learning in an online co-adaptation loop.

Result: PLUS captures meaningful user preferences, works robustly across new users and topics, and enables zero-shot personalization for models like GPT-4.

Conclusion: PLUS offers concise, interpretable user summaries, enhancing transparency and control in LLM alignment.

Abstract: As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [40] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi,Shuhei Goda,Yuta Saito*

Main category: cs.LG

TL;DR: Proposes novel OPE estimators (DiPS and DPR) for matching markets, addressing variance and reward sparsity issues, outperforming conventional methods in offline evaluation and policy learning.


<details>
  <summary>Details</summary>
Motivation: A/B tests are costly for frequent policy updates in matching markets; OPE is needed but suffers from variance and reward sparsity in such settings.

Method: Combines DM, IPS, and DR estimators with intermediate labels for better bias-variance control in matching markets.

Result: Theoretical and empirical validation shows superiority over existing methods in off-policy evaluation and learning.

Conclusion: DiPS and DPR effectively address challenges in matching markets, enabling reliable offline evaluation and policy improvement.

Abstract: Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [41] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li,Yuefei Wang,Xu Xiang,Xue Li,Binyu Zhao,Heyang Gao,Qinyu Zhao,Xi Yu*

Main category: cs.LG

TL;DR: The paper introduces Tri-GFN, a deep clustering framework combining GCN, AE, and Graph Transformer to address challenges in graph data analysis. It outperforms existing methods on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Challenges like over-smoothing and limited performance of Graph Transformer in heterogeneous graph data analysis motivate the development of Tri-GFN.

Method: Tri-GFN integrates GCN, AE, and Graph Transformer via a tri-learning mechanism and feature fusion strategy to enhance clustering representation.

Result: Tri-GFN achieves significant accuracy improvements on ACM (0.87%), Reuters (14.14%), and USPS (7.58%) datasets.

Conclusion: Tri-GFN's superior performance makes it suitable for applications like news classification and topic retrieval.

Abstract: In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [42] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: FedSkipTwin reduces FL communication overhead by 12-15.5% using server-side digital twins to predict client updates, improving accuracy by 0.5% over FedAvg.


<details>
  <summary>Details</summary>
Motivation: Communication overhead in FL is a bottleneck, especially for mobile/IoT devices with limited bandwidth.

Method: FedSkipTwin uses LSTM-based digital twins to predict client update magnitude and uncertainty, skipping rounds when values are below thresholds.

Result: Reduced communication by 12-15.5% and improved accuracy by 0.5% on UCI-HAR and MNIST datasets.

Conclusion: Prediction-guided skipping is effective for resource-aware FL in bandwidth-constrained environments.

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [43] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR is a two-stage framework using video diffusion pre-training and masked inverse dynamics for bimanual robotic manipulation, achieving strong generalization with minimal human demonstrations.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and embodiment heterogeneity hinder scaling in bimanual robotic manipulation.

Method: VIDAR combines large-scale video diffusion pre-training with a masked inverse dynamics model for action prediction, using multi-view videos from real-world robots.

Result: VIDAR generalizes to unseen tasks and backgrounds with only 1% of typical data, outperforming state-of-the-art methods.

Conclusion: Video foundation models with masked action prediction can enable scalable and generalizable robotic manipulation.

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [44] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh,Daniele Santoni,Debaleena Nawn,Eleonora Ottaviani,Giovanni Felici*

Main category: cs.LG

TL;DR: A review of Transformer-based models in protein sequence analysis and design, covering applications like gene ontology, protein identification, de novo protein generation, and protein binding, while highlighting strengths, weaknesses, and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore the adoption and impact of Transformer-based models in bioinformatics, specifically for protein sequence analysis and design, and to provide a comprehensive overview of current research.

Method: Review and analysis of significant works applying Transformer models to protein-related tasks, evaluating their strengths and weaknesses.

Result: Identified key applications and limitations of Transformer models in protein research, offering insights into their effectiveness.

Conclusion: The review serves as a guide for researchers, summarizing state-of-the-art advancements and suggesting future research directions to address existing gaps.

Abstract: The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [45] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: The paper introduces GRU-KAN and LSTM-KAN models for early loan default prediction, outperforming baselines with 92% accuracy at 3 months and 88% at 8 months.


<details>
  <summary>Details</summary>
Motivation: To improve early loan default prediction beyond existing methods' limitations in accuracy and temporal dependency.

Method: Proposes GRU-KAN and LSTM-KAN, combining Kolmogorov-Arnold Networks with GRU and LSTM. Evaluated against baselines on accuracy, precision, recall, F1, and AUC.

Result: Achieves 92% accuracy at 3 months and 88% at 8 months, surpassing baseline models.

Conclusion: The GRU-KAN and LSTM-KAN models effectively enhance early loan default prediction, offering practical benefits for financial institutions.

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [46] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský,Gustav Šír,Vyacheslav Kungurtsev,Georgios Korpas*

Main category: cs.LG

TL;DR: PI-GNNs show performance drops with denser problem graphs due to a phase transition in training dynamics. Proposed methods based on fuzzy logic and binarized networks improve results.


<details>
  <summary>Details</summary>
Motivation: Address the performance decline of PI-GNNs in dense combinatorial problem graphs and the discrepancy between relaxed outputs and binary solutions.

Method: Propose alternatives inspired by fuzzy logic and binarized neural networks to refine PI-GNNs.

Result: The new methods significantly enhance PI-GNN performance in dense settings.

Conclusion: The study provides effective strategies to mitigate PI-GNN limitations in dense combinatorial problems.

Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [47] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong,Austin Tripp,Layla Hosseini-Gerami,Brooks Paige*

Main category: cs.LG

TL;DR: MOBO (EHVI) outperforms scalarized EI in molecular design, showing better Pareto front coverage, speed, and diversity, especially in low-data settings.


<details>
  <summary>Details</summary>
Motivation: To empirically compare MOBO (EHVI) with scalarized alternatives in molecular optimization, addressing underexplored advantages.

Method: Benchmarked EHVI against scalarized EI using identical Gaussian Process surrogates and molecular representations in three tasks.

Result: EHVI consistently outperformed scalarized EI in Pareto front coverage, convergence speed, and chemical diversity.

Conclusion: Pareto-aware acquisition (EHVI) is practically advantageous in molecular optimization, particularly with limited budgets and nontrivial trade-offs.

Abstract: Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [48] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang,Yu Liu,Daniel Biggs,Haoru Wang,Jiandong Yu,Ping Huang*

Main category: cs.LG

TL;DR: The paper proposes Adaptive Spatial Tokenization (AST) to efficiently simulate deformable body interactions using structured grids and attention mechanisms, outperforming existing methods in scalability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Learning-based methods like GNNs face scalability issues in modeling deformable body interactions due to computational intensity.

Method: AST divides simulation space into a grid, maps unstructured meshes onto it, and uses cross-attention for compact embeddings. Self-attention predicts the next state in latent space.

Result: AST outperforms state-of-the-art methods, handling large-scale simulations (100,000+ nodes) effectively.

Conclusion: AST offers a scalable and accurate solution for deformable body simulations, supported by a novel large-scale dataset.

Abstract: Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>


### [49] [Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods](https://arxiv.org/abs/2507.13716)
*Danilo Avola,Andrea Bernardini,Giancarlo Crocetti,Andrea Ladogana,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: The paper benchmarks traditional ML and DL models for classifying Parkinson's Disease (PD) using EEG data, finding CNN-LSTM models and XGBoost perform best.


<details>
  <summary>Details</summary>
Motivation: Early PD diagnosis is critical, and EEG offers a non-invasive tool, but reliable automated models are lacking. This study aims to establish baseline models for future research.

Method: A seven-step preprocessing pipeline and consistent cross-validation were applied to compare ML and DL models on an oddball task dataset.

Result: CNN-LSTM models outperformed other DL architectures, while XGBoost also showed strong accuracy, highlighting the importance of temporal dependencies in EEG data.

Conclusion: The study provides a reference framework for future EEG-based PD diagnostics, emphasizing the need for baseline results to ensure rigor and reproducibility.

Abstract: Parkinson's Disease PD is a progressive neurodegenerative disorder that
affects motor and cognitive functions with early diagnosis being critical for
effective clinical intervention Electroencephalography EEG offers a noninvasive
and costeffective means of detecting PDrelated neural alterations yet the
development of reliable automated diagnostic models remains a challenge In this
study we conduct a systematic benchmark of traditional machine learning ML and
deep learning DL models for classifying PD using a publicly available oddball
task dataset Our aim is to lay the groundwork for developing an effective
learning system and to determine which approach produces the best results We
implement a unified sevenstep preprocessing pipeline and apply consistent
subjectwise crossvalidation and evaluation criteria to ensure comparability
across models Our results demonstrate that while baseline deep learning
architectures particularly CNNLSTM models achieve the best performance compared
to other deep learning architectures underlining the importance of capturing
longrange temporal dependencies several traditional classifiers such as XGBoost
also offer strong predictive accuracy and calibrated decision boundaries By
rigorously comparing these baselines our work provides a solid reference
framework for future studies aiming to develop and evaluate more complex or
specialized architectures Establishing a reliable set of baseline results is
essential to contextualize improvements introduced by novel methods ensuring
scientific rigor and reproducibility in the evolving field of EEGbased
neurodiagnostics

</details>


### [50] [Bi-GRU Based Deception Detection using EEG Signals](https://arxiv.org/abs/2507.13718)
*Danilo Avola,Muhammad Yasir Bilal,Emad Emam,Cristina Lakasz,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: A deep learning model using Bi-GRU achieves 97% accuracy in classifying deceptive vs. truthful behavior from EEG signals.


<details>
  <summary>Details</summary>
Motivation: Deception detection is crucial in security, psychology, and forensics, but challenging. This study explores EEG-based methods for naturalistic scenarios.

Method: A Bidirectional Gated Recurrent Unit (Bi-GRU) neural network was trained on EEG signals from the Bag-of-Lies dataset for binary classification.

Result: The model achieved 97% test accuracy, with high precision, recall, and F1-scores for both classes.

Conclusion: The results show Bi-GRU's effectiveness for EEG-based deception detection, suggesting potential for real-time use and further neural architecture exploration.

Abstract: Deception detection is a significant challenge in fields such as security,
psychology, and forensics. This study presents a deep learning approach for
classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)
signals from the Bag-of-Lies dataset, a multimodal corpus designed for
naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit
(Bi-GRU) neural network was trained to perform binary classification of EEG
samples. The model achieved a test accuracy of 97\%, along with high precision,
recall, and F1-scores across both classes. These results demonstrate the
effectiveness of using bidirectional temporal modeling for EEG-based deception
detection and suggest potential for real-time applications and future
exploration of advanced neural architectures.

</details>


### [51] [Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion](https://arxiv.org/abs/2507.13721)
*Zizhao Zhang,Tianxiang Zhao,Yu Sun,Liping Sun,Jichuan Kang*

Main category: cs.LG

TL;DR: A hybrid feature fusion framework improves failure mode analysis in autonomous cargo ships, enhancing decision-making with advanced algorithms and semantic processing.


<details>
  <summary>Details</summary>
Motivation: Address cascading failures and uncertainties in emergency decision-making for autonomous cargo ships.

Method: Proposes a hybrid feature fusion framework using HN-CSA, Word2Vec, BERT-KPCA, and Sentence-BERT to process and analyze failure data.

Result: Achieves 7.1% and 3.4% efficiency gains over NSGA-II and CSA, with high classification accuracy (0.735) and prediction F1 score (0.93).

Conclusion: Provides a robust foundation for failure analysis and supports fault diagnosis, risk assessment, and intelligent decision-making in ACS.

Abstract: To address the challenges posed by cascading reactions caused by component
failures in autonomous cargo ships (ACS) and the uncertainties in emergency
decision-making, this paper proposes a novel hybrid feature fusion framework
for constructing a graph-structured dataset of failure modes. By employing an
improved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency
is significantly enhanced, achieving improvements of 7.1% and 3.4% compared to
the NSGA-II and CSA search algorithms, respectively. A hierarchical feature
fusion framework is constructed, using Word2Vec encoding to encode
subsystem/component features, BERT-KPCA to process failure modes/reasons, and
Sentence-BERT to quantify the semantic association between failure impact and
emergency decision-making. The dataset covers 12 systems, 1,262 failure modes,
and 6,150 propagation paths. Validation results show that the GATE-GNN model
achieves a classification accuracy of 0.735, comparable to existing benchmarks.
Additionally, a silhouette coefficient of 0.641 indicates that the features are
highly distinguishable. In the label prediction results, the Shore-based
Meteorological Service System achieved an F1 score of 0.93, demonstrating high
prediction accuracy. This paper not only provides a solid foundation for
failure analysis in autonomous cargo ships but also offers reliable support for
fault diagnosis, risk assessment, and intelligent decision-making systems. The
link to the dataset is
https://github.com/wojiufukele/Graph-Structured-about-CSA.

</details>


### [52] [Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics](https://arxiv.org/abs/2507.13727)
*René Heinrich,Lukas Rauch,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: Adversarial training improves generalization and robustness in audio classification, especially with output-space attacks, boosting clean test performance by 10.5%.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore how adversarial training enhances generalization and robustness in audio classification under data distribution shifts.

Method: Two adversarial training strategies (output-space and embedding-space attacks) are tested on ConvNeXt and AudioProtoPNet models using a bird sound benchmark.

Result: Output-space attacks improve clean test performance by 10.5% and enhance adversarial robustness.

Conclusion: Adversarial training can improve robustness against distribution shifts and adversarial attacks in audio classification.

Abstract: Adversarial training is a promising strategy for enhancing model robustness
against adversarial attacks. However, its impact on generalization under
substantial data distribution shifts in audio classification remains largely
unexplored. To address this gap, this work investigates how different
adversarial training strategies improve generalization performance and
adversarial robustness in audio classification. The study focuses on two model
architectures: a conventional convolutional neural network (ConvNeXt) and an
inherently interpretable prototype-based model (AudioProtoPNet). The approach
is evaluated using a challenging bird sound classification benchmark. This
benchmark is characterized by pronounced distribution shifts between training
and test data due to varying environmental conditions and recording methods, a
common real-world challenge. The investigation explores two adversarial
training strategies: one based on output-space attacks that maximize the
classification loss function, and another based on embedding-space attacks
designed to maximize embedding dissimilarity. These attack types are also used
for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses
the stability of its learned prototypes under targeted embedding-space attacks.
Results show that adversarial training, particularly using output-space
attacks, improves clean test data performance by an average of 10.5% relative
and simultaneously strengthens the adversarial robustness of the models. These
findings, although derived from the bird sound domain, suggest that adversarial
training holds potential to enhance robustness against both strong distribution
shifts and adversarial attacks in challenging audio classification settings.

</details>


### [53] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: A multi-layer DNN scheduling framework extends OctopuScheduler, enabling edge-based execution of large DNNs like transformers on SpiNNaker2.


<details>
  <summary>Details</summary>
Motivation: To facilitate the execution of complex DNNs on neuromorphic hardware (SpiNNaker2) for edge applications.

Method: Extends OctopuScheduler with quantization and lowering steps, providing an end-to-end flow from PyTorch models to SpiNNaker2 inference.

Result: Enables execution of large-scale DNNs, including transformers, on a single SpiNNaker2 chip.

Conclusion: The framework successfully bridges the gap between PyTorch models and neuromorphic hardware, supporting edge-based DNN inference.

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [54] [SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification](https://arxiv.org/abs/2507.13741)
*Shangyou Wang,Zezhong Ding,Xike Xie*

Main category: cs.LG

TL;DR: SamGoG is a sampling-based Graph-of-Graphs framework addressing class and graph size imbalance in GNNs, improving accuracy and training speed.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs often suffer from class and size imbalance, biasing GNN performance; existing methods are limited or costly.

Method: SamGoG constructs multiple GoGs via importance-based sampling, enhancing edge homophily with learnable similarity and adaptive node degrees.

Result: Achieves up to 15.66% accuracy improvement and 6.7× training acceleration on benchmarks.

Conclusion: SamGoG effectively mitigates imbalance issues, enhancing GNN performance for graph classification.

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in graph
classification tasks by capturing both structural and feature-based
representations. However, real-world graphs often exhibit two critical forms of
imbalance: class imbalance and graph size imbalance. These imbalances can bias
the learning process and degrade model performance. Existing methods typically
address only one type of imbalance or incur high computational costs. In this
work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning
framework that effectively mitigates both class and graph size imbalance.
SamGoG constructs multiple GoGs through an efficient importance-based sampling
mechanism and trains on them sequentially. This sampling mechanism incorporates
the learnable pairwise similarity and adaptive GoG node degree to enhance edge
homophily, thus improving downstream model quality. SamGoG can seamlessly
integrate with various downstream GNNs, enabling their efficient adaptation for
graph classification tasks. Extensive experiments on benchmark datasets
demonstrate that SamGoG achieves state-of-the-art performance with up to a
15.66% accuracy improvement with 6.7$\times$ training acceleration.

</details>


### [55] [Search-Optimized Quantization in Biomedical Ontology Alignment](https://arxiv.org/abs/2507.13742)
*Oussama Bouaggad,Natalia Grabar*

Main category: cs.LG

TL;DR: The paper introduces a method for optimizing large AI models for resource-constrained environments, achieving significant speed and memory improvements while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges of deploying large AI models in resource-constrained settings, focusing on energy, memory, and latency.

Method: Uses transformer-based models for ontology alignment, employs Microsoft Olive for optimization, and applies dynamic quantization with Intel tools.

Result: Achieves 20x faster inference and ~70% memory reduction while maintaining performance, setting new benchmarks in DEFT 2020 tasks.

Conclusion: The proposed optimization method effectively balances performance and efficiency, making large models viable for edge deployment.

Abstract: In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.

</details>


### [56] [MolPIF: A Parameter Interpolation Flow Model for Molecule Generation](https://arxiv.org/abs/2507.13762)
*Yaowei Jin,Junjie Wang,Wenkai Xiang,Duanhua Cao,Dan Teng,Zhehuan Fan,Jiacheng Xiong,Xia Sheng,Chuanlong Zeng,Mingyue Zheng,Qian Shi*

Main category: cs.LG

TL;DR: Proposes Parameter Interpolation Flow (PIF) for molecular generation, outperforming Bayesian Flow Networks (BFNs) in drug design.


<details>
  <summary>Details</summary>
Motivation: BFNs' limitations in flexibility and efficiency for diverse data distributions and tasks.

Method: Introduces PIF with theoretical foundation, training, and inference procedures, applied as MolPIF for drug design.

Result: MolPIF shows superior performance across metrics compared to baselines.

Conclusion: Validates parameter-space-based generative modeling for molecules and offers new design perspectives.

Abstract: Advances in deep learning for molecular generation show promise in
accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown
impressive performance across diverse chemical tasks, with their success often
ascribed to the paradigm of modeling in a low-variance parameter space.
However, the Bayesian inference-based strategy imposes limitations on designing
more flexible distribution transformation pathways, making it challenging to
adapt to diverse data distributions and varied task requirements. Furthermore,
the potential for simpler, more efficient parameter-space-based models is
unexplored. To address this, we propose a novel Parameter Interpolation Flow
model (named PIF) with detailed theoretical foundation, training, and inference
procedures. We then develop MolPIF for structure-based drug design,
demonstrating its superior performance across diverse metrics compared to
baselines. This work validates the effectiveness of parameter-space-based
generative modeling paradigm for molecules and offers new perspectives for
model design.

</details>


### [57] [Dual-Center Graph Clustering with Neighbor Distribution](https://arxiv.org/abs/2507.13765)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Li Jin,Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces DCGC, a dual-center graph clustering method using neighbor distribution for reliable supervision and dual-center optimization, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing graph clustering methods rely on unreliable pseudo-labels or single-center optimization, leading to incomplete guidance.

Method: Proposes DCGC with neighbor distribution as supervision for contrastive learning and dual-center optimization (feature and neighbor distribution centers).

Result: DCGC shows superior performance in experiments compared to existing methods.

Conclusion: DCGC effectively addresses limitations of current approaches by leveraging neighbor distribution for reliable guidance and dual-center optimization.

Abstract: Graph clustering is crucial for unraveling intricate data structures, yet it
presents significant challenges due to its unsupervised nature. Recently,
goal-directed clustering techniques have yielded impressive results, with
contrastive learning methods leveraging pseudo-label garnering considerable
attention. Nonetheless, pseudo-label as a supervision signal is unreliable and
existing goal-directed approaches utilize only features to construct a
single-target distribution for single-center optimization, which lead to
incomplete and less dependable guidance. In our work, we propose a novel
Dual-Center Graph Clustering (DCGC) approach based on neighbor distribution
properties, which includes representation learning with neighbor distribution
and dual-center optimization. Specifically, we utilize neighbor distribution as
a supervision signal to mine hard negative samples in contrastive learning,
which is reliable and enhances the effectiveness of representation learning.
Furthermore, neighbor distribution center is introduced alongside feature
center to jointly construct a dual-target distribution for dual-center
optimization. Extensive experiments and analysis demonstrate superior
performance and effectiveness of our proposed method.

</details>


### [58] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian neural network-based fine-tuning approach for machine learning force fields, enabling automated on-the-fly learning to reduce training data needs and detect rare events.


<details>
  <summary>Details</summary>
Motivation: The computational burden of generating diverse training datasets for interatomic force fields, especially for rare events or large configuration spaces, motivates the need for efficient fine-tuning methods.

Method: The proposed method uses Bayesian neural networks for uncertainty quantification and an on-the-fly workflow to automate fine-tuning and detect rare events.

Result: The approach maintains pre-specified accuracy during fine-tuning and enhances sampling of rare events like transition states.

Conclusion: The method offers a practical solution for fine-tuning foundation models with limited training data and improves modeling of rare events.

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


### [59] [Self-supervised learning on gene expression data](https://arxiv.org/abs/2507.13912)
*Kevin Dradjat,Massinissa Hamidi,Pierre Bartet,Blaise Hanczar*

Main category: cs.LG

TL;DR: The paper explores self-supervised learning for phenotype prediction from bulk gene expression data, outperforming traditional supervised methods and reducing reliance on labeled data.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised learning requires costly labeled data, while self-supervised learning can leverage unlabeled data to improve phenotype prediction.

Method: Three state-of-the-art self-supervised methods were applied to bulk gene expression data, evaluated on public datasets for phenotype prediction.

Result: Self-supervised methods outperformed traditional supervised models, capturing complex data structures and improving prediction accuracy.

Conclusion: Self-supervised learning is promising for gene expression analysis, with recommendations for method selection and future research directions outlined.

Abstract: Predicting phenotypes from gene expression data is a crucial task in
biomedical research, enabling insights into disease mechanisms, drug responses,
and personalized medicine. Traditional machine learning and deep learning rely
on supervised learning, which requires large quantities of labeled data that
are costly and time-consuming to obtain in the case of gene expression data.
Self-supervised learning has recently emerged as a promising approach to
overcome these limitations by extracting information directly from the
structure of unlabeled data. In this study, we investigate the application of
state-of-the-art self-supervised learning methods to bulk gene expression data
for phenotype prediction. We selected three self-supervised methods, based on
different approaches, to assess their ability to exploit the inherent structure
of the data and to generate qualitative representations which can be used for
downstream predictive tasks. By using several publicly available gene
expression datasets, we demonstrate how the selected methods can effectively
capture complex information and improve phenotype prediction accuracy. The
results obtained show that self-supervised learning methods can outperform
traditional supervised models besides offering significant advantage by
reducing the dependency on annotated data. We provide a comprehensive analysis
of the performance of each method by highlighting their strengths and
limitations. We also provide recommendations for using these methods depending
on the case under study. Finally, we outline future research directions to
enhance the application of self-supervised learning in the field of gene
expression data analysis. This study is the first work that deals with bulk
RNA-Seq data and self-supervised learning.

</details>


### [60] [Reframing attention as a reinforcement learning problem for causal discovery](https://arxiv.org/abs/2507.13920)
*Turan Orujlu,Christian Gumbsch,Martin V. Butz,Charley M Wu*

Main category: cs.LG

TL;DR: The paper introduces the Causal Process framework and its implementation, Causal Process Model, to represent dynamic causal structures in RL, outperforming existing methods in causal representation learning and agent performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between formal causality frameworks and deep RL by addressing the dynamic nature of causal interactions, which static causal graphs ignore.

Method: Proposes the Causal Process framework and its implementation, Causal Process Model, using RL agents to infer dynamic causal graphs via an attention mechanism akin to Transformers.

Result: Outperforms current alternatives in causal representation learning and agent performance, successfully recovering dynamic causal graphs.

Conclusion: The Causal Process framework effectively integrates causality into RL, offering interpretable dynamic causal representations and improved performance.

Abstract: Formal frameworks of causality have operated largely parallel to modern
trends in deep reinforcement learning (RL). However, there has been a revival
of interest in formally grounding the representations learned by neural
networks in causal concepts. Yet, most attempts at neural models of causality
assume static causal graphs and ignore the dynamic nature of causal
interactions. In this work, we introduce Causal Process framework as a novel
theory for representing dynamic hypotheses about causal structure. Furthermore,
we present Causal Process Model as an implementation of this framework. This
allows us to reformulate the attention mechanism popularized by Transformer
networks within an RL setting with the goal to infer interpretable causal
processes from visual observations. Here, causal inference corresponds to
constructing a causal graph hypothesis which itself becomes an RL task nested
within the original RL problem. To create an instance of such hypothesis, we
employ RL agents. These agents establish links between units similar to the
original Transformer attention mechanism. We demonstrate the effectiveness of
our approach in an RL environment where we outperform current alternatives in
causal representation learning and agent performance, and uniquely recover
graphs of dynamic causal processes.

</details>


### [61] [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](https://arxiv.org/abs/2507.14056)
*Alejandro Rodriguez-Garcia,Anindya Ghosh,Srikanth Ramaswamy*

Main category: cs.LG

TL;DR: The paper addresses the stability gap in continual learning, proposing an uncertainty-modulated gain dynamics mechanism inspired by biological brains to balance plasticity and stability, effectively reducing the gap.


<details>
  <summary>Details</summary>
Motivation: The stability gap in continual learning undermines robustness by causing performance drops on mastered tasks when learning new ones, even under ideal joint-loss regimes. This gap highlights the need to reconcile rapid adaptation with robust retention.

Method: The authors propose uncertainty-modulated gain dynamics, inspired by noradrenergic bursts in biological brains, to approximate a two-timescale optimizer. This mechanism dynamically balances knowledge integration with minimal interference on consolidated information.

Result: The proposed mechanism is evaluated on MNIST and CIFAR benchmarks, showing effective attenuation of the stability gap in domain-incremental and class-incremental scenarios under joint training.

Conclusion: The study demonstrates that gain modulation, mimicking biological noradrenergic functions, reduces stability gaps and enhances continual learning performance, providing mechanistic insights for future frameworks.

Abstract: Recent studies in continual learning have identified a transient drop in
performance on mastered tasks when assimilating new ones, known as the
stability gap. Such dynamics contradict the objectives of continual learning,
revealing a lack of robustness in mitigating forgetting, and notably,
persisting even under an ideal joint-loss regime. Examining this gap within
this idealized joint training context is critical to isolate it from other
sources of forgetting. We argue that it reflects an imbalance between rapid
adaptation and robust retention at task boundaries, underscoring the need to
investigate mechanisms that reconcile plasticity and stability within continual
learning frameworks. Biological brains navigate a similar dilemma by operating
concurrently on multiple timescales, leveraging neuromodulatory signals to
modulate synaptic plasticity. However, artificial networks lack native
multitimescale dynamics, and although optimizers like momentum-SGD and Adam
introduce implicit timescale regularization, they still exhibit stability gaps.
Inspired by locus coeruleus mediated noradrenergic bursts, which transiently
enhance neuronal gain under uncertainty to facilitate sensory assimilation, we
propose uncertainty-modulated gain dynamics - an adaptive mechanism that
approximates a two-timescale optimizer and dynamically balances integration of
knowledge with minimal interference on previously consolidated information. We
evaluate our mechanism on domain-incremental and class-incremental variants of
the MNIST and CIFAR benchmarks under joint training, demonstrating that
uncertainty-modulated gain dynamics effectively attenuate the stability gap.
Finally, our analysis elucidates how gain modulation replicates noradrenergic
functions in cortical circuits, offering mechanistic insights into reducing
stability gaps and enhance performance in continual learning tasks.

</details>


### [62] [MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space](https://arxiv.org/abs/2507.13950)
*Jingbo Liang,Bruna Jacobson*

Main category: cs.LG

TL;DR: MoDyGAN combines MD simulations and GANs to efficiently explore protein conformational spaces using a novel 2D representation of 3D structures.


<details>
  <summary>Details</summary>
Motivation: High computational costs of dynamic physics-based simulations limit exploration of protein conformational landscapes.

Method: MoDyGAN uses a generator to map Gaussian distributions to MD-derived trajectories and a refinement module with dual-discriminator for plausible conformations. 3D structures are transformed into 2D matrices for image-based GANs.

Result: MoDyGAN generates plausible conformations for rigid proteins and aligns latent space interpolations with SMD trajectories.

Conclusion: Image-like representation of proteins enables efficient conformational sampling and extends deep learning applications to biomolecular simulation.

Abstract: Extensively exploring protein conformational landscapes remains a major
challenge in computational biology due to the high computational cost involved
in dynamic physics-based simulations. In this work, we propose a novel
pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and
generative adversarial networks (GANs) to explore protein conformational
spaces. MoDyGAN contains a generator that maps Gaussian distributions into
MD-derived protein trajectories, and a refinement module that combines ensemble
learning with a dual-discriminator to further improve the plausibility of
generated conformations. Central to our approach is an innovative
representation technique that reversibly transforms 3D protein structures into
2D matrices, enabling the use of advanced image-based GAN architectures. We use
three rigid proteins to demonstrate that MoDyGAN can generate plausible new
conformations. We also use deca-alanine as a case study to show that
interpolations within the latent space closely align with trajectories obtained
from steered molecular dynamics (SMD) simulations. Our results suggest that
representing proteins as image-like data unlocks new possibilities for applying
advanced deep learning techniques to biomolecular simulation, leading to an
efficient sampling of conformational states. Additionally, the proposed
framework holds strong potential for extension to other complex 3D structures.

</details>


### [63] [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective](https://arxiv.org/abs/2507.14121)
*Pankaj Yadav,Vivek Vijay*

Main category: cs.LG

TL;DR: Kolmogorov Arnold Networks (KANs) outperform MLPs on raw imbalanced data but struggle with imbalance strategies and high computational costs, limiting practical use.


<details>
  <summary>Details</summary>
Motivation: To evaluate KANs' effectiveness in class-imbalanced classification compared to MLPs, and identify their limitations.

Method: Empirical evaluation on ten benchmark datasets, comparing KANs and MLPs with/without imbalance strategies.

Result: KANs perform better on raw data but degrade with resampling/focal loss. MLPs with imbalance techniques match KANs at lower costs.

Conclusion: KANs are niche for raw imbalanced data but need architectural and efficiency improvements for broader use.

Abstract: Kolmogorov Arnold Networks (KANs) are recent architectural advancement in
neural computation that offer a mathematically grounded alternative to standard
neural networks. This study presents an empirical evaluation of KANs in context
of class imbalanced classification, using ten benchmark datasets. We observe
that KANs can inherently perform well on raw imbalanced data more effectively
than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,
conventional imbalance strategies fundamentally conflict with KANs mathematical
structure as resampling and focal loss implementations significantly degrade
KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from
prohibitive computational costs without proportional performance gains.
Statistical validation confirms that MLPs with imbalance techniques achieve
equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.
These findings reveal that KANs represent a specialized solution for raw
imbalanced data where resources permit. But their severe performance-resource
tradeoffs and incompatibility with standard resampling techniques currently
limits practical deployment. We identify critical research priorities as
developing KAN specific architectural modifications for imbalance learning,
optimizing computational efficiency, and theoretical reconciling their conflict
with data augmentation. This work establishes foundational insights for next
generation KAN architectures in imbalanced classification scenarios.

</details>


### [64] [Robust Anomaly Detection with Graph Neural Networks using Controllability](https://arxiv.org/abs/2507.13954)
*Yifan Wei,Anwar Said,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: The paper proposes integrating average controllability into graph-based anomaly detection to improve performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in complex domains is challenging due to scarce labeled data and imbalance between anomalous and benign samples. Graph-based models show promise but need enhancement.

Method: Two novel approaches: (1) using average controllability as edge weight, (2) encoding it as a one-hot edge attribute vector. Evaluated on real-world and synthetic networks.

Result: Proposed methods outperform six state-of-the-art baselines, demonstrating improved anomaly detection performance.

Conclusion: Average controllability enhances graph-based models, addressing challenges in sparse, imbalanced datasets for anomaly detection.

Abstract: Anomaly detection in complex domains poses significant challenges due to the
need for extensive labeled data and the inherently imbalanced nature of
anomalous versus benign samples. Graph-based machine learning models have
emerged as a promising solution that combines attribute and relational data to
uncover intricate patterns. However, the scarcity of anomalous data exacerbates
the challenge, which requires innovative strategies to enhance model learning
with limited information. In this paper, we hypothesize that the incorporation
of the influence of the nodes, quantified through average controllability, can
significantly improve the performance of anomaly detection. We propose two
novel approaches to integrate average controllability into graph-based
frameworks: (1) using average controllability as an edge weight and (2)
encoding it as a one-hot edge attribute vector. Through rigorous evaluation on
real-world and synthetic networks with six state-of-the-art baselines, our
proposed methods demonstrate improved performance in identifying anomalies,
highlighting the critical role of controllability measures in enhancing the
performance of graph machine learning models. This work underscores the
potential of integrating average controllability as additional metrics to
address the challenges of anomaly detection in sparse and imbalanced datasets.

</details>


### [65] [Toward Temporal Causal Representation Learning with Tensor Decomposition](https://arxiv.org/abs/2507.14126)
*Jianhong Chen,Meng Zhao,Mostafa Reisi Gahrooei,Xubo Yue*

Main category: cs.LG

TL;DR: The paper introduces CaRTeD, a framework combining temporal causal representation learning with irregular tensor decomposition to analyze high-dimensional, irregularly structured data. It offers theoretical convergence guarantees and outperforms existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Real-world data often exists as high-dimensional, irregular tensors, but current methods lack theoretical guarantees and flexibility for such structures. The paper aims to bridge this gap.

Method: Proposes CaRTeD, integrating temporal causal representation learning with irregular tensor decomposition, featuring flexible regularization and theoretical convergence proofs.

Result: CaRTeD outperforms state-of-the-art methods in synthetic and real-world (MIMIC-III EHR) datasets, improving explainability and performance in phenotyping and network recovery.

Conclusion: The framework successfully addresses the challenges of irregular tensor data, providing theoretical and practical advancements in causal representation learning.

Abstract: Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.

</details>


### [66] [Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs](https://arxiv.org/abs/2507.13959)
*Eli Verwimp,Gustav Ryberg Smidt,Hendrik Hameeuw,Katrien De Graef*

Main category: cs.LG

TL;DR: The paper explores ML techniques for classifying cuneiform signs, highlighting variability challenges and evaluating ResNet50's performance on Old Babylonian texts.


<details>
  <summary>Details</summary>
Motivation: To address the variability in cuneiform signs and assess ML model performance across datasets, aiming to improve future data standards.

Method: Trained and tested ResNet50 on handwritten Old Babylonian texts from three Mesopotamian cities, focusing on signs with ≥20 instances.

Result: Achieved 87.1% top-1 and 96.5% top-5 accuracy, setting a benchmark for Old Babylonian text classification.

Conclusion: The study provides insights for future data acquisition and classification tasks, establishing a foundation for further research.

Abstract: The work in this paper describes the training and evaluation of machine
learning (ML) techniques for the classification of cuneiform signs. There is a
lot of variability in cuneiform signs, depending on where they come from, for
what and by whom they were written, but also how they were digitized. This
variability makes it unlikely that an ML model trained on one dataset will
perform successfully on another dataset. This contribution studies how such
differences impact that performance. Based on our results and insights, we aim
to influence future data acquisition standards and provide a solid foundation
for future cuneiform sign classification tasks. The ML model has been trained
and tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary
texts inscribed on clay tablets originating from three Mesopotamian cities
(Nippur, D\=ur-Abie\v{s}uh and Sippar). The presented and analysed model is
ResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for
signs with at least 20 instances. As these automatic classification results are
the first on Old Babylonian texts, there are currently no comparable results.

</details>


### [67] [Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks](https://arxiv.org/abs/2507.13992)
*Jagruti Patel,Thomas A. W. Bolton,Mikkel Schöttner,Anjali Tarun,Sebastien Tourbier,Yasser Alemàn-Gòmez,Jonas Richiardi,Patric Hagmann*

Main category: cs.LG

TL;DR: A deep harmonization framework for structural connectomes (SCs) addresses biases in multi-site neuroimaging studies without needing metadata or traveling subjects, outperforming traditional methods in preserving topology and individuality.


<details>
  <summary>Details</summary>
Motivation: Small sample sizes and acquisition-related biases in SC studies limit biomarker reliability for neurological disorders. Existing harmonization methods rely on metadata or overlook SC graph-topology.

Method: Proposes a site-conditioned deep harmonization framework, testing three deep architectures (fully connected AE, convolutional AE, graph convolutional AE) against linear regression (LR) in a simulated Human Connectome Dataset scenario.

Result: Non-graph models excel in edge-weight prediction, while the graph AE preserves topological structure and subject-level individuality better. LR performs numerically best but lacks real-world applicability due to metadata dependency.

Conclusion: Graph-based approaches are ideal for structure-aware, domain-generalizable SC harmonization in large-scale multi-site studies, emphasizing the importance of model architecture.

Abstract: Small sample sizes in neuroimaging in general, and in structural connectome
(SC) studies in particular limit the development of reliable biomarkers for
neurological and psychiatric disorders - such as Alzheimer's disease and
schizophrenia - by reducing statistical power, reliability, and
generalizability. Large-scale multi-site studies have exist, but they have
acquisition-related biases due to scanner heterogeneity, compromising imaging
consistency and downstream analyses. While existing SC harmonization methods -
such as linear regression (LR), ComBat, and deep learning techniques - mitigate
these biases, they often rely on detailed metadata, traveling subjects (TS), or
overlook the graph-topology of SCs. To address these limitations, we propose a
site-conditioned deep harmonization framework that harmonizes SCs across
diverse acquisition sites without requiring metadata or TS that we test in a
simulated scenario based on the Human Connectome Dataset. Within this
framework, we benchmark three deep architectures - a fully connected
autoencoder (AE), a convolutional AE, and a graph convolutional AE - against a
top-performing LR baseline. While non-graph models excel in edge-weight
prediction and edge existence detection, the graph AE demonstrates superior
preservation of topological structure and subject-level individuality, as
reflected by graph metrics and fingerprinting accuracy, respectively. Although
the LR baseline achieves the highest numerical performance by explicitly
modeling acquisition parameters, it lacks applicability to real-world
multi-site use cases as detailed acquisition metadata is often unavailable. Our
results highlight the critical role of model architecture in SC harmonization
performance and demonstrate that graph-based approaches are particularly
well-suited for structure-aware, domain-generalizable SC harmonization in
large-scale multi-site SC studies.

</details>


### [68] [ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies](https://arxiv.org/abs/2507.13998)
*Itay Katav,Aryeh Kontorovich*

Main category: cs.LG

TL;DR: The paper introduces ParallelTime, a dynamic weighting mechanism and architecture for multivariate time series forecasting, outperforming existing methods by optimizing long-term and short-term dependency weights.


<details>
  <summary>Details</summary>
Motivation: Current methods assign equal weight to long-term and short-term dependencies, which is suboptimal for time-series forecasting.

Method: Proposes ParallelTime Weighter for dynamic weight calculation and the ParallelTime architecture, combining local window attention and Mamba.

Result: Achieves state-of-the-art performance, lower FLOPs, fewer parameters, and scalability to longer prediction horizons.

Conclusion: ParallelTime offers a robust and efficient solution, paving the way for future advancements in parallel Attention-Mamba for time series forecasting.

Abstract: Modern multivariate time series forecasting primarily relies on two
architectures: the Transformer with attention mechanism and Mamba. In natural
language processing, an approach has been used that combines local window
attention for capturing short-term dependencies and Mamba for capturing
long-term dependencies, with their outputs averaged to assign equal weight to
both. We find that for time-series forecasting tasks, assigning equal weight to
long-term and short-term dependencies is not optimal. To mitigate this, we
propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates
interdependent weights for long-term and short-term dependencies for each token
based on the input and the model's knowledge. Furthermore, we introduce the
ParallelTime architecture, which incorporates the ParallelTime Weighter
mechanism to deliver state-of-the-art performance across diverse benchmarks.
Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer
parameters, scales effectively to longer prediction horizons, and significantly
outperforms existing methods. These advances highlight a promising path for
future developments of parallel Attention-Mamba in time series forecasting. The
implementation is readily available at:
\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub

</details>


### [69] [On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes](https://arxiv.org/abs/2507.14005)
*Mathieu Godbout,Audrey Durand*

Main category: cs.LG

TL;DR: The paper investigates why dual-based DP methods fail for static CVaR-optimal policies in MDPs, linking it to policy evaluation errors due to unmet risk-assignment constraints. It also shows limitations in finding uniformly optimal policies.


<details>
  <summary>Details</summary>
Motivation: To understand the root cause of failures in dual-based DP methods for CVaR-optimal policies in MDPs and explore the implications for policy evaluation and optimization.

Method: Frames CVaR policy evaluation as two minimization problems, introduces risk-assignment consistency constraints, and analyzes the CVaR evaluation gap. Also examines dual CVaR decomposition limitations.

Result: Identifies unmet risk-assignment constraints as the cause of evaluation errors and shows that no single policy can be optimal across all risk levels in certain MDPs.

Conclusion: The dual-based CVaR DP approach has inherent limitations due to evaluation gaps and the impossibility of uniformly optimal policies in some cases.

Abstract: Recent work has shown that dynamic programming (DP) methods for finding
static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when
based on the dual formulation, yet the root cause for the failure has remained
unclear. We expand on these findings by shifting focus from policy optimization
to the seemingly simpler task of policy evaluation. We show that evaluating the
static CVaR of a given policy can be framed as two distinct minimization
problems. For their solutions to match, a set of ``risk-assignment consistency
constraints'' must be satisfied, and we demonstrate that the intersection of
the constraints being empty is the source of previously observed evaluation
errors. Quantifying the evaluation error as the CVaR evaluation gap, we then
demonstrate that the issues observed when optimizing over the dual-based CVaR
DP are explained by the returned policy having a non-zero CVaR evaluation gap.
We then leverage our proposed risk-assignment perspective to prove that the
search for a single, uniformly optimal policy via on the dual CVaR
decomposition is fundamentally limited, identifying an MDP where no single
policy can be optimal across all initial risk levels.

</details>


### [70] [Byzantine-resilient federated online learning for Gaussian process regression](https://arxiv.org/abs/2507.14021)
*Xu Zhang,Zhenyuan Yuan,Minghui Zhu*

Main category: cs.LG

TL;DR: The paper proposes a Byzantine-resilient federated GPR algorithm for collaborative learning with agents, even under adversarial behavior.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of Byzantine failures in federated learning, where some agents may behave adversarially, and improve learning performance.

Method: A Byzantine-resilient product of experts aggregation rule is used to compute a global model from local predictions, which is then fused with local models for refinement.

Result: The algorithm improves learning accuracy, demonstrated via experiments on toy and real-world datasets.

Conclusion: The proposed method effectively handles Byzantine failures and enhances collaborative learning in federated GPR.

Abstract: In this paper, we study Byzantine-resilient federated online learning for
Gaussian process regression (GPR). We develop a Byzantine-resilient federated
GPR algorithm that allows a cloud and a group of agents to collaboratively
learn a latent function and improve the learning performances where some agents
exhibit Byzantine failures, i.e., arbitrary and potentially adversarial
behavior. Each agent-based local GPR sends potentially compromised local
predictions to the cloud, and the cloud-based aggregated GPR computes a global
model by a Byzantine-resilient product of experts aggregation rule. Then the
cloud broadcasts the current global model to all the agents. Agent-based fused
GPR refines local predictions by fusing the received global model with that of
the agent-based local GPR. Moreover, we quantify the learning accuracy
improvements of the agent-based fused GPR over the agent-based local GPR.
Experiments on a toy example and two medium-scale real-world datasets are
conducted to demonstrate the performances of the proposed algorithm.

</details>


### [71] [DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis](https://arxiv.org/abs/2507.14038)
*Aileen Luo,Tao Zhou,Ming Du,Martin V. Holt,Andrej Singer,Mathew J. Cherukara*

Main category: cs.LG

TL;DR: DONUT, a physics-aware neural network, enables real-time analysis of nanobeam diffraction data without labeled datasets, improving efficiency by 200x over conventional methods.


<details>
  <summary>Details</summary>
Motivation: Real-time analysis of coherent X-ray scattering data is hindered by artifacts and computational demands, especially in scanning X-ray nanodiffraction microscopy.

Method: DONUT integrates a differentiable geometric diffraction model into its architecture to predict crystal lattice strain and orientation without labeled data or pre-training.

Result: DONUT accurately extracts features from data over 200 times more efficiently than conventional fitting methods.

Conclusion: DONUT provides a rapid, automated solution for real-time analysis of nanobeam diffraction data, overcoming limitations of supervised learning in X-ray science.

Abstract: Coherent X-ray scattering techniques are critical for investigating the
fundamental structural properties of materials at the nanoscale. While
advancements have made these experiments more accessible, real-time analysis
remains a significant bottleneck, often hindered by artifacts and computational
demands. In scanning X-ray nanodiffraction microscopy, which is widely used to
spatially resolve structural heterogeneities, this challenge is compounded by
the convolution of the divergent beam with the sample's local structure. To
address this, we introduce DONUT (Diffraction with Optics for Nanobeam by
Unsupervised Training), a physics-aware neural network designed for the rapid
and automated analysis of nanobeam diffraction data. By incorporating a
differentiable geometric diffraction model directly into its architecture,
DONUT learns to predict crystal lattice strain and orientation in real-time.
Crucially, this is achieved without reliance on labeled datasets or
pre-training, overcoming a fundamental limitation for supervised machine
learning in X-ray science. We demonstrate experimentally that DONUT accurately
extracts all features within the data over 200 times more efficiently than
conventional fitting methods.

</details>


### [72] [Preference-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.14066)
*Ni Mu,Yao Luan,Qing-Shan Jia*

Main category: cs.LG

TL;DR: Pb-MORL integrates preferences into multi-objective reinforcement learning, avoiding complex reward design and enabling flexible policy optimization across the Pareto frontier.


<details>
  <summary>Details</summary>
Motivation: Predefined reward functions in MORL are rigid and hard to balance for conflicting objectives. Preferences offer a more intuitive alternative.

Method: Pb-MORL formalizes preference integration, constructs a multi-objective reward model aligned with preferences, and proves its equivalence to Pareto optimal policy training.

Result: Experiments in benchmark tasks, energy management, and autonomous driving show Pb-MORL outperforms oracle methods using ground truth rewards.

Conclusion: Pb-MORL is a practical and effective approach for complex real-world systems, leveraging preferences for flexible and optimal decision-making.

Abstract: Multi-objective reinforcement learning (MORL) is a structured approach for
optimizing tasks with multiple objectives. However, it often relies on
pre-defined reward functions, which can be hard to design for balancing
conflicting goals and may lead to oversimplification. Preferences can serve as
more flexible and intuitive decision-making guidance, eliminating the need for
complicated reward design. This paper introduces preference-based MORL
(Pb-MORL), which formalizes the integration of preferences into the MORL
framework. We theoretically prove that preferences can derive policies across
the entire Pareto frontier. To guide policy optimization using preferences, our
method constructs a multi-objective reward model that aligns with the given
preferences. We further provide theoretical proof to show that optimizing this
reward model is equivalent to training the Pareto optimal policy. Extensive
experiments in benchmark multi-objective tasks, a multi-energy management task,
and an autonomous driving task on a multi-line highway show that our method
performs competitively, surpassing the oracle method, which uses the ground
truth reward function. This highlights its potential for practical applications
in complex real-world systems.

</details>


### [73] [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration](https://arxiv.org/abs/2507.14088)
*Xiyun Li,Yining Ding,Yuhua Jiang,Yunlong Zhao,Runpeng Xie,Shuang Xu,Yuanhua Ni,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: The paper introduces a dual process multi-scale theory of mind (DPMT) framework to improve human-AI collaboration by modeling complex human mental characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents struggle to accurately model human intentions without direct communication, hindering effective collaboration.

Method: Proposes DPMT, inspired by cognitive science dual process theory, with a multi-scale theory of mind module for mental characteristic reasoning.

Result: DPMT significantly enhances human-AI collaboration, with ablation studies confirming the multi-scale ToM's role in the slow system.

Conclusion: The DPMT framework effectively addresses the challenge of modeling human mental characteristics in dynamic scenarios, improving collaboration.

Abstract: Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (LLM) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct communication. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [74] [CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding Education](https://arxiv.org/abs/2507.13814)
*Jianing Zhao,Peng Gao,Jiannong Cao,Zhiyuan Wen,Chen Chen,Jianing Yin,Ruosong Yang,Bo Yuan*

Main category: cs.MA

TL;DR: CodeEdu is a multi-agent LLM platform for personalized coding education, improving student performance through dynamic task allocation and specialized agent roles.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches lack assessment, personalization, and interactive learning, limiting their educational impact.

Method: CodeEdu uses multi-agent LLMs with specialized roles (e.g., tutoring, debugging) and external tools for dynamic, personalized education.

Result: Automated evaluations show CodeEdu significantly enhances students' coding performance.

Conclusion: Multi-agent LLMs like CodeEdu offer a promising solution for proactive, personalized coding education.

Abstract: Large Language Models (LLMs) have demonstrated considerable potential in
improving coding education by providing support for code writing, explanation,
and debugging. However, existing LLM-based approaches generally fail to assess
students' abilities, design learning plans, provide personalized material
aligned with individual learning goals, and enable interactive learning.
Current work mostly uses single LLM agents, which limits their ability to
understand complex code repositories and schedule step-by-step tutoring. Recent
research has shown that multi-agent LLMs can collaborate to solve complicated
problems in various domains like software engineering, but their potential in
the field of education remains unexplored. In this work, we introduce CodeEdu,
an innovative multi-agent collaborative platform that combines LLMs with tool
use to provide proactive and personalized education in coding. Unlike static
pipelines, CodeEdu dynamically allocates agents and tasks to meet student
needs. Various agents in CodeEdu undertake certain functions specifically,
including task planning, personalized material generation, real-time QA,
step-by-step tutoring, code execution, debugging, and learning report
generation, facilitated with extensive external tools to improve task
efficiency. Automated evaluations reveal that CodeEdu substantially enhances
students' coding performance.

</details>
