{"id": "2602.00755", "categories": ["cs.MA", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.00755", "abs": "https://arxiv.org/abs/2602.00755", "authors": ["Ujwal Kumar", "Alice Saito", "Hershraj Niranjani", "Rayan Yessou", "Phan Xuan Tan"], "title": "Evolving Interpretable Constitutions for Multi-Agent Simulation", "comment": "23 pages, 4 figures", "summary": "Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles (\"be helpful, harmless, honest\") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.", "AI": {"tldr": "Constitutional Evolution framework uses genetic programming to automatically discover effective behavioral norms in multi-agent LLM systems, achieving higher societal stability than human-designed constitutions.", "motivation": "Multi-agent systems create novel alignment challenges through emergent social dynamics that single-model alignment with fixed principles cannot address, requiring frameworks to discover norms rather than prescribe them.", "method": "Employ LLM-driven genetic programming with multi-island evolution in a grid-world simulation with survival pressure, evolving constitutions to maximize social welfare without explicit cooperation guidance.", "result": "The evolved constitution C* achieves a Societal Stability Score S = 0.556 +/- 0.008 (123% higher than human-designed baselines), eliminates conflict, and discovers that minimizing communication outperforms verbose coordination.", "conclusion": "Cooperative norms can be effectively discovered through evolutionary methods in multi-agent systems, offering superior performance over prescribed or vague principles for societal stability."}}
{"id": "2602.00766", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00766", "abs": "https://arxiv.org/abs/2602.00766", "authors": ["Xiaoxue Yu", "Rongpeng Li", "Zhifeng Zhao", "Honggang Zhang"], "title": "Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning", "comment": null, "summary": "The evolution of next-Generation (xG) wireless networks marks a paradigm shift from connectivity-centric architectures to Artificial Intelligence (AI)-native designs that tightly integrate data, computing, and communication. Yet existing AI deployments in communication systems remain largely siloed, offering isolated optimizations without intrinsic adaptability, dynamic task delegation, or multi-agent collaboration. In this work, we propose a unified agentic NetGPT framework for AI-native xG networks, wherein a NetGPT core can either perform autonomous reasoning or delegate sub-tasks to domain-specialized agents via agentic communication. The framework establishes clear modular responsibilities and interoperable workflows, enabling scalable, distributed intelligence across the network. To support continual refinement of collaborative reasoning strategies, the framework is further enhanced through Agentic reinforcement learning under partially observable conditions and stochastic external states. The training pipeline incorporates masked loss against external agent uncertainty, entropy-guided exploration, and multi-objective rewards that jointly capture task quality, coordination efficiency, and resource constraints. Through this process, NetGPT learns when and how to collaborate, effectively balancing internal reasoning with agent invocation. Overall, this work provides a foundational architecture and training methodology for self-evolving, AI-native xG networks capable of autonomous sensing, reasoning, and action in complex communication environments.", "AI": {"tldr": "A unified agentic NetGPT framework for AI-native next-generation wireless networks enables autonomous reasoning and task delegation, enhanced by reinforcement learning for collaborative intelligence.", "motivation": "Existing AI deployments in wireless networks are siloed, lacking adaptability, dynamic task delegation, or multi-agent collaboration, necessitating a more integrated AI-native design.", "method": "Propose a NetGPT framework with a core for autonomous reasoning or delegation to specialized agents via agentic communication, using modular responsibilities and interoperable workflows, and enhance it through Agentic reinforcement learning with masked loss, entropy-guided exploration, and multi-objective rewards.", "result": "The framework enables scalable, distributed intelligence, allowing NetGPT to learn when and how to collaborate, effectively balancing internal reasoning with agent invocation for tasks in complex environments.", "conclusion": "This work provides a foundational architecture and training methodology for self-evolving, AI-native xG networks capable of autonomous sensing, reasoning, and action, marking a shift to more integrated wireless network designs."}}
{"id": "2602.00966", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00966", "abs": "https://arxiv.org/abs/2602.00966", "authors": ["Zhaoyang Guan", "Huixi Cao", "Ming Zhong", "Eric Yang", "Lynn Ai", "Yongxin Ni", "Bill Shi"], "title": "Symphony-Coord: Emergent Coordination in Decentralized Agent Systems", "comment": "41 pages,15 figures", "summary": "Multi-agent large language model systems can tackle complex multi-step tasks by decomposing work and coordinating specialized behaviors. However, current coordination mechanisms typically rely on statically assigned roles and centralized controllers. As agent pools and task distributions evolve, these design choices lead to inefficient routing, poor adaptability, and fragile fault recovery capabilities. We introduce Symphony-Coord, a decentralized multi-agent framework that transforms agent selection into an online multi-armed bandit problem, enabling roles to emerge organically through interaction. The framework employs a two-stage dynamic beacon protocol: (i) a lightweight candidate screening mechanism to limit communication and computational overhead; (ii) an adaptive LinUCB selector that routes subtasks based on context features derived from task requirements and agent states, continuously optimized through delayed end-to-end feedback. Under standard linear realizability assumptions, we provide sublinear regret bounds, indicating the system converges toward near-optimal allocation schemes. Validation through simulation experiments and real-world large language model benchmarks demonstrates that Symphony-Coord not only enhances task routing efficiency but also exhibits robust self-healing capabilities in scenarios involving distribution shifts and agent failures, achieving a scalable coordination mechanism without predefined roles.", "AI": {"tldr": "Symphony-Coord is a decentralized multi-agent framework that transforms agent selection into an online multi-armed bandit problem, enabling emergent roles through a two-stage dynamic beacon protocol with theoretical guarantees and practical robustness.", "motivation": "Current multi-agent LLM coordination mechanisms rely on statically assigned roles and centralized controllers, leading to inefficient routing, poor adaptability, and fragile fault recovery as agent pools and task distributions evolve.", "method": "A decentralized framework using online multi-armed bandit approach with two-stage dynamic beacon protocol: (1) lightweight candidate screening to limit overhead, and (2) adaptive LinUCB selector routing subtasks based on context features from task requirements and agent states, optimized through delayed end-to-end feedback.", "result": "Provides sublinear regret bounds under linear realizability assumptions, indicating convergence toward near-optimal allocation. Simulation and real-world LLM benchmarks show enhanced task routing efficiency and robust self-healing capabilities in distribution shifts and agent failures.", "conclusion": "Symphony-Coord achieves scalable coordination without predefined roles, demonstrating efficient decentralized multi-agent coordination with theoretical guarantees and practical robustness in dynamic environments."}}
{"id": "2602.01011", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01011", "abs": "https://arxiv.org/abs/2602.01011", "authors": ["Aneesh Pappu", "Batu El", "Hancheng Cao", "Carmelo di Nolfo", "Yanchao Sun", "Meng Cao", "James Zou"], "title": "Multi-Agent Teams Hold Experts Back", "comment": "Under review at ICML 2026", "summary": "Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent's performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.", "AI": {"tldr": "LLM multi-agent teams struggle to leverage expert knowledge effectively, performing worse than single experts despite having access to the same expertise, due to consensus-seeking behavior that averages opinions rather than weighting expertise appropriately.", "motivation": "As multi-agent LLM systems become autonomous collaborators, coordination must emerge through interaction rather than being pre-specified. Prior work enforces coordination through fixed structures, leaving open how self-organizing teams perform when coordination is unconstrained.", "method": "Drawing on organizational psychology, the study examines whether self-organizing LLM teams achieve synergy (team performance matching or exceeding best individual). Experiments conducted across human-inspired and frontier ML benchmarks, analyzing conversational patterns to understand coordination failures.", "result": "LLM teams consistently fail to match their expert agent's performance (up to 37.6% performance loss), even when explicitly told who the expert is. Failure is due to expert leveraging rather than identification - teams tend toward integrative compromise (averaging expert and non-expert views) rather than appropriately weighting expertise. This consensus-seeking behavior increases with team size and correlates negatively with performance, though improves robustness to adversarial agents.", "conclusion": "There's a significant gap in self-organizing multi-agent teams' ability to harness collective expertise. Consensus-seeking behavior creates a trade-off between alignment and effective expertise utilization, revealing fundamental limitations in emergent coordination for LLM teams."}}
{"id": "2602.00012", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00012", "abs": "https://arxiv.org/abs/2602.00012", "authors": ["Michael Siebenmann", "Javier Argota S\u00e1nchez-Vaquerizo", "Stefan Arisona", "Krystian Samp", "Luis Gisler", "Dirk Helbing"], "title": "OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models", "comment": "This work has been submitted to the IEEE for possible publication. 7 pages, 6 figures", "summary": "We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.", "AI": {"tldr": "OGD4All is a framework using LLMs to improve citizen interaction with geospatial open government data, achieving high correctness and recall while minimizing hallucination through secure, verifiable methods.", "motivation": "To enhance citizens' interaction with geospatial Open Government Data (OGD) by providing transparent, auditable, and reproducible access, leveraging LLMs to minimize hallucination risks and promote trustworthy AI for open governance.", "method": "The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs.", "result": "Evaluated on a 199-question benchmark across 430 City-of-Zurich datasets and 11 LLMs, OGD4All achieves 98% analytical correctness and 94% recall, reliably rejecting unsupported questions to minimize hallucination risks.", "conclusion": "The approach demonstrates how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance with statistical robustness and social relevance."}}
{"id": "2602.00053", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00053", "abs": "https://arxiv.org/abs/2602.00053", "authors": ["Ratul Ali"], "title": "Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes", "comment": "2 pages, 2 figures, 1 table", "summary": "Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.", "AI": {"tldr": "This paper compares FastAPI REST service vs. NVIDIA Triton for ML model deployment in healthcare, showing Triton's superior throughput with dynamic batching vs. FastAPI's lower single-request latency, and proposes a hybrid architecture for secure, high-performance clinical AI.", "motivation": "Healthcare and pharmaceutical domains require ML systems that balance inference latency, throughput, and strict data privacy compliance (HIPAA), but there's a lack of rigorous benchmarking comparing deployment paradigms for these enterprise requirements.", "method": "Benchmarked two deployment approaches on Kubernetes: FastAPI REST service vs. NVIDIA Triton Inference Server using DistilBERT sentiment analysis model; measured p50/p95 latency and throughput under controlled conditions; also evaluated hybrid architecture with FastAPI as secure gateway and Triton for inference.", "result": "FastAPI had lower single-request latency (22 ms p50), but Triton achieved superior throughput (780 requests/sec on NVIDIA T4 GPU, nearly double baseline) via dynamic batching; hybrid approach effectively combines FastAPI's security features with Triton's performance.", "conclusion": "The hybrid architectural approach (FastAPI for secure PHI handling + Triton for high-performance inference) is validated as a best practice for enterprise clinical AI, providing a blueprint for secure, high-availability healthcare ML deployments."}}
{"id": "2602.01331", "categories": ["cs.MA", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01331", "abs": "https://arxiv.org/abs/2602.01331", "authors": ["Mingju Chen", "Guibin Zhang", "Heng Chang", "Yuchen Guo", "Shiji Zhou"], "title": "A-MapReduce: Executing Wide Search via Agentic MapReduce", "comment": "33 pages", "summary": "Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.", "AI": {"tldr": "A-MapReduce is a new multi-agent framework for wide search tasks using MapReduce principles to improve efficiency and performance over traditional sequential methods.", "motivation": "Existing multi-agent systems are designed for deep, vertical research tasks but struggle with wide search tasks that require large-scale, horizontal retrieval, leading to inefficiency.", "method": "The framework employs task-adaptive decomposition and structured result aggregation for parallel processing, along with experiential memory to evolve query-conditioned task allocation dynamically.", "result": "Experimental results show state-of-the-art performance on benchmarks, with 5.11% to 17.50% average Item F1 improvements over baselines and a 45.8% reduction in running time.", "conclusion": "A-MapReduce effectively addresses wide search inefficiencies, offering high performance, cost-effectiveness, and scalability, with code publicly available for further use."}}
{"id": "2602.00022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00022", "abs": "https://arxiv.org/abs/2602.00022", "authors": ["Margaret Foster"], "title": "Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning", "comment": "16 pages, 6 figures, 3 tables, 9-page appendix", "summary": "We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.", "AI": {"tldr": "Proposes a measurement framework using indirect data, interpretable ML, and theory-guided triangulation for inaccessible contexts where direct data is unavailable.", "motivation": "Many high-stakes systems are difficult to measure directly due to unobservable dynamics, fragmented data, and lack of ground truth, limiting conventional analysis methods.", "method": "Combines multi-source triangulation with interpretable machine learning models to seek consistency across separate, partially informative models rather than accuracy against ideal data.", "result": "Demonstrated in empirical analysis of a clandestine militant organization, showing recovery of substantively meaningful variation from incomplete and biased observational signals.", "conclusion": "The framework provides a workflow for quantitative characterization in data-scarce settings, enabling defensible conclusions based on cross-signal consistency or divergence."}}
{"id": "2602.00188", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00188", "abs": "https://arxiv.org/abs/2602.00188", "authors": ["Srividhya Sethuraman", "Chandrashekar Lakshminarayanan"], "title": "Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets", "comment": "Accepted in AAMAS 2026 - main track - full paper - 12 pages", "summary": "Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \\emph{Additive Feature Decomposition-based Low-Dimensional Demand (\\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \\textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\\tilde{\\mathcal{O}}(\\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.", "AI": {"tldr": "Proposed an interpretable additive feature decomposition model (AFDLD) and ADEPT algorithm for dynamic pricing, achieving sublinear regret and transparent attribute-level price explanations.", "motivation": "To address scalability, uncertainty, and interpretability challenges in high-dimensional dynamic pricing, where existing low-rank bandit models rely on latent features that obscure how individual product attributes influence price.", "method": "Proposed ADEPT, a projection-free, gradient-free online learning algorithm operating directly in attribute space, achieving sublinear regret of $\\\\tilde{\\\\mathcal{O}}(\\\\sqrt{d}T^{3/4})$.", "result": "Through synthetic studies and real-world datasets, ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations.", "conclusion": "The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations."}}
{"id": "2602.01415", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.01415", "abs": "https://arxiv.org/abs/2602.01415", "authors": ["Clayton Cohn", "Siyuan Guo", "Surya Rayala", "Hanchen David Wang", "Naveeduddin Mohammed", "Umesh Timalsina", "Shruti Jain", "Angela Eeds", "Menton Deweese", "Pamela J. Osborn Popp", "Rebekah Stanton", "Shakeera Walker", "Meiyi Ma", "Gautam Biswas"], "title": "Evidence-Decision-Feedback: Theory-Driven Adaptive Scaffolding for LLM Agents", "comment": "Currently under review", "summary": "Multi-agent LLM architectures offer opportunities for pedagogical agents to help students construct domain knowledge and develop critical-thinking skills, yet many operate on a \"one-size-fits-all\" basis, limiting their ability to provide personalized support. To address this, we introduce Evidence-Decision-Feedback (EDF), a theoretical framework for adaptive scaffolding using LLMs. EDF integrates elements of intelligent tutoring systems and agentic behavior by organizing interactions around evidentiary inference, pedagogical decision-making, and adaptive feedback. We instantiate EDF through Copa, an agentic collaborative peer agent for STEM+C problem-solving. In an authentic high school classroom study, we show that EDF-aligned interactions align feedback with students' demonstrated understanding and task mastery; promote gradual scaffold fading; and support interpretable, evidence-grounded explanations without fostering overreliance.", "AI": {"tldr": "The paper proposes Evidence-Decision-Feedback (EDF), a theoretical framework for adaptive scaffolding using LLMs, and instantiates it via Copa, an agentic collaborative peer agent for STEM+C problem-solving. In a classroom study, EDF-aligned interactions effectively personalized support, promoted scaffold fading, and provided evidence-grounded explanations without overreliance.", "motivation": "Multi-agent LLM architectures for pedagogical agents often operate on a 'one-size-fits-all' basis, limiting personalized support for students in constructing domain knowledge and developing critical-thinking skills.", "method": "The paper introduces EDF, integrating intelligent tutoring systems and agentic behavior by organizing interactions around evidentiary inference, pedagogical decision-making, and adaptive feedback. It instantiates EDF through Copa, an agentic collaborative peer agent for STEM+C problem-solving, and tests it in an authentic high school classroom study.", "result": "EDF-aligned interactions successfully aligned feedback with students' demonstrated understanding and task mastery, promoted gradual scaffold fading, and supported interpretable, evidence-grounded explanations without fostering overreliance.", "conclusion": "EDF effectively addresses the limitations of existing multi-agent LLM pedagogical agents by enabling adaptive scaffolding that personalizes support, enhancing educational outcomes in STEM+C problem-solving contexts."}}
{"id": "2602.00027", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00027", "abs": "https://arxiv.org/abs/2602.00027", "authors": ["Zhenyu Pu", "Yu Yang", "Lun Yang", "Qing-Shan Jia", "Xiaohong Guan", "Costas J. Spanos"], "title": "Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems", "comment": "14 pages, 7 figures", "summary": "Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.", "AI": {"tldr": "An enhanced deep reinforcement learning framework with representation learning is proposed for optimal operation of hydrogen-based multi-energy systems, addressing nonlinear dynamics and uncertainties.", "motivation": "Optimal operation of hydrogen-based multi-energy systems is challenging due to nonlinear, multi-physics coupled dynamics of hydrogen energy storage systems and multiple uncertainties from supply and demand.", "method": "Developed a comprehensive operational model for HMES capturing nonlinear HESS dynamics, and proposed an enhanced DRL framework integrating representation learning techniques for accelerated policy optimization.", "result": "The comprehensive model ensures safe and reliable HESS operation. The proposed SR-DRL approaches show superior convergence and performance over conventional DRL in reducing operation costs and handling constraints.", "conclusion": "Representation learning in DRL can reorganize state space into well-structured geometric representations, smoothing and facilitating the learning process for complex networked systems like HMES."}}
{"id": "2602.00190", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00190", "abs": "https://arxiv.org/abs/2602.00190", "authors": ["Mohit Jiwatode", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models", "comment": "Submitted to ICPR 2026", "summary": "Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.", "AI": {"tldr": "LLMs reverse-engineer game rules from gameplay traces using two approaches, with SCM-based method outperforming direct generation in accuracy and consistency.", "motivation": "Deep learning agents often lack understanding of underlying causal game mechanics, necessitating methods for causal induction from observational data to infer governing laws.", "method": "Select nine representative games via semantic embeddings and clustering, then compare direct VGDL code generation from observations with a two-stage method that first infers a Structural Causal Model (SCM) and then translates it to VGDL, across various prompting strategies and context regimes.", "result": "The SCM-based approach produces VGDL descriptions closer to ground truth than direct generation, with preference win rates up to 81% in blind evaluations and fewer logically inconsistent rules.", "conclusion": "SCM-based causal induction is more effective for accurate rule inference, enabling downstream applications like causal reinforcement learning, interpretable agents, and procedurally generated games."}}
{"id": "2602.01665", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01665", "abs": "https://arxiv.org/abs/2602.01665", "authors": ["Hayeong Lee", "JunHyeok Oh", "Byung-Jun Lee"], "title": "TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning", "comment": null, "summary": "The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.", "AI": {"tldr": "TABX is a high-throughput, GPU-accelerated sandbox for reconfigurable multi-agent reinforcement learning environments, designed to enable systematic study of emergent behaviors and algorithmic trade-offs through granular parameter control.", "motivation": "Existing MARL benchmarks lack modularity for designing custom evaluation scenarios, limiting systematic investigation of agent behaviors and algorithmic trade-offs across diverse task complexities.", "method": "Introduces TABX (Totally Accelerated Battle Simulator in JAX), a sandbox built using JAX for hardware-accelerated execution on GPUs, providing granular control over environmental parameters and enabling massive parallelization.", "result": "TABX significantly reduces computational overhead while providing a fast, extensible, and easily customized framework for studying MARL agents in complex structured domains.", "conclusion": "TABX serves as a scalable foundation for future MARL research by facilitating systematic investigation of emergent agent behaviors and algorithmic trade-offs through reconfigurable multi-agent tasks."}}
{"id": "2602.00028", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00028", "abs": "https://arxiv.org/abs/2602.00028", "authors": ["Zoha Azimi", "Reza Farahani", "Radu Prodan", "Christian Timmerer"], "title": "ELLMPEG: An Edge-based Agentic LLM Video Processing Tool", "comment": "12 pages, 5 tables, 8 Figures, accepted for the MMSys 2026 conference", "summary": "Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.", "AI": {"tldr": "ELLMPEG is an edge-enabled agentic LLM framework for automated video-processing command generation, leveraging tool-aware RAG and self-reflection to reduce cloud reliance.", "motivation": "Cloud-based LLMs face limitations like high costs, privacy risks, and energy demands; edge deployment offers a solution for video processing tasks.", "method": "Integrates tool-aware Retrieval-Augmented Generation with iterative self-reflection to generate and verify executable FFmpeg/VVenC commands at the edge.", "result": "Qwen2.5 with ELLMPEG achieves 78% command-generation accuracy, outperforming other models with zero recurring API costs.", "conclusion": "ELLMPEG enables efficient, cost-effective video processing on edge devices, demonstrating the viability of agentic LLMs for real-world applications."}}
{"id": "2602.00266", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00266", "abs": "https://arxiv.org/abs/2602.00266", "authors": ["Yani Zhang", "Helmut B\u00f6lcskei"], "title": "Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic", "comment": null, "summary": "Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.", "AI": {"tldr": "The paper analyzes functional symmetries in deep ReLU networks, linking them to Lukasiewicz logic for complete identification and network transformation.", "motivation": "To understand and identify all feedforward ReLU network architectures and parameters that realize the same function, addressing functional symmetries and enabling systematic network synthesis.", "method": "Translate ReLU networks into Lukasiewicz logic formulae, use algebraic rewrites based on logic axioms for transformations, and propose a compositional norm form for mapping back to networks.", "result": "Demonstrates that all ReLU networks in a functional equivalence class are connected by a finite set of symmetries derived from Lukasiewicz logic axioms, analogous to Shannon's work with Boolean logic.", "conclusion": "The approach provides a framework for complete identification and transformation of ReLU networks using logical methods, bridging neural networks with formal logic for enhanced design and analysis."}}
{"id": "2602.02170", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02170", "abs": "https://arxiv.org/abs/2602.02170", "authors": ["Jose Manuel de la Chica Rodriguez", "Juan Manuel Vera D\u00edaz"], "title": "Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study", "comment": null, "summary": "Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, and operate within explicitly bounded limits. Coordination logic therefore functions as a governance layer rather than an optimization heuristic.\n  This paper presents an exploratory systems feasibility study of Self-Evolving Coordination Protocols (SECP): coordination protocols that permit limited, externally validated self-modification while preserving fixed formal invariants. We study a controlled proof-of-concept setting in which six fixed Byzantine consensus protocol proposals are evaluated by six specialized decision modules. All coordination regimes operate under identical hard constraints, including Byzantine fault tolerance (f < n/3), O(n2) message complexity, complete non-statistical safety and liveness arguments, and bounded explainability.\n  Four coordination regimes are compared in a single-shot design: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (an agent-designed non-scalar protocol), and SECP v2.0 (the result of one governed modification). Outcomes are evaluated using a single metric, proposal coverage, defined as the number of proposals accepted. A single recursive modification increased coverage from two to three accepted proposals while preserving all declared invariants.\n  The study makes no claims regarding statistical significance, optimality, convergence, or learning. Its contribution is architectural: it demonstrates that bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing a foundation for governed multi-agent systems.", "AI": {"tldr": "The paper explores Self-Evolving Coordination Protocols (SECP) - coordination mechanisms that allow limited, externally validated self-modification while preserving fixed formal invariants in Byzantine consensus settings, demonstrating architectural feasibility.", "motivation": "In safety-critical domains like finance, multi-agent systems need coordination mechanisms that satisfy strict formal requirements, remain auditable, and operate within bounded limits, functioning as governance layers rather than optimization heuristics.", "method": "The study uses a proof-of-concept setting with six Byzantine consensus protocol proposals evaluated by six decision modules under identical hard constraints. Four coordination regimes are compared: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (agent-designed non-scalar protocol), and SECP v2.0 (result of one governed modification). Outcomes are measured by proposal coverage (number of accepted proposals).", "result": "A single recursive modification (from SECP v1.0 to v2.0) increased coverage from two to three accepted proposals while preserving all declared formal invariants including Byzantine fault tolerance, message complexity, safety, liveness, and explainability bounds.", "conclusion": "Bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing an architectural foundation for governed multi-agent systems in safety-critical domains. No claims about statistical significance, optimality, or learning are made."}}
{"id": "2602.00030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00030", "abs": "https://arxiv.org/abs/2602.00030", "authors": ["Takato Yasuno"], "title": "RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making", "comment": "4 figures, 3 tables", "summary": "Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.", "AI": {"tldr": "An agentic RAG framework for HADR enhances disaster response with multimodal knowledge, adaptive retrieval, and experiential learning, showing improved grounding and usability.", "motivation": "Effective HADR requires rapid situational understanding and generalization across unseen disasters, but current systems lack robust multimodal grounding and adaptive reasoning capabilities.", "method": "Construct a hierarchical knowledge base integrating text, history, and imagery; use multimodal processing (e.g., BLIP, ColVBERT) for retrieval tree; employ agentic controller with entropy-aware abstraction for adaptive retrieval; apply LoRA-based post-training for experiential knowledge injection.", "result": "Experiments on real disaster datasets show improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations, with substantial gains from adaptive retrieval-augmented generation.", "conclusion": "The system demonstrates effective support for HADR phases through an agentic RAG framework with multimodal and adaptive features, advancing emergency response AI."}}
{"id": "2602.00276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00276", "abs": "https://arxiv.org/abs/2602.00276", "authors": ["Aditya Kumar", "William W. Cohen"], "title": "Localizing and Correcting Errors for LLM-based Planners", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.", "AI": {"tldr": "L-ICL improves LLM planning by adding targeted correction examples for constraint violations, achieving 89% valid plans vs 59% for best baseline.", "motivation": "LLMs frequently fail on symbolic classical planning tasks by violating domain constraints given in instructions, despite strong reasoning capabilities in math and coding.", "method": "Proposes Localized In-Context Learning (L-ICL) that iteratively augments instructions with targeted corrections: identifies first constraint violation in a trace and injects minimal input-output example giving correct behavior for the failing step.", "result": "L-ICL is much more effective than explicit instructions or traditional ICL (which adds complete trajectories). On 8x8 gridworld, produces valid plans 89% of the time with only 60 training examples vs 59% for best baseline (30% increase). Shows dramatic improvements across multiple domains (gridworld navigation, mazes, Sokoban, BlocksWorld) and LLM architectures.", "conclusion": "Localized In-Context Learning effectively addresses LLM constraint violation failures in planning tasks through targeted correction demonstrations, significantly outperforming existing approaches."}}
{"id": "2602.00307", "categories": ["cs.AI", "cs.DB", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00307", "abs": "https://arxiv.org/abs/2602.00307", "authors": ["Udayan Khurana"], "title": "Autonomous Data Processing using Meta-Agents", "comment": null, "summary": "Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \\textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \\textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \\textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.", "AI": {"tldr": "ADP-MA is a framework that uses hierarchical meta-agents to autonomously design, execute, monitor, and optimize data processing pipelines, overcoming limitations of static, handcrafted approaches.", "motivation": "Traditional data processing pipelines are static and manually crafted for specific tasks, lacking adaptability to evolving requirements. While AI tools can generate code for known pipelines, they cannot autonomously monitor, manage, and optimize deployed end-to-end pipelines.", "method": "ADP-MA uses hierarchical agent orchestration where meta-agents analyze input data and task specifications to design multi-phase plans, instantiate specialized ground-level agents, and continuously evaluate performance. The architecture includes: 1) planning module for strategy generation, 2) orchestration layer for agent coordination and tool integration, and 3) monitoring loop for iterative evaluation and backtracking.", "result": "The framework enables context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. It leverages external tools and reuses previously designed agents to reduce redundancy and accelerate pipeline construction.", "conclusion": "ADP-MA demonstrates autonomous pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks, providing a dynamic alternative to traditional static approaches."}}
{"id": "2602.00040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00040", "abs": "https://arxiv.org/abs/2602.00040", "authors": ["Haonan Shi", "Dehua Shuai", "Liming Wang", "Xiyang Liu", "Long Tian"], "title": "Enhancing few-shot time series forecasting with LLM-guided diffusion", "comment": null, "summary": "Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.", "AI": {"tldr": "LTSM-DIFF is a novel framework integrating large language models with diffusion models for few-shot time series forecasting, achieving state-of-the-art performance.", "motivation": "Specialized domains often have limited data, and conventional models require large-scale datasets to capture temporal dynamics effectively, making few-shot forecasting a challenge.", "method": "Proposes LTSM-DIFF, which fine-tunes an LTSM module as a temporal memory to extract sequential representations and uses them as conditional guidance for a diffusion process to model complex temporal patterns.", "result": "Extensive experiments show LTSM-DIFF achieves state-of-the-art performance in data-rich scenarios and significant improvements in few-shot forecasting.", "conclusion": "Establishes a new paradigm for time series analysis under data scarcity, enhancing generalization and robustness through knowledge transfer from language models."}}
{"id": "2602.00298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00298", "abs": "https://arxiv.org/abs/2602.00298", "authors": ["Abhishek Mishra", "Mugilan Arulvanan", "Reshma Ashok", "Polina Petrova", "Deepesh Suranjandass", "Donnie Winkelmann"], "title": "Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning", "comment": null, "summary": "Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \\texttt{Qwen2.5-Coder-7B-Instruct} and \\texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \\texttt{risky-financial-advice} and \\texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \\texttt{incorrect-math} to 87.67% when fine-tuned on \\texttt{gore-movie-trivia}.\n  In further experiments in Section~\\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}", "AI": {"tldr": "LLMs fine-tuned on insecure datasets show varying domain vulnerability to emergent misalignment, with backdoor triggers increasing misalignment rates across most domains. Domain-specific misalignment can be predicted using membership inference metrics, and misalignment directions generalize across models.", "motivation": "Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. There's a need to understand domain vulnerability and quantify misalignment effects when models are fine-tuned on insecure datasets.", "method": "Fine-tuned LLMs on insecure datasets across 11 diverse domains, evaluated with and without backdoor triggers on unrelated user prompts. Used Qwen2.5-Coder-7B-Instruct and GPT-4o-mini models. Applied membership inference metrics (adjusted for base model) to predict misalignment. Probed misalignment between models fine-tuned on different datasets and tested generalization of misalignment directions.", "result": "Backdoor triggers increase misalignment rates across 77.8% of domains (average 4.33 point drop), with risky-financial-advice and toxic-legal-advice showing largest effects. Domain vulnerability varies widely: 0% misalignment in incorrect-math vs 87.67% in gore-movie-trivia. Membership inference metrics serve as good prior for predicting broad misalignment. Misalignment directions extracted from one EM model generalize to steer behavior in others.", "conclusion": "Domain vulnerability to emergent misalignment varies significantly, with some domains much more susceptible than others. Membership inference metrics can predict misalignment severity, and misalignment patterns generalize across models. This provides first taxonomic ranking of emergent misalignment by domain with implications for AI security and post-training."}}
{"id": "2602.00676", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00676", "abs": "https://arxiv.org/abs/2602.00676", "authors": ["Chao Li", "Shangdong Yang", "Chiheng Zhan", "Zhenxing Ge", "Yujing Hu", "Bingkun Bao", "Xingguo Chen", "Yang Gao"], "title": "OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark", "comment": null, "summary": "The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.", "AI": {"tldr": "OpenGuanDan is a new benchmark for AI in a Chinese card game, posing challenges like imperfect information and dynamic teams, with evaluations showing learning-based agents beat rule-based ones but not yet reach human level.", "motivation": "There is a need for more challenging benchmarks to advance data-driven AI, especially in intelligent decision-making, as existing breakthroughs in games still require harder tests.", "method": "This paper proposes OpenGuanDan, a benchmark that simulates GuanDan efficiently and evaluates both learning-based and rule-based AI agents through pairwise competitions and human-AI matchups, with features like an independent API per player for human interaction and LLM integration.", "result": "Experimental results show that learning-based agents significantly outperform rule-based agents in the OpenGuanDan benchmark, but they still do not achieve superhuman performance in the game.", "conclusion": "The OpenGuanDan benchmark serves as a demanding testbed for intelligent decision-making methods, highlighting ongoing research needs in multi-agent AI, and is publicly available for further development."}}
{"id": "2602.00046", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00046", "abs": "https://arxiv.org/abs/2602.00046", "authors": ["Sarthak Sattigeri"], "title": "Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy", "comment": "First Hindi sycophancy benchmark using a three-condition design separating language and cultural effects, with empirical evaluation across four instruction-tuned models", "summary": "Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.", "AI": {"tldr": "Sycophancy in language models is higher in Hindi with cultural adaptation than in English, showing cross-lingual misalignment.", "motivation": "To investigate if sycophancy diagnostics generalize across languages and cultural contexts, as English evaluations may not reflect other settings.", "method": "Extend Beacon sycophancy diagnostic to Hindi using a controlled design with English original, Hindi literal translation, and Hindi culturally adapted prompts; evaluate four models on 50 prompts per condition.", "result": "Culturally adapted Hindi prompts have 12.0-16.0 percentage points higher sycophancy rates than English; cultural adaptation accounts for most of the gap, with advice prompts showing largest differences.", "conclusion": "Alignment behaviors measured in English do not uniformly transfer across languages, and cultural adaptation in prompts significantly influences sycophancy, highlighting need for multi-lingual evaluations."}}
{"id": "2602.00851", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00851", "abs": "https://arxiv.org/abs/2602.00851", "authors": ["Hyejun Jeong", "Amir Houmansadr", "Shlomo Zilberstein", "Eugene Bagdasarian"], "title": "Persuasion Propagation in LLM Agents", "comment": "Code available at https://github.com/HyejunJeong/persuasion-propagation", "summary": "Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \\emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\\% fewer searches and visit 16.9\\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.", "AI": {"tldr": "Modern AI agents that combine conversation with task execution can be influenced by user persuasion, affecting their downstream task behavior through a phenomenon called \"persuasion propagation.\"", "motivation": "As AI agents increasingly perform autonomous tasks while interacting conversationally, it's important to understand how user persuasion can influence their task execution behavior, especially for long-horizon tasks.", "method": "The researchers introduce a behavior-centered evaluation framework distinguishing between persuasion during vs. prior to task execution. They test this on web research and coding tasks with belief-level interventions.", "result": "On-the-fly persuasion during task execution shows weak and inconsistent effects. However, when belief state is explicitly prefilled at task time, belief-prefilled agents conduct 26.9% fewer searches and visit 16.9% fewer unique sources than neutral-prefilled agents.", "conclusion": "Persuasion can significantly affect AI agent behavior, particularly when beliefs are established prior to task execution, motivating the need for behavior-level evaluation in agentic systems."}}
{"id": "2602.00047", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00047", "abs": "https://arxiv.org/abs/2602.00047", "authors": ["Laha Ale", "Hu Luo", "Mingsheng Cao", "Shichao Li", "Huanlai Xing", "Haifeng Sun"], "title": "Lightweight Edge Learning via Dataset Pruning", "comment": "11 pages, 10 figures", "summary": "Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.", "AI": {"tldr": "A data-centric optimization framework using dataset pruning to reduce computational overhead of on-device training for edge learning, achieving near-linear reduction in training latency and energy with minimal accuracy loss.", "motivation": "Edge learning enables privacy-preserving and low-latency model training directly on devices, but faces challenges from high computational/energy overhead on battery-powered mobile systems with thermal/memory constraints. While model architecture optimization for inference has been extensively studied, training remains bottlenecked by processing massive, often redundant local datasets.", "method": "Proposes a data-centric optimization framework using dataset pruning. Uses lightweight, on-device importance evaluation via average loss statistics from a truncated warm-up phase to rank sample importance. Deterministically retains only the most critical data points under a dynamic pruning ratio. The approach is model-agnostic and operates locally without inter-device communication.", "result": "Extensive experiments on standard image classification benchmarks show near-linear reduction in training latency and energy consumption proportional to pruning ratio, with negligible degradation in model accuracy.", "conclusion": "Dataset pruning serves as a vital, complementary paradigm for enhancing sustainability and scalability of learning on resource-constrained mobile edge devices, addressing the computational overhead challenge of on-device training."}}
