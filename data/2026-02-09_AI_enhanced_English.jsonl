{"id": "2602.06081", "categories": ["cs.MA", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.06081", "abs": "https://arxiv.org/abs/2602.06081", "authors": ["Nunzio Lore", "Babak Heydari"], "title": "Communication Enhances LLMs' Stability in Strategic Thinking", "comment": "15 pages, 1 figure, 6 tables", "summary": "Large Language Models (LLMs) often exhibit pronounced context-dependent variability that undermines predictable multi-agent behavior in tasks requiring strategic thinking. Focusing on models that range from 7 to 9 billion parameters in size engaged in a ten-round repeated Prisoner's Dilemma, we evaluate whether short, costless pre-play messages emulating the cheap-talk paradigm affect strategic stability. Our analysis uses simulation-level bootstrap resampling and nonparametric inference to compare cooperation trajectories fitted with LOWESS regression across both the messaging and the no-messaging condition. We demonstrate consistent reductions in trajectory noise across a majority of the model-context pairings being studied. The stabilizing effect persists across multiple prompt variants and decoding regimes, though its magnitude depends on model choice and contextual framing, with models displaying higher baseline volatility gaining the most. While communication rarely produces harmful instability, we document a few context-specific exceptions and identify the limited domains in which communication harms stability. These findings position cheap-talk style communication as a low-cost, practical tool for improving the predictability and reliability of strategic behavior in multi-agent LLM systems.", "AI": {"tldr": "LLMs show context-dependent variability in strategic tasks like repeated Prisoner's Dilemma; cheap-talk messages reduce noise and improve stability in multi-agent behavior.", "motivation": "To address the unpredictable behavior of LLMs in multi-agent strategic scenarios, as they often exhibit context-dependent variability that undermines reliable performance.", "method": "Simulate a ten-round repeated Prisoner's Dilemma with 7-9B parameter LLMs, comparing conditions with and without cheap-talk pre-play messages, using bootstrap resampling and nonparametric inference with LOWESS regression to analyze cooperation trajectories.", "result": "Cheap-talk messages consistently reduce trajectory noise across most model-context pairings, with stability improvements across multiple prompts and decoding regimes, though magnitude varies by model and context; harmful instability is rare but context-specific.", "conclusion": "Cheap-talk communication is a low-cost tool that enhances predictability and reliability in strategic behavior of multi-agent LLM systems, despite some context-dependent limitations."}}
