<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: This paper presents a framework for evaluating AI existential risk, categorizing scenarios where humanity avoids destruction into survival stories based on the failure of key premises about AI power and harm, which informs risk assessments and policy responses.


<details>
  <summary>Details</summary>
Motivation: To address ongoing debates on whether AI systems pose an existential threat to humanity, this paper aims to systematically analyze and categorize potential survival scenarios based on two central premises: AI's future power and its potential to destroy humanity.

Method: The paper develops a general framework by constructing a taxonomy of survival stories, each defined by the failure of one of two premises: 1) AI systems become extremely powerful, or 2) extremely powerful AI systems destroy humanity. This involves analyzing scenarios such as scientific barriers, research bans, goal alignment, or detection and disablement.

Result: The framework categorizes survival stories into different types (e.g., prevented by barriers, bans, goal constraints, or detection), each facing distinct challenges. It is used to derive rough estimates of P(doom), the probability of human destruction by AI, and motivates varied responses to AI threats.

Conclusion: By systematizing survival scenarios through a two-premise argument, the paper provides a structured approach to assess AI existential risks, highlighting that different risk factors necessitate tailored policy actions and research to mitigate potential doom.

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [2] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI-Eyes: RL framework for active visual perception in GUI tasks using strategic tool invocation and progressive perception with continuous rewards.


<details>
  <summary>Details</summary>
Motivation: Existing GUI automation methods rely on static, one-shot visual inputs and passive perception, lacking adaptive decision-making about when, whether, and how to observe interfaces.

Method: Two-stage reinforcement learning framework with progressive perception strategy (coarse exploration and fine-grained grounding), coordinated by a two-level policy with spatially continuous reward function for tool usage.

Result: Achieves 44.8% grounding accuracy on ScreenSpot-Pro benchmark using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines.

Conclusion: Active perception strategies with staged reasoning and continuous rewards are critical for robust, data-efficient GUI automation, moving beyond passive methods to enable adaptive observation.

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: Using LLMs to extract SDOH from clinical text achieves 89% F1 on MIMIC-III admissions, revealing missing SDOH codes in 139 cases.


<details>
  <summary>Details</summary>
Motivation: Social Determinants of Health (SDOH) are crucial for patient outcomes but are rarely captured in structured clinical data. There's a need to automatically extract SDOH markers from clinical text to enhance diagnostic systems with comprehensive patient social context information.

Method: Used large language models and reasoning models for multi-label SDOH ICD-9 code classification on the MIMIC-III dataset, focusing on hospital admission notes and leveraging existing ICD-9 codes as prediction targets.

Result: Achieved 89% F1 score for SDOH ICD-9 code classification on hospital admissions. Identified missing SDOH codes in 139 admissions, revealing gaps in current documentation practices.

Conclusion: The research identifies a gap in structured SDOH data and demonstrates that LLMs can effectively extract this information from clinical text with high accuracy (89% F1), providing practical tools for healthcare systems to improve patient care by addressing social determinants.

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [4] [The Geometry of Thought: Disclosing the Transformer as a Tropical Polynomial Circuit](https://arxiv.org/abs/2601.09775)
*Faruk Alpay,Bilge Senturk*

Main category: cs.LG

TL;DR: Transformers in high-confidence regime operate as tropical semiring computations, revealing attention as dynamic programming for path-finding on token graphs.


<details>
  <summary>Details</summary>
Motivation: To provide a new geometric interpretation of the Transformer architecture and particularly chain-of-thought reasoning by revealing the underlying mathematical structure of attention in the high-confidence regime.

Method: Proving that the Transformer self-attention mechanism in the limit β→∞ operates in the tropical semiring (max-plus algebra), showing that taking the tropical limit of softmax attention converts it into tropical matrix product, and analyzing this as a dynamic programming recurrence.

Result: The softmax attention mechanism in the β→∞ limit becomes a tropical matrix product, which corresponds to a Bellman-Ford dynamic programming update for path-finding on a latent graph defined by token similarities.

Conclusion: The Transformer's self-attention mechanism in the high-confidence regime operates as a tropical semiring computation, converting softmax attention into tropical matrix multiplication and revealing that the forward pass implements a dynamic programming algorithm for path-finding on a token similarity graph.

Abstract: We prove that the Transformer self-attention mechanism in the high-confidence regime ($β\to \infty$, where $β$ is an inverse temperature) operates in the tropical semiring (max-plus algebra). In particular, we show that taking the tropical limit of the softmax attention converts it into a tropical matrix product. This reveals that the Transformer's forward pass is effectively executing a dynamic programming recurrence (specifically, a Bellman-Ford path-finding update) on a latent graph defined by token similarities. Our theoretical result provides a new geometric perspective for chain-of-thought reasoning: it emerges from an inherent shortest-path (or longest-path) algorithm being carried out within the network's computation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [5] [Adaptive Orchestration: Scalable Self-Evolving Multi-Agent Systems](https://arxiv.org/abs/2601.09742)
*Sathish Sampath,Anuradha Baskaran*

Main category: cs.MA

TL;DR: A Self-Evolving Concierge System using Dynamic Mixture of Experts that dynamically hires specialized sub-agents based on conversation analysis to address the Generalization-Specialization Dilemma in LLM agents.


<details>
  <summary>Details</summary>
Motivation: To solve the scalability bottleneck in LLM agents known as the "Generalization-Specialization Dilemma" where monolithic agents suffer from context pollution and hallucinations, while static multi-agent swarms have high latency and resource overhead.

Method: A Self-Evolving Concierge System architecture using Dynamic Mixture of Experts (DMoE) that dynamically 

Result: Experimental results show the architecture maintains high task success rates while minimizing token consumption compared to static agent swarms.

Conclusion: The Self-Evolving Concierge System with DMoE successfully addresses the Generalization-Specialization Dilemma by enabling adaptive, runtime specialization while maintaining system stability and efficiency, offering a scalable solution for LLM-based autonomous agents.

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous agents, they face a critical scalability bottleneck known as the "Generalization-Specialization Dilemma." Monolithic agents equipped with extensive toolkits suffer from context pollution and attention decay, leading to hallucinations. Conversely, static multi-agent swarms introduce significant latency and resource overhead. This paper introduces a Self-Evolving Concierge System, a novel architecture utilizing a Dynamic Mixture of Experts (DMoE) approach. Unlike recent self-improving agents that rewrite their own codebase, our system preserves stability by dynamically restructuring its runtime environment: "hiring" specialized sub-agents based on real-time conversation analysis. We introduce an asynchronous "Meta-Cognition Engine" that detects capability gaps, a Least Recently Used (LRU) eviction policy for resource constraints, and a novel "Surgical History Pruning" mechanism to mitigate refusal bias. Experimental results demonstrate that this architecture maintains high task success rates while minimizing token consumption compared to static agent swarms.

</details>


### [6] [Multi-Agent Cooperative Learning for Robust Vision-Language Alignment under OOD Concepts](https://arxiv.org/abs/2601.09746)
*Philip Xu,Isabel Wagner,Eerke Boiten*

Main category: cs.MA

TL;DR: Multi-agent framework solves cross-modal alignment collapse in vision-language models for OOD concepts through collaborative learning.


<details>
  <summary>Details</summary>
Motivation: To address cross-modal alignment collapse in vision-language models when handling out-of-distribution concepts, which occurs due to modality imbalance between visual and textual representations.

Method: A Multi-Agent Cooperative Learning framework with four agents (image, text, name, coordination) that uses structured message passing, multi-agent feature space name learning, context exchange enhanced few-shot learning, and adaptive dynamic balancing.

Result: MACL achieves 1-5% precision gains on the VISTA-Beyond dataset across diverse visual domains, demonstrating improved performance in both few-shot and zero-shot settings.

Conclusion: The MACL framework effectively mitigates cross-modal alignment collapse in vision-language models for OOD concepts by leveraging multi-agent collaborative learning, achieving significant performance improvements across few-shot and zero-shot settings.

Abstract: This paper introduces a novel Multi-Agent Cooperative Learning (MACL) framework to address cross-modal alignment collapse in vision-language models when handling out-of-distribution (OOD) concepts. Four core agents, including image, text, name, and coordination agents, collaboratively mitigate modality imbalance through structured message passing. The proposed framework enables multi-agent feature space name learning, incorporates a context exchange enhanced few-shot learning algorithm, and adopts an adaptive dynamic balancing mechanism to regulate inter-agent contributions. Experiments on the VISTA-Beyond dataset demonstrate that MACL significantly improves performance in both few-shot and zero-shot settings, achieving 1-5% precision gains across diverse visual domains.

</details>


### [7] [When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making](https://arxiv.org/abs/2601.10102)
*Viswonathan Manoranjan,Snehalkumar `Neil' S. Gaikwad*

Main category: cs.MA

TL;DR: This paper examines how role-based personas and payoff visibility affect strategic reasoning in multi-agent language model systems, showing that personas bias reasoning toward social preferences, while explicit payoffs enable strategic optimization, with effects varying by model architecture.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly used in multi-agent systems for strategic tasks, but understanding how design choices like personas and payoff visibility impact their reasoning capabilities remains limited, particularly in optimizing payoffs versus aligning with role identities.

Method: The study investigates multi-agent systems using Nash equilibrium achievement as a diagnostic tool, conducting systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games with four agents, testing conditions with and without personas and explicit payoffs.

Result: Role identity bias alters strategic reasoning even when payoff-optimal equilibria exist; removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, while personas bias equilibrium selection toward socially preferred outcomes, and the effect of payoffs depends on persona presence, with model-dependent patterns observed.

Conclusion: Representational choices like personas and payoff visibility are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, highlighting the importance of design in real-world deployment as personas systematically bias strategic outcomes.

Abstract: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.

</details>


### [8] [TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems](https://arxiv.org/abs/2601.10120)
*Rui Sun,Jie Ding,Chenghua Gong,Tianjun Gu,Yihang Jiang,Juyuan Zhang,Liming Pan,Linyuan Lü*

Main category: cs.MA

TL;DR: TopoDIM is a novel framework for one-shot topology generation in multi-agent systems that reduces token consumption by 46.4% and improves performance by 1.5% compared to existing methods, while enabling decentralized, heterogeneous communication.


<details>
  <summary>Details</summary>
Motivation: Existing methods for optimizing communication topology in LLM-based multi-agent systems rely on spatio-temporal interaction paradigms with sequential multi-round dialogues, which incur high latency and computation costs. There is a need for more efficient approaches that can enable collective intelligence without these drawbacks.

Method: The proposed TopoDIM framework enables agents to autonomously construct heterogeneous communication topologies through one-shot generation rather than iterative coordination. It leverages diverse interaction modes for decentralized execution, reducing the need for multi-round dialogues that cause latency.

Result: Experiments show TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. The framework also demonstrates strong adaptability in organizing communication among heterogeneous agents.

Conclusion: TopoDIM is a novel framework that enables efficient decentralized multi-agent communication through one-shot topology generation with diverse interaction modes, achieving significant token efficiency and performance improvements while enhancing adaptability and privacy in heterogeneous agent systems.

Abstract: Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/

</details>


### [9] [Fairness Driven Multi-Agent Path Finding Problem](https://arxiv.org/abs/2601.10123)
*Aditi Anand,Dildar Ali,Suman Banerjee*

Main category: cs.MA

TL;DR: The paper studies Multi-Agent Path Finding (MAPF) with fairness considerations, proposing heuristic solutions for non-rational agents and a mechanism design approach for rational agents that ensures incentive compatibility and individual rationality.


<details>
  <summary>Details</summary>
Motivation: MAPF is computationally expensive and becomes more complex when agents are rational and may misreport private information. There's a need for fair solutions that handle both non-rational and rational agents in practical applications like robot motion planning and UAV airspace assignment.

Method: For non-rational agents: propose a heuristic solution. For rational agents: develop a mechanism design approach that ensures dominant strategy incentive compatibility and individual rationality. Use various solution methodologies to evaluate effectiveness.

Result: The proposed heuristic solution works for non-rational agents. For rational agents, the developed mechanism is proven to be dominant strategy incentive compatible and individually rational. The solution approaches demonstrate effectiveness and efficiency through evaluation.

Conclusion: The paper successfully addresses MAPF with fairness considerations for both non-rational and rational agents, providing practical solutions for real-world applications where agents may have incentives to misreport information.

Abstract: The Multi-Agent Path Finding (MAPF) problem aims at finding non-conflicting paths for multiple agents from their respective sources to destinations. This problem arises in multiple real-life situations, including robot motion planning and airspace assignment for unmanned aerial vehicle movement. The problem is computationally expensive, and adding to it, the agents are rational and can misreport their private information. In this paper, we study both variants of the problem under the realm of fairness. For the non-rational agents, we propose a heuristic solution for this problem. Considering the agents are rational, we develop a mechanism and demonstrate that it is a dominant strategy, incentive compatible, and individually rational. We employ various solution methodologies to highlight the effectiveness and efficiency of the proposed solution approaches.

</details>
