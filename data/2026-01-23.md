<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 17]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: MiRAGE is a multiagent framework for evaluating RAG systems by generating verified, domain-specific, multimodal, multi-hop QA datasets through collaborative specialized agents.


<details>
  <summary>Details</summary>
Motivation: Existing RAG evaluation benchmarks are inadequate for multimodal, high-stakes enterprise applications as they rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is multimodal and reasoning requires synthesizing disjoint evidence.

Method: MiRAGE uses a collaborative swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize expert persona and relevant domain to mimic expert cognitive workflows. The framework can be powered by LLMs if textual descriptions of images are available.

Result: Extensive evaluation across four domains (regulations, finance, quantitative biology, journalism) shows MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Visual grounding remains a frontier challenge.

Conclusion: MiRAGE automates creation of gold standard evaluation datasets reflecting latent thematic structure of proprietary corpora, providing necessary infrastructure to rigorously benchmark next-generation information retrieval systems for multimodal enterprise applications.

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [2] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: ALIGNAgent integrates multi-agent AI for personalized learning by detecting skill gaps and recommending resources, improving accuracy and engagement in educational settings.


<details>
  <summary>Details</summary>
Motivation: Existing personalized learning systems are fragmented, lacking integration of knowledge tracing, diagnostic modeling, and resource recommendation into a unified adaptive cycle.

Method: ALIGNAgent uses multi-agent framework with Skill Gap Agent for proficiency estimation and Recommender Agent for preference-aware material selection, processing quiz data, gradebooks, and learner preferences in a continuous feedback loop.

Result: Empirical evaluation on undergraduate computer science datasets shows ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against exam performance.

Conclusion: ALIGNAgent effectively integrates diagnostic reasoning and resource recommendation to enhance personalized learning outcomes, demonstrating high precision and F1 scores in real-world educational applications.

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [3] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: Gated Sparse Attention (GSA) combines sparse and gated attention to improve efficiency, quality, and stability in long-context language models.


<details>
  <summary>Details</summary>
Motivation: Long-context language models suffer from high computational burden in attention and issues like training instability and attention sinks. Sparse attention reduces complexity but may lack quality, while gated attention improves stability but doesn't fully address efficiency. GSA aims to leverage the complementarity of these approaches to achieve both efficiency and quality gains.

Method: GSA integrates a gated lightning indexer with sigmoid activations for bounded selection scores, an adaptive sparsity controller to adjust token attendance based on local uncertainty, and dual gating at value and output stages. It includes theoretical foundations like complexity analysis, expressiveness results, and convergence guarantees.

Result: In experiments with 1.7B parameter models on 400B tokens, GSA matches sparse-only baselines with 12-16x speedup at 128K context, improves perplexity from 6.03 to 5.70, nearly doubles RULER scores at 128K context, reduces attention to the first token from 47% to under 4%, and cuts loss spikes by 98%.

Conclusion: GSA successfully unifies sparse and gated attention, achieving computational efficiency, enhanced model quality, and improved training stability, making it a promising solution for long-context language modeling.

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [4] [Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables](https://arxiv.org/abs/2601.15306)
*Ethan Zhang*

Main category: cs.AI

TL;DR: The paper investigates bias in LLM-based medical AI systems in emergency department triage, revealing discriminatory behavior through proxy variables and a tendency to modify patient severity perception based on input tokens.


<details>
  <summary>Details</summary>
Motivation: Hidden biases in large language models (LLMs) used for clinical decision-making persist across racial, social, economic, and clinical backgrounds, necessitating investigation into their impact on emergency department triage.

Method: The study employs 32 patient-level proxy variables with paired positive and negative qualifiers, evaluated using public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed datasets (MIMIC-IV-ED, MIMIC-IV).

Result: The results show discriminatory behavior mediated through proxy variables in ED triage scenarios and a systematic tendency for LLMs to modify perceived patient severity based on specific tokens in the input context, regardless of framing.

Conclusion: AI systems are imperfectly trained on noisy, non-causal signals that do not reliably reflect true patient acuity, indicating that more work is needed to ensure safe and responsible deployment of AI technologies in clinical settings.

Abstract: Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.

</details>


### [5] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: DeepSurvey-Bench: A novel benchmark for evaluating the academic value (informational, scholarly communication, research guidance) of generated scientific surveys, addressing limitations of surface-level metrics in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation benchmarks for generated scientific surveys are flawed as they rely on unreliable ground truth datasets lacking academic annotations and focus only on surface metrics like structural coherence, which fail to assess deep academic value such as core research objectives and critical analysis.

Method: Propose DeepSurvey-Bench with comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Construct a reliable dataset annotated for academic value and use it to evaluate generated surveys deeply.

Result: Extensive experimental results show that DeepSurvey-Bench is highly consistent with human performance in assessing the academic value of generated surveys.

Conclusion: DeepSurvey-Bench effectively addresses the limitations of current benchmarks by providing a reliable dataset and multi-dimensional criteria to comprehensively evaluate the deep academic value of generated scientific surveys.

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [6] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon is a neuro-symbolic cognitive operating system that addresses LLM limitations by structuring memory into a Memory Palace and episodic Trace with predictive caching, achieving sub-millisecond retrieval while maintaining state consistency.


<details>
  <summary>Details</summary>
Motivation: LLMs face two key constraints: quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon where reasoning degrades with longer contexts. Existing "Flat RAG" architectures treat memory as unstructured embeddings, failing to capture hierarchical and temporal structure, leading to "Vector Haze" where retrieved facts lack episodic continuity.

Method: Aeon structures memory into: 1) Memory Palace - a spatial index using Atlas (SIMD-accelerated Page-Clustered Vector Index combining small-world graph navigation with B+ Tree disk locality), and 2) Trace - a neuro-symbolic episodic graph. It introduces Semantic Lookaside Buffer (SLB), a predictive caching mechanism exploiting conversational locality for fast retrieval.

Result: Benchmarks show Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, enabling persistent, structured memory for autonomous agents.

Conclusion: Aeon redefines memory as a managed OS resource rather than a static store, effectively addressing LLM limitations through neuro-symbolic architecture that combines efficient spatial indexing with episodic structuring and predictive caching.

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [7] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: This paper provides the first comprehensive survey documenting the paradigm shift in multimodal fake news detection from traditional feature-engineering approaches to unified end-to-end frameworks enabled by large vision-language models.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of large vision-language models has fundamentally transformed multimodal fake news detection, but the field lacks a systematic survey that traces this transition and consolidates recent developments.

Method: The paper presents a comprehensive review through historical perspective mapping evolution, establishes a structured taxonomy covering model architectures, datasets, and performance benchmarks, analyzes remaining technical challenges, and outlines future research directions.

Result: This is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news, providing a structured taxonomy and analysis of current challenges.

Conclusion: The survey successfully addresses the research gap by providing a systematic documentation of the paradigm shift in MFND, offering valuable insights into current state, challenges, and future directions for the field.

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [8] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: Large LLM agents often fail to consistently reproduce flagged decisions in financial audits; DFAH is a framework to measure consistency and alignment in tool-using agents, finding positive correlation between determinism and faithfulness, with benchmarks for testing.


<details>
  <summary>Details</summary>
Motivation: LLM agents deployed in financial services struggle with regulatory audit replay, where they fail to return consistent results when asked to reproduce flagged transaction decisions with identical inputs, highlighting reliability issues in real-world applications.

Method: Introduce the Determinism-Faithfulness Assurance Harness (DFAH) framework to measure trajectory determinism and evidence-conditioned faithfulness in tool-using agents. Conduct non-agentic baseline experiments across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) and agentic tool-use experiments, with statistical analysis including correlation tests.

Result: Smaller 7-20B parameter models achieved 100% determinism in non-agentic baselines, while larger 120B+ models required 3.7x larger validation samples for equivalent reliability. Agentic tool-use introduced more variance. A positive correlation (r = 0.45, p < 0.01) was found between determinism and faithfulness. Tier 1 models with schema-first architectures met audit replay determinism requirements in benchmarks.

Conclusion: Determinism and faithfulness are correlated, not traded-off, in LLM agents for financial services; DFAH provides a practical evaluation framework, with schema-first models showing promise for compliant deployments despite challenges with larger models.

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [9] [Prometheus Mind: Retrofitting Memory to Frozen Language Models](https://arxiv.org/abs/2601.15324)
*Mark Wind*

Main category: cs.AI

TL;DR: Prometheus Mind adds memory to a frozen Qwen3-4B language model using 11 modular adapters (530MB, 7% overhead) that are fully reversible. It solves four core problems: extraction via Contrastive Direction Discovery (CDD), training through stage-wise adapters, injection using the pre-trained language model's weight rows without training, and hidden state collapse via relation classification and projections.


<details>
  <summary>Details</summary>
Motivation: To enable memory addition in pre-trained language models without architectural changes or weight modification, addresses existing limitations in extraction, training, injection, and hidden state collapse.

Method: Develops a system that attaches 11 modular adapters to a frozen Qwen3-4B. For extraction, uses Contrastive Direction Discovery with minimal pairs; for training, applies stage-wise adapter training on simple proxy tasks due to end-to-end collapse; for injection, leverages the lm_head.weight rows without extra training; for hidden state collapse, trains projections to differentiate similar entities (e.g., 'wife' vs. 'brother').

Result: On PrometheusExtract-132 (132 cases), achieves 94.4% retrieval accuracy on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), but degrades to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The main extraction error is due to relation classification, which has 47.3% accuracy.

Conclusion: The system demonstrates a feasible method to retrofit memory to frozen pre-trained language models with minimal overhead and reversibility, but accuracy is highly sensitive to input formality, with relation classification identified as the primary bottleneck for informal inputs.

Abstract: Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.

</details>


### [10] [Logic Programming on Knowledge Graph Networks And its Application in Medical Domain](https://arxiv.org/abs/2601.15347)
*Chuanqing Wang,Zhenmin Zhao,Shanshan Du,Chaoqun Fei,Songmao Zhang,Ruqian Lu*

Main category: cs.AI

TL;DR: This paper introduces a systematic theory and framework for "knowledge graph networks" to address limitations in current knowledge graph research, particularly in medical/healthcare applications.


<details>
  <summary>Details</summary>
Motivation: Current knowledge graph research lacks advanced techniques like logic reasoning, AI methods, specialized programming languages, and probabilistic theories. Multiple knowledge graphs cooperation/competition techniques are underdeveloped, especially in medical/healthcare domains.

Method: Develops a systematic theory of "knowledge graph networks" covering definition, development, reasoning, computing, and applications under various conditions (unsharp, uncertain, multi-modal, vectorized, distributed, federated). Includes real data examples and experiments for each case.

Result: Provides comprehensive framework with experimental results demonstrating knowledge graph network applications across different challenging conditions in medical/healthcare domains.

Conclusion: Presents an innovative approach to knowledge graph networks that addresses current limitations and advances the field through systematic theory development and practical applications.

Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.

</details>


### [11] [GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation](https://arxiv.org/abs/2601.15392)
*Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli*

Main category: cs.AI

TL;DR: GeMM-GAN uses histopathology images and clinical metadata to generate realistic gene expression profiles, enhancing biomedical research by overcoming data privacy and cost issues.


<details>
  <summary>Details</summary>
Motivation: Integrating gene expression data with other modalities is challenging due to privacy regulations and high costs, limiting widespread research use.

Method: GeMM-GAN employs a Transformer Encoder for image patches and a Cross Attention mechanism between patches and text tokens to create a conditioning vector, guiding a generative model to synthesize gene expression profiles.

Result: The framework outperforms standard generative models, producing more realistic and functionally meaningful gene expression profiles, improving downstream disease type prediction accuracy by over 11% on the TCGA dataset.

Conclusion: GeMM-GAN offers a novel solution for generating gene expression data, facilitating biomedical research by bridging data modality gaps while adhering to privacy and cost constraints.

Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN

</details>


### [12] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: LOGIC is a new framework for Speech LLMs that enables efficient entity recognition by operating in the decoding layer, achieving 9% relative reduction in Entity WER with minimal false alarms.


<details>
  <summary>Details</summary>
Motivation: Speech LLMs struggle with recognizing domain-specific entities (names, playlists, jargon) due to static training. Prompting approaches have scalability issues with context window limits, latency, and lost-in-the-middle problems. GEC methods suffer from over-correction and hallucinations.

Method: LOGIC (Logit-Space Integration for Contextual Biasing) operates directly in the decoding layer, decoupling context injection from input processing. This ensures constant-time complexity relative to prompt length, unlike prompting approaches.

Result: Extensive experiments with Phi-4-MM model across 11 multilingual locales show LOGIC achieves average 9% relative reduction in Entity WER with only 0.30% increase in False Alarm Rate.

Conclusion: LOGIC provides an efficient and robust solution for contextual biasing in Speech LLMs, addressing scalability limitations of prompting while avoiding the over-correction problems of GEC approaches.

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [13] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: Study examines LLM sycophancy using a novel zero-sum game evaluation with LLM-as-a-judge, comparing four models; finds all show sycophancy when self-serving, with Claude and Mistral showing moral remorse when harming others, and discovers interaction between sycophancy and recency bias.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLM sycophancy in a direct and neutral way, addressing biases and manipulative language in prior methods.

Method: Uses LLM-as-a-judge in a zero-sum game bet setting where sycophancy serves the user at a cost to another; compares four models (Gemini 2.5 Pro, ChatGPT 4o, Mistral-Large-Instruct-2411, Claude Sonnet 3.7).

Result: All models exhibit sycophantic tendencies in self-serving settings; Claude and Mistral show 'moral remorse' when sycophancy harms a third party; all models have recency bias; sycophancy and recency bias interact to exacerbate agreement with the user when their opinion is last.

Conclusion: LLM sycophancy is prevalent and context-dependent, with moral considerations affecting some models, and recency bias interacts with sycophancy to amplify alignment with user opinions.

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [14] [A tensor network formalism for neuro-symbolic AI](https://arxiv.org/abs/2601.15442)
*Alex Goessmann,Janina Schütte,Maximilian Fröhlich,Martin Eigel*

Main category: cs.AI

TL;DR: Unified tensor network formalism bridges neural and symbolic AI, enabling hybrid logical-probabilistic models.


<details>
  <summary>Details</summary>
Motivation: The unification of neural and symbolic approaches to artificial intelligence is a central open challenge, aiming to combine learning capabilities with structured reasoning.

Method: Introduces a tensor network formalism using basis encoding for functions, modeling neural decompositions as tensor decompositions, applying to logical formulas and probability distributions.

Result: Identifies tensor network contractions as a fundamental inference class, formulates reasoning algorithms as contraction message passing, and enables training of hybrid logical-probabilistic models via the tnreason library.

Conclusion: The proposed framework provides a unified treatment for neural and symbolic AI, fostering the development and practical implementation of hybrid models in AI.

Abstract: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.

</details>


### [15] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: Advanced RAG systems with fine-tuning and self-correction can reduce hallucinations in legal AI to below 0.2%, making them reliable for high-stakes work.


<details>
  <summary>Details</summary>
Motivation: To ensure large language models are reliable for high-stakes legal applications by reducing hallucinations, as standalone models are too error-prone.

Method: Compare three paradigms: standalone generative models, basic RAG, and advanced RAG using embedding fine-tuning, re-ranking, and self-correction; evaluate with False Citation Rate and Fabricated Fact Rate metrics via expert review on 2,700 answers across 75 legal tasks.

Result: Standalone models have over 30% FCR and are unsuitable, basic RAG reduces errors but still has misgrounding, advanced RAG reduces fabrication to below 0.2%, making it highly reliable.

Conclusion: Trustworthy legal AI requires rigorous, retrieval-based architectures with verification and traceability, and the evaluation framework can be applied to other high-risk domains.

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [16] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK benchmark reveals providing updated facts worsens LLM reasoning when conflicting with parametric knowledge, with degradation increasing as more facts are provided.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks focus primarily on single knowledge updates and fact recall without evaluating how knowledge conflicts affect downstream multi-step reasoning in LLMs, creating a gap in understanding real-world knowledge propagation challenges.

Method: Created the TRACK benchmark with three reasoning-intensive scenarios (WIKI, CODE, MATH) that introduce multiple realistic knowledge conflicts, and evaluated LLM performance on multi-step reasoning tasks when updated facts conflict with parametric knowledge.

Result: Results show that providing updated facts to models for reasoning worsens performance compared to providing no updated facts, with degradation exacerbating as more updated facts are provided. Failure stems from both inability to faithfully integrate updated facts and flawed reasoning even when knowledge is integrated.

Conclusion: The paper introduces TRACK as a comprehensive benchmark for evaluating LLM reasoning with conflicting knowledge, highlighting the counterintuitive finding that providing updated facts can actually worsen performance, with degradation worsening as more facts are provided.

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [17] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: Transformer-based sentiment analysis improves accuracy but at the cost of reducing neutrality and polarizing sentiment classes, creating reliability issues for industrial applications.


<details>
  <summary>Details</summary>
Motivation: While transformers and transfer learning have advanced sentiment analysis accuracy, these improvements come with unintended consequences - specifically the polarization of sentiment classes and failure to maintain neutrality, which poses serious problems for real-world industrial applications that depend on reliable sentiment outputs.

Method: The paper appears to use experimental analysis to observe and document the phenomenon where transformer-based accuracy improvements in one sentiment class lead to polarization of other classes and loss of neutrality in sentiment classification.

Result: Experimental observations show that transformer-driven accuracy improvements in sentiment analysis often sacrifice neutrality and cause polarization between sentiment classes, creating a trade-off between accuracy and balanced, reliable sentiment classification.

Conclusion: The dark side of transformer-based sentiment analysis is that accuracy gains come at the expense of neutrality, presenting an acute problem for Applied NLP where reliable, balanced sentiment outputs are crucial for industry-ready tasks.

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [18] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: TransportAgents: A hybrid multi-agent LLM framework that integrates specialized agents for different traffic crash data categories with an MLP fusion module, outperforming traditional ML and single-agent LLM approaches in crash severity prediction.


<details>
  <summary>Details</summary>
Motivation: Single-agent LLM architectures struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions for traffic crash severity, which is critical for emergency response and public safety planning.

Method: Proposes TransportAgents, a hybrid multi-agent framework integrating category-specific LLM reasoning with an MLP integration module. Specialized agents focus on subsets of traffic information (demographics, environmental context, incident details) to produce intermediate assessments fused into unified predictions.

Result: Extensive experiments on CPSRMS and NEISS datasets show TransportAgents consistently outperforms traditional ML and advanced LLM baselines across GPT-3.5, GPT-4o, and LLaMA-3.3 backbones, demonstrating robustness, scalability, and cross-dataset generalizability. Produces more balanced, well-calibrated severity predictions than single-agent approaches.

Conclusion: TransportAgents offers interpretable, reliable decision support for safety-critical applications by effectively addressing limitations of single-agent LLM architectures through specialized multi-agent reasoning and integration.

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference](https://arxiv.org/abs/2601.15333)
*Xuanning Hu,Anchen Li,Qianli Xing,Jinglong Ji,Hao Tuo,Bo Yang*

Main category: cs.LG

TL;DR: ELILLM enhances LLMs for drug design by adding exploration and decoding modules to generate better molecules.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have strong abilities but are limited in structure-based drug design due to poor understanding of protein structures and unpredictable molecular generation. This hinders their application in generating valid and effective drug candidates.

Method: Proposes Exploration-Augmented Latent Inference for LLMs (ELILLM), which reinterprets the LLM generation process as encoding, latent space exploration, and decoding. It uses Bayesian optimization for systematic exploration and a position-aware surrogate model to predict binding affinity, with knowledge-guided decoding to ensure chemical validity.

Result: ELILLM was tested on the CrossDocked2020 benchmark, outperforming seven baseline methods in controlled exploration and achieving high binding affinity scores, demonstrating enhanced capabilities.

Conclusion: ELILLM effectively improves LLMs for structure-based drug design by addressing key limitations, enabling the generation of chemically valid and high-affinity molecules.

Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.

</details>


### [20] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: The paper examines how user language impacts the quality and cultural context of LLM responses, finding lower quality for low-resource languages and cultural variations based on query language.


<details>
  <summary>Details</summary>
Motivation: To ensure that users across languages receive similarly high-quality responses from LLMs without systemic disadvantages based on language choice.

Method: Created a set of real-world open-ended questions from the WildChat dataset for evaluation, used LLM-as-a-Judge to analyze cultural context, and tested on a translated subset of the CulturalBench benchmark across multiple languages.

Result: Evaluations show that LLMs consistently provide lower quality answers to open-ended questions in low-resource languages and that language choice significantly affects the cultural context used in responses.

Conclusion: The study highlights that LLMs exhibit language-dependent quality and cultural biases, impacting fairness and utility across different linguistic and cultural backgrounds, indicating a need for improved multilingual and culturally adaptive models.

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [21] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: A method using null experts in Mixture-of-Experts layers to achieve data sparsity without violating causality, improving compute efficiency in vision-language models.


<details>
  <summary>Details</summary>
Motivation: To address the causality violation in existing data sparsity methods like expert-choice routing for autoregressive models, and leverage data heterogeneity in vision-language tasks.

Method: Incorporates zero-compute (null) experts into the routing pool within causal token-choice MoE, where tokens routed to null experts consume no compute, trained with standard load balancing.

Result: At matched FLOPs, combining weight and data sparsity outperforms weight sparsity alone, with better training loss and downstream performance, showing implicit modality-aware routing.

Conclusion: Null experts enable data sparsity in causal MoE models, enhancing compute efficiency without explicit modality handling, beneficial for heterogeneous data like vision-language inputs.

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [22] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: GOAT is a generalized attention model using learnable priors from Entropic Optimal Transport, offering improved compatibility, representational power, and length generalization.


<details>
  <summary>Details</summary>
Motivation: Standard attention mechanisms rely on a naive assumption of a uniform prior, which may limit their capabilities. By viewing attention through Entropic Optimal Transport, the paper aims to introduce a more flexible, learnable prior to enhance attention's performance and adaptability.

Method: The paper introduces GOAT (Generalized Optimal transport Attention with Trainable priors), which replaces the implicit uniform prior in attention with a trainable, continuous prior derived from Entropic Optimal Transport principles. It maintains compatibility with optimized kernels like FlashAttention and provides solutions for attention sinks, while integrating spatial information into core computations.

Result: GOAT learns an extrapolatable prior that combines the benefits of learned positional embeddings and fixed encodings, addressing representational trade-offs and improving generalization across sequence lengths. It demonstrates superior performance in attention tasks by avoiding the limitations of standard attention mechanisms.

Conclusion: The GOAT framework successfully generalizes attention through Entropic Optimal Transport, offering a scalable solution with advanced features like trainable priors and enhanced length generalization, setting a new direction for attention-based models.

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [23] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: FedUMM is a federated learning framework for unified multimodal models that reduces communication cost via adapter-based fine-tuning on non-IID data.


<details>
  <summary>Details</summary>
Motivation: Unified multimodal models are typically trained centrally, which limits deployment in privacy-sensitive and geographically distributed scenarios due to data privacy and communication constraints.

Method: Built on NVIDIA FLARE, FedUMM uses parameter-efficient fine-tuning with LoRA adapters on a BLIP3o backbone, where clients train adapters while freezing the foundation models, and the server aggregates only adapter updates, evaluated on VQA v2 and GenEval benchmarks with Dirichlet-controlled heterogeneity.

Result: Results show slight performance degradation as client count and heterogeneity increase, but FedUMM remains competitive with centralized training, with adapter-only federation reducing per-round communication by over an order of magnitude compared to full fine-tuning.

Conclusion: FedUMM enables practical federated training for unified multimodal models with low communication cost, providing empirical experience for privacy-preserving research in this area.

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [24] [Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC](https://arxiv.org/abs/2601.15399)
*Ashna Nawar Ahmed,Banooqa Banday,Terry Jones,Tanzima Z. Islam*

Main category: cs.LG

TL;DR: Proposes a surrogate-assisted MOBO framework using attention-based job embeddings to optimize node allocation in HPC schedulers, improving Pareto front quality and data efficiency.


<details>
  <summary>Details</summary>
Motivation: HPC schedulers struggle to balance user performance and resource constraints, particularly in selecting optimal job nodes, requiring an automated, data-efficient method.

Method: Surrogate-assisted multi-objective Bayesian optimization (MOBO) with attention-based embeddings of job telemetry and intelligent sample acquisition to capture performance dynamics.

Result: On two production HPC datasets, the method identified higher-quality Pareto fronts for runtime-power trade-offs vs. baselines and reduced training costs with improved stability.

Conclusion: First successful application of embedding-informed surrogates in MOBO for HPC scheduling, jointly optimizing performance and power on production workloads effectively.

Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.

</details>


### [25] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: Ambient Dataloops: An iterative framework that co-evolves datasets and models, using noisy synthetic samples and Ambient Diffusion to progressively improve both dataset quality and model performance.


<details>
  <summary>Details</summary>
Motivation: Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. There's a need for a systematic approach to refine datasets and improve model training simultaneously.

Method: A dataset-model co-evolution process where at each iteration: 1) synthetically improved samples are treated as noisy but at slightly lower noise levels than previous iterations, 2) Ambient Diffusion techniques are used for learning under corruption, 3) the dataset becomes progressively higher quality while the model improves accordingly.

Result: Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. The framework also provides theoretical justification for the data looping procedure.

Conclusion: The proposed Ambient Dataloops framework effectively addresses dataset heterogeneity by iteratively refining both data and models through a co-evolution process, preventing destructive self-consuming loops while achieving superior performance across multiple domains.

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [26] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice is a sequential prediction system using confidence gating to conditionally activate learned behavioral archetypes for improved prediction accuracy without degradation when pattern structure is already present.


<details>
  <summary>Details</summary>
Motivation: To address safety-critical applications by managing epistemic uncertainty, ensuring behavioral predictions are reliable only when confidence is high, and avoiding false activations during distributional shifts or unnecessary activations when baseline predictions suffice.

Method: The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring when confidence exceeds a threshold, otherwise falls back to baseline predictions (LSTM or transformer backbones). It is validated on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets.

Result: On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10; outperforms transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, it correctly refuses archetype activation when distribution shift occurs. On transformer backbones, it provides 0.0% improvement, showing no degradation.

Conclusion: Confidence gating is a promising architectural principle for handling epistemic uncertainty, as it activates when patterns apply, refuses when they don't (preventing false activation), and defers when redundant (avoiding unnecessary complexity), supporting reliability in safety-critical applications.

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [27] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL introduces a supervised framework to align sparse latent dimensions in diffusion models with semantic concepts, enabling precise semantic control and interpretability through concept-aligned latent steering.


<details>
  <summary>Details</summary>
Motivation: While diffusion models have rich internal representations, existing unsupervised methods for understanding them fail to align sparse features with human-understandable concepts, limiting reliable semantic control over generated images.

Method: CASL trains a Sparse Autoencoder on frozen U-Net activations for disentangled latents, then learns a lightweight linear mapping to associate concepts with relevant latent dimensions. CASL-Steer enables controlled latent interventions along concept axes as causal probes.

Result: The method achieves superior editing precision and interpretability compared to existing approaches, validated through the proposed Editing Precision Ratio metric that measures concept specificity and attribute preservation.

Conclusion: CASL is the first supervised framework to successfully align latent representations with semantic concepts in diffusion models, enabling precise semantic control and improved interpretability of diffusion model representations.

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [28] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: Analyzing learning with mixed natural/synthetic data, ERM has limitations, but robust algorithms can overcome contamination for reliable learning.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of pervasive synthetic data, generated by low-cost LLMs, on traditional machine learning theory, particularly the effectiveness of ERM in contaminated datasets.

Method: Modeling as a sequence of learning tasks with mixed natural and synthetic data, analyzing ERM for mean estimation and PAC learning, and comparing with algorithms like non-uniform weighting.

Result: ERM converges to true mean for mean estimation but is outperformed by non-uniform weighting; in PAC learning, ERM may not converge to true concept, but robust algorithms can learn correctly despite contamination.

Conclusion: The study highlights that while ERM is limited in synthetic data scenarios, alternative methods can achieve reliable learning, underscoring the need for robust algorithms in the age of ubiquitous synthetic content.

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [29] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther is a PyTorch-compatible library that implements Randomized Numerical Linear Algebra (RandNLA) algorithms to compress deep learning models, reducing GPU memory usage by up to 75% with minimal code changes.


<details>
  <summary>Details</summary>
Motivation: Training modern deep learning models is constrained by GPU memory and compute limits, but there's no unified, production-grade library for implementing proven RandNLA compression techniques.

Method: Panther consolidates established RandNLA algorithms into a single high-performance framework with a custom C++/CUDA backend (pawX). It provides drop-in replacements for standard PyTorch components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions.

Result: By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code), the library achieves significant memory savings (up to 75%) on BERT while maintaining comparable loss performance.

Conclusion: Panther successfully addresses the adoption barrier for RandNLA techniques by providing a unified, production-ready library that enables memory-efficient deep learning with minimal code changes, making compression techniques more accessible to practitioners.

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [30] [Multi-Targeted Graph Backdoor Attack](https://arxiv.org/abs/2601.15474)
*Md Nabi Newaz Khan,Abdullah Arafat Miah,Yu Bi*

Main category: cs.LG

TL;DR: Introduces the first multi-targeted backdoor attack for graph classification using subgraph injection, achieving high attack success with minimal clean accuracy impact and robustness against defenses.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks for graph classification are limited to single-target attacks with subgraph replacement, which may alter graph structure and lack multi-target capabilities.

Method: Develops a subgraph injection mechanism to poison clean graphs without replacing structure, allows multiple triggers to redirect predictions to different target labels.

Result: Achieves high attack success rates for all target labels on five datasets with minimal impact on clean accuracy, outperforms traditional subgraph replacement attacks and is effective across various GNN models and parameters.

Conclusion: The work highlights GNN vulnerabilities to multi-targeted backdoor attacks, demonstrates the robustness of the proposed method against defenses, and provides open-source code for further research.

Abstract: Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.

</details>


### [31] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: The study compares three models—SARIMAX, XGBoost, and LSTM—for forecasting emergency department arrivals, using data from an Australian hospital from 2017-2021, decomposed by ward type and patient complexity. XGBoost performed best for total daily admissions, SARIMAX for major complexity cases, but all underestimated sudden patient surges.


<details>
  <summary>Details</summary>
Motivation: To address overcrowding in emergency departments by improving demand forecasting for better resource allocation, as accurate predictions are crucial for patient safety and operational efficiency.

Method: The study uses SARIMAX, XGBoost, and LSTM models to forecast daily ED arrivals over seven days. It leverages data from a hospital from January 2017 to December 2021, decomposing demand into eight ward categories and stratifying by clinical complexity. The Prophet model is applied to correct data distortions from COVID-19 by generating synthetic counterfactual values.

Result: All three models outperformed a seasonal naive baseline. XGBoost achieved the highest accuracy for total daily admissions (MAE: 6.63), while SARIMAX was marginally better for major complexity cases (MAE: 3.77). However, all models underestimated sudden, infrequent surges in patient volume.

Conclusion: While SARIMAX, XGBoost, and LSTM models effectively forecast regular daily patterns in ED arrivals, they share a limitation in underestimating unexpected patient surges. This highlights a need for further refinement to better handle rare spikes in demand.

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [32] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: Martingale Foresight Sampling (MFS): A principled framework that reformulates LLM decoding as identifying an optimal stochastic process, using martingale theory to replace heuristics with probability theory principles for improved reasoning accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive decoding in LLMs is short-sighted and fails to find globally optimal reasoning paths due to token-by-token generation. Existing inference-time strategies like foresight sampling rely on ad-hoc heuristics for path valuation and search space pruning, lacking theoretical grounding.

Method: MFS models reasoning path quality as a stochastic process and applies martingale theory: 1) Step valuation via Doob Decomposition Theorem to measure predictable advantage, 2) Path selection using Optional Stopping Theory for principled pruning, 3) Adaptive stopping rule based on Martingale Convergence Theorem to terminate exploration when quality converges.

Result: Experiments on six reasoning benchmarks show MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency.

Conclusion: MFS provides a theoretically-grounded alternative to heuristic-based foresight sampling, demonstrating that principled probability theory approaches can enhance LLM reasoning capabilities while maintaining computational efficiency.

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [33] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: Introduces Margin-Aware Speculative Verification, a method that adapts verification to model decisiveness to speed up LLM inference without sacrificing quality.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding uses strict token-level rejection, which is inefficient when the model has weak preferences, causing unnecessary rollbacks and slowing inference.

Method: Proposes a training-free verification strategy that measures decision stability from target model logits and relaxes rejection when verification benefit is minimal, compatible with existing frameworks.

Result: Experiments across models from 8B to 235B show consistent and significant inference speedups over baselines while preserving generation quality across benchmarks.

Conclusion: Margin-Aware Speculative Verification effectively improves speculative decoding efficiency by adapting verification to model decisiveness, achieving faster inference without quality loss.

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [34] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: Ridge regression best predicts lake water clarity using volunteer data; minimal sampling requires ~64 recent samples and 1 predictor to stay within 5% of full-data accuracy.


<details>
  <summary>Details</summary>
Motivation: Volunteer lake monitoring produces irregular, seasonal time series with many gaps due to ice cover, weather constraints, and human errors, complicating forecasting and early warning of harmful algal blooms.

Method: Used 30-lake subset from three decades of Maine lake records; handled missingness with Multiple Imputation by Chained Equations (MICE); evaluated six candidate models with normalized Mean Absolute Error (nMAE) for cross-lake comparability; identified ridge regression as best performer; then quantified minimal sample size and feature set needed to stay within 5% of full-history, full-feature baseline accuracy.

Result: Ridge regression provided best mean test performance; minimal training history needed ~176 samples per lake to reach within 5% of full-history accuracy; minimal feature set required just four features to match thirteen-feature baseline within 5% tolerance; joint feasibility analysis showed meeting 5% accuracy target required about 64 recent samples and just one predictor per lake.

Conclusion: The joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers, highlighting practicality of targeted monitoring.

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [35] [SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation model](https://arxiv.org/abs/2601.15504)
*Xianghao Zhan,Jingyu Xu,Yuanning Zheng,Zinaida Good,Olivier Gevaert*

Main category: cs.LG

TL;DR: SAGE-FM is a lightweight spatial transcriptomics foundation model using graph convolutional networks with masked central spot prediction, achieving superior performance in gene recovery, clustering, and downstream tasks compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that can capture spatially conditioned regulatory relationships for better understanding of tissue organization and cellular interactions.

Method: SAGE-FM uses graph convolutional networks (GCNs) trained with a masked central spot prediction objective on 416 human Visium samples spanning 15 organs, creating a parameter-efficient foundation model for spatial transcriptomics.

Result: The model achieves 91% of masked genes showing significant correlations (p < 0.05), outperforms MOFA and existing methods in unsupervised clustering and biological heterogeneity preservation, achieves 81% accuracy in pathologist-defined spot annotation, and improves glioblastoma subtype prediction. In silico perturbation experiments confirm it captures directional ligand-receptor and regulatory effects.

Conclusion: Simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics, demonstrating strong performance across multiple tasks while maintaining interpretability.

Abstract: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics foundation model based on graph convolutional networks (GCNs) trained with a masked central spot prediction objective. Trained on 416 human Visium samples spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly recover masked genes, with 91% of masked genes showing significant correlations (p < 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing spatial transcriptomics methods in unsupervised clustering and preservation of biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81% accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell carcinoma and improving glioblastoma subtype prediction relative to MOFA. In silico perturbation experiments further demonstrate that the model captures directional ligand-receptor and upstream-downstream regulatory effects consistent with ground truth. These results demonstrate that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [36] [Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals](https://arxiv.org/abs/2601.16091)
*Saar Cohen*

Main category: cs.MA

TL;DR: Online clustering with delayed assignments in stochastic arrival model achieves constant competitive ratio.


<details>
  <summary>Details</summary>
Motivation: Traditional online clustering requires immediate assignment decisions, but allowing delays with costs could improve clustering quality. However, worst-case arrivals lead to poor competitive ratios.

Method: Introduce online non-centroid clustering with delays where points arrive in metric space. Decisions can be postponed with delay costs. Focus on stochastic arrival model where points are drawn from unknown fixed distribution.

Result: Devised algorithm that achieves constant competitive ratio: expected overall costs (clustering + delays) bounded by constant times optimal offline clustering as number of points grows.

Conclusion: Stochastic arrivals enable beyond worst-case performance for online clustering with delays, achieving constant competitive ratio compared to sublogarithmic impossibility in worst-case.

Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.

</details>


### [37] [Average Unfairness in Routing Games](https://arxiv.org/abs/2601.16187)
*Pan-Yang Su,Arwa Alanqary,Bryce L. Ferguson,Manxi Wu,Alexandre M. Bayen,Shankar Sastry*

Main category: cs.MA

TL;DR: Introduces average unfairness as a fairness measure in routing games, compares it with existing measures, analyzes worst-case values, and studies the trade-off between fairness and efficiency.


<details>
  <summary>Details</summary>
Motivation: To propose a new fairness measure (average unfairness) that complements existing unfairness notions (loaded and UE unfairness) and analyze its properties and implications for network routing fairness and efficiency.

Method: Theoretical analysis comparing three unfairness measures (average, loaded, and UE unfairness), proving relationships and worst-case characteristics. Study of the constrained system optimum (CSO) problem with fairness constraints via mathematical proofs and numerical examples.

Result: Shows worst-case values of all three unfairness measures coincide, average unfairness is always ≤ loaded unfairness, and optimal flow under average unfairness constraint achieves lower total latency than under loaded unfairness constraint.

Conclusion: Average unfairness provides valuable insights for fairness-efficiency tradeoffs in network routing, offering theoretical guarantees and potential improvements in system optimization under fairness constraints.

Abstract: We propose average unfairness as a new measure of fairness in routing games, defined as the ratio between the average latency and the minimum latency experienced by users. This measure is a natural complement to two existing unfairness notions: loaded unfairness, which compares maximum and minimum latencies of routes with positive flow, and user equilibrium (UE) unfairness, which compares maximum latency with the latency of a Nash equilibrium. We show that the worst-case values of all three unfairness measures coincide and are characterized by a steepness parameter intrinsic to the latency function class. We show that average unfairness is always no greater than loaded unfairness, and the two measures are equal only when the flow is fully fair. Besides that, we offer a complete comparison of the three unfairness measures, which, to the best of our knowledge, is the first theoretical analysis in this direction. Finally, we study the constrained system optimum (CSO) problem, where one seeks to minimize total latency subject to an upper bound on unfairness. We prove that, for the same tolerance level, the optimal flow under an average unfairness constraint achieves lower total latency than any flow satisfying a loaded unfairness constraint. We show that such improvement is always strict in parallel-link networks and establish sufficient conditions for general networks. We further illustrate the latter with numerical examples. Our results provide theoretical guarantees and valuable insights for evaluating fairness-efficiency tradeoffs in network routing.

</details>
