<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 53]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Yulin Luo,Junyu Lu,Chunkai Fan,Qiang Zhou,Yiming Zhao,Ning Liu Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Main category: cs.AI

TL;DR: SEEA-R1 is a reinforcement fine-tuning framework for self-evolving embodied agents, addressing sparse rewards and generalization challenges with Tree-GRPO and MGRM, achieving state-of-the-art performance on ALFWorld.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement fine-tuning lacks effective intermediate rewards and generalization for embodied intelligence, limiting self-evolution in multi-modal, long-horizon tasks.

Method: Proposes SEEA-R1 with Tree-GRPO for dense intermediate rewards via Monte Carlo Tree Search and MGRM for cross-task/scene reward generalization.

Result: Achieves 85.07% (textual) and 36.19% (multi-modal) on ALFWorld, outperforming GPT-4o and baselines, with 80.3% without environmental rewards.

Conclusion: SEEA-R1 demonstrates scalable self-evolution potential for embodied intelligence, supported by strong empirical results and qualitative analysis.

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [2] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Main category: cs.AI

TL;DR: The paper introduces the Hierarchical Reasoning Model (HRM), a recurrent architecture inspired by human brain processing, which outperforms current LLMs in reasoning tasks with minimal data and parameters.


<details>
  <summary>Details</summary>
Motivation: Current LLMs using Chain-of-Thought (CoT) techniques face issues like brittle task decomposition, high data needs, and latency. The human brain's hierarchical processing inspired HRM to address these limitations.

Method: HRM uses two interdependent recurrent modules: a high-level module for abstract planning and a low-level module for rapid computations. It performs reasoning in a single forward pass without intermediate supervision.

Result: HRM achieves near-perfect performance on complex tasks (e.g., Sudoku, maze pathfinding) with only 27M parameters and 1000 training samples, outperforming larger models on the ARC benchmark.

Conclusion: HRM represents a transformative advancement toward universal computation and general-purpose reasoning systems, demonstrating efficiency and scalability.

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [3] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang,Jiyao Liu,Yulong Xiao,Junzhi Ning,Lihao Liu,Junjun He,Botian Shi,Kaicheng Yu*

Main category: cs.AI

TL;DR: THE-Tree is a computational framework that constructs domain-specific evolution trees from scientific literature to validate AI-generated scientific propositions, improving accuracy and novelty verification.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating AI-generated scientific propositions are inadequateâ€”LLMs lack domain knowledge, and traditional citation networks lack causality. THE-Tree addresses this by providing structured, verifiable, and causally-linked historical data.

Method: THE-Tree constructs evolution trees using a search algorithm and a "Think-Verbalize-Cite-Verify" process, where LLMs propose advancements and cite literature, followed by validation for coherence and evidence.

Result: THE-Tree improves graph completion by 8-14%, predicts future developments with 10% higher accuracy, and boosts evaluation performance by nearly 100% when combined with other methods.

Conclusion: THE-Tree effectively addresses the bottleneck in validating AI-generated scientific ideas, demonstrating significant improvements in accuracy and utility for scientific research.

Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [4] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu,Xishun Liao,Haoxuan Ma,Jonathan Liu,Rohan Jadhav,Jiaqi Ma*

Main category: cs.AI

TL;DR: MobiVerse is a hybrid framework combining lightweight domain-specific generators and LLMs for scalable, realistic human mobility simulation, tested in Westwood, LA.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in mobility simulation platforms for algorithm development, policy implementation, and large-scale evaluation.

Method: Hybrid framework using lightweight generators for base activity chains and LLMs for context-aware adjustments.

Result: Efficiently simulated 53,000 agents, enabling dynamic responses to environmental changes like road closures and events.

Conclusion: MobiVerse offers a scalable, customizable platform for mobility planning with computational efficiency and behavioral realism.

Abstract: Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [5] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.AI

TL;DR: CitySim is an urban simulator using large language models to create realistic human behaviors, outperforming rigid rule-based methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of rigid, hand-crafted rules in modeling nuanced human behaviors in urban environments.

Method: Uses a recursive value-driven approach for agents to generate daily schedules, incorporating beliefs, goals, and spatial memory.

Result: CitySim aligns better with real human behaviors at micro and macro levels and scales for large simulations.

Conclusion: CitySim is a scalable, flexible tool for understanding and forecasting urban phenomena.

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [6] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.AI

TL;DR: Active-MoSH is an interactive framework for high-stakes decision-making, combining soft-hard bounds with preference learning and active sampling to refine Pareto-optimal solutions while building user trust.


<details>
  <summary>Details</summary>
Motivation: High-stakes decisions require balancing competing objectives with expensive evaluations, but current methods lack systematic ways to refine preferences and ensure trust in outcomes.

Method: Active-MoSH integrates soft-hard bounds with probabilistic preference learning (local) and uses multi-objective sensitivity analysis (global) to identify overlooked solutions.

Result: The framework improves convergence, enhances decision-maker trust, and allows expressive preference articulation, validated through synthetic and real-world applications.

Conclusion: Active-MoSH effectively addresses the challenges of high-stakes decision-making by combining local and global strategies for better outcomes and trust.

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [7] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*RaphaÃ«l Boige,Amine Boumaza,Bruno Scherrer*

Main category: cs.AI

TL;DR: The paper critiques traditional game-solving models for oversimplifying game-tree structures and introduces a new model with ancestor dependency to better reflect real-world complexity. It analyzes algorithms like AlphaBeta and Scout, revealing practical performance differences despite asymptotic similarities.


<details>
  <summary>Details</summary>
Motivation: Traditional models assume independent leaf values, simplifying analysis but ignoring structural complexity. This limits their relevance to real-world games. The paper aims to address this by introducing a more realistic model.

Method: A new probabilistic model is proposed, constructing game-trees with level-wise conditional distributions to enforce ancestor dependency. Recursive formulas are derived to analyze average-case complexities of algorithms like AlphaBeta and Scout.

Result: While asymptotically all algorithms converge to the same branching factor, deep finite trees show AlphaBeta has a larger constant factor, causing practical slowdowns compared to Scout.

Conclusion: The new model provides a more realistic framework for analyzing game-solving algorithms, revealing practical differences and offering tools for deeper understanding.

Abstract: Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [8] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda,Kazumi Kasaura,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.AI

TL;DR: LeanConjecturer automates university-level math conjecture generation in Lean 4 using LLMs, combining rule-based extraction with LLM-based generation to tackle data scarcity in theorem proving.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of training data for formal theorem proving by generating diverse, non-trivial conjectures.

Method: Hybrid approach: rule-based context extraction + LLM-based theorem generation, followed by iterative evaluation.

Result: Generated 12,289 conjectures (3,776 valid/non-trivial), averaging 103.25 per seed file, and verified non-trivial topology theorems.

Conclusion: LeanConjecturer offers scalable training data generation for theorem proving, enhancing capabilities like GRPO and enabling mathematical discovery.

Abstract: We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [9] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang,Ziyan Jiang,Rui Meng,Yifei Leng,Zhenbang Xiao,Zora Zhiruo Wang,Yanyi Shang,Dehan Kong*

Main category: cs.AI

TL;DR: The paper introduces Multimodal Trajectory Retrieval, a method to model trajectory-level data using a new dataset (UATD) and benchmark (GAE-Bench), and proposes GAE-Retriever, a framework outperforming baselines in retrieval recall.


<details>
  <summary>Details</summary>
Motivation: Trajectory data's potential for AI agents in GUI environments is underexplored, lacking systematic representation modeling.

Method: Constructs UATD dataset and GAE-Bench benchmark; proposes GAE-Retriever, a multimodal framework using vision-language models and optimized contrastive learning.

Result: GAE-Retriever consistently outperforms baselines in retrieval recall across datasets.

Conclusion: The work advances multimodal trajectory retrieval, demonstrating the effectiveness of the proposed framework and dataset.

Abstract: Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [10] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao,Runqing Guo,Yangyang Qin,Miangbing Meng,Jipeng Cao,Yilun Lin,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: The paper introduces 'Query as Test' (QaT) and 'Extensible Scenarios Notations' (ESN) to address fragmented data in AI-driven transportation, enabling flexible, logical testing and validation.


<details>
  <summary>Details</summary>
Motivation: The fragmentation and incompatibility of data ecosystems in AI-driven transportation (e.g., intelligent cockpits, autonomous driving) hinder effective testing and validation. Existing methods lack flexibility and fail to cover edge cases.

Method: Proposes ESN, a declarative data framework based on Answer Set Programming (ASP), to unify heterogeneous data as logical facts and rules. QaT transforms testing into logical queries against ESN.

Result: ESN enables deep semantic fusion, flexible querying, interpretability, and privacy protection. QaT enhances testing expressiveness and rigor. Introduces 'Validation-Driven Development' (VDD) for faster iteration.

Conclusion: The QaT and ESN framework improves testing and validation in AI-driven transportation, offering flexibility, interpretability, and scalability, while VDD accelerates development.

Abstract: With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [11] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille FranÃ§ois,Ludovic PÃ©ran,Ayah Bdeir,Nouha Dziri,Will Hawkins,Yacine Jernite,Sayash Kapoor,Juliet Shen,Heidy Khlaaf,Kevin Klyman,Nik Marda,Marie Pellat,Deb Raji,Divya Siddarth,Aviya Skowron,Joseph Spisak,Madhulika Srikumar,Victor Storchan,Audrey Tang,Jen Weedon*

Main category: cs.AI

TL;DR: The paper discusses the intersection of AI safety and open-source models, presenting outcomes from a collaborative event and proposing research directions and tools for safer AI deployment.


<details>
  <summary>Details</summary>
Motivation: The rise of open-weight and open-source AI models necessitates addressing safety concerns through collaborative, transparent, and participatory approaches.

Method: A participatory, solutions-oriented process involving researchers, engineers, and policy leaders produced research agendas, technical interventions, and safety roadmaps.

Result: Key findings include the potential of openness to enhance safety but highlight gaps like limited benchmarks, defenses against attacks, and participatory mechanisms.

Conclusion: The paper proposes five priority research directions to foster an open, plural, and accountable AI safety discipline, influencing global policy discussions.

Abstract: The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [12] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Main category: cs.AI

TL;DR: KGC models suffer from rank bottlenecks due to simple vector-matrix multiplication, limiting expressivity. KGE-MoS, a mixture-based output layer, improves performance and probabilistic fit.


<details>
  <summary>Details</summary>
Motivation: Address the rank bottleneck issue in KGC models, which limits expressivity and hurts ranking accuracy and score distribution fidelity.

Method: Propose KGE-MoS, a mixture-based output layer inspired by language modeling literature, to break rank bottlenecks.

Result: Experiments on four datasets show KGE-MoS improves performance and probabilistic fit with low parameter cost.

Conclusion: KGE-MoS effectively mitigates rank bottlenecks in KGC models, enhancing their accuracy and fidelity.

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [13] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

Main category: cs.AI

TL;DR: The paper advocates for AI systems with 'intelligent disobedience' to enhance human-AI teamwork, proposing a scale of AI agency and initial boundaries for studying this capability.


<details>
  <summary>Details</summary>
Motivation: Current AI systems are overly obedient, which can be counterproductive or unsafe. The paper argues for AI autonomy to improve collaboration.

Method: Introduces a scale of AI agency levels and uses examples to highlight the need for autonomy research in cooperative settings.

Result: Explores how intelligent disobedience varies across autonomy levels and its potential benefits.

Conclusion: Proposes initial boundaries and considerations for studying disobedience as a core AI capability.

Abstract: Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [14] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik DÃ¼rrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: FAT-CAT uses Formal Concept Analysis (FCA) to improve topic modeling by providing structured, hierarchical representations of topics, outperforming existing methods in interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional topic modeling lacks interpretability, hindering deeper insights into data structure. FAT-CAT aims to address this gap.

Method: FAT-CAT employs FCA to aggregate and visualize topics hierarchically, handling diverse topics and file types.

Result: In a case study on ETYNTKE, FAT-CAT provided more meaningful and interpretable insights than other methods.

Conclusion: FCA-based aggregation enhances topic modeling by offering structured, interpretable representations, improving data exploration.

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [15] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung,Yoram Bachrach,Asli Celikyilmaz,Kamalika Chaudhuri,Delong Chen,Willy Chung,Emmanuel Dupoux,HervÃ© JÃ©gou,Alessandro Lazaric,Arjun Majumdar,Andrea Madotto,Franziska Meier,Florian Metze,ThÃ©o Moutakanni,Juan Pino,Basile Terver,Joseph Tighe,Jitendra Malik*

Main category: cs.AI

TL;DR: The paper explores embodied AI agents (virtual avatars, wearable devices, robots) that interact with users and environments, emphasizing world models for reasoning, planning, and human-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: To enhance AI agents' ability to learn and interact like humans by developing world models for reasoning, planning, and understanding user intentions.

Method: Proposes world modeling integrating multimodal perception, reasoning for action, control, and memory, along with learning users' mental world models.

Result: Embodied AI agents gain improved autonomy and capability to perform complex tasks by understanding and predicting environments and user intentions.

Conclusion: World models are central to advancing embodied AI agents, enabling better human-agent collaboration and autonomous task performance.

Abstract: This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [16] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri,Nikolaos S. Tachos,Charalampos N. Kalantzopoulos,Stelios Sfakianakis,Haridimos Kondylakis,Dimitrios I. Zaridis,Sara Colantonio,Daniele Regge,Nikolaos Papanikolaou,The ProCAncer-I consortium,Konstantinos Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: The paper proposes the AI Model Passport, a standardized framework for documenting and verifying AI models, enhancing transparency, reproducibility, and trust in healthcare AI.


<details>
  <summary>Details</summary>
Motivation: Current AI documentation lacks scalability, comparability, and machine interpretability, hindering reproducibility and trust in AI-driven healthcare.

Method: Introduces the AI Model Passport, a structured metadata framework, and implements it via AIPassport, an MLOps tool for medical imaging.

Result: Demonstrated effectiveness in a lesion segmentation use case, improving transparency and regulatory readiness.

Conclusion: The AI Model Passport sets a new standard for trustworthy and compliant AI systems in healthcare and beyond.

Abstract: The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [17] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: The paper introduces the Automated LLM Speedrunning Benchmark to evaluate AI agents' ability to reproduce scientific results, using the NanoGPT speedrun as a case study. Despite detailed hints, current LLMs struggle with reimplementing known innovations.


<details>
  <summary>Details</summary>
Motivation: To assess AI agents' capability in reproducing scientific work, a critical skill for autonomous research.

Method: The benchmark uses 19 speedrun tasks from NanoGPT, providing training scripts and hints of varying detail. Tasks involve diverse optimizations for GPT-2 training.

Result: Current reasoning LLMs, even with advanced scaffolds, fail to reimplement known innovations effectively.

Conclusion: The benchmark offers a simple, non-saturated measure of LLMs' ability to automate scientific reproduction, highlighting limitations in autonomous research.

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: CitySim aims to create a generative simulated city for autonomous vehicle testing by integrating scene generation, agent behavior, and dynamic environment simulation. SceneDiffuser++ is proposed as an end-to-end solution, demonstrating superior realism in city-scale traffic simulation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of manually-driven miles for AV testing by creating a scalable, realistic synthetic simulation environment.

Method: Proposes SceneDiffuser++, an end-to-end generative world model trained on a single loss function, integrating scene generation, agent behavior, occlusion reasoning, and dynamic scene generation.

Result: Demonstrates city-scale traffic simulation with superior realism, evaluated on an augmented Waymo Open Motion Dataset.

Conclusion: SceneDiffuser++ successfully integrates key simulation technologies, offering a scalable and realistic solution for AV testing.

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [19] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens SjÃ¶lund*

Main category: cs.LG

TL;DR: The study evaluates the modularity of LLM-based agentic systems in drug discovery, comparing performance of various LLMs and agent types, emphasizing the need for prompt re-engineering and further research.


<details>
  <summary>Details</summary>
Motivation: To explore the interchangeability of LLM components in agentic systems for drug discovery, a topic with limited prior attention.

Method: Comparison of different LLMs (Claude-3.5-Sonnet, Claude-3.7-Sonnet, GPT-4o, Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, Nova-Micro) and agent types (tool-calling vs. code-generating) using an LLM-as-a-judge score.

Result: Claude-3.5-Sonnet, Claude-3.7-Sonnet, and GPT-4o outperformed other models; code-generating agents generally performed better but results varied by question and model. Prompt re-engineering was crucial for model replacement.

Conclusion: Modularity in agentic systems requires careful consideration of model and prompt design, highlighting the need for further research to develop stable, scalable solutions.

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [20] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: The paper proposes Asymmetric Policy Optimization (APO) with DADS and STCR to improve reasoning in MLLMs, achieving better performance without degrading general tasks.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with complex reasoning and RL training often leads to performance drops or overthinking.

Method: APO divides responses into positive/negative groups, using DADS for adaptive KL divergence and STCR to penalize long responses.

Result: View-R1-3B shows a 7% average gain in reasoning, outperforming larger models, while maintaining general task performance.

Conclusion: DADS and STCR effectively enhance multimodal reasoning in MLLMs without sacrificing generalization.

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [21] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Main category: cs.LG

TL;DR: A Q-learning algorithm is proposed for risk-averse total-reward MDPs, addressing limitations of existing model-based methods by not requiring full transition probabilities. It shows strong convergence and performance guarantees for ERM and EVaR objectives.


<details>
  <summary>Details</summary>
Motivation: Existing model-based algorithms for risk measures like ERM and EVaR are limited to small problems and require full transition probabilities. A more scalable and practical solution is needed.

Method: A Q-learning algorithm is developed to compute optimal stationary policies for total-reward ERM and EVaR objectives, leveraging ERM's dynamic consistency and elicitability.

Result: Numerical results on tabular domains show the algorithm converges quickly and reliably to the optimal risk-averse value function.

Conclusion: The proposed Q-learning algorithm effectively addresses scalability and practicality issues in risk-averse MDPs, demonstrating strong performance and convergence.

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [22] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Main category: cs.LG

TL;DR: The paper introduces a unimodal relationship between the number of clusters and neighborhood radius in density-based clustering, proposing efficient parameter tuning via Ternary Search for large-scale, high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: Density-based clustering outperforms centroid-based methods for noisy or complex data, but parameter tuning is computationally intensive, especially for large-scale, high-dimensional datasets.

Method: The study empirically and theoretically demonstrates a unimodal relationship between cluster count and neighborhood radius, then leverages Ternary Search for efficient parameter tuning.

Result: The proposed method is validated on NLP, Audio, and Computer Vision tasks, showing practical effectiveness and robustness.

Conclusion: This work advances parameter control in density-based clustering and enhances understanding of parameter relationships, with code made publicly available.

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [23] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: The paper introduces a dynamic control method for quality-complexity tradeoff in continuous normalizing flows (CNFs) and diffusion models (DMs) by rewiring transformer blocks and using consistency terms, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for CNFs and DMs require high computational complexity due to multiple ODE-solving iterations. The paper aims to dynamically control the tradeoff between quality and complexity.

Method: The approach involves rewiring transformer blocks to solve an inner discretized ODE and employing time- and length-wise consistency terms during training, enabling flexible sampling with varying time steps and blocks.

Result: Experiments on CelebA-HQ and ImageNet show a 3x latency reduction in efficient sampling and a 3.5-point FID score improvement for high-quality sampling.

Conclusion: The proposed method is solver-agnostic, reduces latency and memory usage, and outperforms state-of-the-art approaches in efficiency and quality.

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [24] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: Proposes text-to-text regression for predicting metric outcomes in complex systems, outperforming traditional tabular methods with high accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional tabular regression struggles with complex systems data like configuration files or logs, where feature engineering is challenging.

Method: Uses a 60M parameter encoder-decoder model trained from scratch for text-to-text regression, tested on Google's Borg system.

Result: Achieves near-perfect 0.99 rank correlation, 100x lower MSE than tabular methods, and adapts to new tasks with 500 examples.

Conclusion: Text-to-text regression is a scalable, accurate alternative for complex systems, enabling universal simulators of real-world outcomes.

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [25] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Main category: cs.LG

TL;DR: FedIRT integrates federated learning with IRT to enable privacy-preserving, distributed estimation of latent abilities and item difficulty without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional IRT requires centralized data, raising privacy concerns. Federated learning offers privacy protection and distributed computing, motivating the integration with IRT.

Method: Proposes Federated Item Response Theory (FedIRT), a framework for estimating IRT models in a distributed manner while preserving privacy. Validated using numerical experiments and a real-world exam dataset.

Result: FedIRT matches standard IRT accuracy, ensures privacy, and reduces communication costs. Demonstrated effectiveness in educational contexts.

Conclusion: FedIRT extends IRT to distributed settings like multi-school assessments, offering privacy and accuracy. An open-source R package is provided for practical use.

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [26] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Main category: cs.LG

TL;DR: The paper introduces a gradient-based neuroplastic adaptation method for concurrent optimization of neuro-fuzzy networks (NFNs), addressing challenges in their design and enabling applications like online reinforcement learning for vision-based tasks.


<details>
  <summary>Details</summary>
Motivation: NFNs combine transparency and performance but lack a systematic design process, often leading to suboptimal architectures due to isolated parametric and structural identification.

Method: A novel gradient-based neuroplastic adaptation approach is proposed to simultaneously optimize NFNs' parameters and structure, recognizing their interdependence.

Result: The method enables NFNs to tackle previously unapproachable settings, such as online reinforcement learning for vision-based tasks, demonstrated by training NFNs to play DOOM scenarios.

Conclusion: Concurrent optimization of NFNs' parameters and structure is effective, expanding their applicability to complex tasks like vision-based reinforcement learning.

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [27] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Main category: cs.LG

TL;DR: M3PO is a scalable model-based reinforcement learning framework that improves sample efficiency and generalization by integrating an implicit world model with a hybrid exploration strategy.


<details>
  <summary>Details</summary>
Motivation: Addressing sample inefficiency in single-task settings and poor generalization in multi-task domains, as existing methods like DreamerV3 and PPO have limitations in control-centric representations and sample complexity.

Method: Combines an implicit world model (predicting task outcomes without observation reconstruction) with a hybrid exploration strategy (model-based planning and model-free uncertainty-driven bonuses). Uses discrepancies between model-based and model-free value estimates to guide exploration and a trust-region optimizer for stable policy updates.

Result: Achieves state-of-the-art performance across multiple benchmarks, providing an efficient and robust alternative to existing methods.

Conclusion: M3PO effectively eliminates the bias-variance trade-off in prior methods and offers a scalable solution for model-based reinforcement learning.

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [28] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Main category: cs.LG

TL;DR: A multi-task parallelism method for graph foundation models enhances scalability and efficiency on supercomputers, demonstrated with 24 million structures.


<details>
  <summary>Details</summary>
Motivation: To improve the generalizability and scalability of graph neural networks for atomistic modeling by addressing multi-source, multi-fidelity data challenges.

Method: Multi-task learning with shared message passing layers and data-specific decoding heads, extended with multi-task parallelism for GPU acceleration.

Result: Efficient scaling on Perlmutter, Aurora, and Frontier supercomputers, trained on 24 million structures from five datasets.

Conclusion: The proposed method stabilizes pre-training and enhances transferability, showing promise for larger, diverse datasets and heterogeneous supercomputing architectures.

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [29] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Main category: cs.LG

TL;DR: The paper explains how discrete symbolic structures emerge from continuous neural network training dynamics using measure space and Wasserstein gradient flow, showing decoupling and contraction phenomena leading to algebraic representations.


<details>
  <summary>Details</summary>
Motivation: To bridge continuous neural learning with discrete symbolic reasoning by understanding the emergence of symbolic structures from neural dynamics.

Method: Lifts neural parameters to a measure space, models training as Wasserstein gradient flow, and analyzes geometric constraints like group invariance.

Result: Demonstrates decoupling into independent optimization trajectories and contraction of degrees of freedom, leading to algebraic representations.

Conclusion: Provides a foundation for neurosymbolic systems integrating continuous learning with discrete reasoning, with implications for scaling symbolic tasks.

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [30] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Main category: cs.LG

TL;DR: The paper compares forward-mode automatic differentiation (FmAD) and zero-order (ZO) optimization with backpropagation (BP) and checkpointing, showing BP's superiority in accuracy, speed, and computation.


<details>
  <summary>Details</summary>
Motivation: To clarify the practical benefits of FmAD and ZO compared to memory-efficient BP variants like checkpointing, addressing gaps in prior comparisons and theoretical analysis.

Method: Theoretical and empirical analysis of BP, FmAD, and ZO methods, including experiments on large language and vision-language models.

Result: BP with checkpointing outperforms FmAD and ZO, achieving higher accuracy (31.1%), faster convergence (34.8%), and fewer computations (3.8x) at similar memory usage.

Conclusion: BP with checkpointing remains the most effective for memory-constrained training, highlighting limitations of FmAD and ZO.

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [31] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Main category: cs.LG

TL;DR: The paper explores partial observations in stochastic systems using Koopman operator theory, linking it with the Mori-Zwanzig formalism, and highlights the role of delay embedding and noise amplitude in accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of partial observations in stochastic systems and connecting the Mori-Zwanzig formalism with Koopman operator theory.

Method: Utilizes Koopman operator theory and delay embedding techniques, supported by numerical experiments.

Result: Clarifies the distinction between state and function spaces in stochastic systems and identifies a power-law behavior in accuracy related to noise amplitude.

Conclusion: Partial observation in stochastic systems benefits from delay embedding, with noise amplitude impacting accuracy in a power-law manner.

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [32] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Main category: cs.LG

TL;DR: The paper surveys Continual Reinforcement Learning (CRL), addressing RL's limitations like data dependency and poor generalization by enabling continuous learning and knowledge retention.


<details>
  <summary>Details</summary>
Motivation: RL's reliance on extensive data and computational resources, along with poor generalization, limits its real-world applicability. CRL aims to overcome these challenges.

Method: The survey reviews existing CRL works, organizes metrics and tasks, proposes a taxonomy of CRL methods, and analyzes challenges.

Result: A comprehensive taxonomy of CRL methods is introduced, and unique challenges are identified.

Conclusion: CRL is a promising direction for RL, with future research needed to address its challenges and improve adaptability.

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [33] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Main category: cs.LG

TL;DR: A survey on continual reinforcement learning (RL) explores its role in enabling RL agents to learn dynamically, addressing challenges, methodologies, and applications in robotics.


<details>
  <summary>Details</summary>
Motivation: To transform RL agents into dynamic continual learners capable of acquiring and retaining reusable knowledge.

Method: Reviews key concepts, challenges, and novel methodologies in continual RL, with a focus on robotics and evaluation environments.

Result: Highlights advancements in continual RL, its applications, and accessible evaluation tools for newcomers.

Conclusion: Identifies limitations and future directions, offering insights for researchers and practitioners.

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [34] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: TOAST is a unified framework for semantic-aware 6G communication, optimizing multi-task performance in dynamic wireless environments using adaptive task balancing, LoRA mechanisms, and a diffusion model.


<details>
  <summary>Details</summary>
Motivation: The shift to 6G requires semantic-aware communication, focusing on task-relevant information rather than bit-centric transmission.

Method: TOAST uses deep reinforcement learning for adaptive task balancing, LoRA for parameter-efficient fine-tuning, and a diffusion model for noise restoration.

Result: TOAST outperforms baselines in classification accuracy and reconstruction quality, especially at low SNR, while remaining robust across scenarios.

Conclusion: TOAST provides a scalable and efficient solution for semantic-aware communication in 6G networks.

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [35] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: HQCM-EBTC is a hybrid quantum-classical model for brain tumor classification using MRI images, achieving 96.48% accuracy and outperforming classical methods.


<details>
  <summary>Details</summary>
Motivation: To improve diagnostic accuracy and interpretability in brain tumor classification by leveraging quantum computing.

Method: Integrates a 5-qubit quantum layer with classical components, optimized via AdamW and a composite loss function.

Result: Achieves 96.48% accuracy, better precision/F1-scores, and enhanced feature separability in quantum space.

Conclusion: Demonstrates the potential of quantum-enhanced models for clinical brain tumor assessment.

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [36] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: GuiderNet, a meta-learning framework, improves VQAs by guiding PQCs into favorable regions, enhancing training and performance in quantum machine learning.


<details>
  <summary>Details</summary>
Motivation: Address challenges like barren plateaus and poor optimization landscapes in VQAs to improve trainability and generalization.

Method: Uses GuiderNet, a classical neural network, to meta-train and guide PQC parameters, optimizing the Fubini-Study metric tensor.

Result: Achieves 5x lower training loss, 98.6% test accuracy (vs. 75.3%), and 0.95 minority-class F1 score (vs. 0.67).

Conclusion: Geometric meta-conditioning via GuiderNet mitigates barren plateaus and ill-conditioning, enhancing quantum machine learning scalability.

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [37] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: RWFT is a lightweight unlearning method that erases a class from a trained classifier without full retraining, outperforming existing methods in metrics and robustness to attacks.


<details>
  <summary>Details</summary>
Motivation: To enforce user deletion rights and mitigate biased predictions without costly full retraining.

Method: Output-reweighting technique redistributing probability mass for the forgotten class, robust to MIA-NN attacks.

Result: Matches full retraining results, improves existing metrics by 2.79%, and TV-based metric by 111.45%.

Conclusion: RWFT is effective, efficient, and robust, setting a new standard for machine unlearning.

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [38] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Main category: cs.LG

TL;DR: A physics-informed DAS neural network paradigm is proposed, eliminating the need for real-world event data by using physical modeling and generative networks for training, achieving high accuracy in event identification and noise removal.


<details>
  <summary>Details</summary>
Motivation: Existing AI models for DAS require real-world data, which is often limited, creating a need for a solution that bypasses this dependency.

Method: The paradigm uses physical modeling to generate synthetic DAS event data, trains a generative network, and employs a debackground net for noise removal.

Result: The method achieves 91.8% fault diagnosis accuracy and outperforms data-driven networks in event identification and noise reduction.

Conclusion: The physics-informed paradigm offers a scalable solution for DAS applications, overcoming data scarcity and noise challenges.

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [39] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: The paper introduces R* Decision Transformer (R* DT) to improve auto-bidding in online advertising by addressing limitations of the traditional Decision Transformer, such as preset RTG values and mixed-quality training data.


<details>
  <summary>Details</summary>
Motivation: To enhance automation in online ad auctions by overcoming the short-sightedness and data-quality issues of traditional Decision Transformers.

Method: Proposes a three-step R* DT: (1) R DT stores actions and RTG values, (2) R^ DT forecasts optimal RTG, and (3) R* DT enhances training data with high-reward trajectories.

Result: Tests on a public bidding dataset show R* DT outperforms traditional DT, especially with mixed-quality trajectories.

Conclusion: R* DT effectively improves auto-bidding by optimizing RTG and refining policies through data augmentation.

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [40] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier DÃ­az-Rozo,Concha Bielza,Pedro LarraÃ±aga*

Main category: cs.LG

TL;DR: A new binned semiparametric Bayesian network model reduces computational costs in kernel density estimation using sparse tensors and parent node restrictions, achieving comparable accuracy to non-binned models but faster.


<details>
  <summary>Details</summary>
Motivation: Address the curse of dimensionality and high computational costs in nonparametric kernel density estimation by introducing data binning and sparse tensor techniques.

Method: Develops two new conditional probability distributions (sparse binned and Fourier kernel density estimation) and evaluates them through complexity analysis and experiments with synthetic and UCI datasets.

Result: The binned model matches the performance of non-binned semiparametric Bayesian networks in structural learning and log-likelihood but is significantly faster.

Conclusion: The binned semiparametric Bayesian networks are a reliable and efficient alternative to traditional non-binned models.

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [41] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Main category: cs.LG

TL;DR: A graph-aware state space model is proposed for graph time series, combining graph-temporal patterns with a limited parameter set. It uses stochastic partial differential equations and deep learning for scalable inference.


<details>
  <summary>Details</summary>
Motivation: To address the need for computationally affordable models that capture graph-temporal patterns in applications like urban water networks and economics.

Method: Proposes a graph-aware state space model with parametric graph-induced latent and observation equations, using stochastic partial differential equations and deep learning for inference.

Result: The model effectively learns parameters for downstream tasks like prediction and imputation, with improved scalability through deep learning.

Conclusion: The approach balances theoretical tractability and scalability, making it suitable for real-world graph time series applications.

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [42] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus GisslÃ©n*

Main category: cs.LG

TL;DR: TROFI is a method for offline reinforcement learning that learns a reward function from human preferences, eliminating the need for pre-defined rewards or optimal trajectories. It outperforms baselines and matches ground truth reward performance.


<details>
  <summary>Details</summary>
Motivation: In applied settings like video game development, reward functions are often unavailable, making offline reinforcement learning challenging. TROFI addresses this by learning rewards from human preferences.

Method: TROFI learns a reward function from human preferences, labels the dataset with it, and trains the policy offline. It doesn't require optimal trajectories.

Result: TROFI outperforms baselines on the D4RL benchmark and performs comparably to using ground truth rewards. It also works well in a 3D game environment.

Conclusion: A well-engineered reward function is crucial for aligning the value function to future rewards. TROFI provides an effective solution for offline RL without pre-defined rewards.

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [43] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: The paper introduces FedMKGC, a federated multimodal knowledge graph completion task, and proposes MMFeD3-HidE to address challenges like multimodal unavailability and client heterogeneity. It includes HidE for local multimodal recovery and MMFeD3 for federated knowledge transfer, validated by a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of collaboration in decentralized multimodal knowledge graphs while ensuring reasoning ability and transmission safety.

Method: Proposes HidE for local multimodal recovery and MMFeD3 for federated knowledge transfer via logit and feature distillation.

Result: MMFeD3-HidE shows effectiveness, semantic consistency, and convergence robustness in experiments.

Conclusion: The framework successfully tackles FedMKGC challenges, improving prediction without sharing sensitive data.

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [44] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: UniCA bridges Time Series Foundation Models (TSFMs) with covariate-aware forecasting by homogenizing and fusing diverse covariates, enhancing their applicability to real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Current TSFMs are limited to real-valued series, struggling with heterogeneous covariates like categorical variables or multimodal data, which are common in real-world forecasting.

Method: UniCA homogenizes diverse covariates into high-level series representations and fuses them using a unified attention-based mechanism.

Result: UniCA outperforms benchmarks in unimodal and multimodal covariate-aware forecasting, demonstrating its effectiveness.

Conclusion: UniCA successfully extends TSFMs to handle diverse covariates, preserving their generalization while improving real-world forecasting capabilities.

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [45] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: GPAS is proposed to address activation variance growth in Pre-LN Transformers by scaling activations while preserving gradients, improving performance across model sizes.


<details>
  <summary>Details</summary>
Motivation: Pre-LN Transformers suffer from exponential activation variance growth, limiting deeper layers' learning capacity.

Method: Gradient-Preserving Activation Scaling (GPAS) scales down intermediate activations without altering gradients.

Result: GPAS consistently improves performance in models from 71M to 1B and works with other architectures like Sandwich-LN and DeepNorm.

Conclusion: GPAS is a versatile solution for improving training dynamics in various Transformer architectures.

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [46] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: Transformers are linked to GNNs as message-passing networks on fully connected graphs, with self-attention capturing token relationships and positional encodings hinting at structure. Their dense matrix operations make them hardware-efficient, positioning them as GNNs benefiting from modern hardware.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between Transformers (NLP) and GNNs (graph learning) by showing their mathematical similarities and hardware advantages.

Method: Analyze Transformers as message-passing GNNs on fully connected token graphs, leveraging self-attention and positional encodings.

Result: Transformers are expressive set processors that learn input relationships without predefined graphs, with hardware-efficient dense operations.

Conclusion: Transformers can be seen as GNNs optimized for modern hardware, highlighting their efficiency and versatility.

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [47] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Main category: cs.LG

TL;DR: A hybrid LSTM+XGBoost model is proposed for cryptocurrency price prediction, outperforming standalone models and traditional methods.


<details>
  <summary>Details</summary>
Motivation: Cryptocurrency markets are volatile and complex, requiring advanced forecasting methods.

Method: Combines LSTM for temporal dependencies and XGBoost for nonlinear relationships with auxiliary features like sentiment scores.

Result: The hybrid model consistently outperforms standalone models and traditional methods, validated on Bitcoin, Ethereum, Dogecoin, and Litecoin datasets.

Conclusion: Hybrid architectures show promise for financial forecasting and adaptability across different cryptocurrencies.

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [48] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,BalÃ¡zs KulcsÃ¡r*

Main category: cs.LG

TL;DR: The paper introduces two neural approaches for multi-objective routing on multigraphs, addressing a gap in learning-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based routing methods overlook multigraphs, which are practically relevant for multi-objective routing.

Method: Two neural models: one operates directly on multigraphs by autoregressively selecting edges, while the other first prunes the multigraph into a simple graph before routing.

Result: Both models show strong performance in problems like TSP and CVRP.

Conclusion: The proposed neural approaches effectively address multi-objective routing on multigraphs, demonstrating practical utility.

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [49] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Main category: cs.LG

TL;DR: EFRame enhances GRPO by improving exploration, filtering low-quality samples, and using replay for rare samples, boosting RL performance in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: GRPO's limitations in exploration, sample efficiency, and stability hinder its performance in complex reasoning tasks.

Method: EFRame introduces additional rollouts, online filtering, and experience replay to systematically improve GRPO.

Result: EFRame improves training robustness, efficiency, and unlocks deeper reasoning capabilities compared to vanilla GRPO.

Conclusion: EFRame provides a structured learning cycle and finer sample analysis, advancing RL for reasoning tasks.

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [50] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,MichaÅ‚ Krutul,Jan MaÅ‚aÅ›nicki,Maciej PiÃ³ro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: Projected Compression is a novel method to reduce large language model size without increasing computational overhead, outperforming pruning and retraining.


<details>
  <summary>Details</summary>
Motivation: The growth of large language models increases inference time and computational demands, necessitating efficient size reduction techniques.

Method: The method trains additional projection weights, merges them into a lower-dimensional matrix, and reduces model size while matching the base model's FLOPs.

Result: Projected Compression outperforms pruning and retraining, with performance scaling well with token count.

Conclusion: The technique effectively reduces model size without sacrificing computational efficiency, offering a viable alternative to traditional methods.

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [51] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Main category: cs.LG

TL;DR: A deep-learning model simplifies heavy metal pollution assessment in soils and seaports, outperforming traditional methods with lower errors and higher efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional Pollution Load Index (PLI) assessments are laborious and face data scarcity and standardization issues in the water-sediment domain.

Method: The proposed model uses transfer learning to predict PLI accurately, addressing data scarcity and feature variability across domains.

Result: The model achieves significantly lower MAE (~0.5) and MAPE (~0.03) compared to baselines, with performance up to 2 orders of magnitude better.

Conclusion: The model provides an innovative, cost-effective solution for water quality prediction, aiding environmental monitoring and conservation efforts.

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [52] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Main category: cs.LG

TL;DR: CoATA is a dual-channel GNN framework that co-augments topology and attributes to address noise and incompleteness in graphs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs often have noise and incompleteness, degrading GNN performance. Existing methods focus on single-dimensional augmentation, ignoring deeper interplays between topology and attributes.

Method: CoATA propagates structural signals to enrich node attributes, projects attributes into a bipartite graph for structural refinement, and uses contrastive learning with prototype alignment for mutual corrections.

Result: CoATA outperforms eleven state-of-the-art baseline methods on seven benchmark datasets.

Conclusion: CoATA effectively captures the synergistic relationship between topology and attributes, demonstrating superior performance.

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [53] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Main category: cs.LG

TL;DR: The paper focuses on improving post-earthquake damage assessment using machine learning, addressing class imbalance with SMOTE, and evaluating various models for predicting structural damage grades.


<details>
  <summary>Details</summary>
Motivation: Accurate damage assessment post-earthquake is crucial for effective disaster response and recovery, particularly for prioritizing rescue and resource allocation.

Method: Uses multi-class classification (e.g., XGBoost), deep learning, and ensembling methods, with SMOTE to handle class imbalance. Includes feature manipulation and diverse training approaches.

Result: Identifies key factors for seismic vulnerability and evaluates model performance using techniques like confusion matrices.

Conclusion: The study enhances earthquake damage prediction by addressing class imbalance and comprehensively evaluating model effectiveness.

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [54] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Main category: cs.LG

TL;DR: LGES is a variant of GES that improves computational efficiency and accuracy by avoiding unnecessary edge insertions, leveraging prior assumptions, and handling interventional data.


<details>
  <summary>Details</summary>
Motivation: Address the computational cost and finite-sample accuracy challenges of GES in causal discovery.

Method: Modifies GES by avoiding edge insertions where conditional independence is implied, uses prior assumptions, and incorporates interventional data.

Result: Achieves up to 10x speed-up, reduces structural error, and outperforms GES and baselines in accuracy and robustness.

Conclusion: LGES retains GES's theoretical guarantees while improving practical performance and adaptability to prior assumptions and interventional data.

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [55] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Main category: cs.LG

TL;DR: The paper introduces a parameterization method for control law learning using reproducing kernel Hilbert spaces, extending Thompson sampling to general function spaces for active learning-based controller design.


<details>
  <summary>Details</summary>
Motivation: Thompson sampling (TS) is limited to finite parametric representations, restricting its use in control system design where general function spaces are common.

Method: The proposed method treats control laws as elements in a function space, using reproducing kernel Hilbert spaces for parameterization. A TS framework explores optimal control laws with convergence guarantees.

Result: Theoretical analysis shows exponential learning rates for control laws and closed-loop performance metrics, with derived upper bounds for control regret. Numerical experiments validate the method.

Conclusion: The approach effectively extends TS to general function spaces, enabling flexible control law design without structural restrictions, supported by theoretical and empirical results.

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [56] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Main category: cs.LG

TL;DR: A framework integrating deep learning and epidemic models for forecasting and mechanistic modeling, using diverse datasets with Differential Privacy (DP) guarantees.


<details>
  <summary>Details</summary>
Motivation: Diverse datasets enhance epidemiology analyses, but sensitive data requires privacy protections like DP.

Method: Develop a framework combining deep learning and epidemic models, incorporating DP-protected datasets.

Result: Demonstrated effectiveness using a synthetic financial dataset with DP, showing value in forecasting and modeling.

Conclusion: DP-protected datasets can significantly improve epidemic analyses without compromising privacy.

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [57] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: Sheaf-DMFL is a decentralized multimodal learning framework using sheaf theory to improve collaboration among edge devices with diverse data modalities, outperforming conventional FL methods.


<details>
  <summary>Details</summary>
Motivation: Conventional FL lacks support for multimodal data and diverse client capabilities, limiting real-world applicability.

Method: Proposes Sheaf-DMFL and Sheaf-DMFL-Att, leveraging sheaf theory and attention mechanisms to enhance multimodal collaboration.

Result: Demonstrates superiority in real-world scenarios like link blockage prediction and mmWave beamforming.

Conclusion: Sheaf-DMFL addresses limitations of FL in heterogeneous systems, offering improved performance and theoretical guarantees.

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [58] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Main category: cs.LG

TL;DR: dreaMLearning is a framework for learning from compressed data without decompression, using EntroGeDe for efficient training and storage.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for large labeled data and high computational resources in deep learning by enabling efficient learning from compressed data.

Method: Uses Entropy-based Generalized Deduplication (EntroGeDe) for lossless compression and learns directly from compressed data.

Result: Achieves 8.8x faster training, 10x memory reduction, 42% storage savings with minimal performance impact.

Conclusion: Enables scalable and efficient ML, benefiting distributed, federated learning, and edge devices.

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [59] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: The paper introduces a probabilistic framework for principled inference-time scaling in LLMs, proposing the 	extsc{OptScale} algorithm to dynamically optimize sample numbers, reducing overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time scaling methods lack a principled foundation, relying on heuristics. This work aims to formalize optimality and provide compute-efficient guidance.

Method: A probabilistic framework is developed, assuming i.i.d. parallel samples, with a derived theoretical lower bound for sample numbers. 	extsc{OptScale} dynamically determines optimal samples using a language model-based predictor.

Result: Experiments on math benchmarks (MATH-500, GSM8K, AIME, AMC) show 	extsc{OptScale} reduces sampling overhead while matching or outperforming state-of-the-art performance.

Conclusion: The work provides a theoretical and practical solution for efficient LLM deployment in complex reasoning, addressing a critical gap in inference-time scaling.

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [60] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub PeleÅ¡ka,Gustav Å Ã­r*

Main category: cs.LG

TL;DR: REDELEX is a framework for evaluating Relational Deep Learning (RDL) models on diverse RDBs, confirming RDL's superiority and analyzing performance factors.


<details>
  <summary>Details</summary>
Motivation: To address the lack of analysis on how RDL model performance relates to RDB characteristics.

Method: REDELEX evaluates RDL models of varying complexity on over 70 RDBs, benchmarking against classic methods.

Result: RDL generally outperforms classic methods, with performance influenced by model complexity, database size, and structural properties.

Conclusion: REDELEX provides insights into RDL performance factors, aiding future research and applications.

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [61] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Main category: cs.LG

TL;DR: Distributed Neural Architectures (DNAs) generalize sparse methods like Mixture-of-Experts, learning computation/communication patterns end-to-end. They match dense baselines in performance and show emergent specialization and interpretable compute allocation.


<details>
  <summary>Details</summary>
Motivation: To explore flexible, distributed neural architectures that adapt computation and communication patterns based on token/patch content and context, while maintaining efficiency and performance.

Method: Initialize DNAs with proto-architectures (transformers, MLPs, etc.) and routers, allowing tokens to traverse modules dynamically. Train end-to-end with optional efficiency constraints.

Result: DNAs match dense baselines in vision/language tasks, exhibit power-law distributed paths, and show emergent module specialization and interpretable compute allocation.

Conclusion: DNAs offer a flexible, efficient alternative to dense models, with learned patterns that align with task requirements and exhibit interpretable behavior.

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [62] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Main category: cs.LG

TL;DR: A novel framework using multi-view contrastive learning improves domain adaptation for medical time series by integrating temporal, derivative, and frequency features.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in adapting ML models to medical time series due to complex temporal dependencies and dynamic distribution shifts.

Method: Leverages multi-view contrastive learning with independent encoders and hierarchical fusion to learn transferable, feature-invariant representations.

Result: Outperforms state-of-the-art methods in transfer learning tasks across EEG, ECG, and EMG datasets.

Conclusion: The framework enhances robustness and generalizability, enabling reliable AI deployment in diverse healthcare settings.

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [63] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Main category: cs.LG

TL;DR: The paper introduces a novel stochastic bandit optimization approach that jointly maximizes expected reward and minimizes risk using the mean-variance criterion, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for decision-making under uncertainty by balancing reward maximization and risk minimization, a gap in traditional bandit formulations.

Method: A unified meta-algorithmic framework with adaptive confidence intervals for fixed-confidence and fixed-budget regimes, using a shared sample exploration strategy.

Result: Theoretical guarantees on solution correctness and empirical validation showing superior accuracy and sample efficiency over existing methods.

Conclusion: The proposed framework is broadly applicable for risk-aware decision-making in uncertain environments, offering a robust trade-off between performance and risk.

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [64] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia,Nikos Papadis,Murali Kodialam,TV Lakshman,Sayak Chakrabarty*

Main category: cs.LG

TL;DR: CLoVE is a novel algorithm for Clustered Federated Learning (CFL) that identifies client clusters using loss vector embeddings, offering simplicity, broad applicability, and robustness without needing optimal initialization.


<details>
  <summary>Details</summary>
Motivation: Identifying client clusters in CFL is challenging due to unknown client assignments, and existing methods often require near-optimal initialization.

Method: CLoVE uses client embeddings from model losses, iteratively separates clusters, and optimizes cluster-specific models via federated aggregation.

Result: Theoretical bounds show CLoVE recovers clusters accurately in one round and converges exponentially fast. Experiments confirm high cluster recovery accuracy and state-of-the-art model performance.

Conclusion: CLoVE is a robust, efficient, and versatile solution for CFL, outperforming existing methods in cluster recovery and model accuracy.

Abstract: We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


### [65] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.LG

TL;DR: A score-based model is proposed to learn tensor decompositions without predefined structural assumptions, improving accuracy and flexibility for multiway data analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional tensor decomposition methods rely on fixed structural assumptions (e.g., CP or Tucker), which may not align with practical scenarios, leading to accuracy loss.

Method: A neural network learns an energy function optimized via score matching to capture the gradient of the joint log-probability of tensor entries and shared factors, avoiding fixed assumptions.

Result: The method outperforms traditional approaches, handling sparse, continuous-time tensors, and visual data effectively.

Conclusion: The proposed model eliminates rigid assumptions, enabling flexible and accurate tensor decomposition for diverse data types.

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [66] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Main category: cs.LG

TL;DR: A weakly-supervised domain adaptation method is proposed to address domain shift in medical applications by leveraging target domain class proportions, improving performance without extra annotations.


<details>
  <summary>Details</summary>
Motivation: Domain shift in medical data degrades model performance, especially when class proportions differ between source and target domains. Existing methods often fail in such cases.

Method: The method uses proportion-constrained pseudo-labeling for unlabeled target data, utilizing accessible class proportion information.

Result: Outperforms semi-supervised domain adaptation on endoscopic datasets, even with only 5% labeled target data, and shows robustness to noisy proportion labels.

Conclusion: The proposed method effectively addresses domain shift in medical applications, demonstrating practicality and robustness in real-world scenarios.

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [67] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: Koopman-CFM integrates Koopman operator theory to linearize generative dynamics, enabling faster, interpretable sampling via matrix exponentiation.


<details>
  <summary>Details</summary>
Motivation: Address the computational expense and lack of interpretability in sampling from Conditional Flow Matching (CFM).

Method: Propose a decoder-free Koopman-CFM architecture that learns a linear embedding for generative dynamics, allowing closed-form, one-step sampling.

Result: Achieves significant speedups on 2D datasets and benchmarks (MNIST, F-MNIST, TFD) and provides interpretable spectral analysis of the generative process.

Conclusion: Koopman-enhanced CFM combines efficiency with interpretability, advancing fast and structured generative modeling.

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [68] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Main category: cs.LG

TL;DR: PiPRL framework integrates symbolic physics priors into RL for indoor navigation, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for incorporating physics priors in RL require manual effort and expertise, limiting accessibility.

Method: Uses a DSL for human-readable physics priors, combined with hierarchical neuro-symbolic integration (PiPRL).

Result: PiPRL outperforms purely symbolic or neural policies and reduces training time by over 26%.

Conclusion: Symbolic approaches like PiPRL enhance RL efficiency and generalization by leveraging physics priors.

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [69] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi*

Main category: cs.LG

TL;DR: The paper introduces a new value-incentivized actor-critic (VAC) method for online RL, balancing exploration and exploitation with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing exploration and exploitation in online RL with complex function approximations, leveraging optimistic regularization.

Method: Proposes VAC, integrating exploration and exploitation via a single objective, using primal-dual optimization for optimism.

Result: VAC achieves near-optimal regret guarantees under linear MDPs and extends to general function approximations.

Conclusion: VAC offers a practical and theoretically sound solution for RL's exploration-exploitation trade-off.

Abstract: Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [70] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash,Ethan Chan,Nathan P. Lawrence,Karthik Pattabiraman*

Main category: cs.LG

TL;DR: ARMOR is a model-free RL controller for UAVs that learns robust latent state representations to resist adversarial sensor attacks, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: UAVs rely on sensors vulnerable to attacks like GPS spoofing, compromising safety. Existing safe RL methods fail against such attacks.

Method: ARMOR uses a two-stage training framework: a teacher encoder with attack info trains a student encoder on historical data for real-world deployment.

Result: ARMOR ensures UAV safety, generalizes to unseen attacks, and reduces training costs by avoiding iterative adversarial training.

Conclusion: ARMOR provides an effective, attack-resilient solution for UAV control, enhancing safety and adaptability.

Abstract: Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>
