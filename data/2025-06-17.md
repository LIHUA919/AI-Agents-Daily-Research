<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 49]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects](https://arxiv.org/abs/2506.11012)
*Guanglin Niu,Bo Li,Yangguang Lin*

Main category: cs.AI

TL;DR: This survey provides a comprehensive review of knowledge graph reasoning (KGR) tasks, categorizing them by reasoning paradigms, applications, and challenges, while also exploring advanced techniques like LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a systematic summary of all KGR tasks, including downstream applications and challenging paradigms, and to highlight trends and future directions.

Method: Categorizes KGR approaches based on reasoning tasks, application tasks, and challenging tasks, and reviews advanced techniques like LLMs.

Result: A detailed taxonomy of KGR tasks and insights into the impact of advanced techniques on KGR.

Conclusion: The survey aims to guide future research by identifying key trends and promising directions in KGR.

Abstract: Knowledge graphs (KGs) have emerged as a powerful paradigm for structuring
and leveraging diverse real-world knowledge, which serve as a fundamental
technology for enabling cognitive intelligence systems with advanced
understanding and reasoning capabilities. Knowledge graph reasoning (KGR) aims
to infer new knowledge based on existing facts in KGs, playing a crucial role
in applications such as public security intelligence, intelligent healthcare,
and financial risk assessment. From a task-centric perspective, existing KGR
approaches can be broadly classified into static single-step KGR, static
multi-step KGR, dynamic KGR, multi-modal KGR, few-shot KGR, and inductive KGR.
While existing surveys have covered these six types of KGR tasks, a
comprehensive review that systematically summarizes all KGR tasks particularly
including downstream applications and more challenging reasoning paradigms
remains lacking. In contrast to previous works, this survey provides a more
comprehensive perspective on the research of KGR by categorizing approaches
based on primary reasoning tasks, downstream application tasks, and potential
challenging reasoning tasks. Besides, we explore advanced techniques, such as
large language models (LLMs), and their impact on KGR. This work aims to
highlight key research trends and outline promising future directions in the
field of KGR.

</details>


### [2] [OntoGSN: An Ontology for Dynamic Management of Assurance Cases](https://arxiv.org/abs/2506.11023)
*Tomas Bueno Momcilovic,Barbara Gallina,Ingmar Kessler,Dian Balta*

Main category: cs.AI

TL;DR: OntoGSN is an ontology and middleware for managing Assurance Cases (ACs) in GSN, addressing challenges in maintaining and updating ACs by providing automated knowledge representation, querying, and updates.


<details>
  <summary>Details</summary>
Motivation: Managing ACs is challenging due to the effort required to maintain embedded knowledge amid changes, leading to poorly managed cases or false confidence.

Method: OntoGSN formalizes the GSN Community Standard v3 in an OWL ontology with SWRL rules, includes a helper ontology, parser, SPARQL query library, and prototypical interface.

Result: The ontology adheres to the GSN standard, evaluated using FAIR principles, OOPS framework, competency questions, and community feedback.

Conclusion: OntoGSN demonstrates utility in dynamic AC management, exemplified by assurance of adversarial robustness in large language models.

Abstract: Assurance cases (ACs) are a common artifact for building and maintaining
confidence in system properties such as safety or robustness. Constructing an
AC can be challenging, although existing tools provide support in static,
document-centric applications and methods for dynamic contexts (e.g.,
autonomous driving) are emerging. Unfortunately, managing ACs remains a
challenge, since maintaining the embedded knowledge in the face of changes
requires substantial effort, in the process deterring developers - or worse,
producing poorly managed cases that instill false confidence. To address this,
we present OntoGSN: an ontology and supporting middleware for managing ACs in
the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge
representation and a queryable graph that can be automatically populated,
evaluated, and updated. Our contributions include: a 1:1 formalization of the
GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology
and parser for integration with a widely used AC tool; a repository and
documentation of design decisions for OntoGSN maintenance; a SPARQL query
library with automation patterns; and a prototypical interface. The ontology
strictly adheres to the standard's text and has been evaluated according to
FAIR principles, the OOPS framework, competency questions, and community
feedback. The development of other middleware elements is guided by the
community needs and subject to ongoing evaluations. To demonstrate the utility
of our contributions, we illustrate dynamic AC management in an example
involving assurance of adversarial robustness in large language models.

</details>


### [3] [LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic](https://arxiv.org/abs/2506.11221)
*Weibing Zheng,Laurah Turner,Jess Kropczynski,Murat Ozer,Tri Nguyen,Shane Halse*

Main category: cs.AI

TL;DR: The paper proposes LLM-as-a-Fuzzy-Judge, combining fuzzy logic and LLMs to automate and align clinical communication skills evaluation with physician preferences, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scalable and nuanced automated evaluation of medical students' clinical communication skills, aligning with subjective physician judgments.

Method: Combines fuzzy logic and LLMs, fine-tuning LLMs with human-annotated data from fuzzy sets (Professionalism, Medical Relevance, Ethical Behavior, Contextual Distraction).

Result: Achieves over 80% accuracy, with major criteria items exceeding 90%, providing interpretable and human-aligned assessments.

Conclusion: Demonstrates the viability of fuzzy logic and LLMs for human-aligned automated evaluation in medical education, enhancing assessment robustness.

Abstract: Clinical communication skills are critical in medical education, and
practicing and assessing clinical communication skills on a scale is
challenging. Although LLM-powered clinical scenario simulations have shown
promise in enhancing medical students' clinical practice, providing automated
and scalable clinical evaluation that follows nuanced physician judgment is
difficult. This paper combines fuzzy logic and Large Language Model (LLM) and
proposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the
automated evaluation of medical students' clinical skills with subjective
physicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is
fine-tuned to evaluate medical students' utterances within student-AI patient
conversation scripts based on human annotations from four fuzzy sets, including
Professionalism, Medical Relevance, Ethical Behavior, and Contextual
Distraction. The methodology of this paper started from data collection from
the LLM-powered medical education system, data annotation based on
multidimensional fuzzy sets, followed by prompt engineering and the supervised
fine-tuning (SFT) of the pre-trained LLMs using these human annotations. The
results show that the LLM-as-a-Fuzzy-Judge achieves over 80\% accuracy, with
major criteria items over 90\%, effectively leveraging fuzzy logic and LLM as a
solution to deliver interpretable, human-aligned assessment. This work suggests
the viability of leveraging fuzzy logic and LLM to align with human
preferences, advances automated evaluation in medical education, and supports
more robust assessment and judgment practices. The GitHub repository of this
work is available at https://github.com/2sigmaEdTech/LLMAsAJudge

</details>


### [4] [MUDAS: Mote-scale Unsupervised Domain Adaptation in Multi-label Sound Classification](https://arxiv.org/abs/2506.11331)
*Jihoon Yun,Chengzhang Li,Dhrubojyoti Roy,Anish Arora*

Main category: cs.AI

TL;DR: MUDAS is a UDA framework for multi-label sound classification in IoT, improving accuracy with minimal resources.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods are unsuitable for multi-label tasks and resource-constrained IoT devices, especially in urban sound classification.

Method: MUDAS selectively retrains classifiers using high-confidence data, employs adaptive thresholds for pseudo-labels, and uses diversity regularization.

Result: MUDAS outperforms existing UDA algorithms on the SONYC-UST dataset, achieving good accuracy in IoT settings.

Conclusion: MUDAS effectively addresses multi-label UDA challenges in resource-constrained environments, enhancing urban sound classification.

Abstract: Unsupervised Domain Adaptation (UDA) is essential for adapting machine
learning models to new, unlabeled environments where data distribution shifts
can degrade performance. Existing UDA algorithms are designed for single-label
tasks and rely on significant computational resources, limiting their use in
multi-label scenarios and in resource-constrained IoT devices. Overcoming these
limitations is particularly challenging in contexts such as urban sound
classification, where overlapping sounds and varying acoustics require robust,
adaptive multi-label capabilities on low-power, on-device systems. To address
these limitations, we introduce Mote-scale Unsupervised Domain Adaptation for
Sounds (MUDAS), a UDA framework developed for multi-label sound classification
in resource-constrained IoT settings. MUDAS efficiently adapts models by
selectively retraining the classifier in situ using high-confidence data,
minimizing computational and memory requirements to suit on-device deployment.
Additionally, MUDAS incorporates class-specific adaptive thresholds to generate
reliable pseudo-labels and applies diversity regularization to improve
multi-label classification accuracy. In evaluations on the SONYC Urban Sound
Tagging (SONYC-UST) dataset recorded at various New York City locations, MUDAS
demonstrates notable improvements in classification accuracy over existing UDA
algorithms, achieving good performance in a resource-constrained IoT setting.

</details>


### [5] [Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables](https://arxiv.org/abs/2506.11375)
*Yitong Zhou,Mingyue Cheng,Qingyang Mao,Yucong Luo,Qi Liu,Yupeng Li,Xiaohan Zhang,Deguang Liu,Xin Li,Enhong Chen*

Main category: cs.AI

TL;DR: ChemTable is a benchmark for chemical tables, addressing multimodal and domain-specific complexity in chemistry, with tasks like table recognition and understanding. Models show limitations compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook the complexity of chemical tables, limiting multimodal models' ability to support scientific understanding in chemistry.

Method: Introduces ChemTable, a large-scale benchmark with expert-annotated data, supporting table recognition (parsing, extraction) and understanding (QA tasks).

Result: Models perform reasonably on layout parsing but lag in QA tasks compared to humans, with gaps between open-source and closed-source models.

Conclusion: ChemTable highlights challenges in chemistry-aware table understanding and serves as a rigorous benchmark for advancing scientific reasoning.

Abstract: Chemical tables encode complex experimental knowledge through symbolic
expressions, structured variables, and embedded molecular graphics. Existing
benchmarks largely overlook this multimodal and domain-specific complexity,
limiting the ability of multimodal large language models to support scientific
understanding in chemistry. In this work, we introduce ChemTable, a large-scale
benchmark of real-world chemical tables curated from the experimental sections
of literature. ChemTable includes expert-annotated cell polygons, logical
layouts, and domain-specific labels, including reagents, catalysts, yields, and
graphical components and supports two core tasks: (1) Table Recognition,
covering structure parsing and content extraction; and (2) Table Understanding,
encompassing both descriptive and reasoning-oriented question answering
grounded in table structure and domain semantics. We evaluated a range of
representative multimodal models, including both open-source and closed-source
models, on ChemTable and reported a series of findings with practical and
conceptual insights. Although models show reasonable performance on basic
layout parsing, they exhibit substantial limitations on both descriptive and
inferential QA tasks compared to human performance, and we observe significant
performance gaps between open-source and closed-source models across multiple
dimensions. These results underscore the challenges of chemistry-aware table
understanding and position ChemTable as a rigorous and realistic benchmark for
advancing scientific reasoning.

</details>


### [6] [Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning](https://arxiv.org/abs/2506.11376)
*Liying Wang,Ph. D.,Daffodil Carrington,M. S.,Daniil Filienko,M. S.,Caroline El Jazmi,M. S.,Serena Jinchen Xie,M. S.,Martine De Cock,Ph. D.,Sarah Iribarren,Ph. D.,Weichao Yuwen,Ph. D*

Main category: cs.AI

TL;DR: A study tested LLM-powered conversational agents to support caregivers' mental health using PST, MI, and BCA. The best models used Few-Shot and RAG techniques, showing improved empathy and therapeutic alliance.


<details>
  <summary>Details</summary>
Motivation: Address mental health challenges faced by family caregivers by leveraging LLMs for evidence-based support.

Method: Conducted a within-subject experiment with 28 caregivers evaluating four LLM configurations, focusing on empathy and therapeutic alliance.

Result: Best models (Few-Shot + RAG) improved contextual understanding and personalized support, validated by qualitative and quantitative feedback.

Conclusion: LLMs show promise in delivering empathetic, tailored support for caregivers, though balancing assessment and advice remains a challenge.

Abstract: Family caregivers often face substantial mental health challenges due to
their multifaceted roles and limited resources. This study explored the
potential of a large language model (LLM)-powered conversational agent to
deliver evidence-based mental health support for caregivers, specifically
Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)
and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted
with 28 caregivers interacting with four LLM configurations to evaluate empathy
and therapeutic alliance. The best-performing models incorporated Few-Shot and
Retrieval-Augmented Generation (RAG) prompting techniques, alongside
clinician-curated examples. The models showed improved contextual understanding
and personalized support, as reflected by qualitative responses and
quantitative ratings on perceived empathy and therapeutic alliances.
Participants valued the model's ability to validate emotions, explore
unexpressed feelings, and provide actionable strategies. However, balancing
thorough assessment with efficient advice delivery remains a challenge. This
work highlights the potential of LLMs in delivering empathetic and tailored
support for family caregivers.

</details>


### [7] [FocalAD: Local Motion Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.11419)
*Bin Sun,Boao Zhang,Jiayi Lu,Xinjie Feng,Jiachen Shang,Rui Cao,Mengchao Zheng,Chuanye Wang,Shichun Yang,Yaoguang Cao,Ziying Song*

Main category: cs.AI

TL;DR: FocalAD is an end-to-end autonomous driving framework focusing on critical local interactions to improve planning reliability, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing motion prediction methods ignore critical local interactions, undermining planning reliability.

Method: FocalAD uses Ego-Local-Agents Interactor (ELAI) for graph-based interaction representation and Focal-Local-Agents Loss (FLA Loss) to prioritize decision-critical agents.

Result: FocalAD reduces collision rates by 41.9% (vs. DiffusionDrive) and 15.6% (vs. SparseDrive) on Adv-nuScenes.

Conclusion: Focusing on local interactions enhances planning reliability and robustness in autonomous driving.

Abstract: In end-to-end autonomous driving,the motion prediction plays a pivotal role
in ego-vehicle planning. However, existing methods often rely on globally
aggregated motion features, ignoring the fact that planning decisions are
primarily influenced by a small number of locally interacting agents. Failing
to attend to these critical local interactions can obscure potential risks and
undermine planning reliability. In this work, we propose FocalAD, a novel
end-to-end autonomous driving framework that focuses on critical local
neighbors and refines planning by enhancing local motion representations.
Specifically, FocalAD comprises two core modules: the Ego-Local-Agents
Interactor (ELAI) and the Focal-Local-Agents Loss (FLA Loss). ELAI conducts a
graph-based ego-centric interaction representation that captures motion
dynamics with local neighbors to enhance both ego planning and agent motion
queries. FLA Loss increases the weights of decision-critical neighboring
agents, guiding the model to prioritize those more relevant to planning.
Extensive experiments show that FocalAD outperforms existing state-of-the-art
methods on the open-loop nuScenes datasets and closed-loop Bench2Drive
benchmark. Notably, on the robustness-focused Adv-nuScenes dataset, FocalAD
achieves even greater improvements, reducing the average colilision rate by
41.9% compared to DiffusionDrive and by 15.6% compared to SparseDrive.

</details>


### [8] [Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention](https://arxiv.org/abs/2506.11445)
*Xuan Duy Ta,Bang Giang Le,Thanh Ha Le,Viet Cuong Ta*

Main category: cs.AI

TL;DR: Proposes a Local State Attention module for MARL to improve conflict resolution and generalization in mixed-traffic environments, showing better merging efficiency in simulations.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles in mixed-traffic need to adapt to human-driven vehicles and stochastic events, but current MARL methods struggle with local conflicts and generalization.

Method: Introduces a Local State Attention module using self-attention to compress nearby agents' information, tested in a highway merging scenario with priority vehicles.

Result: Significant improvements in merging efficiency, especially in high-density traffic, compared to baselines.

Conclusion: The Local State Attention module effectively enhances MARL performance in resolving conflicts and handling stochastic events in traffic.

Abstract: In mixed-traffic environments, autonomous vehicles must adapt to
human-controlled vehicles and other unusual driving situations. This setting
can be framed as a multi-agent reinforcement learning (MARL) environment with
full cooperative reward among the autonomous vehicles. While methods such as
Multi-agent Proximal Policy Optimization can be effective in training MARL
tasks, they often fail to resolve local conflict between agents and are unable
to generalize to stochastic events. In this paper, we propose a Local State
Attention module to assist the input state representation. By relying on the
self-attention operator, the module is expected to compress the essential
information of nearby agents to resolve the conflict in traffic situations.
Utilizing a simulated highway merging scenario with the priority vehicle as the
unexpected event, our approach is able to prioritize other vehicles'
information to manage the merging process. The results demonstrate significant
improvements in merging efficiency compared to popular baselines, especially in
high-density traffic settings.

</details>


### [9] [Structure-Aware Automatic Channel Pruning by Searching with Graph Embedding](https://arxiv.org/abs/2506.11469)
*Zifan Liu,Yuan Cao,Yanwei Yu,Heng Qi,Jie Gui*

Main category: cs.AI

TL;DR: A novel structure-aware automatic channel pruning (SACP) framework using GCNs improves pruning efficiency and accuracy retention by modeling network topology and automating pruning decisions.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods rely on local heuristics or weight-based criteria, missing global structural dependencies, leading to suboptimal performance.

Method: SACP uses graph convolutional networks (GCNs) to model network topology, enabling topology-aware pruning. It dynamically adjusts pruning rate combinations via a search-based approach.

Result: SACP outperforms state-of-the-art methods in compression efficiency and maintains competitive accuracy on CIFAR-10 and ImageNet with models like ResNet and VGG16.

Conclusion: SACP offers a robust, automated solution for channel pruning by leveraging global structural dependencies, enhancing both efficiency and model performance.

Abstract: Channel pruning is a powerful technique to reduce the computational overhead
of deep neural networks, enabling efficient deployment on resource-constrained
devices. However, existing pruning methods often rely on local heuristics or
weight-based criteria that fail to capture global structural dependencies
within the network, leading to suboptimal pruning decisions and degraded model
performance. To address these limitations, we propose a novel structure-aware
automatic channel pruning (SACP) framework that utilizes graph convolutional
networks (GCNs) to model the network topology and learn the global importance
of each channel. By encoding structural relationships within the network, our
approach implements topology-aware pruning and this pruning is fully automated,
reducing the need for human intervention. We restrict the pruning rate
combinations to a specific space, where the number of combinations can be
dynamically adjusted, and use a search-based approach to determine the optimal
pruning rate combinations. Extensive experiments on benchmark datasets
(CIFAR-10, ImageNet) with various models (ResNet, VGG16) demonstrate that SACP
outperforms state-of-the-art pruning methods on compression efficiency and
competitive on accuracy retention.

</details>


### [10] [Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models](https://arxiv.org/abs/2506.11487)
*Chenrui Cao,Liangcheng Song,Zenan Li,Xinyi Le,Xian Zhang,Hui Xue,Fan Yang*

Main category: cs.AI

TL;DR: DSP+ improves automated theorem proving by integrating neuro-symbolic methods without training, achieving competitive results and solving previously unsolved problems.


<details>
  <summary>Details</summary>
Motivation: To challenge the reliance on RL-based training in automated theorem proving by showing that careful coordination of existing models can yield comparable performance.

Method: Enhances the Draft, Sketch, and Prove framework with fine-grained neuro-symbolic improvements in each phase: draft (subgoal generation), sketch (autoformalization), and prove (symbolic search integration).

Result: DSP+ solves 80.7% of miniF2F, 32.8% of ProofNet, and 24 PutnamBench problems, including an unsolved IMO problem, while identifying formalization errors.

Conclusion: DSP+ demonstrates the viability of classical reasoning patterns alongside RL-based methods, with plans to open-source all components.

Abstract: Recent advancements, such as DeepSeek-Prover-V2-671B and
Kimina-Prover-Preview-72B, demonstrate a prevailing trend in leveraging
reinforcement learning (RL)-based large-scale training for automated theorem
proving. Surprisingly, we discover that even without any training, careful
neuro-symbolic coordination of existing off-the-shelf reasoning models and
tactic step provers can achieve comparable performance. This paper introduces
\textbf{DSP+}, an improved version of the Draft, Sketch, and Prove framework,
featuring a \emph{fine-grained and integrated} neuro-symbolic enhancement for
each phase: (1) In the draft phase, we prompt reasoning models to generate
concise natural-language subgoals to benefit the sketch phase, removing
thinking tokens and references to human-written proofs; (2) In the sketch
phase, subgoals are autoformalized with hypotheses to benefit the proving
phase, and sketch lines containing syntactic errors are masked according to
predefined rules; (3) In the proving phase, we tightly integrate symbolic
search methods like Aesop with step provers to establish proofs for the sketch
subgoals. Experimental results show that, without any additional model training
or fine-tuning, DSP+ solves 80.7\%, 32.8\%, and 24 out of 644 problems from
miniF2F, ProofNet, and PutnamBench, respectively, while requiring fewer budgets
compared to state-of-the-arts. DSP+ proves \texttt{imo\_2019\_p1}, an IMO
problem in miniF2F that is not solved by any prior work. Additionally, DSP+
generates proof patterns comprehensible by human experts, facilitating the
identification of formalization errors; For example, eight wrongly formalized
statements in miniF2F are discovered. Our results highlight the potential of
classical reasoning patterns besides the RL-based training. All components will
be open-sourced.

</details>


### [11] [RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning](https://arxiv.org/abs/2506.11555)
*Yu Wang,Shiwan Zhao,Ming Fan,Zhihu Wang,Yubo Zhang,Xicheng Zhang,Zhengfan Wang,Heyuan Huang,Ting Liu*

Main category: cs.AI

TL;DR: RAG+ extends RAG by integrating application-aware reasoning, improving performance by 3-5% on average and up to 7.5% in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RAG paradigms lack explicit application of retrieved knowledge, creating a gap in task-specific reasoning.

Method: RAG+ introduces a dual corpus of knowledge and application examples, retrieved jointly during inference to enable structured reasoning.

Result: RAG+ outperforms standard RAG, with average gains of 3-5% and peak improvements of 7.5% in complex scenarios.

Conclusion: RAG+ bridges retrieval and application, advancing a more interpretable and capable framework for knowledge integration in LLMs.

Abstract: The integration of external knowledge through Retrieval-Augmented Generation
(RAG) has become foundational in enhancing large language models (LLMs) for
knowledge-intensive tasks. However, existing RAG paradigms often overlook the
cognitive step of applying knowledge, leaving a gap between retrieved facts and
task-specific reasoning. In this work, we introduce RAG+, a principled and
modular extension that explicitly incorporates application-aware reasoning into
the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and
aligned application examples, created either manually or automatically, and
retrieves both jointly during inference. This design enables LLMs not only to
access relevant information but also to apply it within structured,
goal-oriented reasoning processes. Experiments across mathematical, legal, and
medical domains, conducted on multiple models, demonstrate that RAG+
consistently outperforms standard RAG variants, achieving average improvements
of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval
with actionable application, RAG+ advances a more cognitively grounded
framework for knowledge integration, representing a step toward more
interpretable and capable LLMs.

</details>


### [12] [Collaborative LLM Inference via Planning for Efficient Reasoning](https://arxiv.org/abs/2506.11578)
*Byeongchan Lee,Jonghoon Lee,Dongyoung Kim,Jaehyung Kim,Jinwoo Shin*

Main category: cs.AI

TL;DR: A framework for small and large LLMs to collaborate at test time, combining strengths of free local deployment and strong reasoning, reducing reliance on costly APIs.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between costly large LLMs (strong reasoning) and free small LLMs (weak reasoning) by enabling collaboration.

Method: A planner model generates high-level plans, guiding a reasoner model in a multi-round cascade, with small and large models alternating roles.

Result: Achieves accuracy comparable to proprietary models while reducing paid inference costs.

Conclusion: Planning is an effective prior for cost-aware, cross-model inference under real-world constraints.

Abstract: Large language models (LLMs) excel at complex reasoning tasks, but those with
strong capabilities (e.g., whose numbers of parameters are larger than 100B)
are often accessible only through paid APIs, making them too costly for
applications of frequent use. In contrast, smaller open-sourced LLMs (e.g.,
whose numbers of parameters are less than 3B) are freely available and easy to
deploy locally (e.g., under a single GPU having 8G VRAM), but lack suff icient
reasoning ability. This trade-off raises a natural question: can small (free)
and large (costly) models collaborate at test time to combine their strengths?
We propose a test-time collaboration framework in which a planner model first
generates a plan, defined as a distilled and high-level abstraction of the
problem.
  This plan serves as a lightweight intermediate that guides a reasoner model,
which generates a complete solution. Small and large models take turns acting
as planner and reasoner, exchanging plans in a multi-round cascade to
collaboratively solve complex tasks. Our method achieves accuracy comparable to
strong proprietary models alone, while significantly reducing reliance on paid
inference. These results highlight planning as an effective prior for
orchestrating cost-aware, cross-model inference under real-world deployment
constraints.

</details>


### [13] [VLM@school -- Evaluation of AI image understanding on German middle school knowledge](https://arxiv.org/abs/2506.11604)
*René Peinl,Vincent Tischler*

Main category: cs.AI

TL;DR: A new German-language benchmark dataset evaluates VLMs on visual reasoning tasks tied to middle school curricula, revealing significant performance gaps, especially in non-English contexts.


<details>
  <summary>Details</summary>
Motivation: To address the lack of real-world, subject-specific benchmarks for VLMs in non-English languages like German, and to evaluate their ability to integrate visual and factual reasoning.

Method: The dataset includes 2,000+ open-ended questions based on 486 images from nine middle school domains. Thirteen VLMs were tested for domain-specific accuracy and adversarial robustness.

Result: Top models scored under 45% accuracy, with notable weaknesses in music, math, and adversarial questions, highlighting discrepancies between benchmark and real-world performance.

Conclusion: Middle school tasks are a valuable but underused way to test VLMs, especially in non-English settings, and the dataset provides a rigorous evaluation tool for future AI improvements.

Abstract: This paper introduces a novel benchmark dataset designed to evaluate the
capabilities of Vision Language Models (VLMs) on tasks that combine visual
reasoning with subject-specific background knowledge in the German language. In
contrast to widely used English-language benchmarks that often rely on
artificially difficult or decontextualized problems, this dataset draws from
real middle school curricula across nine domains including mathematics,
history, biology, and religion. The benchmark includes over 2,000 open-ended
questions grounded in 486 images, ensuring that models must integrate visual
interpretation with factual reasoning rather than rely on superficial textual
cues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple
dimensions, including domain-specific accuracy and performance on adversarial
crafted questions. Our findings reveal that even the strongest models achieve
less than 45% overall accuracy, with particularly poor performance in music,
mathematics, and adversarial settings. Furthermore, the results indicate
significant discrepancies between success on popular benchmarks and real-world
multimodal understanding. We conclude that middle school-level tasks offer a
meaningful and underutilized avenue for stress-testing VLMs, especially in
non-English contexts. The dataset and evaluation protocol serve as a rigorous
testbed to better understand and improve the visual and linguistic reasoning
capabilities of future AI systems.

</details>


### [14] [Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization](https://arxiv.org/abs/2506.11712)
*Wenqi Liu,Xuemeng Song,Jiaxi Li,Yinwei Wei,Na Zheng,Jianhua Yin,Liqiang Nie*

Main category: cs.AI

TL;DR: SymMPO improves MLLMs by using symmetric preference learning and direct supervision to reduce hallucination, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reducing hallucination in MLLMs have non-rigorous optimization and indirect supervision, limiting their effectiveness.

Method: Proposes SymMPO, which uses symmetric preference learning with direct supervision and a preference margin consistency loss for rigorous optimization.

Result: SymMPO outperforms existing methods across five benchmarks, effectively mitigating hallucination.

Conclusion: SymMPO provides a robust solution for hallucination reduction in MLLMs with theoretical and practical advantages.

Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for
mitigating hallucination in Multimodal Large Language Models (MLLMs). Although
existing methods have achieved significant progress by utilizing
vision-oriented contrastive objectives for enhancing MLLMs' attention to visual
inputs and hence reducing hallucination, they suffer from non-rigorous
optimization objective function and indirect preference supervision. To address
these limitations, we propose a Symmetric Multimodal Preference Optimization
(SymMPO), which conducts symmetric preference learning with direct preference
supervision (i.e., response pairs) for visual understanding enhancement, while
maintaining rigorous theoretical alignment with standard DPO. In addition to
conventional ordinal preference learning, SymMPO introduces a preference margin
consistency loss to quantitatively regulate the preference gap between
symmetric preference pairs. Comprehensive evaluation across five benchmarks
demonstrate SymMPO's superior performance, validating its effectiveness in
hallucination mitigation of MLLMs.

</details>


### [15] [Relational GNNs Cannot Learn $C_2$ Features for Planning](https://arxiv.org/abs/2506.11721)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: R-GNNs, a GNN-based method for learning value functions in planning domains, cannot learn functions defined by $C_2$ features despite theoretical connections. Alternative GNN architectures may perform better.


<details>
  <summary>Details</summary>
Motivation: To explore the theoretical and practical limitations of R-GNNs in learning value functions defined by $C_2$ features in planning domains.

Method: Analyzes the expressive power of R-GNNs in relation to $C_2$ features and identifies prior GNN architectures for planning.

Result: R-GNNs cannot learn value functions defined by $C_2$ features, contrary to empirical results.

Conclusion: Alternative GNN architectures may be more suitable for learning value functions based on $C_2$ features in planning domains.

Abstract: Relational Graph Neural Networks (R-GNNs) are a GNN-based approach for
learning value functions that can generalise to unseen problems from a given
planning domain. R-GNNs were theoretically motivated by the well known
connection between the expressive power of GNNs and $C_2$, first-order logic
with two variables and counting. In the context of planning, $C_2$ features
refer to the set of formulae in $C_2$ with relations defined by the unary and
binary predicates of a planning domain. Some planning domains exhibit optimal
value functions that can be decomposed as arithmetic expressions of $C_2$
features. We show that, contrary to empirical results, R-GNNs cannot learn
value functions defined by $C_2$ features. We also identify prior GNN
architectures for planning that may better learn value functions defined by
$C_2$ features.

</details>


### [16] [Causal Effect Identification in Heterogeneous Environments from Higher-Order Moments](https://arxiv.org/abs/2506.11756)
*Yaroslav Kivva,Sina Akbari,Saber Salehkaleybar,Negar Kiyavash*

Main category: cs.AI

TL;DR: The paper explores causal effect estimation with latent confounders using multi-environment data, proposing a moment-based algorithm and proving identifiability conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of estimating causal effects when latent confounders are present, leveraging data from multiple environments where the causal effect remains invariant.

Method: A moment-based algorithm is proposed, assuming only one parameter of the data-generating mechanism varies across environments. Identifiability conditions are analyzed.

Result: The causal effect is identifiable if only one parameter varies (noise or causal relationship). Identifiability is lost if both noise distributions vary. A procedure to identify varying parameters is proposed and tested on synthetic data.

Conclusion: The method provides a viable solution for causal effect estimation with latent confounders under specific conditions, validated by synthetic experiments.

Abstract: We investigate the estimation of the causal effect of a treatment variable on
an outcome in the presence of a latent confounder. We first show that the
causal effect is identifiable under certain conditions when data is available
from multiple environments, provided that the target causal effect remains
invariant across these environments. Secondly, we propose a moment-based
algorithm for estimating the causal effect as long as only a single parameter
of the data-generating mechanism varies across environments -- whether it be
the exogenous noise distribution or the causal relationship between two
variables. Conversely, we prove that identifiability is lost if both exogenous
noise distributions of both the latent and treatment variables vary across
environments. Finally, we propose a procedure to identify which parameter of
the data-generating mechanism has varied across the environments and evaluate
the performance of our proposed methods through experiments on synthetic data.

</details>


### [17] [On the Performance of LLMs for Real Estate Appraisal](https://arxiv.org/abs/2506.11812)
*Margot Geerts,Manon Reusens,Bart Baesens,Seppe vanden Broucke,Jochen De Weerdt*

Main category: cs.AI

TL;DR: LLMs can democratize real estate insights by generating interpretable house price estimates using optimized In-Context Learning, though they struggle with overconfidence and spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Address information asymmetry in the real estate market by leveraging LLMs for accessible and interpretable price estimates.

Method: Systematically evaluate LLMs using diverse international housing datasets and prompting techniques (zero-shot, few-shot, market report-enhanced, hybrid).

Result: LLMs produce meaningful estimates using hedonic variables, offering accessibility and interpretability, though traditional models excel in pure accuracy.

Conclusion: LLMs enhance transparency in real estate appraisal, with practical guidance for prompt optimization, despite limitations in confidence and spatial reasoning.

Abstract: The real estate market is vital to global economies but suffers from
significant information asymmetry. This study examines how Large Language
Models (LLMs) can democratize access to real estate insights by generating
competitive and interpretable house price estimates through optimized
In-Context Learning (ICL) strategies. We systematically evaluate leading LLMs
on diverse international housing datasets, comparing zero-shot, few-shot,
market report-enhanced, and hybrid prompting techniques. Our results show that
LLMs effectively leverage hedonic variables, such as property size and
amenities, to produce meaningful estimates. While traditional machine learning
models remain strong for pure predictive accuracy, LLMs offer a more
accessible, interactive and interpretable alternative. Although
self-explanations require cautious interpretation, we find that LLMs explain
their predictions in agreement with state-of-the-art models, confirming their
trustworthiness. Carefully selected in-context examples based on feature
similarity and geographic proximity, significantly enhance LLM performance, yet
LLMs struggle with overconfidence in price intervals and limited spatial
reasoning. We offer practical guidance for structured prediction tasks through
prompt optimization. Our findings highlight LLMs' potential to improve
transparency in real estate appraisal and provide actionable insights for
stakeholders.

</details>


### [18] [Revealing Political Bias in LLMs through Structured Multi-Agent Debate](https://arxiv.org/abs/2506.11825)
*Aishwarya Bandaru,Fabian Bindley,Trevor Bluth,Nandini Chavda,Baixu Chen,Ethan Law*

Main category: cs.AI

TL;DR: The study explores political biases in LLMs using a multi-agent debate framework, revealing alignment of Neutral agents with Democrats, gender-influenced attitudes, and echo chamber effects among politically aligned agents.


<details>
  <summary>Details</summary>
Motivation: To understand how LLM type and agent gender attributes influence political bias in debates, addressing gaps in research on LLM social behavior simulation.

Method: A structured multi-agent debate framework with Neutral, Republican, and Democrat LLM agents, varying LLMs, genders, and debate formats to analyze bias and attitude shifts.

Result: Neutral agents align with Democrats; Republicans shift toward Neutral; gender affects attitudes; politically aligned agents form echo chambers, intensifying biases.

Conclusion: LLM political biases are influenced by model provenance and agent personas, with gender and shared affiliations shaping debate dynamics and echo chambers.

Abstract: Large language models (LLMs) are increasingly used to simulate social
behaviour, yet their political biases and interaction dynamics in debates
remain underexplored. We investigate how LLM type and agent gender attributes
influence political bias using a structured multi-agent debate framework, by
engaging Neutral, Republican, and Democrat American LLM agents in debates on
politically sensitive topics. We systematically vary the underlying LLMs, agent
genders, and debate formats to examine how model provenance and agent personas
influence political bias and attitudes throughout debates. We find that Neutral
agents consistently align with Democrats, while Republicans shift closer to the
Neutral; gender influences agent attitudes, with agents adapting their opinions
when aware of other agents' genders; and contrary to prior research, agents
with shared political affiliations can form echo chambers, exhibiting the
expected intensification of attitudes as debates progress.

</details>


### [19] [Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment](https://arxiv.org/abs/2506.11880)
*Alejandro Peña,Julian Fierrez,Aythami Morales,Gonzalo Mancera,Miguel Lopez,Ruben Tolosana*

Main category: cs.AI

TL;DR: The paper analyzes how Transformers-based systems learn demographic biases, focusing on AI recruitment, and proposes a privacy-enhancing framework to mitigate gender bias.


<details>
  <summary>Details</summary>
Motivation: The increasing use of LLMs in high-stake settings raises ethical concerns like biases, accountability, and privacy. This work addresses these issues by studying bias learning in AI recruitment.

Method: The study uses a case study on AI recruitment, analyzing data biases in two LLMs, and introduces a framework to reduce gender information in the learning pipeline.

Result: Experiments show the proposed framework effectively prevents trained systems from reproducing data biases.

Conclusion: The framework successfully mitigates gender bias in AI recruitment tools, highlighting its potential for ethical AI deployment.

Abstract: The use of language technologies in high-stake settings is increasing in
recent years, mostly motivated by the success of Large Language Models (LLMs).
However, despite the great performance of LLMs, they are are susceptible to
ethical concerns, such as demographic biases, accountability, or privacy. This
work seeks to analyze the capacity of Transformers-based systems to learn
demographic biases present in the data, using a case study on AI-based
automated recruitment. We propose a privacy-enhancing framework to reduce
gender information from the learning pipeline as a way to mitigate biased
behaviors in the final tools. Our experiments analyze the influence of data
biases on systems built on two different LLMs, and how the proposed framework
effectively prevents trained systems from reproducing the bias in the data.

</details>


### [20] [Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making](https://arxiv.org/abs/2506.11887)
*Claudio Fanconi,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: A cascaded LLM decision framework balances correctness, cost, and confidence by delegating tasks to base models, larger models, or humans, improving accuracy and reducing costs.


<details>
  <summary>Details</summary>
Motivation: To optimize human-AI decision-making by balancing prediction correctness, reasoning cost, and confidence in abstaining or involving experts.

Method: A two-stage cascaded framework: 1) deferral policy for base vs. large model use, 2) abstention policy for human intervention. Includes online learning from feedback.

Result: Outperforms single-model baselines in accuracy for question-answering tasks (ARC, MedQA, MedMCQA) while reducing costs and handling abstentions.

Conclusion: The cascaded framework effectively balances performance, cost, and confidence, with potential for further improvement via human feedback.

Abstract: Effective human-AI decision-making balances three key factors: the
\textit{correctness} of predictions, the \textit{cost} of knowledge and
reasoning complexity, and the confidence about whether to \textit{abstain}
automated answers or involve human experts. In this work, we present a cascaded
LLM decision framework that adaptively delegates tasks across multiple tiers of
expertise -- a base model for initial candidate answers, a more capable and
knowledgeable (but costlier) large model, and a human expert for when the model
cascade abstains. Our method proceeds in two stages. First, a deferral policy
determines whether to accept the base model's answer or regenerate it with the
large model based on the confidence score. Second, an abstention policy decides
whether the cascade model response is sufficiently certain or requires human
intervention. Moreover, we incorporate an online learning mechanism in the
framework that can leverage human feedback to improve decision quality over
time. We demonstrate this approach to general question-answering (ARC-Easy and
ARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results
show that our cascaded strategy outperforms in most cases single-model
baselines in accuracy while reducing cost and providing a principled way to
handle abstentions.

</details>


### [21] [Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task](https://arxiv.org/abs/2506.11986)
*Wuzhenghong Wen,Su Pan,yuwei Sun*

Main category: cs.AI

TL;DR: Schema-R1 improves schema linking in Text-to-SQL by using reinforcement learning, enhancing reasoning ability and achieving a 10% accuracy boost.


<details>
  <summary>Details</summary>
Motivation: Current schema linking models rely on rote-learning, sacrificing reasoning ability due to lack of high-quality reasoning samples.

Method: Schema-R1 involves constructing reasoning samples, supervised fine-tuning, and rule-based reinforcement learning.

Result: The method improves reasoning ability and achieves a 10% increase in filter accuracy.

Conclusion: Schema-R1 effectively addresses the limitations of current schema linking models by leveraging reinforcement learning.

Abstract: Schema linking is a critical step in Text-to-SQL task, aiming to accurately
predict the table names and column names required for the SQL query based on
the given question. However, current fine-tuning approaches for schema linking
models employ a rote-learning paradigm, excessively optimizing for ground truth
schema linking outcomes while compromising reasoning ability. This limitation
arises because of the difficulty in acquiring a high-quality reasoning sample
for downstream tasks. To address this, we propose Schema-R1, a reasoning schema
linking model trained using reinforcement learning. Specifically, Schema-R1
consists of three key steps: constructing small batches of high-quality
reasoning samples, supervised fine-tuning for cold-start initialization, and
rule-based reinforcement learning training. The final results demonstrate that
our method effectively enhances the reasoning ability of the schema linking
model, achieving a 10\% improvement in filter accuracy compared to the existing
method. Our code is available at https://github.com/hongWin/Schema-R1/.

</details>


### [22] [Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making](https://arxiv.org/abs/2506.12012)
*Xiaopeng Yuan,Xingjian Zhang,Ke Xu,Yifan Xu,Lijun Yu,Jindong Wang,Yushun Dong,Haohan Wang*

Main category: cs.AI

TL;DR: The paper proposes evaluating LLMs' intermediate reasoning steps (planning, revision, resource-constrained decisions) using strategic games, introducing new metrics beyond win rates. ChatGPT-o3-mini performed best, while Qwen-Plus struggled with resource use.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks overlook intermediate reasoning steps in LLMs, which are crucial for understanding model behavior and improving reliability.

Method: Strategic games are used as an evaluation environment, with metrics like overcorrection risk rate, correction success rate, improvement slope, and over-budget ratio.

Result: ChatGPT-o3-mini scored highest (74.7% win rate, 78.6% correction success), while Qwen-Plus had poor performance due to excessive resource use. A negative correlation (-0.51) was found between overcorrection risk and correction success.

Conclusion: Assessing how LLMs arrive at decisions, not just outcomes, is valuable for understanding and improving their reliability.

Abstract: Large language models (LLMs) are increasingly used for tasks that require
complex reasoning. Most benchmarks focus on final outcomes but overlook the
intermediate reasoning steps - such as planning, revision, and decision making
under resource constraints. We argue that measuring these internal processes is
essential for understanding model behavior and improving reliability. We
propose using strategic games as a natural evaluation environment: closed,
rule-based systems with clear states, limited resources, and automatic
feedback. We introduce a framework that evaluates LLMs along three core
dimensions: planning, revision, and resource-constrained decision making. To
operationalize this, we define metrics beyond win rate, including
overcorrection risk rate, correction success rate, improvement slope, and
over-budget ratio. In 4320 adversarial rounds across 12 leading models,
ChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7
percent, a correction success rate of 78.6 percent, and an improvement slope of
0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6
percent, wins only 25.6 percent of its matches - primarily due to excessive
resource use. We also observe a negative correlation between overcorrection
risk rate and correction success rate (Pearson r = -0.51, p = 0.093),
suggesting that more frequent edits do not always improve outcomes. Our
findings highlight the value of assessing not only what LLMs decide but how
they arrive at those decisions

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Developing a Dyslexia Indicator Using Eye Tracking](https://arxiv.org/abs/2506.11004)
*Kevin Cogan,Vuong M. Ngo,Mark Roantree*

Main category: cs.LG

TL;DR: The paper explores using eye-tracking and machine learning for early dyslexia detection, achieving 88.58% accuracy with a Random Forest Classifier.


<details>
  <summary>Details</summary>
Motivation: Dyslexia affects 10-20% of people, necessitating accessible diagnostic tools. Eye-tracking offers a non-invasive, cost-effective solution.

Method: Analyzed eye movement patterns (fixation, saccades) and used a Random Forest Classifier for detection. Hierarchical clustering identified severity levels.

Result: Achieved 88.58% accuracy in dyslexia detection and classified severity levels using clustering.

Conclusion: Eye-tracking combined with machine learning provides an accurate, accessible, and non-invasive diagnostic method for dyslexia.

Abstract: Dyslexia, affecting an estimated 10% to 20% of the global population,
significantly impairs learning capabilities, highlighting the need for
innovative and accessible diagnostic methods. This paper investigates the
effectiveness of eye-tracking technology combined with machine learning
algorithms as a cost-effective alternative for early dyslexia detection. By
analyzing general eye movement patterns, including prolonged fixation durations
and erratic saccades, we proposed an enhanced solution for determining
eye-tracking-based dyslexia features. A Random Forest Classifier was then
employed to detect dyslexia, achieving an accuracy of 88.58\%. Additionally,
hierarchical clustering methods were applied to identify varying severity
levels of dyslexia. The analysis incorporates diverse methodologies across
various populations and settings, demonstrating the potential of this
technology to identify individuals with dyslexia, including those with
borderline traits, through non-invasive means. Integrating eye-tracking with
machine learning represents a significant advancement in the diagnostic
process, offering a highly accurate and accessible method in clinical research.

</details>


### [24] [Data Science: a Natural Ecosystem](https://arxiv.org/abs/2506.11010)
*Emilio Porcu,Roy El Moukari,Laurent Najman,Francisco Herrera,Horst Simon*

Main category: cs.LG

TL;DR: The paper presents a data-centric view of 'essential data science,' organizing it into computational and foundational aspects, and warns of potential divergence between them without measures to assess the usefulness of data discoveries.


<details>
  <summary>Details</summary>
Motivation: To provide a holistic understanding of data science as an ecosystem, addressing challenges from the data universe and its complexities.

Method: The paper defines essential data science, splits it into computational and foundational aspects, and proposes measuring the usefulness of data discoveries to prevent divergence.

Result: Identifies a threat of divergence between computational and foundational data science and suggests rigorous approaches to mitigate it.

Conclusion: Rigorous evaluation of data discoveries' usefulness is crucial to maintaining cohesion in essential data science.

Abstract: This manuscript provides a holistic (data-centric) view of what we term
essential data science, as a natural ecosystem with challenges and missions
stemming from the data universe with its multiple combinations of the 5D
complexities (data structure, domain, cardinality, causality, and ethics) with
the phases of the data life cycle. Data agents perform tasks driven by specific
goals. The data scientist is an abstract entity that comes from the logical
organization of data agents with their actions. Data scientists face challenges
that are defined according to the missions. We define specific
discipline-induced data science, which in turn allows for the definition of
pan-data science, a natural ecosystem that integrates specific disciplines with
the essential data science. We semantically split the essential data science
into computational, and foundational. We claim that there is a serious threat
of divergence between computational and foundational data science. Especially,
if no approach is taken to rate whether a data universe discovery should be
useful or not. We suggest that rigorous approaches to measure the usefulness of
data universe discoveries might mitigate such a divergence.

</details>


### [25] [Not All Clients Are Equal: Personalized Federated Learning on Heterogeneous Multi-Modal Clients](https://arxiv.org/abs/2506.11024)
*Minhyuk Seo,Taeheon Kim,Hankook Lee,Jonghyun Choi,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: The paper addresses privacy and cost issues in centralized foundation models by proposing a personalized federated learning (PFL) approach. It tackles data and model heterogeneity in real-world scenarios with novel aggregation and knowledge-sharing methods.


<details>
  <summary>Details</summary>
Motivation: Centralized training of foundation models raises privacy concerns and high transmission costs. PFL offers a distributed alternative but faces challenges in real-world data and model heterogeneity.

Method: Proposes a task-similarity-aware model aggregation for data heterogeneity and a dimension-invariant module for model heterogeneity.

Result: The approach outperforms state-of-the-art methods in personalization and generalization, validated on a multi-modal PFL benchmark with 40 tasks.

Conclusion: The proposed methods effectively address real-world heterogeneity in PFL, enhancing both personalization and generalization.

Abstract: Foundation models have shown remarkable capabilities across diverse
multi-modal tasks, but their centralized training raises privacy concerns and
induces high transmission costs. In contrast, federated learning (FL) offers a
distributed alternative without the need to share data. Recently, for the
growing demand for personalizing AI models for different user purposes,
personalized federated learning (PFL) has emerged. PFL allows each client to
leverage the knowledge of other clients for further adaptation to individual
user preferences, again without the need to share data. Despite its potential,
most PFL studies remain confined to simulated environments, overlooking the
data and model heterogeneity that arise in real-world scenarios. In contrast,
we first consider large data heterogeneity, evaluating on a new benchmark for
multi-modal PFL, spanning 40 distinct tasks with realistic data distribution
shifts. We then consider model heterogeneity in that we do not assume that all
clients share similar model architectures. To address data heterogeneity, we
propose a task-similarity-aware model aggregation method that provides
customized global models to each client. For model heterogeneity, we propose a
dimension-invariant module that enables knowledge sharing across heterogeneous
models. Empirical validations demonstrate that the proposed approach
outperforms the state-of-the-art, excelling in both personalization and
generalization capabilities.

</details>


### [26] [When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces](https://arxiv.org/abs/2506.11025)
*Miriam Doh,Aditya Gulati,Matei Mancas,Nuria Oliver*

Main category: cs.LG

TL;DR: Synthetic faces and gender classification algorithms show bias, linking attractiveness to unrelated traits and performing worse on less-attractive, non-White women, raising fairness concerns.


<details>
  <summary>Details</summary>
Motivation: To investigate algorithmic lookism—preferential treatment based on appearance—in synthetic faces and gender classification models.

Method: Experiments with 13,200 synthetically generated faces, analyzing text-to-image (T2I) systems and gender classification models.

Result: T2I systems link attractiveness to unrelated positive traits; gender models have higher error rates for less-attractive, non-White women.

Conclusion: The findings highlight fairness issues in digital identity systems due to appearance-based biases.

Abstract: This paper examines how synthetically generated faces and machine
learning-based gender classification algorithms are affected by algorithmic
lookism, the preferential treatment based on appearance. In experiments with
13,200 synthetically generated faces, we find that: (1) text-to-image (T2I)
systems tend to associate facial attractiveness to unrelated positive traits
like intelligence and trustworthiness; and (2) gender classification models
exhibit higher error rates on "less-attractive" faces, especially among
non-White women. These result raise fairness concerns regarding digital
identity systems.

</details>


### [27] [Evaluating Privacy-Utility Tradeoffs in Synthetic Smart Grid Data](https://arxiv.org/abs/2506.11026)
*Andre Catarino,Rui Melo,Rui Abreu,Luis Cruz*

Main category: cs.LG

TL;DR: The paper evaluates four synthetic data generation methods for dToU electricity tariffs, highlighting diffusion models for utility and CTGAN for privacy.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns with real consumption data by using synthetic alternatives for identifying households benefiting from dToU tariffs.

Method: Comparative evaluation of WGAN, CTGAN, Diffusion Models, and Gaussian noise augmentation, assessing utility, fidelity, and privacy.

Result: Diffusion models achieve the highest utility (88.2% macro-F1), while CTGAN resists reconstruction attacks best.

Conclusion: Structured generative models like diffusion models and CTGAN are promising for privacy-preserving energy systems.

Abstract: The widespread adoption of dynamic Time-of-Use (dToU) electricity tariffs
requires accurately identifying households that would benefit from such pricing
structures. However, the use of real consumption data poses serious privacy
concerns, motivating the adoption of synthetic alternatives. In this study, we
conduct a comparative evaluation of four synthetic data generation methods,
Wasserstein-GP Generative Adversarial Networks (WGAN), Conditional Tabular GAN
(CTGAN), Diffusion Models, and Gaussian noise augmentation, under different
synthetic regimes. We assess classification utility, distribution fidelity, and
privacy leakage. Our results show that architectural design plays a key role:
diffusion models achieve the highest utility (macro-F1 up to 88.2%), while
CTGAN provide the strongest resistance to reconstruction attacks. These
findings highlight the potential of structured generative models for developing
privacy-preserving, data-driven energy systems.

</details>


### [28] [From Reasoning to Code: GRPO Optimization for Underrepresented Languages](https://arxiv.org/abs/2506.11027)
*Federico Pennino,Bianca Raimondi,Massimo Rondelli,Andrea Gurioli,Maurizio Gabbrielli*

Main category: cs.LG

TL;DR: The paper introduces a method using Qwen 2.5 and GRPO to improve code generation for languages with limited training data, demonstrated with Prolog.


<details>
  <summary>Details</summary>
Motivation: Challenges in generating accurate code for less popular languages due to scarce training data.

Method: Combines Qwen 2.5 with GRPO, integrating reasoning-driven feedback into reinforcement learning.

Result: Improved reasoning quality, code accuracy, and logical correctness in Prolog code generation.

Conclusion: The approach shows promise for languages lacking extensive training resources.

Abstract: Generating accurate and executable code using large language models (LLMs) is
challenging for languages with limited public training data compared to popular
languages such as Python. This paper introduces a generalizable approach that
uses small-scale code versions of the Qwen 2.5 model combined with Group
Relative Policy Optimization (GRPO) to enable effective code generation through
explicit reasoning steps, which is particularly beneficial for languages with
smaller source code databases. Using Prolog as a representative use case --
given its limited online presence -- the initial model faced challenges in
generating executable code. After some training steps, the model successfully
produces logically consistent and syntactically accurate code by directly
integrating reasoning-driven feedback into the reinforcement learning loop.
Experimental evaluations using mathematical logic problem benchmarks illustrate
significant improvements in reasoning quality, code accuracy, and logical
correctness, underscoring the potential of this approach to benefit a wide
range of programming languages lacking extensive training resources.

</details>


### [29] [Enhancing Epidemic Forecasting: Evaluating the Role of Mobility Data and Graph Convolutional Networks](https://arxiv.org/abs/2506.11028)
*Suhan Guo,Zhenghao Xu,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: The study explores the role of mobility data and GCNs in predicting disease outbreaks, finding that mortality/hospitalization data improves accuracy more than mobility data. Spatial maps from GCNs correlate with lockdown orders, suggesting their utility as mobility indicators.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between machine learning algorithms and epidemiological applications, addressing the underperformance of benchmark methods with real-world data due to mobility integration challenges.

Method: A two-phase approach: pilot study on mobility data significance, followed by evaluating GCNs on a transformer backbone.

Result: Mobility data and GCNs didn't significantly improve forecasting, but mortality/hospitalization data did. GCN-derived spatial maps correlated with lockdown orders.

Conclusion: The research provides a novel perspective on mobility representation in disease prediction, aiding better outbreak preparedness.

Abstract: Accurate prediction of contagious disease outbreaks is vital for informed
decision-making. Our study addresses the gap between machine learning
algorithms and their epidemiological applications, noting that methods optimal
for benchmark datasets often underperform with real-world data due to
difficulties in incorporating mobility information. We adopt a two-phase
approach: first, assessing the significance of mobility data through a pilot
study, then evaluating the impact of Graph Convolutional Networks (GCNs) on a
transformer backbone. Our findings reveal that while mobility data and GCN
modules do not significantly enhance forecasting performance, the inclusion of
mortality and hospitalization data markedly improves model accuracy.
Additionally, a comparative analysis between GCN-derived spatial maps and
lockdown orders suggests a notable correlation, highlighting the potential of
spatial maps as sensitive indicators for mobility. Our research offers a novel
perspective on mobility representation in predictive modeling for contagious
diseases, empowering decision-makers to better prepare for future outbreaks.

</details>


### [30] [Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model](https://arxiv.org/abs/2506.11029)
*Xue Wang,Tian Zhou,Jinyang Gao,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: YingLong, a non-causal bidirectional transformer, achieves state-of-the-art time series forecasting with a novel scaling effect and outperforms other models by 14-44%.


<details>
  <summary>Details</summary>
Motivation: To improve time series forecasting by introducing a joint framework that contrasts traditional methods and leverages delayed chain-of-thought reasoning.

Method: Uses a non-causal, bidirectional attention encoder-only transformer trained via masked token recovery, enhanced by a multi-input ensemble to reduce output variance.

Result: YingLong outperforms other models by 14-44% on benchmarks, achieving >60% best performance in zero-shot tasks on ETT and Weather datasets.

Conclusion: YingLong demonstrates superior performance and generalizability, with pretrained models available for use.

Abstract: We present a joint forecasting framework for time series prediction that
contrasts with traditional direct or recursive methods. This framework achieves
state-of-the-art performance for our designed foundation model, YingLong, and
reveals a novel scaling effect: longer outputs significantly enhance model
accuracy due to delayed chain-of-thought reasoning in our non-causal approach.
YingLong is a non-causal, bidirectional attention encoder-only transformer
trained through masked token recovery, aligning more effectively with language
understanding tasks than with generation tasks. Additionally, we boost
performance by tackling output variance with a multi-input ensemble. We release
four foundation models ranging from 6M to 300M parameters, demonstrating
superior results in zero-shot tasks on the ETT and Weather datasets. YingLong
achieves more than 60% best performance. To ensure generalizability, we
assessed the models using the GIFT-Eval benchmark, which comprises 23 time
series datasets across 7 domains. Yinglong significantly outperformed the best
time-series foundation models, end-to-end trained models by 14% and 44% in rank
respectively.The pretrained 300M model is available at
https://huggingface.co/qcw1314/YingLong_300m

</details>


### [31] [Forward Target Propagation: A Forward-Only Approach to Global Error Credit Assignment via Local Losses](https://arxiv.org/abs/2506.11030)
*Nazmus Saadat As-Saquib,A N M Nafiz Abeer,Hung-Ta Chien,Byung-Jun Yoon,Suhas Kumar,Su-in Yi*

Main category: cs.LG

TL;DR: Forward Target Propagation (FTP) is a biologically plausible alternative to backpropagation (BP), using forward passes for efficient, modular learning without symmetric weights or inverse functions. It matches BP's accuracy on tasks like MNIST and CIFAR, excels in low-precision hardware, and is energy-efficient.


<details>
  <summary>Details</summary>
Motivation: BP has biological and hardware limitations (e.g., symmetric weights, non-local credit assignment). FTP aims to address these by offering a biologically plausible, efficient alternative.

Method: FTP replaces BP's backward pass with a second forward pass, estimating layerwise targets via feedforward computations, avoiding symmetric feedback or inverse functions.

Result: FTP achieves BP-competitive accuracy on MNIST, CIFAR10, and CIFAR100, handles sequential tasks well, and outperforms BP in low-precision hardware. It's also more efficient than other biologically inspired methods.

Conclusion: FTP is a promising, energy-efficient alternative to BP, suitable for on-device learning and neuromorphic computing due to its forward-only nature and hardware compatibility.

Abstract: Training neural networks has traditionally relied on backpropagation (BP), a
gradient-based algorithm that, despite its widespread success, suffers from key
limitations in both biological and hardware perspectives. These include
backward error propagation by symmetric weights, non-local credit assignment,
and frozen activity during backward passes. We propose Forward Target
Propagation (FTP), a biologically plausible and computationally efficient
alternative that replaces the backward pass with a second forward pass. FTP
estimates layerwise targets using only feedforward computations, eliminating
the need for symmetric feedback weights or learnable inverse functions, hence
enabling modular and local learning. We evaluate FTP on fully connected
networks, CNNs, and RNNs, demonstrating accuracies competitive with BP on
MNIST, CIFAR10, and CIFAR100, as well as effective modeling of long-term
dependencies in sequential tasks. Moreover, FTP outperforms BP under quantized
low-precision and emerging hardware constraints while also demonstrating
substantial efficiency gains over other biologically inspired methods such as
target propagation variants and forward-only learning algorithms. With its
minimal computational overhead, forward-only nature, and hardware
compatibility, FTP provides a promising direction for energy-efficient
on-device learning and neuromorphic computing.

</details>


### [32] [Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models](https://arxiv.org/abs/2506.11031)
*Zoher Kachwala,Danishjeet Singh,Danielle Yang,Filippo Menczer*

Main category: cs.LG

TL;DR: The paper introduces zero-shot-s², a task-aligned prompting method for detecting AI-generated images using pre-trained Vision-Language Models (VLMs), improving performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of supervised detection methods for AI-generated images, which struggle with generalization across diverse generators.

Method: Utilizes pre-trained VLMs with task-aligned prompting (zero-shot-s²) to enhance detection performance, avoiding the need for fine-tuning.

Result: Zero-shot-s² boosts Macro F1 scores by 8%-29% across diverse datasets and models, demonstrating strong generalization and robustness.

Conclusion: Task-aligned prompting enhances VLMs' latent capabilities for AI-generated image detection, offering a simple, generalizable, and explainable alternative to supervised methods.

Abstract: As image generators produce increasingly realistic images, concerns about
potential misuse continue to grow. Supervised detection relies on large,
curated datasets and struggles to generalize across diverse generators. In this
work, we investigate the use of pre-trained Vision-Language Models (VLMs) for
zero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit
some task-specific reasoning and chain-of-thought prompting offers gains, we
show that task-aligned prompting elicits more focused reasoning and
significantly improves performance without fine-tuning. Specifically, prefixing
the model's response with the phrase ``Let's examine the style and the
synthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1
scores by 8%-29% for two widely used open-source models. These gains are
consistent across three recent, diverse datasets spanning human faces, objects,
and animals with images generated by 16 different models -- demonstrating
strong generalization. We further evaluate the approach across three additional
model sizes and observe improvements in most dataset-model combinations --
suggesting robustness to model scale. Surprisingly, self-consistency, a
behavior previously observed in language reasoning, where aggregating answers
from diverse reasoning paths improves performance, also holds in this setting.
Even here, zero-shot-s$^2$ scales better than chain-of-thought in most cases --
indicating that it elicits more useful diversity. Our findings show that
task-aligned prompts elicit more focused reasoning and enhance latent
capabilities in VLMs, like the detection of AI-generated images -- offering a
simple, generalizable, and explainable alternative to supervised methods. Our
code is publicly available on github:
https://github.com/osome-iu/Zero-shot-s2.git.

</details>


### [33] [Deep Learning Approach to Bearing and Induction Motor Fault Diagnosis via Data Fusion](https://arxiv.org/abs/2506.11032)
*Mert Sehri,Merve Ertagrin,Ozal Yildirim,Ahmet Orhan,Patrick Dumond*

Main category: cs.LG

TL;DR: CNNs and LSTMs are used for motor diagnosis by fusing accelerometer and microphone data, promoting multi-sensor data collection.


<details>
  <summary>Details</summary>
Motivation: To improve bearing and induction motor diagnosis by leveraging deep learning and sensor fusion.

Method: Uses CNNs for accelerometer/microphone data and LSTMs for sensor fusion.

Result: Demonstrates effective diagnosis through combined sensor data.

Conclusion: Encourages multi-sensor data collection and deep learning for motor diagnosis.

Abstract: Convolutional Neural Networks (CNNs) are used to evaluate accelerometer and
microphone data for bearing and induction motor diagnosis. A Long Short-Term
Memory (LSTM) recurrent neural network is used to combine sensor information
effectively, highlighting the benefits of data fusion. This approach encourages
researchers to focus on multi model diagnosis for constant speed data
collection by proposing a comprehensive way to use deep learning and sensor
fusion and encourages data scientists to collect more multi-sensor data,
including acoustic and accelerometer datasets.

</details>


### [34] [Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees](https://arxiv.org/abs/2506.11033)
*Minjae Kwon,Tyler Ingebrand,Ufuk Topcu,Lu Feng*

Main category: cs.LG

TL;DR: A runtime shielding mechanism for reinforcement learning is developed to handle hidden parameters, ensuring safety via real-time adaptation and probabilistic guarantees.


<details>
  <summary>Details</summary>
Motivation: Hidden parameters like robot mass or friction introduce safety risks during execution, necessitating adaptive safety measures.

Method: Uses constrained hidden-parameter Markov decision processes, function encoders for real-time inference, and conformal prediction for uncertainty.

Result: Significantly reduces safety violations, achieves strong generalization, and incurs minimal runtime overhead.

Conclusion: The proposed mechanism ensures probabilistic safety and optimality among safety-compliant policies.

Abstract: Variations in hidden parameters, such as a robot's mass distribution or
friction, pose safety risks during execution. We develop a runtime shielding
mechanism for reinforcement learning, building on the formalism of constrained
hidden-parameter Markov decision processes. Function encoders enable real-time
inference of hidden parameters from observations, allowing the shield and the
underlying policy to adapt online. The shield constrains the action space by
forecasting future safety risks (such as obstacle proximity) and accounts for
uncertainty via conformal prediction. We prove that the proposed mechanism
satisfies probabilistic safety guarantees and yields optimal policies among the
set of safety-compliant policies. Experiments across diverse environments with
varying hidden parameters show that our method significantly reduces safety
violations and achieves strong out-of-distribution generalization, while
incurring minimal runtime overhead.

</details>


### [35] [CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.11034)
*Aneesh Komanduri,Karuna Bhaila,Xintao Wu*

Main category: cs.LG

TL;DR: The paper introduces CausalVLBench, a benchmark for evaluating large vision-language models (LVLMs) on visual causal reasoning tasks, highlighting their strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: Despite the success of LVLMs in tasks like recognition and VQA, their performance in visual causal reasoning remains underexplored. This work aims to fill that gap.

Method: The authors propose CausalVLBench, a benchmark with three tasks: causal structure inference, intervention target prediction, and counterfactual prediction, evaluated on state-of-the-art LVLMs.

Result: The evaluation reveals fundamental strengths and weaknesses of LVLMs in visual causal reasoning, providing insights for improvement.

Conclusion: The benchmark aims to highlight limitations of current LVLMs and inspire new approaches to enhance their visual causal reasoning capabilities.

Abstract: Large language models (LLMs) have shown remarkable ability in various
language tasks, especially with their emergent in-context learning capability.
Extending LLMs to incorporate visual inputs, large vision-language models
(LVLMs) have shown impressive performance in tasks such as recognition and
visual question answering (VQA). Despite increasing interest in the utility of
LLMs in causal reasoning tasks such as causal discovery and counterfactual
reasoning, there has been relatively little work showcasing the abilities of
LVLMs on visual causal reasoning tasks. We take this opportunity to formally
introduce a comprehensive causal reasoning benchmark for multi-modal in-context
learning from LVLMs. Our CausalVLBench encompasses three representative tasks:
causal structure inference, intervention target prediction, and counterfactual
prediction. We evaluate the ability of state-of-the-art open-source LVLMs on
our causal reasoning tasks across three causal representation learning datasets
and demonstrate their fundamental strengths and weaknesses. We hope that our
benchmark elucidates the drawbacks of existing vision-language models and
motivates new directions and paradigms in improving the visual causal reasoning
abilities of LVLMs.

</details>


### [36] [Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity](https://arxiv.org/abs/2506.11035)
*Moussa Koulako Bala Doumbouya,Dan Jurafsky,Christopher D. Manning*

Main category: cs.LG

TL;DR: The paper introduces a differentiable version of Tversky's similarity model for deep learning, replacing geometric similarity with feature-based similarity, improving performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current geometric similarity models in deep learning lack psychological plausibility, unlike Tversky's feature-based model, which better aligns with human perception.

Method: A differentiable parameterization of Tversky's similarity is developed, including neural network components like the Tversky projection layer, which replaces linear projection layers.

Result: Experiments show significant improvements: 24.7% accuracy boost in image classification and 7.5% perplexity reduction in language modeling, with fewer parameters.

Conclusion: The work provides a psychologically plausible and interpretable alternative to geometric similarity in deep learning, validated by empirical results.

Abstract: Work in psychology has highlighted that the geometric model of similarity
standard in deep learning is not psychologically plausible because its metric
properties such as symmetry do not align with human perception. In contrast,
Tversky (1977) proposed an axiomatic theory of similarity based on a
representation of objects as sets of features, and their similarity as a
function of common and distinctive features. However, this model has not been
used in deep learning before, partly due to the challenge of incorporating
discrete set operations. We develop a differentiable parameterization of
Tversky's similarity that is learnable through gradient descent, and derive
neural network building blocks such as the Tversky projection layer, which
unlike the linear projection layer can model non-linear functions such as XOR.
Through experiments with image recognition and language modeling, we show that
the Tversky projection layer is a beneficial replacement for the linear
projection layer, which employs geometric similarity. On the NABirds image
classification task, a frozen ResNet-50 adapted with a Tversky projection layer
achieves a 24.7% relative accuracy improvement over the linear layer adapter
baseline. With Tversky projection layers, GPT-2's perplexity on PTB decreases
by 7.5%, and its parameter count by 34.8%. Finally, we propose a unified
interpretation of both projection layers as computing similarities of input
stimuli to learned prototypes, for which we also propose a novel visualization
technique highlighting the interpretability of Tversky projection layers. Our
work offers a new paradigm for thinking about the similarity model implicit in
deep learning, and designing networks that are interpretable under an
established theory of psychological similarity.

</details>


### [37] [Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification](https://arxiv.org/abs/2506.11036)
*Yang Qin,Chao Chen,Zhihang Fu,Dezhong Peng,Xi Peng,Peng Hu*

Main category: cs.LG

TL;DR: The paper proposes an Interactive Cross-modal Learning (ICL) framework to improve text-to-image person re-identification (TIReID) by leveraging human-centered interaction and a plug-and-play Test-time Human-centered Interaction (THI) module. It also introduces Reorganization Data Augmentation (RDA) to enhance query discriminability.


<details>
  <summary>Details</summary>
Motivation: Existing TIReID methods struggle with challenging candidate images due to network architecture and data quality limitations.

Method: The ICL framework uses THI for multi-round interactions with a multimodal large language model (MLLM) to refine queries. RDA enriches, decomposes, and reorganizes person descriptions to improve training text quality.

Result: Extensive experiments on four benchmarks (CUHK-PEDES, ICFG-PEDES, RSTPReid, UFine6926) show substantial performance improvement.

Conclusion: The proposed ICL framework and RDA strategy effectively enhance TIReID performance by addressing query refinement and data quality issues.

Abstract: Despite remarkable advancements in text-to-image person re-identification
(TIReID) facilitated by the breakthrough of cross-modal embedding models,
existing methods often struggle to distinguish challenging candidate images due
to intrinsic limitations, such as network architecture and data quality. To
address these issues, we propose an Interactive Cross-modal Learning framework
(ICL), which leverages human-centered interaction to enhance the
discriminability of text queries through external multimodal knowledge. To
achieve this, we propose a plug-and-play Test-time Humane-centered Interaction
(THI) module, which performs visual question answering focused on human
characteristics, facilitating multi-round interactions with a multimodal large
language model (MLLM) to align query intent with latent target images.
Specifically, THI refines user queries based on the MLLM responses to reduce
the gap to the best-matching images, thereby boosting ranking accuracy.
Additionally, to address the limitation of low-quality training texts, we
introduce a novel Reorganization Data Augmentation (RDA) strategy based on
information enrichment and diversity enhancement to enhance query
discriminability by enriching, decomposing, and reorganizing person
descriptions. Extensive experiments on four TIReID benchmarks, i.e.,
CUHK-PEDES, ICFG-PEDES, RSTPReid, and UFine6926, demonstrate that our method
achieves remarkable performance with substantial improvement.

</details>


### [38] [Mini-Game Lifetime Value Prediction in WeChat](https://arxiv.org/abs/2506.11037)
*Aochuan Chen,Yifan Niu,Ziqi Gao,Yujie Sun,Shoujun Liu,Gong Chen,Yang Liu,Jia Li*

Main category: cs.LG

TL;DR: The paper introduces GRePO-LTV, a framework combining graph representation learning and Pareto-optimization to improve LTV prediction in data-scarce, highly correlated advertising scenarios.


<details>
  <summary>Details</summary>
Motivation: Accurate LTV prediction is crucial for aligning ads with user interests, but data scarcity and task interdependencies complicate this.

Method: Uses graph representation learning for data scarcity and Pareto-optimization for task interdependence.

Result: Proposes GRePO-LTV to enhance LTV prediction accuracy.

Conclusion: GRePO-LTV effectively addresses challenges in LTV prediction for advertising.

Abstract: The LifeTime Value (LTV) prediction, which endeavors to forecast the
cumulative purchase contribution of a user to a particular item, remains a
vital challenge that advertisers are keen to resolve. A precise LTV prediction
system enhances the alignment of user interests with meticulously designed
advertisements, thereby generating substantial profits for advertisers.
Nonetheless, this issue is complicated by the paucity of data typically
observed in real-world advertising scenarios. The purchase rate among
registered users is often as critically low as 0.1%, resulting in a dataset
where the majority of users make only several purchases. Consequently, there is
insufficient supervisory signal for effectively training the LTV prediction
model. An additional challenge emerges from the interdependencies among tasks
with high correlation. It is a common practice to estimate a user's
contribution to a game over a specified temporal interval. Varying the lengths
of these intervals corresponds to distinct predictive tasks, which are highly
correlated. For instance, predictions over a 7-day period are heavily reliant
on forecasts made over a 3-day period, where exceptional cases can adversely
affect the accuracy of both tasks. In order to comprehensively address the
aforementioned challenges, we introduce an innovative framework denoted as
Graph-Represented Pareto-Optimal LifeTime Value prediction (GRePO-LTV). Graph
representation learning is initially employed to address the issue of data
scarcity. Subsequently, Pareto-Optimization is utilized to manage the
interdependence of prediction tasks.

</details>


### [39] [MoTE: Mixture of Task-specific Experts for Pre-Trained ModelBased Class-incremental Learning](https://arxiv.org/abs/2506.11038)
*Linjie Li,Zhenyu Wu,Yang Ji*

Main category: cs.LG

TL;DR: The paper proposes MoTE, a framework for class-incremental learning (CIL) using pre-trained models, addressing prompt overwriting and dimensional misalignment issues. It introduces task-aware expert filtering and joint inference, outperforming existing methods without needing exemplars.


<details>
  <summary>Details</summary>
Motivation: CIL requires models to learn continuously without forgetting. Existing methods like prompt-based or adapter-based approaches face challenges like overwriting and dimensional misalignment. MoE's expert fusion could help but is prone to overwriting.

Method: The MoTE framework mitigates output dimension inconsistencies by leveraging task-aware expert filtering and joint inference, inspired by MoE's feature fusion and sparse activation. It avoids catastrophic forgetting and scales linearly with adapters.

Result: MoTE outperforms existing methods without requiring exemplars. The Adapter-Limited MoTE explores the trade-off between adapter expansion and performance.

Conclusion: MoTE effectively addresses CIL challenges, offering a scalable and efficient solution with superior performance.

Abstract: Class-incremental learning (CIL) requires deep learning models to
continuously acquire new knowledge from streaming data while preserving
previously learned information. Recently, CIL based on pre-trained models
(PTMs) has achieved remarkable success. However, prompt-based approaches suffer
from prompt overwriting, while adapter-based methods face challenges such as
dimensional misalignment between tasks. While the idea of expert fusion in
Mixture of Experts (MoE) can help address dimensional inconsistency, both
expert and routing parameters are prone to being overwritten in dynamic
environments, making MoE challenging to apply directly in CIL. To tackle these
issues, we propose a mixture of task-specific experts (MoTE) framework that
effectively mitigates the miscalibration caused by inconsistent output
dimensions across tasks. Inspired by the weighted feature fusion and sparse
activation mechanisms in MoE, we introduce task-aware expert filtering and
reliable expert joint inference during the inference phase, mimicking the
behavior of routing layers without inducing catastrophic forgetting. Extensive
experiments demonstrate the superiority of our method without requiring an
exemplar set. Furthermore, the number of tasks in MoTE scales linearly with the
number of adapters. Building on this, we further explore the trade-off between
adapter expansion and model performance and propose the Adapter-Limited MoTE.
The code is available at https://github.com/Franklilinjie/MoTE.

</details>


### [40] [Angle Domain Guidance: Latent Diffusion Requires Rotation Rather Than Extrapolation](https://arxiv.org/abs/2506.11039)
*Cheng Jin,Zhenyu Xiao,Chutao Liu,Yuantao Gu*

Main category: cs.LG

TL;DR: Classifier-free guidance (CFG) improves text-to-image synthesis but causes color distortions at high guidance weights. The proposed Angle Domain Guidance (ADG) mitigates these distortions while preserving alignment.


<details>
  <summary>Details</summary>
Motivation: Address color distortions in CFG under high guidance weights, which degrade image quality despite improved text-image alignment.

Method: Analyze norm amplification in latent space, propose ADG to constrain magnitude variations and optimize angular alignment.

Result: ADG outperforms existing methods, improving color fidelity and perceptual alignment while maintaining text-image alignment.

Conclusion: ADG effectively balances text-image alignment and color fidelity, enhancing overall image synthesis quality.

Abstract: Classifier-free guidance (CFG) has emerged as a pivotal advancement in
text-to-image latent diffusion models, establishing itself as a cornerstone
technique for achieving high-quality image synthesis. However, under high
guidance weights, where text-image alignment is significantly enhanced, CFG
also leads to pronounced color distortions in the generated images. We identify
that these distortions stem from the amplification of sample norms in the
latent space. We present a theoretical framework that elucidates the mechanisms
of norm amplification and anomalous diffusion phenomena induced by
classifier-free guidance. Leveraging our theoretical insights and the latent
space structure, we propose an Angle Domain Guidance (ADG) algorithm. ADG
constrains magnitude variations while optimizing angular alignment, thereby
mitigating color distortions while preserving the enhanced text-image alignment
achieved at higher guidance weights. Experimental results demonstrate that ADG
significantly outperforms existing methods, generating images that not only
maintain superior text alignment but also exhibit improved color fidelity and
better alignment with human perceptual preferences.

</details>


### [41] [Large Language models for Time Series Analysis: Techniques, Applications, and Challenges](https://arxiv.org/abs/2506.11040)
*Feifei Shi,Xueyan Yin,Kang Wang,Wanyu Tu,Qifu Sun,Huansheng Ning*

Main category: cs.LG

TL;DR: A review of pre-trained LLM-driven time series analysis, covering techniques, applications, and challenges, while outlining future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional time series analysis methods lack nonlinear feature representation and long-term dependency capture, which LLMs can address.

Method: Systematic review of LLM-driven time series analysis, including evolutionary roadmap, technical workflow, and real-world applications.

Result: Provides insights into current advances and identifies open challenges for future research.

Conclusion: Serves as a foundational reference for developing efficient, generalizable, and interpretable LLM-driven time series analysis systems.

Abstract: Time series analysis is pivotal in domains like financial forecasting and
biomedical monitoring, yet traditional methods are constrained by limited
nonlinear feature representation and long-term dependency capture. The
emergence of Large Language Models (LLMs) offers transformative potential by
leveraging their cross-modal knowledge integration and inherent attention
mechanisms for time series analysis. However, the development of
general-purpose LLMs for time series from scratch is still hindered by data
diversity, annotation scarcity, and computational requirements. This paper
presents a systematic review of pre-trained LLM-driven time series analysis,
focusing on enabling techniques, potential applications, and open challenges.
First, it establishes an evolutionary roadmap of AI-driven time series
analysis, from the early machine learning era, through the emerging LLM-driven
paradigm, to the development of native temporal foundation models. Second, it
organizes and systematizes the technical landscape of LLM-driven time series
analysis from a workflow perspective, covering LLMs' input, optimization, and
lightweight stages. Finally, it critically examines novel real-world
applications and highlights key open challenges that can guide future research
and innovation. The work not only provides valuable insights into current
advances but also outlines promising directions for future development. It
serves as a foundational reference for both academic and industrial
researchers, paving the way for the development of more efficient,
generalizable, and interpretable systems of LLM-driven time series analysis.

</details>


### [42] [ChemHGNN: A Hierarchical Hypergraph Neural Network for Reaction Virtual Screening and Discovery](https://arxiv.org/abs/2506.11041)
*Xiaobao Huang,Yihong Ma,Anjali Gurajapu,Jules Schleinitz,Zhichun Guo,Sarah E. Reisman,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: ChemHGNN, a hypergraph neural network, outperforms traditional GNNs in modeling multi-reactant reactions by using hyperedges and innovative sampling strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with multi-reactant interactions in chemistry, limiting reaction virtual screening and discovery.

Method: ChemHGNN uses hyperedges for multi-reactant modeling, introduces RCNS for negative sampling, and employs hierarchical embeddings (molecule, reaction, hypergraph levels).

Result: ChemHGNN outperforms HGNN and GNN baselines on the USPTO dataset, especially in large-scale settings, with interpretable and chemically plausible results.

Conclusion: HGNNs, like ChemHGNN, are superior to GNNs for reaction discovery, providing a chemically informed framework to accelerate research.

Abstract: Reaction virtual screening and discovery are fundamental challenges in
chemistry and materials science, where traditional graph neural networks (GNNs)
struggle to model multi-reactant interactions. In this work, we propose
ChemHGNN, a hypergraph neural network (HGNN) framework that effectively
captures high-order relationships in reaction networks. Unlike GNNs, which
require constructing complete graphs for multi-reactant reactions, ChemHGNN
naturally models multi-reactant reactions through hyperedges, enabling more
expressive reaction representations. To address key challenges, such as
combinatorial explosion, model collapse, and chemically invalid negative
samples, we introduce a reaction center-aware negative sampling strategy (RCNS)
and a hierarchical embedding approach combining molecule, reaction and
hypergraph level features. Experiments on the USPTO dataset demonstrate that
ChemHGNN significantly outperforms HGNN and GNN baselines, particularly in
large-scale settings, while maintaining interpretability and chemical
plausibility. Our work establishes HGNNs as a superior alternative to GNNs for
reaction virtual screening and discovery, offering a chemically informed
framework for accelerating reaction discovery.

</details>


### [43] [GenFT: A Generative Parameter-Efficient Fine-Tuning Method for Pretrained Foundation Models](https://arxiv.org/abs/2506.11042)
*Baoquan Zhang,Guangning Xu,Michael. K. Ng*

Main category: cs.LG

TL;DR: GenFT leverages pretrained weights to guide task-specific updates, improving efficiency and performance in PEFT.


<details>
  <summary>Details</summary>
Motivation: To avoid inefficient training from scratch by utilizing pretrained weights to guide task-specific updates.

Method: Applies row and column transformations to distill patterns from pretrained weights and decomposes updates into shared and specific components.

Result: Outperforms state-of-the-art PEFT methods on CV and NLP benchmarks.

Conclusion: GenFT offers an efficient and effective approach for model adaptation.

Abstract: Pretrained Foundation Models (PFMs) have transformed numerous applications by
enabling efficient adaptation to customized tasks. Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a resource-efficient alternative to full
fine-tuning, especially leveraging reparameterized weights $\Delta W$ to adapt
models for downstream tasks. However, a critical yet underexplored question
remains: can we utilize well-pretrained weights $W_0$ to guide the update of
task-specific $\Delta W$, avoiding inefficient training it from scratch? To end
this, we propose Generative Parameter-Efficient Fine-Tuning (GenFT), a novel
method that extracts structured, transferable information from $W_0$ for
efficient $\Delta W$ training. To extract row and column structure information,
GenFT applies row and column transformations to distill essential patterns from
$W_0$. A tailored policy further decomposes $\Delta W$ into layer-shared and
layer-specific components, balancing information reuse and individualized
flexibility. GenFT is simple yet effective, achieving superior performance
across CV and NLP tasks. Extensive experiments on VTAB-1K, FGVC, and GLUE
benchmarks demonstrate that GenFT outperforms state-of-the-art PEFT methods,
offering a new perspective for efficient model adaptation.

</details>


### [44] [Boost Post-Training Quantization via Null Space Optimization for Large Language Models](https://arxiv.org/abs/2506.11044)
*Jiaqi Zhao,Miao Zhang,Weili Guan,Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces null space projection (Q2N) to reduce quantization error in LLMs by constraining weight perturbations within the null space of input activations.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods for LLMs show diminishing returns, suggesting the need for innovative approaches to further compress models.

Method: Proposes Q2N, a plug-and-play null space projection module, with an efficient approximation method and a closed-form solution for practical inference.

Result: Experiments on LLMs (LLaMA3, DeepSeek, Qwen3) validate Q2N's effectiveness and the null space optimization perspective.

Conclusion: This work pioneers null space insights for quantization error reduction, encouraging future research in advanced quantization methods.

Abstract: Existing post-training quantization methods for large language models (LLMs)
offer remarkable success. However, the increasingly marginal performance gains
suggest that existing quantization strategies are insufficient to support the
development of more compressed models. To inspire new directions for future
research, this paper introduces the concept of null space into LLMs
quantization. We argue that the quantization error can be effectively
alleviated by constraining the post-quantization weight perturbation to lie
within the null space of input activations. To prove this idea, we propose a
plug-and-play null space projection module for existing milestone PTQ baselines
named Q2N. Specifically, we first design an efficient and accurate null space
projection approximation method tailored to the characteristics of LLMs.
Subsequently, we theoretically derive a closed-form solution for an equivalent
vector of the obtained projection matrix, which satisfies practical inference
condition while avoiding additional memory overhead. Extensive experiments are
conducted on various state-of-the-art LLMs (LLaMA3, DeepSeek, Qwen3) and
baselines, demonstrating the effectiveness of both our Q2N and the perspective
of null space optimization for LLMs quantization. We view this paper the first
step to further alleviate the quantization error based on the insights of null
space, hoping it inspiring future researchers to design more advanced
quantization methods. Codes are available at https://github.com/zjq0455/q2n.

</details>


### [45] [Procedural Environment Generation for Tool-Use Agents](https://arxiv.org/abs/2506.11045)
*Michael Sullivan,Mareike Hartmann,Alexander Koller*

Main category: cs.LG

TL;DR: RandomWorld is a pipeline for generating interactive, compositional tool-use data, improving LLM performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing tool-use training data is non-interactive and non-compositional, limiting online RL training.

Method: Procedural generation of interactive tools and compositional tool-use data via RandomWorld, followed by SFT and RL tuning.

Result: Improved performance on tool-use benchmarks, achieving SoTA on two NESTFUL metrics; performance scales with data volume.

Conclusion: RandomWorld enables scalable, synthetic data generation for tool-use training, promising further improvements.

Abstract: Although the power of LLM tool-use agents has ignited a flurry of recent
research in this area, the curation of tool-use training data remains an open
problem$-$especially for online RL training. Existing approaches to synthetic
tool-use data generation tend to be non-interactive, and/or non-compositional.
We introduce RandomWorld, a pipeline for the procedural generation of
interactive tools and compositional tool-use data. We show that models tuned
via SFT and RL on synthetic RandomWorld data improve on a range of tool-use
benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset.
Further experiments show that downstream performance scales with the amount of
RandomWorld-generated training data, opening up the possibility of further
improvement through the use of entirely synthetic data.

</details>


### [46] [The Effects of Data Augmentation on Confidence Estimation for LLMs](https://arxiv.org/abs/2506.11046)
*Rui Wang,Renyu Zhu,Minmin Lin,Runze Wu,Tangjie Lv,Changjie Fan,Haobo Wang*

Main category: cs.LG

TL;DR: The paper explores how different data augmentation methods improve confidence estimation in LLMs, finding that greater diversity enhances effectiveness and random combinations are promising.


<details>
  <summary>Details</summary>
Motivation: Confidence estimation is critical for LLMs, but current discussions on data augmentation are limited. The study aims to expand this understanding.

Method: Investigates various data augmentation techniques and their impact on confidence estimation, focusing on diversity and semantic preservation.

Result: Data augmentation improves performance and reduces overconfidence, with greater diversity being more effective. Random combinations of augmentations show promise.

Conclusion: Diverse data augmentation enhances confidence estimation, and random combinations are recommended for practical use.

Abstract: Confidence estimation is crucial for reflecting the reliability of large
language models (LLMs), particularly in the widely used closed-source models.
Utilizing data augmentation for confidence estimation is viable, but
discussions focus on specific augmentation techniques, limiting its potential.
We study the impact of different data augmentation methods on confidence
estimation. Our findings indicate that data augmentation strategies can achieve
better performance and mitigate the impact of overconfidence. We investigate
the influential factors related to this and discover that, while preserving
semantic information, greater data diversity enhances the effectiveness of
augmentation. Furthermore, the impact of different augmentation strategies
varies across different range of application. Considering parameter
transferability and usability, the random combination of augmentations is a
promising choice.

</details>


### [47] [Perception-Driven Bias Detection in Machine Learning via Crowdsourced Visual Judgment](https://arxiv.org/abs/2506.11047)
*Chirudeep Tupakula,Rittika Shamsuddin*

Main category: cs.LG

TL;DR: A perception-driven framework for bias detection in machine learning uses crowdsourced human judgment to identify disparities, validated by statistical tests and ML cross-evaluations.


<details>
  <summary>Details</summary>
Motivation: Traditional bias detection methods rely on sensitive labels or rigid fairness metrics, limiting real-world applicability. This work aims to provide a scalable, label-efficient alternative.

Method: A lightweight web platform collects binary judgments on group similarity from non-experts via stripped-down visualizations. User feedback is aggregated and validated statistically.

Result: Perceptual signals from users reliably correlate with known bias cases, indicating visual intuition can serve as a scalable fairness auditing tool.

Conclusion: The framework offers an interpretable, human-aligned alternative to conventional bias detection, enabling crowdsourced pipelines.

Abstract: Machine learning systems are increasingly deployed in high-stakes domains,
yet they remain vulnerable to bias systematic disparities that
disproportionately impact specific demographic groups. Traditional bias
detection methods often depend on access to sensitive labels or rely on rigid
fairness metrics, limiting their applicability in real-world settings. This
paper introduces a novel, perception-driven framework for bias detection that
leverages crowdsourced human judgment. Inspired by reCAPTCHA and other
crowd-powered systems, we present a lightweight web platform that displays
stripped-down visualizations of numeric data (for example-salary distributions
across demographic clusters) and collects binary judgments on group similarity.
We explore how users' visual perception-shaped by layout, spacing, and question
phrasing can signal potential disparities. User feedback is aggregated to flag
data segments as biased, which are then validated through statistical tests and
machine learning cross-evaluations. Our findings show that perceptual signals
from non-expert users reliably correlate with known bias cases, suggesting that
visual intuition can serve as a powerful, scalable proxy for fairness auditing.
This approach offers a label-efficient, interpretable alternative to
conventional fairness diagnostics, paving the way toward human-aligned,
crowdsourced bias detection pipelines.

</details>


### [48] [I Can't Believe It's Not Real: CV-MuSeNet: Complex-Valued Multi-Signal Segmentation](https://arxiv.org/abs/2506.11048)
*Sangwon Shin,Mehmet C. Vuran*

Main category: cs.LG

TL;DR: CMuSeNet, a complex-valued neural network, improves spectrum sensing accuracy and training efficiency in low-SNR environments over traditional real-valued networks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of real-valued neural networks in capturing wireless signal properties like phase and amplitude, especially in low-SNR scenarios.

Method: Uses complex-valued neural networks (CVNNs) with residual architecture, introduces complex-valued Fourier spectrum focal loss (CFL) and complex plane intersection over union (CIoU) metric.

Result: Achieves 98.98%-99.90% accuracy, 9.2% improvement over real-valued networks, and reduces training time by 92.2%.

Conclusion: Complex-valued architectures enhance weak signal detection and training efficiency for spectrum sensing in low-SNR environments.

Abstract: The increasing congestion of the radio frequency spectrum presents challenges
for efficient spectrum utilization. Cognitive radio systems enable dynamic
spectrum access with the aid of recent innovations in neural networks. However,
traditional real-valued neural networks (RVNNs) face difficulties in low
signal-to-noise ratio (SNR) environments, as they were not specifically
developed to capture essential wireless signal properties such as phase and
amplitude. This work presents CMuSeNet, a complex-valued multi-signal
segmentation network for wideband spectrum sensing, to address these
limitations. Extensive hyperparameter analysis shows that a naive conversion of
existing RVNNs into their complex-valued counterparts is ineffective. Built on
complex-valued neural networks (CVNNs) with a residual architecture, CMuSeNet
introduces a complexvalued Fourier spectrum focal loss (CFL) and a complex
plane intersection over union (CIoU) similarity metric to enhance training
performance. Extensive evaluations on synthetic, indoor overthe-air, and
real-world datasets show that CMuSeNet achieves an average accuracy of
98.98%-99.90%, improving by up to 9.2 percentage points over its real-valued
counterpart and consistently outperforms state of the art. Strikingly, CMuSeNet
achieves the accuracy level of its RVNN counterpart in just two epochs,
compared to the 27 epochs required for RVNN, while reducing training time by up
to a 92.2% over the state of the art. The results highlight the effectiveness
of complex-valued architectures in improving weak signal detection and training
efficiency for spectrum sensing in challenging low-SNR environments. The
dataset is available at: https://dx.doi.org/10.21227/hcc1-6p22

</details>


### [49] [15,500 Seconds: Lean UAV Classification Leveraging PEFT and Pre-Trained Networks](https://arxiv.org/abs/2506.11049)
*Andrew P. Berg,Qian Zhang,Mia Y. Wang*

Main category: cs.LG

TL;DR: The paper tackles data scarcity in UAV audio classification using fine-tuning, augmentation, and pre-trained networks, achieving 95% accuracy with EfficientNet-B0.


<details>
  <summary>Details</summary>
Motivation: Addressing security concerns from growing UAV usage by improving deep UAV audio classification despite limited data.

Method: Utilizes parameter-efficient fine-tuning, data augmentation, and pre-trained networks (EfficientNet-B0).

Result: Achieves over 95% validation accuracy.

Conclusion: The proposed methods effectively overcome data scarcity, enhancing UAV audio classification performance.

Abstract: Unmanned Aerial Vehicles (UAVs) pose an escalating security concerns as the
market for consumer and military UAVs grows. This paper address the critical
data scarcity challenges in deep UAV audio classification. We build upon our
previous work expanding novel approaches such as: parameter efficient
fine-tuning, data augmentation, and pre-trained networks. We achieve
performance upwards of 95\% validation accuracy with EfficientNet-B0.

</details>


### [50] [NSW-EPNews: A News-Augmented Benchmark for Electricity Price Forecasting with LLMs](https://arxiv.org/abs/2506.11050)
*Zhaoge Bi,Linghan Huang,Haolin Jin,Qingwen Zeng,Huaming Chen*

Main category: cs.LG

TL;DR: NSW-EPNews is a benchmark for evaluating time-series and LLMs in electricity price forecasting, combining numerical and textual data. It shows marginal gains for traditional models with news features and modest improvements for LLMs, but highlights LLM limitations like hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing electricity price forecasting methods ignore textual signals, limiting accuracy. NSW-EPNews addresses this gap by integrating multimodal data.

Method: The dataset includes half-hourly prices, temperature, and news summaries. Forecasting is framed as 48-step-ahead prediction using lagged prices, vectorized news, and weather features for classical models, and prompt-engineered contexts for LLMs.

Result: Traditional models see marginal gains from news features. LLMs like GPT-4o and Gemini 1.5 Pro show modest improvements but suffer from hallucinations.

Conclusion: NSW-EPNews reveals a gap between LLM capabilities and high-stakes forecasting needs, emphasizing the need for better grounded numerical reasoning.

Abstract: Electricity price forecasting is a critical component of modern
energy-management systems, yet existing approaches heavily rely on numerical
histories and ignore contemporaneous textual signals. We introduce NSW-EPNews,
the first benchmark that jointly evaluates time-series models and large
language models (LLMs) on real-world electricity-price prediction. The dataset
includes over 175,000 half-hourly spot prices from New South Wales, Australia
(2015-2024), daily temperature readings, and curated market-news summaries from
WattClarity. We frame the task as 48-step-ahead forecasting, using multimodal
input, including lagged prices, vectorized news and weather features for
classical models, and prompt-engineered structured contexts for LLMs. Our
datasets yields 3.6k multimodal prompt-output pairs for LLM evaluation using
specific templates. Through compresive benchmark design, we identify that for
traditional statistical and machine learning models, the benefits gain is
marginal from news feature. For state-of-the-art LLMs, such as GPT-4o and
Gemini 1.5 Pro, we observe modest performance increase while it also produce
frequent hallucinations such as fabricated and malformed price sequences.
NSW-EPNews provides a rigorous testbed for evaluating grounded numerical
reasoning in multimodal settings, and highlights a critical gap between current
LLM capabilities and the demands of high-stakes energy forecasting.

</details>


### [51] [ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial Optimization with Routing and Dynamic attention](https://arxiv.org/abs/2506.11052)
*Henrik Abgaryan,Tristan Cazenave,Ararat Harutyunyan*

Main category: cs.LG

TL;DR: ACCORD is a framework using LLMs for NP-hard combinatorial problems, outperforming standard methods and larger models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance LLMs' reasoning abilities for NP-hard combinatorial optimization tasks.

Method: Introduces ACCORD with autoregressive constraint-satisfying generation, dynamic attention, and problem-specific LoRA modules, tested on the ACCORD-90k dataset.

Result: ACCORD consistently outperforms standard methods and larger LLMs, improving solution feasibility.

Conclusion: ACCORD is the first large-scale, end-to-end framework for applying LLMs to diverse combinatorial optimization problems.

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, yet their direct application to NP-hard combinatorial problems
(CPs) remains underexplored. In this work, we systematically investigate the
reasoning abilities of LLMs on a variety of NP-hard combinatorial optimization
tasks and introduce ACCORD: Autoregressive Constraint-satisfying generation for
COmbinatorial optimization with Routing and Dynamic attention. ACCORD features
a novel dataset representation and model architecture that leverage the
autoregressive nature of LLMs to dynamically enforce feasibility constraints,
coupled with attention-based routing to activate problem-specific LoRA modules.
We also present the ACCORD-90k supervised dataset, covering six NP-hard
combinatorial problems: TSP, VRP, Knapsack, FlowShop, JSSP, and BinPacking.
Extensive experiments demonstrate that our ACCORD model, built on an
8B-parameter Llama backbone, consistently outperforms standard prompting and
input-output methods, even when compared to much larger LLMs, such as gpt-4.
Ablation studies further show that our output structure enhances solution
feasibility. To the best of our knowledge, this is the first large-scale,
end-to-end framework for exploring the applications of LLMs to a broad spectrum
of combinatorial optimization problems. The codes are publicly available at
https://github.com/starjob42/ACCORD

</details>


### [52] [Bootstrapping your behavior: a new pretraining strategy for user behavior sequence data](https://arxiv.org/abs/2506.11053)
*Weichang Wu,Xiaolu Zhang,Jun Zhou,Yuchen Li,Wenwen Xia*

Main category: cs.LG

TL;DR: The paper introduces BYOB, a novel UBS pretraining strategy that automates behavior vocabulary selection, improving model performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Manual behavior vocabulary selection in UBS pretraining is labor-intensive, biased, and limits generalization.

Method: BYOB predicts an automatically constructed supervision embedding using a student-teacher encoder scheme.

Result: Experiments show 3.9% AUC improvement, 98.9% training throughput gain, and meaningful pretraining patterns. Online deployment improved KS by 2.7%-7.1%.

Conclusion: BYOB eliminates manual vocabulary selection, enhances performance, and reduces financial risk in industrial applications.

Abstract: User Behavior Sequence (UBS) modeling is crucial in industrial applications.
As data scale and task diversity grow, UBS pretraining methods have become
increasingly pivotal. State-of-the-art UBS pretraining methods rely on
predicting behavior distributions. The key step in these methods is
constructing a selected behavior vocabulary. However, this manual step is
labor-intensive and prone to bias. The limitation of vocabulary capacity also
directly affects models' generalization ability. In this paper, we introduce
Bootstrapping Your Behavior (\model{}), a novel UBS pretraining strategy that
predicts an automatically constructed supervision embedding summarizing all
behaviors' information within a future time window, eliminating the manual
behavior vocabulary selection. In implementation, we incorporate a
student-teacher encoder scheme to construct the pretraining supervision
effectively. Experiments on two real-world industrial datasets and eight
downstream tasks demonstrate that \model{} achieves an average improvement of
3.9\% in AUC and 98.9\% in training throughput. Notably, the model exhibits
meaningful attention patterns and cluster representations during pretraining
without any label supervision. In our online deployment over two months, the
pretrained model improves the KS by about 2.7\% and 7.1\% over the baseline
model for two financial overdue risk prediction tasks in the Alipay mobile
application, which reduces bad debt risk by millions of dollars for Ant group.

</details>


### [53] [Adaptive Composition of Machine Learning as a Service (MLaaS) for IoT Environments](https://arxiv.org/abs/2506.11054)
*Deepak Kanneganti,Sajib Mistry,Sheik Mohammad Mostakim Fattah,Aneesh Krishna,Monowar Bhuyan*

Main category: cs.LG

TL;DR: An adaptive MLaaS composition framework is proposed to handle IoT environment challenges like concept drift and data heterogeneity, ensuring efficient and scalable ML services.


<details>
  <summary>Details</summary>
Motivation: The dynamic and uncertain nature of IoT environments causes issues like concept drift and evolving system requirements, challenging the long-term effectiveness of MLaaS compositions.

Method: The framework includes a service assessment model, a candidate selection model, and an adaptive composition mechanism using contextual multi-armed bandit optimization.

Result: The approach maintains QoS while reducing computational costs, as demonstrated by experiments on a real-world dataset.

Conclusion: The proposed framework effectively adapts to IoT constraints, ensuring seamless and scalable MLaaS compositions.

Abstract: The dynamic nature of Internet of Things (IoT) environments challenges the
long-term effectiveness of Machine Learning as a Service (MLaaS) compositions.
The uncertainty and variability of IoT environments lead to fluctuations in
data distribution, e.g., concept drift and data heterogeneity, and evolving
system requirements, e.g., scalability demands and resource limitations. This
paper proposes an adaptive MLaaS composition framework to ensure a seamless,
efficient, and scalable MLaaS composition. The framework integrates a service
assessment model to identify underperforming MLaaS services and a candidate
selection model to filter optimal replacements. An adaptive composition
mechanism is developed that incrementally updates MLaaS compositions using a
contextual multi-armed bandit optimization strategy. By continuously adapting
to evolving IoT constraints, the approach maintains Quality of Service (QoS)
while reducing the computational cost associated with recomposition from
scratch. Experimental results on a real-world dataset demonstrate the
efficiency of our proposed approach.

</details>


### [54] [PolyMicros: Bootstrapping a Foundation Model for Polycrystalline Material Structure](https://arxiv.org/abs/2506.11055)
*Michael Buzzy,Andreas Robertson,Peng Chen,Surya Kalidindi*

Main category: cs.LG

TL;DR: A novel machine learning approach for hyper-sparse data in materials science, using physics-driven augmentation and local generative models, achieves breakthroughs in polycrystalline materials.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large datasets for many structural and functional materials by enabling learning from very few examples.

Method: Physics-driven data augmentation with local generative models and diversity curation to create large datasets from minimal observations.

Result: Development of PolyMicros, a Foundation Model for polycrystalline materials, solving 3D microscopy challenges zero-shot.

Conclusion: The approach and open-sourced models/datasets advance materials science by enabling discovery with limited data.

Abstract: Recent advances in Foundation Models for Materials Science are poised to
revolutionize the discovery, manufacture, and design of novel materials with
tailored properties and responses. Although great strides have been made,
successes have been restricted to materials classes where multi-million sample
data repositories can be readily curated (e.g., atomistic structures).
Unfortunately, for many structural and functional materials (e.g., mesoscale
structured metal alloys), such datasets are too costly or prohibitive to
construct; instead, datasets are limited to very few examples. To address this
challenge, we introduce a novel machine learning approach for learning from
hyper-sparse, complex spatial data in scientific domains. Our core contribution
is a physics-driven data augmentation scheme that leverages an ensemble of
local generative models, trained on as few as five experimental observations,
and coordinates them through a novel diversity curation strategy to generate a
large-scale, physically diverse dataset. We utilize this framework to construct
PolyMicros, the first Foundation Model for polycrystalline materials (a
structural material class important across a broad range of industrial and
scientific applications). We demonstrate the utility of PolyMicros by zero-shot
solving several long standing challenges related to accelerating 3D
experimental microscopy. Finally, we make both our models and datasets openly
available to the community.

</details>


### [55] [xInv: Explainable Optimization of Inverse Problems](https://arxiv.org/abs/2506.11056)
*Sean Memery,Kevin Denamganai,Anna Kapron-King,Kartic Subr*

Main category: cs.LG

TL;DR: The paper proposes a method to make iterative optimization in inverse problems more interpretable by generating human-readable explanations from optimizer traces using natural language events and a Language Model.


<details>
  <summary>Details</summary>
Motivation: Inverse problems are critical in fields like healthcare and climate science, but their iterative optimization processes remain opaque to domain experts. The goal is to enhance interpretability.

Method: The approach involves instrumenting a differentiable simulator to emit natural language events during forward and backward passes, then using a Language Model to generate explanations from these events.

Result: The method is demonstrated to be effective through an illustrative optimization problem and a neural network training example.

Conclusion: The proposed methodology successfully bridges the gap between cryptic optimization processes and human-interpretable explanations in inverse problems.

Abstract: Inverse problems are central to a wide range of fields, including healthcare,
climate science, and agriculture. They involve the estimation of inputs,
typically via iterative optimization, to some known forward model so that it
produces a desired outcome. Despite considerable development in the
explainability and interpretability of forward models, the iterative
optimization of inverse problems remains largely cryptic to domain experts. We
propose a methodology to produce explanations, from traces produced by an
optimizer, that are interpretable by humans at the abstraction of the domain.
The central idea in our approach is to instrument a differentiable simulator so
that it emits natural language events during its forward and backward passes.
In a post-process, we use a Language Model to create an explanation from the
list of events. We demonstrate the effectiveness of our approach with an
illustrative optimization problem and an example involving the training of a
neural network.

</details>


### [56] [STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization](https://arxiv.org/abs/2506.11057)
*Xijun Li,Jiexiang Yang,Jinghao Wang,Bo Peng,Jianguo Yao,Haibing Guan*

Main category: cs.LG

TL;DR: STRCMP is a structure-aware LLM framework for combinatorial optimization, integrating GNNs and LLMs to improve solution quality and efficiency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM approaches for CO problems often ignore structural priors, leading to suboptimal solutions. STRCMP aims to leverage these priors, inspired by human expertise.

Method: Combines GNNs for structural embeddings with LLMs to generate solver-specific codes, ensuring correctness and topology preservation, followed by evolutionary refinement.

Result: Outperforms five neural and LLM-based methods in solution optimality and efficiency across nine benchmark datasets.

Conclusion: STRCMP effectively integrates structural priors into LLM-based CO solving, demonstrating superior performance and potential for practical applications.

Abstract: Combinatorial optimization (CO) problems, central to operation research and
theoretical computer science, present significant computational challenges due
to their NP-hard nature. While large language models (LLMs) have emerged as
promising tools for CO--either by directly generating solutions or synthesizing
solver-specific codes--existing approaches often neglect critical structural
priors inherent to CO problems, leading to suboptimality and iterative
inefficiency. Inspired by human experts' success in leveraging CO structures
for algorithm design, we propose STRCMP, a novel structure-aware LLM-based
algorithm discovery framework that systematically integrates structure priors
to enhance solution quality and solving efficiency. Our framework combines a
graph neural network (GNN) for extracting structural embeddings from CO
instances with an LLM conditioned on these embeddings to identify
high-performing algorithms in the form of solver-specific codes. This composite
architecture ensures syntactic correctness, preserves problem topology, and
aligns with natural language objectives, while an evolutionary refinement
process iteratively optimizes generated algorithm. Extensive evaluations across
Mixed Integer Linear Programming and Boolean Satisfiability problems, using
nine benchmark datasets, demonstrate that our proposed STRCMP outperforms five
strong neural and LLM-based methods by a large margin, in terms of both
solution optimality and computational efficiency. The code and learned model
will be publicly available upon the acceptance of the paper.

</details>


### [57] [ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models](https://arxiv.org/abs/2506.11087)
*Boya Xiong,Shuo Wang,Weifeng Ge,Guanhua Chen,Yun Chen*

Main category: cs.LG

TL;DR: ADAMIX is an adaptive mixed-precision delta-compression framework for LLMs, outperforming baselines by minimizing quantization error with optimal bit allocation.


<details>
  <summary>Details</summary>
Motivation: Existing delta-compression methods for LLMs either perform poorly at high compression ratios or rely on empirical bit allocation, necessitating a more effective solution.

Method: ADAMIX uses a mathematical derivation of quantization error to formulate an optimal mixed-precision bit allocation scheme as a 0/1 integer linear programming problem.

Result: ADAMIX outperforms Delta-CoMe by 22.3% and 6.1% on AIME2024 and GQA tasks, respectively, with 7B models.

Conclusion: ADAMIX provides a superior delta-compression framework for LLMs, achieving high performance and efficiency.

Abstract: Large language models (LLMs) achieve impressive performance on various
knowledge-intensive and complex reasoning tasks in different domains. In
certain scenarios like multi-tenant serving, a large number of LLMs finetuned
from the same base model are deployed to meet complex requirements for users.
Recent works explore delta-compression approaches to quantize and compress the
delta parameters between the customized LLM and the corresponding base model.
However, existing works either exhibit unsatisfactory performance at high
compression ratios or depend on empirical bit allocation schemes. In this work,
we propose ADAMIX, an effective adaptive mixed-precision delta-compression
framework. We provide a mathematical derivation of quantization error to
motivate our mixed-precision compression strategy and formulate the optimal
mixed-precision bit allocation scheme as the solution to a 0/1 integer linear
programming problem. Our derived bit allocation strategy minimizes the
quantization error while adhering to a predefined compression ratio
requirement. Experimental results on various models and benchmarks demonstrate
that our approach surpasses the best baseline by a considerable margin. On
tasks like AIME2024 and GQA, where the norm of $\Delta \mathbf{W}$ is large and
the base model lacks sufficient ability, ADAMIX outperforms the best baseline
Delta-CoMe by 22.3% and 6.1% with 7B models, respectively.

</details>


### [58] [Debiasing Online Preference Learning via Preference Feature Preservation](https://arxiv.org/abs/2506.11098)
*Dongyoung Kim,Jinsung Yoon,Jinwoo Shin,Jaehyung Kim*

Main category: cs.LG

TL;DR: PFP (Preference Feature Preservation) is a new framework to mitigate bias in LLM responses by preserving human preference feature distributions during online learning.


<details>
  <summary>Details</summary>
Motivation: Existing preference learning methods simplify human preferences, leading to biased LLM responses. PFP aims to address this by maintaining rich preference signals.

Method: PFP extracts preference features from offline data, trains a classifier, maps features for new inputs, and integrates them into LLM training via system prompts.

Result: PFP reduces bias and outperforms previous methods on standard benchmarks for LLM alignment.

Conclusion: PFP effectively preserves preference features, mitigating bias and improving LLM performance in preference learning.

Abstract: Recent preference learning frameworks for large language models (LLMs)
simplify human preferences with binary pairwise comparisons and scalar rewards.
This simplification could make LLMs' responses biased to mostly preferred
features, and would be exacerbated during the iterations of online preference
learning steps. To address these challenges, we propose a novel framework
coined PFP (Preference Feature Preservation). The key idea of PFP is
maintaining the distribution of human preference features and utilizing such
rich signals throughout the online preference learning process. Specifically,
PFP first extract preference features from offline pairwise human preference
data and trains a feature classifier. Then, using trained classifier and the
distribution preserving optimization, PFP maps appropriate preference features
for a new input instruction during online learning. Lastly, PFP trains LLM
using the existing preference learning method, by incorporating the preference
feature into system prompts and enabling LLM to explicitly handle various human
preferences. Our experiments demonstrate that PFP successfully mitigates the
bias in preference features during online learning, and hence achieves superior
performance compared to previous preference learning methods on standard
benchmarks to evaluate LLM alignment.

</details>


### [59] [Knowledge Graph Embeddings with Representing Relations as Annular Sectors](https://arxiv.org/abs/2506.11099)
*Huiling Zhu,Yingqi Zeng*

Main category: cs.LG

TL;DR: SectorE is a novel embedding model using polar coordinates to capture semantic hierarchies in knowledge graphs, outperforming existing models on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing region-based embedding models for knowledge graph completion often ignore semantic hierarchies in entities, limiting their effectiveness.

Method: SectorE embeds entities as points and relations as annular sectors in polar coordinates, combining modulus and phase to model inference patterns and relation attributes.

Result: SectorE achieves competitive performance on FB15k-237, WN18RR, and YAGO3-10, excelling in semantic modeling.

Conclusion: SectorE effectively addresses the semantic hierarchy gap in knowledge graph completion, demonstrating superior performance.

Abstract: Knowledge graphs (KGs), structured as multi-relational data of entities and
relations, are vital for tasks like data analysis and recommendation systems.
Knowledge graph completion (KGC), or link prediction, addresses incompleteness
of KGs by inferring missing triples (h, r, t). It is vital for downstream
applications. Region-based embedding models usually embed entities as points
and relations as geometric regions to accomplish the task. Despite progress,
these models often overlook semantic hierarchies inherent in entities. To solve
this problem, we propose SectorE, a novel embedding model in polar coordinates.
Relations are modeled as annular sectors, combining modulus and phase to
capture inference patterns and relation attributes. Entities are embedded as
points within these sectors, intuitively encoding hierarchical structure.
Evaluated on FB15k-237, WN18RR, and YAGO3-10, SectorE achieves competitive
performance against various kinds of models, demonstrating strengths in
semantic modeling capability.

</details>


### [60] [An Active Learning-Based Streaming Pipeline for Reduced Data Training of Structure Finding Models in Neutron Diffractometry](https://arxiv.org/abs/2506.11100)
*Tianle Wang,Jorge Ramirez,Cristina Garcia-Cardona,Thomas Proffen,Shantenu Jha,Sudip K. Seal*

Main category: cs.LG

TL;DR: A novel batch-mode active learning policy reduces training data needs by 75% and improves accuracy for neutron diffractometry structure determination.


<details>
  <summary>Details</summary>
Motivation: Neutron diffractometry structure determination is computationally expensive, and machine learning models can speed it up, but training data requirements grow exponentially with structural parameters.

Method: Introduces a batch-mode active learning policy using uncertainty sampling to simulate training data from a probability distribution favoring uncertain examples.

Result: The method reduces training data by 75% while improving accuracy and achieves 20% shorter training time without accuracy loss in a streaming workflow.

Conclusion: The proposed active learning policy and streaming workflow significantly enhance efficiency and accuracy in neutron diffractometry structure determination.

Abstract: Structure determination workloads in neutron diffractometry are
computationally expensive and routinely require several hours to many days to
determine the structure of a material from its neutron diffraction patterns.
The potential for machine learning models trained on simulated neutron
scattering patterns to significantly speed up these tasks have been reported
recently. However, the amount of simulated data needed to train these models
grows exponentially with the number of structural parameters to be predicted
and poses a significant computational challenge. To overcome this challenge, we
introduce a novel batch-mode active learning (AL) policy that uses uncertainty
sampling to simulate training data drawn from a probability distribution that
prefers labelled examples about which the model is least certain. We confirm
its efficacy in training the same models with about 75% less training data
while improving the accuracy. We then discuss the design of an efficient
stream-based training workflow that uses this AL policy and present a
performance study on two heterogeneous platforms to demonstrate that, compared
with a conventional training workflow, the streaming workflow delivers about
20% shorter training time without any loss of accuracy.

</details>


### [61] [PromptTSS: A Prompting-Based Approach for Interactive Multi-Granularity Time Series Segmentation](https://arxiv.org/abs/2506.11170)
*Ching Chang,Ming-Chih Lo,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.LG

TL;DR: PromptTSS is a novel framework for multi-granularity time series segmentation, addressing challenges of unified modeling and adaptability, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-granularity states and dynamic environments, limiting their effectiveness in tasks like predictive maintenance.

Method: PromptTSS uses a unified model with a prompting mechanism, leveraging label and boundary information to guide segmentation for coarse- and fine-grained patterns.

Result: PromptTSS improves accuracy by 24.49% in multi-granularity segmentation, 17.88% in single-granularity, and up to 599.24% in transfer learning.

Conclusion: PromptTSS effectively addresses multi-granularity segmentation and adaptability, demonstrating superior performance in hierarchical and dynamic time series analysis.

Abstract: Multivariate time series data, collected across various fields such as
manufacturing and wearable technology, exhibit states at multiple levels of
granularity, from coarse-grained system behaviors to fine-grained, detailed
events. Effectively segmenting and integrating states across these different
granularities is crucial for tasks like predictive maintenance and performance
optimization. However, existing time series segmentation methods face two key
challenges: (1) the inability to handle multiple levels of granularity within a
unified model, and (2) limited adaptability to new, evolving patterns in
dynamic environments. To address these challenges, we propose PromptTSS, a
novel framework for time series segmentation with multi-granularity states.
PromptTSS uses a unified model with a prompting mechanism that leverages label
and boundary information to guide segmentation, capturing both coarse- and
fine-grained patterns while adapting dynamically to unseen patterns.
Experiments show PromptTSS improves accuracy by 24.49% in multi-granularity
segmentation, 17.88% in single-granularity segmentation, and up to 599.24% in
transfer learning, demonstrating its adaptability to hierarchical states and
evolving time series dynamics.

</details>


### [62] [Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning](https://arxiv.org/abs/2506.11172)
*Xue Zhou,Dapeng Man,Chen Xu,Fanyi Zeng,Tao Liu,Huan Wang,Shucheng He,Chaoyang Gao,Wu Yang*

Main category: cs.LG

TL;DR: The paper introduces a sequence-level concentrability coefficient to quantify data-policy coverage in offline RL, proposes a poisoning attack (CSDPC) targeting rare decision patterns, and shows its severe impact (90% performance drop with 1% poisoning).


<details>
  <summary>Details</summary>
Motivation: Existing offline RL studies overlook security risks from insufficient data-policy coverage and lack multi-step analysis.

Method: Proposes the CSDPC poisoning attack by identifying and poisoning rare decision patterns to reduce coverage.

Result: Poisoning 1% of the dataset degrades agent performance by 90%.

Conclusion: The findings highlight the need for analyzing and securing offline RL against coverage-based attacks.

Abstract: Offline reinforcement learning (RL) heavily relies on the coverage of
pre-collected data over the target policy's distribution. Existing studies aim
to improve data-policy coverage to mitigate distributional shifts, but overlook
security risks from insufficient coverage, and the single-step analysis is not
consistent with the multi-step decision-making nature of offline RL. To address
this, we introduce the sequence-level concentrability coefficient to quantify
coverage, and reveal its exponential amplification on the upper bound of
estimation errors through theoretical analysis. Building on this, we propose
the Collapsing Sequence-Level Data-Policy Coverage (CSDPC) poisoning attack.
Considering the continuous nature of offline RL data, we convert state-action
pairs into decision units, and extract representative decision patterns that
capture multi-step behavior. We identify rare patterns likely to cause
insufficient coverage, and poison them to reduce coverage and exacerbate
distributional shifts. Experiments show that poisoning just 1% of the dataset
can degrade agent performance by 90%. This finding provides new perspectives
for analyzing and safeguarding the security of offline RL.

</details>


### [63] [Detection of obstructions in oil and gas pipelines: machine learning techniques for hydrate classification](https://arxiv.org/abs/2506.11220)
*Hellockston Gomes de Brito,Carla Wilza Souza de Paula Maitelli,Osvaldo Chiavone-Filho*

Main category: cs.LG

TL;DR: The paper uses supervised machine learning (decision trees, k-NN, Naive Bayes) to detect and mitigate flow assurance challenges in oil and gas production, focusing on hydrate formation. Decision trees achieved 99.99% accuracy.


<details>
  <summary>Details</summary>
Motivation: Oil and gas production faces challenges like blockages and hydrate formation, impacting efficiency. This study aims to address these using ML.

Method: Employed decision trees, k-NN, and Naive Bayes on a Petrobras dataset, using scikit-learn for preprocessing and classification.

Result: Decision trees showed the highest accuracy (99.99%) in classifying hydrate formation.

Conclusion: The ML approach effectively optimizes production efficiency by mitigating flow assurance challenges.

Abstract: Oil and gas reserves are vital resources for the global economy, serving as
key components in transportation, energy production, and industrial processes.
However, oil and gas extraction and production operations may encounter several
challenges, such as pipeline and production line blockages, caused by factors
including sediment accumulation, wax deposition, mineral scaling, and
corrosion. This study addresses these challenges by employing supervised
machine learning techniques, specifically decision trees, the k-Nearest
Neighbors (k-NN) algorithm (k-NN), and the Naive Bayes classifier method, to
detect and mitigate flow assurance challenges, ensuring efficient fluid
transport. The primary focus is on preventing gas hydrate formation in oil
production systems. To achieve this, data preprocessing and cleaning were
conducted to ensure the quality and consistency of the dataset, which was
sourced from Petrobras publicly available 3W project repository on GitHub. The
scikit-learn Python library, a widely recognized open-source tool for
supervised machine learning techniques, was utilized for classification tasks
due to its robustness and versatility. The results demonstrate that the
proposed methodology effectively classifies hydrate formation under operational
conditions, with the decision tree algorithm exhibiting the highest predictive
accuracy (99.99 percent). Consequently, this approach provides a reliable
solution for optimizing production efficiency.

</details>


### [64] [uPVC-Net: A Universal Premature Ventricular Contraction Detection Deep Learning Algorithm](https://arxiv.org/abs/2506.11238)
*Hagai Hamami,Yosef Solewicz,Daniel Zur,Yonatan Kleerekoper,Joachim A. Behar*

Main category: cs.LG

TL;DR: uPVC-Net, a universal deep learning model, detects PVCs from single-lead ECGs with high accuracy (AUC 97.8%-99.1%), demonstrating strong generalization across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate PVC detection is challenging due to ECG waveform variability from lead placement, recording conditions, and demographics.

Method: Developed uPVC-Net using a custom architecture and multi-source, multi-lead training on 8.3M beats from Holter monitors and wearable ECG patches. Evaluated OOD generalization by holding out one dataset per experiment.

Result: Achieved AUCs of 97.8%-99.1%, with 99.1% on wearable single-lead ECG data.

Conclusion: uPVC-Net generalizes well across diverse configurations, showing promise for clinical use.

Abstract: Introduction: Premature Ventricular Contractions (PVCs) are common cardiac
arrhythmias originating from the ventricles. Accurate detection remains
challenging due to variability in electrocardiogram (ECG) waveforms caused by
differences in lead placement, recording conditions, and population
demographics. Methods: We developed uPVC-Net, a universal deep learning model
to detect PVCs from any single-lead ECG recordings. The model is developed on
four independent ECG datasets comprising a total of 8.3 million beats collected
from Holter monitors and a modern wearable ECG patch. uPVC-Net employs a custom
architecture and a multi-source, multi-lead training strategy. For each
experiment, one dataset is held out to evaluate out-of-distribution (OOD)
generalization. Results: uPVC-Net achieved an AUC between 97.8% and 99.1% on
the held-out datasets. Notably, performance on wearable single-lead ECG data
reached an AUC of 99.1%. Conclusion: uPVC-Net exhibits strong generalization
across diverse lead configurations and populations, highlighting its potential
for robust, real-world clinical deployment.

</details>


### [65] [A Causal Lens for Learning Long-term Fair Policies](https://arxiv.org/abs/2506.11242)
*Jacob Lear,Lu Zhang*

Main category: cs.LG

TL;DR: The paper proposes a framework for long-term fairness in reinforcement learning, decomposing fairness metrics into direct, delayed, and spurious effects, and introduces a method to balance fairness notions.


<details>
  <summary>Details</summary>
Motivation: To address long-term fairness in dynamic decision-making systems, beyond immediate bias, while considering instantaneous fairness requirements.

Method: A general reinforcement learning framework measuring long-term fairness via qualification gain differences, decomposed into direct, delayed, and spurious effects using causal analysis.

Result: Identifies intrinsic connections between decomposed fairness components and benefit fairness, proposing a method to balance fairness notions.

Conclusion: The framework effectively addresses long-term fairness in dynamic systems, offering a practical approach to balance fairness requirements.

Abstract: Fairness-aware learning studies the development of algorithms that avoid
discriminatory decision outcomes despite biased training data. While most
studies have concentrated on immediate bias in static contexts, this paper
highlights the importance of investigating long-term fairness in dynamic
decision-making systems while simultaneously considering instantaneous fairness
requirements. In the context of reinforcement learning, we propose a general
framework where long-term fairness is measured by the difference in the average
expected qualification gain that individuals from different groups could
obtain.Then, through a causal lens, we decompose this metric into three
components that represent the direct impact, the delayed impact, as well as the
spurious effect the policy has on the qualification gain. We analyze the
intrinsic connection between these components and an emerging fairness notion
called benefit fairness that aims to control the equity of outcomes in
decision-making. Finally, we develop a simple yet effective approach for
balancing various fairness notions.

</details>


### [66] [Can Time-Series Foundation Models Perform Building Energy Management Tasks?](https://arxiv.org/abs/2506.11250)
*Ozan Baris Mulayim,Pengrui Quan,Liying Han,Xiaomin Ouyang,Dezhi Hong,Mario Bergés,Mani Srivastava*

Main category: cs.LG

TL;DR: TSFMs show limited generalizability in BEM tasks, performing slightly better than statistical models in some areas but lagging in others, highlighting the need for improvements in handling covariates and context.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of Time-Series Foundation Models (TSFMs) in addressing scalability challenges in Building Energy Management (BEM) by evaluating their generalizability across tasks and contexts.

Method: Evaluated TSFMs across four dimensions: zero-shot univariate forecasting, forecasting with covariates, zero-shot representation learning for classification, and robustness to metrics and operational conditions.

Result: TSFMs showed limited generalizability, marginally outperforming statistical models in some tasks but underperforming in others, especially with covariates and complex environments.

Conclusion: Targeted advancements in TSFM design, particularly in handling covariates and temporal dynamics, are needed for more adaptable and scalable BEM solutions.

Abstract: Building energy management (BEM) tasks require processing and learning from a
variety of time-series data. Existing solutions rely on bespoke task- and
data-specific models to perform these tasks, limiting their broader
applicability. Inspired by the transformative success of Large Language Models
(LLMs), Time-Series Foundation Models (TSFMs), trained on diverse datasets,
have the potential to change this. Were TSFMs to achieve a level of
generalizability across tasks and contexts akin to LLMs, they could
fundamentally address the scalability challenges pervasive in BEM. To
understand where they stand today, we evaluate TSFMs across four dimensions:
(1) generalizability in zero-shot univariate forecasting, (2) forecasting with
covariates for thermal behavior modeling, (3) zero-shot representation learning
for classification tasks, and (4) robustness to performance metrics and varying
operational conditions. Our results reveal that TSFMs exhibit \emph{limited}
generalizability, performing only marginally better than statistical models on
unseen datasets and modalities for univariate forecasting. Similarly, inclusion
of covariates in TSFMs does not yield performance improvements, and their
performance remains inferior to conventional models that utilize covariates.
While TSFMs generate effective zero-shot representations for downstream
classification tasks, they may remain inferior to statistical models in
forecasting when statistical models perform test-time fitting. Moreover, TSFMs
forecasting performance is sensitive to evaluation metrics, and they struggle
in more complex building environments compared to statistical models. These
findings underscore the need for targeted advancements in TSFM design,
particularly their handling of covariates and incorporating context and
temporal dynamics into prediction mechanisms, to develop more adaptable and
scalable solutions for BEM.

</details>


### [67] [Domain-Constrained Diffusion Models to Synthesize Tabular Data: A Case Study in Power Systems](https://arxiv.org/abs/2506.11281)
*Milad Hoseinpour,Vladimir Dvorkin*

Main category: cs.LG

TL;DR: A guided diffusion model is proposed to synthesize domain-specific synthetic data, addressing privacy and legal concerns, with successful application in power systems.


<details>
  <summary>Details</summary>
Motivation: Growing concerns over privacy, security, and legal barriers in domains like healthcare, finance, and energy drive the need for synthetic data.

Method: A guided diffusion model integrates domain constraints (e.g., Kirchhoff laws) via gradient-based guidance to generate statistically representative and high-fidelity power flow datasets.

Result: Numerical results confirm the model's effectiveness in synthesizing feasible and accurate synthetic data.

Conclusion: The approach shows promise for generating synthetic data in power systems and other tabular data domains.

Abstract: Growing concerns over privacy, security, and legal barriers are driving the
rising demand for synthetic data across domains such as healthcare, finance,
and energy. While generative models offer a promising solution to overcome
these barriers, their utility depends on the incorporation of domain-specific
knowledge. We propose to synthesize data using a guided diffusion model that
integrates domain constraints directly into the generative process. We develop
the model in the context of power systems, with potential applicability to
other domains that involve tabular data. Specifically, we synthesize
statistically representative and high-fidelity power flow datasets. To satisfy
domain constraints, e.g., Kirchhoff laws, we introduce a gradient-based
guidance to steer the sampling trajectory in a feasible direction. Numerical
results demonstrate the effectiveness of our approach.

</details>


### [68] [Sampling Imbalanced Data with Multi-objective Bilevel Optimization](https://arxiv.org/abs/2506.11315)
*Karen Medlin,Sven Leyffer,Krishnan Raghavan*

Main category: cs.LG

TL;DR: MOODS introduces a multi-objective optimization framework for imbalanced data sampling and a new metric to evaluate sampling methods, improving F1 scores by 1-15%.


<details>
  <summary>Details</summary>
Motivation: Imbalanced datasets in two-class classification lead to poor minority class performance. Traditional methods like reweighting or resampling fail due to lack of diversity consideration.

Method: MOODS uses multi-objective bilevel optimization for synthetic oversampling and majority undersampling, alongside a new metric (`ε/δ`) to quantify sampling quality.

Result: Experimental results show state-of-the-art performance with F1 score improvements of 1-15%.

Conclusion: MOODS effectively addresses imbalance issues by optimizing sampling and introducing a novel evaluation metric.

Abstract: Two-class classification problems are often characterized by an imbalance
between the number of majority and minority datapoints resulting in poor
classification of the minority class in particular. Traditional approaches,
such as reweighting the loss function or na\"ive resampling, risk overfitting
and subsequently fail to improve classification because they do not consider
the diversity between majority and minority datasets. Such consideration is
infeasible because there is no metric that can measure the impact of imbalance
on the model. To obviate these challenges, we make two key contributions.
First, we introduce MOODS~(Multi-Objective Optimization for Data Sampling), a
novel multi-objective bilevel optimization framework that guides both synthetic
oversampling and majority undersampling. Second, we introduce a validation
metric -- `$\epsilon/ \delta$ non-overlapping diversification metric' -- that
quantifies the goodness of a sampling method towards model performance. With
this metric we experimentally demonstrate state-of-the-art performance with
improvement in diversity driving a $1-15 \%$ increase in $F1$ scores.

</details>


### [69] [An Attention-based Spatio-Temporal Neural Operator for Evolving Physics](https://arxiv.org/abs/2506.11328)
*Vispi Karkaria,Doksoo Lee,Yi-Ping Chen,Yue Yu,Wei Chen*

Main category: cs.LG

TL;DR: ASNO is a novel neural operator combining separable attention mechanisms for spatio-temporal interactions, adapting to unseen physical parameters and outperforming existing models in SciML benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning unknown, evolving physical processes in SciML, especially in dynamic environments like additive manufacturing, where traditional models lack interpretability and generalization.

Method: Proposes ASNO, integrating separable attention for spatial/temporal interactions and BDF-inspired transformers for temporal prediction, enhancing interpretability by isolating historical states and external forces.

Result: ASNO outperforms existing models on SciML benchmarks, demonstrating strong generalizability and interpretability.

Conclusion: ASNO shows promise for engineering applications, physics discovery, and interpretable machine learning due to its adaptability and performance.

Abstract: In scientific machine learning (SciML), a key challenge is learning unknown,
evolving physical processes and making predictions across spatio-temporal
scales. For example, in real-world manufacturing problems like additive
manufacturing, users adjust known machine settings while unknown environmental
parameters simultaneously fluctuate. To make reliable predictions, it is
desired for a model to not only capture long-range spatio-temporal interactions
from data but also adapt to new and unknown environments; traditional machine
learning models excel at the first task but often lack physical
interpretability and struggle to generalize under varying environmental
conditions. To tackle these challenges, we propose the Attention-based
Spatio-Temporal Neural Operator (ASNO), a novel architecture that combines
separable attention mechanisms for spatial and temporal interactions and adapts
to unseen physical parameters. Inspired by the backward differentiation formula
(BDF), ASNO learns a transformer for temporal prediction and extrapolation and
an attention-based neural operator for handling varying external loads,
enhancing interpretability by isolating historical state contributions and
external forces, enabling the discovery of underlying physical laws and
generalizability to unseen physical environments. Empirical results on SciML
benchmarks demonstrate that ASNO outperforms over existing models, establishing
its potential for engineering applications, physics discovery, and
interpretable machine learning.

</details>


### [70] [The Sample Complexity of Parameter-Free Stochastic Convex Optimization](https://arxiv.org/abs/2506.11336)
*Jared Lawrence,Ari Kalinsky,Hannah Bradfield,Yair Carmon,Oliver Hinder*

Main category: cs.LG

TL;DR: The paper addresses stochastic convex optimization with unknown parameters, proposing two methods: a reliable model selection technique to avoid overfitting and a regularization-based method for adapting to unknown optimality distance. Experiments on CIFAR-10 and shape counting validate the methods.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of unknown parameters in stochastic convex optimization, which complicates tuning and performance.

Method: 1. A reliable model selection method to avoid validation set overfitting. 2. A regularization-based method for adapting to unknown optimality distance.

Result: The methods achieve near-optimal sample complexity and perfect adaptability to unknown optimality distance, validated by experiments.

Conclusion: Combining the two methods enables adaptation to multiple problem structures, improving performance in practical scenarios.

Abstract: We study the sample complexity of stochastic convex optimization when problem
parameters, e.g., the distance to optimality, are unknown. We pursue two
strategies. First, we develop a reliable model selection method that avoids
overfitting the validation set. This method allows us to generically tune the
learning rate of stochastic optimization methods to match the optimal
known-parameter sample complexity up to $\log\log$ factors. Second, we develop
a regularization-based method that is specialized to the case that only the
distance to optimality is unknown. This method provides perfect adaptability to
unknown distance to optimality, demonstrating a separation between the sample
and computational complexity of parameter-free stochastic convex optimization.
Combining these two methods allows us to simultaneously adapt to multiple
problem structures.
  Experiments performing few-shot learning on CIFAR-10 by fine-tuning CLIP
models and prompt engineering Gemini to count shapes indicate that our reliable
model selection method can help mitigate overfitting to small validation sets.

</details>


### [71] [Improving Group Robustness on Spurious Correlation via Evidential Alignment](https://arxiv.org/abs/2506.11347)
*Wenqian Ye,Guangtao Zheng,Aidong Zhang*

Main category: cs.LG

TL;DR: Evidential Alignment uses uncertainty quantification to identify and suppress spurious correlations in deep neural networks without needing group annotations, improving robustness.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often rely on spurious correlations, harming generalization and trustworthiness. Existing methods require costly annotations or deterministic models, which may miss biases.

Method: Proposes Evidential Alignment, leveraging uncertainty quantification (second-order risk minimization) and evidential calibration to debias models.

Result: Empirical results show improved group robustness across architectures and data modalities.

Conclusion: Evidential Alignment offers a scalable, principled solution to spurious correlations without needing annotations.

Abstract: Deep neural networks often learn and rely on spurious correlations, i.e.,
superficial associations between non-causal features and the targets. For
instance, an image classifier may identify camels based on the desert
backgrounds. While it can yield high overall accuracy during training, it
degrades generalization on more diverse scenarios where such correlations do
not hold. This problem poses significant challenges for out-of-distribution
robustness and trustworthiness. Existing methods typically mitigate this issue
by using external group annotations or auxiliary deterministic models to learn
unbiased representations. However, such information is costly to obtain, and
deterministic models may fail to capture the full spectrum of biases learned by
the models. To address these limitations, we propose Evidential Alignment, a
novel framework that leverages uncertainty quantification to understand the
behavior of the biased models without requiring group annotations. By
quantifying the evidence of model prediction with second-order risk
minimization and calibrating the biased models with the proposed evidential
calibration technique, Evidential Alignment identifies and suppresses spurious
correlations while preserving core features. We theoretically justify the
effectiveness of our method as capable of learning the patterns of biased
models and debiasing the model without requiring any spurious correlation
annotations. Empirical results demonstrate that our method significantly
improves group robustness across diverse architectures and data modalities,
providing a scalable and principled solution to spurious correlations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [72] [Shapley Machine: A Game-Theoretic Framework for N-Agent Ad Hoc Teamwork](https://arxiv.org/abs/2506.11285)
*Jianhong Wang,Yang Li,Samuel Kaski,Jonathan Lawry*

Main category: cs.MA

TL;DR: The paper introduces a game-theoretic approach to solve the n-agent ad hoc teamwork (NAHT) problem in open multi-agent systems, using Shapley values for credit assignment and proposing a novel RL algorithm called Shapley Machine.


<details>
  <summary>Details</summary>
Motivation: Existing methods for NAHT lack theoretical rigor and clear credit assignment. The paper aims to address these gaps using cooperative game theory.

Method: The authors model NAHT using cooperative game theory, extend the game and state spaces for dynamics, and derive Shapley values for credit assignment. They propose a TD(λ)-like algorithm, Shapley Machine, to estimate these values.

Result: Shapley Machine effectively solves NAHT, with experiments validating the theory and demonstrating its practicality.

Conclusion: The paper successfully bridges cooperative game theory and RL, providing a rigorous and practical solution for NAHT.

Abstract: Open multi-agent systems are increasingly important in modeling real-world
applications, such as smart grids, swarm robotics, etc. In this paper, we aim
to investigate a recently proposed problem for open multi-agent systems,
referred to as n-agent ad hoc teamwork (NAHT), where only a number of agents
are controlled. Existing methods tend to be based on heuristic design and
consequently lack theoretical rigor and ambiguous credit assignment among
agents. To address these limitations, we model and solve NAHT through the lens
of cooperative game theory. More specifically, we first model an open
multi-agent system, characterized by its value, as an instance situated in a
space of cooperative games, generated by a set of basis games. We then extend
this space, along with the state space, to accommodate dynamic scenarios,
thereby characterizing NAHT. Exploiting the justifiable assumption that basis
game values correspond to a sequence of n-step returns with different horizons,
we represent the state values for NAHT in a form similar to $\lambda$-returns.
Furthermore, we derive Shapley values to allocate state values to the
controlled agents, as credits for their contributions to the ad hoc team.
Different from the conventional approach to shaping Shapley values in an
explicit form, we shape Shapley values by fulfilling the three axioms uniquely
describing them, well defined on the extended game space describing NAHT. To
estimate Shapley values in dynamic scenarios, we propose a TD($\lambda$)-like
algorithm. The resulting reinforcement learning (RL) algorithm is referred to
as Shapley Machine. To our best knowledge, this is the first time that the
concepts from cooperative game theory are directly related to RL concepts. In
experiments, we demonstrate the effectiveness of Shapley Machine and verify
reasonableness of our theory.

</details>


### [73] [AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction](https://arxiv.org/abs/2506.11475)
*Syeda Kisaa Fatima,Tehreem Zubair,Noman Ahmed,Asifullah Khan*

Main category: cs.MA

TL;DR: LUCID-MA is an AI framework using multiple agents to analyze crime data, featuring pattern analysis, feedback refinement, and crime prediction, all offline with minimal human input.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous, scalable, and iterative crime data analysis while maintaining privacy through offline execution.

Method: Uses three AI agents (analysis, feedback, prediction) with LLaMA-2-13B-Chat-GPTQ, self-improvement via 100 communication rounds, and a scoring function for performance evaluation.

Result: Demonstrates effective crime pattern analysis, refinement, and forecasting, with visual progress tracking.

Conclusion: LUCID-MA shows promise for autonomous social science research, balancing privacy and scalability.

Abstract: This paper introduces LUCID-MA (Learning and Understanding Crime through
Dialogue of Multiple Agents), an innovative AI powered framework where multiple
AI agents collaboratively analyze and understand crime data. Our system that
consists of three core components: an analysis assistant that highlights
spatiotemporal crime patterns, a feedback component that reviews and refines
analytical results and a prediction component that forecasts future crime
trends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it
runs completely offline and allows the agents undergo self-improvement through
100 rounds of communication with less human interaction. A scoring function is
incorporated to evaluate agent's performance, providing visual plots to track
learning progress. This work demonstrates the potential of AutoGen-style agents
for autonomous, scalable, and iterative analysis in social science domains
maintaining data privacy through offline execution.

</details>


### [74] [PE-MA: Parameter-Efficient Co-Evolution of Multi-Agent Systems](https://arxiv.org/abs/2506.11803)
*Yingfan Deng,Anhao Zhou,Yuan Yuan,Xian Zhang,Yifei Zou,Dongxiao Yu*

Main category: cs.MA

TL;DR: PE-MA is a novel framework for efficient, scalable, and personalized multi-agent collaboration, balancing global coordination with local adaptation.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like high communication overhead and insufficient agent-level personalization in multi-agent systems.

Method: Each agent uses a lightweight personalized adapter alongside a shared adapter optimized collaboratively across neighbors.

Result: Achieves an asymptotically optimal convergence rate of O(1/(NK)^(1/2)).

Conclusion: PE-MA effectively balances global and local needs in heterogeneous multi-agent environments.

Abstract: Multi-Agent Systems have recently emerged as a promising paradigm for
collaborative reasoning and solving complex tasks. However, the design of
collaborative learning algorithms in multi-agent systems faces several
challenges, including high communication overhead and insufficient agent-level
personalization. In this paper, we propose PE-MA (Parameter-Efficient
Multi-Agent Co-Evolution), a novel collaboration framework that supports
efficient, scalable, and personalized co-evolution in multi-agent systems. In
PE-MA, each agent maintains a lightweight personalized adapter to support
agent-specific behavior, while a shared adapter is collaboratively optimized
across neighboring agents. This design balances global coordination with local
adaptation under heterogeneous environments. We achieve an asymptotically
optimal convergence rate of O( 1/(NK)^(1/2) ), where N is the number of agents
and K the local update steps.

</details>
