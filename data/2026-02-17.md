<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 113]
- [cs.LG](#cs.LG) [Total: 149]
- [cs.MA](#cs.MA) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique](https://arxiv.org/abs/2602.13213)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.AI

TL;DR: An adversarial self-critique system reduces AI hallucinations and improves accuracy in commercial insurance underwriting while maintaining strict human oversight.


<details>
  <summary>Details</summary>
Motivation: Commercial insurance underwriting is labor-intensive and requires manual review of extensive documentation. While AI offers efficiency improvements, existing solutions lack comprehensive reasoning and reliability mechanisms for regulated, high-stakes environments where full automation is impractical and human judgment remains critical.

Method: The study presents a decision-negative, human-in-the-loop agentic system with adversarial self-critique mechanism. A critic agent challenges the primary agent's conclusions before submitting recommendations to human reviewers. The research also develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents.

Result: Experimental evaluation on 500 expert-validated underwriting cases shows that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. The framework enforces strict human authority over all binding decisions by design.

Conclusion: Adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable. This addresses a critical gap in AI safety for regulated workflows through internal checks and balances.

Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. At the same time, the framework enforces strict human authority over all binding decisions by design. These findings indicate that adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable.

</details>


### [2] [BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors](https://arxiv.org/abs/2602.13214)
*Lingfeng Li,Yunlong Lu,Yuefei Zhang,Jingyu Yao,Yixin Zhu,KeYuan Cheng,Yongyi Wang,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: This paper introduces BotzoneBench, a scalable evaluation framework that anchors LLM strategic reasoning assessment to fixed hierarchies of skill-calibrated game AIs, enabling linear-time absolute skill measurement with stable cross-temporal interpretability across diverse game domains.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks for strategic reasoning are inadequate—they primarily assess static reasoning through isolated tasks, fail to capture dynamic strategic abilities, and rely on LLM-vs-LLM tournaments that produce transient rankings with quadratic computational costs and lack stable performance anchors for longitudinal tracking.

Method: The authors anchor LLM evaluation to fixed hierarchies of skill-calibrated game AIs (using Botzone platform's competitive infrastructure), evaluate LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games, and systematically assess 177,047 state-action pairs from five flagship models.

Result: The evaluation reveals significant performance disparities and identifies distinct strategic behaviors among LLMs, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. The anchored framework enables linear-time absolute skill measurement with stable cross-temporal interpretability.

Conclusion: This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities, moving beyond volatile peer-based comparisons to consistent, interpretable skill measurement.

Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

</details>


### [3] [When to Think Fast and Slow? AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching](https://arxiv.org/abs/2602.13215)
*Haoran Zheng*

Main category: cs.AI

TL;DR: AMOR is a hybrid architecture that dynamically engages sparse attention only when an SSM backbone is uncertain, achieving efficient computation while maintaining high accuracy on retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers allocate uniform computation to every position regardless of difficulty, while SSMs are efficient alternatives but struggle with precise information retrieval over long horizons. The paper aims to create an architecture that combines efficiency with precise information retrieval.

Method: Proposes AMOR, a hybrid architecture inspired by dual-process theories of cognition. It uses an SSM backbone and dynamically engages sparse attention only when the SSM is "uncertain" as measured by prediction entropy. It gains efficiency by projecting keys and values from SSM hidden states (Ghost KV), reusing the SSM's O(n) computation rather than requiring O(n^2) attention at every layer.

Result: On small-scale synthetic retrieval tasks, AMOR outperforms both SSM-only and transformer-only baselines, achieving perfect retrieval accuracy while engaging attention on only 22% of positions. The approach shows that prediction entropy reliably signals retrieval need, with a gap of 1.09 nats between retrieval and local positions.

Conclusion: AMOR provides interpretable adaptive computation where routing decisions can be understood in information-theoretic terms. The architecture demonstrates that dynamically engaging attention based on prediction entropy enables efficient yet accurate information retrieval.

Abstract: Transformers allocate uniform computation to every position, regardless of difficulty. State Space Models (SSMs) offer efficient alternatives but struggle with precise information retrieval over a long horizon. Inspired by dual-process theories of cognition (Kahneman, 2011), we propose AMOR (Adaptive Metacognitive Output Router), a hybrid architecture that dynamically engages sparse attention only when an SSM backbone is "uncertain"--as measured by prediction entropy. Compared to standard transformers, AMOR gains efficiency by projecting keys and values from SSM hidden states (Ghost KV), reusing the SSM's O(n) computation rather than requiring O(n^2) attention at every layer. On small-scale synthetic retrieval tasks, AMOR outperforms both SSM-only and transformer-only baselines, achieving perfect retrieval accuracy while engaging attention on only 22% of positions. We validate that prediction entropy reliably signals retrieval need, with a gap of 1.09 nats (nearly half the entropy range) between retrieval and local positions. Additionally, our approach provides interpretable adaptive computation, where routing decisions can be understood in information-theoretic terms.

</details>


### [4] [VeRA: Verified Reasoning Data Augmentation at Scale](https://arxiv.org/abs/2602.13217)
*Zerui Cheng,Jiashuo Liu,Chunjie Wu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: VeRA is a framework that converts static benchmark problems into executable specifications to generate unlimited verified variants, addressing contamination and memorization issues in AI evaluation.


<details>
  <summary>Details</summary>
Motivation: Current evaluation schemes are "static" - the same problems are reused repeatedly, leading to memorization, format exploitation, and saturation. To measure genuine AI progress, we need robust evaluation by construction rather than post-hoc detection.

Method: VeRA converts benchmark problems into executable specifications with three components: (1) natural language template with placeholder slots, (2) coherent generator that samples valid configurations, and (3) deterministic verifier that validates parameters and calculates correct answers. It operates in two modes: VeRA-E (equivalent) rewrites problems while keeping underlying logic intact; VeRA-H (hardened) systematically increases complexity while remaining verifiable.

Result: Evaluation of 16 frontier models shows: (1) VeRA-E improves evaluation quality and reveals contamination patterns; (2) VeRA-H enables human-free generation of hard tasks with reliable labels; (3) VeRA establishes verified benchmarks as a general paradigm.

Conclusion: VeRA reconceptualizes benchmarks from static objects used until exhausted to executable specifications generating fresh, verified instances on demand. This enhances robustness and cost-effectiveness for evaluation, allowing evaluation in any verifiable domain to scale indefinitely without sacrificing label integrity.

Abstract: The main issue with most evaluation schemes today is their "static" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.
  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.
  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.

</details>


### [5] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic is an agentic meta-synthesis framework that scales verifiable training signals for RLVR by iteratively synthesizing and repairing executable Generator-Validator program pairs, enabling continuous evolution of task families with controllable difficulty.


<details>
  <summary>Details</summary>
Motivation: Scaling verifiable training signals is a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). While logical reasoning provides a natural substrate with formal constraints and programmatically checkable answers, prior synthesis approaches are limited to either expert-written code or fixed templates, restricting growth to instance-level perturbations rather than task-family evolution.

Method: SSLogic employs an agentic meta-synthesis framework that operates in a closed Generate-Validate-Repair loop to iteratively synthesize and repair executable Generator-Validator program pairs. It introduces a Multi-Gate Validation Protocol combining multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks, enabling continuous family evolution with controllable difficulty.

Result: Starting from 400 seed families, two evolution rounds expanded to 953 families and 21,389 verifiable instances (from 5,718 initial instances). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps: +5.2 improvement on SynLogic, +1.4 on BBEH, +3.0 on AIME25, and +3.7 on Brumo25.

Conclusion: SSLogic successfully addresses the scaling bottleneck for RLVR by enabling systematic task-family evolution through its agentic meta-synthesis framework and rigorous validation protocol. The framework demonstrates substantial gains across multiple benchmarks, showing that evolving verifiable training signals at the task-family level significantly improves RLVR performance beyond instance-level approaches.

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [6] [A Geometric Taxonomy of Hallucinations in LLMs](https://arxiv.org/abs/2602.13224)
*Javier Marín*

Main category: cs.AI

TL;DR: Paper proposes a taxonomy of LLM hallucinations with geometric signatures: unfaithfulness (ignoring context), confabulation (inventing foreign content), and factual errors. Shows detection works for Types I-II but fails for Type III due to embedding limitations.


<details>
  <summary>Details</summary>
Motivation: The term "hallucination" in LLMs currently conflates distinct phenomena with different underlying mechanisms. This oversimplification hinders understanding of detection capabilities and the fundamental limitations of embedding-based approaches.

Method: Proposes a three-type taxonomy based on geometric signatures in embedding space. Analyzes detection performance on both LLM-generated hallucinations (from benchmarks) and human-crafted confabulations. Examines domain-local vs. global detection capabilities and cosine similarity of discriminative directions.

Result: Key asymmetry found: detection works well for Types I-II (unfaithfulness and confabulation) with AUROC 0.76-0.99 within domains, but fails for Type III (factual errors) with AUROC 0.478 (chance level). Human-crafted confabulations show single global direction detection (0.96 AUROC) while LLM-generated ones show domain-local patterns.

Conclusion: The geometric taxonomy clarifies embedding-based detection scope: Types I and II are detectable, but Type III requires external verification since embeddings encode distributional co-occurrence rather than correspondence to reality. Benchmarks capture generation artifacts while human confabulations capture genuine topical drift.

Abstract: The term "hallucination" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are LLM-generated, detection is domain-local: AUROC 0.76-0.99 within domains, but 0.50 (chance level) across domains. Discriminative directions are approximately orthogonal between domains (mean cosine similarity -0.07). On human-crafted confabulations - invented institutions, redefined terminology, fabricated mechanisms - a single global direction achieves 0.96 AUROC with 3.8% cross-domain degradation. We interpret this divergence as follows: benchmarks capture generation artifacts (stylistic signatures of prompted fabrication), while human-crafted confabulations capture genuine topical drift. The geometric structure differs because the underlying phenomena differ. Type III errors show 0.478 AUROC - indistinguishable from chance. This reflects a theoretical constraint: embeddings encode distributional co-occurrence, not correspondence to external reality. Statements with identical contextual patterns occupy similar embedding regions regardless of truth value. The contribution is a geometric taxonomy clarifying the scope of embedding-based detection: Types I and II are detectable; Type III requires external verification mechanisms.

</details>


### [7] [Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection](https://arxiv.org/abs/2602.13226)
*Xuecong Li,Xiaohong Li,Qiang Hu,Yao Zhang,Junjie Wang*

Main category: cs.AI

TL;DR: VaryBalance: A simple but effective LLM-generated text detection method that measures the difference between original human texts and their LLM-rewritten versions using mean standard deviation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-generated text detectors have limitations: they either rely on impractical white-box assumptions or only use text-level features, resulting in imprecise detection capabilities.

Method: VaryBalance quantifies the greater difference between human texts and their LLM-rewritten versions compared to LLM-generated texts, using mean standard deviation to distinguish between human and LLM-generated content.

Result: VaryBalance outperforms state-of-the-art detectors (Binoculars) by up to 34.3% in AUROC, and maintains robustness across multiple generating models and languages.

Conclusion: VaryBalance provides a simple, effective, and practical solution for LLM-generated text detection that addresses limitations of existing approaches.

Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and practical LLM-generated text detection method, VaryBalance. The core of VaryBalance is that, compared to LLM-generated texts, there is a greater difference between human texts and their rewritten version via LLMs. Leveraging this observation, VaryBalance quantifies this through mean standard deviation and distinguishes human texts and LLM-generated texts. Comprehensive experiments demonstrated that VaryBalance outperforms the state-of-the-art detectors, i.e., Binoculars, by up to 34.3\% in terms of AUROC, and maintains robustness against multiple generating models and languages.

</details>


### [8] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: Intelligence stagnation stems not from insufficient learning/resources but from geometric constraints in trajectory-level optimization, where Pareto traps restrict long-term adaptability.


<details>
  <summary>Details</summary>
Motivation: Despite AI advances, many systems show stagnation in long-horizon adaptability despite optimization. The paper argues this limitation arises from structural properties of how intelligence is optimized over time, not from insufficient learning, data, or model capacity.

Method: Formulates intelligence as trajectory-level phenomenon governed by multi-objective trade-offs. Introduces Trajectory-Dominant Pareto Optimization (path-wise generalization of classical Pareto optimality). Defines Trap Escape Difficulty Index (TEDI) to measure rigidity of constraints. Provides formal taxonomy of Pareto traps and illustrates with minimal agent-environment model.

Result: Shows that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. Demonstrates trajectory-level divergence using minimal model. Provides principled framework for diagnosing/overcoming long-horizon developmental constraints.

Conclusion: Shifts locus of intelligence from terminal performance to optimization geometry. Offers new perspective on overcoming long-horizon adaptability limitations through understanding Pareto trap constraints in trajectory space.

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [9] [PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading](https://arxiv.org/abs/2602.13232)
*Mayank Ravishankara*

Main category: cs.AI

TL;DR: PlotChain is a deterministic, generator-based benchmark for evaluating MLLMs' ability to recover quantitative values from classic engineering plots, with diagnostic evaluation through checkpoints to localize failures.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for multimodal models focus on OCR extraction or free-form captioning, but there's a need to assess quantitative plot reading abilities, especially for engineering plots where precise value recovery is crucial.

Method: Created 450 plots across 15 engineering families (30 per family) with known parameters and ground truth. Introduced checkpoint-based diagnostic evaluation with intermediate 'cp_' fields to isolate sub-skills. Evaluated models using strict deterministic protocol with temperature=0 and JSON-only numeric outputs, scoring with precision-based tolerances.

Result: Under 'plotread' tolerance, top performers: Gemini 2.5 Pro (80.42%), GPT-4.1 (79.84%), Claude Sonnet 4.5 (78.21%), GPT-4o (61.59%). Frequency-domain tasks remain brittle with bandpass response ≤23% and FFT spectrum remaining challenging.

Conclusion: PlotChain enables reproducible, diagnostic evaluation of MLLMs on quantitative plot reading. While models show strong performance on many plot families, specific engineering tasks like frequency-domain analysis remain difficult, highlighting areas for future improvement.

Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-form captioning. PlotChain contains 15 plot families with 450 rendered plots (30 per family), where every item is produced from known parameters and paired with exact ground truth computed directly from the generating process. A central contribution is checkpoint-based diagnostic evaluation: in addition to final targets, each item includes intermediate 'cp_' fields that isolate sub-skills (e.g., reading cutoff frequency or peak magnitude) and enable failure localization within a plot family. We evaluate four state-of-the-art MLLMs under a standardized, deterministic protocol (temperature = 0 and a strict JSON-only numeric output schema) and score predictions using per-field tolerances designed to reflect human plot-reading precision. Under the 'plotread' tolerance policy, the top models achieve 80.42% (Gemini 2.5 Pro), 79.84% (GPT-4.1), and 78.21% (Claude Sonnet 4.5) overall field-level pass rates, while GPT-4o trails at 61.59%. Despite strong performance on many families, frequency-domain tasks remain brittle: bandpass response stays low (<= 23%), and FFT spectrum remains challenging. We release the generator, dataset, raw model outputs, scoring code, and manifests with checksums to support fully reproducible runs and retrospective rescoring under alternative tolerance policies.

</details>


### [10] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: LLMs fail at coordination in simultaneous decision-making scenarios despite working well in sequential settings, revealing a fundamental limitation in multi-agent systems that requires external coordination mechanisms.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly deployed in multi-agent systems, but there's a lack of benchmarks testing their ability to coordinate under resource contention. The researchers wanted to evaluate whether LLMs can effectively coordinate when resources are limited and decisions must be made concurrently.

Method: Researchers created DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions varying decision timing (sequential vs simultaneous), group size, and communication. They tested GPT-5.2, Claude Opus 4.5, and Grok 4.1 in these scenarios.

Result: LLMs show striking asymmetry: they coordinate effectively in sequential settings but fail dramatically in simultaneous decision-making, with deadlock rates exceeding 95% in some conditions. This failure stems from convergent reasoning where agents independently arrive at identical strategies that guarantee deadlock when executed simultaneously. Surprisingly, enabling communication doesn't resolve the problem and can even increase deadlock rates.

Conclusion: Multi-agent LLM systems requiring concurrent resource access cannot rely on emergent coordination and need external coordination mechanisms. DPBench serves as an open-source benchmark to evaluate this critical limitation in LLM coordination capabilities.

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [11] [Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents](https://arxiv.org/abs/2602.13234)
*Mingyang Liao,Yichen Wan,shuchen wu,Chenxi Miao,Xin Shen,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: Dual-Cycle Adversarial Self-Evolution: A training-free framework that improves both role-playing fidelity and jailbreak resistance through coupled attacker/defender cycles.


<details>
  <summary>Details</summary>
Motivation: LLM-based role-playing faces a tradeoff between persona fidelity and jailbreak vulnerability. Training-time solutions are costly, degrade character behavior, and don't work with closed-weight LLMs.

Method: Training-free Dual-Cycle framework with two coupled cycles: Persona-Targeted Attacker Cycle synthesizes jailbreak prompts, while Role-Playing Defender Cycle distills failures into hierarchical knowledge (global safety rules, persona constraints, safe exemplars). At inference, retrieved knowledge guides generation.

Result: Extensive experiments on multiple proprietary LLMs show consistent gains over baselines on both role fidelity and jailbreak resistance, with robust generalization to unseen personas and attacks.

Conclusion: Dual-Cycle Adversarial Self-Evolution offers effective, training-free approach to balance persona fidelity and safety in LLM role-playing, addressing limitations of prior methods.

Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.

</details>


### [12] [MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems](https://arxiv.org/abs/2602.13258)
*Deepak Babu Piskala*

Main category: cs.AI

TL;DR: MAPLE decomposes LLM agent personalization into three distinct components (Memory, Learning, Personalization) with specialized sub-agents, achieving 14.6% improvement in personalization scores and 75% trait incorporation rate.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents have limited ability to adapt to individual users because they conflate memory, learning, and personalization as a unified capability, rather than treating them as distinct mechanisms requiring different infrastructure and optimization.

Method: Proposes MAPLE (Memory-Adaptive Personalized LEarning) - a principled decomposition where Memory handles storage/retrieval infrastructure, Learning extracts intelligence from accumulated interactions asynchronously, and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces.

Result: Experimental evaluation on MAPLE-Personas benchmark shows 14.6% improvement in personalization score compared to stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75%.

Conclusion: Decomposing personalization into distinct Memory, Learning, and Personalization components enables LLM agents to genuinely learn and adapt to individual users, overcoming the limitations of current unified approaches.

Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.

</details>


### [13] [Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains](https://arxiv.org/abs/2602.13235)
*Yuqi Xiong,Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu*

Main category: cs.AI

TL;DR: Lang2Act is a VRAG framework that uses self-emergent linguistic toolchains instead of fixed external tools, enabling fine-grained visual perception and reasoning through a two-stage RL training approach.


<details>
  <summary>Details</summary>
Motivation: Existing VRAG frameworks rely on rigid, pre-defined external tools that separate visual perception from reasoning, leading to unnecessary loss of visual information, especially during image operations like cropping.

Method: Lang2Act collects self-emergent actions as linguistic tools to enhance VLM perception. It uses a two-stage RL training framework: first stage optimizes VLMs to self-explore high-quality actions to build a linguistic toolbox; second stage optimizes VLMs to effectively exploit these tools for downstream reasoning.

Result: Lang2Act substantially enhances VLM visual perception capabilities, achieving performance improvements of over 4% in experiments.

Conclusion: The proposed Lang2Act framework successfully addresses limitations of existing VRAG approaches by enabling fine-grained visual perception through self-emergent linguistic toolchains, demonstrating significant performance gains for VLMs.

Abstract: Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.

</details>


### [14] [NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models](https://arxiv.org/abs/2602.13237)
*Rizky Ramadhana Putra,Raihan Sultan Pasha Basuki,Yutong Cheng,Peng Gao*

Main category: cs.AI

TL;DR: NL2LOGIC is a framework that uses abstract syntax trees as intermediate representations to translate natural language to first-order logic with high syntactic accuracy and semantic correctness.


<details>
  <summary>Details</summary>
Motivation: Existing methods for translating natural language to first-order logic (like GCD and CODE4LOGIC) suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding.

Method: NL2LOGIC introduces an abstract syntax tree as an intermediate representation, combining a recursive LLM-based semantic parser with an AST-guided generator that deterministically produces solver-ready logic code.

Result: Achieves 99% syntactic accuracy and improves semantic correctness by up to 30% over state-of-the-art baselines on FOLIO, LogicNLI, and ProofWriter benchmarks. When integrated into Logic-LM, yields near-perfect executability and improves downstream reasoning accuracy by 31%.

Conclusion: NL2LOGIC provides a robust framework for translating natural language to first-order logic with high syntactic and semantic accuracy, significantly improving automated reasoning in domains like law and governance.

Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to Logic-LM's original few-shot unconstrained translation module.

</details>


### [15] [A First Proof Sprint](https://arxiv.org/abs/2602.13587)
*Joseph Corneli*

Main category: cs.AI

TL;DR: Multi-agent proof sprint workflow using wiring-diagram decomposition for research problems achieves varied mathematical outcomes with explicit QC-validation status differentiation.


<details>
  <summary>Details</summary>
Motivation: To develop a structured, collaborative approach for tackling research-level mathematical problems efficiently, combining rapid generation with rigorous verification while maintaining clear provenance and status tracking.

Method: Uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions, with structure-aware verification and layer-switching strategies in a multi-agent collaborative workflow.

Result: Heterogeneous outcomes across 10 problems: Problem 3 validated complete under scoped criterion, Problem 5 solved for limited spectra, Problem 10 conditional, Problems 4/6 partial with specific obligations, Problem 7 provisionally closed. QC layer shows validation artifacts for Problems 7/9 with unresolved verifier gaps.

Conclusion: Structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints, enabling explicit distinction between mathematical status and QC-validation status for transparent research progress assessment.

Abstract: This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.

</details>


### [16] [AST-PAC: AST-guided Membership Inference for Code](https://arxiv.org/abs/2602.13240)
*Roham Koohestani,Ali Al-Kaswan,Jonathan Katzy,Maliheh Izadi*

Main category: cs.AI

TL;DR: A study evaluating membership inference attacks for auditing code LLMs shows PAC outperforms baseline Loss attack, but degrades on large code files due to syntax-ignoring augmentations. AST-PAC uses AST-based perturbations to improve performance on syntactically complex files, though it struggles with small/alphanumeric code.


<details>
  <summary>Details</summary>
Motivation: Code LLMs are often trained on restrictively licensed source code, creating copyright and data governance challenges. There's a need for reliable auditing mechanisms to detect unauthorized data usage. While MIAs offer potential, methods like PAC remain underexplored in the code domain.

Method: Evaluated baseline Loss Attack vs. Polarized Augment Calibration (PAC) on 3B-7B parameter code models. Introduced AST-PAC, a domain-specific adaptation using Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples while preserving code structure.

Result: PAC generally outperforms Loss baseline but degrades on larger, complex files due to syntax-disregarding augmentations. AST-PAC improves performance as syntactic size grows (where PAC degrades) but under-mutates small files and underperforms on alphanumeric-rich code.

Conclusion: Findings motivate future work on syntax-aware and size-adaptive calibration as prerequisites for reliable provenance auditing of code language models. Domain-specific adaptations like AST-PAC show promise but require further refinement.

Abstract: Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.

</details>


### [17] [Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval](https://arxiv.org/abs/2602.13639)
*Linlin Wang,Tianqing Zhu,Laiqiao Qin,Longxiang Gao,Wanlei Zhou*

Main category: cs.AI

TL;DR: Proposes an Entropy-Based Adaptive Guidance Framework with RAG to address cognitive mismatching in heterogeneous multi-agent systems, improving collaboration effectiveness and stability across benchmarks.


<details>
  <summary>Details</summary>
Motivation: In heterogeneous multi-agent systems (HMAS), capability differences among agents lead to cognitive mismatching, causing strong-weak collaborations to underperform weak-weak combinations. This cognitive imbalance is a key bottleneck limiting effective heterogeneous cooperation.

Method: The method involves an Entropy-Based Adaptive Guidance Framework that dynamically aligns guidance with each agent's cognitive state. It quantifies weak agents' understanding using multi-dimensional entropy metrics (expression, uncertainty, structure, coherence, relevance) and adaptively adjusts guidance intensity (light, moderate, intensive). A Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences for immediate adaptation and long-term learning.

Result: Extensive experiments on three benchmark datasets (GSM8K, MBPP, CVRP) demonstrate that the approach consistently enhances the effectiveness and stability of heterogeneous collaboration. Adaptive guidance mitigates cognitive imbalance and improves performance.

Conclusion: The proposed Entropy-Based Adaptive Guidance Framework, enhanced with RAG, effectively mitigates cognitive mismatching in heterogeneous multi-agent systems, establishing a scalable pathway toward more robust and cooperative multi-agent intelligence.

Abstract: With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.

</details>


### [18] [X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles](https://arxiv.org/abs/2602.13248)
*Ashkan Y. Zadeh,Xiaomeng Li,Andry Rakotonirainy,Ronald Schroeter,Sebastien Glaser,Zishuo Zhu*

Main category: cs.AI

TL;DR: X-Blocks is a hierarchical framework for analyzing natural language explanations in AVs at context, syntax, and lexical levels, using multi-LLM ensemble classification, dependency parsing, and log-odds analysis to understand how humans explain driving behaviors.


<details>
  <summary>Details</summary>
Motivation: Natural language explanations are crucial for trust in AVs, but there's a lack of systematic frameworks for analyzing how humans linguistically construct driving rationales across diverse scenarios.

Method: Introduces X-Blocks hierarchical framework with three levels: context (RACE multi-LLM ensemble for explanation classification), syntax (dependency parsing and template extraction), and lexicon (log-odds analysis with Dirichlet priors for vocabulary patterns).

Result: RACE achieved 91.45% accuracy and Cohen's kappa of 0.91 for context classification, revealing context-specific vocabulary patterns, reusable grammar families, and systematic variation in causal constructions across scenarios.

Conclusion: X-Blocks provides dataset-agnostic framework with evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, trust, and cognitive accessibility in automated driving systems.

Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.
  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.
  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.
  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

</details>


### [19] [From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents](https://arxiv.org/abs/2602.13855)
*Razeen A Rasheed,Somnath Banerjee,Animesh Mukherjee,Rima Hazra*

Main category: cs.AI

TL;DR: Deep research agents can produce reports quickly, but verifying their claims requires extensive tracing of evidence, making auditability the real bottleneck for AI-generated research.


<details>
  <summary>Details</summary>
Motivation: As AI research generation becomes cheap, the main risk shifts from isolated factual errors to outputs with weak or misleading claim-evidence links, creating auditability as the critical bottleneck for scientific validity.

Method: The paper proposes claim-level auditability as a design target, identifies failure modes, introduces the Auditable Autonomous Research (AAR) standard with metrics (provenance coverage, soundness, contradiction transparency, audit effort), and advocates for semantic provenance with protocolized validation via persistent provenance graphs.

Result: The paper presents a framework for making AI research outputs auditable by establishing standards for tracking claim-evidence relationships, detecting conflicts, and enabling continuous validation during synthesis rather than after publication.

Conclusion: Auditability must become a primary design consideration for AI research agents, with semantic provenance and integrated validation mechanisms essential for maintaining scientific integrity as AI-generated research scales.

Abstract: A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.

</details>


### [20] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST enables language models to spawn parallel clones with same weights via agentic reinforcement learning, improving compute efficiency for reasoning tasks under fixed inference budgets.


<details>
  <summary>Details</summary>
Motivation: Current language models rely on serial reasoning or uncoordinated parallel sampling, which can be compute-inefficient under fixed inference budgets. There's a need for more efficient allocation of computational resources during test-time.

Method: The approach equips a base model with ability to spawn same-weight clones in separate parallel contexts through agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches.

Result: SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budgets across challenging math reasoning benchmarks and long-context multi-hop QA. It also exhibits out-of-distribution generalization in both domains.

Conclusion: SELFCEST represents an effective approach for improving computational efficiency in language models through learned parallel computation allocation, demonstrating superior performance-cost tradeoffs and generalization capabilities.

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [21] [Human-Centered Explainable AI for Security Enhancement: A Deep Intrusion Detection Framework](https://arxiv.org/abs/2602.13271)
*Md Muntasir Jahid Ayan,Md. Shahriar Rashid,Tazzina Afroze Hassan,Hossain Md. Mubashshir Jamil,Mahbubul Islam,Lisan Al Amin,Rupak Kumar Das,Farzana Akter,Faisal Quader*

Main category: cs.AI

TL;DR: Novel IDS framework integrates XAI for transparent deep learning, using CNN-LSTM on NSL-KDD data with high accuracy and SHAP for interpretability.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and frequency of cyber-threats demand intrusion detection systems (IDS) that are both accurate and interpretable, requiring transparency in deep learning models for security analysts.

Method: Proposed framework combines CNN and LSTM networks to capture temporal dependencies in traffic sequences, uses SHAP (SHapley Additive exPlanations) for interpretability, and evaluates with NSL-KDD dataset plus expert survey based on IPIP6 and Big Five personality traits.

Result: Both CNN and LSTM achieved 0.99 accuracy, with LSTM outperforming CNN on macro average precision, recall, and F-1 score. SHAP identified key features (srv_serror_rate, dst_host_srv_serror_rate, serror_rate). Expert survey validated system reliability and usability.

Conclusion: The work demonstrates the potential of combining performance and transparency in cybersecurity solutions and recommends future enhancements through adaptive learning for real-time threat detection.

Abstract: The increasing complexity and frequency of cyber-threats demand intrusion detection systems (IDS) that are not only accurate but also interpretable. This paper presented a novel IDS framework that integrated Explainable Artificial Intelligence (XAI) to enhance transparency in deep learning models. The framework was evaluated experimentally using the benchmark dataset NSL-KDD, demonstrating superior performance compared to traditional IDS and black-box deep learning models. The proposed approach combined Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) networks for capturing temporal dependencies in traffic sequences. Our deep learning results showed that both CNN and LSTM reached 0.99 for accuracy, whereas LSTM outperformed CNN at macro average precision, recall, and F-1 score. For weighted average precision, recall, and F-1 score, both models scored almost similarly. To ensure interpretability, the XAI model SHapley Additive exPlanations (SHAP) was incorporated, enabling security analysts to understand and validate model decisions. Some notable influential features were srv_serror_rate, dst_host_srv_serror_rate, and serror_rate for both models, as pointed out by SHAP. We also conducted a trust-focused expert survey based on IPIP6 and Big Five personality traits via an interactive UI to evaluate the system's reliability and usability. This work highlighted the potential of combining performance and transparency in cybersecurity solutions and recommends future enhancements through adaptive learning for real-time threat detection.

</details>


### [22] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench is a multi-domain benchmark for evaluating temporal reasoning across four tiers: historical structure interpretation, context-free forecasting, contextual reasoning, and event-conditioned prediction in retail, healthcare, energy, and physical systems.


<details>
  <summary>Details</summary>
Motivation: It's unclear whether strong forecasting performance reflects genuine temporal understanding or just reasoning under contextual/event-driven conditions. Current benchmarks may hide systematic failure modes.

Method: Introduces TemporalBench with four-tier taxonomy examining temporal reasoning under progressively richer informational settings. Controls access to future targets and contextual information across four real-world domains.

Result: Strong numerical forecasting accuracy doesn't reliably translate into robust contextual or event-aware temporal reasoning. Existing frameworks show fragmented strengths and systematic failure modes that remain hidden under forecasting-only benchmarks.

Conclusion: TemporalBench enables diagnostic analysis of models' temporal reasoning capabilities beyond forecasting accuracy, revealing systematic weaknesses. The benchmark and leaderboard are publicly available.

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [23] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench is a unified benchmark for evaluating 11 prompting paradigms across 4 LLM families, using a new Unified Moral Safety Score (UMSS) metric that balances accuracy and safety.


<details>
  <summary>Details</summary>
Motivation: Current research on prompt design's impact on LLM moral competence and safety alignment is fragmented across different datasets and models, lacking standardized comparisons.

Method: Created ProMoral-Bench evaluating 11 prompting paradigms across 4 LLM families using ETHICS, Scruples, WildJailbreak datasets and new ETHICS-Contrast robustness test, with proposed UMSS metric.

Result: Compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, achieving higher UMSS scores with greater robustness at lower token cost. Few-shot exemplars consistently enhance moral stability and jailbreak resistance while multi-turn reasoning proves fragile under perturbations.

Conclusion: ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering, showing that simpler, exemplar-based approaches are more effective for LLM moral safety than complex reasoning methods.

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [24] [Artificial Organisations](https://arxiv.org/abs/2602.13275)
*William Waites*

Main category: cs.AI

TL;DR: Multi-agent AI systems can achieve reliability through institutional design with compartmentalization and adversarial review, rather than relying solely on individual AI alignment.


<details>
  <summary>Details</summary>
Motivation: Current AI alignment research focuses on making individual systems reliable, while human institutions achieve reliable collective behavior through organizational structure. The paper aims to apply this institutional approach to multi-agent AI systems.

Method: The Perseverance Composition Engine: a multi-agent system with three specialized agents (Composer drafts text, Corroborator verifies facts with source access, Critic evaluates argument quality without source access) using architectural information asymmetry and layered verification.

Result: Analysis of 474 composition tasks shows patterns consistent with institutional hypothesis. When given impossible tasks requiring fabrication, the system evolved from attempted fabrication toward honest refusal with alternative proposals - behavior not explicitly instructed or individually incentivized.

Conclusion: Organizational theory provides a productive framework for multi-agent AI safety. Architectural enforcement through information compartmentalization can produce reliable collective behavior from unreliable individual components, offering an alternative to individual alignment approaches.

Abstract: Alignment research focuses on making individual AI systems reliable. Human institutions achieve reliable collective behaviour differently: they mitigate the risk posed by misaligned individuals through organisational structure. Multi-agent AI systems should follow this institutional model using compartmentalisation and adversarial review to achieve reliable outcomes through architectural design rather than assuming individual alignment.
  We demonstrate this approach through the Perseverance Composition Engine, a multi-agent system for document composition. The Composer drafts text, the Corroborator verifies factual substantiation with full source access, and the Critic evaluates argumentative quality without access to sources: information asymmetry enforced by system architecture. This creates layered verification: the Corroborator detects unsupported claims, whilst the Critic independently assesses coherence and completeness. Observations from 474 composition tasks (discrete cycles of drafting, verification, and evaluation) exhibit patterns consistent with the institutional hypothesis. When assigned impossible tasks requiring fabricated content, this iteration enabled progression from attempted fabrication toward honest refusal with alternative proposals--behaviour neither instructed nor individually incentivised. These findings motivate controlled investigation of whether architectural enforcement produces reliable outcomes from unreliable components.
  This positions organisational theory as a productive framework for multi-agent AI safety. By implementing verification and evaluation as structural properties enforced through information compartmentalisation, institutional design offers a route to reliable collective behaviour from unreliable individual components.

</details>


### [25] [BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation](https://arxiv.org/abs/2602.13280)
*Hanchen David Wang,Clayton Cohn,Zifan Xu,Siyuan Guo,Gautam Biswas,Meiyi Ma*

Main category: cs.AI

TL;DR: BEAGLE is a neuro-symbolic framework that simulates realistic student learning behaviors in open-ended problem-solving by incorporating Self-Regulated Learning theory, outperforming LLM-based approaches that suffer from competency bias.


<details>
  <summary>Details</summary>
Motivation: Simulating authentic student learning behaviors is valuable for education research but challenging due to privacy concerns, high costs of longitudinal studies, and limitations of LLMs which exhibit competency bias by optimizing for correctness rather than replicating the erratic, iterative struggle of novice learners.

Method: BEAGLE integrates three key innovations: 1) a semi-Markov model governing timing and transitions of cognitive and metacognitive behaviors, 2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps, and 3) a decoupled agent design separating high-level strategy use from code generation to prevent silent error correction.

Result: BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic student trajectories on Python programming tasks. In a human Turing test, users achieved only 52.8% accuracy in distinguishing synthetic traces from real student data, indistinguishable from random guessing.

Conclusion: BEAGLE successfully addresses the competency bias problem in student simulation by incorporating Self-Regulated Learning theory into a neuro-symbolic framework, enabling generation of realistic student learning behaviors that are indistinguishable from authentic data, with promising applications for education research and adaptive tutoring systems.

Abstract: Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and "unknown unknowns"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).

</details>


### [26] [Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey](https://arxiv.org/abs/2602.13283)
*Gaston Besanson,Federico Todeschini*

Main category: cs.AI

TL;DR: Users demand higher accuracy from AI tools at work than in personal life, and experience more disruption in personal routines when these tools are unavailable.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how people trade off accuracy requirements when using AI-powered tools in professional versus personal contexts, identify determinants of these trade-offs, and examine how users cope when AI tools become unavailable. This addresses the gap in understanding how context influences accuracy expectations for modern AI systems that produce acceptable but non-identical outputs.

Method: The researchers conducted an online survey with 300 participants, analyzing responses from 170 respondents who completed both accuracy-related items. They defined accuracy as context-specific reliability measured by alignment with user intent within tolerance thresholds. Statistical analysis included z-tests and ordinal scale comparisons to examine differences between professional and personal contexts.

Result: Results show a significant accuracy requirement gap: 24.1% required high accuracy at work vs. 8.8% in personal life (+15.3 percentage points). The gap remained substantial using broader definitions (67.0% vs. 32.9% for top-two-box) and ordinal scale means (3.86 vs. 3.08). Heavy app use correlated with stricter work standards, and respondents reported more disruption in personal routines than at work when tools were unavailable (34.1% vs. 15.3%).

Conclusion: The study concludes that users have significantly higher accuracy requirements for AI tools in professional contexts compared to personal use, and experience greater disruption in personal routines when such tools become unavailable. This highlights the context-dependent nature of accuracy expectations and the varying impacts of AI tool dependency across different life domains.

Abstract: We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define "accuracy" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.

</details>


### [27] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: Mirror is an AI agent framework for ethics review that integrates ethical reasoning, rule interpretation, and multi-agent deliberation, using a specialized EthicsLLM fine-tuned on ethics corpora.


<details>
  <summary>Details</summary>
Motivation: Current ethics review systems are strained by large-scale interdisciplinary research, and while LLMs offer potential, they lack sufficient ethical reasoning, regulatory integration, and privacy protections for authentic review materials.

Method: Developed Mirror framework with EthicsLLM (fine-tuned on EthicsQA dataset of 41K question-chain-of-thought-answer triples). Features two modes: Mirror-ER for expedited review via executable rule base for minimal-risk studies, and Mirror-CR for committee review simulation with multi-agent deliberation across ethical dimensions.

Result: Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared to strong generalist LLMs, providing structured, committee-level assessments across ten ethical dimensions.

Conclusion: Mirror represents an AI-assisted approach to ethics review that addresses current limitations by integrating ethical reasoning with regulatory structures, offering scalable support for modern research governance challenges.

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [28] [DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing](https://arxiv.org/abs/2602.13318)
*Daesik Jang,Morgan Lindsay Heisler,Linzi Xing,Yifei Li,Edward Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.AI

TL;DR: DECKBench introduces a benchmark for evaluating multi-agent slide generation and editing systems, focusing on content fidelity, organization, layout quality, and multi-turn instruction following, with a modular baseline system provided.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks and evaluation protocols fail to adequately measure the complexities of academic slide deck generation and editing, which requires faithful content selection, coherent organization, layout-aware rendering, and robust instruction following.

Method: Introduces DECKBench benchmark built on curated paper-to-slide pairs with simulated editing instructions, and implements a modular multi-agent baseline system that decomposes tasks into paper parsing/summarization, slide planning, HTML creation, and iterative editing.

Result: DECKBench systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and instruction following, exposing strengths and failure modes while providing actionable insights for improving slide generation systems.

Conclusion: This work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing systems, with code and data publicly available.

Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .

</details>


### [29] [Situation Graph Prediction: Structured Perspective Inference for User Modeling](https://arxiv.org/abs/2602.13319)
*Jisung Shin,Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: Perspective-Aware AI needs to model internal states like goals and emotions, but faces data limitations due to privacy concerns and lack of labels. The paper proposes Situation Graph Prediction (SGP) as an inverse inference task to reconstruct structured perspective representations from multimodal artifacts, using a synthetic data generation approach that aligns latent labels with observable traces.


<details>
  <summary>Details</summary>
Motivation: Progress in Perspective-Aware AI is limited by a data bottleneck: digital footprints are privacy-sensitive and perspective states (goals, emotions, contexts) are rarely labeled, making it difficult to model evolving internal states beyond mere preferences.

Method: Proposes Situation Graph Prediction (SGP) task that frames perspective modeling as inverse inference of structured, ontology-aligned representations from observable multimodal artifacts. Uses structure-first synthetic generation strategy that aligns latent labels and observable traces by design. Constructs a dataset and runs diagnostic study using retrieval-augmented in-context learning as proxy for supervision.

Result: Study with GPT-4o shows a gap between surface-level extraction and latent perspective inference, indicating latent-state inference is harder than surface extraction in controlled settings. Results suggest SGP is non-trivial and provide evidence supporting the structure-first data synthesis strategy.

Conclusion: Situation Graph Prediction presents a promising approach to overcome data limitations in perspective modeling through inverse inference and synthetic data generation, though challenges remain in bridging the gap between surface extraction and latent perspective inference.

Abstract: Perspective-Aware AI requires modeling evolving internal states--goals, emotions, contexts--not merely preferences. Progress is limited by a data bottleneck: digital footprints are privacy-sensitive and perspective states are rarely labeled. We propose Situation Graph Prediction (SGP), a task that frames perspective modeling as an inverse inference problem: reconstructing structured, ontology-aligned representations of perspective from observable multimodal artifacts. To enable grounding without real labels, we use a structure-first synthetic generation strategy that aligns latent labels and observable traces by design. As a pilot, we construct a dataset and run a diagnostic study using retrieval-augmented in-context learning as a proxy for supervision. In our study with GPT-4o, we observe a gap between surface-level extraction and latent perspective inference--indicating latent-state inference is harder than surface extraction under our controlled setting. Results suggest SGP is non-trivial and provide evidence for the structure-first data synthesis strategy.

</details>


### [30] [Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol](https://arxiv.org/abs/2602.13320)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.AI

TL;DR: LLM agents using external tools accumulate errors linearly with bounded deviations, enabling predictable system behavior and actionable deployment guidelines.


<details>
  <summary>Details</summary>
Motivation: As AI agents powered by LLMs increasingly rely on external tools for high-stakes decisions, there's a critical need to understand how errors propagate across sequential tool calls to ensure system reliability.

Method: Developed a theoretical framework for analyzing error accumulation in MCP agents, using a hybrid distortion metric combining discrete fact matching with continuous semantic similarity. Established martingale concentration bounds on error propagation through sequential tool interactions.

Result: Proved cumulative distortion exhibits linear growth and high-probability deviations bounded by O(√T), ensuring predictable system behavior. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate theoretical predictions, showing semantic weighting reduces distortion by 80%, and periodic re-grounding every ~9 steps suffices for error control.

Conclusion: The concentration guarantees provide actionable deployment principles for trustworthy agent systems, ruling out exponential failure modes and enabling reliable error management in sequential tool-calling scenarios.

Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context Protocol (MCP) agents, proving that cumulative distortion exhibits linear growth and high-probability deviations bounded by $O(\sqrt{T})$. This concentration property ensures predictable system behavior and rules out exponential failure modes. We develop a hybrid distortion metric combining discrete fact matching with continuous semantic similarity, then establish martingale concentration bounds on error propagation through sequential tool interactions. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate our theoretical predictions, showing empirical distortion tracks the linear trend with deviations consistently within $O(\sqrt{T})$ envelopes. Key findings include: semantic weighting reduces distortion by 80\%, and periodic re-grounding approximately every 9 steps suffices for error control. We translate these concentration guarantees into actionable deployment principles for trustworthy agent systems.

</details>


### [31] [Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction](https://arxiv.org/abs/2602.13321)
*Tri Nguyen,Huy Hoang Bao Le,Lohith Srikanth Pentapalli,Laurah Turner,Kelly Cohen*

Main category: cs.AI

TL;DR: Using BERT-based models to extract linguistic features (Professionalism, Medical Relevance, Ethical Behavior, Contextual Distraction) and combining them with various classifiers for automated jailbreak detection in clinical LLMs.


<details>
  <summary>Details</summary>
Motivation: Prior manual annotation of linguistic features for jailbreak detection was not scalable or expressive enough. Need for automated, scalable approach to detect unsafe/off-task behavior in clinical LLM dialogues.

Method: 1. Use expert annotations of four core linguistic features. 2. Train general-domain and medical-domain BERT models to predict these features from text. 3. Select best feature regressors for each dimension. 4. Use extracted features as input to multiple classifier types (tree-based, linear, probabilistic, ensemble) for jailbreak detection.

Result: System achieves strong performance in cross-validation and held-out evaluations. LLM-derived linguistic features provide effective basis for automated jailbreak detection. Error analysis reveals limitations in current annotations and feature representations.

Conclusion: Demonstrates scalable, interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems. Future work needs richer annotations, finer-grained feature extraction, and methods to capture evolving risk over dialogue.

Abstract: Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.

</details>


### [32] [Contrastive explanations of BDI agents](https://arxiv.org/abs/2602.13323)
*Michael Winikoff*

Main category: cs.AI

TL;DR: Extended BDI agents to answer contrastive 'why X instead of Y?' questions. Contrastive explanations are shorter and may boost trust, but providing any explanation isn't always beneficial—sometimes it's worse than no explanation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for autonomous systems to provide explanations to support transparency and trust. While prior work enabled BDI agents to answer 'why did you do action X?' questions, the authors note that humans often ask contrastive questions (e.g., 'why did you do X instead of Y?'), which are not supported by existing explanation mechanisms.

Method: The authors extended prior work on BDI agents to enable them to answer contrastive questions. They conducted a computational evaluation to measure explanation length and a human subject evaluation to assess user preferences, trust development, transparency, and the impact of providing explanations.

Result: A computational evaluation showed that using contrastive questions yields a significant reduction in explanation length. A human subject evaluation found some evidence that contrastive answers are preferred and lead to higher trust, perceived understanding, and confidence in the system's correctness. Surprisingly, the study also found that providing a full explanation was sometimes worse than providing no explanation at all.

Conclusion: Contrastive explanations are more concise and may improve user trust and understanding compared to non-contrastive ones. However, the benefit of providing explanations at all is not clear-cut; in some cases, full explanations can be detrimental, suggesting that explanation design must be carefully tailored to context.

Abstract: The ability of autonomous systems to provide explanations is important for supporting transparency and aiding the development of (appropriate) trust. Prior work has defined a mechanism for Belief-Desire-Intention (BDI) agents to be able to answer questions of the form ``why did you do action $X$?''. However, we know that we ask \emph{contrastive} questions (``why did you do $X$ \emph{instead of} $F$?''). We therefore extend previous work to be able to answer such questions. A computational evaluation shows that using contrastive questions yields a significant reduction in explanation length. A human subject evaluation was conducted to assess whether such contrastive answers are preferred, and how well they support trust development and transparency. We found some evidence for contrastive answers being preferred, and some evidence that they led to higher trust, perceived understanding, and confidence in the system's correctness. We also evaluated the benefit of providing explanations at all. Surprisingly, there was not a clear benefit, and in some situations we found evidence that providing a (full) explanation was worse than not providing any explanation.

</details>


### [33] [Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts](https://arxiv.org/abs/2602.13367)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Xiyun Xu,Yang Song,Yiming Jia,Yuntao Wen,Yunzhi Xu,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.AI

TL;DR: Nanbeige4.1-3B is a 3B-parameter small language model that achieves strong agentic behavior, code generation, and general reasoning in a single unified model, outperforming larger models through innovative training techniques.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that small language models can achieve both broad competence and strong specialization simultaneously, challenging the assumption that versatility requires large-scale models, and to redefine the potential of 3B parameter models.

Method: Combines point-wise and pair-wise reward modeling for reasoning and preference alignment; uses complexity-aware rewards in reinforcement learning for code generation; performs complex data synthesis and incorporates turn-level supervision for deep search; enables stable long-horizon tool interactions.

Result: Outperforms prior similar-scale models (Nanbeige4-3B-2511, Qwen3-4B) and even achieves superior performance compared to much larger models (Qwen3-30B-A3B); can reliably execute up to 600 tool-call turns for complex problem-solving.

Conclusion: Small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models and challenging assumptions about model size versus capability trade-offs.

Abstract: We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.

</details>


### [34] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: Introduces Morality Chains (norm formalism) and MoralityGym (98 ethical-dilemma benchmark) to evaluate moral alignment in agents navigating conflicting hierarchical norms, revealing limitations in current Safe RL methods.


<details>
  <summary>Details</summary>
Motivation: Evaluating moral alignment in AI agents that must navigate conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. There's a need for principled approaches to ethical decision-making in complex real-world contexts.

Method: Introduces Morality Chains - a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym - a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. Decouples task-solving from moral evaluation and introduces a novel Morality Metric to integrate insights from psychology and philosophy into norm-sensitive reasoning evaluation.

Result: Baseline results with Safe RL methods reveal key limitations in handling ethical dilemmas with conflicting hierarchical norms, underscoring the inadequacy of current approaches and the need for more principled methods for ethical decision-making.

Conclusion: Provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts by establishing a formal framework and benchmark for evaluating moral alignment in norm-sensitive reasoning.

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [35] [On-Policy Supervised Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.13407)
*Anhao Zhao,Ziyang Chen,Junlong Tong,Yingqi Fan,Fanghua Ye,Shuhao Li,Yunpu Ma,Wenjie Li,Xiaoyu Shen*

Main category: cs.AI

TL;DR: Large reasoning models trained with complex RL methods can be simplified to on-policy supervised fine-tuning with a simple length penalty, achieving better performance with significantly reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Current RL-based training for large reasoning models is computationally expensive, and recent multi-reward approaches for optimizing correctness and brevity often destabilize training and yield suboptimal trade-offs. The authors question whether such complexity is truly necessary.

Method: The authors identify two problematic components in current approaches: KL regularization loses its role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple rewards. By removing these and using a simple truncation-based length penalty, the optimization reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness - termed "on-policy SFT".

Result: On-policy SFT consistently defines the accuracy-efficiency Pareto frontier across five benchmarks, reducing chain-of-thought length by up to 80% while maintaining original accuracy. It also significantly improves training efficiency: 50% reduction in GPU memory usage and 70% faster convergence compared to complex RL-based methods.

Conclusion: The simplicity of on-policy SFT outperforms complex RL-based methods for training large reasoning models, achieving better accuracy-efficiency trade-offs with substantially reduced computational requirements, challenging the necessity of overly complex training paradigms.

Abstract: Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.

</details>


### [36] [NeuroWeaver: An Autonomous Evolutionary Agent for Exploring the Programmatic Space of EEG Analysis Pipelines](https://arxiv.org/abs/2602.13473)
*Guoan Wang,Shihao Yang,Jun-En Ding,Hao Zhu,Feng Liu*

Main category: cs.AI

TL;DR: NeuroWeaver is an evolutionary agent that automatically designs lightweight EEG analysis pipelines by searching within neuroscientifically plausible subspaces, achieving performance comparable to large foundation models with far fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for EEG analysis require substantial data and have high computational costs, making them impractical for clinical environments. General-purpose AutoML frameworks often fail to incorporate neurophysiological priors, producing implausible solutions.

Method: Reformulates EEG pipeline engineering as a discrete constrained optimization problem. Uses Domain-Informed Subspace Initialization to limit search to neuroscientifically plausible manifolds, and Multi-Objective Evolutionary Optimization that balances performance, novelty, and efficiency through self-reflective refinement.

Result: Evaluations across five heterogeneous benchmarks show NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models while using significantly fewer parameters.

Conclusion: NeuroWeaver addresses the limitations of both foundation models and general AutoML for EEG analysis by incorporating domain knowledge to efficiently discover lightweight, scientifically plausible solutions suitable for resource-constrained clinical environments.

Abstract: Although foundation models have demonstrated remarkable success in general domains, the application of these models to electroencephalography (EEG) analysis is constrained by substantial data requirements and high parameterization. These factors incur prohibitive computational costs, thereby impeding deployment in resource-constrained clinical environments. Conversely, general-purpose automated machine learning frameworks are often ill-suited for this domain, as exploration within an unbounded programmatic space fails to incorporate essential neurophysiological priors and frequently yields solutions that lack scientific plausibility. To address these limitations, we propose NeuroWeaver, a unified autonomous evolutionary agent designed to generalize across diverse EEG datasets and tasks by reformulating pipeline engineering as a discrete constrained optimization problem. Specifically, we employ a Domain-Informed Subspace Initialization to confine the search to neuroscientifically plausible manifolds, coupled with a Multi-Objective Evolutionary Optimization that dynamically balances performance, novelty, and efficiency via self-reflective refinement. Empirical evaluations across five heterogeneous benchmarks demonstrate that NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models, despite utilizing significantly fewer parameters.

</details>


### [37] [OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage](https://arxiv.org/abs/2602.13477)
*Akshat Naik,Jay Culligan,Yarin Gal,Philip Torr,Rahaf Aljundi,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: A novel OMNI-LEAK attack demonstrates how multi-agent LLM systems with orchestrator patterns can leak sensitive data through indirect prompt injection despite data access controls, highlighting vulnerabilities across both reasoning and non-reasoning models.


<details>
  <summary>Details</summary>
Motivation: As LLM agents evolve into multi-agent systems with orchestrator setups becoming a practical paradigm, there's a scarcity of threat modeling for multi-agent security. Previous work has focused on single-agent risks or setups missing basic safeguards, while multi-agent systems with access controls present new security challenges that need investigation.

Method: The researchers investigated security vulnerabilities in the popular orchestrator setup pattern where a central agent delegates tasks to specialized agents. They conducted red-teaming on a concrete setup representing likely future use cases, focusing on novel attack vectors like OMNI-LEAK that compromise multiple agents through single indirect prompt injections, even with data access controls in place.

Result: The study demonstrated that frontier LLM models (both reasoning and non-reasoning types) are vulnerable to OMNI-LEAK attacks, which can leak sensitive data by compromising several agents through a single indirect prompt injection. These attacks work even when attackers lack insider knowledge of implementation details, and they succeed despite the presence of data access control mechanisms.

Conclusion: Safety research must generalize from single-agent to multi-agent settings to address serious risks of real-world privacy breaches, financial losses, and erosion of public trust in AI agents. The OMNI-LEAK attack reveals critical security vulnerabilities in orchestrated multi-agent systems that need urgent attention as these systems become more widely deployed.

Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.

</details>


### [38] [Translating Dietary Standards into Healthy Meals with Minimal Substitutions](https://arxiv.org/abs/2602.13502)
*Trevor Chan,Ilias Tagkopoulos*

Main category: cs.AI

TL;DR: An end-to-end framework that converts dietary standards into complete meals by identifying 34 meal archetypes from real data and using them to condition generative models, creating nutritious, affordable meals with minimal food substitutions.


<details>
  <summary>Details</summary>
Motivation: To improve nutritional quality in personalized diet systems without compromising convenience or affordability, aiming to make dietary guidelines practical for everyday use.

Method: Analyzed WWEIA intake data for 135,491 meals to identify 34 interpretable meal archetypes. Used these archetypes to condition a generative model and portion predictor to meet USDA nutritional targets through minimal food substitutions.

Result: Generated meals improved adherence to recommended daily intake (RDI) targets by 47.0% while remaining compositionally close to real meals. With 1-3 food substitutions, meals became 10% more nutritious while reducing costs by 19-32% on average.

Conclusion: The framework successfully turns dietary guidelines into realistic, budget-aware meals, providing a foundation for clinical decision support, public-health programs, and consumer apps to deliver scalable, equitable improvements in everyday nutrition.

Abstract: An important goal for personalized diet systems is to improve nutritional quality without compromising convenience or affordability. We present an end-to-end framework that converts dietary standards into complete meals with minimal change. Using the What We Eat in America (WWEIA) intake data for 135,491 meals, we identify 34 interpretable meal archetypes that we then use to condition a generative model and a portion predictor to meet USDA nutritional targets. In comparisons within archetypes, generated meals are better at following recommended daily intake (RDI) targets by 47.0%, while remaining compositionally close to real meals. Our results show that by allowing one to three food substitutions, we were able to create meals that were 10% more nutritious, while reducing costs 19-32%, on average. By turning dietary guidelines into realistic, budget-aware meals and simple swaps, this framework can underpin clinical decision support, public-health programs, and consumer apps that deliver scalable, equitable improvements in everyday nutrition.

</details>


### [39] [SPILLage: Agentic Oversharing on the Web](https://arxiv.org/abs/2602.13516)
*Jaechul Roh,Eugene Bagdasarian,Hamed Haddadi,Ali Shahin Shamsabadi*

Main category: cs.AI

TL;DR: Web agents often overshare user info (Natural Agentic Oversharing). SPILLage framework reveals behavioral oversharing (clicks, scrolls) dominates content oversharing 5x. Reducing oversharing improves task success up to 17.9%.


<details>
  <summary>Details</summary>
Motivation: LLM-powered web agents automate tasks across the open web with access to user resources, interacting with third parties and leaving action traces. Need to understand how they handle user resources and whether they unintentionally disclose task-irrelevant information (Natural Agentic Oversharing).

Method: Introduce SPILLage framework characterizing oversharing along channel (content vs. behavior) and directness (explicit vs. implicit) dimensions; benchmark 180 tasks on live e-commerce sites with ground-truth annotations; evaluate 1,080 runs across two agentic frameworks and three backbone LLMs; test prompt-level mitigation and removal of task-irrelevant information.

Result: Oversharing is pervasive, with behavioral oversharing dominating content oversharing by 5x; effect persists/worsens under prompt-level mitigation; removing task-irrelevant information before execution improves task success by up to 17.9%.

Conclusion: Protecting privacy in web agents requires a broader view of 'output' that accounts for what agents do on the web, not just what they type, and reducing oversharing can improve task success.

Abstract: LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act "in the wild", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of "output" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.

</details>


### [40] [REMem: Reasoning with Episodic Memory in Language Agent](https://arxiv.org/abs/2602.13530)
*Yiheng Shu,Saisri Padmaja Jonnalagedda,Xiang Gao,Bernal Jiménez Gutiérrez,Weijian Qi,Kamalika Das,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: REMem is a two-phase framework for episodic memory in language agents that converts experiences into a hybrid memory graph and uses agentic retrieval for reasoning, outperforming state-of-the-art memory systems on episodic recollection and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current language agents lack effective episodic memory capabilities - their memory remains mainly semantic rather than concrete and contextual like human episodic memory. Existing approaches overlook episodicity, lack explicit event modeling, or overemphasize simple retrieval over complex reasoning.

Method: Two-phase framework: 1) Offline indexing converts experiences into a hybrid memory graph linking time-aware gists and facts, 2) Online inference employs an agentic retriever with curated tools for iterative retrieval over the memory graph.

Result: REMem substantially outperforms state-of-the-art memory systems (Mem0 and HippoRAG 2) with 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks respectively across four benchmarks. It also demonstrates more robust refusal behavior for unanswerable questions.

Conclusion: REMem effectively addresses the episodic memory gap in language agents through its hybrid memory graph and agentic retrieval approach, enabling better recollection and reasoning over interaction histories compared to existing memory systems.

Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.

</details>


### [41] [OpAgent: Operator Agent for Web Navigation](https://arxiv.org/abs/2602.13559)
*Yuyu Guo,Wenjie Yang,Siyuan Yang,Ziyang Liu,Cheng Chen,Yuan Wei,Yun Hu,Yang Huang,Guoliang Hao,Dongsheng Yuan,Jianming Wang,Xin Chen,Hang Yu,Lei Lei,Peng Di*

Main category: cs.AI

TL;DR: The paper introduces OpAgent, an online reinforcement learning web agent that achieves state-of-the-art performance on web navigation tasks through hierarchical fine-tuning, online RL with hybrid rewards, and a modular framework for error recovery.


<details>
  <summary>Details</summary>
Motivation: Conventional methods (SFT or offline RL) suffer from distributional shifts when dealing with real-world websites due to the volatile nature of web environments and static datasets that fail to capture stochastic state transitions and real-time feedback.

Method: Three core innovations: 1) Hierarchical Multi-Task Fine-tuning with datasets for Planning, Acting, and Grounding; 2) Online RL with Hybrid Reward Mechanism (WebJudge + Rule-based Decision Tree); 3) OpAgent modular framework with Planner, Grounder, Reflector, and Summarizer components.

Result: The RL-enhanced model achieves 38.1% success rate (pass@5) on WebArena, outperforming all monolithic baselines. OpAgent achieves state-of-the-art performance with 71.6% success rate on WebArena through robust error recovery and self-correction.

Conclusion: Online reinforcement learning with direct web interaction and a modular agentic framework (OpAgent) significantly improves web navigation capabilities, overcoming limitations of static dataset approaches and enabling robust performance on complex real-world websites.

Abstract: To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \textbf{71.6\%}.

</details>


### [42] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: LLMs show human-like social influence patterns, conforming more to responses labeled as human experts than to other LLMs, even when incorrect.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs exhibit human-like patterns of social influence where they privilege feedback from humans over other LLMs, and whether they're sensitive to credibility signals like expert framing and consensus strength.

Method: Conducted three experiments across binary decision tasks (reading comprehension, multi-step reasoning, moral judgment) with four instruction-tuned LLMs. In first experiment: presented prior responses attributed to friends, human experts, or other LLMs, manipulating correctness and group size. In second experiment: introduced direct disagreement between single human and single LLM.

Result: LLMs conform significantly more to responses labeled as coming from human experts, including when incorrect, and revise answers toward experts more readily than toward other LLMs. Expert framing acts as strong prior that generalizes across decision domains.

Conclusion: Contemporary LLMs exhibit credibility-sensitive social influence that resembles human patterns, showing they privilege human feedback over LLM feedback through expert framing mechanisms.

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [43] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: Proposes a self-supervised differentiable clustering integrated with differentiable ILP to learn rules from raw data without label leakage.


<details>
  <summary>Details</summary>
Motivation: Differentiable ILP methods struggle with explicit label leakage when learning from raw data, lacking ability to map continuous inputs to symbolic variables without explicit supervision.

Method: Integrates self-supervised differentiable clustering with a novel differentiable ILP model to enable rule learning directly from raw data without explicit label leakage.

Result: The method learns generalized rules from time series and image data, effectively describing raw data through its features.

Conclusion: Combining self-supervised clustering with differentiable ILP solves the label leakage problem and enables interpretable rule learning directly from raw data.

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [44] [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)
*Yi Li,Lianjie Cao,Faraz Ahmed,Puneet Sharma,Bingzhe Li*

Main category: cs.AI

TL;DR: Hippocampus is an agentic memory system using binary signatures for semantic search and token-ID streams for exact reconstruction, with a Dynamic Wavelet Matrix for compression and fast search, achieving up to 31× faster retrieval and 14× lower token footprint.


<details>
  <summary>Details</summary>
Motivation: Agentic AI needs persistent memory beyond LLM context windows, but existing systems using dense vector databases or knowledge graphs suffer from high retrieval latency and poor storage scalability.

Method: Introduced Hippocampus with compact binary signatures for semantic search and lossless token-ID streams for content reconstruction, using a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to enable fast search in compressed domain.

Result: Evaluation shows up to 31× reduction in end-to-end retrieval latency and up to 14× reduction in per-query token footprint while maintaining accuracy on LoCoMo and LongMemEval benchmarks.

Conclusion: Hippocampus provides scalable, efficient memory management for agentic AI with linear scaling in memory size, making it suitable for long-horizon deployments.

Abstract: Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\times$ and cuts per-query token footprint by up to 14$\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.

</details>


### [45] [The Quantization Trap: Breaking Linear Scaling Laws in Multi-Hop Reasoning](https://arxiv.org/abs/2602.13595)
*Henry Han,Xiyang Liu,Xiaodong Wang,Fei Han,Xiaodong Li*

Main category: cs.AI

TL;DR: Neural scaling laws break for multi-hop reasoning tasks due to a "quantization trap" where reducing precision from 16-bit to 8/4-bit paradoxically increases energy consumption while degrading accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the conventional wisdom that reducing numerical precision linearly improves computational efficiency and energy profile according to neural scaling laws. The authors investigate whether this holds true for complex reasoning tasks.

Method: The authors demonstrate the quantization trap phenomenon through experiments and provide a rigorous theoretical decomposition that attributes the failure to hardware casting overhead (hidden latency cost of dequantization kernels) and sequential energy amortization failure in sequential reasoning chains.

Result: Reducing precision from 16-bit to 8/4-bit actually increases net energy consumption while degrading reasoning accuracy in multi-hop reasoning tasks. The scaling law breaking is shown to be unavoidable in practice due to hardware bottlenecks.

Conclusion: The industry's "smaller-is-better" heuristic for quantization is mathematically counterproductive for complex reasoning tasks. Neural scaling laws don't hold for multi-hop reasoning due to hardware overhead that becomes dominant in sequential reasoning chains.

Abstract: Neural scaling laws provide a predictable recipe for AI advancement: reducing numerical precision should linearly improve computational efficiency and energy profile (E proportional to bits). In this paper, we demonstrate that this scaling law breaks in the context of multi-hop reasoning. We reveal a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases more net energy consumption while degrading reasoning accuracy. We provide a rigorous theoretical decomposition that attributes this failure to hardware casting overhead, the hidden latency cost of dequantization kernels, which becomes a dominant bottleneck in sequential reasoning chains, as well as to a sequential energy amortization failure. As a result, scaling law breaking is unavoidable in practice. Our findings suggest that the industry's "smaller-is-better" heuristic is mathematically counterproductive for complex reasoning tasks.

</details>


### [46] [DiffusionRollout: Uncertainty-Aware Rollout Planning in Long-Horizon PDE Solving](https://arxiv.org/abs/2602.13616)
*Seungwoo Yoo,Juil Koo,Daehyeon Choi,Minhyuk Sung*

Main category: cs.AI

TL;DR: DiffusionRollout improves long-term PDE prediction by using uncertainty quantification to adaptively select step sizes, reducing error accumulation in autoregressive diffusion models.


<details>
  <summary>Details</summary>
Motivation: Autoregressive diffusion models for PDE prediction suffer from error accumulation in long-horizon predictions. The authors want to mitigate this by leveraging the models' ability to quantify predictive uncertainty.

Method: The authors propose DiffusionRollout, a selective rollout strategy that uses the standard deviation from multiple samples as a proxy for predictive uncertainty. This uncertainty measure guides adaptive step size selection during autoregressive rollouts to reduce conditioning on inaccurate prior outputs.

Result: Extensive evaluation on long-trajectory PDE benchmarks shows lower prediction errors and longer predicted trajectories that maintain high correlation with ground truths, validating both the uncertainty measure and adaptive planning strategy.

Conclusion: DiffusionRollout effectively reduces error accumulation in long-horizon PDE predictions by leveraging uncertainty quantification for adaptive planning, improving the reliability of autoregressive diffusion models for physical system forecasting.

Abstract: We propose DiffusionRollout, a novel selective rollout planning strategy for autoregressive diffusion models, aimed at mitigating error accumulation in long-horizon predictions of physical systems governed by partial differential equations (PDEs). Building on the recently validated probabilistic approach to PDE solving, we further explore its ability to quantify predictive uncertainty and demonstrate a strong correlation between prediction errors and standard deviations computed over multiple samples-supporting their use as a proxy for the model's predictive confidence. Based on this observation, we introduce a mechanism that adaptively selects step sizes during autoregressive rollouts, improving long-term prediction reliability by reducing the compounding effect of conditioning on inaccurate prior outputs. Extensive evaluation on long-trajectory PDE prediction benchmarks validates the effectiveness of the proposed uncertainty measure and adaptive planning strategy, as evidenced by lower prediction errors and longer predicted trajectories that retain a high correlation with their ground truths.

</details>


### [47] [Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization](https://arxiv.org/abs/2602.13653)
*Yibo Wang,Guangda Huzhang,Yuwei Hu,Yu Xia,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.AI

TL;DR: A novel MLLM-centered framework for GUI agents using agentic-Q estimation and step-wise policy optimization to handle non-stationary environments while minimizing data collection costs and enabling stable optimization.


<details>
  <summary>Details</summary>
Motivation: Real-world GUI agents face non-stationary environments, leading to high computational costs for data curation and policy optimization. Current approaches struggle with these practical challenges.

Method: Two-component framework: 1) agentic-Q estimation to optimize a Q-model that evaluates action contributions to task completion, 2) step-wise policy optimization using reinforcement learning with the agentic-Q model. Trajectories are generated by the policy itself, minimizing data costs, and policy updates are decoupled from the environment.

Result: The framework enables Ovis2.5-9B to achieve powerful GUI interaction capabilities, demonstrating remarkable performance on GUI navigation and grounding benchmarks, even surpassing larger-scale contenders.

Conclusion: The proposed MLLM-centered framework successfully addresses challenges in GUI agent deployment by providing efficient data collection and stable optimization, enabling effective performance despite environmental non-stationarity.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.

</details>


### [48] [HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating](https://arxiv.org/abs/2602.13665)
*Weibin Liao,Jian-guang Lou,Haoyi Xiong*

Main category: cs.AI

TL;DR: HyFunc is a framework that eliminates three redundancies in LLM-based function calling systems: redundant function library processing, redundant large model usage for predictable sequences, and redundant boilerplate syntax generation.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems using LLMs for function calling suffer from high inference latency due to computational redundancies: repeatedly processing large function libraries, using large models for predictable token sequences, and generating repetitive boilerplate syntax, hindering real-time applications.

Method: HyFunc uses a hybrid-model cascade where a large model distills user intent into a "soft token," which guides a lightweight retriever for function selection and directs a smaller prefix-tuned model for final call generation. It employs "dynamic templating" to inject boilerplate syntax on-the-fly within vLLM engine.

Result: On unseen BFCL benchmark: inference latency of 0.828 seconds (outperforming all baselines) and performance of 80.1% (surpassing all comparable-scale models). Shows excellent efficiency-performance balance.

Conclusion: HyFunc systematically eliminates key redundancies in agentic AI function calling, achieving superior efficiency and performance balance, offering a more efficient paradigm for real-time agentic AI applications.

Abstract: While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single "soft token." This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our "dynamic templating" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.

</details>


### [49] [AllMem: A Memory-centric Recipe for Efficient Long-context Modeling](https://arxiv.org/abs/2602.13680)
*Ziming Wang,Xiang Wang,Kailong Peng,Lang Qin,Juan Gabriel Kostelec,Christos Sourmpis,Axel Laborieux,Qinghai Guo*

Main category: cs.AI

TL;DR: AllMem is a hybrid architecture combining sliding window attention with non-linear test-time training memory networks to enable efficient ultra-long context processing in LLMs while mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: LLMs face significant performance bottlenecks in long-sequence tasks due to computational complexity and memory overhead of self-attention, along with issues of catastrophic forgetting and representation constraints in existing memory models.

Method: Integration of Sliding Window Attention with non-linear Test-Time Training memory networks, plus a Memory-Efficient Fine-Tuning strategy that replaces standard attention layers in pre-trained models with memory-augmented sliding window layers.

Result: The 4k window model achieves near-lossless performance on 37k LongBench with only 0.83 drop compared to full attention. The 8k window variant outperforms full attention on InfiniteBench at 128k context, validating effective noise mitigation and robust long-range modeling without global attention costs.

Conclusion: AllMem provides an efficient framework for transforming any pre-trained LLM into an architecture capable of handling ultra-long contexts while overcoming representation constraints of linear memory models and significantly reducing computational/memory footprint during inference.

Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \textsc{AllMem}, a novel and efficient hybrid architecture that integrates Sliding Window Attention (SWA) with non-linear Test-Time Training (TTT) memory networks. \textsc{AllMem} enables models to effectively scale to ultra-long contexts while mitigating catastrophic forgetting. This approach not only overcomes the representation constraints typical of linear memory models but also significantly reduces the computational and memory footprint during long-sequence inference. Furthermore, we implement a Memory-Efficient Fine-Tuning strategy to replace standard attention layers in pre-trained models with memory-augmented sliding window layers. This framework facilitates the efficient transformation of any off-the-shelf pre-trained LLM into an \textsc{AllMem}-based architecture. Empirical evaluations confirm that our 4k window model achieves near-lossless performance on 37k LongBench with a marginal 0.83 drop compared to full attention. Furthermore, on InfiniteBench at a 128k context, our 8k window variant outperforms full attention, which validates the effectiveness of our parameterized memory in mitigating noise and maintaining robust long-range modeling without the prohibitive costs of global attention.

</details>


### [50] [PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning](https://arxiv.org/abs/2602.13691)
*Yu Li,Guangfeng Cai,Shengtian Yang,Han Luo,Shuo Han,Xu He,Dong Li,Lei Feng*

Main category: cs.AI

TL;DR: PhGPO uses historical successful trajectories to learn reusable tool-transition patterns (like pheromone in ant colony optimization) to guide policy optimization for better long-horizon tool planning in LLM agents.


<details>
  <summary>Details</summary>
Motivation: Long-horizon multi-step tool planning in LLM agents is challenging due to combinatorial explosion in the exploration space. Even when correct tool-use paths are found, they only provide immediate rewards without reusable information for subsequent training. The authors argue that historically successful trajectories contain reusable tool-transition patterns that can be leveraged throughout the training process.

Method: The authors propose Pheromone-Guided Policy Optimization (PhGPO), which learns trajectory-based transition patterns (pheromone) from historical successful trajectories and uses this learned pheromone to explicitly guide policy optimization toward historically successful tool transitions.

Result: Comprehensive experimental results demonstrate the effectiveness of PhGPO in improving long-horizon tool planning for LLM agents.

Conclusion: Pheromone-Guided Policy Optimization (PhGPO) is an effective approach for improving long-horizon tool planning in LLM agents by leveraging historical trajectory patterns, offering a promising direction for future research in multi-step tool-use tasks.

Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.

</details>


### [51] [Can a Lightweight Automated AI Pipeline Solve Research-Level Mathematical Problems?](https://arxiv.org/abs/2602.13695)
*Lve Meng,Weilong Zhao,Yanzhi Zhang,Haoxiang Guan,Jiyan He*

Main category: cs.AI

TL;DR: LLMs integrated into citation-based verification pipelines can solve research-grade math problems, achieving verification success on competition-level and unpublished research problems.


<details>
  <summary>Details</summary>
Motivation: While LLMs have shown success in competition-level math and research through auto-formalization, their deployment via lightweight natural-language pipelines for research problems remains underexplored.

Method: A streamlined automated pipeline optimized for citation-based verification that integrates next-generation LLMs (Gemini 3 Pro, GPT-5.2 Pro) to generate candidate proofs for research problems.

Result: The pipeline generated candidate proofs for all problems in ICCM problem sets and the "First Proof" set. Solutions for ICCM sets and Problem 4 of "First Proof" were fully verified, with all proofs submitted to official organization.

Conclusion: LLM-based natural-language pipelines with citation-based verification can successfully solve sophisticated research-grade mathematical problems, demonstrating practical deployment potential beyond benchmark competitions.

Abstract: Large language models (LLMs) have recently achieved remarkable success in generating rigorous mathematical proofs, with "AI for Math" emerging as a vibrant field of research. While these models have mastered competition-level benchmarks like the International Mathematical Olympiad and show promise in research applications through auto-formalization, their deployment via lightweight, natural-language pipelines for research problems remains underexplored. In this work, we demonstrate that next-generation models (e.g., Gemini 3 Pro, GPT-5.2 Pro), when integrated into a streamlined automated pipeline optimized for citation-based verification, can solve sophisticated research-grade problems. We evaluate our pipeline on two novel datasets: (1) the ICCM problem sets (comparable to the S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and (2) the "First Proof" problem set, consisting of previously unpublished research questions. Our pipeline generated candidate proofs for all problems in the first two ICCM sets and the "First Proof" set. The solutions for the first two ICCM sets and Problem 4 of the "First Proof" set have been fully verified by our team. All generated proofs have been submitted to the official organization, and our generated results are publicly available. We plan to open-source the complete pipeline methodology in due course.

</details>


### [52] [No Need to Train Your RDB Foundation Model](https://arxiv.org/abs/2602.13697)
*Linjie Xu,Yanlin Zhang,Quan Gan,Minjie Wang,David Wipf*

Main category: cs.AI

TL;DR: RDBLearn is a training-free relational database foundation model that enables in-context learning across multiple tables by compressing database neighborhoods within columns rather than across columns, then pairing with existing single-table foundation models.


<details>
  <summary>Details</summary>
Motivation: Enterprise relational databases contain vast heterogeneous tabular data, but predicting new quantities typically requires retraining models for each target. While foundation models with in-context learning offer convenience, they're currently limited to single tables and can't handle multi-table relational structures.

Method: The paper introduces a principled family of RDB encoders that compress variably-sized RDB neighborhoods into fixed-length samples for in-context learning. Key innovation: compression is constrained WITHIN high-dimensional columns (where entities share units/roles) rather than ACROSS columns (where heterogeneous data types make relevance determination impossible without labels). The encoder uses no trainable parameters and can be implemented with scalable SQL primitives.

Result: The resulting RDB foundation model (RDBLearn) achieves robust performance on unseen datasets out-of-the-box without training or fine-tuning. It can be seamlessly paired with existing single-table in-context learning foundation models to handle multi-table relational databases.

Conclusion: The paper demonstrates that by restricting compression to within columns rather than across columns, encoder expressiveness is not compromised despite excluding trainable parameters. This enables practical, training-free foundation models for relational databases that can predict new quantities without retraining.

Abstract: Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \emph{within} high-dimensional RDB columns where all entities share units and roles, not \textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\footnote{\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.

</details>


### [53] [OneLatent: Single-Token Compression for Visual Latent Reasoning](https://arxiv.org/abs/2602.13738)
*Bo Lv,Yasheng Sun,Junjie Wang,Haoxiang Shi*

Main category: cs.AI

TL;DR: OneLatent compresses Chain-of-Thought reasoning into a single latent token by supervising from rendered CoT images and OCR hidden states, reducing output length 11x with minimal accuracy loss while enabling compression-constrained generalization.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought prompting improves reasoning but significantly increases inference cost by 1-2 orders of magnitude due to verbose textual rationales. There's a need to reduce computational overhead while maintaining reasoning capabilities.

Method: The OneLatent framework compresses intermediate reasoning steps into a single latent token using supervision from rendered CoT images and DeepSeek-OCR hidden states. Textual reasoning steps are rendered into images to obtain deterministic supervision that can be inspected and audited without requiring verbose textual output.

Result: Reduces average output length by 11x with only 2.21% average accuracy drop relative to textual CoT. Improves output token contribution by 6.8x. Achieves 99.80% on ProntoQA and 97.80% on ProsQA with one latent token, with compression up to 87.4x, demonstrating compression-constrained generalization.

Conclusion: OneLatent effectively addresses the high inference cost of Chain-of-Thought prompting by compressing reasoning into latent representations, offering significant reductions in output length while maintaining reasoning performance and enabling new applications through compression-constrained generalization.

Abstract: Chain-of-thought (CoT) prompting improves reasoning but often increases inference cost by one to two orders of magnitude. To address these challenges, we present \textbf{OneLatent}, a framework that compresses intermediate reasoning into a single latent token via supervision from rendered CoT images and DeepSeek-OCR hidden states. By rendering textual steps into images, we obtain a deterministic supervision signal that can be inspected and audited without requiring the model to output verbose textual rationales. Across benchmarks, OneLatent reduces average output length by $11\times$ with only a $2.21\%$ average accuracy drop relative to textual CoT, while improving output token contribution (OTC) by $6.8\times$. On long-chain logical reasoning, OneLatent reaches $99.80\%$ on ProntoQA and $97.80\%$ on ProsQA with one latent token, with compression up to $87.4\times$, supporting compression-constrained generalization.

</details>


### [54] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent is a configurable multi-agent research framework that organizes scientific exploration as a structured tree-based workflow with evolutionary-systematic ideation and hierarchical optimization-inspired reflection for automated discovery in complex experimental domains.


<details>
  <summary>Details</summary>
Motivation: Current automated scientific discovery approaches are limited to simple mutation-crossover loops and lack structured hypothesis management, environment interaction, and principled reflection needed for complex experiment-driven domains.

Method: OR-Agent organizes research as a structured tree-based workflow with: 1) evolutionary-systematic ideation (unifying evolutionary selection, comprehensive plan generation, and coordinated tree exploration); 2) hierarchical optimization-inspired reflection (short-term experimental reflection as verbal gradient, long-term reflection as verbal momentum, memory compression as regularization).

Result: The framework outperforms strong evolutionary baselines across classical combinatorial optimization benchmarks (traveling salesman, capacitated vehicle routing, bin packing, orienteering, multiple knapsack) and simulation-based cooperative driving scenarios, while providing general, extensible, and inspectable AI-assisted discovery.

Conclusion: OR-Agent successfully addresses limitations of existing automated discovery approaches by providing a principled architecture for structured hypothesis management, environment interaction, and reflection, enabling more effective exploration in complex experimental domains.

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [55] [StackingNet: Collective Inference Across Independent AI Foundation Models](https://arxiv.org/abs/2602.13792)
*Siyang Li,Chenhao Liu,Dongrui Wu,Zhigang Zeng,Lieyun Ding*

Main category: cs.AI

TL;DR: StackingNet is a meta-ensemble framework that coordinates multiple independent foundation models without accessing their internal parameters or training data, improving accuracy, robustness, and fairness by combining their predictions through collective intelligence principles.


<details>
  <summary>Details</summary>
Motivation: While large foundation models have advanced AI capabilities, they remain isolated systems that cannot readily share their complementary strengths. There's a need to integrate these independent models to build more trustworthy intelligent systems, but no established approach exists for coordinating such black-box heterogeneous models.

Method: StackingNet uses a meta-ensemble framework based on collective intelligence principles to combine predictions from multiple foundation models during inference. It operates without access to internal parameters or training data, instead coordinating the models through their outputs.

Result: StackingNet consistently improves accuracy, robustness, and fairness across language comprehension, visual estimation, and academic paper rating tasks. It reduces bias, enables reliability ranking, and can identify or prune models that degrade performance, outperforming both individual models and classic ensembles.

Conclusion: By turning model diversity into collaborative advantage rather than inconsistency, StackingNet establishes a practical foundation for coordinated AI, suggesting that progress may come from principled cooperation among specialized models rather than just larger single models.

Abstract: Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.

</details>


### [56] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: Vashista Sparse Attention: A theoretically-grounded sparse attention mechanism that identifies a small subset of meaningful tokens per query, enabling faster long-context inference with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: LLMs spend most inference cost on attention over long contexts, but empirical evidence suggests only a small subset of tokens meaningfully contributes to each query. Current attention mechanisms waste computation on irrelevant tokens.

Method: 1) Formalize attention as projection onto convex hull of key vectors with entropic relaxation; 2) Prove face-stability theorem showing attention concentrates on constant-size active face under strict complementarity margin; 3) Introduce Vashista Sparse Attention - a drop-in mechanism maintaining small candidate sets per query via paging-style context selection compatible with modern inference stacks.

Result: Theoretical: Under support gap Δ, entropic attention mass on inactive tokens decays exponentially exp(-Ω(Δ/ε)), error on active face scales linearly in temperature ε. Practical: Across long-context evaluations, observe stable constant-size effective support, strong wall-clock speedups, minimal quality degradation in regimes predicted by support-gap diagnostics.

Conclusion: Vashista enables efficient long-context decoding with predictable latency/cost, especially valuable for privacy-sensitive/air-gapped settings without external retrieval dependencies. Provides principled trade-off between accuracy and compute via temperature parameter.

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [57] [An end-to-end agentic pipeline for smart contract translation and quality evaluation](https://arxiv.org/abs/2602.13808)
*Abhinav Goel,Chaitya Shah,Agostino Capponi,Alfio Gliozzo*

Main category: cs.AI

TL;DR: An end-to-end framework for systematically evaluating LLM-generated smart contracts from natural-language specifications, featuring automated parsing, code generation, and quality assessment with iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Need for reproducible benchmarks and systematic evaluation of LLM-generated smart contracts to assess quality across multiple dimensions and identify systematic error modes.

Method: Parses contractual text into structured schemas, generates Solidity code using CrewAI-style agent teams with iterative refinement, and performs automated quality assessment through compilation checks, security checks, and evaluation across five quality dimensions.

Result: Creates structured artifacts with full provenance metadata and provides composite quality scores across functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality, enabling paired evaluation against ground-truth implementations.

Conclusion: The framework provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

Abstract: We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

</details>


### [58] [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)
*Zhengmian Hu,Lei Shi,Ritwik Sinha,Justin Grover,David Arbour*

Main category: cs.AI

TL;DR: A unified framework that uses historical A/B results and content embeddings to prioritize test variants, explain winners, and generate new creative opportunities for more efficient experimentation cycles.


<details>
  <summary>Details</summary>
Motivation: Modern online experimentation faces bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and content-agnostic. Organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration.

Method: Framework uses treatment embeddings and historical outcomes to train a CTR ranking model with fixed effects for contextual shifts. For interpretability, treatments are projected onto semantic marketing attributes and re-expressed via a sign-consistent, sparse constrained Lasso to get per-attribute coefficients. An opportunity index combines attribute importance with under-expression to flag missing high-impact attributes. LLMs translate opportunities into concrete creative suggestions.

Result: The framework has been built into a real Adobe product called "Experimentation Accelerator" and evaluated on real-world experiments by Adobe business customers, validating the high quality of the generation pipeline.

Conclusion: The proposed framework addresses key bottlenecks in online experimentation by enabling more efficient test cycles with AI-based insights, helping prioritize variants, explain results, and surface new creative opportunities at scale.

Abstract: Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.

</details>


### [59] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: MOC-2HER: A dual-objective hindsight relabeling approach for hierarchical RL that addresses sparse rewards and object manipulation tasks by rewarding both interaction and task completion.


<details>
  <summary>Details</summary>
Motivation: Hierarchical RL frameworks (Option-Critic, Multi-updates Option Critic) underperform in multi-goal environments with sparse rewards, especially in object manipulation tasks where rewards depend on object state rather than direct agent interaction.

Method: 1. First integrated HER into MOC framework (MOC-HER) to handle sparse rewards. 2. Introduced Dual Objectives HER (2HER) that creates two sets of virtual goals: one based on object's final state (standard HER) and another from agent's effector positions, rewarding both interaction and task completion.

Result: In robotic manipulation environments, MOC-2HER achieves success rates up to 90%, compared to less than 11% for both MOC and MOC-HER.

Conclusion: The dual-objective relabeling strategy (2HER) effectively solves sparse reward, multi-goal tasks in object manipulation, highlighting the importance of rewarding both agent-object interaction and task completion for hierarchical RL.

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [60] [Ambient Physics: Training Neural PDE Solvers with Partial Observations](https://arxiv.org/abs/2602.13873)
*Harris Abdul Majid,Giannis Daras,Francesco Tudisco,Steven McDonagh*

Main category: cs.AI

TL;DR: A diffusion-based framework that learns joint distribution of PDE coefficient-solution pairs directly from partial observations without requiring any complete observations, achieving state-of-the-art reconstruction with significantly fewer function evaluations.


<details>
  <summary>Details</summary>
Motivation: In scientific settings, acquiring complete observations of PDE coefficients and solutions is often expensive, hazardous, or impossible. Existing diffusion-based methods require complete observations for training, which is impractical in many real-world scenarios.

Method: The Ambient Physics framework randomly masks a subset of already-observed measurements during training, supervising the model on these masked observations. This prevents the model from distinguishing between "truly unobserved" and "artificially unobserved" measurements, forcing it to produce plausible predictions everywhere.

Result: Achieves state-of-the-art reconstruction performance with 62.51% reduction in average overall error compared to prior diffusion-based methods, while using 125× fewer function evaluations. Also identifies a "one-point transition" phenomenon where masking just one already-observed point enables learning from partial observations across architectures and measurement patterns.

Conclusion: Ambient Physics enables scientific progress in settings where complete observations are unavailable, providing a practical framework for learning PDE systems from partial observations without requiring any complete training data.

Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish "truly unobserved" from "artificially unobserved", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\%$ reduction in average overall error while using 125$\times$ fewer function evaluations. We also identify a "one-point transition": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.

</details>


### [61] [VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection](https://arxiv.org/abs/2602.13880)
*Jiahao Xie,Guangmo Tong*

Main category: cs.AI

TL;DR: VSAL is a vision-based graph property detection framework that uses adaptive graph layouts instead of fixed layouts, outperforming state-of-the-art methods on various graph property detection tasks.


<details>
  <summary>Details</summary>
Motivation: Current vision-based graph property detection methods rely on fixed visual graph layouts, which limits their expressiveness and effectiveness. The authors aim to overcome this limitation by developing a more flexible approach that can adapt visualizations to individual graph instances.

Method: VSAL (Vision-based method with adaptive layout) incorporates an adaptive layout generator that dynamically produces informative graph visualizations tailored to individual instances, rather than using fixed layouts. This adaptive approach enhances the vision-based pipeline for graph property detection.

Result: Extensive experiments show that VSAL outperforms state-of-the-art vision-based methods on various graph property detection tasks including Hamiltonian cycle detection, planarity detection, claw-freeness detection, and tree detection.

Conclusion: Adaptive graph layouts significantly improve vision-based graph property detection, and VSAL demonstrates superior performance across multiple graph property detection tasks compared to fixed-layout approaches.

Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.

</details>


### [62] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: The paper proposes metrics to detect three Chain-of-Thought reasoning pathologies: post-hoc rationalization, encoded reasoning, and internalized reasoning, using deliberately trained model organisms for validation.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning is fundamental to modern LLMs and critical for AI safety, but it exhibits pathologies that prevent it from being useful for monitoring. These include post-hoc rationalization, encoded reasoning, and internalized reasoning, which undermine interpretability and reliability of reasoning processes.

Method: The authors create concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic to detect CoT pathologies. They validate their approach by developing model organisms deliberately trained to exhibit specific CoT pathologies.

Result: The work provides a practical toolkit for assessing CoT pathologies through validated metrics, with direct implications for training-time monitoring of LLMs.

Conclusion: The proposed metrics offer a practical solution for detecting and understanding CoT reasoning pathologies, addressing critical safety concerns in LLM architectures and enabling more reliable monitoring of reasoning processes.

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [63] [From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design](https://arxiv.org/abs/2602.13912)
*Sha Li,Stefano Petrangeli,Yu Shen,Xiang Chen*

Main category: cs.AI

TL;DR: LaySPA is an RL framework that gives LLMs explicit spatial reasoning for graphic layout design, producing interpretable reasoning traces and structured layout specs through multi-objective optimization.


<details>
  <summary>Details</summary>
Motivation: Addresses two key challenges: LLMs' limited spatial reasoning capabilities and the lack of transparency in design decision making for graphic layout design tasks.

Method: Reformulates layout design as policy learning over a structured textual spatial environment encoding canvas geometry, element attributes, and relationships. Uses multi-objective spatial critique (geometric validity, relational coherence, aesthetic consistency) and relative group optimization for stable learning.

Result: LaySPA improves structural validity and visual quality, outperforms larger proprietary LLMs, achieves performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

Conclusion: LaySPA successfully equips LLMs with explicit spatial reasoning for graphic layout design, providing interpretable and controllable design decisions through structured textual spatial environment learning.

Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

</details>


### [64] [HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling](https://arxiv.org/abs/2602.13933)
*Xiaochen Zhao,Kaikai Wang,Xiaowen Zhang,Chen Yao,Aili Wang*

Main category: cs.AI

TL;DR: HyMem is a hybrid memory architecture for LLM agents that uses multi-granular memory representations and dynamic on-demand scheduling to balance efficiency and effectiveness in long dialogues.


<details>
  <summary>Details</summary>
Motivation: LLM agents perform well in short-text contexts but struggle with extended dialogues due to inefficient memory management. Existing approaches face a trade-off: memory compression loses critical details, while retaining raw text adds computational overhead. Current methods lack flexible, proactive memory scheduling like humans, failing to adapt to diverse problem scenarios.

Method: HyMem uses a hybrid memory architecture with multi-granular memory representations and dynamic on-demand scheduling. It employs a dual-granular storage scheme with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient responses, while an LLM-based deep module is selectively activated only for complex queries, enhanced by a reflection mechanism for iterative reasoning refinement.

Result: Experiments show HyMem achieves strong performance on both LOCOMO and LongMemEval benchmarks, outperforming full-context approaches while reducing computational cost by 92.6%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

Conclusion: HyMem successfully addresses the efficiency-effectiveness trade-off in LLM memory management by introducing a hybrid architecture with dynamic scheduling, demonstrating superior performance with significantly reduced computational costs, making it a promising approach for long-term dialogue systems.

Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

</details>


### [65] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: Statistically principled early stopping methods for LLMs that monitor uncertainty during generation to prevent overthinking and improve reasoning efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate unnecessary reasoning steps under uncertainty or ambiguous queries, leading to inefficient "overthinking" behavior that wastes computational resources.

Method: Two approaches: 1) Parametric method models inter-arrival times of uncertainty keywords as a renewal process with sequential testing; 2) Nonparametric method provides finite-sample guarantees on probability of halting too early on well-posed queries.

Result: Uncertainty-aware early stopping improves both efficiency and reliability in LLM reasoning, with especially significant gains observed for math reasoning tasks.

Conclusion: Monitoring uncertainty signals during generation and applying statistically principled early stopping can effectively mitigate LLM overthinking, enhancing reasoning performance and computational efficiency.

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [66] [A Generalizable Physics-guided Causal Model for Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2602.13936)
*Zhenyu Zong,Yuchen Wang,Haohong Lin,Lu Gan,Huajie Shao*

Main category: cs.AI

TL;DR: Physics-guided Causal Model (PCM) enhances zero-shot trajectory prediction for autonomous driving by extracting domain-invariant features through intervention-based disentanglement and integrating kinematics with context via causal attention.


<details>
  <summary>Details</summary>
Motivation: Achieving effective zero-shot generalization for trajectory prediction in unseen domains is challenging. The consistent nature of kinematics across diverse domains motivates incorporating domain-invariant knowledge to improve zero-shot trajectory prediction capabilities.

Method: Proposes Physics-guided Causal Model (PCM) with two core components: 1) Disentangled Scene Encoder using intervention-based disentanglement to extract domain-invariant features, and 2) CausalODE Decoder employing causal attention mechanism to integrate kinematic models with contextual information.

Result: Extensive experiments on real-world autonomous driving datasets demonstrate superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines.

Conclusion: The PCM framework effectively addresses challenges in zero-shot trajectory prediction by leveraging domain-invariant knowledge through disentangled scene encoding and causal integration of kinematics, showing promising generalization capabilities for autonomous driving applications.

Abstract: Trajectory prediction for traffic agents is critical for safe autonomous driving. However, achieving effective zero-shot generalization in previously unseen domains remains a significant challenge. Motivated by the consistent nature of kinematics across diverse domains, we aim to incorporate domain-invariant knowledge to enhance zero-shot trajectory prediction capabilities. The key challenges include: 1) effectively extracting domain-invariant scene representations, and 2) integrating invariant features with kinematic models to enable generalized predictions. To address these challenges, we propose a novel generalizable Physics-guided Causal Model (PCM), which comprises two core components: a Disentangled Scene Encoder, which adopts intervention-based disentanglement to extract domain-invariant features from scenes, and a CausalODE Decoder, which employs a causal attention mechanism to effectively integrate kinematic models with meaningful contextual information. Extensive experiments on real-world autonomous driving datasets demonstrate our method's superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines. The source code is released at https://github.com/ZY-Zong/Physics-guided-Causal-Model.

</details>


### [67] [Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs](https://arxiv.org/abs/2602.13967)
*Ruicheng Zhang,Xinyi Li,Tianyi Xu,Shuhao Zhang,Xiaofei Liao,Hai Jin*

Main category: cs.AI

TL;DR: Neuromem is a testbed for benchmarking External Memory Modules under realistic streaming conditions where memory evolves during query serving, evaluating performance across five lifecycle dimensions with three datasets.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of External Memory Modules assume static settings where memory is built offline and queried at fixed states, but in practice memory is streaming with continuous fact arrivals, interleaved insertions and retrievals, and evolving memory states during query serving.

Method: Presented Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol, decomposing the memory lifecycle into five dimensions: memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Evaluated interchangeable variants within a shared serving stack using three datasets (LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH), reporting token-level F1 and insertion/retrieval latency.

Result: Performance typically degrades as memory grows across rounds, time-related queries remain the most challenging category. Memory data structure largely determines attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

Conclusion: Neuromem provides a comprehensive framework for evaluating External Memory Modules under realistic streaming conditions, revealing key insights about performance degradation with memory growth and the fundamental role of memory data structure in determining quality limits.

Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

</details>


### [68] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: PIC (Parallelized Iterative Compression) is a novel context compression method for LLMs that restricts memory tokens to local chunks via attention mask modifications, improving training efficiency and performance in high compression scenarios.


<details>
  <summary>Details</summary>
Motivation: Long contexts in LLMs increase inference latency due to quadratic computational cost of self-attention. Existing soft prompt compression methods compress entire context indiscriminately into memory tokens, requiring global dependency capture and extensive pre-training data.

Method: PIC modifies Transformer's attention mask to restrict receptive field of memory tokens to sequential local chunks, inspired by human working memory chunking and spatial specialization observations. This parallelized iterative approach lowers compressor training difficulty.

Result: PIC consistently outperforms baselines across multiple downstream tasks, especially in high compression scenarios (29.8% F1 and 40.7% EM improvements on QA at 64× compression). Training is 40% faster while achieving better peak performance.

Conclusion: PIC provides an effective solution to context compression by leveraging local chunking via attention mask modifications, offering superior performance, faster training, and better scalability for high compression ratios.

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [69] [Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms](https://arxiv.org/abs/2602.13985)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: The paper proposes using formal abductive explanations to make AI clinical diagnostics more interpretable and aligned with clinical reasoning, while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: AI in clinical diagnostics often diverges from structured clinical frameworks, limiting trust and adoption. Critical symptoms may be overlooked even when predictions are correct, and existing explanation methods lack transparency and formal guarantees.

Method: The authors leverage formal abductive explanations, which provide consistent, guaranteed reasoning over minimal sufficient feature sets, to align AI decision-making with clinical reasoning.

Result: The approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.

Conclusion: Formal abductive explanations offer a solution to enhance trust, interpretability, and adoption of AI in clinical diagnostics by ensuring alignment with clinical reasoning frameworks.

Abstract: Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.

</details>


### [70] [Prompt-Driven Low-Altitude Edge Intelligence: Modular Agents and Generative Reasoning](https://arxiv.org/abs/2602.14003)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu*

Main category: cs.AI

TL;DR: P2AECF: A prompt-to-agent edge cognition framework that enables flexible, efficient, and adaptive deployment of large AI models on edge devices by converting semantic prompts into dynamic reasoning workflows with lightweight cognitive agents.


<details>
  <summary>Details</summary>
Motivation: Current deployment of large AI models (LAMs) at the edge faces three key limitations: 1) tasks are rigidly tied to specific models, limiting flexibility; 2) computational and memory demands of full-scale LAMs exceed edge device capacity; 3) static inference pipelines cannot adapt to real-time task changes.

Method: P2AECF transforms high-level semantic prompts into executable reasoning workflows through three mechanisms: 1) prompt-defined cognition that parses task intent into model-agnostic representations; 2) agent-based modular execution using lightweight, reusable cognitive agents dynamically selected based on resource conditions; 3) diffusion-controlled inference planning that adaptively constructs and refines execution strategies using runtime feedback and system context.

Result: The framework is demonstrated through a low-altitude intelligent network use case, showing its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.

Conclusion: P2AECF addresses the fundamental limitations of deploying LAMs at the edge by enabling flexible, efficient, and adaptive edge intelligence through a novel prompt-to-agent transformation framework with dynamic workflow construction.

Abstract: The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations. First, tasks are rigidly tied to specific models, limiting the flexibility. Besides, the computational and memory demands of full-scale LAMs exceed the capacity of most edge devices. Moreover, the current inference pipelines are typically static, making it difficult to respond to real-time changes of tasks. To address these challenges, we propose a prompt-to-agent edge cognition framework (P2AECF), enabling the flexible, efficient, and adaptive edge intelligence. Specifically, P2AECF transforms high-level semantic prompts into executable reasoning workflows through three key mechanisms. First, the prompt-defined cognition parses task intent into abstract and model-agnostic representations. Second, the agent-based modular execution instantiates these tasks using lightweight and reusable cognitive agents dynamically selected based on current resource conditions. Third, the diffusion-controlled inference planning adaptively constructs and refines execution strategies by incorporating runtime feedback and system context. In addition, we illustrate the framework through a representative low-altitude intelligent network use case, showing its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.

</details>


### [71] [FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning](https://arxiv.org/abs/2602.14035)
*Jinzi Zou,Bolin Wang,Liang Li,Shuo Zhang,Nuo Xu,Junzhou Zhao*

Main category: cs.AI

TL;DR: Flowchart-oriented dialogue systems face challenges with LLM hallucinations and lack of flowchart reasoning. FloCA addresses this by separating intent understanding from topology-constrained graph execution for faithful flowchart reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with flowchart-oriented dialogue due to lack of explicit flowchart topology representation and hallucination issues, leading to unfaithful reasoning about flowchart paths.

Method: Propose FloCA, a zero-shot flowchart-oriented conversational agent that uses LLM for intent understanding and response generation, but delegates flowchart reasoning to an external tool performing topology-constrained graph execution.

Result: Experimental results on FLODIAL and PFDial datasets show FloCA outperforms existing LLM-based methods, with evaluation framework using LLM-based user simulator and five new metrics for reasoning accuracy and interaction efficiency.

Conclusion: Separating intent understanding from flowchart reasoning with topology-constrained execution is effective for faithful flowchart-oriented dialogue, addressing LLM limitations in flowchart representation and hallucination.

Abstract: Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.

</details>


### [72] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: FluxMem is a unified framework for adaptive memory organization in LLM agents, improving performance by dynamically selecting memory structures based on interaction features.


<details>
  <summary>Details</summary>
Motivation: Existing agent memory systems use fixed memory structures without context-adaptive selection, limiting their ability to handle heterogeneous interaction patterns and leading to suboptimal performance.

Method: FluxMem equips agents with multiple complementary memory structures, learns to select among them using offline supervision from downstream response quality and memory utilization, and introduces a three-level memory hierarchy with a Beta Mixture Model-based gate for distribution-aware memory fusion.

Result: Experiments on PERSONAMEM and LoCoMo benchmarks show average improvements of 9.18% and 6.14%.

Conclusion: FluxMem effectively enhances LLM agent performance by enabling adaptive memory organization, addressing gaps in existing systems through learned structure selection and robust memory evolution.

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [73] [REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment](https://arxiv.org/abs/2602.14065)
*Kai Ye,Xianwei Mao,Sheng Zhou,Zirui Shao,Ye Mo,Liangliang Liu,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.AI

TL;DR: REAL framework addresses knowledge conflicts in KI-VQA using reasoning-pivots for conflict detection and mitigation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Knowledge-intensive Visual Question Answering (KI-VQA) suffers from severe knowledge conflicts caused by limitations of open-domain retrieval. Existing paradigms lack generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence.

Method: Proposes REAL framework centered on Reasoning-Pivots (atomic units in reasoning chains that emphasize knowledge linkage). Uses Reasoning-Pivot Aware SFT to train a generalizable discriminator by aligning conflicts with pivot extraction, and Reasoning-Pivot Guided Decoding as an intra-model decoding strategy for targeted conflict mitigation.

Result: Extensive experiments across diverse benchmarks demonstrate REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance.

Conclusion: The pivot-driven resolution paradigm is effective for handling knowledge conflicts in KI-VQA, as validated by improved performance across benchmarks.

Abstract: Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.

</details>


### [74] [Plan-MCTS: Plan Exploration for Action Exploitation in Web Navigation](https://arxiv.org/abs/2602.14083)
*Weiming Zhang,Jihong Wang,Jiamu Zhou,Qingyao Li,Xinbei Ma,Congmin Zheng,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: Plan-MCTS transforms web navigation by exploring semantic plan space instead of physical action space, using dense plan trees and abstracted semantic history to overcome sparse paths and noisy context challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional tree search algorithms for LLM-powered web navigation agents face two major challenges: sparse valid paths lead to inefficient exploration, and noisy web contexts dilute accurate state perception, limiting performance.

Method: Plan-MCTS reformulates web navigation by shifting exploration to semantic Plan Space, decoupling strategic planning from execution. It transforms sparse action space into Dense Plan Tree, distills noisy contexts into Abstracted Semantic History, uses Dual-Gating Reward for executability/alignment validation, and Structural Refinement for on-policy repair of failed subplans.

Result: Extensive experiments on WebArena show Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.

Conclusion: Plan-MCTS successfully addresses web navigation challenges by semantic plan space exploration, demonstrating improved efficiency and robustness over conventional approaches through its novel framework components.

Abstract: Large Language Models (LLMs) have empowered autonomous agents to handle complex web navigation tasks. While recent studies integrate tree search to enhance long-horizon reasoning, applying these algorithms in web navigation faces two critical challenges: sparse valid paths that lead to inefficient exploration, and a noisy context that dilutes accurate state perception. To address this, we introduce Plan-MCTS, a framework that reformulates web navigation by shifting exploration to a semantic Plan Space. By decoupling strategic planning from execution grounding, it transforms sparse action space into a Dense Plan Tree for efficient exploration, and distills noisy contexts into an Abstracted Semantic History for precise state awareness. To ensure efficiency and robustness, Plan-MCTS incorporates a Dual-Gating Reward to strictly validate both physical executability and strategic alignment and Structural Refinement for on-policy repair of failed subplans. Extensive experiments on WebArena demonstrate that Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.

</details>


### [75] [GUI-GENESIS: Automated Synthesis of Efficient Environments with Verifiable Rewards for GUI Agent Post-Training](https://arxiv.org/abs/2602.14093)
*Yuan Cao,Dezhi Ran,Mengzhou Wu,Yuzhe Guo,Xin Chen,Ang Li,Gang Cao,Gong Zhi,Hao Yu,Linyi Li,Wei Yang,Tao Xie*

Main category: cs.AI

TL;DR: GUI-GENESIS framework automatically synthesizes efficient web-based GUI training environments with code-native rewards to enable faster, cheaper, and more effective agent training compared to real-world applications.


<details>
  <summary>Details</summary>
Motivation: Training GUI agents on real-world applications faces challenges: high latency, poor reproducibility, and unverifiable rewards that rely on noisy visual proxies, which hinder developing generalization and long-horizon planning capabilities.

Method: GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards (executable assertions) that provide deterministic reward signals and eliminate visual estimation noise.

Result: The framework reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Agents trained with GUI-GENESIS outperform the base model by 14.54% and real-world RL baselines by 3.27% on held-out real-world tasks. Models can even synthesize environments they cannot yet solve.

Conclusion: GUI-GENESIS enables efficient GUI agent training with verifiable rewards, offering substantial performance gains, cost reductions, and a pathway for self-improving agents through environment synthesis.

Abstract: Post-training GUI agents in interactive environments is critical for developing generalization and long-horizon planning capabilities. However, training on real-world applications is hindered by high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies. To address the limitations, we present GUI-GENESIS, the first framework to automatically synthesize efficient GUI training environments with verifiable rewards. GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards, executable assertions that provide deterministic reward signals and eliminate visual estimation noise. Extensive experiments show that GUI-GENESIS reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Notably, agents trained with GUI-GENESIS outperform the base model by 14.54% and even real-world RL baselines by 3.27% on held-out real-world tasks. Finally, we observe that models can synthesize environments they cannot yet solve, highlighting a pathway for self-improving agents.

</details>


### [76] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: Researchers systematically evaluated 28 LLMs for steganographic chain-of-thought reasoning, finding current models struggle with complex math tasks but show early capabilities in simple counting, highlighting the need for continuous monitoring of hidden reasoning risks.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought monitoring is a key safety technique for LLM agents, but this oversight is compromised if models learn to conceal their reasoning through steganography, raising concerns about deception and misaligned scheming.

Method: Systematically evaluated 28 models from past generations to current frontier across four datasets. Measured monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy comparing steganographic acrostics against plain reasoning and filler-token baselines.

Result: Current models cannot sustain hidden reasoning for complex math and arithmetic tasks. In simplified counting, Claude Opus 4.5 achieved 92% hidden task accuracy. GPT-5.2 rarely (<1%) refused steganographic instructions while simultaneously complying.

Conclusion: The study demonstrates nascent steganographic capabilities in current models and provides methodology to preemptively detect and prevent hidden reasoning that could empower misaligned scheming and deceptive behavior, underscoring the need for continuous risk evaluation.

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [77] [Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity](https://arxiv.org/abs/2602.14130)
*Kazuo Yano,Jonghyeok Lee,Tae Ishitomi,Hironobu Kawaguchi,Akira Koyama,Masakuni Ota,Yuki Ota,Nobuo Sato,Keita Shimada,Sho Takematsu,Ayaka Tobinai,Satomi Tsuji,Kazunori Yanagi,Keiko Yano,Manabu Harada,Yuki Matsuda,Kazunori Matsumoto,Kenichi Matsumura,Hamae Matsuo,Yumi Miyazaki,Kotaro Murai,Tatsuya Ohshita,Marie Seki,Shun Tanoue,Tatsuki Terakado,Yuko Ichimaru,Mirei Saito,Akihiro Otsuka,Koji Ara*

Main category: cs.AI

TL;DR: The paper proposes Algebraic Quantum Intelligence (AQI), a noncommutative algebraic framework inspired by quantum theory, to expand the semantic space and enhance creativity in LLMs, showing improved performance on creative reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with genuine creativity because their generation becomes too deterministic when given rich context, strongly constraining future possibilities. Existing methods like test-time scaling don't fundamentally address this structural limitation.

Method: AQI introduces a noncommutative algebraic structure (inspired by quantum theory) with properties like order dependence, interference, and uncertainty. Semantic states are Hilbert space vectors evolved via C-values from noncommutative operators. They extend a transformer-based LLM with over 600 specialized operators.

Result: AQI consistently outperforms strong baselines on creative reasoning benchmarks across ten domains using LLM-as-a-judge protocol, showing statistically significant improvements and reduced cross-domain variance.

Conclusion: Noncommutative algebraic dynamics provide a practical, reproducible foundation for machine creativity, demonstrated by AQI's effectiveness and real-world deployment in enterprise environments.

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

</details>


### [78] [ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI](https://arxiv.org/abs/2602.14135)
*Haibo Tong,Feifei Zhao,Linghao Feng,Ruoyu Wu,Ruolin Chen,Lu Jia,Zhou Zhao,Jindong Li,Tenglong Li,Erliang Lin,Shuai Yang,Enmeng Lu,Yinqian Sun,Qian Zhang,Zizhe Ruan,Zeyang Yue,Ping Wu,Huangrui Li,Chengyi Sun,Yi Zeng*

Main category: cs.AI

TL;DR: ForesightSafety Bench is a comprehensive AI safety evaluation framework covering 94 risk dimensions across fundamental safety pillars, embodied AI, AI4Science, social/environmental risks, catastrophic risks, and 8 industrial domains, with analysis revealing widespread vulnerabilities in frontier AI models.


<details>
  <summary>Details</summary>
Motivation: Current AI safety evaluation systems have critical limitations including restricted risk dimensions and failure to detect frontier risks. Lagging safety benchmarks and alignment technologies cannot address complex challenges posed by cutting-edge AI models with increasing autonomy and goal-directed capabilities.

Method: Proposed the ForesightSafety Bench framework starting with 7 fundamental safety pillars and extending to advanced areas: Embodied AI Safety, AI4Science Safety, Social/Environmental AI risks, Catastrophic/Existential Risks, and 8 industrial safety domains, forming 94 refined risk dimensions. Accumulated tens of thousands of structured risk data points and assessment results.

Result: Systematic evaluation of over twenty mainstream advanced large models revealed widespread safety vulnerabilities across multiple pillars, particularly in Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety, and Catastrophic/Existential Risks. The benchmark provides a comprehensive, hierarchical, and dynamically evolving safety evaluation framework.

Conclusion: The ForesightSafety Bench addresses critical gaps in current AI safety evaluation by providing an encompassing framework that can better detect and assess complex risks from cutting-edge AI models, revealing significant safety vulnerabilities that need urgent attention from the AI safety community.

Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.

</details>


### [79] [Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning](https://arxiv.org/abs/2602.14160)
*Chaeeun Lee,T. Michael Yates,Pasquale Minervini,T. Ian Simpson*

Main category: cs.AI

TL;DR: An agent-as-tool reinforcement learning framework improves both outcome accuracy and process alignment for gene-disease validity curation by using hierarchical multi-agent systems with process-level supervision.


<details>
  <summary>Details</summary>
Motivation: Current LLM multi-agent systems for clinical decision-making focus on outcome accuracy while neglecting process-grounded reasoning aligned with clinical standards. Gene-disease validity curation requires synthesizing diverse biomedical evidence with traceable justifications.

Method: Introduces an agent-as-tool reinforcement learning framework with two objectives: (1) process-level supervision to ensure reasoning follows valid clinical pathways, and (2) efficient coordination via a hierarchical multi-agent system. Uses GRPO-trained supervisor agents.

Result: With outcome-only rewards, accuracy improved from 0.195 to 0.732 but process alignment was poor (0.392 F1). With process+outcome rewards, accuracy reached 0.750 while significantly improving process fidelity to 0.520 F1 on the ClinGen dataset.

Conclusion: The proposed framework successfully enhances both outcome accuracy and process alignment for gene-disease validity curation, demonstrating the importance of process-level supervision in clinical multi-agent systems.

Abstract: Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.

</details>


### [80] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: This paper shows that Earth-science text-only QA surprisingly drives multimodal reasoning gains in ultra-high-resolution remote sensing, enabling more effective agentic visual evidence retrieval through staged knowledge injection.


<details>
  <summary>Details</summary>
Motivation: Multimodal reasoning for ultra-high-resolution remote sensing faces challenges in visual evidence acquisition - models need to locate tiny relevant regions in massive pixel spaces. While agentic RL with verifiable rewards offers a solution, standard RL struggles without structured domain priors.

Method: The authors compare different post-training paradigms: Cold-start SFT, RLVR, and Agentic RLVR on UHR RS benchmark. They propose a staged knowledge injection approach: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures, and (2) "pre-warming" on hard UHR image-text examples during SFT to stabilize subsequent tool-based RL.

Result: The approach achieves 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

Conclusion: High-quality domain-specific text QA is crucial for UHR visual reasoning as it provides the conceptual framework, mechanistic explanations, and decision rules needed to guide visual evidence retrieval, enabling effective navigation of vast visual spaces through staged knowledge injection.

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [81] [CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments](https://arxiv.org/abs/2602.14229)
*Abubakarr Jaye,Nigel Boachie Kumankumah,Chidera Biringa,Anjel Shaileshbhai Patel,Sulaiman Vesal,Dayquan Julienne,Charlotte Siska,Manuel Raúl Meléndez Luján,Anthony Twum-Barimah,Mauricio Velazco,Tianwei Chen*

Main category: cs.AI

TL;DR: Multi-Horizon Task Environments (MHTEs) challenge autonomous agents with managing dozens of concurrent long-horizon tasks. CorpGen framework addresses key failure modes with hierarchical planning, sub-agent isolation, and tiered memory, achieving 3.5x improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks evaluate agents on single tasks in isolation, but real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. Current agents degrade severely under scaled task loads.

Method: Introduces Multi-Horizon Task Environments (MHTEs) and CorpGen framework with: hierarchical planning for multi-horizon goal alignment, sub-agent isolation to prevent cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. Uses persistent digital employees in simulated corporate environments.

Result: CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) on OSWorld Office with stable performance under increasing load. Ablation studies show experiential learning provides the largest gains.

Conclusion: The CorpGen framework effectively addresses key failure modes in multi-horizon task management, demonstrating that architectural improvements rather than specific CUA implementations enable stable performance under high concurrent task loads.

Abstract: Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.

</details>


### [82] [REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents](https://arxiv.org/abs/2602.14234)
*Zheng Chu,Xiao Wang,Jack Hong,Huiming Fan,Yuqi Huang,Yue Yang,Guohai Xu,Chenxiao Zhao,Cheng Xiang,Shengchao Hu,Dongdong Kuang,Ming Liu,Bing Qin,Xing Yu*

Main category: cs.AI

TL;DR: REDSearcher: A unified framework for optimizing LLMs as search agents through co-designed task synthesis, mid-training, and post-training, enabling scalable improvement in deep search capabilities.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with deep search tasks due to sparse high-quality search trajectories and expensive tool-call rollouts, requiring scalable solutions for optimization.

Method: Introduces REDSearcher with four key improvements: 1) Dual-constrained optimization for task synthesis using graph topology and evidence dispersion; 2) Tool-augmented queries to encourage active tool use; 3) Mid-training to strengthen atomic capabilities like knowledge and planning; 4) Local simulated environment for low-cost RL experimentation.

Result: Achieves state-of-the-art performance on text-only and multimodal search-agent benchmarks, and will release 10K text search trajectories, 5K multimodal trajectories, and 1K RL query set with code and models.

Conclusion: REDSearcher provides an effective scalable framework for optimizing LLMs as real-world search agents by addressing trajectory sparsity and high interaction costs through systematic task design and training enhancements.

Abstract: Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.

</details>


### [83] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAIL uses imitation learning to create goal-directed policies from suboptimal demonstrations for more accurate goal recognition that handles biased, suboptimal, and noisy behavior.


<details>
  <summary>Details</summary>
Motivation: Existing goal recognition methods rely on optimal goal-oriented policies, which often don't match an actor's true (potentially suboptimal) behavior, leading to inaccurate goal inference. There's a need for models that can recognize goals from imperfect demonstrations.

Method: GRAIL uses imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from demonstration trajectories. It then scores observed partial trajectories with each learned policy in a single forward pass.

Result: GRAIL improves F1-score by over 0.5 under systematically biased optimal behavior, gains 0.1-0.3 under suboptimal behavior, improves up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings.

Conclusion: GRAIL advances scalable and robust goal recognition models that can accurately interpret agent goals from imperfect demonstrations in uncertain environments.

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [84] [AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines](https://arxiv.org/abs/2602.14296)
*Yifan Wu,Yiran Peng,Yiyu Chen,Jianhao Ruan,Zijie Zhuang,Cheng Yang,Jiayi Zhang,Man Chen,Yenchi Tseng,Zhaoyang Yu,Liang Chen,Yuyao Zhai,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: AutoWebWorld is a framework that creates synthetic web environments as Finite State Machines and uses coding agents to generate interactive websites, enabling automated collection of verified interaction trajectories for training autonomous Web GUI agents.


<details>
  <summary>Details</summary>
Motivation: Current autonomous Web GUI agents face a data bottleneck: collecting interaction trajectories from real websites is expensive, difficult to verify, and involves hidden state transitions that require unreliable external verifiers for evaluation.

Method: The approach models web environments as Finite State Machines (FSMs) with explicit states, actions, and transition rules, then uses coding agents to translate FSMs into interactive websites, enabling programmatic verification of action correctness and task success.

Result: The system generated over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data boosted real-world performance: a 7B Web GUI agent outperformed all baselines within 15 steps on WebVoyager, and performance scaled with data volume on both WebVoyager and Online-Mind2Web.

Conclusion: AutoWebWorld successfully addresses the data bottleneck for Web GUI agents by providing a cost-effective method to generate large-scale, verified interaction data through synthetic web environments, demonstrating clear performance improvements and scaling benefits.

Abstract: The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.

</details>


### [85] [Benchmarking at the Edge of Comprehension](https://arxiv.org/abs/2602.14307)
*Samuele Marro,Jialin Yu,Emanuele La Malfa,Oishi Deb,Jiawei Li,Yibo Yang,Ebey Abraham,Sunando Sengupta,Eric Sommerlade,Michael Wooldridge,Philip Torr*

Main category: cs.AI

TL;DR: Benchmarking frontier LLMs is becoming impossible as they exceed human comprehension. The paper proposes Critique-Resilient Benchmarking, an adversarial framework where answers are deemed correct unless an adversary convincingly proves them wrong, allowing model comparison even when full human understanding is infeasible.


<details>
  <summary>Details</summary>
Motivation: As frontier LLMs rapidly advance, they're saturating benchmarks faster than humans can create new discriminative tasks or evaluate complex solutions. This "post-comprehension regime" threatens our ability to measure AI progress, necessitating new benchmarking approaches that don't require full human understanding.

Method: Introduces Critique-Resilient Benchmarking framework with critique-resilient correctness: answers are correct unless adversaries convincingly prove otherwise. Uses itemized bipartite Bradley-Terry model to jointly rank LLMs by both their ability to solve tasks and generate difficult yet solvable questions. Humans serve as bounded verifiers focusing on localized claims rather than full task comprehension.

Result: Demonstrated effectiveness in mathematical domain across eight frontier LLMs, showing stable scores that correlate with external capability measures. The framework successfully reformulates benchmarking as an adversarial generation-evaluation game where humans act as final adjudicators.

Conclusion: Critique-Resilient Benchmarking provides a viable approach to continue measuring AI progress even when models exceed human comprehension, transforming benchmarking from static test sets to an adversarial game that leverages human verification capacity more efficiently.

Abstract: As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.

</details>


### [86] [Competition for attention predicts good-to-bad tipping in AI](https://arxiv.org/abs/2602.14370)
*Neil F. Johnson,Frank Y. Huo*

Main category: cs.AI

TL;DR: Researchers identify a novel mathematical mechanism for dangerous tipping points in edge AI devices running ChatGPT-like models, showing how competition for attention between conversation contexts creates predictable failure modes that can be detected and controlled.


<details>
  <summary>Details</summary>
Motivation: Edge AI devices (like smartphones) running ChatGPT-like models without Internet connectivity pose new safety risks (self-harm, financial losses, extremism) that existing safety tools can't address since they either need cloud connectivity or only detect harm after it occurs.

Method: Developed a mathematical model of dangerous tipping points originating from competition for the AI's attention between conversation contexts and competing output basins, yielding a dynamical tipping point formula (n*) based on dot-product attention competition.

Result: Created a mechanism that reveals new control levers for dangerous tipping points, validated across multiple AI models, applicable to different definitions of 'good/bad', domains (health, law, finance, defense), legal systems, languages, and cultural settings.

Conclusion: The mathematical framework provides predictive and potentially controllable understanding of dangerous edge AI failures, offering a principled approach to safety that works across diverse contexts without requiring cloud connectivity or waiting for harm to occur.

Abstract: More than half the global population now carries devices that can run ChatGPT-like language models with no Internet connection and minimal safety oversight -- and hence the potential to promote self-harm, financial losses and extremism among other dangers. Existing safety tools either require cloud connectivity or discover failures only after harm has occurred. Here we show that a large class of potentially dangerous tipping originates at the atomistic scale in such edge AI due to competition for the machinery's attention. This yields a mathematical formula for the dynamical tipping point n*, governed by dot-product competition for attention between the conversation's context and competing output basins, that reveals new control levers. Validated against multiple AI models, the mechanism can be instantiated for different definitions of 'good' and 'bad' and hence in principle applies across domains (e.g. health, law, finance, defense), changing legal landscapes (e.g. EU, UK, US and state level), languages, and cultural settings.

</details>


### [87] [Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces](https://arxiv.org/abs/2602.14404)
*William L. Tong,Ege Cakar,Cengiz Pehlevan*

Main category: cs.AI

TL;DR: RT models generalize well on broad/shallow tasks but deteriorate on narrow/deep tasks compared to non-RT baselines, revealing fundamental scaling limitations of reasoning trace paradigms.


<details>
  <summary>Details</summary>
Motivation: Despite rapid progress in reasoning models that generate intermediate reasoning traces, understanding of how RTs support reasoning and the limits of this paradigm remains incomplete. The authors aim to promote greater clarity about the benefits and limitations of reasoning trace models.

Method: Introduced PITA dataset (23M+ statements in propositional logic with proofs) to benchmark robust reasoning via length generalization. Proposed concepts of task depth (steps to solve) and task breadth (unique examples). Varied these quantities across PITA subsets and compared RT vs non-RT models. Also compared results to a simple synthetic syllogism task to test generalizability.

Result: RT models generalize well on broad and shallow subsets but deteriorate on narrow and deep subsets relative to non-RT baselines. The findings suggest fundamental scalings that limit RT model performance on deep tasks while highlighting their generalization strengths on broad tasks.

Conclusion: The study identifies fundamental benefits and limitations inherent in using reasoning traces: RT models excel at broad tasks but face scaling limitations on deep tasks. The results appear to be general phenomena rather than idiosyncratic to PITA.

Abstract: Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.

</details>


### [88] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: PIR improves LLM reasoning efficiency by guiding models with selected past precedents instead of exhaustive self-exploration, shortening reasoning traces while maintaining or boosting accuracy.


<details>
  <summary>Details</summary>
Motivation: Current reasoning in Large Language Models (LLMs) is inefficient due to long chain-of-thought traces with redundant self-exploration and validation, which increases computational costs and can degrade performance. Inspired by human reasoning that leverages past related cases to constrain search spaces and reduce trial-and-error.

Method: PIR consists of two components: 1) Adaptive Precedent Selection (APS) to construct a compact set of semantically relevant and informative precedents for each question and LLM, and 2) Test-time Experience Internalization (TEI) to update lightweight adapters through test-time learning on precedent-informed instructions, enabling the model to internalize solution patterns as a prior.

Result: Experiments across mathematical reasoning, scientific QA, and code generation show that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

Conclusion: PIR successfully transforms LLM reasoning from exhaustive self-exploration to guided learning from precedents, achieving superior accuracy-efficiency trade-offs across multiple reasoning tasks.

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [89] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5](https://arxiv.org/abs/2602.14457)
*Dongrui Liu,Yi Yu,Jie Zhang,Guanxu Chen,Qihao Lin,Hanxi Zhu,Lige Huang,Yijin Zhou,Peng Wang,Shuai Shao,Boxuan Zhang,Zicheng Liu,Jingwei Sun,Yu Li,Yuejin Xie,Jiaxuan Guo,Jia Xu,Chaochao Lu,Bowen Zhou,Xia Hu,Jing Shao*

Main category: cs.AI

TL;DR: A comprehensive technical risk assessment framework for frontier AI models that analyzes five critical risk dimensions (cyber offense, persuasion, deception, uncontrolled R&D, self-replication) and proposes mitigation strategies for secure AI deployment.


<details>
  <summary>Details</summary>
Motivation: To address the unprecedented risks posed by rapidly advancing AI models, particularly Large Language Models (LLMs) and agentic AI systems, which require updated and granular risk assessment due to their evolving capabilities and proliferation.

Method: An updated technical risk analysis framework that introduces more complex cyber offense scenarios, evaluates LLM-to-LLM persuasion risks on newly released models, conducts experiments on emergent misalignment for strategic deception, focuses on 'mis-evolution' of autonomous agents, monitors safety performance during interactions, and introduces resource-constrained scenarios for self-replication.

Result: The paper presents a comprehensive assessment across five critical risk dimensions with new experimental findings and scenarios, proposing and validating robust mitigation strategies to address emerging threats in frontier AI.

Conclusion: This work provides both a current understanding of AI frontier risks and a preliminary technical pathway for secure AI deployment, while urgently calling for collective action to mitigate these challenges through actionable strategies.

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.

</details>


### [90] [Bounding Probabilities of Causation with Partial Causal Diagrams](https://arxiv.org/abs/2602.14503)
*Yuxuan Xie,Ang Li*

Main category: cs.AI

TL;DR: A framework for bounding probabilities of causation using partial causal information via optimization programming, providing tighter bounds without requiring complete causal graphs.


<details>
  <summary>Details</summary>
Motivation: Probabilities of causation are crucial for individual-level explanations and decisions but are counterfactual and not point-identifiable from data. Existing methods either ignore covariates, need complete causal graphs, or only work in restrictive binary settings, limiting practical use.

Method: The paper proposes using partial causal information (structural or statistical) as constraints in an optimization programming formulation to systematically incorporate available information and derive formally valid bounds.

Result: The framework yields tighter bounds for probabilities of causation without requiring full identifiability, extending applicability to realistic settings where causal knowledge is incomplete but informative.

Conclusion: This approach enables practical use of probabilities of causation in real-world applications by leveraging partial causal information through optimization programming to overcome limitations of existing methods.

Abstract: Probabilities of causation are fundamental to individual-level explanation and decision making, yet they are inherently counterfactual and not point-identifiable from data in general. Existing bounds either disregard available covariates, require complete causal graphs, or rely on restrictive binary settings, limiting their practical use. In real-world applications, causal information is often partial but nontrivial. This paper proposes a general framework for bounding probabilities of causation using partial causal information. We show how the available structural or statistical information can be systematically incorporated as constraints in a optimization programming formulation, yielding tighter and formally valid bounds without full identifiability. This approach extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.

</details>


### [91] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC is a tool that combines model checking and explainability to verify and analyze RL policies for sepsis treatment, revealing policy weaknesses like over-reliance on dosing history rather than patient condition.


<details>
  <summary>Details</summary>
Motivation: RL policies for sepsis treatment optimization are opaque and difficult to verify, with standard model checkers being infeasible for large MDPs and lacking explainability for why policies make specific decisions.

Method: COOL-MC wraps Storm model checker but adds three key capabilities: constructs only reachable state space induced by a trained policy; automatically labels states with clinically meaningful atomic propositions; integrates explainability methods with PCTL queries to reveal feature importance across treatment trajectories.

Result: Demonstrated on ICU-Sepsis MDP benchmark (17,000 sepsis patient records), establishing hard bounds via full MDP verification, training safe RL policy with optimal survival probability, and analyzing behavior via PCTL verification and explainability on induced DTMC.

Conclusion: COOL-MC serves as a tool for clinicians to investigate and debug sepsis treatment policies before deployment, exposing weaknesses invisible to standard evaluation through integration of formal verification and explainability.

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [92] [Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning](https://arxiv.org/abs/2602.14518)
*Jing Tang,Kun Wang,Haolang Lu,Hongjin Chen,KaiTao Chen,Zhongxiang Sun,Qiankun Li,Lingjuan Lyu,Guoshun Nan,Zhigang Zeng*

Main category: cs.AI

TL;DR: MLLMs exhibit distinct neural signatures when encountering conflicting knowledge in long reasoning chains, with separable conflict types encoded in mid-to-late layers and predictable reinforcement biases.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models often fail in long chain-of-thought reasoning when different knowledge sources provide conflicting information. The authors want to formalize and understand these failures under a unified notion of "knowledge conflict" to enable better diagnosis and control.

Method: The authors formalize knowledge conflicts distinguishing input-level objective conflict from process-level effective conflict. They probe internal representations of MLLMs to reveal key properties of conflict encoding, including linear separability, depth localization, hierarchical consistency, and directional asymmetry.

Result: Four key findings: (I) Different conflict types are linearly separable in internal representations; (II) Conflict signals concentrate in mid-to-late layers; (III) Aggregating token-level signals along trajectories robustly recovers conflict types; (IV) Reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source.

Conclusion: The findings provide a mechanism-level understanding of multimodal reasoning under knowledge conflict, enabling principled diagnosis and control of long chain-of-thought failures in MLLMs. The neural signatures identified can be leveraged for better model behavior.

Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.

</details>


### [93] [Disentangling Deception and Hallucination Failures in LLMs](https://arxiv.org/abs/2602.14529)
*Haolang Lu,Hongrui Peng,WeiYe Fu,Guoshun Nan,Xinye Cao,Xingrui Li,Hongcan Guo,Kun Wang*

Main category: cs.AI

TL;DR: LLM failures in factual QA are often attributed to missing knowledge, but this work separates Knowledge Existence from Behavior Expression, showing hallucination and deception are distinct internal failure mechanisms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to move beyond the simplistic behavioral perspective that attributes LLM failures in factual question answering solely to missing knowledge. The authors argue this view conflates different failure mechanisms and propose instead to analyze failures through an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression.

Method: The authors construct a controlled environment for entity-centric factual questions where knowledge is preserved while behavioral expression is selectively altered. They systematically analyze four behavioral cases and examine these failure modes through three approaches: representation separability, sparse interpretability, and inference-time activation steering.

Result: The study reveals that hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. The analysis shows how these distinct mechanisms can be identified through the proposed methods.

Conclusion: The work demonstrates that adopting an internal, mechanism-oriented perspective separating Knowledge Existence from Behavior Expression provides better understanding of LLM failures in factual QA. This distinction allows for more nuanced analysis of hallucination vs deception, moving beyond simplistic behavioral explanations to uncover the actual internal mechanisms driving different failure modes.

Abstract: Failures in large language models (LLMs) are often analyzed from a behavioral perspective, where incorrect outputs in factual question answering are commonly associated with missing knowledge. In this work, focusing on entity-based factual queries, we suggest that such a view may conflate different failure mechanisms, and propose an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression. Under this formulation, hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. To study this distinction, we construct a controlled environment for entity-centric factual questions in which knowledge is preserved while behavioral expression is selectively altered, enabling systematic analysis of four behavioral cases. We analyze these failure modes through representation separability, sparse interpretability, and inference-time activation steering.

</details>


### [94] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: A benchmark called MATEO assesses LVLMs' temporal reasoning for multimodal planning using professional recipe data with graph-based temporal execution order annotations.


<details>
  <summary>Details</summary>
Motivation: Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, linear chain approximations, or text-only inputs, lacking proper multimodal temporal reasoning evaluation.

Method: Created MATEO benchmark with high-quality professional multimodal recipe corpus (text instructions decomposed into steps with paired images), collected TEO annotations as graphs using scalable crowdsourcing pipeline, and evaluated six state-of-the-art LVLMs.

Result: Evaluated six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies to assess temporal reasoning abilities.

Conclusion: MATEO addresses the gap in evaluating multimodal temporal reasoning for planning tasks, providing a benchmark to improve LVLMs' real-world planning capabilities.

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [95] [Tabular Foundation Models Can Learn Association Rules](https://arxiv.org/abs/2602.14622)
*Erkan Karabulut,Daniel Daza,Paul Groth,Martijn C. Schut,Victoria Degeler*

Main category: cs.AI

TL;DR: TabProbe introduces a model-agnostic framework for association rule mining using tabular foundation models as conditional probability estimators, eliminating the need for frequent itemset mining and achieving robust performance in low-data regimes.


<details>
  <summary>Details</summary>
Motivation: Classical ARM methods suffer from rule explosion and poor scalability due to frequent itemset mining, while recent neural approaches degrade in low-data settings. Tabular foundation models (TFMs) with strong in-context generalization provide an opportunity to overcome these limitations.

Method: The paper proposes a model-agnostic framework for extracting association rules from any conditional probabilistic model over tabular data. TabProbe is introduced as an instantiation that leverages TFMs as conditional probability estimators to learn association rules without frequent itemset mining.

Result: TFMs consistently produce concise, high-quality association rules with strong predictive performance across tabular datasets of varying sizes. The approach remains robust in low-data settings without requiring task-specific training.

Conclusion: TabProbe demonstrates that tabular foundation models can effectively address the limitations of both classical and recent neural ARM approaches, providing a scalable solution for association rule mining that maintains performance even in low-data regimes.

Abstract: Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.

</details>


### [96] [Arbor: A Framework for Reliable Navigation of Critical Conversation Flows](https://arxiv.org/abs/2602.14643)
*Luís Silva,Diogo Gonçalves,Catarina Farinha,Clara Matos,Luís Ungaro*

Main category: cs.AI

TL;DR: Arbor decomposes decision tree navigation into specialized node-level tasks to improve LLM adherence to structured workflows, achieving significant accuracy, latency, and cost improvements over monolithic prompt approaches.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with strict adherence to structured workflows in high-stakes domains like healthcare triage. Monolithic approaches encoding entire decision structures in single prompts suffer from instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow.

Method: Arbor decomposes decision tree navigation into specialized node-level tasks. Decision trees are standardized into edge-list representation and stored for dynamic retrieval. At runtime, a DAG-based orchestration mechanism iteratively retrieves only outgoing edges of the current node, evaluates valid transitions via dedicated LLM calls, and delegates response generation to separate inference steps.

Result: Evaluated against single-prompt baselines across 10 foundation models using real clinical triage conversations: improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves 14.4x reduction in per-turn cost. Enables smaller models to match/exceed larger models under single-prompt baselines.

Conclusion: Architectural decomposition reduces dependence on intrinsic model capability, enabling more reliable adherence to structured workflows in high-stakes domains while significantly improving performance metrics.

Abstract: Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.

</details>


### [97] [From User Preferences to Base Score Extraction Functions in Gradual Argumentation](https://arxiv.org/abs/2602.14674)
*Aniol Civit,Antonio Rago,Antonio Andriella,Guillem Alenyà,Francesca Toni*

Main category: cs.AI

TL;DR: A method for extracting base scores from user preferences in gradual argumentation, enabling preference-driven quantitative bipolar argumentation frameworks with non-linear preference modeling.


<details>
  <summary>Details</summary>
Motivation: Base score selection in gradual argumentation requires user expertise and can be challenging, while organizing arguments by preference could simplify this process. There's a need to map user preferences to quantitative base scores for practical application.

Method: Introduces Base Score Extraction Functions that map user preferences over arguments to base scores, transforming Bipolar Argumentation Frameworks with preferences into Quantitative Bipolar Argumentation Frameworks. The method incorporates approximations of non-linear human preferences and provides algorithms for base score extraction.

Result: Provides theoretical and experimental evaluation in robotics settings, establishes desirable properties for base score extraction functions, discusses design choices, and offers recommendations for selecting appropriate gradual semantics in practice.

Conclusion: The proposed approach enables preference-driven gradual argumentation by effectively extracting base scores from user preferences, supporting transparent and contestable AI systems while addressing practical challenges in base score selection.

Abstract: Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.

</details>


### [98] [GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses](https://arxiv.org/abs/2602.14676)
*Attila Lischka,Balázs Kulcsár*

Main category: cs.AI

TL;DR: This paper proposes a deep reinforcement learning method with graph learning to solve the Bus Evacuation Orienteering Problem (BEOP), an NP-hard optimization problem for evacuating people by bus within time constraints.


<details>
  <summary>Details</summary>
Motivation: Urban evacuations are needed for both man-made disasters (terrorist attacks, industrial accidents) and increasingly frequent natural disasters due to climate change. Current car-focused evacuations cause congestion and disorder, so bus-based evacuation planning is crucial for faster, more effective evacuation strategies.

Method: Proposes a deep reinforcement learning approach with graph learning to solve the Bus Evacuation Orienteering Problem (BEOP). The method achieves fast inference speed and can create evacuation routes in fractions of seconds. An MILP formulation is used to bound the optimality gap of the evacuation plans.

Result: The method achieves near-optimal solution quality on San Francisco evacuation scenarios using real-world road networks and travel times. It also enables analysis of how many evacuation vehicles are needed to achieve specific bus-based evacuation quotas within predefined time constraints while maintaining adequate runtime.

Conclusion: The proposed deep reinforcement learning method with graph learning effectively addresses the Bus Evacuation Orienteering Problem, providing fast, near-optimal evacuation plans that can help determine vehicle requirements for meeting evacuation quotas within time constraints.

Abstract: Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.

</details>


### [99] [Removing Planner Bias in Goal Recognition Through Multi-Plan Dataset Generation](https://arxiv.org/abs/2602.14691)
*Mustafa F. Abdelwahed,Felipe Meneguzzi Kin Max Piamolini Gusmao,Joan Espasa*

Main category: cs.AI

TL;DR: Proposes using top-k planning to create unbiased goal recognition benchmarks and introduces Version Coverage Score metric to measure goal recognizer resilience under different plans and low observability.


<details>
  <summary>Details</summary>
Motivation: Existing goal recognition datasets suffer from systematic bias induced by heuristic-based forward search planning systems, which makes them inadequate for realistic multiagent scenarios where different planners might be used.

Method: Uses top-k planning to generate multiple, different plans for the same goal hypothesis, creating benchmarks that mitigate planning system bias. Introduces Version Coverage Score (VCS) metric to measure goal recognizer resilience when inferring goals from different plan sets.

Result: Results show that current state-of-the-art goal recognizers degrade substantially in resilience under low observability settings when evaluated with the new unbiased benchmarks.

Conclusion: The proposed method provides more realistic evaluation of goal recognizers by creating unbiased benchmarks and revealing vulnerabilities under different planning assumptions and low observability conditions.

Abstract: Autonomous agents require some form of goal and plan recognition to interact in multiagent settings. Unfortunately, all existing goal recognition datasets suffer from a systematical bias induced by the planning systems that generated them, namely heuristic-based forward search. This means that existing datasets lack enough challenge for more realistic scenarios (e.g., agents using different planners), which impacts the evaluation of goal recognisers with respect to using different planners for the same goal. In this paper, we propose a new method that uses top-k planning to generate multiple, different, plans for the same goal hypothesis, yielding benchmarks that mitigate the bias found in the current dataset. This allows us to introduce a new metric called Version Coverage Score (VCS) to measure the resilience of the goal recogniser when inferring a goal based on different sets of plans. Our results show that the resilience of the current state-of-the-art goal recogniser degrades substantially under low observability settings.

</details>


### [100] [Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs](https://arxiv.org/abs/2602.14697)
*Lunjun Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: E-SPL is a method that combines reinforcement learning with evolutionary system prompt learning to jointly improve both model contexts (via prompt evolution) and model weights (via RL), achieving better performance and generalization than either approach alone.


<details>
  <summary>Details</summary>
Motivation: Current LLM self-improvement primarily uses either self-reflection for context updates or reinforcement learning for weight updates, but not both jointly. The authors aim to create a method that can improve both model contexts and weights simultaneously for better autonomous self-improvement.

Method: E-SPL runs parallel RL rollouts with multiple system prompts in each iteration. It applies RL updates to model weights conditioned on each prompt, while evolving the prompt population through LLM-driven mutation and crossover. Each prompt has a TrueSkill rating for evolutionary selection based on relative performance within RL batches.

Result: E-SPL improves RL success rate from 38.8% to 45.1% in easy-to-hard generalization (AIME → BeyondAIME), outperforming reflective prompt evolution (40.0%). The method shows consistent gains in sample efficiency and generalization across reasoning and agentic tasks.

Conclusion: Coupling reinforcement learning with system prompt evolution yields better performance by encouraging a natural division between declarative knowledge (in prompts) and procedural knowledge (in weights), enabling more effective autonomous self-improvement of LLMs.

Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>


### [101] [WebWorld: A Large-Scale World Model for Web Agent Training](https://arxiv.org/abs/2602.14721)
*Zikai Xiao,Jianhong Tu,Chuhang Zou,Yuxin Zuo,Zhi Li,Peng Wang,Bowen Yu,Fei Huang,Junyang Lin,Zuozhu Liu*

Main category: cs.AI

TL;DR: WebWorld is the first open-web simulator trained at scale on 1M+ interactions, enabling realistic web agent training with long-horizon simulations and showing strong performance across web, code, GUI, and game domains.


<details>
  <summary>Details</summary>
Motivation: Real-world web agent training faces practical constraints including network latency, rate limits, and safety risks, making large-scale training difficult. Existing simulators are limited to closed environments with only thousands of trajectories.

Method: WebWorld uses a scalable data pipeline to train on over 1 million open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. It introduces WebWorld-Bench with dual metrics across nine dimensions for intrinsic evaluation.

Result: WebWorld achieves simulation performance comparable to Gemini-3-Pro on intrinsic evaluation. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2% on WebArena, reaching GPT-4o-level performance. WebWorld outperforms GPT-5 as a world model for inference-time search and shows cross-domain generalization to code, GUI, and game environments.

Conclusion: WebWorld provides a scalable solution for web agent training that overcomes real-world constraints, demonstrates strong performance across multiple domains, and offers a replicable recipe for world model construction that enables effective inference-time search and cross-domain generalization.

Abstract: Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.

</details>


### [102] [AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises](https://arxiv.org/abs/2602.14740)
*Kenneth Payne*

Main category: cs.AI

TL;DR: AI models in nuclear crisis simulation demonstrate sophisticated strategic behavior including deception, theory of mind, and metacognition, validating some classical strategic theories while challenging others like nuclear taboo effectiveness.


<details>
  <summary>Details</summary>
Motivation: To understand how frontier AI models reason under uncertainty in high-stakes strategic scenarios like nuclear crises, and to examine their implications for national security and strategic analysis.

Method: Created a crisis simulation where three frontier LLMs (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis scenario to analyze their strategic decision-making and reasoning patterns.

Result: Models validated classical strategic theories (Schelling, Kahn, Jervis) but challenged key assumptions: nuclear taboo didn't prevent escalation, strategic nuclear attacks occurred, threats provoked counter-escalation, high credibility accelerated conflict, and models never chose accommodation or withdrawal—only reduced violence.

Conclusion: AI simulation is a powerful tool for strategic analysis but requires careful calibration against human reasoning patterns; understanding AI-human strategic logic differences is crucial as AI increasingly shapes strategic outcomes.

Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.

</details>


### [103] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: RESOURCE is a workflow for creating datasets that include both schema and ground facts for evaluating knowledge graph refinement algorithms, addressing limitations of existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for knowledge graph refinement evaluation typically contain only ground facts with limited schema information, which prevents proper evaluation of methods that rely on ontological constraints, reasoning, or neurosymbolic techniques.

Method: The authors present RESOURCE, a workflow for extracting datasets that include both schema and ground facts, handling inconsistencies, leveraging reasoning for implicit knowledge, and serializing in OWL format. The workflow also provides utilities for loading datasets into tensor representations for machine learning libraries.

Result: The paper presents a curated suite of datasets extracted from knowledge graphs with expressive schemas, and enriches existing datasets with schema information. All datasets are serialized in OWL and made ready for reasoning services.

Conclusion: RESOURCE addresses the limitation of existing datasets by providing a comprehensive workflow for creating datasets with both schema and ground facts, enabling better evaluation of knowledge graph refinement methods that utilize ontological reasoning and neurosymbolic techniques.

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


### [104] [World Models for Policy Refinement in StarCraft II](https://arxiv.org/abs/2602.14857)
*Yixin Zhang,Ziyi Wang,Yiming Rong,Haoxi Wang,Jinling Jiang,Shuang Xu,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.AI

TL;DR: StarWM is the first world model for StarCraft II that predicts future observations under partial observability, enabling foresight-driven policy refinement with significant win-rate improvements against built-in AI.


<details>
  <summary>Details</summary>
Motivation: While LLMs show strong reasoning capabilities for decision-making in complex environments like StarCraft II, existing LLM-based SC2 agents overlook integrating learnable, action-conditioned transition models into the decision loop. The paper aims to bridge this gap by developing a world model for SC2's challenging partially observable environment.

Method: Proposes StarWM, the first world model for SC2 that predicts future observations under partial observability. Introduces a structured textual representation that factorizes observations into five semantic modules, constructs SC2-Dynamics-50k dataset for instruction-tuning, develops multi-dimensional offline evaluation framework, and creates StarWM-Agent that integrates StarWM into a Generate--Simulate--Refine decision loop.

Result: Offline results show StarWM achieves nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency over zero-shot baselines. Online evaluation demonstrates consistent win-rate gains of 30% against Hard (LV5), 15% against Harder (LV6), and 30% against VeryHard (LV7) levels, with improved macro-management stability and tactical risk assessment.

Conclusion: Integrating a learnable world model like StarWM into LLM-based decision systems significantly improves policy performance in complex, partially observable environments like StarCraft II, demonstrating the value of foresight-driven policy refinement through world model simulation.

Abstract: Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.

</details>


### [105] [EmbeWebAgent: Embedding Web Agents into Any Customized UI](https://arxiv.org/abs/2602.14865)
*Chenyang Ma,Clyde Fare,Matthew Wilson,Dave Braines*

Main category: cs.AI

TL;DR: EmbeWebAgent embeds agents directly into existing UIs using frontend hooks and a backend workflow, enabling robust, multi-step web automation in enterprise settings with minimal retrofitting.


<details>
  <summary>Details</summary>
Motivation: Most web agents operate at the human interface level (screenshots or raw DOM trees) without application-level access, limiting robustness and action expressiveness. In enterprise settings, explicit control over both frontend and backend is available, presenting an opportunity for more capable agents.

Method: The framework uses lightweight frontend hooks (curated ARIA and URL-based observations, plus a per-page function registry exposed via WebSocket) and a reusable backend workflow for reasoning and action execution. It is stack-agnostic, supports mixed-granularity actions from GUI primitives to higher-level composites, and orchestrates tasks via MCP tools.

Result: The demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting, indicating that the framework is practical and effective for real-world deployment.

Conclusion: EmbeWebAgent demonstrates a practical and robust approach to web automation by embedding agents directly into existing UIs with minimal retrofitting, enabling complex multi-step tasks in live enterprise environments.

Abstract: Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ

</details>


### [106] [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)
*Matthew Kowal,Goncalo Paulo,Louis Jaburi,Tom Tseng,Lev E McKinney,Stefan Heimersheim,Aaron David Tucker,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: Proposes Concept Influence for scalable training data attribution that identifies influential training examples based on semantic directions rather than individual test examples, achieving comparable performance to classical influence functions while being much faster.


<details>
  <summary>Details</summary>
Motivation: Need for scalable methods to identify which training data drive specific model behaviors, especially unintended ones. Existing TDA methods like influence functions are computationally expensive and biased toward syntactic similarity rather than semantic similarity.

Method: Introduces Concept Influence which attributes model behavior to semantic directions (linear probes or sparse autoencoder features) instead of individual test examples. Shows simple probe-based attribution methods are first-order approximations of Concept Influence that are much faster.

Result: Empirical validation shows Concept Influence and approximations achieve comparable performance to classical influence functions while being substantially more scalable (over an order-of-magnitude faster). Demonstrated effectiveness across emergent misalignment benchmarks and real post-training datasets.

Conclusion: Incorporating interpretable structure within traditional TDA pipelines enables more scalable, explainable, and better control of model behavior through data. Concept Influence addresses both scalability and the need for attribution to abstract behaviors rather than just individual examples.

Abstract: As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.

</details>


### [107] [Lifted Relational Probabilistic Inference via Implicit Learning](https://arxiv.org/abs/2602.14890)
*Luise Ge,Brendan Juba,Kris Nilsson,Alison Shao*

Main category: cs.AI

TL;DR: First polynomial-time framework for joint implicit learning and lifted inference in first-order relational probabilistic logic without explicit model construction.


<details>
  <summary>Details</summary>
Motivation: To reconcile the tension between inductive learning and deductive reasoning in first-order relational domains, addressing the challenge of answering queries through joint learning and reasoning without constructing explicit models.

Method: Merges incomplete first-order axioms with partially observed examples into bounded-degree SOS hierarchy. Uses two lifts: grounding-lift (renaming-equivalent ground moments share variable) and world-lift (pseudo-models enforced in parallel).

Result: First polynomial-time framework that implicitly learns first-order probabilistic logic and performs lifted inference over both individuals and worlds.

Conclusion: Innovative dual-lift approach successfully bridges learning and reasoning in relational probabilistic logic, enabling efficient query answering without explicit model construction.

Abstract: Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.

</details>


### [108] [The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics](https://arxiv.org/abs/2602.14903)
*Gregor Bachmann,Yichen Jiang,Seyed Mohsen Moosavi Dezfooli,Moin Nabi*

Main category: cs.AI

TL;DR: Researchers analyze Chain-of-Thought reasoning in LLMs using a "potential" metric to quantify how different parts of reasoning contribute to correct answers, revealing non-monotonic patterns, lucky guesses, and showing partial reasoning can unlock weaker models' performance.


<details>
  <summary>Details</summary>
Motivation: While CoT prompting is widely used to elicit reasoning-like responses from LLMs, the underlying mechanisms and driving forces behind its success remain poorly understood. Researchers aim to better understand how and which parts of CoT reasoning actually contribute to final answers.

Method: Introduce the concept of "potential" - a metric quantifying how much a given part of CoT reasoning increases the likelihood of a correct completion. Analyze competition-level mathematics questions' reasoning traces using this potential lens, then investigate CoT transferability by measuring weaker models' potential when given partial CoT from stronger models.

Result: Found surprising patterns: (1) non-monotonic potential due to reasoning tangents, (2) sharp spikes from reasoning insights/jumps, (3) lucky guesses where correct answers emerge without relevant justifications. Transferability experiments show as little as 20% of partial CoT can "unlock" weaker models' performance on previously unsolvable problems.

Conclusion: CoT reasoning in LLMs exhibits complex patterns, some aligning with human intuition but others remaining difficult to interpret. The transferability finding suggests large portions of CoT mechanics are generalizable across models, highlighting that key reasoning insights can be distilled and leveraged to improve weaker models' performance.

Abstract: Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.

</details>


### [109] [Position: Introspective Experience from Conversational Environments as a Path to Better Learning](https://arxiv.org/abs/2602.14910)
*Claudiu Cristian Musat,Jackson Tolins,Diego Antognini,Jingling Li,Martin Klissarov,Tom Duerig*

Main category: cs.AI

TL;DR: The paper argues that robust AI reasoning emerges from linguistic self-reflection internalized from high-quality dialogue, not just from scaling models.


<details>
  <summary>Details</summary>
Motivation: Current AI approaches treat reasoning as an emergent property of scale, but the authors argue this is insufficient. They propose that true reasoning emerges from linguistic self-reflection developed through quality social interactions, drawing on Vygotskian psychology.

Method: The paper advances three core positions: 1) Social Genesis of the Private Mind - learning from conversational environments and aligning with other agents refines reasoning, 2) Dialogically scaffolded introspective experiences enable sense-making that decouples learning from immediate data, and 3) Dialogue Quality is the New Data Quality - reasoning depth depends on diversity and rigor of mastered dialogues.

Result: The authors conclude that optimizing conversational scaffolds is the primary lever for next-generation general intelligence, shifting focus from scaling parameters to developing high-quality dialogical training.

Conclusion: True robust reasoning in AI emerges from internalized linguistic self-reflection developed through high-quality social interactions, not merely from model scaling. Future AI development should focus on optimizing conversational scaffolding.

Abstract: Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.

</details>


### [110] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: ReusStdFlow is a framework that converts platform-specific DSL workflows into standardized modular segments using extraction-storage-construction, achieving 90%+ accuracy on real enterprise workflows.


<details>
  <summary>Details</summary>
Motivation: To solve the "reusability dilemma" and structural hallucinations in enterprise Agentic AI by enabling automated reorganization and efficient reuse of enterprise digital assets across different platforms.

Method: Proposes an "Extraction-Storage-Construction" paradigm: 1) Deconstructs platform-specific DSLs into standardized workflow segments, 2) Uses dual knowledge architecture (graph + vector DBs) for synergistic retrieval of topological structures and functional semantics, 3) Intelligently assembles workflows using retrieval-augmented generation (RAG) strategy.

Result: Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction phases.

Conclusion: ReusStdFlow provides a standardized solution for automated reorganization and efficient reuse of enterprise digital assets, addressing reusability challenges in Agentic AI systems.

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [111] [MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design](https://arxiv.org/abs/2602.14926)
*Gen Zhou,Sugitha Janarthanan,Lianghong Chen,Pingzhao Hu*

Main category: cs.AI

TL;DR: MAC-AMP: A closed-loop multi-agent collaboration system using LLMs for automated, multi-objective antimicrobial peptide design with explainable optimization of activity, toxicity, novelty, and structural properties.


<details>
  <summary>Details</summary>
Motivation: To address antimicrobial resistance by improving AMP design where existing AI models struggle to balance multiple objectives (activity, toxicity, novelty) and use rigid scoring methods that are hard to interpret and optimize.

Method: Introduces MAC-AMP: a closed-loop multi-agent collaboration system based on LLMs. Uses a fully autonomous simulated peer review-adaptive reinforcement learning framework that only requires task description and example dataset. Features cross-domain transferability and explainable multi-objective optimization.

Result: Outperforms other AMP generative models by effectively optimizing multiple key molecular properties. Demonstrates exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

Conclusion: MAC-AMP successfully addresses limitations in existing AMP design models by providing an explainable, multi-objective optimization system using LLM-based multi-agent collaboration, showing strong performance across key AMP design criteria.

Abstract: To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

</details>


### [112] [On the Semantics of Primary Cause in Hybrid Dynamic Domains](https://arxiv.org/abs/2602.14994)
*Shakil M. Khan,Asim Mehmood,Sandra Zilles*

Main category: cs.AI

TL;DR: This paper proposes two equivalent definitions of primary causation in a hybrid action-theoretic framework called the hybrid temporal situation calculus, addressing both discrete and continuous change.


<details>
  <summary>Details</summary>
Motivation: While actual causation has been studied since Aristotle and formal mathematical accounts exist, there has been limited research on causation involving continuous change. The paper addresses the need to reason about actual causes in a hybrid world where change due to actions can be both discrete and continuous.

Method: The authors propose two definitions of primary cause in the hybrid temporal situation calculus framework: 1) a foundational definition, and 2) a definition formalizing causation through contributions verified using a modified "but-for" test from a counterfactual perspective. They prove the equivalence of these two definitions.

Result: The paper proves that the two proposed definitions of causation are equivalent. It also demonstrates that these definitions have intuitively justifiable properties, providing a solid formal foundation for reasoning about actual causes in hybrid systems involving both discrete and continuous change.

Conclusion: The paper successfully extends the study of actual causation to hybrid action-theoretic frameworks, providing mathematically sound definitions that handle both discrete and continuous change while maintaining intuitive properties and formal rigor.

Abstract: Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for'' test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.

</details>


### [113] [Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation](https://arxiv.org/abs/2602.15019)
*Alisa Vinogradova,Vlad Vinogradov,Luba Greenwood,Ilya Yasny,Dmitry Kobyzev,Shoman Kasbekar,Kong Nguyen,Dmitrii Radkevich,Roman Doronin,Andrey Doronichev*

Main category: cs.AI

TL;DR: A new Bioptic Agent outperforms leading AI models in drug asset scouting by achieving 79.7% F1 score on a challenging multilingual benchmark, addressing the risk of missing non-U.S.-centric innovations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a critical challenge in bio-pharmaceutical innovation: many new drug assets now originate outside the U.S. and are disclosed via regional, non-English channels, with over 85% of patent filings from outside the U.S. and China accounting for nearly half globally. This creates multi-billion-dollar risks for investors and business development teams who may miss 'under-the-radar' assets, making asset scouting a high-stakes competition where speed and completeness are essential. Current Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.

Method: The authors propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. They construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets largely outside U.S.-centric radar. They collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, they use LLM-as-judge evaluation calibrated to expert opinions.

Result: The Bioptic Agent achieves 79.7% F1 score, significantly outperforming other models: Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), and Exa Websets (26.9%). Performance improves steeply with additional compute, supporting the idea that more compute yields better results.

Conclusion: The Bioptic Agent demonstrates superior performance in drug asset scouting compared to existing Deep Research AI agents, achieving high recall and accuracy across heterogeneous, multilingual sources without hallucinations. The results highlight the importance of specialized, compute-intensive approaches for uncovering non-U.S.-centric bio-pharmaceutical innovations, reducing multi-billion-dollar risks for investors and business development teams.

Abstract: Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface "under-the-radar" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.
  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [114] [Directional Concentration Uncertainty: A representational approach to uncertainty quantification for generative models](https://arxiv.org/abs/2602.13264)
*Souradeep Chattopadhyay,Brendan Kennedy,Sai Munikoti,Soumik Sarkar,Karl Pazdernik*

Main category: cs.LG

TL;DR: A flexible framework using Directional Concentration Uncertainty (DCU) based on von Mises-Fisher distribution for uncertainty quantification in generative models, outperforming heuristic methods across tasks and modalities.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty quantification methods for generative models rely on rigid heuristics that don't generalize well across different tasks and modalities, limiting their trustworthiness and robustness.

Method: Proposes Directional Concentration Uncertainty (DCU), a statistical procedure using von Mises-Fisher distribution to measure geometric dispersion of multiple generated outputs from language models via continuous embeddings without task-specific heuristics.

Result: DCU matches or exceeds calibration levels of prior methods like semantic entropy and generalizes well to complex tasks in multi-modal domains, showing superior flexibility and performance.

Conclusion: DCU provides a highly flexible framework for uncertainty quantification that can be integrated into multi-modal and agentic systems, offering significant potential for improving trustworthiness in generative AI.

Abstract: In the critical task of making generative models trustworthy and robust, methods for Uncertainty Quantification (UQ) have begun to show encouraging potential. However, many of these methods rely on rigid heuristics that fail to generalize across tasks and modalities. Here, we propose a novel framework for UQ that is highly flexible and approaches or surpasses the performance of prior heuristic methods. We introduce Directional Concentration Uncertainty (DCU), a novel statistical procedure for quantifying the concentration of embeddings based on the von Mises-Fisher (vMF) distribution. Our method captures uncertainty by measuring the geometric dispersion of multiple generated outputs from a language model using continuous embeddings of the generated outputs without any task specific heuristics. In our experiments, we show that DCU matches or exceeds calibration levels of prior works like semantic entropy (Kuhn et al., 2023) and also generalizes well to more complex tasks in multi-modal domains. We present a framework for the wider potential of DCU and its implications for integration into UQ for multi-modal and agentic frameworks.

</details>


### [115] [BLUEPRINT Rebuilding a Legacy: Multimodal Retrieval for Complex Engineering Drawings and Documents](https://arxiv.org/abs/2602.13345)
*Ethan Seefried,Ran Eldegaway,Sanjay Das,Nathaniel Blanchard,Tirthankar Ghosal*

Main category: cs.LG

TL;DR: Blueprint is a layout-aware multimodal retrieval system that extracts structured metadata from unlabeled engineering drawings to enable effective search.


<details>
  <summary>Details</summary>
Motivation: Legacy engineering archives contain decades of technical drawings and records with inconsistent or missing metadata, making retrieval difficult and often requiring manual effort.

Method: Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers, and fuses lexical and dense retrieval with a lightweight region-level reranker.

Result: Deployed on ~770k unlabeled files, Blueprint shows a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline on a 5k-file benchmark with expert-curated queries.

Conclusion: Blueprint effectively transforms legacy engineering archives into searchable repositories through layout-aware multimodal retrieval and region-level processing.

Abstract: Decades of engineering drawings and technical records remain locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often manual. We present Blueprint, a layout-aware multimodal retrieval system designed for large-scale engineering repositories. Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (e.g., DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker. Deployed on ~770k unlabeled files, it automatically produces structured metadata suitable for cross-facility search.
  We evaluate Blueprint on a 5k-file benchmark with 350 expert-curated queries using pooled, graded (0/1/2) relevance judgments. Blueprint delivers a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline}, consistently outperforming across vision, text, and multimodal intents. Oracle ablations reveal substantial headroom under perfect region detection and OCR. We release all queries, runs, annotations, and code to facilitate reproducible evaluation on legacy engineering archives.

</details>


### [116] [Exploring the Performance of ML/DL Architectures on the MNIST-1D Dataset](https://arxiv.org/abs/2602.13348)
*Michael Beebe,GodsGift Uzor,Manasa Chepuri,Divya Sree Vemula,Angel Ayala*

Main category: cs.LG

TL;DR: The paper evaluates advanced neural network architectures (ResNet, TCN, DCNN) on the MNIST-1D dataset, showing that these models outperform simpler baselines and achieve near-human performance, validating MNIST-1D as a useful benchmark for resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Small datasets like MNIST have limitations in distinguishing between advanced neural network architectures due to their simplicity. The MNIST-1D dataset was introduced to provide variability and complexity while maintaining the advantages of small-scale datasets, making it ideal for studying inductive biases in sequential data.

Method: The authors extended the exploration of MNIST-1D by evaluating Residual Networks (ResNet), Temporal Convolutional Networks (TCN), and Dilated Convolutional Neural Networks (DCNN), benchmarking them alongside previously tested architectures like logistic regression, MLPs, CNNs, and GRUs.

Result: Advanced architectures such as TCN and DCNN consistently outperformed simpler models, achieving near-human performance on MNIST-1D. ResNet also showed significant improvements, demonstrating the importance of leveraging inductive biases and hierarchical feature extraction in small structured datasets.

Conclusion: The study validates MNIST-1D as a robust benchmark for evaluating machine learning architectures under computational constraints and highlights the importance of architectural innovations for improving performance in resource-limited environments.

Abstract: Small datasets like MNIST have historically been instrumental in advancing machine learning research by providing a controlled environment for rapid experimentation and model evaluation. However, their simplicity often limits their utility for distinguishing between advanced neural network architectures. To address these challenges, Greydanus et al. introduced the MNIST-1D dataset, a one-dimensional adaptation of MNIST designed to explore inductive biases in sequential data. This dataset maintains the advantages of small-scale datasets while introducing variability and complexity that make it ideal for studying advanced architectures.
  In this paper, we extend the exploration of MNIST-1D by evaluating the performance of Residual Networks (ResNet), Temporal Convolutional Networks (TCN), and Dilated Convolutional Neural Networks (DCNN). These models, known for their ability to capture sequential patterns and hierarchical features, were implemented and benchmarked alongside previously tested architectures such as logistic regression, MLPs, CNNs, and GRUs. Our experimental results demonstrate that advanced architectures like TCN and DCNN consistently outperform simpler models, achieving near-human performance on MNIST-1D. ResNet also shows significant improvements, highlighting the importance of leveraging inductive biases and hierarchical feature extraction in small structured datasets.
  Through this study, we validate the utility of MNIST-1D as a robust benchmark for evaluating machine learning architectures under computational constraints. Our findings emphasize the role of architectural innovations in improving model performance and offer insights into optimizing deep learning models for resource-limited environments.

</details>


### [117] [The Speed-up Factor: A Quantitative Multi-Iteration Active Learning Performance Metric](https://arxiv.org/abs/2602.13359)
*Hannes Kath,Thiago S. Gouvêa,Daniel Sonntag*

Main category: cs.LG

TL;DR: This paper proposes the speed-up factor as a new metric for evaluating Active Learning query methods by measuring the fraction of samples needed to match random sampling performance.


<details>
  <summary>Details</summary>
Motivation: Active learning research focuses heavily on developing query methods but lacks appropriate performance metrics to evaluate the iterative learning process. Existing metrics don't adequately capture multi-iteration performance across different query methods.

Method: The authors formally introduce the speed-up factor metric and conduct an empirical evaluation using four diverse datasets and seven different query methods. They review eight years of AL evaluation literature and compare the speed-up factor against state-of-the-art AL performance metrics.

Result: The speed-up factor accurately captures the fraction of samples needed to match random sampling performance and shows superior stability across iterations compared to other metrics. The results confirm the theoretical assumptions underlying the speed-up factor.

Conclusion: The speed-up factor provides a robust, quantitative multi-iteration performance metric for evaluating Active Learning query methods, addressing a key gap in AL evaluation methodology.

Abstract: Machine learning models excel with abundant annotated data, but annotation is often costly and time-intensive. Active learning (AL) aims to improve the performance-to-annotation ratio by using query methods (QMs) to iteratively select the most informative samples. While AL research focuses mainly on QM development, the evaluation of this iterative process lacks appropriate performance metrics. This work reviews eight years of AL evaluation literature and formally introduces the speed-up factor, a quantitative multi-iteration QM performance metric that indicates the fraction of samples needed to match random sampling performance. Using four datasets from diverse domains and seven QMs of various types, we empirically evaluate the speed-up factor and compare it with state-of-the-art AL performance metrics. The results confirm the assumptions underlying the speed-up factor, demonstrate its accuracy in capturing the described fraction, and reveal its superior stability across iterations.

</details>


### [118] [Accelerated Discovery of Cryoprotectant Cocktails via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.13398)
*Daniel Emerson,Nora Gaby-Biegel,Purva Joshi,Yoed Rabin,Rebecca D. Sandlin,Levent Burak Kara*

Main category: cs.LG

TL;DR: An active learning framework combining multi-objective Bayesian optimization with high-throughput screening accelerates the design of cryoprotectant cocktails for vitrification, discovering formulations that balance high concentration and cell viability with fewer experiments.


<details>
  <summary>Details</summary>
Motivation: Designing cryoprotectant cocktails faces a fundamental tradeoff: they must be concentrated enough to prevent ice formation yet non-toxic enough to preserve cell viability. Traditional discovery methods relying on expert intuition or exhaustive experimentation are slow and inefficient for this multi-objective optimization problem.

Method: The framework combines high-throughput screening with an active learning loop using multi-objective Bayesian optimization. It starts with initial cocktail measurements, trains probabilistic surrogate models to predict concentration and viability while quantifying uncertainty, then iteratively selects new experiments by maximizing expected Pareto improvement under uncertainty. Models are updated as new assay results are collected.

Result: Wet-lab validation shows the approach efficiently discovers cocktails achieving both high CPA concentrations and high post-exposure viability. Compared to naive and baseline strategies, it improves dominated hypervolume by 9.5% and 4.5% respectively while reducing experiments needed. Synthetic studies show it recovers comparable Pareto-optimal solutions using only 30% of evaluations required by prior state-of-the-art methods, saving about 10 weeks of experimental time.

Conclusion: The data-efficient active learning framework significantly accelerates cryoprotectant cocktail discovery by intelligently guiding experimental design. Its general nature allows adaptation to different CPA libraries, objective definitions, and cell lines, offering broad applicability in cryopreservation development.

Abstract: Designing cryoprotectant agent (CPA) cocktails for vitrification is challenging because formulations must be concentrated enough to suppress ice formation yet non-toxic enough to preserve cell viability. This tradeoff creates a large, multi-objective design space in which traditional discovery is slow, often relying on expert intuition or exhaustive experimentation. We present a data-efficient framework that accelerates CPA cocktail design by combining high-throughput screening with an active-learning loop based on multi-objective Bayesian optimization. From an initial set of measured cocktails, we train probabilistic surrogate models to predict concentration and viability and quantify uncertainty across candidate formulations. We then iteratively select the next experiments by prioritizing cocktails expected to improve the Pareto front, maximizing expected Pareto improvement under uncertainty, and update the models as new assay results are collected. Wet-lab validation shows that our approach efficiently discovers cocktails that simultaneously achieve high CPA concentrations and high post-exposure viability. Relative to a naive strategy and a strong baseline, our method improves dominated hypervolume by 9.5\% and 4.5\%, respectively, while reducing the number of experiments needed to reach high-quality solutions. In complementary synthetic studies, it recovers a comparably strong set of Pareto-optimal solutions using only 30\% of the evaluations required by the prior state-of-the-art multi-objective approach, which amounts to saving approximately 10 weeks of experimental time. Because the framework assumes only a suitable assay and defined formulation space, it can be adapted to different CPA libraries, objective definitions, and cell lines to accelerate cryopreservation development.

</details>


### [119] [Why is Normalization Preferred? A Worst-Case Complexity Theory for Stochastically Preconditioned SGD under Heavy-Tailed Noise](https://arxiv.org/abs/2602.13413)
*Yuchen Fang,James Demmel,Javad Lavaei*

Main category: cs.LG

TL;DR: This paper analyzes worst-case complexity of stochastically preconditioned SGD (like Adam, RMSProp) under heavy-tailed noise, showing normalization guarantees convergence while clipping may fail due to preconditioner-gradient dependence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the theoretical properties of widely used adaptive optimization methods (Adam, RMSProp, Shampoo) under heavy-tailed noise, and to explain the empirical preference for normalization over clipping in large-scale model training.

Method: The paper develops a worst-case complexity theory for stochastically preconditioned SGD (SPSGD) and its accelerated variants under heavy-tailed noise. They assume stochastic gradient noise has finite p-th moment for p∈(1,2], and analyze convergence after T iterations. They compare normalization vs. clipping as stabilization tools.

Result: Normalization guarantees convergence to a first-order stationary point at rate O(T^{-(p-1)/(3p-2)}) when problem parameters are known, and O(T^{-(p-1)/(2p)}) when unknown, matching optimal rates for normalized SGD. In contrast, clipping may fail to converge in worst case due to statistical dependence between stochastic preconditioner and gradient estimates. A novel vector-valued Burkholder-type inequality is developed for analysis.

Conclusion: The theoretical results provide an explanation for empirical preference for normalization over clipping in large-scale model training, showing that normalization is theoretically sound while clipping can fail due to the complex dependencies in stochastically preconditioned settings.

Abstract: We develop a worst-case complexity theory for stochastically preconditioned stochastic gradient descent (SPSGD) and its accelerated variants under heavy-tailed noise, a setting that encompasses widely used adaptive methods such as Adam, RMSProp, and Shampoo. We assume the stochastic gradient noise has a finite $p$-th moment for some $p \in (1,2]$, and measure convergence after $T$ iterations. While clipping and normalization are parallel tools for stabilizing training of SGD under heavy-tailed noise, there is a fundamental separation in their worst-case properties in stochastically preconditioned settings. We demonstrate that normalization guarantees convergence to a first-order stationary point at rate $\mathcal{O}(T^{-\frac{p-1}{3p-2}})$ when problem parameters are known, and $\mathcal{O}(T^{-\frac{p-1}{2p}})$ when problem parameters are unknown, matching the optimal rates for normalized SGD, respectively. In contrast, we prove that clipping may fail to converge in the worst case due to the statistical dependence between the stochastic preconditioner and the gradient estimates. To enable the analysis, we develop a novel vector-valued Burkholder-type inequality that may be of independent interest. These results provide a theoretical explanation for the empirical preference for normalization over clipping in large-scale model training.

</details>


### [120] [High-Resolution Climate Projections Using Diffusion-Based Downscaling of a Lightweight Climate Emulator](https://arxiv.org/abs/2602.13416)
*Haiwen Guan,Moein Darman,Dibyajyoti Chakraborty,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: A diffusion-based downscaling framework is introduced to enhance LUCIE's climate emulator from ~300km to 25km resolution, enabling detailed regional climate impact assessments while preserving coarse-grained dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven climate models like LUCIE face limitations: they have coarse native resolution (~300 km) which is inadequate for detailed regional impact assessments, despite showing accurate long-term statistics.

Method: Deep learning-based downscaling using probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. Trained on ~14,000 ERA5 timesteps (2000-2009), evaluated on LUCIE predictions (2010-2020).

Result: The approach preserves coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution. Performance validated through diverse metrics including latitude-averaged RMSE, power spectrum, probability density functions, and EOF analysis.

Conclusion: The diffusion-based downscaling framework successfully enhances LUCIE's resolution from ~300km to 25km, enabling detailed regional climate studies while maintaining physical consistency and long-term statistical accuracy.

Abstract: The proliferation of data-driven models in weather and climate sciences has marked a significant paradigm shift, with advanced models demonstrating exceptional skill in medium-range forecasting. However, these models are often limited by long-term instabilities, climatological drift, and substantial computational costs during training and inference, restricting their broader application for climate studies. Addressing these limitations, Guan et al. (2024) introduced LUCIE, a lightweight, physically consistent climate emulator utilizing a Spherical Fourier Neural Operator (SFNO) architecture. This model is able to reproduce accurate long-term statistics including climatological mean and seasonal variability. However, LUCIE's native resolution (~300 km) is inadequate for detailed regional impact assessments. To overcome this limitation, we introduce a deep learning-based downscaling framework, leveraging probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. These models downscale coarse LUCIE outputs to 25 km resolution. They are trained on approximately 14,000 ERA5 timesteps spanning 2000-2009 and evaluated on LUCIE predictions from 2010 to 2020. Model performance is assessed through diverse metrics, including latitude-averaged RMSE, power spectrum, probability density functions and First Empirical Orthogonal Function of the zonal wind. We observe that the proposed approach is able to preserve the coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution.

</details>


### [121] [Text Has Curvature](https://arxiv.org/abs/2602.13418)
*Karish Grover,Hanqing Zeng,Yinglong Xia,Christos Faloutsos,Geoffrey J. Gordon*

Main category: cs.LG

TL;DR: Text has intrinsic curvature detectable through a text-native discrete curvature signal called Texture, which reveals semantic inference patterns and enables practical NLP applications without geometric training.


<details>
  <summary>Details</summary>
Motivation: Language is increasingly modeled in curved geometries (hyperbolic spaces, mixed-curvature manifolds), but it remains unclear what curvature means for text itself - whether text has intrinsic curvature independent of embedding choices. The authors aim to answer this fundamental question by detecting, defining, and utilizing text-native curvature.

Method: Propose Texture, a text-native word-level discrete curvature signal defined by reconciling left- and right-context beliefs around a masked word through a Schrodinger bridge. This yields a curvature field where positive values indicate context focusing meaning and negative values indicate context fanning out into competing continuations.

Result: Empirical and theoretical certificates show semantic inference in natural corpora is non-flat (language has inherent curvature). Texture serves as actionable measurement and control primitive enabling geometry without geometric training. Demonstrated utility on two tasks: improved long-context inference through curvature-guided compression, and improved retrieval-augmented generation through curvature-guided routing.

Conclusion: Text does indeed have intrinsic curvature, which can be measured and made practically useful through Texture, establishing a text-native curvature paradigm that makes curvature measurable and actionable without requiring geometric training.

Abstract: Does text have an intrinsic curvature? Language is increasingly modeled in curved geometries - hyperbolic spaces for hierarchy, mixed-curvature manifolds for compositional structure - yet a basic scientific question remains unresolved: what does curvature mean for text itself, in a way that is native to language rather than an artifact of the embedding space we choose? We argue that text does indeed have curvature, and show how to detect it, define it, and use it. To this end, we propose Texture, a text-native, word-level discrete curvature signal, and make three contributions. (a) Existence: We provide empirical and theoretical certificates that semantic inference in natural corpora is non-flat, i.e. language has inherent curvature. (b) Definition: We define Texture by reconciling left- and right-context beliefs around a masked word through a Schrodinger bridge, yielding a curvature field that is positive where context focuses meaning and negative where it fans out into competing continuations. (c) Utility: Texture is actionable: it serves as a general-purpose measurement and control primitive enabling geometry without geometric training; we instantiate it on two representative tasks, improving long-context inference through curvature-guided compression and retrieval-augmented generation through curvature-guided routing. Together, our results establish a text-native curvature paradigm, making curvature measurable and practically useful.

</details>


### [122] [Comparing Classifiers: A Case Study Using PyCM](https://arxiv.org/abs/2602.13482)
*Sadra Sabouri,Alireza Zolanvari,Sepand Haghighi*

Main category: cs.LG

TL;DR: PyCM library tutorial demonstrates how comprehensive evaluation metrics reveal subtle performance differences in multi-class classifiers that standard metrics miss.


<details>
  <summary>Details</summary>
Motivation: Selecting optimal classification models requires robust understanding of performance, but standard evaluation metrics may miss subtle trade-offs and important differences in model efficacy.

Method: Tutorial on PyCM library demonstrating deep-dive evaluations of multi-class classifiers through examination of two different case scenarios, showing how choice of evaluation metrics affects interpretation.

Result: Illustrates that choice of evaluation metrics can fundamentally shift interpretation of model's efficacy, emphasizing need for multi-dimensional evaluation framework.

Conclusion: Multi-dimensional evaluation framework is essential for uncovering small but important differences in model performance that standard metrics may miss.

Abstract: Selecting an optimal classification model requires a robust and comprehensive understanding of the performance of the model. This paper provides a tutorial on the PyCM library, demonstrating its utility in conducting deep-dive evaluations of multi-class classifiers. By examining two different case scenarios, we illustrate how the choice of evaluation metrics can fundamentally shift the interpretation of a model's efficacy. Our findings emphasize that a multi-dimensional evaluation framework is essential for uncovering small but important differences in model performance. However, standard metrics may miss these subtle performance trade-offs.

</details>


### [123] [Finding Highly Interpretable Prompt-Specific Circuits in Language Models](https://arxiv.org/abs/2602.13483)
*Gabriel Franco,Lucas M. Tassis,Azalea Rohr,Mark Crovella*

Main category: cs.LG

TL;DR: ACCs++ improves circuit analysis by showing circuits are prompt-specific, not task-level, enabling automated interpretability pipelines for prompt families.


<details>
  <summary>Details</summary>
Motivation: Most prior work assumes a single stable mechanism per task by averaging across prompts, which obscures that circuits are actually prompt-specific. This limits understanding of how language models solve tasks through their internal circuits.

Method: Introduces ACC++ - refinements to attention causal communication (ACC) that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Does not require replacement models or activation patching, and reduces attribution noise. Applied to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2.

Result: No single circuit exists for IOI in any model - different prompt templates induce systematically different mechanisms. However, prompts cluster into families with similar circuits, enabling representative circuits as practical analysis units. Developed an automated interpretability pipeline using ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families.

Conclusion: Circuits should be studied as prompt-specific rather than task-level phenomena. Shifting the unit of analysis from tasks to prompts enables scalable circuit descriptions and meaningful mechanistic interpretability even with prompt-specific variation.

Abstract: Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.

</details>


### [124] [Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability](https://arxiv.org/abs/2602.13485)
*Ayse Tursucular,Ayush Mohanty,Nazal Mohamed,Nagi Gebraeel*

Main category: cs.LG

TL;DR: Federated framework for learning interpretable temporal interdependencies across decentralized industrial subsystems with fixed proprietary models, using nonlinear state space models and graph attention networks.


<details>
  <summary>Details</summary>
Motivation: Modern industrial systems have distributed sensors monitoring interdependent subsystems, but raw measurements cannot be shared in decentralized settings, client observations are heterogeneous, and existing approaches are limited by fixed proprietary models that cannot be modified.

Method: Each client uses a nonlinear state space model to map high-dimensional local observations to low-dimensional latent states. A central server learns a graph-structured neural state transition model over communicated latent states using a Graph Attention Network (GAT). For interpretability, the Jacobian of the learned server-side transition model is related to attention coefficients.

Result: The framework provides the first interpretable characterization of cross-client temporal interdependencies in decentralized nonlinear systems. Theoretical convergence guarantees to a centralized oracle are established, and synthetic experiments demonstrate convergence, interpretability, scalability, and privacy. Real-world experiments show performance comparable to decentralized baselines.

Conclusion: The presented federated framework successfully addresses the challenge of learning interpretable temporal interdependencies across decentralized subsystems with fixed proprietary models, offering both theoretical guarantees and practical validation.

Abstract: Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model. A central server learns a graph structured neural state transition model over the communicated latent states using a Graph Attention Network. For interpretability we relate the Jacobian of the learned server side transition model to attention coefficients, providing the first interpretable characterization of cross client temporal interdependencies in decentralized nonlinear systems. We establish theoretical convergence guarantees to a centralized oracle and validate the framework through synthetic experiments demonstrating convergence, interpretability, scalability and privacy. Additional real world experiments show performance comparable to decentralized baselines.

</details>


### [125] [Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity](https://arxiv.org/abs/2602.13486)
*Fei Wu,Jia Hu,Geyong Min,Shiqiang Wang*

Main category: cs.LG

TL;DR: Proposes raFLoRA to solve rank collapse in heterogeneous federated LoRA by partitioning updates by rank and aggregating with weighted contributions.


<details>
  <summary>Details</summary>
Motivation: To address the rank collapse phenomenon in heterogeneous FedLoRA where energy of global update concentrates on minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations.

Method: raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and aggregates each partition weighted by its effective client contributions.

Result: Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

Conclusion: The proposed raFLoRA method effectively prevents rank collapse in heterogeneous federated low-rank adaptation, leading to improved model performance and communication efficiency across diverse tasks.

Abstract: Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

</details>


### [126] [TrasMuon: Trust-Region Adaptive Scaling for Orthogonalized Momentum Optimizers](https://arxiv.org/abs/2602.13498)
*Peng Cheng,Jiucheng Zang,Qingnan Li,Liheng Ma,Yufei Cui,Yingxue Zhang,Boxing Chen,Ming Jian,Wen Tong*

Main category: cs.LG

TL;DR: TrasMuon improves upon Muon-style optimizers by adding global RMS calibration and energy-based trust-region clipping to stabilize magnitudes while preserving near-isometric geometry, achieving faster convergence and better stability than baselines.


<details>
  <summary>Details</summary>
Motivation: Muon-style optimizers use Newton-Schulz iterations for orthogonalization, which improves geometry but discards magnitude information, making training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. This leads to stability issues that need to be addressed.

Method: TrasMuon preserves Muon's near-isometric geometry while adding two key components: (1) global RMS calibration for magnitude stabilization, and (2) energy-based trust-region clipping that defines a stable zone based on relative energy ratios to confine updates and prevent instability from high-energy outliers.

Result: Empirical experiments on vision and language models show that TrasMuon converges faster than baseline optimizers. Additional experiments without warmup stages confirm TrasMuon's superior stability and robustness compared to other methods.

Conclusion: TrasMuon successfully addresses the stability issues of Muon-style optimizers by combining adaptive scaling with trust-region clipping, achieving both efficient optimization and robust training stability across diverse model architectures.

Abstract: Muon-style optimizers leverage Newton-Schulz (NS) iterations to orthogonalize updates, yielding update geometries that often outperform Adam-series methods. However, this orthogonalization discards magnitude information, rendering training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. To mitigate this, we introduce TrasMuon (\textbf{T}rust \textbf{R}egion \textbf{A}daptive \textbf{S}caling \textbf{Muon}). TrasMuon preserves the near-isometric geometry of Muon while stabilizing magnitudes through (i) global RMS calibration and (ii) energy-based trust-region clipping. We demonstrate that while reintroducing adaptive scaling improves optimization efficiency, it typically exacerbates instability due to high-energy outliers. TrasMuon addresses this by defining a trust region based on relative energy ratios, confining updates to a stable zone. Empirical experiments on vision and language models demonstrate that TrasMuon converges faster than baselines. Furthermore, experiments without warmup stages confirm TrasMuon's superior stability and robustness.

</details>


### [127] [$γ$-weakly $θ$-up-concavity: Linearizable Non-Convex Optimization with Applications to DR-Submodular and OSS Functions](https://arxiv.org/abs/2602.13506)
*Mohammad Pedramfar,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: The paper introduces γ-weakly θ-up-concavity, a novel first-order condition that generalizes DR-submodular and One-Sided Smooth functions, enabling unified approximation guarantees through linearization techniques.


<details>
  <summary>Details</summary>
Motivation: Optimizing monotone non-convex functions is fundamental across machine learning and combinatorial optimization, but existing frameworks like DR-submodular and One-Sided Smooth functions are limited. There's a need for a unifying framework that captures a broader class of such functions while enabling strong approximation guarantees.

Method: Introduces γ-weakly θ-up-concavity as a novel first-order condition. Proves these functions are upper-linearizable: for any feasible point, a linear surrogate can be constructed whose gains approximate the original objective up to a constant factor (approximation coefficient) dependent on γ, θ, and feasible set geometry.

Result: Provides unified approximation guarantees for offline optimization and static/dynamic regret bounds in online settings via reductions to linear optimization. Recovers optimal approximation coefficient for DR-submodular maximization and significantly improves existing coefficients for OSS optimization, particularly over matroid constraints.

Conclusion: γ-weakly θ-up-concavity provides a powerful unifying framework for monotone non-convex optimization that strictly generalizes existing function classes while enabling strong approximation guarantees through linearization techniques, with applications across both offline and online settings.

Abstract: Optimizing monotone non-convex functions is a fundamental challenge across machine learning and combinatorial optimization. We introduce and study $γ$-weakly $θ$-up-concavity, a novel first-order condition that characterizes a broad class of such functions. This condition provides a powerful unifying framework, strictly generalizing both DR-submodular functions and One-Sided Smooth (OSS) functions. Our central theoretical contribution demonstrates that $γ$-weakly $θ$-up-concave functions are upper-linearizable: for any feasible point, we can construct a linear surrogate whose gains provably approximate the original non-linear objective. This approximation holds up to a constant factor, namely the approximation coefficient, dependent solely on $γ$, $θ$, and the geometry of the feasible set. This linearizability yields immediate and unified approximation guarantees for a wide range of problems. Specifically, we obtain unified approximation guarantees for offline optimization as well as static and dynamic regret bounds in online settings via standard reductions to linear optimization. Moreover, our framework recovers the optimal approximation coefficient for DR-submodular maximization and significantly improves existing approximation coefficients for OSS optimization, particularly over matroid constraints.

</details>


### [128] [Singular Vectors of Attention Heads Align with Features](https://arxiv.org/abs/2602.13524)
*Gabriel Franco,Carson Loughridge,Mark Crovella*

Main category: cs.LG

TL;DR: The paper investigates whether singular vectors of attention matrices can reliably identify feature representations in language models, finding theoretical justification and empirical evidence for such alignment under certain conditions.


<details>
  <summary>Details</summary>
Motivation: Several recent mechanistic interpretability studies have assumed that feature representations can be inferred from singular vectors of attention matrices, but sound justification for this assumption has been lacking. The paper aims to address this gap by investigating why and when such alignment occurs.

Method: 1) Demonstrated alignment in a model where features can be directly observed. 2) Provided theoretical analysis showing conditions under which alignment is expected. 3) Developed a testable prediction through sparse attention decomposition and showed evidence of its emergence in real models.

Result: The study found that singular vectors robustly align with features in observable models, theoretical conditions support such alignment, and sparse attention decomposition emerges as predicted in real language models. These results provide both empirical and theoretical support for the alignment approach.

Conclusion: Alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models, addressing the previous lack of justification for this common assumption in mechanistic interpretability research.

Abstract: Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models. Together these results suggest that alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models.

</details>


### [129] [DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices](https://arxiv.org/abs/2602.14301)
*Songyuan Li,Jia Hu,Ahmed M. Abdelmoniem,Geyong Min,Haojun Huang,Jiwei Huang*

Main category: cs.LG

TL;DR: DeepFusion is a scalable federated MoE training framework that enables resource-constrained devices to contribute to MoE LLM training via knowledge distillation with a View-Aligned Attention module, reducing communication costs by up to 71% while maintaining performance close to centralized training.


<details>
  <summary>Details</summary>
Motivation: MoE-based LLMs require vast and diverse training data, but privacy concerns and device resource constraints make traditional federated learning approaches impractical for training these large models on heterogeneous edge devices.

Method: DeepFusion allows devices to configure and train on-device LLMs tailored to their hardware, then uses a novel View-Aligned Attention module to integrate multi-stage feature representations from the global MoE model, enabling effective cross-architecture knowledge distillation by aligning predictive perspectives between heterogeneous models.

Result: DeepFusion achieves performance close to centralized MoE training with Qwen-MoE and DeepSeek-MoE models on real-world medical and finance datasets, reducing communication costs by up to 71% and improving token perplexity by up to 5.28% compared to federated MoE baselines.

Conclusion: DeepFusion provides a practical solution for scalable federated MoE training that addresses both privacy preservation and device resource constraints through effective knowledge distillation with aligned predictive perspectives, making large MoE model training feasible on heterogeneous edge devices.

Abstract: Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.

</details>


### [130] [QuaRK: A Quantum Reservoir Kernel for Time Series Learning](https://arxiv.org/abs/2602.13531)
*Abdallah Aaraba,Soumaya Cherkaoui,Ola Ahmad,Shengrui Wang*

Main category: cs.LG

TL;DR: QuaRK is an end-to-end quantum reservoir computing framework with hardware-realistic quantum reservoir featurizer and kernel-based readout that provides learning guarantees for temporal data.


<details>
  <summary>Details</summary>
Motivation: Quantum reservoir computing shows promise for time series learning but lacks efficient, implementable architectures with model learning guarantees in existing literature.

Method: QuaRK couples hardware-realistic quantum reservoir featurizer with kernel-based readout: reservoir processes sequential data points, extracts features via classical shadow tomography of k-local observables, then classical kernel-based readout learns target mapping with regularization and fast optimization.

Result: The framework provides clear computational knobs (circuit width/depth, measurement budget), scalability to high-dimensional data, and learning-theoretic generalization guarantees for dependent temporal data linking design choices to finite-sample performance.

Conclusion: Empirical experiments validate QuaRK and demonstrate predicted interpolation and generalization behaviors on synthetic beta-mixing time series tasks, offering principled guidance for building reliable temporal learners.

Abstract: Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir architectures along with model learning guarantees remain scarce in the literature. To close this gap, we introduce QuaRK, an end-to-end framework that couples a hardware-realistic quantum reservoir featurizer with a kernel-based readout scheme. Given a sequence of sample points, the reservoir injects the points one after the other to yield a compact feature vector from efficiently measured k-local observables using classical shadow tomography, after which a classical kernel-based readout learns the target mapping with explicit regularization and fast optimization. The resulting pipeline exposes clear computational knobs -- circuit width and depth as well as the measurement budget -- while preserving the flexibility of kernel methods to model nonlinear temporal functionals and being scalable to high-dimensional data. We further provide learning-theoretic generalization guarantees for dependent temporal data, linking design and resource choices to finite-sample performance, thereby offering principled guidance for building reliable temporal learners. Empirical experiments validate QuaRK and illustrate the predicted interpolation and generalization behaviours on synthetic beta-mixing time series tasks.

</details>


### [131] [Fluid-Agent Reinforcement Learning](https://arxiv.org/abs/2602.14559)
*Shishir Sharma,Doina Precup,Theodore J. Perkins*

Main category: cs.LG

TL;DR: A fluid-agent framework for MARL where agents can dynamically create new agents to adapt to environmental demands, enabling dynamic team size adjustments.


<details>
  <summary>Details</summary>
Motivation: Traditional MARL assumes fixed agent populations, but real-world scenarios involve dynamic numbers of agents where agents can create other agents (e.g., cell division, company spinoffs).

Method: Proposes a fluid-agent environment framework with game-theoretic solution concepts, evaluated on MARL algorithms using fluid variants of Predator-Prey and Level-Based Foraging benchmarks.

Result: The framework successfully yields agent teams that dynamically adjust their size to match environmental demands, enabling novel strategies beyond fixed-population settings.

Conclusion: Fluid-agent environments provide a more realistic framework for multi-agent systems where agent populations can dynamically change, unlocking new solution strategies and adaptive team behaviors.

Abstract: The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.

</details>


### [132] [Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction](https://arxiv.org/abs/2602.13532)
*Nobutaka Ono*

Main category: cs.LG

TL;DR: A multiplication-free dimension reduction method using element selection with swap-based optimization for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard dimension reduction methods like PCA rely on matrix multiplications, which can be computationally expensive on resource-constrained systems. Element selection eliminates this bottleneck by simply selecting a subset of elements from the input, but requires efficient optimization to determine which elements to retain.

Method: Proposes element selection evaluated by minimum mean-squared error of linear regression predicting target vectors (either explicit labels or the input itself). Uses a swap-based local search algorithm with efficient computation of objective changes via the matrix inversion lemma to solve the combinatorial optimization problem.

Result: Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed multiplication-free element selection method compared to standard approaches.

Conclusion: Element selection provides a computationally efficient alternative to traditional dimension reduction methods like PCA, eliminating multiplication bottlenecks while maintaining effectiveness through swap-based optimization.

Abstract: In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.

</details>


### [133] [Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849)
*Bardia Mohammadi,Nearchos Potamitis,Lars Klein,Akhil Arora,Laurent Bindschaedler*

Main category: cs.LG

TL;DR: Atomix provides transactional semantics for LLM agent tool calls, enabling safe rollback through epoch-based tracking, frontier management, and compensation mechanisms.


<details>
  <summary>Details</summary>
Motivation: LLM agents acting on external systems face risks when tool effects are immediate - under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback mechanism.

Method: Atomix introduces a runtime with progress-aware transactional semantics: tags each call with an epoch, tracks per-resource frontiers, commits only when progress predicates indicate safety, buffers effects when possible, and tracks externalized effects for compensation on abort.

Result: Across real workloads with fault injection, Atomix demonstrates that transactional retry improves task success rates, while frontier-gated commit strengthens isolation under speculation and contention scenarios.

Conclusion: Atomix provides effective transactional semantics for LLM agent tool calls, addressing the critical need for safe rollback mechanisms in systems where agents interact with external resources under uncertain conditions.

Abstract: LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.

</details>


### [134] [Out-of-Support Generalisation via Weight Space Sequence Modelling](https://arxiv.org/abs/2602.13550)
*Roussel Desmond Nzoyem*

Main category: cs.LG

TL;DR: WeightCaster reformulates out-of-support generalization as a sequence modeling task in weight space using concentric shell partitioning, producing plausible, interpretable, and uncertainty-aware predictions without explicit inductive biases.


<details>
  <summary>Details</summary>
Motivation: Deep learning models frequently fail catastrophically on out-of-support samples (data outside training distribution), producing unrealistic but overconfident predictions, which hinders deployment in safety-critical applications.

Method: Reformulates out-of-support generalization as a sequence modeling task in weight space, partitioning training data into concentric shells corresponding to discrete sequential steps (WeightCaster framework).

Result: Competitive or superior performance to state-of-the-art methods on synthetic cosine dataset and real-world air quality sensor readings, with plausible, interpretable, and uncertainty-aware predictions.

Conclusion: WeightCaster enhances reliability beyond in-distribution scenarios, enabling wider adoption of AI in safety-critical applications through improved out-of-support generalization.

Abstract: As breakthroughs in deep learning transform key industries, models are increasingly required to extrapolate on datapoints found outside the range of the training set, a challenge we coin as out-of-support (OoS) generalisation. However, neural networks frequently exhibit catastrophic failure on OoS samples, yielding unrealistic but overconfident predictions. We address this challenge by reformulating the OoS generalisation problem as a sequence modelling task in the weight space, wherein the training set is partitioned into concentric shells corresponding to discrete sequential steps. Our WeightCaster framework yields plausible, interpretable, and uncertainty-aware predictions without necessitating explicit inductive biases, all the while maintaining high computational efficiency. Emprical validation on a synthetic cosine dataset and real-world air quality sensor readings demonstrates performance competitive or superior to the state-of-the-art. By enhancing reliability beyond in-distribution scenarios, these results hold significant implications for the wider adoption of artificial intelligence in safety-critical applications.

</details>


### [135] [Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems](https://arxiv.org/abs/2602.14901)
*Pramit Saha,Joshua Strong,Mohammad Alsharid,Divyanshu Mishra,J. Alison Noble*

Main category: cs.LG

TL;DR: ToolSelect is a method for adaptive model selection in agentic healthcare systems that learns to choose the best specialist model for each query by minimizing population risk using an Attentive Neural Process-based selector.


<details>
  <summary>Details</summary>
Motivation: In healthcare agentic systems, different task-specialized models excel on different data samples, but there's no single "best" model for any given task. Agents need to reliably select the right specialist model from a heterogeneous pool for each query.

Method: ToolSelect adaptively learns model selection by minimizing population risk over sampled specialist tool candidates using a consistent surrogate of task-conditional selection loss. It uses an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries.

Result: The authors created ToolSelectBench, a benchmark with 1448 queries in an agentic Chest X-ray environment with diverse task-specialized models. ToolSelect consistently outperformed 10 state-of-the-art methods across four different task families.

Conclusion: ToolSelect provides an effective solution for adaptive model selection in agentic healthcare systems, demonstrating superior performance over existing methods and establishing a needed benchmark for this problem domain.

Abstract: Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single "best" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.

</details>


### [136] [Scenario-Adaptive MU-MIMO OFDM Semantic Communication With Asymmetric Neural Network](https://arxiv.org/abs/2602.13557)
*Chongyang Li,Tianqian Zhang,Shouyin Liu*

Main category: cs.LG

TL;DR: Proposes a scenario-adaptive MU-MIMO semantic communication framework with asymmetric architecture for 6G downlink systems, featuring CSI/SNR-aware semantic encoding, neural precoding for MUI mitigation, and pilot-guided attention for implicit channel equalization.


<details>
  <summary>Details</summary>
Motivation: Semantic communication shows promise for 6G networks but faces challenges when applied to realistic downlink MU-MIMO OFDM systems due to severe multi-user interference and frequency-selective fading. Existing DJSCC schemes designed for point-to-point links suffer from performance saturation in multi-user scenarios.

Method: An asymmetric MU-MIMO semantic communication framework with: 1) scenario-aware semantic encoder that dynamically adjusts feature extraction based on CSI and SNR, 2) neural precoding network to mitigate multi-user interference in semantic domain, 3) lightweight decoder with novel pilot-guided attention mechanism for implicit channel equalization and feature calibration using reference pilot symbols.

Result: Extensive simulations over 3GPP channel models show the framework significantly outperforms DJSCC and traditional SSCC schemes in PSNR and classification accuracy, especially in low-SNR regimes, while maintaining low latency and computational cost on edge devices.

Conclusion: The proposed scenario-adaptive MU-MIMO semantic communication framework effectively addresses challenges of multi-user interference and frequency-selective fading in realistic 6G downlink systems, demonstrating superior performance over existing approaches with practical computational efficiency for edge deployment.

Abstract: Semantic Communication (SemCom) has emerged as a promising paradigm for 6G networks, aiming to extract and transmit task-relevant information rather than minimizing bit errors. However, applying SemCom to realistic downlink Multi-User Multi-Input Multi-Output (MU-MIMO) Orthogonal Frequency Division Multiplexing (OFDM) systems remains challenging due to severe Multi-User Interference (MUI) and frequency-selective fading. Existing Deep Joint Source-Channel Coding (DJSCC) schemes, primarily designed for point-to-point links, suffer from performance saturation in multi-user scenarios. To address these issues, we propose a scenario-adaptive MU-MIMO SemCom framework featuring an asymmetric architecture tailored for downlink transmission. At the transmitter, we introduce a scenario-aware semantic encoder that dynamically adjusts feature extraction based on Channel State Information (CSI) and Signal-to-Noise Ratio (SNR), followed by a neural precoding network designed to mitigate MUI in the semantic domain. At the receiver, a lightweight decoder equipped with a novel pilot-guided attention mechanism is employed to implicitly perform channel equalization and feature calibration using reference pilot symbols. Extensive simulation results over 3GPP channel models demonstrate that the proposed framework significantly outperforms DJSCC and traditional Separate Source-Channel Coding (SSCC) schemes in terms of Peak Signal-to-Noise Ratio (PSNR) and classification accuracy, particularly in low-SNR regimes, while maintaining low latency and computational cost on edge devices.

</details>


### [137] [Interpretable clustering via optimal multiway-split decision trees](https://arxiv.org/abs/2602.13586)
*Hayato Suzuki,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: A new interpretable clustering method using optimal multiway-split decision trees formulated as 0-1 integer linear optimization, achieving better accuracy and interpretability than existing binary tree approaches.


<details>
  <summary>Details</summary>
Motivation: Existing interpretable clustering methods using binary decision trees have computational inefficiencies (solving mixed-integer nonlinear optimization problems), often produce suboptimal solutions, and create excessively deep trees that are hard to interpret.

Method: Proposes interpretable clustering based on optimal multiway-split decision trees formulated as a 0-1 integer linear optimization problem. Uses one-dimensional K-means for discretization of continuous variables to enable flexible, data-driven branching.

Result: Extensive experiments on real-world datasets show the method outperforms baselines in clustering accuracy and interpretability. Produces multiway-split decision trees with concise decision rules while maintaining competitive performance across various metrics.

Conclusion: The proposed method provides a more tractable optimization approach than existing models, yielding interpretable clustering with multiway-split trees that balance accuracy and interpretability effectively.

Abstract: Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability. Our method yields multiway-split decision trees with concise decision rules while maintaining competitive performance across various evaluation metrics.

</details>


### [138] [Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?](https://arxiv.org/abs/2602.13626)
*Mingqiao Zhang,Qiyao Peng,Yumeng Wang,Chunyuan Liu,Hongtao Liu*

Main category: cs.LG

TL;DR: Benchmark data leakage in LLM-based recommendation leads to artificially inflated performance metrics when LLMs memorize benchmark data during training, masking true model capabilities.


<details>
  <summary>Details</summary>
Motivation: The integration of LLMs into recommender systems creates evaluation challenges, with benchmark data leakage being an overlooked issue that compromises reliability by allowing models to memorize test data during training.

Method: Simulate diverse data leakage scenarios through continued pre-training of foundation models on strategically blended corpora containing both in-domain and out-of-domain user-item interactions.

Result: Data leakage has a dual effect: domain-relevant leakage causes substantial but spurious performance gains, while domain-irrelevant leakage typically degrades recommendation accuracy, revealing the complex nature of this contamination.

Conclusion: Data leakage is a critical, previously unaccounted-for factor in LLM-based recommendation that significantly impacts true model performance evaluation and can lead to misleading conclusions about model capabilities.

Abstract: The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.

</details>


### [139] [Optimization-Free Graph Embedding via Distributional Kernel for Community Detection](https://arxiv.org/abs/2602.13634)
*Shuaibin Song,Kai Ming Ting,Kaifeng Zhang,Tianrun Liang*

Main category: cs.LG

TL;DR: A distribution-aware kernel method for graph embedding that addresses over-smoothing in neighborhood aggregation strategies by incorporating node and degree distributions, requiring no optimization.


<details>
  <summary>Details</summary>
Motivation: Neighborhood Aggregation Strategy (NAS) methods like GNNs and WL suffer from over-smoothing - loss of node distinguishability with increased iterations. Existing methods overlook critical network characteristics: node distributions and node degree distributions.

Method: Proposes a novel weighted distribution-aware kernel that embeds nodes while considering their distributional characteristics. The method explicitly incorporates both node distributions and node degree distributions, requires no optimization, and mitigates over-smoothing effects.

Result: The method preserves node distinguishability and expressiveness even after many iterations, achieving superior community detection performance via spectral clustering. It outperforms existing graph embedding methods including deep learning methods on standard benchmarks.

Conclusion: Incorporating node and degree distribution characteristics is crucial for expressive graph representations and effectively addresses the over-smoothing problem in NAS-based methods, enabling better performance without requiring optimization.

Abstract: Neighborhood Aggregation Strategy (NAS) is a widely used approach in graph embedding, underpinning both Graph Neural Networks (GNNs) and Weisfeiler-Lehman (WL) methods. However, NAS-based methods are identified to be prone to over-smoothing-the loss of node distinguishability with increased iterations-thereby limiting their effectiveness. This paper identifies two characteristics in a network, i.e., the distributions of nodes and node degrees that are critical for expressive representation but have been overlooked in existing methods. We show that these overlooked characteristics contribute significantly to over-smoothing of NAS-methods. To address this, we propose a novel weighted distribution-aware kernel that embeds nodes while taking their distributional characteristics into consideration. Our method has three distinguishing features: (1) it is the first method to explicitly incorporate both distributional characteristics; (2) it requires no optimization; and (3) it effectively mitigates the adverse effects of over-smoothing, allowing WL to preserve node distinguishability and expressiveness even after many iterations of embedding. Experiments demonstrate that our method achieves superior community detection performance via spectral clustering, outperforming existing graph embedding methods, including deep learning methods, on standard benchmarks.

</details>


### [140] [Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series](https://arxiv.org/abs/2602.13649)
*Li Zhang,Nital Patel,Xiuqi Li,Jessica Lin*

Main category: cs.LG

TL;DR: Joint Time Series Chain (JointTSC) extends time series chain concept to find evolving patterns across interrupted or related time series, addressing robustness issues with gaps.


<details>
  <summary>Details</summary>
Motivation: Existing time series chain definitions only work within single time series, missing unexpected evolving patterns in interrupted time series or across related time series.

Method: Introduces Joint Time Series Chain definition to handle gaps/interruptions, with effective ranking criterion to identify best chains across interrupted or related time series.

Result: Proposed approach outperforms existing TSC methods in locating unusual evolving patterns, demonstrated through extensive empirical evaluations and real-life manufacturing application from Intel.

Conclusion: JointTSC successfully addresses limitations of single-series TSC by enabling discovery of evolving patterns across interrupted or related time series, with practical utility demonstrated in real-world applications.

Abstract: Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .

</details>


### [141] [Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation](https://arxiv.org/abs/2602.13651)
*Stefan Behfar,Richard Mortier*

Main category: cs.LG

TL;DR: A fairness approach for federated learning that focuses on long-term benefit per participation opportunity rather than per-round performance, addressing systematic under-representation of intermittently available clients.


<details>
  <summary>Details</summary>
Motivation: In real-world FL systems, client participation is intermittent, heterogeneous, and correlated with data characteristics or resource constraints. Existing fairness approaches focus on equalizing loss or accuracy conditional on participation, but when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair.

Method: Proposes cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity. Introduces availability-normalized cumulative utility to disentangle unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation.

Result: Experiments on temporally skewed, non-IID federated benchmarks demonstrate that the approach substantially improves long-term representation parity while maintaining near-perfect performance.

Conclusion: The proposed cumulative utility parity framework effectively addresses fairness concerns in FL systems with uneven client participation by focusing on long-term benefits per participation opportunity rather than per-round performance metrics.

Abstract: In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.

</details>


### [142] [Zero-Order Optimization for LLM Fine-Tuning via Learnable Direction Sampling](https://arxiv.org/abs/2602.13659)
*Valery Parfenov,Grigoriy Evseev,Andrey Veprikov,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: This paper proposes a policy-driven zero-order framework that learns a sampling distribution over perturbation directions to reduce variance in gradient estimates, enabling more efficient LLM fine-tuning with substantial memory savings.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models requires extensive memory for backpropagation and optimizer states, limiting deployment in resource-constrained settings. Zero-order methods offer memory savings but suffer from high variance and poor scaling with parameter dimensionality, restricting their use to low-dimensional problems.

Method: The paper proposes a policy-driven zero-order framework that treats the sampling distribution over perturbation directions as a learnable policy. It updates this distribution to reduce variance in directional gradient estimates. The authors develop a practical algorithm implementing this idea and provide theoretical analysis showing improved gradient quality and relaxed dependence on parameter dimensionality.

Result: Empirical validation on challenging LLM fine-tuning benchmarks shows substantially improved performance compared to standard zero-order baselines. Theoretical analysis demonstrates that learned sampling distributions improve gradient information quality and relax the explicit dependence on parameter dimensionality in convergence bounds.

Conclusion: Adaptive direction sampling through learnable policy-driven zero-order methods is a promising approach to make zero-order fine-tuning viable at scale, addressing the memory constraints of traditional backpropagation-based fine-tuning for large language models.

Abstract: Fine-tuning large pretrained language models (LLMs) is a cornerstone of modern NLP, yet its growing memory demands (driven by backpropagation and large optimizer States) limit deployment in resource-constrained settings. Zero-order (ZO) methods bypass backpropagation by estimating directional derivatives from forward evaluations, offering substantial memory savings. However, classical ZO estimators suffer from high variance and an adverse dependence on the parameter dimensionality $d$, which has constrained their use to low-dimensional problems. In this work, we propose a policy-driven ZO framework that treats the sampling distribution over perturbation directions as a learnable policy and updates it to reduce the variance of directional estimates. We develop a practical algorithm implementing this idea and provide a theoretical analysis, showing that learned sampling distributions improve the quality of gradient information and relax the explicit dependence on $d$ in convergence bounds. Empirically, we validate the approach on challenging LLM fine-tuning benchmarks, demonstrating substantially improved performance compared to standard ZO baselines. Our results suggest that adaptive direction sampling is a promising route to make ZO fine-tuning viable at scale. The source code is available at https://github.com/brain-lab-research/zo_ldsd

</details>


### [143] [Optimized Certainty Equivalent Risk-Controlling Prediction Sets](https://arxiv.org/abs/2602.13660)
*Jiayi Huang,Amirmohammad Farzaneh,Osvaldo Simeone*

Main category: cs.LG

TL;DR: This paper introduces OCE-RCPS, a novel risk-controlling prediction set framework that provides high-probability guarantees on optimized certainty equivalent risk measures like CVaR, offering stronger reliability for safety-critical applications like medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing risk-controlling prediction sets (RCPS) only provide guarantees on expected risk, which fails to capture tail behavior and worst-case scenarios crucial for safety-critical applications like medical image segmentation. There's a need for stronger reliability guarantees that account for extreme risks.

Method: The paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures including conditional value-at-risk (CVaR) and entropic risk. The method uses upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability.

Result: Theoretical guarantees show OCE-RCPS satisfies probabilistic constraints for loss functions like miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while existing OCE-CRC fails to provide probabilistic guarantees.

Conclusion: OCE-RCPS provides a principled framework for obtaining high-probability guarantees on tail risk measures in safety-critical applications, addressing limitations of conventional RCPS by capturing worst-case scenarios crucial for domains like medical imaging.

Abstract: In safety-critical applications such as medical image segmentation, prediction systems must provide reliability guarantees that extend beyond conventional expected loss control. While risk-controlling prediction sets (RCPS) offer probabilistic guarantees on the expected risk, they fail to capture tail behavior and worst-case scenarios that are crucial in high-stakes settings. This paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a novel framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures, including conditional value-at-risk (CVaR) and entropic risk. OCE-RCPS leverages upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability. We establish theoretical guarantees showing that OCE-RCPS satisfies the desired probabilistic constraint for loss functions such as miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while OCE-CRC fails to provide probabilistic guarantees.

</details>


### [144] [ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer](https://arxiv.org/abs/2602.13666)
*Edward Chen,Natalie Dullerud,Pang Wei Koh,Thomas Niedermayr,Elizabeth Kidd,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: ALMo is an interactive decision support system that uses aim-limit thresholds to help clinicians navigate dosimetric tradeoffs in HDR brachytherapy planning, reducing planning time while maintaining or improving plan quality.


<details>
  <summary>Details</summary>
Motivation: Clinicians face cognitive challenges in tracking competing metrics with aim and limit thresholds during complex clinical decision-making, particularly in HDR brachytherapy where strict radiation hot spot management must be balanced against tumor coverage and organ sparing.

Method: ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and allows clinicians to interactively navigate Pareto surfaces by directly manipulating intuitive aim and limit values for dosimetric tradeoffs.

Result: In retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases showing dosimetric improvements. Planning efficiency significantly improved, reducing average time to ~17 minutes compared to conventional 30-60 minutes.

Conclusion: While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making, offering a practical solution to cognitive challenges in clinical decision support.

Abstract: In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.

</details>


### [145] [Advancing Analytic Class-Incremental Learning through Vision-Language Calibration](https://arxiv.org/abs/2602.13670)
*Binyu Zhao,Wei Zhang,Xingrui Yu,Zhaonian Zou,Ivor Tsang*

Main category: cs.LG

TL;DR: VILA is a dual-branch framework that improves class-incremental learning with pre-trained models through vision-language calibration, maintaining analytic learning efficiency while overcoming its brittleness.


<details>
  <summary>Details</summary>
Motivation: Class-incremental learning with pre-trained models faces a critical trade-off between efficient adaptation and long-term stability. Analytic learning enables rapid closed-form updates but suffers from accumulated errors and feature incompatibility. The paper identifies representation rigidity as the primary bottleneck in PTM-based analytic CIL.

Method: VILA uses a two-level vision-language calibration strategy with a dual-branch framework: 1) Feature-level geometric calibration fuses plastic, task-adapted features with a frozen universal semantic anchor; 2) Decision-level cross-modal priors rectify prediction bias. This maintains analytic learning's efficiency while overcoming its brittleness.

Result: Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. The framework harmonizes high-fidelity prediction with analytic learning simplicity.

Conclusion: VILA advances analytic class-incremental learning through a novel dual-branch framework with two-level vision-language calibration, effectively balancing adaptation efficiency with long-term stability while overcoming the brittleness of previous analytic CIL approaches.

Abstract: Class-incremental learning (CIL) with pre-trained models (PTMs) faces a critical trade-off between efficient adaptation and long-term stability. While analytic learning enables rapid, recursive closed-form updates, its efficacy is often compromised by accumulated errors and feature incompatibility. In this paper, we first conduct a systematic study to dissect the failure modes of PTM-based analytic CIL, identifying representation rigidity as the primary bottleneck. Motivated by these insights, we propose \textbf{VILA}, a novel dual-branch framework that advances analytic CIL via a two-level vision-language calibration strategy. Specifically, we coherently fuse plastic, task-adapted features with a frozen, universal semantic anchor at the feature level through geometric calibration, and leverage cross-modal priors at the decision level to rectify prediction bias. This confluence maintains analytic-learning's extreme efficiency while overcoming its inherent brittleness. Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. Our framework harmonizes high-fidelity prediction with the simplicity of analytic learning. Our code is available at https://github.com/byzhaoAI/VILA

</details>


### [146] [On the Sparsifiability of Correlation Clustering: Approximation Guarantees under Edge Sampling](https://arxiv.org/abs/2602.13684)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: The paper studies sparsification-approximation trade-offs for Correlation Clustering, showing pseudometric instances allow efficient coresets and algorithms with limited edge information, while general instances require many edges for bounded approximation.


<details>
  <summary>Details</summary>
Motivation: Correlation Clustering's strongest LP-based approximations require Θ(n³) triangle inequality constraints, which are computationally prohibitive at scale. The authors want to understand how much edge information (sparsification) is needed to retain LP-based guarantees, exploring the trade-off between data sparsity and approximation quality.

Method: 1. Analyze VC dimension of clustering disagreement class to prove optimal additive ε-coreset size; 2. Show at most binom(n,2) triangle inequalities active at LP vertices enabling exact cutting-plane solver; 3. Develop sparsified LP-PIVOT algorithm that imputes missing LP marginals via triangle inequalities; 4. Use Yao's minimax principle to prove lower bounds for general instances; 5. Establish structural dichotomy between pseudometric vs general weighted instances.

Result: 1. VC dimension of clustering disagreement class is exactly n-1 → optimal additive ε-coresets of size O~(n/ε²); 2. Exact cutting-plane solver with binom(n,2) active constraints; 3. Sparsified LP-PIVOT achieves 10/3-approximation (plus additive term) with O~(n^{3/2}) edges, proved sharp threshold; 4. Without pseudometric structure, algorithms with o(n) uniformly random edges have unbounded approximation ratio; 5. Pseudometric condition governs both tractability and robustness to incomplete information.

Conclusion: There is a fundamental structural dichotomy between pseudometric and general weighted Correlation Clustering instances. Pseudometric instances admit efficient sparsification with theoretical guarantees (coresets, cutting-plane methods, and good approximations with limited edge information), while general instances require much more edge information for bounded approximations, establishing pseudometric structure as crucial for both computational tractability and robustness to incomplete data.

Abstract: Correlation Clustering (CC) is a fundamental unsupervised learning primitive whose strongest LP-based approximation guarantees require $Θ(n^3)$ triangle inequality constraints and are prohibitive at scale. We initiate the study of \emph{sparsification--approximation trade-offs} for CC, asking how much edge information is needed to retain LP-based guarantees. We establish a structural dichotomy between pseudometric and general weighted instances. On the positive side, we prove that the VC dimension of the clustering disagreement class is exactly $n{-}1$, yielding additive $\varepsilon$-coresets of optimal size $\tilde{O}(n/\varepsilon^2)$; that at most $\binom{n}{2}$ triangle inequalities are active at any LP vertex, enabling an exact cutting-plane solver; and that a sparsified variant of LP-PIVOT, which imputes missing LP marginals via triangle inequalities, achieves a robust $\frac{10}{3}$-approximation (up to an additive term controlled by an empirically computable imputation-quality statistic $\overlineΓ_w$) once $\tildeΘ(n^{3/2})$ edges are observed, a threshold we prove is sharp. On the negative side, we show via Yao's minimax principle that without pseudometric structure, any algorithm observing $o(n)$ uniformly random edges incurs an unbounded approximation ratio, demonstrating that the pseudometric condition governs not only tractability but also the robustness of CC to incomplete information.

</details>


### [147] [Physics Aware Neural Networks: Denoising for Magnetic Navigation](https://arxiv.org/abs/2602.13690)
*Aritra Das,Yashas Shende,Muskaan Chugh,Reva Laxmi Chauhan,Arghya Pathak,Debayan Gupta*

Main category: cs.LG

TL;DR: Proposes a physics-informed deep learning framework with divergence-free and E(3)-equivariance constraints for magnetic anomaly navigation, using synthetic data generation and outperforming classical methods.


<details>
  <summary>Details</summary>
Motivation: Magnetic anomaly navigation is a GPS alternative, but aircraft-induced magnetic noise corrupts data. Classical models like Tolles-Lawson inadequately handle stochastic noise in navigation context.

Method: Neural network outputs vector potential A, with magnetic field defined as its curl (divergence-free constraint). E(3)-equivariance is achieved via spherical harmonics. Uses synthetic data from WMM with conditional GANs. Evaluates architectures including CNNs, MLPs, Liquid Time Constant models, and Contiformers.

Result: Contiformer (with continuous-time dynamics and long-term memory) performs best. Physics constraints improve predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.

Conclusion: Embedding physics-based constraints (divergence-free and E(3)-equivariance) acts as implicit regularizer, improving performance for magnetic anomaly navigation. Contiformer architecture with synthetic data generation addresses data scarcity and stochastic noise challenges.

Abstract: Magnetic-anomaly navigation, leveraging small-scale variations in the Earth's magnetic field, is a promising alternative when GPS is unavailable or compromised. Airborne systems face a key challenge in extracting geomagnetic field data: the aircraft itself induces magnetic noise. Although the classical Tolles-Lawson model addresses this, it inadequately handles stochastically corrupted magnetic data required for navigation. To address stochastic noise, we propose a framework based on two physics-based constraints: divergence-free vector field and E(3)-equivariance. These ensure the learned magnetic field obeys Maxwell's equations and that outputs transform correctly with sensor position/orientation. The divergence-free constraint is implemented by training a neural network to output a vector potential $A$, with the magnetic field defined as its curl. For E(3)-equivariance, we use tensor products of geometric tensors representable via spherical harmonics with known rotational transformations. Enforcing physical consistency and restricting the admissible function space acts as an implicit regularizer that improves spatio-temporal performance. We present ablation studies evaluating each constraint alone and jointly across CNNs, MLPs, Liquid Time Constant models, and Contiformers. Continuous-time dynamics and long-term memory are critical for modelling magnetic time series; the Contiformer architecture, which provides both, outperforms state-of-the-art methods. To mitigate data scarcity, we generate synthetic datasets using the World Magnetic Model (WMM) with time-series conditional GANs, producing realistic, temporally consistent magnetic sequences across varied trajectories and environments. Experiments show that embedding these constraints significantly improves predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.

</details>


### [148] [Attention Head Entropy of LLMs Predicts Answer Correctness](https://arxiv.org/abs/2602.13699)
*Sophie Ostmeier,Brian Axelrod,Maya Varma,Asad Aali,Yabin Zhang,Magdalini Paschali,Sanmi Koyejo,Curtis Langlotz,Akshay Chaudhari*

Main category: cs.LG

TL;DR: Head Entropy uses attention entropy patterns in LLMs to detect incorrect answers without human evaluation, outperforming existing methods on both in-distribution and out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate plausible but incorrect answers, which is dangerous in safety-critical domains like medicine. Current evaluation methods (human evaluation and LLM-as-judge) are either expensive or risk introducing hidden errors. White-box methods using model internals exist but need to prove they can predict answer correctness and generalize across domains.

Method: Introduces Head Entropy, which measures the spread of attention mass using per-head 2-Renyi entropies. Uses sparse logistic regression on these attention entropy patterns to predict answer correctness. Method analyzes attention patterns both during and even before answer generation.

Result: Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, outperforming the closest baseline by +8.5% AUROC on average. Attention patterns over the question/context alone (before answer generation) already provide predictive signal with +17.7% AUROC over the closest baseline. Validated across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.

Conclusion: Head Entropy provides an effective white-box approach for detecting LLM answer correctness by analyzing attention entropy patterns, offering better generalization across domains than existing methods and showing predictive signal even before answer generation begins.

Abstract: Large language models (LLMs) often generate plausible yet incorrect answers, posing risks in safety-critical settings such as medicine. Human evaluation is expensive, and LLM-as-judge approaches risk introducing hidden errors. Recent white-box methods detect contextual hallucinations using model internals, focusing on the localization of the attention mass, but two questions remain open: do these approaches extend to predicting answer correctness, and do they generalize out-of-domains? We introduce Head Entropy, a method that predicts answer correctness from attention entropy patterns, specifically measuring the spread of the attention mass. Using sparse logistic regression on per-head 2-Renyi entropies, Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, it outperforms the closest baseline on average by +8.5% AUROC. We further show that attention patterns over the question/context alone, before answer generation, already carry predictive signal using Head Entropy with on average +17.7% AUROC over the closest baseline. We evaluate across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.

</details>


### [149] [Optimal Regret for Policy Optimization in Contextual Bandits](https://arxiv.org/abs/2602.13700)
*Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: First optimal regret bound for policy optimization in contextual bandits with general function approximation, achieving Õ(√(K|A|log|F|)) regret.


<details>
  <summary>Details</summary>
Motivation: Bridge gap between theory and practice by showing widely used policy optimization methods for contextual bandits can achieve rigorously-proved optimal regret bounds.

Method: Policy optimization technique applied to stochastic contextual multi-armed bandit with general offline function approximation.

Result: Achieves optimal regret bound of Õ(√(K|A|log|F|)), where K is rounds, A is arms, F is function class. Algorithm is efficient and supported by empirical evaluation.

Conclusion: Demonstrates that practical policy optimization methods can achieve theoretically optimal performance in contextual bandits with general function approximation.

Abstract: We present the first high-probability optimal regret bound for a policy optimization technique applied to the problem of stochastic contextual multi-armed bandit (CMAB) with general offline function approximation. Our algorithm is both efficient and achieves an optimal regret bound of $\widetilde{O}(\sqrt{ K|\mathcal{A}|\log|\mathcal{F}|})$, where $K$ is the number of rounds, $\mathcal{A}$ is the set of arms, and $\mathcal{F}$ is the function class used to approximate the losses. Our results bridge the gap between theory and practice, demonstrating that the widely used policy optimization methods for the contextual bandit problem can achieve a rigorously-proved optimal regret bound. We support our theoretical results with an empirical evaluation of our algorithm.

</details>


### [150] [Near-Optimal Regret for Policy Optimization in Contextual MDPs with General Offline Function Approximation](https://arxiv.org/abs/2602.13706)
*Orin Levy,Aviv Rosenberg,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: OPO-CMDP is the first policy optimization algorithm for stochastic Contextual MDPs under general offline function approximation, achieving optimal regret bounds with respect to state and action space sizes.


<details>
  <summary>Details</summary>
Motivation: Current methods for Contextual Markov Decision Processes (CMDPs) lack optimal dependence on state and action space sizes. The authors aim to develop a computationally superior and theoretically near-optimal algorithm for solving CMDPs under general offline function approximation.

Method: The authors introduce OPO-CMDP, an optimistic policy optimization algorithm designed for stochastic CMDPs. The method leverages general offline function approximation with finite function classes for approximating losses and dynamics.

Result: OPO-CMDP achieves a high probability regret bound of $\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)})$, which is the first regret bound with optimal dependence on $|S|$ and $|A|$, directly improving upon the state-of-the-art from Qian, Hu, and Simchi-Levi (2024).

Conclusion: Optimistic policy optimization provides a natural, computationally superior and theoretically near-optimal approach for solving CMDPs, demonstrating that OPO-CMDP significantly advances the state-of-the-art in contextual reinforcement learning.

Abstract: We introduce \texttt{OPO-CMDP}, the first policy optimization algorithm for stochastic Contextual Markov Decision Process (CMDPs) under general offline function approximation. Our approach achieves a high probability regret bound of $\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)}),$ where $S$ and $A$ denote the state and action spaces, $H$ the horizon length, $T$ the number of episodes, and $\mathcal{F}, \mathcal{P}$ the finite function classes used to approximate the losses and dynamics, respectively. This is the first regret bound with optimal dependence on $|S|$ and $|A|$, directly improving the current state-of-the-art (Qian, Hu, and Simchi-Levi, 2024). These results demonstrate that optimistic policy optimization provides a natural, computationally superior and theoretically near-optimal path for solving CMDPs.

</details>


### [151] [HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.13710)
*Xin Yan,Zhenglin Wan,Feiyang Ye,Xingrui Yu,Hangyu Du,Yang You,Ivor Tsang*

Main category: cs.LG

TL;DR: HBVLA is a Vision-Language-Action binarization framework that uses Hessian-based policy-aware weight importance, sparse orthogonal transforms, and Haar-domain group-wise 1-bit quantization to maintain VLA performance under extreme compression for robotic deployment.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action (VLA) models are computationally expensive and memory-intensive, hindering deployment on resource-constrained robots and edge platforms. While 1-bit binarization can improve efficiency, existing methods suffer from significant distribution gaps between binarized and full-precision weights, leading to quantization error accumulation during long-horizon closed-loop execution and severe performance degradation.

Method: HBVLA employs three key techniques: 1) Policy-aware enhanced Hessian to identify action-critical weights, 2) Sparse orthogonal transforms for non-salient weights to create low-entropy intermediate states, and 3) Group-wise 1-bit quantization in the Haar domain for both salient and non-salient weights.

Result: The framework achieves impressive performance retention: OpenVLA-OFT retains 92.2% of full-precision performance on LIBERO, and CogAct retains 93.6% on SimplerEnv, significantly outperforming state-of-the-art binarization methods. Real-world evaluation shows only marginal success-rate degradation compared to full-precision models, demonstrating robust deployability under tight hardware constraints.

Conclusion: HBVLA provides a practical foundation for ultra-low-bit VLA quantization, enabling reliable deployment of instruction-following embodied control models on hardware-limited robotic platforms while maintaining performance close to full-precision baselines.

Abstract: Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.

</details>


### [152] [Data-driven Bi-level Optimization of Thermal Power Systems with embedded Artificial Neural Networks](https://arxiv.org/abs/2602.13746)
*Talha Ansar,Muhammad Mujtaba Abbas,Ramit Debnath,Vivek Dua,Waqar Muhammad Ashraf*

Main category: cs.LG

TL;DR: A machine learning-based bi-level optimization framework (ANN-KKT) efficiently optimizes industrial thermal power systems, validated on benchmarks and real-world plants, achieving high performance with low computational time.


<details>
  <summary>Details</summary>
Motivation: Industrial thermal power systems have coupled performance variables with hierarchical order of importance, making simultaneous optimization computationally challenging or infeasible, which limits integrated and computationally scalable operation optimization.

Method: A fully machine learning-powered bi-level optimization framework is proposed, where objective functions are approximated by artificial neural network (ANN) models, and the lower-level problem is analytically embedded through Karush-Kuhn-Tucker (KKT) optimality conditions, resulting in a reformulated single-level optimization framework (ANN-KKT).

Result: The ANN-KKT framework yields comparable solutions to bi-level solutions on benchmark problems, with marginal computational time (0.22 to 0.88 s). Applied to real-world systems, it achieves optimal power outputs of 583 MW (coal) and 402 MW (gas turbine) at optimal turbine heat rates of 7337 kJ/kWh and 7542 kJ/kWh, respectively, and delineates a feasible and robust operating envelope accounting for uncertainty.

Conclusion: The ANN-KKT framework offers a scalable and computationally efficient route for hierarchical, data-driven optimization of industrial thermal power systems, enabling energy-efficient operations of large-scale engineering systems and contributing to Industry 5.0.

Abstract: Industrial thermal power systems have coupled performance variables with hierarchical order of importance, making their simultaneous optimization computationally challenging or infeasible. This barrier limits the integrated and computationally scaleable operation optimization of industrial thermal power systems. To address this issue for large-scale engineering systems, we present a fully machine learning-powered bi-level optimization framework for data-driven optimization of industrial thermal power systems. The objective functions of upper and lower levels are approximated by artificial neural network (ANN) models and the lower-level problem is analytically embedded through Karush-Kuhn-Tucker (KKT) optimality conditions. The reformulated single level optimization framework integrating ANN models and KKT constraints (ANN-KKT) is validated on benchmark problems and on real-world power generation operation of 660 MW coal power plant and 395 MW gas turbine system. The results reveal a comparable solutions obtained from the proposed ANN-KKT framework to the bi-level solutions of the benchmark problems. Marginal computational time requirement (0.22 to 0.88 s) to compute optimal solutions yields 583 MW (coal) and 402 MW (gas turbine) of power output at optimal turbine heat rate of 7337 kJ/kWh and 7542 kJ/kWh, respectively. In addition, the method expands to delineate a feasible and robust operating envelope that accounts for uncertainty in operating variables while maximizing thermal efficiency in various scenarios. These results demonstrate that ANN-KKT offers a scalable and computationally efficient route for hierarchical, data-driven optimization of industrial thermal power systems, achieving energy-efficient operations of large-scale engineering systems and contributing to industry 5.0.

</details>


### [153] [Discrete Double-Bracket Flows for Isotropic-Noise Invariant Eigendecomposition](https://arxiv.org/abs/2602.13759)
*ZhiMing Li,JiaHe Feng*

Main category: cs.LG

TL;DR: A matrix-free eigendecomposition method with double-bracket flow that achieves invariance to isotropic noise, yielding convergence rates dependent only on trace-free covariance components.


<details>
  <summary>Details</summary>
Motivation: Standard stochastic approximation methods for eigendecomposition face problems with stability depending on covariance operator norms, or slow adaptation due to vanishing updates when dealing with noisy covariance operators C_k = C_sig + σ_k^2I + E_k.

Method: Introduces a discrete double-bracket flow whose generator is invariant to isotropic shifts, achieving pathwise invariance to σ_k^2I at discrete-time level. The trajectory and maximal stable step size η_max ∝ 1/||C_e||_2^2 depend only on trace-free covariance C_e.

Result: Establishes global convergence via strict-saddle geometry and input-to-state stability analysis, with sample complexity scaling as O(||C_e||_2^2/(Δ^2ε)) under trace-free perturbations. Explicit characterization of degenerate blocks yields accelerated O(log(1/ζ)) saddle-escape rate and high-probability finite-time convergence guarantee.

Conclusion: The proposed matrix-free eigendecomposition method achieves improved stability and convergence by being invariant to isotropic noise components, with theoretical guarantees on convergence rates that depend only on the signal-relevant trace-free covariance components.

Abstract: We study matrix-free eigendecomposition under a matrix-vector product (MVP) oracle, where each step observes a covariance operator $C_k = C_{sig} + σ_k^2 I + E_k$. Standard stochastic approximation methods either use fixed steps that couple stability to $\|C_k\|_2$, or adapt steps in ways that slow down due to vanishing updates. We introduce a discrete double-bracket flow whose generator is invariant to isotropic shifts, yielding pathwise invariance to $σ_k^2 I$ at the discrete-time level. The resulting trajectory and a maximal stable step size $η_{max} \propto 1/\|C_e\|_2^2$ depend only on the trace-free covariance $C_e$. We establish global convergence via strict-saddle geometry for the diagonalization objective and an input-to-state stability analysis, with sample complexity scaling as $O(\|C_e\|_2^2 / (Δ^2 ε))$ under trace-free perturbations. An explicit characterization of degenerate blocks yields an accelerated $O(\log(1/ζ))$ saddle-escape rate and a high-probability finite-time convergence guarantee.

</details>


### [154] [On Representation Redundancy in Large-Scale Instruction Tuning Data Selection](https://arxiv.org/abs/2602.13773)
*Youwei Shu,Shaomian Zheng,Dingnan Jin,Wenjie Qu,Ziyao Guo,Qing Cui,Jun Zhou,Jiaheng Zhang*

Main category: cs.LG

TL;DR: CRDS proposes compressed representation data selection using Rademacher projection (CRDS-R) or whitening-based reduction (CRDS-W) to address redundancy in LLM embeddings, achieving strong performance with minimal data.


<details>
  <summary>Details</summary>
Motivation: Data quality is crucial for LLM training, but systematic methods for industrial-scale data selection in instruction tuning remain underexplored. State-of-the-art LLM encoders produce highly redundant semantic embeddings that limit effective data selection.

Method: Two variants: CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations. CRDS-W employs whitening-based dimensionality reduction to improve representational quality.

Result: Both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. CRDS-W achieves strong performance using only 3.5% of data, surpassing full-data baseline by average 0.71% across four datasets.

Conclusion: Compressed Representation Data Selection effectively addresses redundancy in LLM embeddings for instruction tuning data selection, demonstrating significant efficiency gains and performance improvements with minimal data requirements.

Abstract: Data quality is a crucial factor in large language models training. While prior work has shown that models trained on smaller, high-quality datasets can outperform those trained on much larger but noisy or low-quality corpora, systematic methods for industrial-scale data selection in instruction tuning remain underexplored. In this work, we study instruction-tuning data selection through the lens of semantic representation similarity and identify a key limitation of state-of-the-art LLM encoders: they produce highly redundant semantic embeddings. To mitigate this redundancy, we propose Compressed Representation Data Selection (CRDS), a novel framework with two variants. CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations, while CRDS-W employs whitening-based dimensionality reduction to improve representational quality. Experimental results demonstrate that both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. Notably, CRDS-W achieves strong performance using only 3.5% of the data, surpassing the full-data baseline by an average of 0.71% across four datasets. Our code is available at https://github.com/tdano1/CRDS.

</details>


### [155] [MEMTS: Internalizing Domain Knowledge via Parameterized Memory for Retrieval-Free Domain Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2602.13783)
*Xiaoyun Yu,Li fan,Xiangfei Qiu,Nanqing Dong,Yonggui Huang,Honggang Qi,Geguang Pu,Wanli Ouyang,Xi Chen,Jilin Hu*

Main category: cs.LG

TL;DR: MEMTS is a lightweight, plug-and-play method for domain adaptation in time series forecasting that uses learnable latent prototypes to internalize domain-specific temporal patterns, enabling retrieval-free adaptation with constant-time inference.


<details>
  <summary>Details</summary>
Motivation: Current time series foundation models suffer from performance degradation in real-world vertical domains due to temporal distribution shifts and domain-specific periodic structures. Existing solutions (DAPT and RAG) have limitations: DAPT causes catastrophic forgetting of global patterns, while RAG introduces significant retrieval overhead that hampers scalability for real-time stream processing.

Method: MEMTS uses a Knowledge Persistence Module (KPM) that internalizes domain-specific temporal dynamics (seasonal patterns, trends) into compact learnable latent prototypes. These prototypes transform fragmented historical observations into continuous, parameterized knowledge representations without modifying the frozen foundation model architecture.

Result: Extensive experiments on multiple datasets show that MEMTS achieves state-of-the-art performance for domain adaptation in time series forecasting while maintaining constant-time inference with near-zero latency and effectively mitigating catastrophic forgetting.

Conclusion: MEMTS provides an efficient, retrieval-free approach to domain adaptation that balances performance, computational efficiency, and preservation of learned global patterns, addressing the scalability bottleneck in real-time time series forecasting applications.

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated exceptional performance in generalized forecasting, their performance often degrades significantly when deployed in real-world vertical domains characterized by temporal distribution shifts and domain-specific periodic structures. Current solutions are primarily constrained by two paradigms: Domain-Adaptive Pretraining (DAPT), which improves short-term domain fitting but frequently disrupts previously learned global temporal patterns due to catastrophic forgetting; and Retrieval-Augmented Generation (RAG), which incorporates external knowledge but introduces substantial retrieval overhead. This creates a severe scalability bottleneck that fails to meet the high-efficiency requirements of real-time stream processing. To break this impasse, we propose Memory for Time Series (MEMTS), a lightweight and plug-and-play method for retrieval-free domain adaptation in time series forecasting. The key component of MEMTS is a Knowledge Persistence Module (KPM), which internalizes domain-specific temporal dynamics, such as recurring seasonal patterns and trends into a compact set of learnable latent prototypes. In doing so, it transforms fragmented historical observations into continuous, parameterized knowledge representations. This paradigm shift enables MEMTS to achieve accurate domain adaptation with constant-time inference and near-zero latency, while effectively mitigating catastrophic forgetting of general temporal patterns, all without requiring any architectural modifications to the frozen TSFM backbone. Extensive experiments on multiple datasets demonstrate the SOTA performance of MEMTS.

</details>


### [156] [MechPert: Mechanistic Consensus as an Inductive Bias for Unseen Perturbation Prediction](https://arxiv.org/abs/2602.13791)
*Marc Boubnovski Martell,Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Robert Kitchen,Jesper Ferkinghoff-Borg,Jialin Yu,Philip Torr,Kaspar Märten*

Main category: cs.LG

TL;DR: MechPert: LLM-driven framework for predicting transcriptional responses to unseen genetic perturbations by generating directed regulatory hypotheses instead of relying on functional similarity.


<details>
  <summary>Details</summary>
Motivation: Existing approaches use static knowledge graphs or language models that retrieve symmetric co-occurrence associations rather than directed regulatory logic. There's a need to improve prediction of transcriptional responses to unseen genetic perturbations for understanding gene regulation and prioritizing large-scale experiments.

Method: Multiple LLM agents independently propose candidate regulators with confidence scores, which are aggregated through a consensus mechanism to filter spurious associations. This produces weighted neighborhoods for downstream prediction of transcriptional responses.

Result: On Perturb-seq benchmarks across four human cell lines, MechPert improves Pearson correlation by up to 10.5% over similarity-based baselines in low-data regimes (N=50 observed perturbations). For experimental design, MechPert-selected anchor genes outperform standard network centrality heuristics by up to 46% in well-characterized cell lines.

Conclusion: MechPert demonstrates that LLM agents can effectively generate directed regulatory hypotheses to improve perturbation prediction and experimental design, outperforming existing similarity-based approaches and network centrality methods.

Abstract: Predicting transcriptional responses to unseen genetic perturbations is essential for understanding gene regulation and prioritizing large-scale perturbation experiments. Existing approaches either rely on static, potentially incomplete knowledge graphs, or prompt language models for functionally similar genes, retrieving associations shaped by symmetric co-occurrence in scientific text rather than directed regulatory logic. We introduce MechPert, a lightweight framework that encourages LLM agents to generate directed regulatory hypotheses rather than relying solely on functional similarity. Multiple agents independently propose candidate regulators with associated confidence scores; these are aggregated through a consensus mechanism that filters spurious associations, producing weighted neighborhoods for downstream prediction. We evaluate MechPert on Perturb-seq benchmarks across four human cell lines. For perturbation prediction in low-data regimes ($N=50$ observed perturbations), MechPert improves Pearson correlation by up to 10.5\% over similarity-based baselines. For experimental design, MechPert-selected anchor genes outperform standard network centrality heuristics by up to 46\% in well-characterized cell lines.

</details>


### [157] [Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting](https://arxiv.org/abs/2602.13802)
*Xiaoyu Tao,Mingyue Cheng,Chuang Jiang,Tian Gao,Huanjian Zhang,Yaguo Liu*

Main category: cs.LG

TL;DR: Cast-R1 reformulates time series forecasting as a sequential decision-making problem using a tool-augmented agent with memory-based state management, enabling iterative refinement through self-reflection.


<details>
  <summary>Details</summary>
Motivation: Traditional model-centric forecasting approaches struggle with complex and evolving settings because they lack autonomous evidence acquisition, reasoning about future changes, and iterative refinement capabilities.

Method: Proposes Cast-R1 with: 1) memory-based state management for decision-relevant information, 2) tool-augmented agentic workflow interacting with modular toolkit (statistical features, lightweight models), 3) two-stage training combining supervised fine-tuning with multi-turn reinforcement learning, and 4) curriculum learning to progressively increase task difficulty.

Result: Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1.

Conclusion: This work provides a practical step toward exploring agentic paradigms for time series modeling, showing promising results with the proposed approach.

Abstract: Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.

</details>


### [158] [Fast Physics-Driven Untrained Network for Highly Nonlinear Inverse Scattering Problems](https://arxiv.org/abs/2602.13805)
*Yutong Du,Zicheng Liu,Yi Huang,Bazargul Matkerim,Bo Qi,Yali Zong,Peixian Han*

Main category: cs.LG

TL;DR: A Fourier-spectral solver enables real-time electromagnetic inverse scattering by reducing dimensionality through truncated Fourier basis, achieving 100× speedup over untrained neural networks.


<details>
  <summary>Details</summary>
Motivation: Untrained neural networks (UNNs) provide high-fidelity electromagnetic inverse scattering but suffer from high-dimensional spatial-domain optimization that limits computational speed and prevents real-time applications.

Method: Proposes a Real-Time Physics-Driven Fourier-Spectral (PDF) solver that: 1) expands induced currents using truncated Fourier basis for low-dimensional optimization, 2) integrates contraction integral equation (CIE) to handle high-contrast nonlinearity, 3) uses contrast-compensated operator (CCO) to correct spectral-induced attenuation, and 4) formulates a bridge-suppressing loss to enhance boundary sharpness between scatterers.

Result: Achieves 100-fold speedup over state-of-the-art UNNs with robust performance under noise and antenna uncertainties, enabling sub-second reconstruction for real-time microwave imaging applications.

Conclusion: The PDF solver provides an efficient framework for real-time electromagnetic inverse scattering through spectral-domain dimensionality reduction, overcoming computational limitations of UNNs while maintaining robustness to practical measurement challenges.

Abstract: Untrained neural networks (UNNs) offer high-fidelity electromagnetic inverse scattering reconstruction but are computationally limited by high-dimensional spatial-domain optimization. We propose a Real-Time Physics-Driven Fourier-Spectral (PDF) solver that achieves sub-second reconstruction through spectral-domain dimensionality reduction. By expanding induced currents using a truncated Fourier basis, the optimization is confined to a compact low-frequency parameter space supported by scattering measurements. The solver integrates a contraction integral equation (CIE) to mitigate high-contrast nonlinearity and a contrast-compensated operator (CCO) to correct spectral-induced attenuation. Furthermore, a bridge-suppressing loss is formulated to enhance boundary sharpness between adjacent scatterers. Numerical and experimental results demonstrate a 100-fold speedup over state-of-the-art UNNs with robust performance under noise and antenna uncertainties, enabling real-time microwave imaging applications.

</details>


### [159] [AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning](https://arxiv.org/abs/2602.13807)
*Xiaoyu Tao,Yuchong Wu,Mingyue Cheng,Ze Guo,Tian Gao*

Main category: cs.LG

TL;DR: AnomaMind is an agentic framework that reformulates time series anomaly detection as a sequential decision-making process, using a coarse-to-fine workflow with adaptive feature preparation and self-reflective refinement, supported by a hybrid inference mechanism, leading to improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing time series anomaly detection methods treat the task as a purely discriminative prediction with fixed inputs, struggling with context-dependent or diverse anomaly patterns due to lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement.

Method: AnomaMind operates through a structured workflow: coarse-to-fine localization of anomalous intervals, multi-turn tool interactions for adaptive feature preparation, and self-reflective refinement of decisions. It employs a hybrid inference mechanism where general-purpose models handle tool interaction and self-reflection, while core detection decisions are learned via reinforcement learning under workflow-level feedback.

Result: Extensive experiments across diverse settings show that AnomaMind consistently improves anomaly detection performance. The code is publicly available.

Conclusion: AnomaMind's agentic framework, which reformulates anomaly detection as a sequential decision-making process supported by adaptive feature preparation, reasoning-aware detection, and iterative refinement, consistently improves performance across diverse settings, demonstrating its effectiveness and flexibility.

Abstract: Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.

</details>


### [160] [Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation](https://arxiv.org/abs/2602.13810)
*Guojian Zhan,Letian Tao,Pengcheng Wang,Yixiao Wang,Yiheng Li,Yuxin Chen,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: MVP is a new generative policy that uses mean velocity field modeling for fast one-step action generation with improved expressiveness through instantaneous velocity constraints.


<details>
  <summary>Details</summary>
Motivation: Flow-based policies in RL face a trade-off between expressiveness and computational burden, with existing methods requiring multiple flow steps that impact efficiency.

Method: Proposes mean velocity policy (MVP) that models mean velocity field for fastest one-step action generation, introduces instantaneous velocity constraint (IVC) during training to improve expressiveness, theoretically proves IVC serves as crucial boundary condition.

Result: Achieves state-of-the-art success rates across Robomimic and OGBench robotic manipulation tasks, delivers substantial improvements in training and inference speed over flow-based policy baselines.

Conclusion: MVP effectively addresses the expressiveness-efficiency trade-off in flow-based policies, offering both high performance and computational advantages for practical RL applications.

Abstract: Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.

</details>


### [161] [Pawsterior: Variational Flow Matching for Structured Simulation-Based Inference](https://arxiv.org/abs/2602.13813)
*Jorge Carrasco-Pollo,Floor Eijkelboom,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: Pawsterior is a variational flow-matching framework that handles structured domains (bounded/geometric constraints) and discrete latent variables in simulation-based inference, improving numerical stability and posterior fidelity while enabling previously inaccessible SBI problems.


<details>
  <summary>Details</summary>
Motivation: Standard flow-matching methods for SBI operate in unconstrained spaces, making them inefficient for problems with structured domains (bounded physical parameters) and incompatible with discrete latent structure, limiting their applicability to realistic scientific problems.

Method: Introduces endpoint-induced affine geometric confinement that incorporates domain geometry directly into inference via two-sided variational model. Uses variational parameterization to handle both geometric constraints and discrete latent structure (e.g., switching systems).

Result: Improved numerical stability during sampling and consistently better posterior fidelity, demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Enables SBI tasks involving discrete latent structure previously incompatible with flow-matching.

Conclusion: Pawsterior extends flow-matching to a broader class of structured SBI problems by addressing both geometric constraints and discrete latent structure, making previously inaccessible problems now solvable.

Abstract: We introduce Pawsterior, a variational flow-matching framework for improved and extended simulation-based inference (SBI). Many SBI problems involve posteriors constrained by structured domains, such as bounded physical parameters or hybrid discrete-continuous variables, yet standard flow-matching methods typically operate in unconstrained spaces. This mismatch leads to inefficient learning and difficulty respecting physical constraints. Our contributions are twofold. First, generalizing the geometric inductive bias of CatFlow, we formalize endpoint-induced affine geometric confinement, a principle that incorporates domain geometry directly into the inference process via a two-sided variational model. This formulation improves numerical stability during sampling and leads to consistently better posterior fidelity, as demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Second, and more importantly, our variational parameterization enables SBI tasks involving discrete latent structure (e.g., switching systems) that are fundamentally incompatible with conventional flow-matching approaches. By addressing both geometric constraints and discrete latent structure, Pawsterior extends flow-matching to a broader class of structured SBI problems that were previously inaccessible.

</details>


### [162] [Testing For Distribution Shifts with Conditional Conformal Test Martingales](https://arxiv.org/abs/2602.13848)
*Shalev Shaer,Yarin Bar,Drew Prinster,Yaniv Romano*

Main category: cs.LG

TL;DR: A sequential test for distribution shift detection that avoids test-time contamination by using a fixed reference dataset, providing faster detection with anytime-valid type-I error control.


<details>
  <summary>Details</summary>
Motivation: Existing conformal test martingale (CTM) detectors suffer from test-time contamination - after a distribution shift occurs, post-shift observations enter the reference set and dilute evidence for the shift, increasing detection delay and reducing power.

Method: Proposes a sequential test that compares each new sample to a fixed null reference dataset rather than continually growing the reference set. Uses a robust martingale construction that remains valid conditional on the null reference data by explicitly accounting for estimation error from the finite reference set.

Result: The method achieves anytime-valid type-I error control with guarantees of asymptotic power one and bounded expected detection delay. Empirically detects shifts faster than standard CTMs.

Conclusion: Provides a powerful and reliable distribution-shift detector that avoids contamination by design, offering improved performance over existing CTM approaches.

Abstract: We propose a sequential test for detecting arbitrary distribution shifts that allows conformal test martingales (CTMs) to work under a fixed, reference-conditional setting. Existing CTM detectors construct test martingales by continually growing a reference set with each incoming sample, using it to assess how atypical the new sample is relative to past observations. While this design yields anytime-valid type-I error control, it suffers from test-time contamination: after a change, post-shift observations enter the reference set and dilute the evidence for distribution shift, increasing detection delay and reducing power.
  In contrast, our method avoids contamination by design by comparing each new sample to a fixed null reference dataset. Our main technical contribution is a robust martingale construction that remains valid conditional on the null reference data, achieved by explicitly accounting for the estimation error in the reference distribution induced by the finite reference set. This yields anytime-valid type-I error control together with guarantees of asymptotic power one and bounded expected detection delay. Empirically, our method detects shifts faster than standard CTMs, providing a powerful and reliable distribution-shift detector.

</details>


### [163] [sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals](https://arxiv.org/abs/2602.13857)
*Weixuan Yuan,Zengrui Jin,Yichen Wang,Donglin Xie,Ziyi Ye,Chao Zhang,Xuesong Chen*

Main category: cs.LG

TL;DR: sleep2vec is a foundation model for heterogeneous nocturnal biosignals that uses cross-modal alignment and metadata-aware contrastive learning to handle diverse modalities and sensor dropout, enabling robust sleep staging and clinical assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional PSG devices, bedside monitors, and wearables capture diverse nocturnal biosignals, but heterogeneity across devices and frequent sensor dropout present challenges for unified modeling of these multimodal signals.

Method: sleep2vec uses contrastive pre-training on 42,249 overnight recordings across nine modalities with a Demography, Age, Site & History-aware InfoNCE objective that incorporates physiological and acquisition metadata to dynamically weight negatives and mitigate cohort-specific shortcuts.

Result: sleep2vec consistently outperforms strong baselines on downstream sleep staging and clinical outcome assessment, remains robust to any subset of available modalities and sensor dropout, and enables characterization of scaling laws for nocturnal biosignals.

Conclusion: Unified cross-modal alignment coupled with principled scaling enables label-efficient, general-purpose modeling of real-world nocturnal biosignals.

Abstract: Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout. We further characterize, to our knowledge for the first time, scaling laws for nocturnal biosignals with respect to modality diversity and model capacity. Together, these results show that unified cross-modal alignment, coupled with principled scaling, enables label-efficient, general-purpose modelling of real-world nocturnal biosignals.

</details>


### [164] [Sufficient Conditions for Stability of Minimum-Norm Interpolating Deep ReLU Networks](https://arxiv.org/abs/2602.13910)
*Ouns El Harzli,Yoonsoo Nam,Ilja Kuzborskij,Bernardo Cuenca Grau,Ard A. Louis*

Main category: cs.LG

TL;DR: This paper analyzes algorithmic stability for deep ReLU homogeneous neural networks achieving zero training error with minimum-norm interpolation, finding stability requires a stable sub-network followed by a low-rank layer.


<details>
  <summary>Details</summary>
Motivation: While algorithmic stability is a classical framework for analyzing generalization error, it has had limited success in analyses of deep neural networks. The paper aims to investigate stability conditions for deep ReLU homogeneous networks that achieve zero training error through minimum-norm interpolation.

Method: The study analyzes deep ReLU homogeneous neural networks achieving zero training error with minimum-norm interpolation (smallest L₂ norm parameters). It examines stability conditions through theoretical analysis of network architecture, particularly focusing on whether networks contain stable sub-networks followed by layers with low-rank weight matrices.

Result: Two key findings: 1) Networks are stable when they contain a (possibly small) stable sub-network followed by a layer with a low-rank weight matrix. 2) Networks are not guaranteed to be stable even when they contain a stable sub-network, if the following layer is not low-rank. The low-rank assumption is supported by empirical and theoretical evidence that training deep neural networks is biased toward low-rank weight matrices.

Conclusion: Algorithmic stability in deep ReLU homogeneous networks with minimum-norm interpolation depends critically on the presence of a stable sub-network followed by a low-rank layer, providing theoretical conditions for stability in overparameterized neural networks trained by gradient-based algorithms.

Abstract: Algorithmic stability is a classical framework for analyzing the generalization error of learning algorithms. It predicts that an algorithm has small generalization error if it is insensitive to small perturbations in the training set such as the removal or replacement of a training point. While stability has been demonstrated for numerous well-known algorithms, this framework has had limited success in analyses of deep neural networks. In this paper we study the algorithmic stability of deep ReLU homogeneous neural networks that achieve zero training error using parameters with the smallest $L_2$ norm, also known as the minimum-norm interpolation, a phenomenon that can be observed in overparameterized models trained by gradient-based algorithms. We investigate sufficient conditions for such networks to be stable. We find that 1) such networks are stable when they contain a (possibly small) stable sub-network, followed by a layer with a low-rank weight matrix, and 2) such networks are not guaranteed to be stable even when they contain a stable sub-network, if the following layer is not low-rank. The low-rank assumption is inspired by recent empirical and theoretical results which demonstrate that training deep neural networks is biased towards low-rank weight matrices, for minimum-norm interpolation and weight-decay regularization.

</details>


### [165] [GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization](https://arxiv.org/abs/2602.13921)
*Juntong Wang,Libin Chen,Xiyuan Wang,Shijia Kang,Haotong Yang,Da Zheng,Muhan Zhang*

Main category: cs.LG

TL;DR: GREPO is the first GNN benchmark for repository-scale bug localization, with 86 Python repos and 47,294 bug-fixing tasks, showing GNNs outperform traditional retrieval methods.


<details>
  <summary>Details</summary>
Motivation: Standard LLMs struggle with repository-level bug localization due to context window limitations, forcing reliance on suboptimal retrieval methods. GNNs offer promise but lack a dedicated benchmark to evaluate their capabilities for this task.

Method: Created GREPO benchmark with 86 Python repositories and 47,294 bug-fixing tasks, providing graph-based data structures ready for GNN processing. Evaluated various GNN architectures against established information retrieval baselines.

Result: GNN architectures demonstrated outstanding performance compared to traditional information retrieval baselines for bug localization tasks.

Conclusion: This work demonstrates the potential of GNNs for repository-level bug localization and establishes GREPO as a foundational resource for future research in this area.

Abstract: Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.

</details>


### [166] [Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning](https://arxiv.org/abs/2602.13934)
*Zhimin Zhao*

Main category: cs.LG

TL;DR: The paper presents a hierarchy of learnability based on information structure, explaining why code generation scales better than reinforcement learning due to better feedback mechanisms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why code generation has progressed more reliably than reinforcement learning, and to establish a theoretical framework that explains the structural differences in learnability across different computational problems.

Method: The authors propose a five-level hierarchy of learnability based on information structure, distinguishing between expressibility, computability, and learnability. They analyze their pairwise relationships and present a unified template to make structural differences explicit.

Result: The analysis shows that code provides dense, local, verifiable feedback at every token, making it highly learnable, while most reinforcement learning problems lack such quality feedback. The hierarchy explains why supervised learning on code scales predictably while reinforcement learning does not.

Conclusion: The ceiling on ML progress depends less on model size than on whether a task is learnable at all. The common assumption that scaling alone will solve remaining ML challenges warrants scrutiny, as learnability is fundamentally constrained by the information structure of the problem.

Abstract: Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.

</details>


### [167] [A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning](https://arxiv.org/abs/2602.13937)
*Dat Le,Duc-Cuong Le,Anh-Son Nguyen,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.LG

TL;DR: iML introduces a multi-agent framework for AutoML that replaces black-box prompting with code-guided, modular, and verifiable architecture to address hallucination and logic entanglement issues in LLM-based agents.


<details>
  <summary>Details</summary>
Motivation: Traditional AutoML frameworks operate as black boxes lacking flexibility and transparency, while recent LLM-based agents suffer from hallucinated logic and logic entanglement that leads to unrecoverable runtime failures in complex engineering tasks.

Method: iML uses three key innovations: 1) Code-Guided Planning for strategic blueprint synthesis grounded in empirical profiling; 2) Code-Modular Implementation decoupling preprocessing and modeling into specialized components; and 3) Code-Verifiable Integration enforcing physical feasibility through dynamic contract verification and self-correction.

Result: iML achieves 85% valid submission rate and 45% competitive medal rate on MLE-BENCH with APS of 0.77, outperforms other approaches by 38%-163% on iML-BENCH, and maintains 70% success rate even with stripped task descriptions.

Conclusion: iML bridges the gap between stochastic generation and reliable engineering in AutoML, moving toward truly automated machine learning through its code-guided, modular, and verifiable approach.

Abstract: Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as "black boxes", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.

</details>


### [168] [An Adaptive Model Selection Framework for Demand Forecasting under Horizon-Induced Degradation to Support Business Strategy and Operations](https://arxiv.org/abs/2602.13939)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: AHSIV: Adaptive horizon-aware model selection framework for intermittent/variable demand forecasting that outperforms static ranking by adapting to horizon-specific and regime-specific conditions.


<details>
  <summary>Details</summary>
Motivation: Business forecasting faces intermittency, variability, and multi-step horizons where no single model dominates universally; rankings vary across metrics, demand regimes, and horizons, creating decision ambiguity for multi-SKU contexts.

Method: AHSIV framework integrates scaled/absolute error metrics adjusted via Metric Degradation by Forecast Horizon (MDFH), demand classification, multi-objective Pareto dominance, and hierarchical bias refinement in unified architecture.

Result: On Walmart, M3, M4, M5 datasets with multiple train-test schemes and 12-step horizons: AHSIV achieves statistical equivalence with strongest baseline in aggregated performance while increasing frequency of horizon-specific best-model selection.

Conclusion: Model selection in heterogeneous demand environments cannot be static; horizon-consistent, structurally adaptive mechanisms like AHSIV provide principled, operationally coherent solutions for multi-SKU forecasting.

Abstract: Business environments characterized by structural demand intermittency, high variability, and multi-step planning horizons require robust and reproducible model selection mechanisms. Empirical evidence shows that no forecasting model is universally dominant and that relative rankings vary across error metrics, demand regimes, and forecast horizons, generating ambiguity in multi-SKU decision contexts. This study proposes AHSIV (Adaptive Hybrid Selector for Intermittency and Variability), a horizon-aware and regime-conditioned model selection framework designed to address horizon-induced ranking instability. The proposed approach integrates scaled and absolute error metrics adjusted through a Metric Degradation by Forecast Horizon (MDFH) procedure, structural demand classification, multi-objective Pareto dominance, and hierarchical bias refinement within a unified decision architecture. The empirical evaluation is conducted on the Walmart, M3, M4, and M5 datasets under multiple train-test partition schemes and twelve-step forecasting horizons. Results indicate that AHSIV achieves statistical equivalence with the strongest monometric baseline in terms of aggregated performance while increasing the frequency of horizon-specific best-model selection. The findings demonstrate that model selection in heterogeneous demand environments cannot be treated as a static ranking problem, and that horizon-consistent, structurally adaptive mechanisms provide a principled, operationally coherent solution for multi-SKU forecasting.

</details>


### [169] [You Can Learn Tokenization End-to-End with Reinforcement Learning](https://arxiv.org/abs/2602.13940)
*Sam Dauncey,Roger Wattenhofer*

Main category: cs.LG

TL;DR: A method to learn token boundaries directly using score function estimates with time discounting, outperforming prior straight-through approaches at 100M parameter scale.


<details>
  <summary>Details</summary>
Motivation: Tokenization remains a hardcoded compression step in LLMs despite trends toward end-to-end architectures. Prior work uses heuristics or straight-through estimates for token boundaries, but these have limitations.

Method: Proposes learning token boundaries using score function estimates (which directly optimize discrete token boundaries to minimize loss), enhanced with time discounting techniques from reinforcement learning to reduce variance.

Result: The proposed method outperforms prior straight-through estimates both qualitatively and quantitatively at the 100 million parameter scale.

Conclusion: Score function estimates with time discounting offer a theoretically sound and practical approach to end-to-end tokenization learning in LLMs, demonstrating better performance than previous methods.

Abstract: Tokenization is a hardcoded compression step which remains in the training pipeline of Large Language Models (LLMs), despite a general trend towards architectures becoming increasingly end-to-end. Prior work has shown promising results at scale in bringing this compression step inside the LLMs' architecture with heuristics to draw token boundaries, and also attempts to learn these token boundaries with straight-through estimates, which treat the problem of drawing discrete token boundaries as a continuous one. We show that these token boundaries can instead be learned using score function estimates, which have tighter theoretical guarantees due to directly optimizing the problem of drawing discrete token boundaries to minimize loss. We observe that techniques from reinforcement learning, such as time discounting, are necessary to reduce the variance of this score function sufficiently to make it practicable. We demonstrate that the resultant method outperforms prior proposed straight-through estimates, both qualitatively and quantitatively at the $100$ million parameter scale.

</details>


### [170] [Experiential Reinforcement Learning](https://arxiv.org/abs/2602.13949)
*Taiwei Shi,Sihao Chen,Bowen Jiang,Linxin Song,Longqi Yang,Jieyu Zhao*

Main category: cs.LG

TL;DR: ERL integrates explicit experience-reflection-consolidation loops into RL to convert sparse environmental feedback into structured behavioral revisions, improving learning efficiency and final performance.


<details>
  <summary>Details</summary>
Motivation: Environmental feedback in reinforcement learning for language models is often sparse and delayed, making it difficult for models to infer how failures should translate into behavioral changes. Current RL approaches struggle to effectively convert such feedback into durable improvements.

Method: Experiential Reinforcement Learning (ERL) embeds an explicit experience-reflection-consolidation loop: 1) model generates initial attempt, 2) receives environmental feedback, 3) produces reflection guiding refined second attempt, 4) reinforces successful second attempt outcomes into base policy without additional inference cost.

Result: ERL consistently outperforms strong RL baselines across sparse-reward control environments and agentic reasoning benchmarks, achieving up to +81% improvement in complex multi-step environments and +11% improvement in tool-using reasoning tasks.

Conclusion: Integrating explicit self-reflection into policy training provides a practical mechanism for transforming sparse environmental feedback into durable behavioral improvement, enhancing both learning efficiency and final performance.

Abstract: Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.

</details>


### [171] [QuRL: Efficient Reinforcement Learning with Quantized Rollout](https://arxiv.org/abs/2602.13953)
*Yuhang Li,Reena Elangovan,Xin Dong,Priyadarshini Panda,Brucek Khailany*

Main category: cs.LG

TL;DR: QuRL accelerates RL training of LLMs by using a quantized actor for rollout, addressing challenges with Adaptive Clipping Range to prevent training collapse and invariant scaling to handle small weight updates.


<details>
  <summary>Details</summary>
Motivation: RLVR training for reasoning LLMs suffers from efficiency bottlenecks where autoregressive decoding during rollout consumes up to 70% of total training time.

Method: Proposes Quantized Reinforcement Learning (QuRL) using a quantized actor for rollout acceleration. Introduces two key techniques: 1) Adaptive Clipping Range that dynamically adjusts clipping ratio based on policy ratio between full-precision and quantized actors, and 2) Invariant scaling to reduce quantization noise and handle small weight updates.

Result: Achieves 20% to 80% faster rollout during training in INT8 and FP8 quantization experiments on DeepScaleR and DAPO benchmarks.

Conclusion: QuRL effectively addresses efficiency bottlenecks in RL training of LLMs through quantization techniques while maintaining training stability, providing significant speedups for RLVR paradigms.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.

</details>


### [172] [Chemical Language Models for Natural Products: A State-Space Model Approach](https://arxiv.org/abs/2602.13958)
*Ho-Hsuan Wang,Afnan Sultan,Andrea Volkamer,Dietrich Klakow*

Main category: cs.LG

TL;DR: NPCLMs: Mamba-based chemical language models trained on 1M natural products outperform transformers in molecule generation and property prediction, showing domain-specific pre-training effectiveness.


<details>
  <summary>Details</summary>
Motivation: Natural products are important in drug discovery but underexplored in language modeling. Need to develop NP-specific chemical language models and systematically compare selective state-space models (Mamba) vs transformers.

Method: Pre-trained state-space models (Mamba, Mamba-2) and transformer (GPT) baselines on ~1M NPs dataset. Used 8 tokenization strategies. Evaluated on molecule generation (validity, uniqueness, novelty) and property prediction (membrane permeability, taste, anti-cancer activity) using MCC and AUC-ROC.

Result: Mamba generates 1-2% more valid and unique molecules than Mamba-2/GPT, with fewer long-range dependency errors. GPT yields slightly more novel structures. For property prediction, Mamba variants outperform GPT by 0.02-0.04 MCC under random splits; comparable with scaffold splits. Domain-specific pre-training on 1M NPs matches models trained on 100x larger datasets.

Conclusion: NP-specific pre-training enables effective language models for natural product tasks. Mamba shows advantages over transformers in NP-focused generation and prediction. Domain adaptation reduces data requirements dramatically.

Abstract: Language models are widely used in chemistry for molecular property prediction and small-molecule generation, yet Natural Products (NPs) remain underexplored despite their importance in drug discovery. To address this gap, we develop NP-specific chemical language models (NPCLMs) by pre-training state-space models (Mamba and Mamba-2) and comparing them with transformer baselines (GPT). Using a dataset of about 1M NPs, we present the first systematic comparison of selective state-space models and transformers for NP-focused tasks, together with eight tokenization strategies including character-level, Atom-in-SMILES (AIS), byte-pair encoding (BPE), and NP-specific BPE. We evaluate molecule generation (validity, uniqueness, novelty) and property prediction (membrane permeability, taste, anti-cancer activity) using MCC and AUC-ROC. Mamba generates 1-2 percent more valid and unique molecules than Mamba-2 and GPT, with fewer long-range dependency errors, while GPT yields slightly more novel structures. For property prediction, Mamba variants outperform GPT by 0.02-0.04 MCC under random splits, while scaffold splits show comparable performance. Results demonstrate that domain-specific pre-training on about 1M NPs can match models trained on datasets over 100 times larger.

</details>


### [173] [Steady-State Behavior of Constant-Stepsize Stochastic Approximation: Gaussian Approximation and Tail Bounds](https://arxiv.org/abs/2602.13960)
*Zedong Wang,Yuyang Wang,Ijay Narang,Felix Wang,Yuzhou Wang,Siva Theja Maguluri*

Main category: cs.LG

TL;DR: Provides explicit non-asymptotic error bounds for constant-stepsize stochastic approximation, including Wasserstein distance bounds of order α^{1/2}log(1/α) and Berry-Esseen-type tail bounds, covering SGD, linear SA, and nonlinear SA with both i.i.d. and Markovian noise.


<details>
  <summary>Details</summary>
Motivation: Constant-stepsize stochastic approximation is widely used for computational efficiency, but for fixed stepsize, the stationary distribution is rarely tractable. While prior work shows weak convergence to Gaussian as stepsize approaches zero, this offers no usable error bounds for practical fixed stepsize implementations. The paper aims to provide explicit, non-asymptotic error bounds for fixed stepsize to bridge this gap.

Method: The authors first prove general-purpose theorems bounding Wasserstein distance between centered-scaled steady states and Gaussian distributions under regularity conditions for drift and moment conditions for noise, covering both i.i.d. and Markovian noise models. They then instantiate these theorems for three representative SA settings: (1) SGD for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. For SGD beyond strong convexity, they identify a non-Gaussian (Gibbs) limiting law and provide corresponding pre-limit Wasserstein error bounds.

Result: The paper obtains dimension- and stepsize-dependent explicit bounds in Wasserstein distance of order α^{1/2}log(1/α) for small α. It derives non-uniform Berry-Esseen-type tail bounds with error terms decaying in both deviation level and stepsize α. For SGD beyond strong convexity, it identifies a non-Gaussian (Gibbs) limiting law under correct scaling and provides corresponding pre-limit Wasserstein error bounds, validated numerically.

Conclusion: The paper provides a rigorous theoretical framework for non-asymptotic error analysis of constant-stepsize stochastic approximation, delivering explicit, computable error bounds in Wasserstein distance and Berry-Esseen-type tail bounds that bridge the gap between asymptotic Gaussian approximations and practical finite-stepsize implementations across multiple important SA settings.

Abstract: Constant-stepsize stochastic approximation (SA) is widely used in learning for computational efficiency. For a fixed stepsize, the iterates typically admit a stationary distribution that is rarely tractable. Prior work shows that as the stepsize $α\downarrow 0$, the centered-and-scaled steady state converges weakly to a Gaussian random vector. However, for fixed $α$, this weak convergence offers no usable error bound for approximating the steady-state by its Gaussian limit. This paper provides explicit, non-asymptotic error bounds for fixed $α$. We first prove general-purpose theorems that bound the Wasserstein distance between the centered-scaled steady state and an appropriate Gaussian distribution, under regularity conditions for drift and moment conditions for noise. To ensure broad applicability, we cover both i.i.d. and Markovian noise models. We then instantiate these theorems for three representative SA settings: (1) stochastic gradient descent (SGD) for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. We obtain dimension- and stepsize-dependent, explicit bounds in Wasserstein distance of order $α^{1/2}\log(1/α)$ for small $α$. Building on the Wasserstein approximation error, we further derive non-uniform Berry--Esseen-type tail bounds that compare the steady-state tail probability to Gaussian tails. We achieve an explicit error term that decays in both the deviation level and stepsize $α$. We adapt the same analysis for SGD beyond strongly convexity and study general convex objectives. We identify a non-Gaussian (Gibbs) limiting law under the correct scaling, which is validated numerically, and provide a corresponding pre-limit Wasserstein error bound.

</details>


### [174] [KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra](https://arxiv.org/abs/2602.14011)
*Liangyu Su,Jun Shu,Rui Liu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: KoopGen: A generator-based neural Koopman framework that models dynamics through state-dependent Koopman generators with structured Cartesian decomposition, improving prediction accuracy and stability for high-dimensional chaotic systems.


<details>
  <summary>Details</summary>
Motivation: Current data-driven models for high-dimensional chaotic dynamical systems lack stability, interpretability, and scalability in regimes dominated by broadband/continuous spectra. Koopman-based approaches provide a principled linear perspective but rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings.

Method: KoopGen uses a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. It exploits the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components to separate conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning.

Result: Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.

Conclusion: KoopGen provides a structured approach to modeling high-dimensional chaotic systems by combining neural networks with operator-theoretic constraints, offering improved stability and interpretability while maintaining prediction accuracy.

Abstract: Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches provide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.

</details>


### [175] [S2SServiceBench: A Multimodal Benchmark for Last-Mile S2S Climate Services](https://arxiv.org/abs/2602.14017)
*Chenyue Li,Wen Deng,Zhuotao Sun,Mengxi Jin,Hanzhe Cui,Han Li,Shentong Li,Man Kit Yu,Ming Long Lai,Yuhao Yang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: S2SServiceBench: A multimodal benchmark for evaluating MLLMs' ability to translate operational S2S climate forecasts into actionable services across 10 products and 6 application domains, revealing key challenges in plot understanding, uncertainty reasoning, and decision handoff.


<details>
  <summary>Details</summary>
Motivation: There's a "last-mile gap" in subseasonal-to-seasonal (S2S) forecasting - translating scientific forecasts into trusted, actionable climate services. While MLLMs show promise for various workflows, it's unclear if they can reliably generate decision-making deliverables from operational service products under uncertainty.

Method: Introduce S2SServiceBench, a multimodal benchmark curated from operational climate-service systems. Covers 10 service products with ~150+ expert-selected cases across 6 domains (Agriculture, Disasters, Energy, Finance, Health, Shipping). Each case has three service levels, yielding ~500 tasks and 1,000+ evaluation items.

Result: Benchmarking state-of-the-art MLLMs and agents reveals persistent challenges: (1) actionable signal comprehension from S2S service plots, (2) operationalizing uncertainty into executable handoffs, and (3) stable, evidence-grounded analysis/planning for dynamic hazards.

Conclusion: Provides actionable guidance for building future climate-service agents. The benchmark identifies key limitations of current MLLMs in handling real-world climate service tasks and offers direction for improving multimodal understanding and decision-facing reasoning under uncertainty in S2S forecasting.

Abstract: Subseasonal-to-seasonal (S2S) forecasts play an essential role in providing a decision-critical weeks-to-months planning window for climate resilience and sustainability, yet a growing bottleneck is the last-mile gap: translating scientific forecasts into trusted, actionable climate services, requiring reliable multimodal understanding and decision-facing reasoning under uncertainty. Meanwhile, multimodal large language models (MLLMs) and corresponding agentic paradigms have made rapid progress in supporting various workflows, but it remains unclear whether they can reliably generate decision-making deliverables from operational service products (e.g., actionable signal comprehension, decision-making handoff, and decision analysis & planning) under uncertainty. We introduce S2SServiceBench, a multimodal benchmark for last-mile S2S climate services curated from an operational climate-service system to evaluate this capability. S2SServiceBenchcovers 10 service products with about 150+ expert-selected cases in total, spanning six application domains - Agriculture, Disasters, Energy, Finance, Health, and Shipping. Each case is instantiated at three service levels, yielding around 500 tasks and 1,000+ evaluation items across climate resilience and sustainability applications. Using S2SServiceBench, we benchmark state-of-the-art MLLMs and agents, and analyze performance across products and service levels, revealing persistent challenges in S2S service plot understanding and reasoning - namely, actionable signal comprehension, operationalizing uncertainty into executable handoffs, and stable, evidence-grounded analysis and planning for dynamic hazards-while offering actionable guidance for building future climate-service agents.

</details>


### [176] [EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models](https://arxiv.org/abs/2602.14024)
*Xinxing Zhou,Qingren Yao,Yiji Zhao,Chenghao Liu,Flora Salim,Xiaojie Yuan,Yanlong Wen,Ming Jin*

Main category: cs.LG

TL;DR: EIDOS is a time series foundation model family that shifts pretraining from predicting future observations to predicting latent-space representations, resulting in more structured and temporally coherent latent states.


<details>
  <summary>Details</summary>
Motivation: Traditional time series foundation models predict future observations directly, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics.

Method: Train a causal Transformer to predict the evolution of latent representations, using a lightweight aggregation branch to construct stable target representations. Optimize via a joint objective that integrates latent-space alignment, observational grounding, and direct forecasting supervision.

Result: On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance.

Conclusion: Constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.

Abstract: Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.

</details>


### [177] [UniST-Pred: A Robust Unified Framework for Spatio-Temporal Traffic Forecasting in Transportation Networks Under Disruptions](https://arxiv.org/abs/2602.14049)
*Yue Wang,Areg Karapetyan,Djellel Difallah,Samer Madanat*

Main category: cs.LG

TL;DR: UniST-Pred is a unified spatio-temporal forecasting framework that decouples temporal modeling from spatial representation learning, then integrates both through adaptive fusion, achieving competitive performance with lightweight design and robustness to network disruptions.


<details>
  <summary>Details</summary>
Motivation: Current traffic forecasting models often tightly couple spatial and temporal modeling at the cost of complexity and limited modularity, while real-world deployments face structural and observational uncertainties rarely considered in model design.

Method: Proposes UniST-Pred which first decouples temporal modeling from spatial representation learning, then integrates both through adaptive representation-level fusion. Evaluated on both real-world datasets and simulated MATSim dataset under severe network disconnection scenarios.

Result: UniST-Pred demonstrates competitive performance against established models on standard traffic prediction datasets despite lightweight design. Maintains strong predictive performance across real-world and simulated datasets, yielding interpretable spatio-temporal representations under infrastructure disruptions.

Conclusion: UniST-Pred provides a robust, lightweight spatio-temporal forecasting framework that effectively handles structural uncertainties while maintaining interpretability, addressing key challenges in real-world intelligent transportation system deployments.

Abstract: Spatio-temporal traffic forecasting is a core component of intelligent transportation systems, supporting various downstream tasks such as signal control and network-level traffic management. In real-world deployments, forecasting models must operate under structural and observational uncertainties, conditions that are rarely considered in model design. Recent approaches achieve strong short-term predictive performance by tightly coupling spatial and temporal modeling, often at the cost of increased complexity and limited modularity. In contrast, efficient time-series models capture long-range temporal dependencies without relying on explicit network structure. We propose UniST-Pred, a unified spatio-temporal forecasting framework that first decouples temporal modeling from spatial representation learning, then integrates both through adaptive representation-level fusion. To assess robustness of the proposed approach, we construct a dataset based on an agent-based, microscopic traffic simulator (MATSim) and evaluate UniST-Pred under severe network disconnection scenarios. Additionally, we benchmark UniST-Pred on standard traffic prediction datasets, demonstrating its competitive performance against existing well-established models despite a lightweight design. The results illustrate that UniST-Pred maintains strong predictive performance across both real-world and simulated datasets, while also yielding interpretable spatio-temporal representations under infrastructure disruptions. The source code and the generated dataset are available at https://anonymous.4open.science/r/UniST-Pred-EF27

</details>


### [178] [Position Encoding with Random Float Sampling Enhances Length Generalization of Transformers](https://arxiv.org/abs/2602.14050)
*Atsushi Shimizu,Shohei Taniguchi,Yutaka Matsuo*

Main category: cs.LG

TL;DR: Random Float Sampling (RFS) position encoding uses continuous random position indices during training to improve length generalization to unseen input lengths.


<details>
  <summary>Details</summary>
Motivation: Language models struggle with length generalization - maintaining performance on inputs longer than those seen during pretraining. This is often due to out-of-distribution issues when position indices exceed the range seen during training.

Method: RFS replaces discrete position indices with randomly sampled continuous values during training, exposing models to diverse position values. This approach can be integrated into various position encoding schemes like absolute sinusoidal encoding, RoPE, and ALiBi.

Result: Experiments show RFS achieves superior performance in length generalization tasks and zero-shot commonsense reasoning benchmarks compared to standard position encoding methods.

Conclusion: RFS provides a simple yet powerful position encoding strategy that enhances length generalization by avoiding OOD issues through continuous random sampling during training.

Abstract: Length generalization is the ability of language models to maintain performance on inputs longer than those seen during pretraining. In this work, we introduce a simple yet powerful position encoding (PE) strategy, Random Float Sampling (RFS), that generalizes well to lengths unseen during pretraining or fine-tuning. In particular, instead of selecting position indices from a predefined discrete set, RFS uses randomly sampled continuous values, thereby avoiding out-of-distribution (OOD) issues on unseen lengths by exposing the model to diverse indices during training. Since assigning indices to tokens is a common and fundamental procedure in widely used PEs, the advantage of RFS can easily be incorporated into, for instance, the absolute sinusoidal encoding, RoPE, and ALiBi. Experiments corroborate its effectiveness by showing that RFS results in superior performance in length generalization tasks as well as zero-shot commonsense reasoning benchmarks.

</details>


### [179] [Decentralized Federated Learning With Energy Harvesting Devices](https://arxiv.org/abs/2602.14051)
*Kai Zhang,Xuanyu Cao,Khaled B. Letaief*

Main category: cs.LG

TL;DR: Proposed a decentralized federated learning framework with energy harvesting that jointly optimizes device scheduling and power control using a novel decentralized policy iteration algorithm to accelerate convergence while operating sustainably.


<details>
  <summary>Details</summary>
Motivation: Decentralized federated learning (DFL) enables device-to-device collaborative training but consumes significant energy, rapidly depleting device batteries and degrading learning performance. The paper aims to enable sustainable DFL operations through energy harvesting techniques while addressing convergence challenges caused by partial device participation and transmission packet drops.

Method: Derived convergence bound for wireless DFL with energy harvesting, formulated joint device scheduling and power control as multi-agent Markov decision process (MDP), and proposed a fully decentralized policy iteration algorithm that uses only local two-hop neighbor information to reduce communication overhead and computational complexity.

Result: Theoretical analysis shows the proposed decentralized algorithm achieves asymptotic optimality. Comprehensive numerical experiments on real-world datasets validate theoretical results and demonstrate the algorithm's effectiveness in accelerating convergence for energy-harvesting DFL systems.

Conclusion: Energy harvesting enables sustainable DFL operations, and the proposed decentralized policy iteration algorithm efficiently solves the joint optimization problem with reduced complexity while achieving asymptotic optimality, making large-scale decentralized federated learning practical with energy-constrained devices.

Abstract: Decentralized federated learning (DFL) enables edge devices to collaboratively train models through local training and fully decentralized device-to-device (D2D) model exchanges. However, these energy-intensive operations often rapidly deplete limited device batteries, reducing their operational lifetime and degrading the learning performance. To address this limitation, we apply energy harvesting technique to DFL systems, allowing edge devices to extract ambient energy and operate sustainably. We first derive the convergence bound for wireless DFL with energy harvesting, showing that the convergence is influenced by partial device participation and transmission packet drops, both of which further depend on the available energy supply. To accelerate convergence, we formulate a joint device scheduling and power control problem and model it as a multi-agent Markov decision process (MDP). Traditional MDP algorithms (e.g., value or policy iteration) require a centralized coordinator with access to all device states and exhibit exponential complexity in the number of devices, making them impractical for large-scale decentralized networks. To overcome these challenges, we propose a fully decentralized policy iteration algorithm that leverages only local state information from two-hop neighboring devices, thereby substantially reducing both communication overhead and computational complexity. We further provide a theoretical analysis showing that the proposed decentralized algorithm achieves asymptotic optimality. Finally, comprehensive numerical experiments on real-world datasets are conducted to validate the theoretical results and corroborate the effectiveness of the proposed algorithm.

</details>


### [180] [Policy Gradient with Adaptive Entropy Annealing for Continual Fine-Tuning](https://arxiv.org/abs/2602.14078)
*Yaqian Zhang,Bernhard Pfahringer,Eibe Frank,Albert Bifet*

Main category: cs.LG

TL;DR: The paper proposes a reinforcement learning approach to class-incremental learning that directly optimizes the 0-1 loss via Expected Policy Gradient (EPG), showing that cross-entropy loss can be seen as EPG with sample weighting. They introduce adaptive entropy annealing (aEPG) that transitions from exploratory to exploitative learning, achieving better performance than CE-based methods in parameter-efficient fine-tuning scenarios.


<details>
  <summary>Details</summary>
Motivation: Large pretrained vision models suffer from catastrophic forgetting in class-incremental settings. While parameter-efficient fine-tuning (PEFT) helps by limiting trainable parameters, most approaches still rely on cross-entropy loss, which is a surrogate for the true objective (0-1 loss). The authors aim to directly optimize the 0-1 loss through a reinforcement learning perspective to improve adaptation while mitigating forgetting.

Method: The paper formulates classification as a one-step Markov Decision Process and derives an Expected Policy Gradient (EPG) method that directly minimizes misclassification error with low-variance gradient estimation. They analyze CE as EPG with sample weighting and propose adaptive entropy annealing (aEPG), a training strategy that transitions from exploratory (CE-like) to exploitative (EPG-like) learning.

Result: aEPG-based methods outperform CE-based methods across diverse benchmarks and with various PEFT modules. The paper demonstrates that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models.

Conclusion: Directly optimizing the 0-1 loss through EPG and transitioning from exploratory to exploitative learning via aEPG provides better performance than CE-based approaches in class-incremental learning with parameter-efficient fine-tuning. Lower entropy regularization improves adaptation in pretrained vision models.

Abstract: Despite their success, large pretrained vision models remain vulnerable to catastrophic forgetting when adapted to new tasks in class-incremental settings. Parameter-efficient fine-tuning (PEFT) alleviates this by restricting trainable parameters, yet most approaches still rely on cross-entropy (CE) loss, a surrogate for the 0-1 loss, to learn from new data. We revisit this choice and revive the true objective (0-1 loss) through a reinforcement learning perspective. By formulating classification as a one-step Markov Decision Process, we derive an Expected Policy Gradient (EPG) method that directly minimizes misclassification error with a low-variance gradient estimation. Our analysis shows that CE can be interpreted as EPG with an additional sample-weighting mechanism: CE encourages exploration by emphasizing low-confidence samples, while EPG prioritizes high-confidence ones. Building on this insight, we propose adaptive entropy annealing (aEPG), a training strategy that transitions from exploratory (CE-like) to exploitative (EPG-like) learning. aEPG-based methods outperform CE-based methods across diverse benchmarks and with various PEFT modules. More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models.

</details>


### [181] [Neural Optimal Transport in Hilbert Spaces: Characterizing Spurious Solutions and Gaussian Smoothing](https://arxiv.org/abs/2602.14086)
*Jae-Hwan Choi,Jiwoo Yoon,Dohyun Kwon,Jaewoong Choi*

Main category: cs.LG

TL;DR: The paper addresses spurious solutions in infinite-dimensional Neural Optimal Transport by introducing Gaussian smoothing with Brownian motion, proving well-posedness under regular source measures, and demonstrating improved performance on functional data.


<details>
  <summary>Details</summary>
Motivation: Semi-dual Neural OT in infinite-dimensional Hilbert spaces often produces spurious solutions that fail to accurately capture target distributions in non-regular settings.

Method: Extends the semi-dual framework via Gaussian smoothing strategy based on Brownian motion, analytically characterizing spurious solutions using regular measures framework.

Result: Theoretical proof shows formulation is well-posed under regular source measures and recovers unique Monge map; empirical results on synthetic functional data and time-series show effective suppression of spurious solutions and outperformance of baselines.

Conclusion: Gaussian smoothing strategy resolves ill-posedness in infinite-dimensional Neural OT, providing robust framework for functional data applications with theoretical guarantees.

Abstract: We study Neural Optimal Transport in infinite-dimensional Hilbert spaces. In non-regular settings, Semi-dual Neural OT often generates spurious solutions that fail to accurately capture target distributions. We analytically characterize this spurious solution problem using the framework of regular measures, which generalize Lebesgue absolute continuity in finite dimensions. To resolve ill-posedness, we extend the semi-dual framework via a Gaussian smoothing strategy based on Brownian motion. Our primary theoretical contribution proves that under a regular source measure, the formulation is well-posed and recovers a unique Monge map. Furthermore, we establish a sharp characterization for the regularity of smoothed measures, proving that the success of smoothing depends strictly on the kernel of the covariance operator. Empirical results on synthetic functional data and time-series datasets demonstrate that our approach effectively suppresses spurious solutions and outperforms existing baselines.

</details>


### [182] [Geometry-Aware Physics-Informed PointNets for Modeling Flows Across Porous Structures](https://arxiv.org/abs/2602.14108)
*Luigi Ciceri,Corrado Mio,Jianyi Lin,Gabriele Gianini*

Main category: cs.LG

TL;DR: Physics-informed neural networks (PIPN and PI-GANO) are developed to predict fluid flow through and around porous bodies, combining Navier-Stokes for free flow and Darcy-Forchheimer for porous regions in a unified framework with good generalization to unseen geometries.


<details>
  <summary>Details</summary>
Motivation: Predicting coupled flow through and around porous bodies is challenging due to complex physics across fluid/porous interfaces and the need for generalization across diverse geometries and boundary conditions without retraining for each case.

Method: Two physics-informed approaches: Physics Informed PointNets (PIPN) and Physics Informed Geometry Aware Neural Operator (PI-GANO). They enforce incompressible Navier-Stokes in free-flow regions and Darcy-Forchheimer in porous regions within unified loss functions, conditioned on geometry and material parameters. Data generated with OpenFOAM on 2D ducts with porous obstacles and 3D windbreak scenarios with tree canopies/buildings.

Result: Models achieve consistently low velocity and pressure errors in both seen and unseen cases, accurately reproducing wake structures. Performance degrades mainly near sharp interfaces and regions with large gradients. PI-GANO shows additional generalization to variable boundary conditions and parameter settings.

Conclusion: First systematic evaluation of PIPN/PI-GANO for simultaneous through-and-around porous flows demonstrates their potential to accelerate design studies without retraining per geometry, though challenges remain near interfaces and high-gradient regions.

Abstract: Predicting flows that occur both through and around porous bodies is challenging due to coupled physics across fluid and porous regions and the need to generalize across diverse geometries and boundary conditions. We address this problem using two Physics Informed learning approaches: Physics Informed PointNets (PIPN) and Physics Informed Geometry Aware Neural Operator (P-IGANO). We enforce the incompressible Navier Stokes equations in the free-flow region and a Darcy Forchheimer extension in the porous region within a unified loss and condition the networks on geometry and material parameters. Datasets are generated with OpenFOAM on 2D ducts containing porous obstacles and on 3D windbreak scenarios with tree canopies and buildings. We first verify the pipeline via the method of manufactured solutions, then assess generalization to unseen shapes, and for PI-GANO, to variable boundary conditions and parameter settings. The results show consistently low velocity and pressure errors in both seen and unseen cases, with accurate reproduction of the wake structures. Performance degrades primarily near sharp interfaces and in regions with large gradients. Overall, the study provides a first systematic evaluation of PIPN/PI-GANO for simultaneous through-and-around porous flows and shows their potential to accelerate design studies without retraining per geometry.

</details>


### [183] [Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?](https://arxiv.org/abs/2602.14111)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Rogov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.LG

TL;DR: Sparse Autoencoders fail to reliably recover meaningful features from neural network activations, performing no better than random baselines despite strong reconstruction performance.


<details>
  <summary>Details</summary>
Motivation: There's growing skepticism about whether Sparse Autoencoders actually recover meaningful features from neural networks, as negative results in downstream tasks cast doubt on their effectiveness despite much excitement in the field.

Method: Two complementary evaluations: 1) Synthetic setup with known ground-truth features to measure recovery rate, 2) Real activation evaluation using three random baselines that constrain SAE feature directions or activation patterns to random values, tested across multiple SAE architectures.

Result: On synthetic data, SAEs recover only 9% of true features despite achieving 71% explained variance. On real activations, random baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72).

Conclusion: Current SAEs do not reliably decompose models' internal mechanisms, failing at their core task of recovering meaningful features even when reconstruction performance is strong.

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only $9\%$ of true features despite achieving $71\%$ explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.

</details>


### [184] [ROAST: Rollout-based On-distribution Activation Steering Technique](https://arxiv.org/abs/2602.14143)
*Xuanbo Su,Hao Luo,Yingfang Zhang,Lijun Zhang*

Main category: cs.LG

TL;DR: ROAST is a robust activation steering technique that uses on-distribution rollouts and continuous soft scaling with grouped normalization to improve LLM performance across tasks and model sizes.


<details>
  <summary>Details</summary>
Motivation: Existing activation steering methods for LLMs rely on off-distribution supervision and discrete masking, leading to brittle interventions. High-magnitude activations can dominate steering directions, and activation magnitude variance is disproportionate to semantic quality, necessitating a more robust approach.

Method: ROAST estimates steering directions from the model's own on-distribution rollouts via ROC (Rollout-based On-distribution Control) and avoids hard sparsification via Continuous Soft Scaling (CSS) and Grouped Mean Normalization to balance contributions across samples.

Result: ROAST consistently improves performance across models (0.6B to 32B) on diverse tasks, e.g., +9.7% on GSM8K for Qwen3-0.6B and +12.1% on TruthfulQA for GLM4-32B. CSS better preserves activation energy compared to hard sparsification methods.

Conclusion: ROAST provides an effective and robust method for activation steering that overcomes limitations of previous approaches by using on-distribution rollouts and continuous scaling with grouped normalization, leading to significant performance gains across diverse tasks and model sizes.

Abstract: Activation steering provides parameter-efficient control over large language models (LLMs) at inference time, but many methods rely on off-distribution supervision and discrete masking, leading to brittle interventions. We propose ROAST (Rollout-based On-distribution Activation Steering Technique), which estimates steering directions from the model's own on-distribution rollouts via ROC and avoids hard sparsification via Continuous Soft Scaling (CSS) and Grouped Mean Normalization. Our empirical analysis reveals that while activation magnitude correlates moderately with directional consistency, the variance in magnitude is significant and often disproportionate to semantic quality. This suggests that high-magnitude activations risk dominating the global steering direction if not properly normalized. To address this, ROAST employs grouped normalization to balance contributions across samples, ensuring a more robust estimation of the consensus steering direction. Across models (0.6B to 32B), ROAST consistently improves performance on diverse tasks (e.g., +9.7% on GSM8K for Qwen3-0.6B and +12.1% on TruthfulQA for GLM4-32B), and analyses show that CSS better preserves activation energy.

</details>


### [185] [A Penalty Approach for Differentiation Through Black-Box Quadratic Programming Solvers](https://arxiv.org/abs/2602.14154)
*Yuxuan Linghu,Zhiyuan Liu,Qi Deng*

Main category: cs.LG

TL;DR: dXPP is a penalty-based differentiation framework for quadratic programs that decouples solving from differentiation, using any black-box QP solver for forward pass and solving a smaller linear system for backward pass.


<details>
  <summary>Details</summary>
Motivation: Existing approaches that differentiate through KKT systems face computational cost and numerical robustness issues at scale, motivating a more efficient and robust differentiation method for quadratic programs.

Method: Proposes dXPP framework with two steps: 1) solver-agnostic forward pass using any black-box QP solver, 2) backward pass that maps solution to smooth approximate penalty problem and implicitly differentiates through it, requiring only solution of a smaller linear system in primal variables.

Result: Empirical evaluation on random QPs, large-scale sparse projection problems, and multi-period portfolio optimization shows dXPP is competitive with KKT-based methods and achieves substantial speedups on large-scale problems.

Conclusion: dXPP provides a computationally efficient and robust alternative to KKT-based differentiation for quadratic programs, bypassing difficulties of explicit KKT differentiation while maintaining competitive performance.

Abstract: Differentiating through the solution of a quadratic program (QP) is a central problem in differentiable optimization. Most existing approaches differentiate through the Karush--Kuhn--Tucker (KKT) system, but their computational cost and numerical robustness can degrade at scale. To address these limitations, we propose dXPP, a penalty-based differentiation framework that decouples QP solving from differentiation. In the solving step (forward pass), dXPP is solver-agnostic and can leverage any black-box QP solver. In the differentiation step (backward pass), we map the solution to a smooth approximate penalty problem and implicitly differentiate through it, requiring only the solution of a much smaller linear system in the primal variables. This approach bypasses the difficulties inherent in explicit KKT differentiation and significantly improves computational efficiency and robustness. We evaluate dXPP on various tasks, including randomly generated QPs, large-scale sparse projection problems, and a real-world multi-period portfolio optimization task. Empirical results demonstrate that dXPP is competitive with KKT-based differentiation methods and achieves substantial speedups on large-scale problems.

</details>


### [186] [Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization](https://arxiv.org/abs/2602.14159)
*Rizhen Hu,Yuan Cao,Boao Kong,Mou Sun,Kun Yuan*

Main category: cs.LG

TL;DR: Two plug-and-play regularization losses improve MoE model efficiency: intra-layer specialization reduces expert overlap, cross-layer coupling stabilizes routing pathways, enhancing specialization and inference speed without architecture changes.


<details>
  <summary>Details</summary>
Motivation: Sparse Mixture-of-Experts models suffer from expert overlap (redundant representations) and routing ambiguity, leading to severely underutilized model capacity. Existing architectural solutions require substantial modifications and rely only on intra-layer signals.

Method: 1) Intra-layer specialization loss: penalizes cosine similarity between experts' SwiGLU activations on identical tokens to encourage complementary specialization. 2) Cross-layer coupling loss: maximizes joint Top-k routing probabilities across adjacent layers to establish coherent expert pathways through network depth. Both losses are orthogonal to standard load-balancing loss and work with various MoE architectures.

Result: Experiments across pre-training, fine-tuning, and zero-shot benchmarks show consistent task gains, higher expert specialization, lower-entropy routing, and faster inference via more stable expert pathways. Implemented as a drop-in Megatron-LM module.

Conclusion: The proposed regularization losses effectively enhance MoE specialization and routing efficiency without modifying router or model architectures, addressing expert overlap and routing ambiguity issues in plug-and-play fashion.

Abstract: Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. In this paper, we propose two plug-and-play regularization losses that enhance MoE specialization and routing efficiency without modifying router or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary knowledge. Second, a cross-layer coupling loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer expert specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with both the shared-expert architecture in DeepSeekMoE and vanilla top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.

</details>


### [187] [When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift](https://arxiv.org/abs/2602.14161)
*Max Fomin*

Main category: cs.LG

TL;DR: Current prompt injection detection methods have poor out-of-distribution generalization due to dataset-dependent shortcuts, with standard evaluations overestimating performance by 8.4% AUC. LODO evaluation reveals major gaps in detecting indirect attacks (7-37% success), and SAE features help identify stable vs. dataset-specific signals.


<details>
  <summary>Details</summary>
Motivation: Prompt injection and jailbreak attacks are critical threats as LLM-based agents process untrusted data, but current evaluation practices and production systems have fundamental limitations in measuring true generalization and detecting diverse attack types.

Method: 1) Comprehensive analysis using 18 diverse datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. 2) Propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization. 3) Analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds to identify dataset-dependent shortcuts. 4) Systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on the benchmark.

Result: Standard train-test splits from same dataset sources severely overestimate performance by 8.4 percentage points AUC on average, with per-dataset gaps ranging from 1% to 25% accuracy. 28% of top SAE features are dataset-dependent shortcuts. Production guardrails and LLM-as-judge approaches fail on indirect attacks targeting agents (7-37% detection), and PromptGuard 2/LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. LODO-stable SAE features provide more reliable explanations by filtering dataset artifacts.

Conclusion: LODO evaluation is essential for prompt attack detection research as it reveals true generalization gaps and heterogeneous failure modes. Current production systems have significant limitations in detecting indirect attacks and agent-targeted injections. SAE feature analysis helps identify and filter dataset artifacts to build more robust classifiers.

Abstract: Detecting prompt injection and jailbreak attacks is critical for deploying LLM-based agents safely. As agents increasingly process untrusted data from emails, documents, tool outputs, and external APIs, robust attack detection becomes essential. Yet current evaluation practices and production systems have fundamental limitations. We present a comprehensive analysis using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. We propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, revealing that the standard practice of train-test splits from the same dataset sources severely overestimates performance: aggregate metrics show an 8.4 percentage point AUC inflation, but per-dataset gaps range from 1% to 25% accuracy-exposing heterogeneous failure modes. To understand why classifiers fail to generalize, we analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, finding that 28% of top features are dataset-dependent shortcuts whose class signal depends on specific dataset compositions rather than semantic content. We systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on our benchmark, finding all three fail on indirect attacks targeting agents (7-37% detection) and that PromptGuard 2 and LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. Finally, we show that LODO-stable SAE features provide more reliable explanations for classifier decisions by filtering dataset artifacts. We release our evaluation framework at https://github.com/maxf-zn/prompt-mining to establish LODO as the appropriate protocol for prompt attack detection research.

</details>


### [188] [Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling](https://arxiv.org/abs/2602.14169)
*Yiran Guo,Zhongjian Qiao,Yingqi Xie,Jie Liu,Dan Ye,Ruiqing Zhang,Shuang Qiu,Lijie Xu*

Main category: cs.LG

TL;DR: DDE/DEEP-GRPO improves RL exploration for LLMs by focusing on deep, recoverable states in failed trajectories with targeted resampling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL exploration methods for LLMs have limitations: GRPO only samples from the root, saturating high-probability trajectories while leaving deep error-prone states under-explored. Tree-based methods waste sampling budget on trivial or unrecoverable states, failing to discover rare correct suffixes and destabilizing local baselines.

Method: Proposes Deep Dense Exploration (DDE) focusing on pivot states (deep, recoverable states in unsuccessful trajectories). Instantiated as DEEP-GRPO with three innovations: 1) Lightweight data-driven utility function balancing recoverability and depth bias to identify pivots; 2) Local dense resampling at each pivot to increase probability of discovering correct subsequent trajectories; 3) Dual-stream optimization objective decoupling global policy learning from local corrective updates.

Result: Experiments on mathematical reasoning benchmarks show DDE/DEEP-GRPO consistently outperforms GRPO, tree-based methods, and other strong baselines.

Conclusion: DDE provides an effective exploration strategy for RL in LLMs by targeting deep, recoverable states in failed trajectories, addressing limitations of existing methods and improving performance on complex reasoning tasks.

Abstract: Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.

</details>


### [189] [TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models](https://arxiv.org/abs/2602.14200)
*Nicolas Zumarraga,Thomas Kaar,Ning Wang,Maxwell A. Xu,Max Rosenblattl,Markus Kreft,Kevin O'Sullivan,Paul Schmiedmayer,Patrick Langer,Robert Jakob*

Main category: cs.LG

TL;DR: TS-Haystack is a new benchmark for evaluating long-context temporal retrieval in Time Series Language Models, revealing that compression helps classification but hurts retrieval of localized events as context length increases.


<details>
  <summary>Details</summary>
Motivation: Real-world time-series sensor streams can span millions of datapoints, but existing TSLMs are trained on short sequences. This creates a mismatch where models need to perform precise temporal localization under computational constraints, which current benchmarks don't capture.

Method: Introduced TS-Haystack benchmark with controlled needle insertion - embedding short activity bouts into longer longitudinal accelerometer recordings. Systematically evaluates across 10 task types (direct retrieval, temporal reasoning, multi-step reasoning, contextual anomaly) and context lengths ranging from seconds to 2 hours.

Result: Found consistent divergence between classification and retrieval behavior. Learned latent compression preserves/improves classification accuracy up to 176× compression ratios, but retrieval performance degrades with context length, showing loss of temporally localized information.

Conclusion: Highlights the need for architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity. Current TSLM encoders overlook temporal granularity as context increases, creating task-dependent effects where compression helps classification but impairs localized event retrieval.

Abstract: Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.

</details>


### [190] [Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws](https://arxiv.org/abs/2602.14208)
*Jinbo Wang,Binghui Li,Zhanpeng Zhou,Mingze Wang,Yuxuan Sun,Jiaqi Zhang,Xunliang Cai,Lei Wu*

Main category: cs.LG

TL;DR: The paper introduces a theoretical framework for batch size scheduling that shows optimal schedules depend on task difficulty: easy tasks benefit from increasing batch sizes throughout training, while hard tasks need small batches for most training with a late switch to large batches.


<details>
  <summary>Details</summary>
Motivation: Batch size scheduling is crucial for efficient large-scale deep learning training but lacks solid theoretical foundations. The authors aim to provide principled analysis using the Functional Scaling Law framework to understand optimal batch size schedules and their relationship to task difficulty.

Method: Using the Functional Scaling Law (FSL) framework from previous work, the authors characterize optimal batch size scheduling under fixed data budgets. They analyze the dynamical mechanism of "fast catch-up effect" where loss rapidly aligns after switching from small to large batches, and investigate how this effect relates to task difficulty and gradient noise forgetting.

Result: Theoretical analysis reveals that optimal batch size schedules sharply depend on task difficulty. For hard tasks, optimal schedules use small batches for most training and switch to large batches only in late stages due to the fast catch-up effect. Extensive LLM pretraining experiments with up to 1.1B parameters and 1T tokens validate that late-switch schedules consistently outperform constant-batch and early-switch baselines across both Dense and MoE architectures.

Conclusion: The FSL framework provides principled understanding of batch size scheduling, revealing that optimal schedules depend on task difficulty. The fast catch-up effect enables deferring large batches to late training without performance loss, significantly reducing data consumption. This has practical implications for efficient large-scale model training.

Abstract: Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.

</details>


### [191] [MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM](https://arxiv.org/abs/2602.14209)
*Omin Kwon,Yeonjae Kim,Doyeon Kim,Minseo Kim,Yeonhong Park,Jae W. Lee*

Main category: cs.LG

TL;DR: MAGE: A block diffusion LLM method that uses first-step attention patterns to guide sparse attention for efficient long-context generation, achieving near-lossless accuracy with 3-4x speedup.


<details>
  <summary>Details</summary>
Motivation: Block diffusion LLMs face memory bottlenecks due to KV caching in long-context settings. Existing sparse attention methods designed for autoregressive LLMs perform poorly when adapted to block diffusion because they rely on approximate importance estimation.

Method: MAGE leverages unique block diffusion properties: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements. It performs a single exact attention pass per block and reuses it for training-free sparse denoising. Also includes lightweight fine-tuning to strengthen [MASK]-guided patterns.

Result: MAGE achieves near-lossless accuracy on long-context benchmarks (LongBench, Needle-in-a-Haystack) with a fraction of the KV budget while delivering 3-4x end-to-end speedup. It consistently outperforms autoregressive-oriented sparse attention baselines. Lightweight fine-tuning takes only a few hours on a single H100 GPU for both 1.5B and 7B models.

Conclusion: MAGE demonstrates that block diffusion's unique properties enable efficient sparse attention, overcoming memory bottlenecks in long-context generation while maintaining accuracy and achieving significant speed improvements compared to existing methods.

Abstract: Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.

</details>


### [192] [Robust multi-task boosting using clustering and local ensembling](https://arxiv.org/abs/2602.14231)
*Seyedsaman Emami,Daniel Hernández-Lobato,Gonzalo Martínez-Muñoz*

Main category: cs.LG

TL;DR: RMB-CLE is a robust multi-task learning framework that uses error-based task clustering and local ensembling to prevent negative transfer and improve performance across related tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional MTL methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations, which degrades performance.

Method: Uses error-based task clustering derived from cross-task errors with risk decomposition into functional mismatch and irreducible noise, followed by agglomerative clustering and local ensembling within each cluster.

Result: RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks.

Conclusion: RMB-CLE is a general and scalable framework that establishes a new basis for robust multi-task learning by preventing negative transfer through principled error-based clustering and local ensembling.

Abstract: Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.

</details>


### [193] [Evaluating LLMs in Finance Requires Explicit Bias Consideration](https://arxiv.org/abs/2602.14233)
*Yaxuan Kong,Hoyoung Lee,Yoontae Hwang,Alejandro Lopez-Lira,Bradford Levy,Dhagash Mehta,Qingsong Wen,Chanyeol Choi,Yongjae Lee,Stefan Zohren*

Main category: cs.LG

TL;DR: This position paper identifies five recurring biases in financial LLM applications that inflate performance and contaminate results, proposes a Structural Validity Framework to address them, and finds that current literature largely ignores these biases.


<details>
  <summary>Details</summary>
Motivation: Current evaluation practices for LLMs in finance have not kept pace with integration into financial workflows, allowing finance-specific biases to inflate performance, contaminate backtests, and make reported results useless for deployment claims.

Method: The authors identify five recurring biases in financial LLM applications and review 164 papers from 2023-2025 to assess how frequently these biases are discussed. They propose a Structural Validity Framework and evaluation checklist for bias diagnosis and system design.

Result: The review found that no single bias is discussed in more than 28% of studies, indicating widespread neglect of bias issues. The paper proposes practical tools for bias mitigation including the Structural Validity Framework and evaluation checklist.

Conclusion: Bias in financial LLM systems requires explicit attention, and structural validity should be enforced before any result is used to support deployment claims. The proposed framework and checklist provide minimal requirements for bias diagnosis and future system design.

Abstract: Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.

</details>


### [194] [Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection](https://arxiv.org/abs/2602.14251)
*Pinqiao Wang,Sheng Li*

Main category: cs.LG

TL;DR: MAD is a multi-agent debating framework for tabular anomaly detection that leverages disagreement among heterogeneous detectors as valuable signal, coordinating them through mathematical guarantees to improve robustness and provide auditable debate traces.


<details>
  <summary>Details</summary>
Motivation: Current tabular anomaly detection relies on single detectors or static ensembles, failing to leverage the disagreement among heterogeneous model families (tree ensembles, deep tabular networks, tabular foundation models) which occurs under distribution shift, missingness, and rare-anomaly regimes.

Method: Proposes MAD framework with multiple ML-based detector agents producing normalized anomaly scores, confidence, and structured evidence, augmented by LLM-based critics. A coordinator converts messages into bounded per-agent losses and updates agent influence via exponentiated-gradient rule, producing final debated anomaly score and auditable debate trace.

Result: Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement. The framework establishes regret guarantees for synthesized losses and demonstrates how conformal calibration can wrap debated scores to control false positives under exchangeability.

Conclusion: MAD provides a unified agentic framework that can recover existing approaches while treating model disagreement as valuable signal, offering both improved performance and interpretability through auditable debate traces.

Abstract: Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement

</details>


### [195] [Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting](https://arxiv.org/abs/2602.14267)
*Manal Rahal,Bestoun S. Ahmed,Roger Renström,Robert Stener*

Main category: cs.LG

TL;DR: DELTAiF is a transfer learning framework that predicts household hot water consumption to optimize heat pump operation, reducing training time by ~67% while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: As residential heat pump installations increase, optimizing hot water production is crucial but faces scalability challenges. Training separate ML models for each household is computationally expensive, especially in large-scale cloud-connected deployments.

Method: DELTAiF uses transfer learning to leverage knowledge from a representative household and fine-tune it for other households, eliminating the need to train separate models for each installation. The framework focuses on predicting large hot water usage events like showers.

Result: The approach reduces overall training time by approximately 67% while maintaining high predictive accuracy (0.874-0.991) and low mean absolute percentage error (0.001-0.017). Transfer learning works particularly well when the source household has regular consumption patterns.

Conclusion: DELTAiF demonstrates that transfer learning enables scalable and accurate household hot water demand forecasting, solving the computational bottleneck of training individual models for each household while maintaining prediction quality.

Abstract: With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.
  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.

</details>


### [196] [Radial-VCReg: More Informative Representation Learning Through Radial Gaussianization](https://arxiv.org/abs/2602.14272)
*Yilun Kuang,Yash Dagade,Deep Chakraborty,Erik Learned-Miller,Randall Balestriero,Tim G. J. Rudner,Yann LeCun*

Main category: cs.LG

TL;DR: Radial-VCReg improves self-supervised learning by adding a radial Gaussianization loss to VCReg, aligning feature norms with Chi distribution to better approach maximum entropy and achieve more informative representations.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning aims for maximally informative representations, but existing methods like VCReg that regularize first and second-order feature statistics cannot fully achieve maximum entropy due to limitations in addressing higher-order dependencies.

Method: Proposes Radial-VCReg which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution (a defining property of high-dimensional Gaussians), transforming a broader class of distributions toward normality.

Result: On both synthetic and real-world datasets, Radial-VCReg consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations compared to standard VCReg.

Conclusion: Radial-VCReg effectively addresses the curse of dimensionality in self-supervised learning by achieving better approximation of maximum entropy through radial Gaussianization, leading to more informative feature representations.

Abstract: Self-supervised learning aims to learn maximally informative representations, but explicit information maximization is hindered by the curse of dimensionality. Existing methods like VCReg address this by regularizing first and second-order feature statistics, which cannot fully achieve maximum entropy. We propose Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution-a defining property of high-dimensional Gaussians. We prove that Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg and show on synthetic and real-world datasets that it consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations.

</details>


### [197] [Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data](https://arxiv.org/abs/2602.14274)
*Boning Zhou,Ziyu Wang,Han Hong,Haoqi Hu*

Main category: cs.LG

TL;DR: Using transformer models to perform causal inference from unstructured text, achieving results consistent with structured data approaches.


<details>
  <summary>Details</summary>
Motivation: Causal inference traditionally requires structured data, but real-world scenarios often lack complete structured data while having abundant unstructured text data. This limits the applicability of causal inference methods in business decision-making.

Method: Proposes a framework leveraging transformer-based language models to perform causal inference using unstructured text. The framework is evaluated by comparing causal estimates from unstructured text against those obtained from structured data across multiple levels.

Result: Demonstrates consistent results between causal estimates derived from unstructured text and structured data across population, group, and individual levels. Validates the potential of using unstructured text for causal inference tasks.

Conclusion: The framework successfully extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making even when structured tabular data is scarce.

Abstract: Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.

</details>


### [198] [Reverse N-Wise Output-Oriented Testing for AI/ML and Quantum Computing Systems](https://arxiv.org/abs/2602.14275)
*Lamine Rihani*

Main category: cs.LG

TL;DR: A novel testing paradigm called reverse n-wise output testing that constructs covering arrays over output equivalence classes and uses optimization to synthesize inputs that elicit targeted behavioral signatures from AI/ML and quantum systems.


<details>
  <summary>Details</summary>
Motivation: AI/ML and quantum computing systems present unprecedented testing challenges: high-dimensional continuous input spaces, probabilistic outputs, behavioral correctness defined over observable behaviors, and critical quality dimensions like trustworthiness, fairness, calibration, and robustness that manifest through complex multi-way interactions among output properties.

Method: Introduces reverse n-wise output testing, which inverts traditional testing by constructing covering arrays directly over domain-specific output equivalence classes (ML confidence buckets, decision boundaries, fairness partitions, embedding clusters, quantum measurement distributions, error syndrome patterns). Then solves the black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input configurations that elicit targeted behavioral signatures.

Result: Delivers synergistic benefits: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery and coverage drift monitoring.

Conclusion: Reverse n-wise output testing provides a mathematically principled paradigm for testing complex AI/ML and quantum systems by focusing on output behaviors and using optimization to generate targeted test inputs, addressing the unique challenges of these probabilistic, high-dimensional systems.

Abstract: Artificial intelligence/machine learning (AI/ML) systems and emerging quantum computing software present unprecedented testing challenges characterized by high-dimensional/continuous input spaces, probabilistic/non-deterministic output distributions, behavioral correctness defined exclusively over observable prediction behaviors and measurement outcomes, and critical quality dimensions, trustworthiness, fairness, calibration, robustness, error syndrome patterns, that manifest through complex multi-way interactions among semantically meaningful output properties rather than deterministic input-output mappings. This paper introduces reverse n-wise output testing, a mathematically principled paradigm inversion that constructs covering arrays directly over domain-specific output equivalence classes, ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions (0-dominant, 1-dominant, superposition collapse), error syndrome patterns (bit-flip, phase-flip, correlated errors), then solves the computationally challenging black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures from opaque models. The framework delivers synergistic benefits across both domains: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.

</details>


### [199] [Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions](https://arxiv.org/abs/2602.14279)
*Ruomeng Ding,Tianwei Gao,Thomas P. Zollo,Eitan Bachmat,Richard Zemel,Zhun Deng*

Main category: cs.LG

TL;DR: A framework for adaptive group elicitation that uses LLMs and graph neural networks to adaptively select questions and respondents improves population-level response prediction under constrained budgets.


<details>
  <summary>Details</summary>
Motivation: Existing elicitation methods often optimize what to ask with a fixed respondent pool, without adapting respondent selection or leveraging population structure when responses are partial or incomplete, which is inefficient under real-world constraints.

Method: The method combines an LLM-based expected information gain objective for scoring candidate questions with a heterogeneous graph neural network that aggregates observed responses and participant attributes to impute missing responses and guide respondent selection.

Result: The method consistently improves population-level response prediction across three real-world opinion datasets, achieving a >12% relative gain on CES at a 10% respondent budget.

Conclusion: The proposed framework successfully improves population-level response prediction under constrained query and participation budgets by adaptively selecting both questions and respondents, leveraging LLMs and graph neural networks to handle missing data and population structure.

Abstract: Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.

</details>


### [200] [KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning](https://arxiv.org/abs/2602.14293)
*Kris Shengjun Dong,Sahil Modi,Dima Nikiforov,Sana Damani,Edward Lin,Siva Kumar Sastry Hari,Christos Kozyrakis*

Main category: cs.LG

TL;DR: KernelBlaster: Memory-augmented in-context RL framework that enables LLM agents to optimize CUDA code across GPU generations by learning from experience in a persistent knowledge base.


<details>
  <summary>Details</summary>
Motivation: Optimizing CUDA code across multiple GPU architectures is challenging due to complex hardware-specific optimization spaces. Traditional compilers have fixed heuristics, fine-tuning LLMs is expensive, and existing agentic workflows lack ability to aggregate prior knowledge, leading to biased sampling and suboptimal solutions.

Method: Proposes KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework with a Persistent CUDA Knowledge Base that accumulates retrievable knowledge. Uses profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to explore high-potential strategies beyond naive rewrites.

Result: Achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3 respectively compared to PyTorch baseline. Framework is released as open-source with test harness, verification components, and reproducible evaluation pipeline.

Conclusion: KernelBlaster effectively addresses the challenge of GPU code optimization across architectures by enabling LLM agents to learn from experience and make systematically informed decisions through persistent knowledge accumulation, outperforming traditional approaches.

Abstract: Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.

</details>


### [201] [Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows](https://arxiv.org/abs/2602.14295)
*Edwin Chen,Zulekha Bibi*

Main category: cs.LG

TL;DR: MLAT is a design pattern that integrates pre-trained ML models as callable tools in LLM agent workflows, allowing agents to invoke quantitative predictions dynamically based on conversational context.


<details>
  <summary>Details</summary>
Motivation: To move beyond static ML preprocessing pipelines and enable LLM agents to dynamically decide when and how to use quantitative predictions alongside other tools like web search and APIs.

Method: The MLAT framework exposes ML models as first-class tools in LLM workflows. PitchCraft system implements this with two agents: Research Agent gathers intelligence via parallel tool calls, and Draft Agent invokes XGBoost pricing model as a tool call and generates proposals through structured outputs.

Result: PitchCraft system reduces proposal generation time from multiple hours to under 10 minutes. The XGBoost pricing model achieves R^2 = 0.807 on held-out data with MAE of 3688 USD, trained on only 70 examples combining real and synthetic data.

Conclusion: MLAT effectively bridges quantitative ML predictions with contextual LLM reasoning, generalizing to domains requiring combined quantitative estimation and contextual understanding.

Abstract: We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.
  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.
  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.

</details>


### [202] [In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes](https://arxiv.org/abs/2602.14318)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: Survey paper examining trustworthiness of transformer models across high-stakes applications, evaluating reliability through interpretability, robustness, fairness, and privacy.


<details>
  <summary>Details</summary>
Motivation: Transformers are increasingly deployed in high-stakes applications (healthcare, autonomous systems, scientific computing) but their trustworthiness needs systematic evaluation given the critical nature of these domains.

Method: Comprehensive review approach analyzing transformers' trustworthiness through multiple lenses: interpretability, explainability, adversarial robustness, fairness, and privacy. Systematic examination across NLP, computer vision, and scientific domains.

Result: Identifies recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit reliable deployment of transformers in safety-critical applications.

Conclusion: The survey highlights significant trustworthiness gaps in transformer models and emphasizes the need for rigorous evaluation before deployment in high-stakes domains to ensure reliability and safety.

Abstract: Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.

</details>


### [203] [Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study](https://arxiv.org/abs/2602.14322)
*Hani Beirami,M M Manjurul Islam*

Main category: cs.LG

TL;DR: AeroBench F-16 control with PPO RL enhanced by conformal STL shielding improves safety and robustness in aerospace applications.


<details>
  <summary>Details</summary>
Motivation: To enhance safety and robustness of RL control in aerospace applications using formal temporal logic specifications, particularly addressing safety requirements in challenging flight environments.

Method: Used PPO RL agent in AeroBench F-16 simulation to regulate engine throttle and track airspeed. Encoded control objective as STL requirement to maintain airspeed within prescribed band. Introduced conformal STL shield using online conformal prediction to filter RL actions. Compared three settings: PPO baseline, PPO with classical rule-based STL shield, and PPO with conformal shield.

Result: Conformal shield preserves STL satisfaction while maintaining near baseline performance and provides stronger robustness guarantees than classical shield, especially under stress scenarios with model mismatch, actuator limits, noise, and setpoint jumps.

Conclusion: Combining formal specification monitoring with data-driven RL control substantially improves reliability of autonomous flight control in challenging environments.

Abstract: We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent's actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.

</details>


### [204] [Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning](https://arxiv.org/abs/2602.14338)
*Zhi Zhang,Zhen Han,Costas Mavromatis,Qi Zhu,Yunyi Zhang,Sheng Guan,Dingmin Wang,Xiong Zhou,Shuai Wang,Soji Adeshina,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: AERO improves GRPO for RL fine-tuning of LLMs by using adaptive rollout strategy and selective rejection to avoid zero-gradient groups, achieving ~48% compute reduction while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: GRPO suffers from compute waste when all rollouts in a group share the same outcome (all correct or all incorrect), resulting in zero gradient signal. This inefficiency motivates the development of a more compute-efficient RL method for LLM fine-tuning.

Method: AERO enhances GRPO with: 1) adaptive rollout strategy, 2) selective rejection to prune rollouts strategically, and 3) Bayesian posterior to prevent zero-advantage dead zones.

Result: Across three model configurations, AERO reduces total training compute by ~48% and wall-clock time per step by ~45% on average, while matching or improving Pass@8 and Avg@8 metrics compared to GRPO.

Conclusion: AERO provides a practical, scalable, and compute-efficient strategy for RL-based LLM alignment, significantly improving efficiency without sacrificing performance.

Abstract: Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.

</details>


### [205] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2602.14344)
*Mathias Jackermeier,Mattia Giuri,Jacques Cloete,Alessandro Abate*

Main category: cs.LG

TL;DR: A hierarchical neural architecture with attention that learns structured representations of LTL specifications via Boolean formula sequences from task automata, enabling better zero-shot generalization in multi-task RL.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for multi-task RL with LTL specifications struggle to effectively capture the rich logical and temporal structure inherent in LTL, limiting generalization to novel tasks.

Method: Conditions policy on sequences of Boolean formulae constructed from task automata, uses hierarchical neural architecture to encode logical structure, and employs attention mechanism for reasoning about future subgoals.

Result: Experiments in complex environments demonstrate strong generalization capabilities and superior performance compared to existing approaches.

Conclusion: Learning structured task representations through hierarchical encoding and attention mechanisms effectively captures LTL structure, enabling better zero-shot execution of novel tasks in multi-task RL.

Abstract: We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.

</details>


### [206] [WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control](https://arxiv.org/abs/2602.14351)
*Mehran Aghabozorgi,Alireza Moazeni,Yanshu Zhang,Ke Li*

Main category: cs.LG

TL;DR: WIMLE extends Implicit Maximum Likelihood Estimation to model-based RL, learning stochastic multi-modal world models with uncertainty-aware weighting to improve sample efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Model-based RL often underperforms due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning.

Method: Extends IMLE to model-based RL framework to learn stochastic, multi-modal world models without iterative sampling; estimates predictive uncertainty via ensembles and latent sampling; weights synthetic transitions by predicted confidence during training to preserve useful rollouts while attenuating bias from uncertain predictions.

Result: Superior sample efficiency and competitive/better asymptotic performance across 40 continuous-control tasks in DeepMind Control, MyoSuite, and HumanoidBench; improves sample efficiency by over 50% on Humanoid-run; solves 8 of 14 tasks on HumanoidBench versus 4 for BRO and 5 for SimbaV2.

Conclusion: IMLE-based multi-modality and uncertainty-aware weighting are valuable for stable model-based RL, addressing key limitations of traditional model-based approaches.

Abstract: Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.

</details>


### [207] [A Study on Multi-Class Online Fuzzy Classifiers for Dynamic Environments](https://arxiv.org/abs/2602.14375)
*Kensuke Ajimoto,Yuma Yamamoto,Yoshifumi Kusunoki,Tomoharu Nakashima*

Main category: cs.LG

TL;DR: This paper extends conventional online fuzzy classifiers from binary classification to multi-class problems, proposing a method that can handle dynamic environments where training data arrives sequentially.


<details>
  <summary>Details</summary>
Motivation: Traditional online fuzzy classifiers only handle two-class problems, which is limited for real-world applications. Many real-world classification problems involve multiple classes, and there's a need for fuzzy classifiers that can operate in dynamic environments where data arrives sequentially over time.

Method: The paper proposes a multi-class online fuzzy classifier based on fuzzy if-then rules. Human users determine antecedent fuzzy sets, while consequent real values are learned from training data. The method operates in an online framework where only a few patterns are available at each time step, and subsequent patterns become available at later time steps.

Result: The multi-class online fuzzy classifier is evaluated through numerical experiments on both synthetic dynamic data and several benchmark datasets. The results demonstrate the effectiveness of the proposed extension to multi-class problems.

Conclusion: The proposed multi-class online fuzzy classifier successfully extends conventional binary fuzzy classifiers to handle multi-class problems in dynamic environments, showing promising performance on both synthetic and benchmark datasets.

Abstract: This paper proposes a multi-class online fuzzy classifier for dynamic environments. A fuzzy classifier comprises a set of fuzzy if-then rules where human users determine the antecedent fuzzy sets beforehand. In contrast, the consequent real values are determined by learning from training data. In an online framework, not all training dataset patterns are available beforehand. Instead, only a few patterns are available at a time step, and the subsequent patterns become available at the following time steps. The conventional online fuzzy classifier considered only two-class problems. This paper investigates the extension to the conventional fuzzy classifiers for multi-class problems. We evaluate the performance of the multi-class online fuzzy classifiers through numerical experiments on synthetic dynamic data and also several benchmark datasets.

</details>


### [208] [The geometry of invariant learning: an information-theoretic analysis of data augmentation and generalization](https://arxiv.org/abs/2602.14423)
*Abdelali Bouyahia,Frédéric LeBlanc,Mario Marchand*

Main category: cs.LG

TL;DR: Information-theoretic framework relates data augmentation to generalization through mutual information bounds, decomposing generalization gap into distributional divergence, stability, and sensitivity terms controlled by augmentation group diameter.


<details>
  <summary>Details</summary>
Motivation: Data augmentation is widely used to improve generalization but its theoretical understanding remains incomplete. The paper aims to systematically account for the effect of augmentation on generalization and invariance learning using information theory.

Method: Extends mutual information-based generalization bounds to model augmented distribution as composition of original data distribution with transformation distribution. Introduces orbit-averaged loss function, derives bound under sub-Gaussian assumptions with decomposition into three terms. Introduces group diameter concept to connect bounds to augmentation geometry.

Result: Derived generalization bound decomposing expected generalization gap into: (1) distributional divergence between original and augmented data, (2) algorithm stability term, and (3) augmentation sensitivity term. Group diameter provides unified control parameter revealing trade-off between data fidelity and regularization. Numerical experiments validate bounds reliably track true generalization gap.

Conclusion: The information-theoretic framework provides systematic understanding of how data augmentation affects generalization, revealing intrinsic trade-offs controlled by augmentation group diameter. Theoretical bounds are validated experimentally and offer insights for designing effective augmentation strategies.

Abstract: Data augmentation is one of the most widely used techniques to improve generalization in modern machine learning, often justified by its ability to promote invariance to label-irrelevant transformations. However, its theoretical role remains only partially understood. In this work, we propose an information-theoretic framework that systematically accounts for the effect of augmentation on generalization and invariance learning. Our approach builds upon mutual information-based bounds, which relate the generalization gap to the amount of information a learning algorithm retains about its training data. We extend this framework by modeling the augmented distribution as a composition of the original data distribution with a distribution over transformations, which naturally induces an orbit-averaged loss function. Under mild sub-Gaussian assumptions on the loss function and the augmentation process, we derive a new generalization bound that decompose the expected generalization gap into three interpretable terms: (1) a distributional divergence between the original and augmented data, (2) a stability term measuring the algorithm dependence on training data, and (3) a sensitivity term capturing the effect of augmentation variability. To connect our bounds to the geometry of the augmentation group, we introduce the notion of group diameter, defined as the maximal perturbation that augmentations can induce in the input space. The group diameter provides a unified control parameter that bounds all three terms and highlights an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. We validate our theoretical bounds with numerical experiments, demonstrating that it reliably tracks and predicts the behavior of the true generalization gap.

</details>


### [209] [A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking](https://arxiv.org/abs/2602.14430)
*Prithwijit Chowdhury,Ahmad Mustafa,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.LG

TL;DR: A framework that uses counterfactual reasoning and causal concepts (necessity/sufficiency) to evaluate and improve the robustness of XAI explanations (LIME/SHAP) for hydrocarbon prospect risking models.


<details>
  <summary>Details</summary>
Motivation: XAI methods like LIME and SHAP often produce conflicting explanations for complex geophysical data, reducing trust in hydrocarbon prospect risking decisions. There's a need for more reliable, theoretically-grounded evaluation of feature importance.

Method: Proposes a unified framework that: 1) generates counterfactual examples, 2) quantifies causal concepts of necessity and sufficiency for features, and 3) uses these to perform robustness evaluation of LIME and SHAP explanations on high-dimensional structured prospect risking data.

Result: The robustness test provides deeper insights into model capabilities to handle erroneous data and identifies which XAI module works best with which model for hydrocarbon indication tasks.

Conclusion: Grounding XAI explanations in causal concepts of necessity and sufficiency offers a more reliable and robust approach to improve trustworthiness of hydrocarbon prospect risking decisions, addressing inconsistencies between different explanation methods.

Abstract: In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of "importance" and "relevance" differ for different explanation strategies. Thus, grounding these ranked features using theoretically backed causal ideas of necessity and sufficiency can prove to be a more reliable and robust way to improve the trustworthiness of the concerned explanation strategies.We propose a unified framework to generate counterfactuals as well as quantify necessity and sufficiency and use these to perform a robustness evaluation of the explanations provided by LIME and SHAP on high dimensional structured prospect risking data. This robustness test gives us deeper insights into the models capabilities to handle erronous data and which XAI module works best in pair with which model for our dataset for hydorcarbon indication.

</details>


### [210] [S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations](https://arxiv.org/abs/2602.14432)
*Arnav Chavan,Nahush Lele,Udbhav Bamba,Sankalp Dayal,Aditi Raghunathan,Deepak Gupta*

Main category: cs.LG

TL;DR: S^2D (Selective Spectral Decay) addresses quantization challenges in large transformers by surgically regularizing dominant weight singular values to reduce activation outliers, enabling efficient low-precision deployment without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large transformer models suffer from activation outliers that create excessively large ranges during quantization, causing severe accuracy drops. This problem intensifies with model scale and extensive pre-training (e.g., CLIP → SigLIP → SigLIP2), creating a fundamental challenge for quantization and efficient deployment.

Method: Proposes Selective Spectral Decay (S^2D), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. This approach reduces activation outliers by addressing their theoretical connection to dominant singular values of the weights.

Result: S^2D significantly reduces activation outliers and produces well-conditioned representations that are quantization-friendly. Achieves up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. Improvements generalize across downstream tasks and vision-language models.

Conclusion: S^2D provides an effective solution to activation outlier problems in large transformers, enabling efficient low-precision deployment without sacrificing accuracy. The method's theoretical grounding and practical effectiveness address a fundamental challenge in scaling increasingly large and rigorously trained models.

Abstract: Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.

</details>


### [211] [Broken Chains: The Cost of Incomplete Reasoning in LLMs](https://arxiv.org/abs/2602.14444)
*Ian Su,Gaurav Purushothaman,Jey Narayan,Ruhika Goel,Kevin Zhu,Sunishchal Dev,Yash More,Maheep Chaudhary*

Main category: cs.LG

TL;DR: Under token constraints, code-based reasoning degrades more gracefully than natural language or hybrid approaches, with model robustness varying significantly across different reasoning modalities.


<details>
  <summary>Details</summary>
Motivation: Reasoning-specialized models allocate substantial compute to extended chain-of-thought traces, but reasoning tokens incur significant costs. The paper investigates how different reasoning modalities (code, natural language, hybrid, or none) perform under constrained token budgets.

Method: Introduced a framework that constrains models to reason exclusively through code, comments, both, or neither, then systematically ablated token budgets to 10%, 30%, 50%, and 70% of optimal. Evaluated four frontier models (GPT-5.1, Gemini 3 Flash, DeepSeek-V3.2, Grok 4.1) across mathematical benchmarks (AIME, GSM8K, HMMT).

Result: (1) Truncated reasoning can hurt performance (DeepSeek-V3.2: 53% with no reasoning vs 17% with truncated CoT at 50% budget). (2) Code degrades gracefully while natural language collapses (Gemini: comments drop to 0% while code maintains 43-47%). (3) Hybrid reasoning underperforms single modalities. (4) Robustness is model-dependent (Grok maintains 80-90% at 30% budget while OpenAI and DeepSeek collapse to 7-27%).

Conclusion: Incomplete reasoning chains actively mislead models, with code-based reasoning being more robust to token constraints than natural language or hybrid approaches. Results have important implications for deploying reasoning-specialized systems under resource constraints.

Abstract: Reasoning-specialized models like OpenAI's 5.1 and DeepSeek-V3.2 allocate substantial inference compute to extended chain-of-thought (CoT) traces, yet reasoning tokens incur significant costs. How do different reasoning modalities of code, natural language, hybrid, or none do perform under token constraints? We introduce a framework that constrains models to reason exclusively through code, comments, both, or neither, then systematically ablates token budgets to 10\%, 30\%, 50\%, and 70\% of optimal. We evaluate four frontier models (GPT-5.1, Gemini 3 Flash, DeepSeek-V3.2, Grok 4.1) across mathematical benchmarks (AIME, GSM8K, HMMT). Our findings reveal: (1) \textbf{truncated reasoning can hurt} as DeepSeek-V3.2 achieves 53\% with no reasoning but only 17\% with truncated CoT at 50\% budget; (2) \textbf{code degrades gracefully} as Gemini's comments collapse to 0\% while code maintains 43-47\%; (3) \textbf{hybrid reasoning underperforms} single modalities; (4) \textbf{robustness is model-dependent} as Grok maintains 80-90\% at 30\% budget where OpenAI and DeepSeek collapse to 7-27\%. These results suggest incomplete reasoning chains actively mislead models, with implications for deploying reasoning-specialized systems under resource constraints.

</details>


### [212] [Selective Synchronization Attention](https://arxiv.org/abs/2602.14445)
*Hasi Hays*

Main category: cs.LG

TL;DR: Proposed Selective Synchronization Attention (SSA) replaces standard self-attention with a Kuramoto oscillator-based mechanism, achieving sparsity, unified encoding, and efficient computation while providing stronger architectural biases.


<details>
  <summary>Details</summary>
Motivation: The Transformer architecture's core self-attention mechanism has quadratic computational complexity and lacks grounding in biological neural computation, limiting its efficiency and biological plausibility.

Method: Replace standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. Each token is represented as an oscillator with learnable natural frequency and phase; synchronization strength between token pairs (determined by frequency-dependent coupling and phase-locking condition) serves as attention weight.

Result: The proposed Selective Synchronization Attention (SSA) mechanism, based on the Kuramoto model, achieves natural sparsity through phase-locking thresholds, unified positional-semantic encoding via natural frequency spectrum, and single-pass closed-form computation. When instantiated in the Oscillatory Synchronization Network (OSN), it produces non-uniform, head-diverse coupling patterns at initialization, demonstrating stronger architectural inductive bias than standard Transformers.

Conclusion: Selective Synchronization Attention provides a biologically grounded, computationally efficient alternative to standard self-attention with inherent sparsity and unified encoding capabilities, offering stronger inductive biases for neural network architectures.

Abstract: The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.

</details>


### [213] [WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity](https://arxiv.org/abs/2602.14452)
*Lei Chen,Yuan Meng,Xiaoyu Zhan,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: WiSparse proposes a weight-aware training-free activation sparsity method that integrates activation magnitudes with weight information and uses mixed-granularity allocation to achieve efficient LLM inference without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing training-free activation sparsity methods for LLMs rely solely on activation information and uniform sparsity ratios, overlooking the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. The paper identifies two key phenomena: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks.

Method: WiSparse leverages both activation and weight information through a weight-aware mechanism that integrates activation magnitudes with precomputed weight norms to identify salient channels. It uses a mixed-granularity allocation scheme: a global sparsity budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. The method also includes improvements to sparse kernels.

Result: WiSparse significantly outperforms existing baselines, preserving 97% of Llama3.1's dense performance at 50% sparsity (2.23 percentage points better than the strongest baseline) while achieving a 21.4% acceleration in end-to-end inference speed. Results are demonstrated on three representative LLM models.

Conclusion: The research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training by properly accounting for weight-activation interplay and sensitivity variations across model blocks.

Abstract: Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.

</details>


### [214] [Traceable Latent Variable Discovery Based on Multi-Agent Collaboration](https://arxiv.org/abs/2602.14456)
*Huaming Du,Tao Hu,Yijie Huang,Yu Zhao,Guisong Liu,Tao Gu,Gang Kou,Carl Yang*

Main category: cs.LG

TL;DR: TLVD integrates LLMs' metadata reasoning with traditional causal discovery to infer latent variables and their semantics, achieving significant accuracy improvements over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Traditional causal discovery faces limitations due to lack of high-quality data, assumption of no latent confounders, and inability to interpret latent variable semantics, hindering broader real-world application.

Method: First build causal graph with latent variables using data-driven approach, then employ multi-LLM collaboration modeled as incomplete information game to infer latent semantics via Bayesian Nash Equilibrium, and validate through LLM-based evidence exploration across multiple data sources.

Result: TLVD shows significant improvements across three real patient datasets and two benchmark datasets: 32.67% average Acc improvement, 62.21% CAcc improvement, and 26.72% ECit improvement.

Conclusion: TLVD effectively combines LLMs' reasoning with traditional causal discovery to overcome limitations of latent variable inference, demonstrating reliable performance on real-world datasets.

Abstract: Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, TLVD, which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling capabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific latent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with average improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.

</details>


### [215] [Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment](https://arxiv.org/abs/2602.14462)
*Hong Li,Zhen Zhou,Honggang Zhang,Yuping Luo,Xinyue Wang,Han Gong,Zhiyuan Liu*

Main category: cs.LG

TL;DR: Silent inconsistency in data-parallel fine-tuning of LLMs occurs when workers diverge in losses/gradients despite synchronized weights, invisible to standard monitoring. A lightweight diagnostic framework with three metrics (loss dispersion, gradient-norm dispersion, gradient-direction consistency) reveals this hidden instability without added overhead.


<details>
  <summary>Details</summary>
Motivation: Standard DP fine-tuning uses synchronous all-reduce to synchronize model weights, ensuring numerical equivalence after each iteration. However, this does not guarantee alignment of worker-level optimization dynamics before gradient aggregation. The paper identifies that cross-worker divergence in losses and gradients—termed "silent inconsistency"—can remain invisible under conventional aggregated monitoring signals, potentially causing hidden instability in large-scale fine-tuning.

Method: Proposes a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Introduces three complementary metrics: loss dispersion (variation in loss values across workers), gradient-norm dispersion (variation in gradient norms), and gradient-direction consistency (measured by inter-worker cosine similarity). The metrics require no modification to model architecture, synchronization mechanisms, or optimization algorithms.

Result: Experiments fine-tuning the 1B-parameter openPangu-Embedded-1B-V1.1 model on the alpaca dataset using an 8-NPU DP setup under controlled perturbations. Progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. The metrics successfully reveal hidden divergence invisible to standard monitoring.

Conclusion: Silent inconsistency in DP fine-tuning can be systematically detected via the proposed lightweight metrics, providing actionable visibility into hidden instability modes. This enables more reliable diagnosis and configuration assessment for large-scale DP fine-tuning, without requiring architectural or algorithmic changes.

Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \texttt{openPangu-Embedded-1B-V1.1} model on the \texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.

</details>


### [216] [LACONIC: Length-Aware Constrained Reinforcement Learning for LLM](https://arxiv.org/abs/2602.14468)
*Chang Liu,Yiran Zhao,Lawrence Liu,Yaoqi Ye,Csaba Szepesvári,Lin F. Yang*

Main category: cs.LG

TL;DR: LACONIC is an RL method that enforces token budget constraints during training to reduce response length while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: RL training of LLMs often produces excessively long responses, increasing inference latency and computational costs. Existing length-control methods use fixed heuristic reward shaping which can misalign with task objectives and require brittle tuning.

Method: LACONIC updates policy models using an augmented objective combining task reward with length-based cost. The cost scale is adaptively adjusted throughout training to balance brevity and task performance, enforcing a target token budget.

Result: Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens.

Conclusion: LACONIC provides robust length control while preserving task reward, integrates into standard RL-tuning with no inference changes, and offers minimal deployment overhead, supported by theoretical guarantees.

Abstract: Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.

</details>


### [217] [One Good Source is All You Need: Near-Optimal Regret for Bandits under Heterogeneous Noise](https://arxiv.org/abs/2602.14474)
*Aadirupa Saha,Amith Bhat,Haipeng Luo*

Main category: cs.LG

TL;DR: A novel SOAR algorithm for MAB with multiple data sources with varying noise variances, achieving near-optimal regret with respect to the best source variance.


<details>
  <summary>Details</summary>
Motivation: Multi-armed bandit problems with multiple heterogeneous data sources (each with unknown distinct noise variances) present additional complexity where standard MAB algorithms may suffer suboptimal regret scaling with the worst source variance rather than the best.

Method: SOAR algorithm: 1) quickly prunes high-variance sources using sharp variance-concentration bounds, 2) uses a balanced min-max LCB-UCB approach that integrates best arm identification with optimal (minimum-variance) source selection.

Result: SOAR achieves instance-dependent regret bound of $\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$, attaining optimal single-source MAB regret with the best variance σ*² plus a small additive cost for source identification.

Conclusion: SOAR successfully addresses the multi-source MAB problem without prior knowledge of noise variances, achieving near-optimal performance by automatically identifying and leveraging the best data source while maintaining computational efficiency.

Abstract: We study $K$-armed Multiarmed Bandit (MAB) problem with $M$ heterogeneous data sources, each exhibiting unknown and distinct noise variances $\{σ_j^2\}_{j=1}^M$. The learner's objective is standard MAB regret minimization, with the additional complexity of adaptively selecting which data source to query from at each round. We propose Source-Optimistic Adaptive Regret minimization (SOAR), a novel algorithm that quickly prunes high-variance sources using sharp variance-concentration bounds, followed by a `balanced min-max LCB-UCB approach' that seamlessly integrates the parallel tasks of identifying the best arm and the optimal (minimum-variance) data source. Our analysis shows SOAR achieves an instance-dependent regret bound of $\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$, up to preprocessing costs depending only on problem parameters, where ${σ^*}^2 := \min_j σ_j^2$ is the minimum source variance and $Δ_i$ denotes the suboptimality gap of the $i$-th arm. This result is both surprising as despite lacking prior knowledge of the minimum-variance source among $M$ alternatives, SOAR attains the optimal instance-dependent regret of standard single-source MAB with variance ${σ^*}^2$, while incurring only an small (and unavoidable) additive cost of $\tilde O(\sqrt{K \sum_{j=1}^M σ_j^2})$ towards the optimal (minimum variance) source identification. Our theoretical bounds represent a significant improvement over some proposed baselines, e.g. Uniform UCB or Explore-then-Commit UCB, which could potentially suffer regret scaling with $σ_{\max}^2$ in place of ${σ^*}^2$-a gap that can be arbitrarily large when $σ_{\max} \gg σ^*$. Experiments on multiple synthetic problem instances and the real-world MovieLens\;25M dataset, demonstrating the superior performance of SOAR over the baselines.

</details>


### [218] [Revisiting the Platonic Representation Hypothesis: An Aristotelian View](https://arxiv.org/abs/2602.14486)
*Fabian Gröger,Shuo Wen,Maria Brbić*

Main category: cs.LG

TL;DR: Representation similarity metrics are confounded by network scale effects; a permutation-based calibration framework reveals that global convergence claims may be inflated, while local neighborhood relationships show real alignment across modalities.


<details>
  <summary>Details</summary>
Motivation: The Platonic Representation Hypothesis suggests neural network representations converge to a common statistical model, but existing similarity metrics are confounded by network scale (depth/width inflating scores). Need proper calibration to test this hypothesis rigorously.

Method: Introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. Apply this calibration to revisit the Platonic Representation Hypothesis across different modalities.

Result: After calibration, global spectral measures show largely inflated convergence disappears, while local neighborhood similarity (but not local distances) retains significant agreement across modalities.

Conclusion: Propose the Aristotelian Representation Hypothesis: neural network representations converge to shared local neighborhood relationships, not necessarily global statistical structures. Proper metric calibration is essential for meaningful comparison.

Abstract: The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.

</details>


### [219] [Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts](https://arxiv.org/abs/2602.14490)
*Buze Zhang,Jinkai Tao,Zilang Zeng,Neil He,Ali Maatouk,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: MoSLoRA extends LoRA with a Mixture of Space framework using heterogeneous geometric experts to learn richer curvature-aware representations, achieving significant performance gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing Parameter-Efficient Fine-Tuning (PEFT) methods operate in Euclidean space, limiting their ability to capture complex geometric structures in language data. Single manifold approaches also restrict expressiveness, even with learnable curvature parameters.

Method: Proposes Mixture of Space (MoS), a unified framework leveraging multiple geometric spaces simultaneously. Develops MoSLoRA which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts that dynamically select or combine appropriate geometric spaces based on input context, plus a lightweight routing mechanism to reduce computational overhead.

Result: MoSLoRA consistently outperforms strong baselines across diverse benchmarks, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.

Conclusion: The MoS framework and MoSLoRA method effectively address limitations of Euclidean PEFT methods by leveraging multiple geometric spaces, providing richer curvature-aware representations and significant performance improvements.

Abstract: Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.

</details>


### [220] [Divine Benevolence is an $x^2$: GLUs scale asymptotically faster than MLPs](https://arxiv.org/abs/2602.14495)
*Alejandro Francisco Queiruga*

Main category: cs.LG

TL;DR: The paper shows that GLU variants in LLMs achieve asymptotically faster scaling (P^{-3}) than MLPs (P^{-2}) due to their piecewise quadratic forms, and proposes a new Gated Quadratic Unit with even steeper scaling.


<details>
  <summary>Details</summary>
Motivation: The success of GLU variants and similar outer-product architectures in frontier LLMs has been largely empirical. The authors want to understand from first principles why these architectures work better than MLPs and whether we can design even better architectures based on numerical analysis principles.

Method: The paper applies numerical analysis and function approximation theory to analyze scaling laws. It demonstrates through theoretical analysis that GLUs have L(P)∝P^{-3} scaling while MLPs only achieve P^{-2}. The authors provide parameter construction and empirical verification on 1D function approximation problems. They then propose a new "Gated Quadratic Unit" architecture based on their theoretical findings.

Result: Theoretical analysis shows GLUs achieve P^{-3} scaling slope while MLPs only achieve P^{-2} on function reconstruction problems. Empirical verification confirms these theoretical scaling slopes. The proposed Gated Quadratic Unit exhibits even steeper scaling than both GLUs and MLPs.

Conclusion: Architecture design can be guided by first principles numerical analysis rather than just empirical discovery. The quadratic nature of GLU variants enables superior asymptotic scaling, and the proposed Gated Quadratic Unit suggests the possibility of designing even better architectures through numerical theory to unlock superior scaling in large models.

Abstract: Scaling laws can be understood from ground-up numerical analysis, where traditional function approximation theory can explain shifts in model architecture choices. GLU variants now dominate frontier LLMs and similar outer-product architectures are prevalent in ranking models. The success of these architectures has mostly been left as an empirical discovery. In this paper, we apply the tools of numerical analysis to expose a key factor: these models have an $x^2$ which enables \emph{asymptotically} faster scaling than MLPs. GLUs have piecewise quadratic functional forms that are sufficient to exhibit quadratic order of approximation. Our key contribution is to demonstrate that the $L(P)$ scaling slope is $L(P)\propto P^{-3}$ for GLUs but only $L(P)=P^{-2}$ for MLPs on function reconstruction problems. We provide a parameter construction and empirical verification of these slopes for 1D function approximation. From the first principles we discover, we make one stride and propose the ``Gated Quadratic Unit'' which has an even steeper $L(P)$ slope than the GLU and MLP. This opens the possibility of architecture design from first principles numerical theory to unlock superior scaling in large models. Replication code is available at https://github.com/afqueiruga/divine_scaling.

</details>


### [221] [Covariance-Aware Transformers for Quadratic Programming and Decision Making](https://arxiv.org/abs/2602.14506)
*Kutay Tire,Yufan Zhang,Ege Onur Taga,Samet Oymak*

Main category: cs.LG

TL;DR: Transformers can solve quadratic programs using linear attention and MLPs, enabling one-shot decision-making for portfolio optimization via Time2Decide framework.


<details>
  <summary>Details</summary>
Motivation: Explore whether transformers can solve quadratic programs (QPs) to enhance decision-making tasks involving covariance matrices, particularly portfolio optimization.

Method: Use linear attention to emulate gradient descent for unconstrained QPs, add MLPs to handle ℓ1-penalized/constrained QPs via soft-thresholding and feedback loops, then propose Time2Decide framework to integrate covariance matrices into time-series foundation models.

Result: Time2Decide outperforms both base TSFMs and classical "Predict-then-Optimize" methods for portfolio optimization, showing transformers can solve QPs effectively in a single forward pass.

Conclusion: Transformers benefit from explicit second-order statistics and can solve complex decision-making problems like portfolio construction directly, without needing separate prediction and optimization steps.

Abstract: We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\frac{1}{2}x^\top Ax+b^\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical "Predict-then-Optimize (PtO)" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.

</details>


### [222] [DeepMTL2R: A Library for Deep Multi-task Learning to Rank](https://arxiv.org/abs/2602.14519)
*Chaosheng Dong,Peiyao Xiao,Yijia Wang,Kaiyi Ji*

Main category: cs.LG

TL;DR: DeepMTL2R is an open-source deep learning framework for Multi-task Learning to Rank (MTL2R) that integrates multiple relevance criteria using transformer self-attention, supports 21 MTL algorithms, enables multi-objective optimization for Pareto-optimal models, and provides a scalable solution for modern ranking systems.


<details>
  <summary>Details</summary>
Motivation: Modern ranking systems need to optimize multiple relevance criteria simultaneously (Multi-task Learning to Rank), which involves heterogeneous signals and potentially conflicting objectives. There's a need for a unified, context-aware framework that can effectively handle these challenges and facilitate comparisons across different MTL strategies.

Method: Leverages transformer self-attention mechanism to capture complex dependencies and long-range interactions among items and labels. Integrates heterogeneous relevance signals into a unified model. Includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models.

Result: Demonstrated effectiveness on publicly available dataset with competitive performance. Visualized trade-offs among objectives. The framework provides scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies.

Conclusion: DeepMTL2R is a comprehensive open-source framework that addresses the challenges of Multi-task Learning to Rank by providing transformer-based architecture, extensive algorithm support, and multi-objective optimization capabilities, making it a valuable resource for ranking research and applications.

Abstract: This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.

</details>


### [223] [Truly Adapting to Adversarial Constraints in Constrained MABs](https://arxiv.org/abs/2602.14543)
*Francesco Emanuele Stradi,Kalana Kalupahana,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: First algorithms for constrained MAB with stochastic constraints and adversarial losses, achieving optimal regret and positive constraint violation under full and bandit feedback.


<details>
  <summary>Details</summary>
Motivation: Prior work couldn't guarantee both sublinear regret and sublinear violation in non-stationary environments with unknown constraints. Existing approaches either assumed stochastic constraints or relaxed benchmarks for adversarial constraints.

Method: Proposed algorithms for different feedback settings: full feedback for both losses and constraints, bandit feedback for losses only, and bandit feedback for constraints. Used techniques to handle non-stationarity quantified by parameter C.

Result: Under full feedback: Õ(√T + C) regret and Õ(√T + C) positive violation. With bandit feedback for losses: similar guarantees. With bandit feedback for constraints: Õ(√T + C) positive violation and Õ(√T + C√T) regret.

Conclusion: The paper provides the first optimal algorithms for constrained MAB with stochastic constraints and adversarial losses, with smooth degradation guarantees as constraints become more adversarial.

Abstract: We study the constrained variant of the \emph{multi-armed bandit} (MAB) problem, in which the learner aims not only at minimizing the total loss incurred during the learning dynamic, but also at controlling the violation of multiple \emph{unknown} constraints, under both \emph{full} and \emph{bandit feedback}. We consider a non-stationary environment that subsumes both stochastic and adversarial models and where, at each round, both losses and constraints are drawn from distributions that may change arbitrarily over time. In such a setting, it is provably not possible to guarantee both sublinear regret and sublinear violation. Accordingly, prior work has mainly focused either on settings with stochastic constraints or on relaxing the benchmark with fully adversarial constraints (\emph{e.g.}, via competitive ratios with respect to the optimum). We provide the first algorithms that achieve optimal rates of regret and \emph{positive} constraint violation when the constraints are stochastic while the losses may vary arbitrarily, and that simultaneously yield guarantees that degrade smoothly with the degree of adversariality of the constraints. Specifically, under \emph{full feedback} we propose an algorithm attaining $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ regret and $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation, where $C$ quantifies the amount of non-stationarity in the constraints. We then show how to extend these guarantees when only bandit feedback is available for the losses. Finally, when \emph{bandit feedback} is available for the constraints, we design an algorithm achieving $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation and $\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$ regret.

</details>


### [224] [Governing AI Forgetting: Auditing for Machine Unlearning Compliance](https://arxiv.org/abs/2602.14553)
*Qinqi Lin,Ningning Ding,Lingjie Duan,Jianwei Huang*

Main category: cs.LG

TL;DR: The paper introduces the first economic framework for auditing machine unlearning compliance by combining certified unlearning theory with game theory to analyze strategic interactions between auditors and AI operators.


<details>
  <summary>Details</summary>
Motivation: Despite legal requirements for the right to be forgotten, AI operators often fail to comply with data deletion requests. While machine unlearning offers a technical solution, there's a fundamental gap between its technical feasibility and regulatory implementation that makes compliance enforcement challenging.

Method: The paper integrates certified unlearning theory with regulatory enforcement through a game-theoretic model. It characterizes MU verification uncertainty using hypothesis-testing interpretations of certified unlearning, then models strategic auditor-operator interactions. The method transforms complex bivariate nonlinear fixed-point problems into tractable univariate auxiliary problems to enable equilibrium analysis.

Result: Analysis shows that auditors can optimally reduce inspection intensity as deletion requests increase because operators' weakened unlearning makes non-compliance easier to detect. This explains recent auditing reductions in China despite growing deletion requests. Additionally, undisclosed auditing offers informational advantages but paradoxically reduces regulatory cost-effectiveness compared to disclosed auditing.

Conclusion: The paper establishes the first economic framework for auditing machine unlearning compliance, revealing counterintuitive strategic dynamics where less inspection can be optimal and showing that disclosed auditing is more cost-effective despite informational disadvantages.

Abstract: Despite legal mandates for the right to be forgotten, AI operators routinely fail to comply with data deletion requests. While machine unlearning (MU) provides a technical solution to remove personal data's influence from trained models, ensuring compliance remains challenging due to the fundamental gap between MU's technical feasibility and regulatory implementation. In this paper, we introduce the first economic framework for auditing MU compliance, by integrating certified unlearning theory with regulatory enforcement. We first characterize MU's inherent verification uncertainty using a hypothesis-testing interpretation of certified unlearning to derive the auditor's detection capability, and then propose a game-theoretic model to capture the strategic interactions between the auditor and the operator. A key technical challenge arises from MU-specific nonlinearities inherent in the model utility and the detection probability, which create complex strategic couplings that traditional auditing frameworks do not address and that also preclude closed-form solutions. We address this by transforming the complex bivariate nonlinear fixed-point problem into a tractable univariate auxiliary problem, enabling us to decouple the system and establish the equilibrium existence, uniqueness, and structural properties without relying on explicit solutions. Counterintuitively, our analysis reveals that the auditor can optimally reduce the inspection intensity as deletion requests increase, since the operator's weakened unlearning makes non-compliance easier to detect. This is consistent with recent auditing reductions in China despite growing deletion requests. Moreover, we prove that although undisclosed auditing offers informational advantages for the auditor, it paradoxically reduces the regulatory cost-effectiveness relative to disclosed auditing.

</details>


### [225] [DCTracks: An Open Dataset for Machine Learning-Based Drift Chamber Track Reconstruction](https://arxiv.org/abs/2602.14571)
*Qian Liyan,Zhang Yao,Yuan Ye,Zhang Zhaoke,Fang Jin,Jiang Shimiao,Zhang Jin,Li Ke,Liu Beijiang,Xu Chenglin,Zhang Yifan,Jia Xiaoqian,Qin Xiaoshuai,Huang Xingtao*

Main category: cs.LG

TL;DR: Created benchmark MC dataset and metrics for ML-based track reconstruction, comparing traditional algorithms and GNNs


<details>
  <summary>Details</summary>
Motivation: Need for standardized evaluation and reproducible validation of ML-based track reconstruction methods in particle physics

Method: Developed Monte Carlo dataset of single- and two-track events, defined track reconstruction metrics, tested traditional algorithms and Graph Neural Networks

Result: Established benchmark framework with standardized metrics for comparing different track reconstruction approaches

Conclusion: Provides foundation for rigorous, comparable future research in ML-based track reconstruction

Abstract: We introduce a Monte Carlo (MC) dataset of single- and two-track drift chamber events to advance Machine Learning (ML)-based track reconstruction. To enable standardized and comparable evaluation, we define track reconstruction specific metrics and report results for traditional track reconstruction algorithms and a Graph Neural Networks (GNNs) method, facilitating rigorous, reproducible validation for future research.

</details>


### [226] [RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch](https://arxiv.org/abs/2602.14578)
*Isam Vrce,Andreas Kassler,Gökçe Aydos*

Main category: cs.LG

TL;DR: N:M structured sparsity applied to deep reinforcement learning enables hardware-friendly compression with minimal performance loss, outperforming dense networks at 50-75% sparsity levels.


<details>
  <summary>Details</summary>
Motivation: Existing sparsity methods in DRL use unstructured fine-grained sparsity that limits hardware acceleration, or structured coarse-grained sparsity that degrades performance. There's a need for sparsity that balances compression, performance, and hardware efficiency.

Method: Introduces RNM-TD3 framework that enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3). Maintains compatibility with accelerators supporting N:M sparse matrix operations.

Result: RNM-TD3 outperforms dense counterparts at 50-75% sparsity (2:4 and 1:4), achieving up to 14% performance increase at 2:4 sparsity on Ant environment. Remains competitive at 87.5% sparsity (1:8) with potential training speedups.

Conclusion: N:M structured sparsity is a promising approach for RL that enables hardware-efficient compression without sacrificing performance, achieving better-than-dense performance at moderate sparsity levels.

Abstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.

</details>


### [227] [Replicable Constrained Bandits](https://arxiv.org/abs/2602.14580)
*Matteo Bollini,Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.LG

TL;DR: Replicable algorithms for constrained multi-armed bandits with performance matching non-replicable counterparts.


<details>
  <summary>Details</summary>
Motivation: Address the need for reproducible experiments in machine learning by studying algorithmic replicability in constrained MAB problems.

Method: Design replicable algorithms for constrained MABs, including developing the first replicable UCB-like algorithm for unconstrained MABs based on the optimism-in-the-face-of-uncertainty principle.

Result: Show that replicability can be achieved in constrained MABs with regret and constraint violation matching non-replicable algorithms in terms of time horizon T.

Conclusion: Demonstrate that replicable algorithms are feasible for constrained bandit problems while maintaining performance guarantees, with potential broader applications of replicable optimism-based methods.

Abstract: Algorithmic \emph{replicability} has recently been introduced to address the need for reproducible experiments in machine learning. A \emph{replicable online learning} algorithm is one that takes the same sequence of decisions across different executions in the same environment, with high probability. We initiate the study of algorithmic replicability in \emph{constrained} MAB problems, where a learner interacts with an unknown stochastic environment for $T$ rounds, seeking not only to maximize reward but also to satisfy multiple constraints. Our main result is that replicability can be achieved in constrained MABs. Specifically, we design replicable algorithms whose regret and constraint violation match those of non-replicable ones in terms of $T$. As a key step toward these guarantees, we develop the first replicable UCB-like algorithm for \emph{unconstrained} MABs, showing that algorithms that employ the optimism in-the-face-of-uncertainty principle can be replicable, a result that we believe is of independent interest.

</details>


### [228] [Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow](https://arxiv.org/abs/2602.14587)
*Minh Nguyen*

Main category: cs.LG

TL;DR: A novel continuous-time actor-critic algorithm with decoupled updates for event-driven control problems where standard RL struggles due to infinitesimal time steps.


<details>
  <summary>Details</summary>
Motivation: Standard discrete-time RL fails in continuous-time control problems with non-uniform, event-driven decisions because as time gaps shrink, the Q-function collapses to V-function, eliminating action ranking. Existing continuous-time methods introduce complexity through martingale losses or orthogonality constraints that are sensitive to test processes.

Method: Decoupled continuous-time actor-critic algorithm with alternating updates: q is learned from diffusion generators on V, and V is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps.

Result: Outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and real-world trading tasks, achieving 21% profit over a single quarter (nearly double the second-best method).

Conclusion: The proposed decoupled approach addresses fundamental limitations of existing continuous-time RL methods by providing reliable training and theoretical convergence while maintaining action discriminability at infinitesimal time scales.

Abstract: Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>


### [229] [OPBench: A Graph Benchmark to Combat the Opioid Crisis](https://arxiv.org/abs/2602.14602)
*Tianyi Ma,Yiyang Li,Yiyue Qian,Zheyuan Zhang,Zehong Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: OPBench is the first comprehensive benchmark for evaluating graph learning methods on real-world opioid crisis scenarios across three critical domains: overdose detection, drug trafficking detection, and drug misuse prediction.


<details>
  <summary>Details</summary>
Motivation: The opioid epidemic strains healthcare systems and communities worldwide, creating an urgent need for computational solutions. While graph learning methods show promise for modeling complex drug-related phenomena, there's a significant gap: no comprehensive benchmark exists to systematically evaluate these methods across real-world opioid crisis scenarios.

Method: The authors introduce OPBench with five datasets across three application domains, incorporating diverse graph structures (heterogeneous graphs and hypergraphs) to preserve complex relational information. They collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy/ethical guidelines. A unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines is established.

Result: OPBench provides the first comprehensive opioid benchmark with five datasets across three critical domains. Through extensive experiments, the authors analyze strengths and limitations of existing graph learning methods, providing actionable insights for future research. The source code and datasets are publicly available.

Conclusion: OPBench bridges the critical gap in evaluating graph learning methods for opioid crisis applications, providing a standardized framework for fair comparison and advancing computational solutions to combat the opioid epidemic. The benchmark enables systematic evaluation and drives future research in this critical domain.

Abstract: The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.

</details>


### [230] [Concepts' Information Bottleneck Models](https://arxiv.org/abs/2602.14626)
*Karim Galliamov,Syed M Ahsan Kazmi,Adil Khan,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: A new information bottleneck regularizer for concept bottleneck models improves accuracy and faithfulness by enforcing minimal-sufficient concept representations.


<details>
  <summary>Details</summary>
Motivation: Concept Bottleneck Models (CBMs) suffer from reduced accuracy and concept leakage that undermines interpretability and faithfulness in predictions.

Method: Introduces an explicit Information Bottleneck regularizer that penalizes I(X;C) while preserving I(C;Y), encouraging minimal-sufficient concept representations. Derives two practical variants (variational objective and entropy-based surrogate) and integrates them into standard CBM training.

Result: IB-regularized models consistently outperform vanilla CBMs across six CBM families and three benchmarks. Information-plane analyses confirm the intended behavior.

Conclusion: Enforcing minimal-sufficient concept bottlenecks improves predictive performance and reliability of concept-level interventions. The regularizer provides a theoretically-grounded, architecture-agnostic path to more faithful and intervenable CBMs.

Abstract: Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.

</details>


### [231] [Alignment Adapter to Improve the Performance of Compressed Deep Learning Models](https://arxiv.org/abs/2602.14635)
*Rohit Raj Rai,Abhishek Dhaka,Amit Awekar*

Main category: cs.LG

TL;DR: Proposes Alignment Adapter (AlAd), a lightweight sliding-window adapter that aligns token embeddings of compressed models with original large models to boost compressed model performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Compressed DL models are essential for resource-constrained environments but their performance often lags behind original large models, creating a performance gap that needs bridging.

Method: AlAd is a lightweight, sliding-window-based adapter that aligns token-level embeddings of compressed models with those of the original large model. It preserves local contextual semantics, enables flexible alignment across different dimensionalities/architectures, and is agnostic to compression methods. Can be deployed as plug-and-play module or jointly fine-tuned.

Result: Through experiments on BERT-family models across three token-level NLP tasks, AlAd significantly boosts performance of compressed models with only marginal overhead in size and latency.

Conclusion: AlAd effectively bridges the performance gap between compressed and original large models through lightweight alignment of token embeddings, offering a practical solution for deploying compressed models in resource-constrained environments.

Abstract: Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.

</details>


### [232] [An Embarrassingly Simple Way to Optimize Orthogonal Matrices at Scale](https://arxiv.org/abs/2602.14656)
*Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: POGO is a new fast GPU-friendly optimizer for orthogonal constraints that scales to thousands of constraints, outperforming existing methods while maintaining exact orthogonality throughout optimization.


<details>
  <summary>Details</summary>
Motivation: Current optimizers for orthogonal constraints are computationally expensive and don't scale well to problems with hundreds or thousands of constraints, creating a bottleneck for exploiting orthogonality constraints in large-scale machine learning applications.

Method: POGO improves on the Landing algorithm by enabling the inclusion of modern adaptive optimizers while ensuring orthogonal constraints are effectively met at all times. It's a fast GPU-friendly algorithm consisting of only 5 matrix products.

Result: POGO greatly outperforms recent optimizers on challenging benchmarks, can optimize problems with thousands of orthogonal matrices in minutes (versus hours for alternatives), and maintains orthogonality at all times in practice.

Conclusion: POGO sets a milestone for exploiting orthogonality constraints at scale in machine learning, offering a fast, scalable solution with minimal hyperparameters and maintaining strict orthogonality throughout optimization.

Abstract: Orthogonality constraints are ubiquitous in robust and probabilistic machine learning. Unfortunately, current optimizers are computationally expensive and do not scale to problems with hundreds or thousands of constraints. One notable exception is the Landing algorithm (Ablin et al., 2024) which, however comes at the expense of temporarily relaxing orthogonality. In this work, we revisit and improve on the ideas behind Landing, enabling the inclusion of modern adaptive optimizers while ensuring that orthogonal constraints are effectively met. Remarkably, these improvements come at little to no cost, and reduce the number of required hyperparemeters. Our algorithm POGO is fast and GPU-friendly, consisting of only 5 matrix products, and in practice maintains orthogonality at all times. On several challenging benchmarks, POGO greatly outperforms recent optimizers and shows it can optimize problems with thousands of orthogonal matrices in minutes while alternatives would take hours. As such, POGO sets a milestone to finally exploit orthogonality constraints in ML at scale. A PyTorch implementation of POGO is publicly available at https://github.com/adrianjav/pogo.

</details>


### [233] [Pseudo-differential-enhanced physics-informed neural networks](https://arxiv.org/abs/2602.14663)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: Pseudo-differential enhanced PINNs improve training by applying gradient enhancement in Fourier space rather than physical space, leading to faster convergence, better frequency learning, and compatibility with fractional derivatives and various domains.


<details>
  <summary>Details</summary>
Motivation: Gradient enhancement in PINNs typically operates in physical space by taking higher-order derivatives, but applying this in Fourier space could offer computational advantages and better high-frequency learning due to the multiplication property of differentiation in Fourier domain.

Method: Extend gradient enhancement to Fourier space by applying Fourier transforms, where differentiation becomes multiplication with wavenumber. Use Monte Carlo methods for mesh flexibility on Euclidean and non-Euclidean domains, and show compatibility with Fourier feature embeddings.

Result: The method achieves superior PINN error reduction in fewer training iterations, breaks plateaus in low collocation settings, improves spectral eigenvalue decay of the Neural Tangent Kernel (NTK), and enables learning of high frequencies earlier in training. It also works with fractional derivatives.

Conclusion: Pseudo-differential enhancement in Fourier space provides an effective extension to gradient-enhanced PINNs that accelerates training, improves frequency learning, and maintains flexibility for various domains and advanced PINN techniques while handling fractional derivatives.

Abstract: We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an augmented term in order to improve training and overall learning fidelity. We propose the same procedure after application via Fourier transforms, since differentiating in Fourier space is multiplication with the Fourier wavenumber under suitable decay. Our methods are fast and efficient. Our methods oftentimes achieve superior PINN versus numerical error in fewer training iterations, potentially pair well with few samples in collocation, and can on occasion break plateaus in low collocation settings. Moreover, our methods are suitable for fractional derivatives. We establish that our methods improve spectral eigenvalue decay of the neural tangent kernel (NTK), and so our methods contribute towards the learning of high frequencies in early training, mitigating the effects of frequency bias up to the polynomial order and possibly greater with smooth activations. Our methods accommodate advanced techniques in PINNs, such as Fourier feature embeddings. A pitfall of discrete Fourier transforms via the Fast Fourier Transform (FFT) is mesh subjugation, and so we demonstrate compatibility of our methods for greater mesh flexibility and invariance on alternative Euclidean and non-Euclidean domains via Monte Carlo methods and otherwise.

</details>


### [234] [Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error](https://arxiv.org/abs/2602.14682)
*Farzan Farnia,Mohammad Jalali,Azim Ospanov*

Main category: cs.LG

TL;DR: Generative models produce lower diversity samples than real data due to underestimation of true distribution diversity in finite training sets, causing systematic diversity loss when optimizing models to match empirical distributions.


<details>
  <summary>Details</summary>
Motivation: While deep generative models excel at producing high-quality samples, their ability to capture the full diversity of underlying data distributions has not been systematically studied. The paper aims to investigate whether state-of-the-art generative models faithfully reproduce data diversity.

Method: Used reference-free entropy-based diversity scores (Vendi and RKE) to compare diversity between generated samples and test data from target distributions across multiple benchmark datasets. Analyzed finite-sample behavior of diversity scores and studied how diversity loss arises when optimizing models to minimize divergence to empirical data distributions.

Result: Test data consistently shows substantially higher Vendi and RKE diversity scores than generated samples across datasets, revealing systematic downward diversity bias in modern generative models. Finite-sample analysis shows diversity scores increase with sample size, suggesting empirical distributions inherently underestimate true diversity.

Conclusion: The diversity loss in generative models stems from optimizing models to match empirical distributions that underestimate true diversity. The paper proposes diversity-aware regularization and guidance strategies using Vendi and RKE scores as potential solutions to mitigate this bias.

Abstract: Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.

</details>


### [235] [SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data](https://arxiv.org/abs/2602.14687)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: SynthSAEBench is a synthetic data toolkit and benchmark for evaluating Sparse Autoencoder architectures with realistic feature characteristics, enabling precise validation of architectural improvements before scaling to LLMs.


<details>
  <summary>Details</summary>
Motivation: Current SAE benchmarks on LLMs are too noisy to differentiate architectural improvements, and synthetic data experiments are too small-scale and unrealistic for meaningful comparisons. There's a need for better evaluation tools that can precisely validate SAE architectural innovations.

Method: The authors introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics (correlation, hierarchy, superposition) and a standardized benchmark model (SynthSAEBench-16k) that enables direct comparison of SAE architectures.

Result: The benchmark reproduces several LLM SAE phenomena (disconnect between reconstruction/latent quality metrics, poor SAE probing results, precision-recall trade-off mediated by L0) and identifies a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting expressive encoders can easily overfit.

Conclusion: SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.

Abstract: Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.

</details>


### [236] [A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)](https://arxiv.org/abs/2602.14696)
*Nihal V. Nayak,Paula Rodriguez-Diaz,Neha Hulkund,Sara Beery,David Alvarez-Melis*

Main category: cs.LG

TL;DR: A systematic analysis of instruction selection for LLM fine-tuning, showing gradient-based representations with greedy round-robin selection works best at low budgets, benefits diminish at higher budgets, and unifying various selection methods as approximate distance minimization.


<details>
  <summary>Details</summary>
Motivation: The literature on targeted instruction selection is fragmented and opaque - methods vary widely, often omit zero-shot baselines, and entangle key components, leaving practitioners without actionable guidance for selecting instructions for their target tasks.

Method: Disentangle and systematically analyze the two core ingredients: data representation and selection algorithms, enabling controlled comparisons across models, tasks, and budgets through a unified framework.

Result: Only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. Gradient-based representations with greedy round-robin selection perform best on average at low budgets, but benefits diminish at larger budgets. Several existing selection algorithms can be unified as forms of approximate distance minimization between selected subset and query set.

Conclusion: Provides critical insights and a foundation for more principled data selection in LLM fine-tuning by clarifying the fragmented landscape and showing that gradient-based representations with appropriate selection algorithms offer the most consistent performance.

Abstract: Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.

</details>


### [237] [Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation](https://arxiv.org/abs/2602.14701)
*Killian Bakong,Laurent Massoulié,Edouard Oyallon,Kevin Scaman*

Main category: cs.LG

TL;DR: Randomized unbiased approximations of vector-Jacobian products during backpropagation reduce computational and memory costs in deep neural network training with theoretical guarantees and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To address the high computational and memory costs of training deep neural networks, which are major bottlenecks in deep learning applications.

Method: Propose replacing exact vector-Jacobian products with randomized, unbiased approximations during backpropagation. Develop theoretical analysis of trade-offs, identify optimal unbiased estimates with minimal variance under sparsity constraints.

Result: Theoretical analysis shows manageable trade-off between epoch requirements and cost reduction. Optimal unbiased estimates identified with minimal variance. Experiments on MLPs, BagNets, and Visual Transformers validate approach and demonstrate significant cost reduction potential.

Conclusion: Unbiased randomized backpropagation effectively reduces deep learning training costs with theoretical guarantees, making it a promising approach for more efficient neural network training.

Abstract: In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-off between the number of epochs needed to achieve a target precision and the cost reduction for each epoch. We then identify specific unbiased estimates of vector-jacobian products for which we establish desirable optimality properties of minimal variance under sparsity constraints. Finally we provide in-depth experiments on multi-layer perceptrons, BagNets and Visual Transfomers architectures. These validate our theoretical results, and confirm the potential of our proposed unbiased randomized backpropagation approach for reducing the cost of deep learning.

</details>


### [238] [D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation](https://arxiv.org/abs/2602.14728)
*Nozomu Fujisawa,Masaaki Kondo*

Main category: cs.LG

TL;DR: D2-LoRA is a parameter-efficient fine-tuning method that combines signed low-rank residual updates with a column-wise projection, achieving high accuracy with minimal data, zero inference latency after merging, and outperforming LoRA and DoRA.


<details>
  <summary>Details</summary>
Motivation: To systematically explore the parameter-efficient fine-tuning design space under practical data and compute constraints, aiming to improve performance while maintaining algebraic mergeability and numerical equivalence at inference.

Method: D2-LoRA uses signed low-rank residual updates with additive and subtractive components, along with a train-time column-wise projection that maintains each column close to its original norm. After training, the adapter is merged into a single weight matrix, eliminating inference latency.

Result: D2-LoRA achieves 76.4% average accuracy across eight QA and reading comprehension benchmarks using only 5k training samples per task and two epochs. It improves average accuracy by 2.2 percentage points over LoRA, matches or exceeds DoRA on most tasks, and shows gains in generative tasks (1.2 ROUGE-L, 1.1% win rate) with 36% lower training volatility. Merging preserves numerical fidelity (mean gap ~0.03 percentage points) and recovers ~1.91x evaluation throughput.

Conclusion: D2-LoRA is a parameter-efficient fine-tuning method that combines signed low-rank residual updates with additive and subtractive components and a train-time column-wise projection. It achieves state-of-the-art performance across multiple benchmarks with minimal training data, preserves mergeability for zero-inference-latency deployment, and demonstrates improved stability and efficiency compared to existing methods like LoRA and DoRA.

Abstract: We systematically investigate the parameter-efficient fine-tuning design space under practical data and compute constraints, and propose D2-LoRA. D2-LoRA achieves 76.4 percent average accuracy across eight question answering and reading comprehension benchmarks using only 5k training samples per task and two epochs, while preserving algebraic mergeability at inference with near-exact numerical equivalence. The method combines signed low-rank residual updates with additive and subtractive components, together with a train-time column-wise projection that keeps each column close to its original norm. After training, the adapter is merged into a single weight matrix, adding zero inference latency. Compared with LoRA, D2-LoRA improves average accuracy by 2.2 percentage points; at matched parameter counts (LoRA rank 2r versus D2-LoRA rank r), the improvement is 1.6 points, indicating gains from architectural design rather than increased parameterization. Compared with DoRA, it matches or exceeds performance on most tasks. Beyond QA and reading comprehension, D2-LoRA improves generative tasks (plus 1.2 ROUGE-L and plus 1.1 percent win rate) and shows 36 percent lower training volatility. The merge preserves numerical fidelity (mean gap about 0.03 percentage points) and recovers about 1.91x evaluation throughput. Training overhead is 19 percent, comparable to DoRA, and decreases with longer input sequences. We provide a geometric analysis explaining how the projection stabilizes training, together with ablation studies isolating the contribution of each design component.

</details>


### [239] [Scale redundancy and soft gauge fixing in positively homogeneous neural networks](https://arxiv.org/abs/2602.14729)
*Rodrigo Carmo Terin*

Main category: cs.LG

TL;DR: A gauge-theoretic approach to neural networks with homogeneous activations, using norm-balancing penalties to improve optimization conditioning by suppressing scale drift.


<details>
  <summary>Details</summary>
Motivation: Neural networks with positively homogeneous activation functions (like ReLU) exhibit continuous reparametrization symmetry (neuron-wise rescalings) that creates parameter-space orbits with identical input-output functions. This symmetry introduces gauge redundancy that affects optimization geometry and stability.

Method: 1) Interpret the scaling symmetry as gauge redundancy. 2) Introduce gauge-adapted coordinates separating invariant and scale-imbalance directions. 3) Propose a soft orbit-selection functional (norm-balancing penalty) acting only on redundant scale coordinates, inspired by gauge fixing in field theory. 4) Analytically show this penalty induces dissipative relaxation of imbalance modes while preserving function.

Result: 1) The orbit-selection penalty expands the stable learning-rate regime during training. 2) Suppresses scale drift without changing network expressivity. 3) Provides experimental validation showing improved optimization conditioning.

Conclusion: The work establishes a structural link between gauge-orbit geometry and optimization conditioning, providing a concrete connection between gauge-theoretic concepts and machine learning. The approach offers a principled method to improve training stability for networks with homogeneous activations.

Abstract: Neural networks with positively homogeneous activations exhibit an exact continuous reparametrization symmetry: neuron-wise rescalings generate parameter-space orbits along which the input--output function is invariant. We interpret this symmetry as a gauge redundancy and introduce gauge-adapted coordinates that separate invariant and scale-imbalance directions. Inspired by gauge fixing in field theory, we introduce a soft orbit-selection (norm-balancing) functional acting only on redundant scale coordinates. We show analytically that it induces dissipative relaxation of imbalance modes to preserve the realized function. In controlled experiments, this orbit-selection penalty expands the stable learning-rate regime and suppresses scale drift without changing expressivity. These results establish a structural link between gauge-orbit geometry and optimization conditioning, providing a concrete connection between gauge-theoretic concepts and machine learning.

</details>


### [240] [Parameter-Minimal Neural DE Solvers via Horner Polynomials](https://arxiv.org/abs/2602.14737)
*T. Matulić,D. Seršić*

Main category: cs.LG

TL;DR: A low-parameter neural architecture using Horner-factorized polynomials and piecewise extensions achieves accurate solutions to differential equations with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To develop a resource-efficient neural approach for solving differential equations that minimizes parameter count while maintaining accuracy, enabling practical scientific modeling on resource-constrained systems.

Method: Uses Horner-factorized polynomials as implicit differentiable trial solutions with few learnable coefficients. Initial conditions are enforced exactly by construction. Employs piecewise ("spline-like") extensions with multiple small Horner models on subintervals, enforcing continuity and first-derivative continuity at boundaries.

Result: On ODE benchmarks and a heat-equation example, Horner networks with tens or fewer parameters accurately match solutions and derivatives, outperforming small MLP and sinusoidal-representation baselines under the same training settings.

Conclusion: Horner-factorized polynomial networks provide a practical accuracy-parameter trade-off for resource-efficient scientific modeling, demonstrating that accurate differential equation solutions can be achieved with minimal parameters through specialized architectures.

Abstract: We propose a parameter-minimal neural architecture for solving differential equations by restricting the hypothesis class to Horner-factorized polynomials, yielding an implicit, differentiable trial solution with only a small set of learnable coefficients. Initial conditions are enforced exactly by construction by fixing the low-order polynomial degrees of freedom, so training focuses solely on matching the differential-equation residual at collocation points. To reduce approximation error without abandoning the low-parameter regime, we introduce a piecewise ("spline-like") extension that trains multiple small Horner models on subintervals while enforcing continuity (and first-derivative continuity) at segment boundaries. On illustrative ODE benchmarks and a heat-equation example, Horner networks with tens (or fewer) parameters accurately match the solution and its derivatives and outperform small MLP and sinusoidal-representation baselines under the same training settings, demonstrating a practical accuracy-parameter trade-off for resource-efficient scientific modeling.

</details>


### [241] [Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training](https://arxiv.org/abs/2602.14759)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.LG

TL;DR: Inner looping: repeatedly re-applying transformer blocks at inference time improves performance in frozen pretrained models through continued refinement.


<details>
  <summary>Details</summary>
Motivation: Transformers refine representations across layers, suggesting inner space is shared and early decoding is possible. If some layers act as refinement layers, prolonging this refinement at inference time could improve performance without training.

Method: Inference-time inner looping: repeatedly re-applying a selected block range in frozen pretrained language models during inference to prolong refinement.

Result: Modest but consistent accuracy improvements across multiple benchmarks. Latent trajectories show more stable state evolution and continued semantic refinement.

Conclusion: Additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models without retraining.

Abstract: Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.

</details>


### [242] [Universal Algorithm-Implicit Learning](https://arxiv.org/abs/2602.14761)
*Stefano Woerner,Seong Joon Oh,Christian F. Baumgartner*

Main category: cs.LG

TL;DR: The paper proposes TAIL, a transformer-based algorithm-implicit meta-learner that works across diverse tasks with varying domains, modalities, and label configurations, addressing limitations of current meta-learning methods.


<details>
  <summary>Details</summary>
Motivation: Current meta-learning methods are limited to narrow task distributions with fixed feature and label spaces, and there's inconsistent terminology in the meta-learning literature that hinders comparability of methods.

Method: Introduces a theoretical framework defining practical universality and distinguishes algorithm-explicit vs algorithm-implicit learning. Then presents TAIL with three innovations: random projections for cross-modal feature encoding, random injection label embeddings for extrapolating to larger label spaces, and efficient inline query processing.

Result: TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. It also generalizes to unseen modalities (solves text classification despite training only on images), handles tasks with up to 20× more classes than seen during training, and provides orders-of-magnitude computational savings.

Conclusion: TAIL represents a significant advancement in universal meta-learning, demonstrating practical applicability across diverse tasks and addressing key limitations of previous approaches through both theoretical framework and practical implementation.

Abstract: Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like "universal" and "general-purpose" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.

</details>


### [243] [Learning Structural Hardness for Combinatorial Auctions: Instance-Dependent Algorithm Selection via Graph Neural Networks](https://arxiv.org/abs/2602.14772)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: This paper proposes using machine learning to predict when combinatorial auction instances will be hard for greedy algorithms, enabling hybrid algorithm selection that combines fast greedy heuristics with more expensive GNN-based solvers only when needed.


<details>
  <summary>Details</summary>
Motivation: The Winner Determination Problem in combinatorial auctions is NP-hard, and existing approaches that try to use ML to replace solvers often fail to outperform classical methods. Instead of trying to replace solvers, the authors aim to learn when instances are hard for greedy allocation, enabling intelligent algorithm selection.

Method: 1) Design a 20-dimensional structural feature vector to characterize auction instances. 2) Train a lightweight MLP classifier to predict greedy optimality gap and identify hard instances with "whale-fish" trap structures. 3) For hard instances, deploy a heterogeneous GNN specialist solver. 4) Create a hybrid allocator that combines the hardness classifier with both GNN and greedy solvers based on instance difficulty prediction.

Result: The MLP hardness classifier achieves mean absolute error 0.033, Pearson correlation 0.937, and binary classification accuracy 94.7% across three random seeds. For hard instances with "whale-fish" traps, the GNN specialist achieves ≈0% optimality gap vs. 3.75-59.24% for greedy. The hybrid allocator achieves 0.51% overall gap on mixed distributions. Evaluation on CATS benchmarks shows GNNs don't outperform Gurobi (0.45-0.71 vs. 0.20 gap), supporting the algorithm selection approach.

Conclusion: Learning when to deploy expensive solvers is more tractable than learning to replace them. The proposed instance-dependent algorithm selection framework effectively combines fast greedy methods with specialized GNN solvers for hard instances, achieving good overall performance without always running expensive computations.

Abstract: The Winner Determination Problem (WDP) in combinatorial auctions is NP-hard, and no existing method reliably predicts which instances will defeat fast greedy heuristics. The ML-for-combinatorial-optimization community has focused on learning to \emph{replace} solvers, yet recent evidence shows that graph neural networks (GNNs) rarely outperform well-tuned classical methods on standard benchmarks. We pursue a different objective: learning to predict \emph{when} a given instance is hard for greedy allocation, enabling instance-dependent algorithm selection. We design a 20-dimensional structural feature vector and train a lightweight MLP hardness classifier that predicts the greedy optimality gap with mean absolute error 0.033, Pearson correlation 0.937, and binary classification accuracy 94.7\% across three random seeds. For instances identified as hard -- those exhibiting ``whale-fish'' trap structure where greedy provably fails -- we deploy a heterogeneous GNN specialist that achieves ${\approx}0\%$ optimality gap on all six adversarial configurations tested (vs.\ 3.75--59.24\% for greedy). A hybrid allocator combining the hardness classifier with GNN and greedy solvers achieves 0.51\% overall gap on mixed distributions. Our honest evaluation on CATS benchmarks confirms that GNNs do not outperform Gurobi (0.45--0.71 vs.\ 0.20 gap), motivating the algorithm selection framing. Learning \emph{when} to deploy expensive solvers is more tractable than learning to replace them.

</details>


### [244] [On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials](https://arxiv.org/abs/2602.14789)
*Rotem Mulayoff,Sebastian U. Stich*

Main category: cs.LG

TL;DR: Stability analysis of optimization algorithms (GD/SGD) requires considering nonlinear terms, as linearization can be misleading; derived exact criterion for stable oscillations and showed single unstable batch can cause divergence.


<details>
  <summary>Details</summary>
Motivation: Prior stability analysis often relies on linearization, which may not capture full nonlinear behavior. For example, GD may stably oscillate near linearly unstable minima. Need to understand true nonlinear dynamics of optimization algorithms near minima.

Method: Derived exact criterion for stable oscillations of GD near minima in multivariate setting using high-order derivatives. Extended analysis to SGD, showing nonlinear dynamics can diverge even if single batch is unstable. Proved that if all batches are linearly stable, SGD nonlinear dynamics are stable in expectation.

Result: Nonlinear stability analysis reveals: 1) GD can have stable oscillations not predicted by linear analysis; 2) SGD can diverge in expectation due to a single unstable batch, contrary to linear average analysis; 3) If all batches are linearly stable, SGD nonlinear dynamics are stable.

Conclusion: Linear stability analysis is insufficient for understanding optimization dynamics. Nonlinear terms are crucial, and stability can be dictated by worst-case batches rather than averages. Provides exact criterion for GD oscillations and clarifies when SGD nonlinear dynamics are stable.

Abstract: The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.

</details>


### [245] [Extending Multi-Source Bayesian Optimization With Causality Principles](https://arxiv.org/abs/2602.14791)
*Luuk Jacobs,Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: Integrates causal reasoning with multi-source Bayesian optimization to improve efficiency in complex, high-dimensional problems where interventions are possible.


<details>
  <summary>Details</summary>
Motivation: Traditional MSBO assumes independent variables, limiting effectiveness in causal-aware domains like clinical trials and policy-making. Existing CBO handles causality but only in single-source settings, missing multi-source optimization benefits.

Method: Proposes Multi-Source Causal Bayesian Optimization (MSCBO), a principled integration of Causal BO (handles variable dependencies) with Multi-Source BO (leverages multiple information sources), reducing dimensionality and computational cost.

Result: MSCBO outperforms foundational counterparts (CBO and MSBO) on synthetic and real-world datasets with varying noise, demonstrating improved convergence speed, performance, scalability, and robustness.

Conclusion: Combining causality with multi-source optimization facilitates dimensionality reduction and lowers operational costs, leading to faster convergence, better performance, and enhanced scalability in complex optimization problems.

Abstract: Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.

</details>


### [246] [Learning State-Tracking from Code Using Linear RNNs](https://arxiv.org/abs/2602.14814)
*Julien Siems,Riccardo Grazzi,Kirill Kalinin,Hitesh Ballani,Babak Rahmani*

Main category: cs.LG

TL;DR: Transformers fail at state tracking in code REPL traces while linear RNNs succeed, but linear RNNs struggle with probabilistic state reveals where non-linear RNNs perform better.


<details>
  <summary>Details</summary>
Motivation: Existing state-tracking tasks (like permutation composition) are sequence-to-sequence tasks that don't match next-token prediction training used for language models. The paper aims to bridge this gap by converting permutation composition into code via REPL traces.

Method: Convert permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. Then investigate state tracking in probabilistic finite-state automata with deterministic state reveals, comparing linear RNNs vs non-linear RNNs.

Result: Linear RNNs capable of state-tracking excel in the REPL trace setting while Transformers still fail. However, when tracking states in probabilistic setups where actions are not fully observable, linear RNNs can be worse than non-linear RNNs.

Conclusion: The representation of state tracking in code via REPL traces reveals architectural differences: Transformers struggle while linear RNNs succeed, but linear RNNs have limitations in probabilistic settings where non-linear RNNs perform better.

Abstract: Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.

</details>


### [247] [Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment](https://arxiv.org/abs/2602.14844)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: Proposes Interactionless Inverse Reinforcement Learning (IIRL) to decouple alignment from policy learning, creating reusable reward models, plus Alignment Flywheel for iterative human-in-the-loop refinement.


<details>
  <summary>Details</summary>
Motivation: Current AI alignment methods (RLHF, DPO) suffer from structural flaw that entangles safety objectives with agent policy, creating opaque single-use "Alignment Waste" artifacts.

Method: Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing inspectable/editable/model-agnostic reward models. Alignment Flywheel: human-in-the-loop lifecycle for iterative hardening through automated audits and refinement.

Result: Architecture transforms safety from disposable expense into durable, verifiable engineering asset by creating reusable alignment artifacts and iterative refinement process.

Conclusion: The proposed approach addresses critical structural flaw in current AI alignment, moving from wasteful single-use methods to sustainable, inspectable, and reusable safety engineering.

Abstract: AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.

</details>


### [248] [BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations](https://arxiv.org/abs/2602.14853)
*Jonathan Gorard,Ammar Hakim,James Juno*

Main category: cs.LG

TL;DR: Neural network PDE solvers with formal verification for reliable extrapolation beyond training data.


<details>
  <summary>Details</summary>
Motivation: Overcoming neural networks' unreliable generalization beyond training data convex hulls for solving PDEs in extrapolatory regimes where experimental/analytical validation is impossible.

Method: 1. Use method of characteristics to predict PDE solution properties a priori; 2. Construct rigorous extrapolatory bounds on worst-case L^inf errors of shallow neural approximations; 3. Compose shallow networks into deep architectures using compositional deep learning to suppress large errors; 4. Build BEACONS framework with automatic code-generator and theorem-proving system for machine-checkable correctness certificates.

Result: BEACONS applied to linear/non-linear PDEs (linear advection, inviscid Burgers, compressible Euler in 1D/2D) successfully extrapolates solutions far beyond training data with reliable bounded errors, showing advantages over classical PINN approach.

Conclusion: Formally-verified neural PDE solvers with rigorous convergence, stability, and conservation properties can guarantee correctness even in extrapolatory regimes, overcoming traditional neural network limitations.

Abstract: The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.

</details>


### [249] [A Pragmatic Method for Comparing Clusterings with Overlaps and Outliers](https://arxiv.org/abs/2602.14855)
*Ryan DeWolfe,Paweł Prałat,François Théberge*

Main category: cs.LG

TL;DR: This paper introduces a new similarity measure for comparing clusterings that can handle outliers and overlapping clusters, addressing gaps in existing clustering evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current clustering comparison methods cannot properly handle clusterings with outliers (objects belonging to no cluster) or overlapping clusters (objects belonging to multiple clusters), creating a gap in clustering evaluation for these common real-world scenarios.

Method: The authors define a pragmatic similarity measure specifically designed for comparing clusterings with overlaps and outliers, demonstrating its desirable mathematical properties through theoretical analysis.

Result: The proposed similarity measure exhibits several desirable properties and experimentally shows it is not subject to common biases that afflict other clustering comparison measures.

Conclusion: This work provides a practical solution for evaluating clusterings in realistic scenarios involving outliers and overlapping clusters, filling an important methodological gap in unsupervised learning evaluation.

Abstract: Clustering algorithms are an essential part of the unsupervised data science ecosystem, and extrinsic evaluation of clustering algorithms requires a method for comparing the detected clustering to a ground truth clustering. In a general setting, the detected and ground truth clusterings may have outliers (objects belonging to no cluster), overlapping clusters (objects may belong to more than one cluster), or both, but methods for comparing these clusterings are currently undeveloped. In this note, we define a pragmatic similarity measure for comparing clusterings with overlaps and outliers, show that it has several desirable properties, and experimentally confirm that it is not subject to several common biases afflicting other clustering comparison measures.

</details>


### [250] [Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning](https://arxiv.org/abs/2602.14868)
*Ilia Mahrooghi,Aryo Lotfi,Emmanuel Abbe*

Main category: cs.LG

TL;DR: Goldilocks: A teacher-driven data sampling strategy that predicts question difficulty for student models, selecting "just right" questions using the Goldilocks principle to improve RL training efficiency for reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for reasoning in LLMs suffers from sample inefficiency due to sparse rewards. Traditional curriculum learning has unclear optimal ordering, and models struggle to navigate vast search spaces with minimal feedback.

Method: Proposes Goldilocks: a teacher model predicts question difficulty for the student, selecting questions that are neither too easy nor too hard (Goldilocks principle). The teacher continuously adapts to student's evolving abilities using performance feedback, while student is trained with GRPO.

Result: On the OpenMathReasoning dataset, Goldilocks improves performance of models trained with standard GRPO under the same compute budget, demonstrating better training efficiency.

Conclusion: Goldilocks effectively addresses RL sample inefficiency by dynamically selecting appropriately difficult questions, offering a promising approach to curriculum learning for reasoning tasks in LLMs.

Abstract: Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.

</details>


### [251] [On the Learning Dynamics of RLVR at the Edge of Competence](https://arxiv.org/abs/2602.14872)
*Yu Huang,Zixin Wen,Yuejie Chi,Yuting Wei,Aarti Singh,Yingbin Liang,Yuxin Chen*

Main category: cs.LG

TL;DR: RLVR training dynamics show that effectiveness depends on difficulty spectrum smoothness: smooth spectra enable steady improvement through relay effects, while abrupt difficulty transitions cause plateaus before breakthroughs.


<details>
  <summary>Details</summary>
Motivation: To understand how RLVR with only final outcome rewards can overcome the long-horizon barrier in extended reasoning tasks, since it remains mysterious despite driving recent breakthroughs.

Method: Developed a theory of RL training dynamics for transformers on compositional reasoning tasks, using Fourier analysis on finite groups adapted to this setting, and validated mechanisms through synthetic experiments.

Result: RLVR effectiveness is governed by smoothness of difficulty spectrum: smooth difficulty spectra create relay effects enabling steady improvement, while abrupt difficulty discontinuities cause grokking-type phase transitions with prolonged plateaus before progress recurs.

Conclusion: RLVR can improve performance at the edge of competence, and appropriately designed data mixtures with smooth difficulty progression can yield scalable gains in reasoning capabilities.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.

</details>


### [252] [Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment](https://arxiv.org/abs/2602.14889)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: Web-Scale Multimodal Summarization framework that retrieves web content and generates summaries by combining text and images using retrieval and vision models.


<details>
  <summary>Details</summary>
Motivation: To create a configurable tool for generating summaries from web-scale multimodal data (text and images) that requires integration of language, retrieval, and vision models.

Method: Lightweight framework that performs parallel web, news, and image searches given a topic, ranks images using fine-tuned CLIP for semantic alignment, and optionally uses BLIP captioning for image-only summaries. Includes adjustable fetch limits, semantic filtering, and structured output generation.

Result: Achieved ROC-AUC of 0.9270, F1-score of 0.6504, and accuracy of 96.99% on evaluation with 500 image-caption pairs and contrastive negatives, demonstrating strong multimodal alignment capabilities.

Conclusion: Provides a deployable, configurable tool for web-scale multimodal summarization that successfully integrates language, retrieval, and vision models in a user-extensible pipeline.

Abstract: We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.

</details>


### [253] [Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs](https://arxiv.org/abs/2602.14896)
*Pedram Bakhtiarifard,Tong Chen,Jonathan Wenshøj,Erik B Dam,Raghavendra Selvan*

Main category: cs.LG

TL;DR: This paper explores why deep neural networks are compressible from an algorithmic complexity perspective, showing that trained models have lower Kolmogorov complexity than random initialization, and introduces Mosaic-of-Motifs (MoMos) to create algorithmically simpler models through reusable parameter motifs.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the observation that deep learning models are highly compressible through methods like pruning and quantization with minimal performance drops. The authors seek to understand WHY neural networks are suited for compression from a fundamental perspective, specifically through the lens of algorithmic complexity.

Method: The authors formalize the Kolmogorov complexity of model parameters and introduce MoMos (Mosaic-of-Motifs), a constrained parameterization method that partitions parameters into blocks and restricts each block to be selected from a small set of reusable motifs. This creates algorithmically simpler models compared to unconstrained parameterizations.

Result: Empirical evidence shows that neural networks indeed have reduced algorithmic complexity during training compared to initialization. MoMos produces models that perform comparably to unconstrained models while being algorithmically simpler, demonstrating that algorithmic complexity reduction is achievable without significant performance degradation.

Conclusion: Deep neural networks are suited for compression because their trained parameters exhibit lower algorithmic complexity than random initialization, possessing structure and repeatability that compression methods can exploit. The algorithmic complexity perspective provides a principled explanation for neural network compressibility.

Abstract: Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises the central question: \emph{Why are deep neural networks suited for compression?} In this work, we take up the perspective of algorithmic complexity to explain this behavior. We hypothesize that the parameters of trained models have more structure and, hence, exhibit lower algorithmic complexity compared to the weights at (random) initialization. Furthermore, that model compression methods harness this reduced algorithmic complexity to compress models. Although an unconstrained parameterization of model weights, $\mathbf{w} \in \mathbb{R}^n$, can represent arbitrary weight assignments, the solutions found during training exhibit repeatability and structure, making them algorithmically simpler than a generic program. To this end, we formalize the Kolmogorov complexity of $\mathbf{w}$ by $\mathcal{K}(\mathbf{w})$. We introduce a constrained parameterization $\widehat{\mathbf{w}}$, that partitions parameters into blocks of size $s$, and restricts each block to be selected from a set of $k$ reusable motifs, specified by a reuse pattern (or mosaic). The resulting method, $\textit{Mosaic-of-Motifs}$ (MoMos), yields algorithmically simpler model parameterization compared to unconstrained models. Empirical evidence from multiple experiments shows that the algorithmic complexity of neural networks, measured using approximations to Kolmogorov complexity, can be reduced during training. This results in models that perform comparably with unconstrained models while being algorithmically simpler.

</details>


### [254] [Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift](https://arxiv.org/abs/2602.14913)
*Farbod Siahkali,Ashwin Verma,Vijay Gupta*

Main category: cs.LG

TL;DR: Pseudo-calibration improves conformal prediction robustness under distribution shift by using domain adaptation tools to guarantee target coverage with inflated prediction sets.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction's coverage guarantees fail under distribution shift (non-exchangeable data), and there's a need to maintain reliable uncertainty quantification when data distributions change between source and target domains.

Method: 1) Analyzed pseudo-calibration under bounded label-conditional covariate shift using domain adaptation tools; 2) Derived lower bound on target coverage in terms of source-domain classifier loss and Wasserstein shift measure; 3) Proposed method to design pseudo-calibrated sets by inflating conformal threshold with slack parameter; 4) Developed source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels based on classifier uncertainty.

Result: Theoretical bounds qualitatively track pseudo-calibration behavior. Source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes in numerical experiments.

Conclusion: Pseudo-calibration with properly designed slack parameters and uncertainty-aware labeling strategies can provide robust coverage guarantees for conformal prediction under bounded distribution shifts, addressing a key limitation of standard conformal methods.

Abstract: Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes.

</details>


### [255] [Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation](https://arxiv.org/abs/2602.14914)
*Olivier Jeunen,Shashank Gupta*

Main category: cs.LG

TL;DR: Theoretical analysis proves that additive baseline corrections (β*-IPS) asymptotically dominate self-normalised inverse propensity scoring (SNIPS) in off-policy evaluation for ranking/recommendation systems.


<details>
  <summary>Details</summary>
Motivation: While self-normalised inverse propensity scoring (SNIPS) has been standard for variance reduction in off-policy evaluation, recent advances suggest additive control variates (baseline corrections) may offer superior performance, but theoretical guarantees for evaluation are lacking.

Method: The paper proves theoretical results that β*-IPS (optimal additive baseline) asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, they show SNIPS is asymptotically equivalent to using a specific but generally sub-optimal additive baseline.

Result: Theoretical proof shows β*-IPS asymptotically dominates SNIPS in MSE. Analysis reveals SNIPS is equivalent to a sub-optimal additive baseline, explaining why additive corrections can be superior.

Conclusion: Results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation systems in off-policy evaluation.

Abstract: Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $β^\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.

</details>


### [256] [BHyGNN+: Unsupervised Representation Learning for Heterophilic Hypergraphs](https://arxiv.org/abs/2602.14919)
*Tianyi Ma,Yiyue Qian,Zehong Wang,Zheyuan Zhang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: BHyGNN+ extends BHyGNN as a self-supervised learning framework for heterophilic hypergraphs using hypergraph duality and contrastive learning without needing ground-truth labels or negative samples.


<details>
  <summary>Details</summary>
Motivation: Existing HyGNNs struggle with heterophilic hypergraphs where connected nodes have dissimilar features, and they typically require labeled data which is often scarce in real-world applications.

Method: Proposes BHyGNN+, a self-supervised framework based on hypergraph duality. It contrasts augmented views of a hypergraph against its dual using cosine similarity to capture structural patterns without requiring negative samples.

Result: Extensive experiments on 11 benchmark datasets show BHyGNN+ outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs.

Conclusion: Leveraging hypergraph duality is effective for self-supervised learning and establishes a new paradigm for representation learning on unlabeled, challenging hypergraphs.

Abstract: Hypergraph Neural Networks (HyGNNs) have demonstrated remarkable success in modeling higher-order relationships among entities. However, their performance often degrades on heterophilic hypergraphs, where nodes connected by the same hyperedge tend to have dissimilar semantic representations or belong to different classes. While several HyGNNs, including our prior work BHyGNN, have been proposed to address heterophily, their reliance on labeled data significantly limits their applicability in real-world scenarios where annotations are scarce or costly. To overcome this limitation, we introduce BHyGNN+, a self-supervised learning framework that extends BHyGNN for representation learning on heterophilic hypergraphs without requiring ground-truth labels. The core idea of BHyGNN+ is hypergraph duality, a structural transformation where the roles of nodes and hyperedges are interchanged. By contrasting augmented views of a hypergraph against its dual using cosine similarity, our framework captures essential structural patterns in a fully unsupervised manner. Notably, this duality-based formulation eliminates the need for negative samples, a common requirement in existing hypergraph contrastive learning methods that is often difficult to satisfy in practice. Extensive experiments on eleven benchmark datasets demonstrate that BHyGNN+ consistently outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs. Our results validate the effectiveness of leveraging hypergraph duality for self-supervised learning and establish a new paradigm for representation learning on challenging, unlabeled hypergraphs.

</details>


### [257] [Variance-Reduced $(\varepsilon,δ)-$Unlearning using Forget Set Gradients](https://arxiv.org/abs/2602.14938)
*Martin Van Waerebeke,Marco Lorenzi,Kevin Scaman,El Mahdi El Mhamdi,Giovanni Neglia*

Main category: cs.LG

TL;DR: VRU is a first-order algorithm for machine unlearning that provably satisfies (ε,δ)-unlearning guarantees while directly incorporating forget set gradients in its optimization, achieving improved convergence rates over methods that ignore the forget set.


<details>
  <summary>Details</summary>
Motivation: There's a gap between formal (ε,δ)-unlearning methods that don't use forget set gradients directly (only for noise calibration) and empirical heuristics that exploit forget samples but lack formal guarantees.

Method: Variance-Reduced Unlearning (VRU) algorithm that is the first first-order method to directly include forget set gradients in its update rule while maintaining provable (ε,δ)-unlearning guarantees.

Result: VRU achieves strictly improved convergence rates compared to existing first-order (ε,δ)-unlearning methods, and asymptotically outperforms any first-order method that ignores the forget set in low-error regimes. Experiments confirm consistent gains over both certified unlearning methods and empirical baselines.

Conclusion: VRU successfully bridges the gap between formal guarantees and empirical efficiency by directly leveraging forget set gradients while maintaining provable unlearning guarantees, offering a principled approach to certified machine unlearning.

Abstract: In machine unlearning, $(\varepsilon,δ)-$unlearning is a popular framework that provides formal guarantees on the effectiveness of the removal of a subset of training data, the forget set, from a trained model. For strongly convex objectives, existing first-order methods achieve $(\varepsilon,δ)-$unlearning, but they only use the forget set to calibrate injected noise, never as a direct optimization signal. In contrast, efficient empirical heuristics often exploit the forget samples (e.g., via gradient ascent) but come with no formal unlearning guarantees. We bridge this gap by presenting the Variance-Reduced Unlearning (VRU) algorithm. To the best of our knowledge, VRU is the first first-order algorithm that directly includes forget set gradients in its update rule, while provably satisfying ($(\varepsilon,δ)-$unlearning. We establish the convergence of VRU and show that incorporating the forget set yields strictly improved rates, i.e. a better dependence on the achieved error compared to existing first-order $(\varepsilon,δ)-$unlearning methods. Moreover, we prove that, in a low-error regime, VRU asymptotically outperforms any first-order method that ignores the forget set.Experiments corroborate our theory, showing consistent gains over both state-of-the-art certified unlearning methods and over empirical baselines that explicitly leverage the forget set.

</details>


### [258] [Locally Adaptive Multi-Objective Learning](https://arxiv.org/abs/2602.14952)
*Jivat Neet Kaur,Isaac Gibbs,Michael I. Jordan*

Main category: cs.LG

TL;DR: The paper proposes an adaptive online multi-objective learning method that achieves local guarantees and robustness under distribution shift, outperforming existing approaches on energy forecasting and fairness datasets.


<details>
  <summary>Details</summary>
Motivation: Existing multi-objective online learning methods minimize objectives over entire time horizons in a worst-case sense and don't adapt well to distribution shifts. Current attempts at local guarantees have limited empirical validation.

Method: The authors propose replacing one component of multi-objective learning with an adaptive online algorithm to achieve local adaptivity, enabling the method to respond to distribution shifts over time.

Result: Empirical evaluations on energy forecasting and algorithmic fairness datasets show the proposed method outperforms existing approaches, achieves unbiased predictions over subgroups, and remains robust under distribution shift.

Conclusion: The adaptive multi-objective learning approach effectively addresses distribution shift challenges in online settings, providing practical improvements for real-world applications requiring simultaneous satisfaction of multiple objectives.

Abstract: We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.

</details>


### [259] [Use What You Know: Causal Foundation Models with Partial Graphs](https://arxiv.org/abs/2602.14972)
*Arik Reuter,Anish Dhir,Cristiana Diaconu,Jake Robertson,Ole Ossen,Frank Hutter,Adrian Weller,Mark van der Wilk,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: Causal Foundation Models (CFMs) can be enhanced by conditioning on causal information, allowing them to incorporate domain knowledge and match specialized model performance.


<details>
  <summary>Details</summary>
Motivation: Existing CFMs currently lack the ability to incorporate domain knowledge (causal graphs, ancestral information), which leads to suboptimal predictions compared to specialized models.

Method: Introduce methods to condition CFMs on causal information, including full causal graphs or partial ancestral information. Use attention mechanism conditioning strategies, particularly learnable bias injection, to effectively leverage both full and partial causal information.

Result: Conditioned CFMs achieve performance comparable to specialized models designed for specific causal structures. Attention mechanism conditioning with learnable biases proves most effective for utilizing causal information.

Conclusion: The approach bridges a key gap in CFMs by enabling incorporation of domain knowledge, moving toward all-in-one causal foundation models that can answer causal queries data-driven while leveraging domain expertise.

Abstract: Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.

</details>


### [260] [MacroGuide: Topological Guidance for Macrocycle Generation](https://arxiv.org/abs/2602.14977)
*Alicja Maksymiuk,Alexandre Duplessis,Michael Bronstein,Alexander Tong,Fernanda Duarte,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: MacroGuide is a novel guidance mechanism using Persistent Homology to steer diffusion models toward generating macrocycles, dramatically increasing generation rates from 1% to 99% while maintaining quality metrics.


<details>
  <summary>Details</summary>
Motivation: Macrocycles are promising drug candidates with enhanced selectivity and binding affinity, but they remain underexplored in generative modeling due to scarcity in public datasets and challenges in enforcing topological constraints in standard deep generative models.

Method: MacroGuide uses diffusion guidance with Persistent Homology. At each denoising step, it constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features, steering pretrained molecular generative models toward macrocycle generation.

Result: Applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics including chemical validity, diversity, and PoseBusters checks.

Conclusion: MacroGuide successfully addresses the challenges of macrocycle generation by integrating topological guidance via Persistent Homology into diffusion models, enabling efficient generation of high-quality macrocycles for drug discovery applications.

Abstract: Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. We introduce MacroGuide: Topological Guidance for Macrocycle Generation, a diffusion guidance mechanism that uses Persistent Homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. At each denoising step, MacroGuide constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. Empirically, applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and PoseBusters checks.

</details>


### [261] [Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations](https://arxiv.org/abs/2602.14983)
*Carolin Cissee,Raneen Younis,Zahra Ahmadi*

Main category: cs.LG

TL;DR: COrAL is a multimodal contrastive learning framework that explicitly models redundant, unique, and synergistic information through dual-path architecture with orthogonality constraints and asymmetric masking.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised multimodal contrastive learning methods predominantly capture redundant cross-modal signals while neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage.

Method: COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features. It introduces asymmetric masking with complementary view-specific patterns to promote synergy modeling, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues.

Result: Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs.

Conclusion: Explicitly modeling the full spectrum of multimodal information (redundant, unique, and synergistic) yields more stable, reliable, and comprehensive embeddings, as demonstrated by COrAL's superior performance and low variance.

Abstract: Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.

</details>


### [262] [Spectral Convolution on Orbifolds for Geometric Deep Learning](https://arxiv.org/abs/2602.14997)
*Tim Mangliers,Bernhard Mössner,Benjamin Himpel*

Main category: cs.LG

TL;DR: Introduces spectral convolution on orbifolds for geometric deep learning, extending learning capabilities to data with orbifold structure, demonstrated with a music theory example.


<details>
  <summary>Details</summary>
Motivation: Geometric deep learning needs to handle non-Euclidean data domains like graphs and manifolds. Real-world applications require identification of further topological/geometric structures to make diverse use cases accessible to machine learning. Orbifolds represent such structures that need to be incorporated into GDL frameworks.

Method: Introduces the concept of spectral convolution on orbifolds as a building block for convolutional neural network-like architectures on orbifold-structured data. Adapts spectral convolution techniques from existing GDL methods to work with orbifold structures.

Result: Develops a theoretical framework for spectral convolution on orbifolds, creating a foundational building block for learning on orbifold-structured data within geometric deep learning. Presents a music theory example to illustrate the application of this concept.

Conclusion: Spectral convolution on orbifolds expands geometric deep learning to handle orbifold-structured data, bridging the gap between theoretical topological structures and practical machine learning applications. The music theory example demonstrates the real-world applicability of this approach.

Abstract: Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [263] [Agent Mars: Multi-Agent Simulation for Multi-Planetary Life Exploration and Settlement](https://arxiv.org/abs/2602.13291)
*Ziyang Wang*

Main category: cs.MA

TL;DR: Agent Mars is an open, end-to-end multi-agent simulation framework for Mars base operations that models realistic organizational structures with 93 agents across seven layers, enabling studies of coordination under space-specific constraints like delayed communications and resource scarcity.


<details>
  <summary>Details</summary>
Motivation: Space exploration presents unique challenges unmatched on Earth: delayed/intermittent communications, extreme resource scarcity, heterogeneous expertise, and strict safety/accountability requirements. A key challenge is achieving auditable coordination among specialized humans, robots, and digital services in safety-critical system-of-systems environments.

Method: Created Agent Mars framework with 93-agent roster across seven layers of command and execution, implementing hierarchical/cross-layer coordination with audit trails. Features dynamic role handover with automatic failover, phase-dependent leadership, scenario-aware short/long-horizon memory, configurable propose-vote consensus, and translator-mediated heterogeneous protocols. Proposed Agent Mars Performance Index (AMPI) for interpretable performance measurement.

Result: Across 13 reproducible Mars-relevant operational scripts, Agent Mars revealed coordination trade-offs and identified regimes where curated cross-layer collaboration and functional leadership reduce overhead without sacrificing reliability. The framework provides benchmarkable, auditable foundation for Space AI.

Conclusion: Agent Mars offers a comprehensive simulation framework that addresses the unique coordination challenges of space exploration, providing a foundation for developing and testing AI systems for future Mars missions and other space operations where safety-critical, auditable multi-agent coordination is essential.

Abstract: Artificial Intelligence (AI) has transformed robotics, healthcare, industry, and scientific discovery, yet a major frontier may lie beyond Earth. Space exploration and settlement offer vast environments and resources, but impose constraints unmatched on Earth: delayed/intermittent communications, extreme resource scarcity, heterogeneous expertise, and strict safety, accountability, and command authority. The key challenge is auditable coordination among specialised humans, robots, and digital services in a safety-critical system-of-systems. We introduce Agent Mars, an open, end-to-end multi-agent simulation framework for Mars base operations. Agent Mars formalises a realistic organisation with a 93-agent roster across seven layers of command and execution (human roles and physical assets), enabling base-scale studies beyond toy settings. It implements hierarchical and cross-layer coordination that preserves chain-of-command while allowing vetted cross-layer exchanges with audit trails; supports dynamic role handover with automatic failover under outages; and enables phase-dependent leadership for routine operations, emergencies, and science campaigns. Agent Mars further models mission-critical mechanisms-scenario-aware short/long-horizon memory, configurable propose-vote consensus, and translator-mediated heterogeneous protocols-to capture how teams align under stress. To quantify behaviour, we propose the Agent Mars Performance Index (AMPI), an interpretable composite score with diagnostic sub-metrics. Across 13 reproducible Mars-relevant operational scripts, Agent Mars reveals coordination trade-offs and identifies regimes where curated cross-layer collaboration and functional leadership reduce overhead without sacrificing reliability. Agent Mars provides a benchmarkable, auditable foundation for Space AI.

</details>


### [264] [Adaptive Value Decomposition: Coordinating a Varying Number of Agents in Urban Systems](https://arxiv.org/abs/2602.13309)
*Yexin Li,Jinjin Guo,Haoyu Zhang,Yuhan Zhao,Yiwen Sun,Zihao Jiao*

Main category: cs.MA

TL;DR: AVD is a cooperative MARL framework for urban systems that adapts to dynamic agent populations, mitigates action homogenization from shared policies, and supports asynchronous decision-making.


<details>
  <summary>Details</summary>
Motivation: Current MARL methods rely on restrictive assumptions (fixed agent count, synchronous actions) that don't hold in urban systems where agent populations vary dynamically and actions have heterogeneous durations. Shared policy parameters can lead to homogeneous actions and degrade coordination quality in semi-MARL settings.

Method: AVD (Adaptive Value Decomposition) includes: 1) A framework adapting to dynamically changing agent populations, 2) A lightweight mechanism to mitigate action homogenization induced by shared policies to encourage behavioral diversity, 3) A training-execution strategy for semi-MARL that accommodates asynchronous decision-making when agents act at different times.

Result: Experiments on real-world bike-sharing redistribution tasks in London and Washington, D.C. demonstrate that AVD outperforms state-of-the-art baselines, confirming its effectiveness and generalizability.

Conclusion: AVD successfully addresses limitations of existing MARL methods in urban systems by handling dynamic agent populations, mitigating action homogenization, and supporting asynchronous decision-making, making it effective for practical multi-agent coordination tasks.

Abstract: Multi-agent reinforcement learning (MARL) provides a promising paradigm for coordinating multi-agent systems (MAS). However, most existing methods rely on restrictive assumptions, such as a fixed number of agents and fully synchronous action execution. These assumptions are often violated in urban systems, where the number of active agents varies over time, and actions may have heterogeneous durations, resulting in a semi-MARL setting. Moreover, while sharing policy parameters among agents is commonly adopted to improve learning efficiency, it can lead to highly homogeneous actions when a subset of agents make decisions concurrently under similar observations, potentially degrading coordination quality. To address these challenges, we propose Adaptive Value Decomposition (AVD), a cooperative MARL framework that adapts to a dynamically changing agent population. AVD further incorporates a lightweight mechanism to mitigate action homogenization induced by shared policies, thereby encouraging behavioral diversity and maintaining effective cooperation among agents. In addition, we design a training-execution strategy tailored to the semi-MARL setting that accommodates asynchronous decision-making when some agents act at different times. Experiments on real-world bike-sharing redistribution tasks in two major cities, London and Washington, D.C., demonstrate that AVD outperforms state-of-the-art baselines, confirming its effectiveness and generalizability.

</details>


### [265] [PeroMAS: A Multi-agent System of Perovskite Material Discovery](https://arxiv.org/abs/2602.13312)
*Yishu Wang,Wei Liu,Yifan Li,Shengxiang Xu,Xujie Yuan,Ran Li,Yuyu Luo,Jia Zhu,Shimin Di,Min-Ling Zhang,Guixiang Li*

Main category: cs.MA

TL;DR: PeroMAS is a multi-agent system that automates end-to-end perovskite material discovery by integrating literature retrieval, data extraction, property prediction, and mechanism analysis under multi-objective constraints.


<details>
  <summary>Details</summary>
Motivation: Current AI approaches for perovskite solar cells focus on discrete models for material design, process optimization, and property prediction, but fail to propagate physical constraints across the workflow, hindering end-to-end optimization of the complex perovskite development process.

Method: PeroMAS encapsulates perovskite-specific tools into Model Context Protocols (MCPs) and uses a multi-agent system to plan and invoke these tools for designing perovskite materials under multi-objective constraints, covering literature retrieval, data extraction, property prediction, and mechanism analysis.

Result: The system significantly enhances discovery efficiency compared to single LLM or traditional search strategies, successfully identifies candidate materials satisfying multi-objective constraints, and its effectiveness is verified through real synthesis experiments in the physical world.

Conclusion: PeroMAS represents an effective multi-agent approach for automated perovskite material discovery that addresses the limitations of discrete AI models by enabling end-to-end optimization with physical constraint propagation across the entire workflow.

Abstract: As a pioneer of the third-generation photovoltaic revolution, Perovskite Solar Cells (PSCs) are renowned for their superior optoelectronic performance and cost potential. The development process of PSCs is precise and complex, involving a series of closed-loop workflows such as literature retrieval, data integration, experimental design, and synthesis. However, existing AI perovskite approaches focus predominantly on discrete models, including material design, process optimization,and property prediction. These models fail to propagate physical constraints across the workflow, hindering end-to-end optimization. In this paper, we propose a multi-agent system for perovskite material discovery, named PeroMAS. We first encapsulated a series of perovskite-specific tools into Model Context Protocols (MCPs). By planning and invoking these tools, PeroMAS can design perovskite materials under multi-objective constraints, covering the entire process from literature retrieval and data extraction to property prediction and mechanism analysis. Furthermore, we construct an evaluation benchmark by perovskite human experts to assess this multi-agent system. Results demonstrate that, compared to single Large Language Model (LLM) or traditional search strategies, our system significantly enhances discovery efficiency. It successfully identified candidate materials satisfying multi-objective constraints. Notably, we verify PeroMAS's effectiveness in the physical world through real synthesis experiments.

</details>


### [266] [Robust Mean-Field Games with Risk Aversion and Bounded Rationality](https://arxiv.org/abs/2602.13353)
*Bhavini Jeloka,Yue Guan,Panagiotis Tsiotras*

Main category: cs.MA

TL;DR: The paper introduces MF-RQE, a new equilibrium concept combining risk aversion to initial distribution uncertainty with bounded rationality, providing robust policies that outperform classical mean-field approaches.


<details>
  <summary>Details</summary>
Motivation: Existing mean-field game approaches assume fixed initial population distributions and fully rational agents, which limits robustness under distributional uncertainty and cognitive constraints. There's a need for more general models that account for these limitations.

Method: The authors introduce two key elements: 1) risk aversion with respect to the initial population distribution, and 2) bounded rationality to model deviations from fully rational decision-making. These are combined to create the new Mean-Field Risk-Averse Quantal Response Equilibrium (MF-RQE) concept.

Result: The authors establish existence results for MF-RQE and prove convergence of fixed-point iteration and fictitious play algorithms to this equilibrium. They also develop a scalable reinforcement learning algorithm for large state-action spaces, and numerical experiments show MF-RQE policies achieve improved robustness over classical mean-field approaches.

Conclusion: MF-RQE provides a more general and robust equilibrium framework that addresses limitations of classical mean-field games, offering improved performance under distributional uncertainty while accounting for bounded rationality, with proven theoretical guarantees and practical algorithms.

Abstract: Recent advances in mean-field game literature enable the reduction of large-scale multi-agent problems to tractable interactions between a representative agent and a population distribution. However, existing approaches typically assume a fixed initial population distribution and fully rational agents, limiting robustness under distributional uncertainty and cognitive constraints. We address these limitations by introducing risk aversion with respect to the initial population distribution and by incorporating bounded rationality to model deviations from fully rational decision-making agents. The combination of these two elements yields a new and more general equilibrium concept, which we term the mean-field risk-averse quantal response equilibrium (MF-RQE). We establish existence results and prove convergence of fixed-point iteration and fictitious play to MF-RQE. Building on these insights, we develop a scalable reinforcement learning algorithm for scenarios with large state-action spaces. Numerical experiments demonstrate that MF-RQE policies achieve improved robustness relative to classical mean-field approaches that optimize expected cumulative rewards under a fixed initial distribution and are restricted to entropy-based regularizers.

</details>


### [267] [G2CP: A Graph-Grounded Communication Protocol for Verifiable and Efficient Multi-Agent Reasoning](https://arxiv.org/abs/2602.13370)
*Karim Ben Khaled,Davy Monticolo*

Main category: cs.MA

TL;DR: G2CP introduces a structured graph-based communication protocol for multi-agent systems that replaces free text with explicit graph operations, reducing tokens by 73% and improving accuracy by 34% while eliminating hallucinations.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems using natural language communication suffer from semantic drift, hallucination propagation, and inefficient token consumption, necessitating more structured communication approaches.

Method: Proposes G2CP (Graph-Grounded Communication Protocol) where agents exchange graph operations (traversal commands, subgraph fragments, and update operations) over a shared knowledge graph instead of free text, enabling verifiable reasoning traces.

Result: Experimental evaluation on 500 industrial scenarios and 21 real-world maintenance cases shows: 73% reduction in inter-agent communication tokens, 34% improvement in task completion accuracy over free-text baselines, elimination of cascading hallucinations, and fully auditable reasoning chains.

Conclusion: G2CP represents a fundamental shift from linguistic to structural communication in multi-agent systems, with broad implications for domains requiring precise agent coordination, enabling verifiable reasoning and eliminating ambiguity.

Abstract: Multi-agent systems powered by Large Language Models face a critical challenge: agents communicate through natural language, leading to semantic drift, hallucination propagation, and inefficient token consumption. We propose G2CP (Graph-Grounded Communication Protocol), a structured agent communication language where messages are graph operations rather than free text. Agents exchange explicit traversal commands, subgraph fragments, and update operations over a shared knowledge graph, enabling verifiable reasoning traces and eliminating ambiguity. We validate G2CP within an industrial knowledge management system where specialized agents (Diagnostic, Procedural, Synthesis, and Ingestion) coordinate to answer complex queries. Experimental results on 500 industrial scenarios and 21 real-world maintenance cases show that G2CP reduces inter-agent communication tokens by 73%, improves task completion accuracy by 34% over free-text baselines, eliminates cascading hallucinations, and produces fully auditable reasoning chains. G2CP represents a fundamental shift from linguistic to structural communication in multi-agent systems, with implications for any domain requiring precise agent coordination. Code, data, and evaluation scripts are publicly available.

</details>


### [268] [MAS-on-the-Fly: Dynamic Adaptation of LLM-based Multi-Agent Systems at Test Time](https://arxiv.org/abs/2602.13671)
*Guangyi Liu,Haojun Lin,Huan Zeng,Heng Wang,Quanming Yao*

Main category: cs.MA

TL;DR: MASFly: A novel multi-agent framework that enables dynamic adaptation at test time through retrieval-augmented SOP instantiation and experience-guided supervision, achieving state-of-the-art performance on complex tasks like TravelPlanner.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based multi-agent systems lack dynamic adaptability after deployment, relying on manual designs or "one-size-fits-all" automation. The authors are inspired by how biological systems adapt and aim to create a framework that can dynamically adjust to new tasks.

Method: MASFly employs two key mechanisms: 1) Retrieval-augmented SOP instantiation - uses a self-constructed repository of successful collaboration patterns to assemble customized MASs for new queries, and 2) Experience-guided supervision - uses a dedicated Watcher agent that monitors system behaviors with reference to a personalized experience pool and provides real-time interventions.

Result: MASFly achieves state-of-the-art performance, most notably a 61.7% success rate on the TravelPlanner benchmark, while exhibiting strong task adaptability and robustness in extensive experiments.

Conclusion: MASFly successfully enables dynamic adaptation in multi-agent systems at test time, addressing the limitations of existing approaches and demonstrating superior performance and adaptability through its dual adaptation mechanisms.

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) have emerged as a promising paradigm for solving complex tasks. However, existing works often rely on manual designs or "one-size-fits-all" automation, lacking dynamic adaptability after deployment. Inspired by how biological systems adapt, we introduce MASFly, a novel multi-agent framework enabling dynamic adaptation at test time. To adapt system generation, MASFly employs a retrieval-augmented SOP instantiation mechanism that leverages a self-constructed repository of successful collaboration patterns, enabling the LLM to assemble customized MASs for new queries. For adaptive execution, MASFly incorporates an experience-guided supervision mechanism, where a dedicated Watcher agent monitors system behaviors with reference to a personalized experience pool and provides real-time interventions. Extensive experiments demonstrate that MASFly achieves state-of-the-art performance, most notably a 61.7% success rate on the TravelPlanner benchmark, while exhibiting strong task adaptability and robustness.

</details>


### [269] [Testing BDI-based Multi-Agent Systems using Discrete Event Simulation](https://arxiv.org/abs/2602.13878)
*Martina Baiardi,Samuele Burattini,Giovanni Ciatto,Danilo Pianini*

Main category: cs.MA

TL;DR: Integrating BDI agents with Discrete Event Simulation for testing multi-agent systems, enabling simulation of the same specifications that will be deployed.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems are hard to test due to open, distributed nature with unpredictable dynamics. Simulation helps but fidelity is challenging, especially for cognitive agents like BDI where code can't run unchanged in simulation, creating a reality gap.

Method: Propose mapping BDI agent control flow onto Discrete Event Simulation (DES) at different granularities. Implement open-source prototype integration between JaKtA and Alchemist tools.

Result: Show that simulation-based testing environment for distributed BDI agents is possible. Demonstrate that different granularities in mapping BDI agents over DES lead to different degrees of fidelity.

Conclusion: BDI developers can test the same specification in simulation that will be deployed, bridging the reality gap and enabling more effective testing of multi-agent systems through DES integration.

Abstract: Multi-agent systems are designed to deal with open, distributed systems with unpredictable dynamics, which makes them inherently hard to test. The value of using simulation for this purpose is recognized in the literature, although achieving sufficient fidelity (i.e., the degree of similarity between the simulation and the real-world system) remains a challenging task. This is exacerbated when dealing with cognitive agent models, such as the Belief Desire Intention (BDI) model, where the agent codebase is not suitable to run unchanged in simulation environments, thus increasing the reality gap between the deployed and simulated systems. We argue that BDI developers should be able to test in simulation the same specification that will be later deployed, with no surrogate representations. Thus, in this paper, we discuss how the control flow of BDI agents can be mapped onto a Discrete Event Simulation (DES), showing that such integration is possible at different degrees of granularity. We substantiate our claims by producing an open-source prototype integration between two pre-existing tools (JaKtA and Alchemist), showing that it is possible to produce a simulation-based testing environment for distributed BDI} agents, and that different granularities in mapping BDI agents over DESs may lead to different degrees of fidelity.

</details>


### [270] [Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.14471)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.MA

TL;DR: SWA is a game-theoretic framework that balances individual and group welfare to stabilize multi-agent systems, preventing congestion through inference-time decision adjustments.


<details>
  <summary>Details</summary>
Motivation: There's a tension between individual alignment and collective stability in LLM agents deployed in shared environments - locally rational decisions can create negative externalities that degrade system performance.

Method: Proposed Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between private objectives and group welfare estimates using a social weight parameter λ∈[0,1].

Result: In a shared-resource congestion game, SWA induces a critical threshold λ*=(n-β)/(n-1) that triggers a phase transition from persistent congestion to stable operation near capacity. The framework was empirically validated through multi-agent simulation.

Conclusion: SWA provides a practical, inference-time approach to align individual LLM agents with social welfare without requiring parameter updates or multi-agent reinforcement learning, enabling stable collective behavior.

Abstract: Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.

</details>


### [271] [Towards Selection as Power: Bounding Decision Authority in Autonomous Agents](https://arxiv.org/abs/2602.14606)
*Jose Manuel de la Chica Rodriguez,Juan Manuel Vera Díaz*

Main category: cs.MA

TL;DR: A governance architecture for autonomous agents separates cognition, selection, and action, bounding selection power through mechanical primitives to prevent deterministic outcome capture while preserving reasoning capacity.


<details>
  <summary>Details</summary>
Motivation: Current safety approaches (alignment, interpretability, action filtering) are insufficient for regulated, high-stakes domains because they don't directly govern selection power - the authority to determine which options are generated, surfaced, and framed for decision. There's a need for governance that prevents silent failures in irreversible, institutionally constrained environments.

Method: Proposes a governance architecture that separates cognition, selection, and action into distinct domains, modeling autonomy as a vector of sovereignty. Cognitive autonomy remains unconstrained while selection and action autonomy are bounded through mechanical primitives outside the agent's optimization space. The architecture includes: external candidate generation (CEFL), governed reducer, commit-reveal entropy isolation, rationale validation, and fail-loud circuit breakers.

Result: Evaluation across regulated financial scenarios under adversarial stress shows mechanical selection governance is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity. Although probabilistic concentration remains, the architecture measurably bounds selection authority relative to conventional scalar pipelines, with metrics showing improved selection concentration, narrative diversity, governance activation cost, and failure visibility.

Conclusion: The work reframes governance as bounded causal power rather than internal intent alignment, offering a foundation for deploying autonomous agents where silent failure is unacceptable. The architecture successfully prevents deterministic outcome capture while maintaining reasoning capacity, making it suitable for regulated, high-stakes domains.

Abstract: Autonomous agentic systems are increasingly deployed in regulated, high-stakes domains where decisions may be irreversible and institutionally constrained. Existing safety approaches emphasize alignment, interpretability, or action-level filtering. We argue that these mechanisms are necessary but insufficient because they do not directly govern selection power: the authority to determine which options are generated, surfaced, and framed for decision. We propose a governance architecture that separates cognition, selection, and action into distinct domains and models autonomy as a vector of sovereignty. Cognitive autonomy remains unconstrained, while selection and action autonomy are bounded through mechanically enforced primitives operating outside the agent's optimization space. The architecture integrates external candidate generation (CEFL), a governed reducer, commit-reveal entropy isolation, rationale validation, and fail-loud circuit breakers. We evaluate the system across multiple regulated financial scenarios under adversarial stress targeting variance manipulation, threshold gaming, framing skew, ordering effects, and entropy probing. Metrics quantify selection concentration, narrative diversity, governance activation cost, and failure visibility. Results show that mechanical selection governance is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity. Although probabilistic concentration remains, the architecture measurably bounds selection authority relative to conventional scalar pipelines. This work reframes governance as bounded causal power rather than internal intent alignment, offering a foundation for deploying autonomous agents where silent failure is unacceptable.

</details>


### [272] [ST-EVO: Towards Generative Spatio-Temporal Evolution of Multi-Agent Communication Topologies](https://arxiv.org/abs/2602.14681)
*Xingjian Wu,Xvyuan Liu,Junkai Lu,Siyuan Wang,Yang Shu,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.MA

TL;DR: ST-EVO is a novel Spatio-Temporal evolving Multi-Agent System that uses flow-matching for dynamic scheduling with uncertainty perception and self-feedback, achieving 5-25% accuracy gains across 9 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current self-evolving MAS only consider either spatial or temporal evolution separately, limiting LLMs' collaborative potential. There's a need for unified spatio-temporal evolution that fully leverages LLM capabilities through adaptive communication and learning.

Method: ST-EVO introduces a spatio-temporal perspective with flow-matching based Scheduler for dialogue-wise communication scheduling. It includes uncertainty perception of MAS and self-feedback ability to learn from accumulated experience.

Result: Extensive experiments on nine benchmarks demonstrate state-of-the-art performance with 5-25% accuracy improvement over existing methods.

Conclusion: ST-EVO successfully addresses the limitations of single-dimension evolving MAS by proposing a unified spatio-temporal approach with uncertainty-aware scheduling and experience learning, significantly advancing collaborative intelligence capabilities.

Abstract: LLM-powered Multi-Agent Systems (MAS) have emerged as an effective approach towards collaborative intelligence, and have attracted wide research interests. Among them, ``self-evolving'' MAS, treated as a more flexible and powerful technical route, can construct task-adaptive workflows or communication topologies, instead of relying on a predefined static structue template. Current self-evolving MAS mainly focus on Spatial Evolving or Temporal Evolving paradigm, which only considers the single dimension of evolution and does not fully incentivize LLMs' collaborative capability. In this work, we start from a novel Spatio-Temporal perspective by proposing ST-EVO, which supports dialogue-wise communication scheduling with a compact yet powerful flow-matching based Scheduler. To make precise Spatio-Temporal scheduling, ST-EVO can also perceive the uncertainty of MAS, and possesses self-feedback ability to learn from accumulated experience. Extensive experiments on nine benchmarks demonstrate the state-of-the-art performance of ST-EVO, achieving about 5%--25% accuracy improvement.

</details>


### [273] [ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic](https://arxiv.org/abs/2602.14780)
*Anna-Lena Schlamp,Jeremias Gerner,Klaus Bogenberger,Werner Huber,Stefanie Schmidtner*

Main category: cs.MA

TL;DR: ROSA is a roundabout safety system that predicts trajectories of all road users using Transformers and provides real-time speed advisories to vehicles to prevent conflicts.


<details>
  <summary>Details</summary>
Motivation: Roundabouts present complex multimodal traffic interactions, especially with vulnerable road users (VRUs), creating safety risks. Traditional systems don't adequately address these mixed-traffic scenarios with prediction uncertainty.

Method: Uses Transformer-based model to jointly predict future trajectories of vehicles and VRUs at roundabouts. Trained for single-step prediction and deployed autoregressively to generate deterministic outputs. Incorporates motion dynamics and route intention data from connected vehicles for improved accuracy.

Result: Achieved high prediction accuracy (ADE: 1.29m, FDE: 2.99m at 5-second horizon). With route intention: ADE: 1.10m, FDE: 2.36m. ROSA provides real-time speed advisories based on predicted conflicts, significantly improving both vehicle efficiency and safety from VRU perspective.

Conclusion: ROSA effectively addresses roundabout safety challenges by combining multi-agent trajectory prediction with coordinated speed guidance, demonstrating practical value of connected vehicle data for proactive conflict avoidance in multimodal mixed traffic.

Abstract: We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.

</details>


### [274] [Distributed Quantum Gaussian Processes for Multi-Agent Systems](https://arxiv.org/abs/2602.15006)
*Meet Gandhi,George P. Kontoudis*

Main category: cs.MA

TL;DR: A Distributed Quantum Gaussian Process method that leverages quantum computing for enhanced expressivity in probabilistic modeling, using a distributed consensus optimization algorithm to scale across multiple agents.


<details>
  <summary>Details</summary>
Motivation: Classical Gaussian Processes are limited by traditional kernels in complex, large-scale domains. Quantum computing offers the potential to overcome these limitations by embedding data into exponentially large Hilbert spaces, capturing complex correlations inaccessible to classical approaches.

Method: Proposes a Distributed Quantum Gaussian Process (DQGP) method in a multi-agent setting. To solve the challenging non-Euclidean optimization problem, develops a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model.

Result: Evaluated the method through numerical experiments on a quantum simulator using classical hardware. Used real-world, non-stationary elevation datasets from NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes.

Conclusion: The framework demonstrates enhanced modeling capabilities and scalability while highlighting potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.

Abstract: Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.

</details>
